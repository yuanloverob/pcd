{
  "cve_id": "CVE-2023-3765",
  "cve_desc": "Absolute Path Traversal in GitHub repository mlflow/mlflow prior to 2.5.0.",
  "repo": "mlflow/mlflow",
  "patch_hash": "6dde93758d42455cb90ef324407919ed67668b9b",
  "patch_info": {
    "commit_hash": "6dde93758d42455cb90ef324407919ed67668b9b",
    "repo": "mlflow/mlflow",
    "commit_url": "https://github.com/mlflow/mlflow/commit/6dde93758d42455cb90ef324407919ed67668b9b",
    "files": [
      "mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py",
      "mlflow/pyfunc/backend.py",
      "mlflow/pyfunc/scoring_server/__init__.py",
      "tests/models/test_cli.py"
    ],
    "message": "Fix potential issues with PyFuncBackend in cli (#9053)\n\nSigned-off-by: Serena Ruan <serena.rxy@gmail.com>",
    "before_after_code_files": [
      "mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py||mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py",
      "mlflow/pyfunc/backend.py||mlflow/pyfunc/backend.py",
      "mlflow/pyfunc/scoring_server/__init__.py||mlflow/pyfunc/scoring_server/__init__.py",
      "tests/models/test_cli.py||tests/models/test_cli.py"
    ]
  },
  "patch_diff": {
    "mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py||mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py": [
      "File: mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py -> mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "[No context available]",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1: \"\"\"",
      "2: This script should be executed in a fresh python interpreter process using `subprocess`.",
      "3: \"\"\"",
      "4: import argparse",
      "6: from mlflow.pyfunc.scoring_server import _predict",
      "9: def parse_args():",
      "10:     parser = argparse.ArgumentParser()",
      "11:     parser.add_argument(\"--model-uri\", required=True)",
      "12:     parser.add_argument(\"--input-path\", required=False)",
      "13:     parser.add_argument(\"--output-path\", required=False)",
      "14:     parser.add_argument(\"--content-type\", required=True)",
      "15:     return parser.parse_args()",
      "18: def main():",
      "19:     args = parse_args()",
      "20:     _predict(",
      "21:         model_uri=args.model_uri,",
      "22:         input_path=args.input_path if args.input_path else None,",
      "23:         output_path=args.output_path if args.output_path else None,",
      "24:         content_type=args.content_type,",
      "25:     )",
      "28: if __name__ == \"__main__\":",
      "29:     main()",
      "",
      "---------------"
    ],
    "mlflow/pyfunc/backend.py||mlflow/pyfunc/backend.py": [
      "File: mlflow/pyfunc/backend.py -> mlflow/pyfunc/backend.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "3: import pathlib",
      "4: import subprocess",
      "5: import posixpath",
      "6: import sys",
      "7: import warnings",
      "8: import ctypes",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "6: import shlex",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "24: from mlflow.utils.conda import get_or_create_conda_env, get_conda_bin_executable",
      "25: from mlflow.tracking.artifact_utils import _download_artifact_from_uri",
      "26: from mlflow.utils import env_manager as _EnvManager",
      "27: from mlflow.utils.file_utils import (",
      "28:     path_to_local_file_uri,",
      "29:     get_or_create_tmp_dir,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "28: from mlflow.pyfunc import _mlflow_pyfunc_backend_predict",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "143:         local_uri = path_to_local_file_uri(local_path)",
      "145:         if self._env_manager != _EnvManager.LOCAL:",
      "160:         else:",
      "161:             scoring_server._predict(local_uri, input_path, output_path, content_type)",
      "",
      "[Removed Lines]",
      "146:             command = (",
      "147:                 'python -c \"from mlflow.pyfunc.scoring_server import _predict; _predict('",
      "148:                 \"model_uri={model_uri}, \"",
      "149:                 \"input_path={input_path}, \"",
      "150:                 \"output_path={output_path}, \"",
      "151:                 \"content_type={content_type})\"",
      "152:                 '\"'",
      "153:             ).format(",
      "154:                 model_uri=repr(local_uri),",
      "155:                 input_path=repr(input_path),",
      "156:                 output_path=repr(output_path),",
      "157:                 content_type=repr(content_type),",
      "158:             )",
      "159:             return self.prepare_env(local_path).execute(command)",
      "",
      "[Added Lines]",
      "148:             predict_cmd = [",
      "149:                 \"python\",",
      "150:                 _mlflow_pyfunc_backend_predict.__file__,",
      "151:                 \"--model-uri\",",
      "152:                 str(local_uri),",
      "153:                 \"--content-type\",",
      "154:                 shlex.quote(str(content_type)),",
      "155:             ]",
      "156:             if input_path:",
      "157:                 predict_cmd += [\"--input-path\", shlex.quote(str(input_path))]",
      "158:             if output_path:",
      "159:                 predict_cmd += [\"--output-path\", shlex.quote(str(output_path))]",
      "160:             return self.prepare_env(local_path).execute(\" \".join(predict_cmd))",
      "",
      "---------------"
    ],
    "mlflow/pyfunc/scoring_server/__init__.py||mlflow/pyfunc/scoring_server/__init__.py": [
      "File: mlflow/pyfunc/scoring_server/__init__.py -> mlflow/pyfunc/scoring_server/__init__.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "17: import json",
      "18: import logging",
      "19: import os",
      "20: import sys",
      "21: import traceback",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "20: import shlex",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "31: from mlflow.types import Schema",
      "32: from mlflow.utils import reraise",
      "33: from mlflow.utils.file_utils import path_to_local_file_uri",
      "34: from mlflow.utils.proto_json_utils import (",
      "35:     NumpyEncoder,",
      "36:     dataframe_from_parsed_json,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "35: from mlflow.utils.os import is_windows",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "328: ) -> Tuple[str, Dict[str, str]]:",
      "329:     local_uri = path_to_local_file_uri(model_uri)",
      "330:     timeout = timeout or MLFLOW_SCORING_SERVER_REQUEST_TIMEOUT.get()",
      "331:     # NB: Absolute windows paths do not work with mlflow apis, use file uri to ensure",
      "332:     # platform compatibility.",
      "334:         args = [f\"--timeout={timeout}\"]",
      "335:         if port and host:",
      "337:         elif host:",
      "340:         if nworkers:",
      "341:             args.append(f\"-w {nworkers}\")",
      "",
      "[Removed Lines]",
      "333:     if os.name != \"nt\":",
      "336:             args.append(f\"-b {host}:{port}\")",
      "338:             args.append(f\"-b {host}\")",
      "",
      "[Added Lines]",
      "336:     if not is_windows():",
      "339:             address = shlex.quote(f\"{host}:{port}\")",
      "340:             args.append(f\"-b {address}\")",
      "342:             args.append(f\"-b {shlex.quote(host)}\")",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "347:     else:",
      "348:         args = []",
      "349:         if host:",
      "352:         if port:",
      "353:             args.append(f\"--port={port}\")",
      "",
      "[Removed Lines]",
      "350:             args.append(f\"--host={host}\")",
      "",
      "[Added Lines]",
      "354:             args.append(f\"--host={shlex.quote(host)}\")",
      "",
      "---------------"
    ],
    "tests/models/test_cli.py||tests/models/test_cli.py": [
      "File: tests/models/test_cli.py -> tests/models/test_cli.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "28: from mlflow.environment_variables import MLFLOW_DISABLE_ENV_MANAGER_CONDA_WARNING",
      "29: from mlflow.exceptions import MlflowException",
      "30: from mlflow.protos.databricks_pb2 import ErrorCode, BAD_REQUEST",
      "31: from mlflow.pyfunc.scoring_server import (",
      "32:     CONTENT_TYPE_JSON,",
      "33:     CONTENT_TYPE_CSV,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "31: from mlflow.pyfunc.backend import PyFuncBackend",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "36: from mlflow.utils.environment import _mlflow_conda_env",
      "37: from mlflow.utils import env_manager as _EnvManager",
      "38: from mlflow.utils import PYTHON_VERSION",
      "39: from tests.helper_functions import (",
      "40:     pyfunc_build_image,",
      "41:     pyfunc_serve_from_docker_image,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "40: from mlflow.utils.process import ShellCommandException",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "174:         with mlflow.start_run() as active_run:",
      "175:             mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"impredicting\")",
      "176:             model_uri = f\"runs:/{active_run.info.run_id}/model\"",
      "178:         input_json_path = tmp.path(\"input.json\")",
      "179:         input_csv_path = tmp.path(\"input.csv\")",
      "180:         output_json_path = tmp.path(\"output.json\")",
      "",
      "[Removed Lines]",
      "177:         model_registry_uri = \"models:/{name}/{stage}\".format(name=\"impredicting\", stage=\"None\")",
      "",
      "[Added Lines]",
      "179:         model_registry_uri = \"models:/impredicting/None\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "331:         assert all(expected == actual)",
      "334: def test_prepare_env_passes(sk_model):",
      "335:     if no_conda:",
      "336:         pytest.skip(\"This test requires conda.\")",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "336: def test_predict_check_content_type(iris_data, sk_model, tmp_path):",
      "337:     with mlflow.start_run():",
      "338:         mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"impredicting\")",
      "339:     model_registry_uri = \"models:/impredicting/None\"",
      "340:     input_json_path = tmp_path / \"input.json\"",
      "341:     input_csv_path = tmp_path / \"input.csv\"",
      "342:     output_json_path = tmp_path / \"output.json\"",
      "344:     x, _ = iris_data",
      "345:     with input_json_path.open(\"w\") as f:",
      "346:         json.dump({\"dataframe_split\": pd.DataFrame(x).to_dict(orient=\"split\")}, f)",
      "348:     pd.DataFrame(x).to_csv(input_csv_path, index=False)",
      "350:     # Throw errors for invalid content_type",
      "351:     prc = subprocess.run(",
      "352:         [",
      "353:             \"mlflow\",",
      "354:             \"models\",",
      "355:             \"predict\",",
      "356:             \"-m\",",
      "357:             model_registry_uri,",
      "358:             \"-i\",",
      "359:             input_json_path,",
      "360:             \"-o\",",
      "361:             output_json_path,",
      "362:             \"-t\",",
      "363:             \"invalid\",",
      "364:             \"--env-manager\",",
      "365:             \"local\",",
      "366:         ],",
      "367:         stdout=subprocess.PIPE,",
      "368:         stderr=subprocess.PIPE,",
      "369:         env=env_with_tracking_uri(),",
      "370:         check=False,",
      "371:     )",
      "372:     assert prc.returncode != 0",
      "373:     assert \"Unknown content type\" in prc.stderr.decode(\"utf-8\")",
      "376: def test_predict_check_input_path(iris_data, sk_model, tmp_path):",
      "377:     with mlflow.start_run():",
      "378:         mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"impredicting\")",
      "379:     model_registry_uri = \"models:/impredicting/None\"",
      "380:     input_json_path = tmp_path / \"input with space.json\"",
      "381:     input_csv_path = tmp_path / \"input.csv\"",
      "382:     output_json_path = tmp_path / \"output.json\"",
      "384:     x, _ = iris_data",
      "385:     with input_json_path.open(\"w\") as f:",
      "386:         json.dump({\"dataframe_split\": pd.DataFrame(x).to_dict(orient=\"split\")}, f)",
      "388:     pd.DataFrame(x).to_csv(input_csv_path, index=False)",
      "390:     # Valid input path with space",
      "391:     prc = subprocess.run(",
      "392:         [",
      "393:             \"mlflow\",",
      "394:             \"models\",",
      "395:             \"predict\",",
      "396:             \"-m\",",
      "397:             model_registry_uri,",
      "398:             \"-i\",",
      "399:             f\"{input_json_path}\",",
      "400:             \"-o\",",
      "401:             output_json_path,",
      "402:             \"--env-manager\",",
      "403:             \"local\",",
      "404:         ],",
      "405:         stdout=subprocess.PIPE,",
      "406:         stderr=subprocess.PIPE,",
      "407:         env=env_with_tracking_uri(),",
      "408:         check=False,",
      "409:         text=True,",
      "410:     )",
      "411:     assert prc.returncode == 0",
      "413:     # Throw errors for invalid input_path",
      "414:     prc = subprocess.run(",
      "415:         [",
      "416:             \"mlflow\",",
      "417:             \"models\",",
      "418:             \"predict\",",
      "419:             \"-m\",",
      "420:             model_registry_uri,",
      "421:             \"-i\",",
      "422:             f'{input_json_path}\"; echo ThisIsABug! \"',",
      "423:             \"-o\",",
      "424:             output_json_path,",
      "425:             \"--env-manager\",",
      "426:             \"local\",",
      "427:         ],",
      "428:         stdout=subprocess.PIPE,",
      "429:         stderr=subprocess.PIPE,",
      "430:         env=env_with_tracking_uri(),",
      "431:         check=False,",
      "432:         text=True,",
      "433:     )",
      "434:     assert prc.returncode != 0",
      "435:     assert \"ThisIsABug!\" not in prc.stdout",
      "436:     assert \"FileNotFoundError\" in prc.stderr",
      "438:     prc = subprocess.run(",
      "439:         [",
      "440:             \"mlflow\",",
      "441:             \"models\",",
      "442:             \"predict\",",
      "443:             \"-m\",",
      "444:             model_registry_uri,",
      "445:             \"-i\",",
      "446:             f'{input_csv_path}\"; echo ThisIsABug! \"',",
      "447:             \"-o\",",
      "448:             output_json_path,",
      "449:             \"-t\",",
      "450:             \"csv\",",
      "451:             \"--env-manager\",",
      "452:             \"local\",",
      "453:         ],",
      "454:         stdout=subprocess.PIPE,",
      "455:         stderr=subprocess.PIPE,",
      "456:         env=env_with_tracking_uri(),",
      "457:         check=False,",
      "458:         text=True,",
      "459:     )",
      "460:     assert prc.returncode != 0",
      "461:     assert \"ThisIsABug!\" not in prc.stdout",
      "462:     assert \"FileNotFoundError\" in prc.stderr",
      "465: def test_predict_check_output_path(iris_data, sk_model, tmp_path):",
      "466:     with mlflow.start_run():",
      "467:         mlflow.sklearn.log_model(sk_model, \"model\", registered_model_name=\"impredicting\")",
      "468:     model_registry_uri = \"models:/impredicting/None\"",
      "469:     input_json_path = tmp_path / \"input.json\"",
      "470:     input_csv_path = tmp_path / \"input.csv\"",
      "471:     output_json_path = tmp_path / \"output.json\"",
      "473:     x, _ = iris_data",
      "474:     with input_json_path.open(\"w\") as f:",
      "475:         json.dump({\"dataframe_split\": pd.DataFrame(x).to_dict(orient=\"split\")}, f)",
      "477:     pd.DataFrame(x).to_csv(input_csv_path, index=False)",
      "479:     prc = subprocess.run(",
      "480:         [",
      "481:             \"mlflow\",",
      "482:             \"models\",",
      "483:             \"predict\",",
      "484:             \"-m\",",
      "485:             model_registry_uri,",
      "486:             \"-i\",",
      "487:             input_json_path,",
      "488:             \"-o\",",
      "489:             f'{output_json_path}\"; echo ThisIsABug! \"',",
      "490:             \"--env-manager\",",
      "491:             \"local\",",
      "492:         ],",
      "493:         stdout=subprocess.PIPE,",
      "494:         stderr=subprocess.PIPE,",
      "495:         env=env_with_tracking_uri(),",
      "496:         check=False,",
      "497:         text=True,",
      "498:     )",
      "499:     assert prc.returncode == 0",
      "500:     assert \"ThisIsABug!\" not in prc.stdout",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "574:         )",
      "577: def test_change_conda_env_root_location(tmp_path, sk_model):",
      "578:     env_root1_path = tmp_path / \"root1\"",
      "579:     env_root1_path.mkdir()",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "746: def test_host_invalid_value():",
      "747:     class MyModel(mlflow.pyfunc.PythonModel):",
      "748:         def predict(self, ctx, model_input):",
      "749:             return model_input",
      "751:     with mlflow.start_run():",
      "752:         model_info = mlflow.pyfunc.log_model(",
      "753:             python_model=MyModel(), artifact_path=\"test_model\", registered_model_name=\"model\"",
      "754:         )",
      "756:     with mock.patch(\"mlflow.models.cli.get_flavor_backend\", return_value=PyFuncBackend({})):",
      "757:         with pytest.raises(ShellCommandException, match=r\"Non-zero exit code: 1\"):",
      "758:             CliRunner().invoke(",
      "759:                 models_cli.serve,",
      "760:                 [\"--model-uri\", model_info.model_uri, \"--host\", \"localhost & echo BUG\"],",
      "761:                 catch_exceptions=False,",
      "762:             )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "4374f624f92f8160471dbf8d388d117b186dee00",
      "candidate_info": {
        "commit_hash": "4374f624f92f8160471dbf8d388d117b186dee00",
        "repo": "mlflow/mlflow",
        "commit_url": "https://github.com/mlflow/mlflow/commit/4374f624f92f8160471dbf8d388d117b186dee00",
        "files": [
          "mlflow/models/cli.py",
          "mlflow/pyfunc/backend.py",
          "mlflow/utils/cli_args.py",
          "tests/models/test_cli.py",
          "tests/pyfunc/docker/test_docker.py",
          "tests/pyfunc/docker/test_docker_flavors.py",
          "tests/pyfunc/test_scoring_server.py",
          "tests/resources/dockerfile/Dockerfile_conda",
          "tests/resources/dockerfile/Dockerfile_install_mlflow_virtualenv"
        ],
        "message": "Fix generate_dockerfile env_manager (#11690)\n\nSigned-off-by: Serena Ruan <serena.rxy@gmail.com>",
        "before_after_code_files": [
          "mlflow/models/cli.py||mlflow/models/cli.py",
          "mlflow/pyfunc/backend.py||mlflow/pyfunc/backend.py",
          "mlflow/utils/cli_args.py||mlflow/utils/cli_args.py",
          "tests/models/test_cli.py||tests/models/test_cli.py",
          "tests/pyfunc/docker/test_docker.py||tests/pyfunc/docker/test_docker.py",
          "tests/pyfunc/docker/test_docker_flavors.py||tests/pyfunc/docker/test_docker_flavors.py",
          "tests/pyfunc/test_scoring_server.py||tests/pyfunc/test_scoring_server.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "mlflow/pyfunc/backend.py||mlflow/pyfunc/backend.py",
            "tests/models/test_cli.py||tests/models/test_cli.py"
          ],
          "candidate": [
            "mlflow/pyfunc/backend.py||mlflow/pyfunc/backend.py",
            "tests/models/test_cli.py||tests/models/test_cli.py"
          ]
        }
      },
      "candidate_diff": {
        "mlflow/models/cli.py||mlflow/models/cli.py": [
          "File: mlflow/models/cli.py -> mlflow/models/cli.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "170:     default=\"mlflow-dockerfile\",",
          "171:     help=\"Output directory where the generated Dockerfile is stored.\",",
          "172: )",
          "174: @cli_args.MLFLOW_HOME",
          "175: @cli_args.INSTALL_JAVA",
          "176: @cli_args.INSTALL_MLFLOW",
          "",
          "[Removed Lines]",
          "173: @cli_args.ENV_MANAGER",
          "",
          "[Added Lines]",
          "173: @cli_args.ENV_MANAGER_DOCKERFILE",
          "",
          "---------------"
        ],
        "mlflow/pyfunc/backend.py||mlflow/pyfunc/backend.py": [
          "File: mlflow/pyfunc/backend.py -> mlflow/pyfunc/backend.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "58: # Directory to store loaded model inside the Docker context directory",
          "59: _MODEL_DIR_NAME = \"model_dir\"",
          "62: class PyFuncBackend(FlavorBackend):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "60: LOCAL_ENV_MANAGER_ERROR_MESSAGE = \"We cannot use 'LOCAL' environment manager \"",
          "61: \"for your model configuration. Please specify a virtualenv or conda environment \"",
          "62: \"manager instead with `--env-manager` argument.\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "67:     def __init__(",
          "68:         self,",
          "69:         config,",
          "70:         workers=1,",
          "72:         install_mlflow=False,",
          "73:         create_env_root_dir=False,",
          "74:         env_root_dir=None,",
          "",
          "[Removed Lines]",
          "71:         env_manager=_EnvManager.VIRTUALENV,",
          "",
          "[Added Lines]",
          "73:         env_manager,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "76:     ):",
          "77:         \"\"\"",
          "78:         Args:",
          "79:             env_root_dir: Root path for conda env. If None, use Conda's default environments",
          "80:                 directory. Note if this is set, conda package cache path becomes",
          "81:                 \"{env_root_dir}/conda_cache_pkgs\" instead of the global package cache",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "82:             env_manager: Environment manager to use for preparing the environment. If None,",
          "83:                 MLflow will automatically pick the env manager based on the model's flavor",
          "84:                 configuration for generate_dockerfile. It can't be None for other methods.",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "86:         super().__init__(config=config, **kwargs)",
          "87:         self._nworkers = workers or 1",
          "88:         if env_manager == _EnvManager.CONDA and ENV not in config:",
          "89:             env_manager = _EnvManager.LOCAL",
          "90:         self._env_manager = env_manager",
          "91:         self._install_mlflow = install_mlflow",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "95:             warnings.warn(",
          "96:                 \"Conda environment is not specified in config `env`. Using local environment.\"",
          "97:             )",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "376:             model_path = _download_artifact_from_uri(model_uri, output_path=model_cwd)",
          "377:             base_image = self._get_base_image(model_path, install_java)",
          "384:             model_install_steps = self._model_installation_steps(",
          "385:                 model_path, env_manager, install_mlflow, enable_mlserver",
          "386:             )",
          "387:             entrypoint = f\"from mlflow.models import container as C; C._serve('{env_manager}')\"",
          "389:         else:",
          "390:             base_image = UBUNTU_BASE_IMAGE",
          "391:             model_install_steps = \"\"",
          "392:             # If model_uri is not specified, dependencies are installed at runtime",
          "393:             entrypoint = (",
          "398:             )",
          "400:         dockerfile_text = docker_utils.generate_dockerfile(",
          "",
          "[Removed Lines]",
          "379:             # We don't need virtualenv or conda if base image is python",
          "380:             env_manager = (",
          "381:                 _EnvManager.LOCAL if base_image.startswith(\"python\") else self._env_manager",
          "382:             )",
          "394:                 self._get_install_pyfunc_deps_cmd(",
          "395:                     self._env_manager, install_mlflow, enable_mlserver",
          "396:                 )",
          "397:                 + f\" C._serve('{self._env_manager}')\"",
          "",
          "[Added Lines]",
          "388:             if base_image.startswith(\"python\"):",
          "389:                 # we can directly use local env for python image",
          "390:                 env_manager = _EnvManager.LOCAL if self._env_manager is None else self._env_manager",
          "391:                 if env_manager in [_EnvManager.CONDA, _EnvManager.VIRTUALENV]:",
          "392:                     # we can directly use ubuntu image for conda and virtualenv",
          "393:                     base_image = UBUNTU_BASE_IMAGE",
          "394:             elif base_image == UBUNTU_BASE_IMAGE:",
          "395:                 env_manager = (",
          "396:                     _EnvManager.VIRTUALENV if self._env_manager is None else self._env_manager",
          "397:                 )",
          "398:                 # installing python on ubuntu image is problematic and not recommended officially",
          "399:                 # so we recommend using conda or virtualenv instead on ubuntu image",
          "400:                 if env_manager == _EnvManager.LOCAL:",
          "401:                     raise MlflowException.invalid_parameter_value(LOCAL_ENV_MANAGER_ERROR_MESSAGE)",
          "402:             # shouldn't reach here but add this so we can validate base_image value above",
          "403:             else:",
          "404:                 raise MlflowException(f\"Unexpected base image value '{base_image}'\")",
          "411:         # if no model_uri specified, user must use virtualenv or conda env based on ubuntu image",
          "414:             env_manager = self._env_manager or _EnvManager.VIRTUALENV",
          "415:             if env_manager == _EnvManager.LOCAL:",
          "416:                 raise MlflowException.invalid_parameter_value(LOCAL_ENV_MANAGER_ERROR_MESSAGE)",
          "421:                 self._get_install_pyfunc_deps_cmd(env_manager, install_mlflow, enable_mlserver)",
          "422:                 + f\" C._serve('{env_manager}')\"",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "402:             base_image=base_image,",
          "403:             model_install_steps=model_install_steps,",
          "404:             entrypoint=entrypoint,",
          "406:             mlflow_home=mlflow_home,",
          "407:             enable_mlserver=enable_mlserver,",
          "408:             # always disable env creation at runtime for pyfunc",
          "",
          "[Removed Lines]",
          "405:             env_manager=self._env_manager,",
          "",
          "[Added Lines]",
          "430:             env_manager=env_manager,",
          "",
          "---------------"
        ],
        "mlflow/utils/cli_args.py||mlflow/utils/cli_args.py": [
          "File: mlflow/utils/cli_args.py -> mlflow/utils/cli_args.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "123: \"\"\",",
          "124: )",
          "127: INSTALL_MLFLOW = click.option(",
          "128:     \"--install-mlflow\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "126: ENV_MANAGER_DOCKERFILE = _create_env_manager_option(",
          "127:     default=None,",
          "128:     # '\\b' prevents rewrapping text:",
          "129:     # https://click.palletsprojects.com/en/8.1.x/documentation/#preventing-rewrapping",
          "130:     help_string=\"\"\"",
          "131: If specified, create an environment for MLmodel using the specified",
          "132: environment manager. The following values are supported:",
          "134: \\b",
          "135: - local: use the local environment",
          "136: - virtualenv: use virtualenv (and pyenv for Python version management)",
          "137: - conda: use conda",
          "139: If unspecified, default to None, then MLflow will automatically pick the env manager",
          "140: based on the model's flavor configuration.",
          "141: If model-uri is specified: if python version is specified in the flavor configuration",
          "142: and no java installation is required, then we use local environment. Otherwise we use virtualenv.",
          "143: If no model-uri is provided, we use virtualenv.",
          "144: \"\"\",",
          "145: )",
          "",
          "---------------"
        ],
        "tests/models/test_cli.py||tests/models/test_cli.py": [
          "File: tests/models/test_cli.py -> tests/models/test_cli.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "759:             python_model=MyModel(), artifact_path=\"test_model\", registered_model_name=\"model\"",
          "760:         )",
          "763:         with pytest.raises(ShellCommandException, match=r\"Non-zero exit code: 1\"):",
          "764:             CliRunner().invoke(",
          "765:                 models_cli.serve,",
          "",
          "[Removed Lines]",
          "762:     with mock.patch(\"mlflow.models.cli.get_flavor_backend\", return_value=PyFuncBackend({})):",
          "",
          "[Added Lines]",
          "762:     with mock.patch(",
          "763:         \"mlflow.models.cli.get_flavor_backend\",",
          "764:         return_value=PyFuncBackend({}, env_manager=_EnvManager.VIRTUALENV),",
          "765:     ):",
          "",
          "---------------"
        ],
        "tests/pyfunc/docker/test_docker.py||tests/pyfunc/docker/test_docker.py": [
          "File: tests/pyfunc/docker/test_docker.py -> tests/pyfunc/docker/test_docker.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: from mlflow.models.docker_utils import build_image_from_context",
          "16: from mlflow.models.flavor_backend_registry import get_flavor_backend",
          "17: from mlflow.utils import PYTHON_VERSION",
          "19: from mlflow.version import VERSION",
          "21: from tests.pyfunc.docker.conftest import RESOURCE_DIR, get_released_mlflow_version",
          "",
          "[Removed Lines]",
          "18: from mlflow.utils.env_manager import VIRTUALENV",
          "",
          "[Added Lines]",
          "18: from mlflow.utils.env_manager import CONDA, LOCAL, VIRTUALENV",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "62: @dataclass",
          "63: class Param:",
          "64:     expected_dockerfile: str",
          "66:     mlflow_home: Optional[str] = None",
          "67:     install_mlflow: bool = False",
          "68:     enable_mlserver: bool = False",
          "",
          "[Removed Lines]",
          "65:     env_manager: str = VIRTUALENV",
          "",
          "[Added Lines]",
          "65:     env_manager: Optional[str] = None",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "74:     \"params\",",
          "75:     [",
          "76:         Param(expected_dockerfile=\"Dockerfile_default\"),",
          "77:         Param(install_mlflow=True, expected_dockerfile=\"Dockerfile_install_mlflow\"),",
          "78:         Param(enable_mlserver=True, expected_dockerfile=\"Dockerfile_enable_mlserver\"),",
          "79:         Param(mlflow_home=\".\", expected_dockerfile=\"Dockerfile_with_mlflow_home\"),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "77:         Param(expected_dockerfile=\"Dockerfile_default\", env_manager=LOCAL),",
          "78:         Param(expected_dockerfile=\"Dockerfile_java_flavor\", env_manager=VIRTUALENV),",
          "79:         Param(expected_dockerfile=\"Dockerfile_conda\", env_manager=CONDA),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "118:     model_path = save_model(tmp_path)",
          "119:     add_spark_flavor_to_model(model_path)",
          "123:     backend.generate_dockerfile(",
          "124:         model_uri=model_path,",
          "",
          "[Removed Lines]",
          "121:     backend = get_flavor_backend(model_path, docker_build=True)",
          "",
          "[Added Lines]",
          "124:     backend = get_flavor_backend(model_path, docker_build=True, env_manager=None)",
          "",
          "---------------"
        ],
        "tests/pyfunc/docker/test_docker_flavors.py||tests/pyfunc/docker/test_docker_flavors.py": [
          "File: tests/pyfunc/docker/test_docker_flavors.py -> tests/pyfunc/docker/test_docker_flavors.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "114:     flavor = flavor.split(\"_\")[0]  # Remove _pt or _tf from the flavor name",
          "116:     # Build an image",
          "118:     backend.build_image(",
          "119:         model_uri=model_path,",
          "120:         image_name=TEST_IMAGE_NAME,",
          "",
          "[Removed Lines]",
          "117:     backend = get_flavor_backend(model_uri=model_path, docker_build=True)",
          "",
          "[Added Lines]",
          "117:     backend = get_flavor_backend(model_uri=model_path, docker_build=True, env_manager=None)",
          "",
          "---------------"
        ],
        "tests/pyfunc/test_scoring_server.py||tests/pyfunc/test_scoring_server.py": [
          "File: tests/pyfunc/test_scoring_server.py -> tests/pyfunc/test_scoring_server.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "761:     server_proc = None",
          "762:     try:",
          "763:         server_proc = get_flavor_backend(",
          "765:         ).serve(",
          "766:             model_uri=model_path,",
          "767:             port=port,",
          "",
          "[Removed Lines]",
          "764:             model_path, eng_manager=_EnvManager.CONDA, workers=1, install_mlflow=False",
          "",
          "[Added Lines]",
          "764:             model_path, env_manager=_EnvManager.CONDA, workers=1, install_mlflow=False",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "3a1d12fa77be8b7632bb526fa4299fafb83a4a6d",
      "candidate_info": {
        "commit_hash": "3a1d12fa77be8b7632bb526fa4299fafb83a4a6d",
        "repo": "mlflow/mlflow",
        "commit_url": "https://github.com/mlflow/mlflow/commit/3a1d12fa77be8b7632bb526fa4299fafb83a4a6d",
        "files": [
          "mlflow/__init__.py",
          "mlflow/models/flavor_backend_registry.py",
          "mlflow/models/model.py",
          "mlflow/pyfunc/backend.py",
          "mlflow/rfunc/__init__.py",
          "mlflow/store/artifact/dbfs_artifact_repo.py",
          "mlflow/utils/model_utils.py",
          "mlflow/utils/rest_utils.py",
          "tests/gateway/__init__.py",
          "tests/gateway/providers/__init__.py"
        ],
        "message": "Fix test issues, Refactor and simplify backend logic (#11845)\n\nSigned-off-by: Mahmoud Lababidi <lababidi@lababidi.xyz>\nSigned-off-by: Mahmoud Lababidi <lababidi@gmail.com>\nCo-authored-by: WeichenXu <weichen.xu@databricks.com>",
        "before_after_code_files": [
          "mlflow/__init__.py||mlflow/__init__.py",
          "mlflow/models/flavor_backend_registry.py||mlflow/models/flavor_backend_registry.py",
          "mlflow/models/model.py||mlflow/models/model.py",
          "mlflow/pyfunc/backend.py||mlflow/pyfunc/backend.py",
          "mlflow/rfunc/__init__.py||mlflow/rfunc/__init__.py",
          "mlflow/store/artifact/dbfs_artifact_repo.py||mlflow/store/artifact/dbfs_artifact_repo.py",
          "mlflow/utils/model_utils.py||mlflow/utils/model_utils.py",
          "mlflow/utils/rest_utils.py||mlflow/utils/rest_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "mlflow/pyfunc/backend.py||mlflow/pyfunc/backend.py"
          ],
          "candidate": [
            "mlflow/pyfunc/backend.py||mlflow/pyfunc/backend.py"
          ]
        }
      },
      "candidate_diff": {
        "mlflow/__init__.py||mlflow/__init__.py": [
          "File: mlflow/__init__.py -> mlflow/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "68: pyfunc = LazyLoader(\"mlflow.pyfunc\", globals(), \"mlflow.pyfunc\")",
          "69: pyspark = LazyLoader(\"mlflow.pyspark\", globals(), \"mlflow.pyspark\")",
          "70: pytorch = LazyLoader(\"mlflow.pytorch\", globals(), \"mlflow.pytorch\")",
          "71: recipes = LazyLoader(\"mlflow.recipes\", globals(), \"mlflow.recipes\")",
          "72: sentence_transformers = LazyLoader(",
          "73:     \"mlflow.sentence_transformers\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "71: rfunc = LazyLoader(\"mlflow.rfunc\", globals(), \"mlflow.rfunc\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "194:     \"log_table\",",
          "195:     \"log_text\",",
          "196:     \"login\",",
          "197:     \"register_model\",",
          "198:     \"run\",",
          "199:     \"search_experiments\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "198:     \"pyfunc\",",
          "",
          "---------------"
        ],
        "mlflow/models/flavor_backend_registry.py||mlflow/models/flavor_backend_registry.py": [
          "File: mlflow/models/flavor_backend_registry.py -> mlflow/models/flavor_backend_registry.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: def _get_flavor_backend_for_local_model(model=None, build_docker=True, **kwargs):",
          "22:     from mlflow.pyfunc.backend import PyFuncBackend",
          "23:     from mlflow.rfunc.backend import RFuncBackend",
          "25:     if not model:",
          "26:         return pyfunc.FLAVOR_NAME, PyFuncBackend({}, **kwargs)",
          "34:     return None, None",
          "",
          "[Removed Lines]",
          "21:     from mlflow import pyfunc",
          "28:     _flavor_backends = {pyfunc.FLAVOR_NAME: PyFuncBackend, \"crate\": RFuncBackend}",
          "29:     for flavor_name, flavor_config in model.flavors.items():",
          "30:         if flavor_name in _flavor_backends:",
          "31:             backend = _flavor_backends[flavor_name](flavor_config, **kwargs)",
          "32:             if build_docker and backend.can_build_image() or backend.can_score_model():",
          "33:                 return flavor_name, backend",
          "",
          "[Added Lines]",
          "21:     from mlflow import pyfunc, rfunc",
          "28:     backends = {pyfunc.FLAVOR_NAME: PyFuncBackend, rfunc.FLAVOR_NAME: RFuncBackend}",
          "29:     for flavor, Backend in backends.items():",
          "30:         if flavor in model.flavors:",
          "31:             backend = Backend(model.flavors[flavor], **kwargs)",
          "32:             if (build_docker and backend.can_build_image()) or backend.can_score_model():",
          "33:                 return flavor, backend",
          "",
          "---------------"
        ],
        "mlflow/models/model.py||mlflow/models/model.py": [
          "File: mlflow/models/model.py -> mlflow/models/model.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "8: from pathlib import Path",
          "9: from pprint import pformat",
          "10: from typing import Any, Callable, Dict, List, Literal, NamedTuple, Optional, Union",
          "12: import yaml",
          "14: import mlflow",
          "15: from mlflow.artifacts import download_artifacts",
          "16: from mlflow.exceptions import MlflowException",
          "18: from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository",
          "19: from mlflow.store.artifact.runs_artifact_repo import RunsArtifactRepository",
          "20: from mlflow.tracking._model_registry import DEFAULT_AWAIT_MAX_SLEEP_SECONDS",
          "",
          "[Removed Lines]",
          "17: from mlflow.protos.databricks_pb2 import INVALID_PARAMETER_VALUE",
          "",
          "[Added Lines]",
          "11: from urllib.parse import urlparse",
          "18: from mlflow.protos.databricks_pb2 import INVALID_PARAMETER_VALUE, RESOURCE_DOES_NOT_EXIST",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "551:             # Load the Model object from a remote model directory",
          "552:             model2 = Model.load(\"s3://mybucket/path/to/my/model\")",
          "553:         \"\"\"",
          "554:         path = download_artifacts(artifact_uri=path)",
          "555:         if os.path.isdir(path):",
          "556:             path = os.path.join(path, MLMODEL_FILE_NAME)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "555:         # Check if the path is a local directory and not remote",
          "556:         path_scheme = urlparse(str(path)).scheme",
          "557:         if (not path_scheme or path_scheme == \"file\") and not os.path.exists(path):",
          "558:             raise MlflowException(",
          "559:                 f'Could not find an \"{MLMODEL_FILE_NAME}\" configuration file at \"{path}\"',",
          "560:                 RESOURCE_DOES_NOT_EXIST,",
          "561:             )",
          "",
          "---------------"
        ],
        "mlflow/pyfunc/backend.py||mlflow/pyfunc/backend.py": [
          "File: mlflow/pyfunc/backend.py -> mlflow/pyfunc/backend.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "13: from mlflow import pyfunc",
          "14: from mlflow.exceptions import MlflowException",
          "16: from mlflow.models.docker_utils import PYTHON_SLIM_BASE_IMAGE, UBUNTU_BASE_IMAGE",
          "18: from mlflow.pyfunc import (",
          "19:     ENV,",
          "20:     _extract_conda_env,",
          "",
          "[Removed Lines]",
          "15: from mlflow.models import FlavorBackend, docker_utils",
          "17: from mlflow.models.model import MLMODEL_FILE_NAME, Model",
          "",
          "[Added Lines]",
          "15: from mlflow.models import FlavorBackend, Model, docker_utils",
          "17: from mlflow.models.model import MLMODEL_FILE_NAME",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "23:     scoring_server,",
          "24: )",
          "25: from mlflow.tracking.artifact_utils import _download_artifact_from_uri",
          "27: from mlflow.utils.conda import get_conda_bin_executable, get_or_create_conda_env",
          "28: from mlflow.utils.environment import Environment, _PythonEnv",
          "29: from mlflow.utils.file_utils import (",
          "",
          "[Removed Lines]",
          "26: from mlflow.utils import env_manager as _EnvManager",
          "",
          "[Added Lines]",
          "26: from mlflow.utils import env_manager as em",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "91:         \"\"\"",
          "92:         super().__init__(config=config, **kwargs)",
          "93:         self._nworkers = workers or 1",
          "95:             warnings.warn(",
          "96:                 \"Conda environment is not specified in config `env`. Using local environment.\"",
          "97:             )",
          "99:         self._env_manager = env_manager",
          "100:         self._install_mlflow = install_mlflow",
          "101:         self._env_id = os.environ.get(\"MLFLOW_HOME\", VERSION) if install_mlflow else None",
          "",
          "[Removed Lines]",
          "94:         if env_manager == _EnvManager.CONDA and ENV not in config:",
          "98:             env_manager = _EnvManager.LOCAL",
          "",
          "[Added Lines]",
          "94:         if env_manager == em.CONDA and ENV not in config:",
          "98:             env_manager = em.LOCAL",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "114:             else:",
          "115:                 root_tmp_dir = get_or_create_tmp_dir()",
          "121:         local_path = _download_artifact_from_uri(model_uri)",
          "122:         if self._create_env_root_dir:",
          "",
          "[Removed Lines]",
          "117:             env_root_dir = os.path.join(root_tmp_dir, \"envs\")",
          "118:             os.makedirs(env_root_dir, exist_ok=True)",
          "119:             return env_root_dir",
          "",
          "[Added Lines]",
          "117:             envs_root_dir = os.path.join(root_tmp_dir, \"envs\")",
          "118:             os.makedirs(envs_root_dir, exist_ok=True)",
          "119:             return envs_root_dir",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "127:         else:",
          "128:             env_root_dir = self._env_root_dir",
          "131:             activate_cmd = _get_or_create_virtualenv(",
          "132:                 local_path,",
          "133:                 self._env_id,",
          "",
          "[Removed Lines]",
          "130:         if self._env_manager == _EnvManager.VIRTUALENV:",
          "",
          "[Added Lines]",
          "130:         if self._env_manager == em.VIRTUALENV:",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "136:                 pip_requirements_override=pip_requirements_override,",
          "137:             )",
          "138:             self._environment = Environment(activate_cmd)",
          "140:             conda_env_path = os.path.join(local_path, _extract_conda_env(self._config[ENV]))",
          "141:             self._environment = get_or_create_conda_env(",
          "142:                 conda_env_path,",
          "",
          "[Removed Lines]",
          "139:         elif self._env_manager == _EnvManager.CONDA:",
          "",
          "[Added Lines]",
          "139:         elif self._env_manager == em.CONDA:",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "146:                 pip_requirements_override=pip_requirements_override,",
          "147:             )",
          "150:             raise Exception(\"Prepare env should not be called with local env manager!\")",
          "151:         else:",
          "152:             raise Exception(f\"Unexpected env manager value '{self._env_manager}'\")",
          "",
          "[Removed Lines]",
          "149:         elif self._env_manager == _EnvManager.LOCAL:",
          "",
          "[Added Lines]",
          "149:         elif self._env_manager == em.LOCAL:",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "176:         # platform compatibility.",
          "177:         local_uri = path_to_local_file_uri(local_path)",
          "180:             predict_cmd = [",
          "181:                 \"python\",",
          "182:                 _mlflow_pyfunc_backend_predict.__file__,",
          "",
          "[Removed Lines]",
          "179:         if self._env_manager != _EnvManager.LOCAL:",
          "",
          "[Added Lines]",
          "179:         if self._env_manager != em.LOCAL:",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "190:             if output_path:",
          "191:                 predict_cmd += [\"--output-path\", shlex.quote(str(output_path))]",
          "194:                 # Conda use = instead of == for version pinning",
          "195:                 pip_requirements_override = [",
          "197:                 ]",
          "199:             environment = self.prepare_env(",
          "",
          "[Removed Lines]",
          "193:             if pip_requirements_override and self._env_manager == _EnvManager.CONDA:",
          "196:                     l.replace(\"==\", \"=\") for l in pip_requirements_override",
          "",
          "[Added Lines]",
          "193:             if pip_requirements_override and self._env_manager == em.CONDA:",
          "196:                     pip_req.replace(\"==\", \"=\") for pip_req in pip_requirements_override",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "281:             #  does not support prctl. We need to find an approach to address it.",
          "282:             command = \"exec \" + command",
          "285:             return self.prepare_env(local_path).execute(",
          "286:                 command,",
          "287:                 command_env,",
          "",
          "[Removed Lines]",
          "284:         if self._env_manager != _EnvManager.LOCAL:",
          "",
          "[Added Lines]",
          "284:         if self._env_manager != em.LOCAL:",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "330:         )",
          "332:     def can_score_model(self):",
          "334:             # noconda => already in python and dependencies are assumed to be installed.",
          "335:             return True",
          "336:         conda_path = get_conda_bin_executable(\"conda\")",
          "",
          "[Removed Lines]",
          "333:         if self._env_manager == _EnvManager.LOCAL:",
          "",
          "[Added Lines]",
          "333:         if self._env_manager == em.LOCAL:",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "388:             if base_image.startswith(\"python\"):",
          "389:                 # we can directly use local env for python image",
          "392:                     # we can directly use ubuntu image for conda and virtualenv",
          "393:                     base_image = UBUNTU_BASE_IMAGE",
          "394:             elif base_image == UBUNTU_BASE_IMAGE:",
          "398:                 # installing python on ubuntu image is problematic and not recommended officially",
          "401:                     raise MlflowException.invalid_parameter_value(LOCAL_ENV_MANAGER_ERROR_MESSAGE)",
          "402:             # shouldn't reach here but add this so we can validate base_image value above",
          "403:             else:",
          "",
          "[Removed Lines]",
          "390:                 env_manager = _EnvManager.LOCAL if self._env_manager is None else self._env_manager",
          "391:                 if env_manager in [_EnvManager.CONDA, _EnvManager.VIRTUALENV]:",
          "395:                 env_manager = (",
          "396:                     _EnvManager.VIRTUALENV if self._env_manager is None else self._env_manager",
          "397:                 )",
          "399:                 # so we recommend using conda or virtualenv instead on ubuntu image",
          "400:                 if env_manager == _EnvManager.LOCAL:",
          "",
          "[Added Lines]",
          "390:                 env_manager = self._env_manager or em.LOCAL",
          "391:                 if env_manager in [em.CONDA, em.VIRTUALENV]:",
          "395:                 env_manager = self._env_manager or em.VIRTUALENV",
          "397:                 # , so we recommend using conda or virtualenv instead on ubuntu image",
          "398:                 if env_manager == em.LOCAL:",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "411:         # if no model_uri specified, user must use virtualenv or conda env based on ubuntu image",
          "412:         else:",
          "413:             base_image = UBUNTU_BASE_IMAGE",
          "416:                 raise MlflowException.invalid_parameter_value(LOCAL_ENV_MANAGER_ERROR_MESSAGE)",
          "418:             model_install_steps = \"\"",
          "",
          "[Removed Lines]",
          "414:             env_manager = self._env_manager or _EnvManager.VIRTUALENV",
          "415:             if env_manager == _EnvManager.LOCAL:",
          "",
          "[Added Lines]",
          "412:             env_manager = self._env_manager or em.VIRTUALENV",
          "413:             if env_manager == em.LOCAL:",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "459:             return UBUNTU_BASE_IMAGE",
          "461:         # Get Python version from MLmodel",
          "462:         try:",
          "464:             model = Model.load(model_config_path)",
          "466:             conf = model.flavors[pyfunc.FLAVOR_NAME]",
          "467:             env_conf = conf[pyfunc.ENV]",
          "470:             python_env = _PythonEnv.from_yaml(python_env_config_path)",
          "471:             return PYTHON_SLIM_BASE_IMAGE.format(version=python_env.python)",
          "",
          "[Removed Lines]",
          "463:             model_config_path = os.path.join(model_path, MLMODEL_FILE_NAME)",
          "468:             python_env_config_path = os.path.join(model_path, env_conf[_EnvManager.VIRTUALENV])",
          "",
          "[Added Lines]",
          "460:         model_config_path = os.path.join(model_path, MLMODEL_FILE_NAME)",
          "466:             python_env_config_path = os.path.join(model_path, env_conf[em.VIRTUALENV])",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "497:         return steps",
          "499:     def _get_install_pyfunc_deps_cmd(",
          "501:     ):",
          "502:         return (",
          "503:             \"from mlflow.models import container as C; \"",
          "",
          "[Removed Lines]",
          "500:         self, env_manager: _EnvManager, install_mlflow: bool, enable_mlserver: bool",
          "",
          "[Added Lines]",
          "498:         self, env_manager: str, install_mlflow: bool, enable_mlserver: bool",
          "",
          "---------------"
        ],
        "mlflow/rfunc/__init__.py||mlflow/rfunc/__init__.py": [
          "File: mlflow/rfunc/__init__.py -> mlflow/rfunc/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "38:       model: r_model.bin",
          "40: \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42: FLAVOR_NAME = \"crate\"",
          "",
          "---------------"
        ],
        "mlflow/store/artifact/dbfs_artifact_repo.py||mlflow/store/artifact/dbfs_artifact_repo.py": [
          "File: mlflow/store/artifact/dbfs_artifact_repo.py -> mlflow/store/artifact/dbfs_artifact_repo.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "14: from mlflow.tracking._tracking_service import utils",
          "15: from mlflow.utils.databricks_utils import get_databricks_host_creds",
          "16: from mlflow.utils.file_utils import relative_path_to_artifact_path",
          "18: from mlflow.utils.string_utils import strip_prefix",
          "19: from mlflow.utils.uri import (",
          "20:     get_databricks_profile_uri_from_artifact_uri,",
          "",
          "[Removed Lines]",
          "17: from mlflow.utils.rest_utils import RESOURCE_DOES_NOT_EXIST, http_request, http_request_safe",
          "",
          "[Added Lines]",
          "17: from mlflow.utils.rest_utils import RESOURCE_NON_EXISTENT, http_request, http_request_safe",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "144:         # /api/2.0/dbfs/list will not have the 'files' key in the response for empty directories.",
          "145:         infos = []",
          "146:         artifact_prefix = strip_prefix(self.artifact_uri, \"dbfs:\")",
          "148:             return []",
          "149:         dbfs_files = json_response.get(\"files\", [])",
          "150:         for dbfs_file in dbfs_files:",
          "",
          "[Removed Lines]",
          "147:         if json_response.get(\"error_code\", None) == RESOURCE_DOES_NOT_EXIST:",
          "",
          "[Added Lines]",
          "147:         if json_response.get(\"error_code\", None) == RESOURCE_NON_EXISTENT:",
          "",
          "---------------"
        ],
        "mlflow/utils/model_utils.py||mlflow/utils/model_utils.py": [
          "File: mlflow/utils/model_utils.py -> mlflow/utils/model_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "33:         The dictionary contains all flavor configurations with flavor name as key.",
          "35:     \"\"\"",
          "47: def _get_flavor_configuration(model_path, flavor_name):",
          "",
          "[Removed Lines]",
          "36:     model_configuration_path = os.path.join(model_path, MLMODEL_FILE_NAME)",
          "37:     if not os.path.exists(model_configuration_path):",
          "38:         raise MlflowException(",
          "39:             f'Could not find an \"{MLMODEL_FILE_NAME}\" configuration file at \"{model_path}\"',",
          "40:             RESOURCE_DOES_NOT_EXIST,",
          "41:         )",
          "43:     model_conf = Model.load(model_configuration_path)",
          "44:     return model_conf.flavors",
          "",
          "[Added Lines]",
          "37:     return Model.load(model_path).flavors",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "58:         The flavor configuration as a dictionary.",
          "60:     \"\"\"",
          "63:         raise MlflowException(",
          "70: def _get_flavor_configuration_from_uri(model_uri, flavor_name, logger):",
          "",
          "[Removed Lines]",
          "61:     flavors = _get_all_flavor_configurations(model_path)",
          "62:     if flavor_name not in flavors:",
          "64:             f'Model does not have the \"{flavor_name}\" flavor',",
          "65:             RESOURCE_DOES_NOT_EXIST,",
          "66:         )",
          "67:     return flavors[flavor_name]",
          "",
          "[Added Lines]",
          "54:     try:",
          "55:         return Model.load(model_path).flavors[flavor_name]",
          "56:     except KeyError as ex:",
          "58:             f'Model does not have the \"{flavor_name}\" flavor', RESOURCE_DOES_NOT_EXIST",
          "59:         ) from ex",
          "",
          "---------------"
        ],
        "mlflow/utils/rest_utils.py||mlflow/utils/rest_utils.py": [
          "File: mlflow/utils/rest_utils.py -> mlflow/utils/rest_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: )",
          "31: from mlflow.utils.string_utils import strip_suffix",
          "34: _REST_API_PATH_PREFIX = \"/api/2.0\"",
          "",
          "[Removed Lines]",
          "33: RESOURCE_DOES_NOT_EXIST = \"RESOURCE_DOES_NOT_EXIST\"",
          "",
          "[Added Lines]",
          "33: RESOURCE_NON_EXISTENT = \"RESOURCE_DOES_NOT_EXIST\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5105be165ea5de128867c909fac3695b65943d5d",
      "candidate_info": {
        "commit_hash": "5105be165ea5de128867c909fac3695b65943d5d",
        "repo": "mlflow/mlflow",
        "commit_url": "https://github.com/mlflow/mlflow/commit/5105be165ea5de128867c909fac3695b65943d5d",
        "files": [
          "docs/source/deployment/index.rst",
          "mlflow/models/__init__.py",
          "mlflow/models/cli.py",
          "mlflow/models/python_api.py",
          "mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py",
          "mlflow/pyfunc/backend.py",
          "mlflow/rfunc/backend.py",
          "mlflow/utils/cli_args.py",
          "mlflow/utils/conda.py",
          "mlflow/utils/virtualenv.py",
          "tests/models/test_cli.py",
          "tests/models/test_python_api.py"
        ],
        "message": "Enhance `predict` API to serve for env validation purpose. (#10759)\n\nSigned-off-by: B-Step62 <yuki.watanabe@databricks.com>\nSigned-off-by: Yuki Watanabe <31463517+B-Step62@users.noreply.github.com>\nCo-authored-by: Ben Wilson <39283302+BenWilson2@users.noreply.github.com>",
        "before_after_code_files": [
          "mlflow/models/__init__.py||mlflow/models/__init__.py",
          "mlflow/models/cli.py||mlflow/models/cli.py",
          "mlflow/models/python_api.py||mlflow/models/python_api.py",
          "mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py||mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py",
          "mlflow/pyfunc/backend.py||mlflow/pyfunc/backend.py",
          "mlflow/rfunc/backend.py||mlflow/rfunc/backend.py",
          "mlflow/utils/cli_args.py||mlflow/utils/cli_args.py",
          "mlflow/utils/conda.py||mlflow/utils/conda.py",
          "mlflow/utils/virtualenv.py||mlflow/utils/virtualenv.py",
          "tests/models/test_cli.py||tests/models/test_cli.py",
          "tests/models/test_python_api.py||tests/models/test_python_api.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py||mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py",
            "mlflow/pyfunc/backend.py||mlflow/pyfunc/backend.py",
            "tests/models/test_cli.py||tests/models/test_cli.py"
          ],
          "candidate": [
            "mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py||mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py",
            "mlflow/pyfunc/backend.py||mlflow/pyfunc/backend.py",
            "tests/models/test_cli.py||tests/models/test_cli.py"
          ]
        }
      },
      "candidate_diff": {
        "mlflow/models/__init__.py||mlflow/models/__init__.py": [
          "File: mlflow/models/__init__.py -> mlflow/models/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "41:     make_metric,",
          "42: )",
          "43: from mlflow.models.flavor_backend import FlavorBackend",
          "45: from mlflow.models.model import Model, get_model_info",
          "47: from mlflow.utils.environment import infer_pip_requirements",
          "89: __all__ = [",
          "90:     \"Model\",",
          "91:     \"FlavorBackend\",",
          "",
          "[Removed Lines]",
          "44: from mlflow.models.flavor_backend_registry import get_flavor_backend",
          "46: from mlflow.utils import env_manager as _EnvManager",
          "50: def build_docker(",
          "51:     model_uri=None,",
          "52:     name=\"mlflow-pyfunc\",",
          "53:     env_manager=_EnvManager.VIRTUALENV,",
          "54:     mlflow_home=None,",
          "55:     install_mlflow=False,",
          "56:     enable_mlserver=False,",
          "57: ):",
          "58:     \"\"\"",
          "59:     Builds a Docker image whose default entrypoint serves an MLflow model at port 8080, using the",
          "60:     python_function flavor. The container serves the model referenced by ``model_uri``, if",
          "61:     specified. If ``model_uri`` is not specified, an MLflow Model directory must be mounted as a",
          "62:     volume into the /opt/ml/model directory in the container.",
          "64:     .. warning::",
          "66:         If ``model_uri`` is unspecified, the resulting image doesn't support serving models with",
          "67:         the RFunc or Java MLeap model servers.",
          "69:     NB: by default, the container will start nginx and gunicorn processes. If you don't need the",
          "70:     nginx process to be started (for instance if you deploy your container to Google Cloud Run),",
          "71:     you can disable it via the DISABLE_NGINX environment variable:",
          "73:     .. code:: bash",
          "75:         docker run -p 5001:8080 -e DISABLE_NGINX=true \"my-image-name\"",
          "77:     See https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html for more information on the",
          "78:     'python_function' flavor.",
          "79:     \"\"\"",
          "80:     get_flavor_backend(model_uri, docker_build=True, env_manager=env_manager).build_image(",
          "81:         model_uri,",
          "82:         name,",
          "83:         mlflow_home=mlflow_home,",
          "84:         install_mlflow=install_mlflow,",
          "85:         enable_mlserver=enable_mlserver,",
          "86:     )",
          "",
          "[Added Lines]",
          "44: from mlflow.models.python_api import build_docker",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "105: # Under skinny-mlflow requirements, the following packages cannot be imported",
          "106: # because of lack of numpy/pandas library, so wrap them with try...except block",
          "107: try:",
          "108:     from mlflow.models.signature import ModelSignature, infer_signature, set_signature",
          "109:     from mlflow.models.utils import ModelInputExample, add_libraries_to_model, validate_schema",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "66:     from mlflow.models.python_api import predict",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "115:         \"validate_schema\",",
          "116:         \"add_libraries_to_model\",",
          "117:         \"set_signature\",",
          "118:     ]",
          "119: except ImportError:",
          "120:     pass",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "77:         \"predict\",",
          "",
          "---------------"
        ],
        "mlflow/models/cli.py||mlflow/models/cli.py": [
          "File: mlflow/models/cli.py -> mlflow/models/cli.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "3: import click",
          "6: from mlflow.models.flavor_backend_registry import get_flavor_backend",
          "7: from mlflow.utils import cli_args",
          "8: from mlflow.utils import env_manager as _EnvManager",
          "",
          "[Removed Lines]",
          "5: from mlflow.models import build_docker as build_docker_api",
          "",
          "[Added Lines]",
          "5: from mlflow.models import python_api",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "97:         }'",
          "99:     \"\"\"",
          "102:     return get_flavor_backend(",
          "103:         model_uri, env_manager=env_manager, workers=workers, install_mlflow=install_mlflow",
          "",
          "[Removed Lines]",
          "100:     env_manager = _EnvManager.LOCAL if no_conda else env_manager or _EnvManager.VIRTUALENV",
          "",
          "[Added Lines]",
          "100:     env_manager = _EnvManager.LOCAL if no_conda else env_manager",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "125: )",
          "126: @cli_args.ENV_MANAGER",
          "127: @cli_args.INSTALL_MLFLOW",
          "136:     \"\"\"",
          "137:     Generate predictions in json format using a saved MLflow model. For information about the input",
          "138:     data formats accepted by this function, see the following documentation:",
          "139:     https://www.mlflow.org/docs/latest/models.html#built-in-deployment-tools.",
          "140:     \"\"\"",
          "152: @commands.command(\"prepare-env\")",
          "",
          "[Removed Lines]",
          "128: def predict(",
          "129:     model_uri,",
          "130:     input_path,",
          "131:     output_path,",
          "132:     content_type,",
          "133:     env_manager,",
          "134:     install_mlflow,",
          "135: ):",
          "141:     env_manager = env_manager or _EnvManager.VIRTUALENV",
          "142:     return get_flavor_backend(",
          "143:         model_uri, env_manager=env_manager, install_mlflow=install_mlflow",
          "144:     ).predict(",
          "145:         model_uri=model_uri,",
          "146:         input_path=input_path,",
          "147:         output_path=output_path,",
          "148:         content_type=content_type,",
          "149:     )",
          "",
          "[Added Lines]",
          "128: @click.option(",
          "129:     \"--pip-requirements-override\",",
          "130:     \"-r\",",
          "131:     default=None,",
          "132:     help=\"Specify packages and versions to override the dependencies defined \"",
          "133:     \"in the model. Must be a comma-separated string like x==y,z==a.\",",
          "134: )",
          "135: def predict(**kwargs):",
          "141:     return python_api.predict(**kwargs)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "163:     downloading dependencies or initializing a conda environment. After preparation,",
          "164:     calling predict or serve should be fast.",
          "165:     \"\"\"",
          "167:     return get_flavor_backend(",
          "168:         model_uri, env_manager=env_manager, install_mlflow=install_mlflow",
          "169:     ).prepare_env(model_uri=model_uri)",
          "",
          "[Removed Lines]",
          "166:     env_manager = env_manager or _EnvManager.VIRTUALENV",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "194:         _logger.info(\"Generating Dockerfile for model %s\", model_uri)",
          "195:     else:",
          "196:         _logger.info(\"Generating Dockerfile\")",
          "198:     backend = get_flavor_backend(model_uri, docker_build=True, env_manager=env_manager)",
          "199:     if backend.can_build_image():",
          "200:         backend.generate_dockerfile(",
          "",
          "[Removed Lines]",
          "197:     env_manager = env_manager or _EnvManager.VIRTUALENV",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "220: @cli_args.MLFLOW_HOME",
          "221: @cli_args.INSTALL_MLFLOW",
          "222: @cli_args.ENABLE_MLSERVER",
          "224:     \"\"\"",
          "225:     Builds a Docker image whose default entrypoint serves an MLflow model at port 8080, using the",
          "226:     python_function flavor. The container serves the model referenced by ``--model-uri``, if",
          "",
          "[Removed Lines]",
          "223: def build_docker(model_uri, name, env_manager, mlflow_home, install_mlflow, enable_mlserver):",
          "",
          "[Added Lines]",
          "213: def build_docker(**kwargs):",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "263:     See https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html for more information on the",
          "264:     'python_function' flavor.",
          "265:     \"\"\"",
          "",
          "[Removed Lines]",
          "266:     env_manager = env_manager or _EnvManager.VIRTUALENV",
          "267:     build_docker_api(",
          "268:         model_uri,",
          "269:         name,",
          "270:         env_manager=env_manager,",
          "271:         mlflow_home=mlflow_home,",
          "272:         install_mlflow=install_mlflow,",
          "273:         enable_mlserver=enable_mlserver,",
          "274:     )",
          "",
          "[Added Lines]",
          "256:     python_api.build_docker(**kwargs)",
          "",
          "---------------"
        ],
        "mlflow/models/python_api.py||mlflow/models/python_api.py": [
          "File: mlflow/models/python_api.py -> mlflow/models/python_api.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: import json",
          "2: import os",
          "3: from io import StringIO",
          "4: from typing import Any, Dict, List, Optional, Union",
          "6: from mlflow.exceptions import MlflowException",
          "7: from mlflow.models.flavor_backend_registry import get_flavor_backend",
          "8: from mlflow.utils import env_manager as _EnvManager",
          "9: from mlflow.utils.file_utils import TempDir",
          "12: def build_docker(",
          "13:     model_uri=None,",
          "14:     name=\"mlflow-pyfunc\",",
          "15:     env_manager=_EnvManager.VIRTUALENV,",
          "16:     mlflow_home=None,",
          "17:     install_mlflow=False,",
          "18:     enable_mlserver=False,",
          "19: ):",
          "20:     \"\"\"",
          "21:     Builds a Docker image whose default entrypoint serves an MLflow model at port 8080, using the",
          "22:     python_function flavor. The container serves the model referenced by ``model_uri``, if",
          "23:     specified. If ``model_uri`` is not specified, an MLflow Model directory must be mounted as a",
          "24:     volume into the /opt/ml/model directory in the container.",
          "26:     .. warning::",
          "28:         If ``model_uri`` is unspecified, the resulting image doesn't support serving models with",
          "29:         the RFunc or Java MLeap model servers.",
          "31:     NB: by default, the container will start nginx and gunicorn processes. If you don't need the",
          "32:     nginx process to be started (for instance if you deploy your container to Google Cloud Run),",
          "33:     you can disable it via the DISABLE_NGINX environment variable:",
          "35:     .. code:: bash",
          "37:         docker run -p 5001:8080 -e DISABLE_NGINX=true \"my-image-name\"",
          "39:     See https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html for more information on the",
          "40:     'python_function' flavor.",
          "41:     \"\"\"",
          "42:     get_flavor_backend(model_uri, docker_build=True, env_manager=env_manager).build_image(",
          "43:         model_uri,",
          "44:         name,",
          "45:         mlflow_home=mlflow_home,",
          "46:         install_mlflow=install_mlflow,",
          "47:         enable_mlserver=enable_mlserver,",
          "48:     )",
          "51: _CONTENT_TYPE_CSV = \"csv\"",
          "52: _CONTENT_TYPE_JSON = \"json\"",
          "55: def predict(",
          "56:     model_uri: str,",
          "57:     # TODO: This is currently subset of PyfuncInput, ideally we should cover all",
          "58:     input_data: Union[str, Dict[str, Any], List[Any], \"pd.DataFrame\", None] = None,  # noqa: F821",
          "59:     input_path: Optional[str] = None,",
          "60:     content_type: str = _CONTENT_TYPE_JSON,",
          "61:     output_path: Optional[str] = None,",
          "62:     env_manager: _EnvManager = _EnvManager.VIRTUALENV,",
          "63:     install_mlflow: bool = False,",
          "64:     pip_requirements_override: Optional[List[str]] = None,",
          "65: ):",
          "66:     \"\"\"",
          "67:     Generate predictions in json format using a saved MLflow model. For information about the input",
          "68:     data formats accepted by this function, see the following documentation:",
          "69:     https://www.mlflow.org/docs/latest/models.html#built-in-deployment-tools.",
          "71:     :param model_uri: URI to the model. A local path, a local or remote URI e.g. runs:/, s3://.",
          "72:     :param input_data: Input data for prediction. It can be one of the following:",
          "74:         - A Python dictionary that contains either:",
          "75:            - single input payload, when content type is \"json\".",
          "76:            - Pandas DataFrame, when content type is \"csv\".",
          "77:         - A Python list. The content type has to be \"csv\".",
          "78:         - A Pandas DataFrame. The content type has to be \"csv\".",
          "79:         - A string represents serialized input data. e.g. '{\"inputs\": [1, 2]}'",
          "80:         - None to input data from stdin.",
          "82:     :param input_path: Path to a file containing input data. If provided, 'input_data' must be None.",
          "83:     :param content_type: Content type of the input data. Can be one of {\u2018json\u2019, \u2018csv\u2019}.",
          "84:     :param output_path: File to output results to as json. If not provided, output to stdout.",
          "85:     :param env_manager: Specify a way to create an environment for MLmodel inference:",
          "87:         - virtualenv (default): use virtualenv (and pyenv for Python version management)",
          "88:         - local: use the local environment",
          "89:         - conda: use conda",
          "91:     :param install_mlflow: If specified and there is a conda or virtualenv environment to be",
          "92:         activated mlflow will be installed into the environment after it has been activated.",
          "93:         The version of installed mlflow will be the same as the one used to invoke this command.",
          "94:     :param pip_requirements_override: If specified, install the specified python dependencies to",
          "95:         the model inference environment. This is particularly useful when you want to add extra",
          "96:         dependencies or try different versions of the dependencies defined in the logged model.",
          "98:     Code example:",
          "100:     .. code-block:: python",
          "102:         import mlflow",
          "104:         run_id = \"...\"",
          "106:         mlflow.pyfunc.predict(",
          "107:             model_uri=f\"runs:/{run_id}/model\",",
          "108:             input_data={\"x\": 1, \"y\": 2},",
          "109:             content_type=\"json\",",
          "110:         )",
          "112:         # Run prediction with addtioanl pip dependencies",
          "113:         mlflow.pyfunc.predict(",
          "114:             model_uri=f\"runs:/{run_id}/model\",",
          "115:             input_data='{\"x\": 1, \"y\": 2}',",
          "116:             content_type=\"json\",",
          "117:             pip_requirements_override=[\"scikit-learn==0.23.2\"],",
          "118:         )",
          "120:     \"\"\"",
          "121:     if content_type not in [_CONTENT_TYPE_JSON, _CONTENT_TYPE_CSV]:",
          "122:         raise MlflowException.invalid_parameter_value(",
          "123:             f\"Content type must be one of {_CONTENT_TYPE_JSON} or {_CONTENT_TYPE_CSV}.\"",
          "124:         )",
          "126:     def _predict(_input_path: str):",
          "127:         return get_flavor_backend(",
          "128:             model_uri, env_manager=env_manager, install_mlflow=install_mlflow",
          "129:         ).predict(",
          "130:             model_uri=model_uri,",
          "131:             input_path=_input_path,",
          "132:             output_path=output_path,",
          "133:             content_type=content_type,",
          "134:             pip_requirements_override=pip_requirements_override,",
          "135:         )",
          "137:     if input_data is not None and input_path is not None:",
          "138:         raise MlflowException.invalid_parameter_value(",
          "139:             \"Both input_data and input_path are provided. Only one of them should be specified.\"",
          "140:         )",
          "141:     elif input_data is not None:",
          "142:         input_data = _serialize_input_data(input_data, content_type)",
          "144:         # Write input data to a temporary file",
          "145:         with TempDir() as tmp:",
          "146:             input_path = os.path.join(tmp.path(), f\"input.{content_type}\")",
          "147:             with open(input_path, \"w\") as f:",
          "148:                 f.write(input_data)",
          "150:             _predict(input_path)",
          "151:     else:",
          "152:         _predict(input_path)",
          "155: def _serialize_input_data(input_data, content_type):",
          "156:     # build-docker command is available in mlflow-skinny (which doesn't contain pandas)",
          "157:     # so we shouldn't import pandas at the top level",
          "158:     import pandas as pd",
          "160:     valid_input_types = {",
          "161:         _CONTENT_TYPE_CSV: (str, list, dict, pd.DataFrame),",
          "162:         _CONTENT_TYPE_JSON: (str, dict),",
          "163:     }.get(content_type)",
          "165:     if not isinstance(input_data, valid_input_types):",
          "166:         raise MlflowException.invalid_parameter_value(",
          "167:             f\"Input data must be one of {valid_input_types} when content type is '{content_type}'.\"",
          "168:         )",
          "170:     # If the input is already string, check if the input string can be deserialized correctly",
          "171:     if isinstance(input_data, str):",
          "172:         _validate_string(input_data, content_type)",
          "173:         return input_data",
          "175:     try:",
          "176:         if content_type == _CONTENT_TYPE_CSV:",
          "177:             # We should not set header=False here because the scoring server expects the",
          "178:             # first row to be the header",
          "179:             return pd.DataFrame(input_data).to_csv(index=False)",
          "180:         else:",
          "181:             return json.dumps(input_data)",
          "182:     except Exception as e:",
          "183:         raise MlflowException.invalid_parameter_value(",
          "184:             message=\"Input data could not be serialized to {content_type}.\"",
          "185:         ) from e",
          "188: def _validate_string(input_data: str, content_type: str):",
          "189:     try:",
          "190:         if content_type == _CONTENT_TYPE_CSV:",
          "191:             import pandas as pd",
          "193:             pd.read_csv(StringIO(input_data))",
          "194:         else:",
          "195:             json.loads(input_data)",
          "196:     except Exception as e:",
          "197:         target = \"JSON\" if content_type == _CONTENT_TYPE_JSON else \"Pandas Dataframe\"",
          "198:         raise MlflowException.invalid_parameter_value(",
          "199:             message=f\"Failed to deserialize input string data to {target}.\"",
          "200:         ) from e",
          "",
          "---------------"
        ],
        "mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py||mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py": [
          "File: mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py -> mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "15:     return parser.parse_args()",
          "18: def main():",
          "19:     args = parse_args()",
          "28: if __name__ == \"__main__\":",
          "",
          "[Removed Lines]",
          "20:     _predict(",
          "21:         model_uri=args.model_uri,",
          "22:         input_path=args.input_path if args.input_path else None,",
          "23:         output_path=args.output_path if args.output_path else None,",
          "24:         content_type=args.content_type,",
          "25:     )",
          "",
          "[Added Lines]",
          "18: # Guidance for fixing missing module error",
          "19: _MISSING_MODULE_HELP_MSG = (",
          "20:     \"Exception occurred while running inference: {e}\"",
          "21:     \"\\n\\n\"",
          "22:     \"\\033[93m[Hint] It appears that your MLflow Model doesn't contain the required \"",
          "23:     \"dependency '{missing_module}' to run model inference. When logging a model, MLflow \"",
          "24:     \"detects dependencies based on the model flavor, but it is possible that some \"",
          "25:     \"dependencies are not captured. In this case, you can manually add dependencies \"",
          "26:     \"using the `extra_pip_requirements` parameter of `mlflow.pyfunc.log_model`.\\033[0m\"",
          "27:     \"\"\"",
          "29: \\033[1mSample code:\\033[0m",
          "30:     ----",
          "31:     mlflow.pyfunc.log_model(",
          "32:         artifact_path=\"model\",",
          "33:         python_model=your_model,",
          "34:         extra_pip_requirements=[\"{missing_module}==x.y.z\"]",
          "35:     )",
          "36:     ----",
          "38:     For mode guidance on fixing missing dependencies, please refer to the MLflow docs:",
          "39:     https://www.mlflow.org/docs/latest/deployment/index.html#how-to-fix-dependency-errors-when-serving-my-model",
          "40: \"\"\"",
          "41: )",
          "47:     try:",
          "48:         _predict(",
          "49:             model_uri=args.model_uri,",
          "50:             input_path=args.input_path if args.input_path else None,",
          "51:             output_path=args.output_path if args.output_path else None,",
          "52:             content_type=args.content_type,",
          "53:         )",
          "54:     except ModuleNotFoundError as e:",
          "55:         message = _MISSING_MODULE_HELP_MSG.format(e=str(e), missing_module=e.name)",
          "56:         raise RuntimeError(message) from e",
          "",
          "---------------"
        ],
        "mlflow/pyfunc/backend.py||mlflow/pyfunc/backend.py": [
          "File: mlflow/pyfunc/backend.py -> mlflow/pyfunc/backend.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "11: from pathlib import Path",
          "13: from mlflow.environment_variables import MLFLOW_DISABLE_ENV_CREATION",
          "14: from mlflow.models import FlavorBackend",
          "15: from mlflow.models.container import ENABLE_MLSERVER",
          "16: from mlflow.models.docker_utils import (",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "14: from mlflow.exceptions import MlflowException",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "37:     path_to_local_file_uri,",
          "38: )",
          "39: from mlflow.utils.nfs_on_spark import get_nfs_cache_root_dir",
          "41: from mlflow.utils.virtualenv import (",
          "42:     _get_or_create_virtualenv,",
          "43:     _get_pip_install_mlflow,",
          "",
          "[Removed Lines]",
          "40: from mlflow.utils.process import cache_return_value_per_process",
          "",
          "[Added Lines]",
          "41: from mlflow.utils.process import ShellCommandException, cache_return_value_per_process",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "84:         self._env_root_dir = env_root_dir",
          "85:         self._environment = None",
          "88:         if self._environment is not None:",
          "89:             return self._environment",
          "",
          "[Removed Lines]",
          "87:     def prepare_env(self, model_uri, capture_output=False):",
          "",
          "[Added Lines]",
          "88:     def prepare_env(self, model_uri, capture_output=False, pip_requirements_override=None):",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "114:                 self._env_id,",
          "115:                 env_root_dir=env_root_dir,",
          "116:                 capture_output=capture_output,",
          "117:             )",
          "118:             self._environment = Environment(activate_cmd)",
          "119:         elif self._env_manager == _EnvManager.CONDA:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "118:                 pip_requirements_override=pip_requirements_override,",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "123:                 env_id=self._env_id,",
          "124:                 capture_output=capture_output,",
          "125:                 env_root_dir=env_root_dir,",
          "126:             )",
          "128:         elif self._env_manager == _EnvManager.LOCAL:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "128:                 pip_requirements_override=pip_requirements_override,",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "138:         return self._environment",
          "141:         \"\"\"",
          "142:         Generate predictions using generic python model saved with MLflow. The expected format of",
          "143:         the input JSON is the MLflow scoring format.",
          "",
          "[Removed Lines]",
          "140:     def predict(self, model_uri, input_path, output_path, content_type):",
          "",
          "[Added Lines]",
          "143:     def predict(",
          "144:         self,",
          "145:         model_uri,",
          "146:         input_path,",
          "147:         output_path,",
          "148:         content_type,",
          "149:         pip_requirements_override=None,",
          "150:     ):",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "161:                 predict_cmd += [\"--input-path\", shlex.quote(str(input_path))]",
          "162:             if output_path:",
          "163:                 predict_cmd += [\"--output-path\", shlex.quote(str(output_path))]",
          "165:         else:",
          "166:             scoring_server._predict(local_uri, input_path, output_path, content_type)",
          "168:     def serve(",
          "",
          "[Removed Lines]",
          "164:             return self.prepare_env(local_path).execute(\" \".join(predict_cmd))",
          "",
          "[Added Lines]",
          "175:             if pip_requirements_override and self._env_manager == _EnvManager.CONDA:",
          "176:                 # Conda use = instead of == for version pinning",
          "177:                 pip_requirements_override = [",
          "178:                     l.replace(\"==\", \"=\") for l in pip_requirements_override",
          "179:                 ]",
          "181:             environment = self.prepare_env(",
          "182:                 local_path, pip_requirements_override=pip_requirements_override",
          "183:             )",
          "185:             try:",
          "186:                 environment.execute(\" \".join(predict_cmd))",
          "187:             except ShellCommandException as e:",
          "188:                 raise MlflowException(",
          "189:                     f\"{e}\\n\\nAn exception occurred while running model prediction within a \"",
          "190:                     f\"{self._env_manager} environment. You can find the error message \"",
          "191:                     f\"from the prediction subprocess by scrolling above.\"",
          "192:                 ) from None",
          "194:             if pip_requirements_override:",
          "195:                 raise MlflowException(",
          "196:                     \"`pip_requirements_override` is not supported for local env manager.\"",
          "197:                     \"Please use conda or virtualenv instead.\"",
          "198:                 )",
          "",
          "---------------"
        ],
        "mlflow/rfunc/backend.py||mlflow/rfunc/backend.py": [
          "File: mlflow/rfunc/backend.py -> mlflow/rfunc/backend.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "4: import subprocess",
          "5: import sys",
          "7: from mlflow.models import FlavorBackend",
          "8: from mlflow.tracking.artifact_utils import _download_artifact_from_uri",
          "9: from mlflow.utils.string_utils import quote",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "7: from mlflow.exceptions import MlflowException",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "28:     version_pattern = re.compile(r\"version ([0-9]+\\.[0-9]+\\.[0-9]+)\")",
          "31:         \"\"\"",
          "32:         Generate predictions using R model saved with MLflow.",
          "33:         Return the prediction results as a JSON.",
          "34:         \"\"\"",
          "35:         model_path = _download_artifact_from_uri(model_uri)",
          "36:         str_cmd = (",
          "37:             \"mlflow:::mlflow_rfunc_predict(model_path = '{0}', input_path = {1}, \"",
          "",
          "[Removed Lines]",
          "30:     def predict(self, model_uri, input_path, output_path, content_type):",
          "",
          "[Added Lines]",
          "31:     def predict(",
          "32:         self, model_uri, input_path, output_path, content_type, pip_requirements_override=None",
          "33:     ):",
          "38:         if pip_requirements_override is not None:",
          "39:             raise MlflowException(\"pip_requirements_override is not supported in the R backend.\")",
          "",
          "---------------"
        ],
        "mlflow/utils/cli_args.py||mlflow/utils/cli_args.py": [
          "File: mlflow/utils/cli_args.py -> mlflow/utils/cli_args.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "80:     return None",
          "84:     return click.option(",
          "85:         \"--env-manager\",",
          "87:         type=click.UNPROCESSED,",
          "88:         callback=_resolve_env_manager,",
          "89:         help=help_string,",
          "",
          "[Removed Lines]",
          "83: def _create_env_manager_option(help_string):",
          "86:         default=None,",
          "",
          "[Added Lines]",
          "83: def _create_env_manager_option(help_string, default=None):",
          "86:         default=default,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "93: ENV_MANAGER = _create_env_manager_option(",
          "94:     # '\\b' prevents rewrapping text:",
          "95:     # https://click.palletsprojects.com/en/8.1.x/documentation/#preventing-rewrapping",
          "96:     help_string=\"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "94:     default=_EnvManager.VIRTUALENV,",
          "",
          "---------------"
        ],
        "mlflow/utils/conda.py||mlflow/utils/conda.py": [
          "File: mlflow/utils/conda.py -> mlflow/utils/conda.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "200:     }",
          "204:     \"\"\"",
          "205:     Given a `Project`, creates a conda environment containing the project's dependencies if such a",
          "206:     conda environment doesn't already exist. Returns the name of the conda environment.",
          "",
          "[Removed Lines]",
          "203: def get_or_create_conda_env(conda_env_path, env_id=None, capture_output=False, env_root_dir=None):",
          "",
          "[Added Lines]",
          "203: def get_or_create_conda_env(",
          "204:     conda_env_path,",
          "205:     env_id=None,",
          "206:     capture_output=False,",
          "207:     env_root_dir=None,",
          "208:     pip_requirements_override=None,",
          "209: ):",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "213:     :param capture_output: Specify the capture_output argument while executing the",
          "214:                            \"conda env create\" command.",
          "215:     :param env_root_dir: See doc of PyFuncBackend constructor argument `env_root_dir`.",
          "216:     \"\"\"",
          "218:     conda_path = get_conda_bin_executable(\"conda\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "222:     :param pip_requirements_override: If specified, install the specified python dependencies to",
          "223:                                       the environment (upgrade if already installed).",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "266:             if \"PYTEST_CURRENT_TEST\" in os.environ",
          "267:             else _create_conda_env",
          "268:         )",
          "270:             conda_env_path,",
          "271:             conda_env_create_path,",
          "272:             project_env_name,",
          "273:             conda_extra_env_vars,",
          "274:             capture_output,",
          "275:         )",
          "276:     except Exception:",
          "277:         try:",
          "278:             if project_env_name in _list_conda_environments(conda_extra_env_vars):",
          "",
          "[Removed Lines]",
          "269:         return _create_conda_env_func(",
          "",
          "[Added Lines]",
          "277:         conda_env = _create_conda_env_func(",
          "285:         if pip_requirements_override:",
          "286:             _logger.info(",
          "287:                 \"Installing additional dependencies specified\"",
          "288:                 f\"by pip_requirements_override: {pip_requirements_override}\"",
          "289:             )",
          "290:             cmd = [",
          "291:                 conda_path,",
          "292:                 \"install\",",
          "293:                 \"-n\",",
          "294:                 project_env_name,",
          "295:                 \"--yes\",",
          "296:                 \"--quiet\",",
          "298:             ]",
          "299:             process._exec_cmd(cmd, extra_env=conda_extra_env_vars, capture_output=capture_output)",
          "301:         return conda_env",
          "",
          "---------------"
        ],
        "mlflow/utils/virtualenv.py||mlflow/utils/virtualenv.py": [
          "File: mlflow/utils/virtualenv.py -> mlflow/utils/virtualenv.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "322: def _get_or_create_virtualenv(",
          "324: ):",
          "325:     \"\"\"",
          "326:     Restores an MLflow model's environment with pyenv and virtualenv and returns a command",
          "",
          "[Removed Lines]",
          "323:     local_model_path, env_id=None, env_root_dir=None, capture_output=False",
          "",
          "[Added Lines]",
          "323:     local_model_path,",
          "324:     env_id=None,",
          "325:     env_root_dir=None,",
          "326:     capture_output=False,",
          "327:     pip_requirements_override=None,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "334:                    environment after the environment has been activated.",
          "335:     :return: Command to activate the created virtualenv environment",
          "336:              (e.g. \"source /path/to/bin/activate\").",
          "337:     \"\"\"",
          "338:     _validate_pyenv_is_available()",
          "339:     _validate_virtualenv_is_available()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "341:     :param pip_requirements_override: If specified, install the specified python dependencies to",
          "342:                                       the environment (upgrade if already installed).",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "361:     env_name = _get_virtualenv_name(python_env, local_model_path, env_id)",
          "362:     env_dir = virtual_envs_root_path / env_name",
          "363:     try:",
          "365:             local_model_path,",
          "366:             python_bin_path,",
          "367:             env_dir,",
          "",
          "[Removed Lines]",
          "364:         return _create_virtualenv(",
          "",
          "[Added Lines]",
          "370:         activate_cmd = _create_virtualenv(",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "369:             extra_env=extra_env,",
          "370:             capture_output=capture_output,",
          "371:         )",
          "372:     except:",
          "373:         _logger.warning(\"Encountered unexpected error while creating %s\", env_dir)",
          "374:         if env_dir.exists():",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "379:         # Install additional dependencies specified by `requirements_override`",
          "380:         if pip_requirements_override:",
          "381:             _logger.info(",
          "382:                 \"Installing additional dependencies specified by \"",
          "383:                 f\"pip_requirements_override: {pip_requirements_override}\"",
          "384:             )",
          "385:             cmd = _join_commands(",
          "386:                 activate_cmd,",
          "387:                 f\"python -m pip install --quiet -U {' '.join(pip_requirements_override)}\",",
          "388:             )",
          "389:             _exec_cmd(cmd, capture_output=capture_output, extra_env=extra_env)",
          "391:         return activate_cmd",
          "",
          "---------------"
        ],
        "tests/models/test_cli.py||tests/models/test_cli.py": [
          "File: tests/models/test_cli.py -> tests/models/test_cli.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "368:         check=False,",
          "369:     )",
          "370:     assert prc.returncode != 0",
          "374: def test_predict_check_input_path(iris_data, sk_model, tmp_path):",
          "",
          "[Removed Lines]",
          "371:     assert \"Unknown content type\" in prc.stderr.decode(\"utf-8\")",
          "",
          "[Added Lines]",
          "371:     assert \"Content type must be one of json or csv.\" in prc.stderr.decode(\"utf-8\")",
          "",
          "---------------"
        ],
        "tests/models/test_python_api.py||tests/models/test_python_api.py": [
          "File: tests/models/test_python_api.py -> tests/models/test_python_api.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: import sys",
          "2: from unittest import mock",
          "4: import numpy as np",
          "5: import pandas as pd",
          "6: import pytest",
          "8: import mlflow",
          "9: from mlflow.exceptions import MlflowException",
          "10: from mlflow.models.python_api import (",
          "11:     _CONTENT_TYPE_CSV,",
          "12:     _CONTENT_TYPE_JSON,",
          "13:     _serialize_input_data,",
          "14: )",
          "15: from mlflow.utils.env_manager import CONDA, VIRTUALENV",
          "18: @pytest.mark.parametrize(",
          "19:     (\"input_data\", \"expected_data\", \"content_type\"),",
          "20:     [",
          "21:         (",
          "22:             \"x,y\\n1,3\\n2,4\",",
          "23:             pd.DataFrame({\"x\": [1, 2], \"y\": [3, 4]}),",
          "24:             _CONTENT_TYPE_CSV,",
          "25:         ),",
          "26:         (",
          "27:             {\"inputs\": {\"a\": [1]}},",
          "28:             {\"a\": np.array([1])},",
          "29:             _CONTENT_TYPE_JSON,",
          "30:         ),",
          "31:     ],",
          "32: )",
          "33: def test_predict(input_data, expected_data, content_type):",
          "34:     class TestModel(mlflow.pyfunc.PythonModel):",
          "35:         def predict(self, context, model_input):",
          "36:             if type(model_input) == pd.DataFrame:",
          "37:                 assert model_input.equals(expected_data)",
          "38:             else:",
          "39:                 assert model_input == expected_data",
          "40:             return {}",
          "42:     with mlflow.start_run() as run:",
          "43:         mlflow.pyfunc.log_model(",
          "44:             artifact_path=\"model\",",
          "45:             python_model=TestModel(),",
          "46:         )",
          "47:         run_id = run.info.run_id",
          "49:     mlflow.models.predict(",
          "50:         model_uri=f\"runs:/{run_id}/model\",",
          "51:         input_data=input_data,",
          "52:         content_type=content_type,",
          "53:     )",
          "56: @pytest.mark.parametrize(",
          "57:     \"env_manager\",",
          "58:     [VIRTUALENV, CONDA],",
          "59: )",
          "60: def test_predict_with_pip_requirements_override(env_manager):",
          "61:     if env_manager == CONDA:",
          "62:         if sys.platform == \"win32\":",
          "63:             pytest.skip(\"Skipping conda tests on Windows\")",
          "65:     class TestModel(mlflow.pyfunc.PythonModel):",
          "66:         def predict(self, context, model_input):",
          "67:             # XGBoost should be installed by pip_requirements_override",
          "68:             import xgboost",
          "70:             assert xgboost.__version__ == \"1.7.3\"",
          "72:             # Scikit-learn version should be overridden to 1.3.0 by pip_requirements_override",
          "73:             import sklearn",
          "75:             assert sklearn.__version__ == \"1.3.0\"",
          "77:     with mlflow.start_run() as run:",
          "78:         mlflow.pyfunc.log_model(",
          "79:             artifact_path=\"model\",",
          "80:             python_model=TestModel(),",
          "81:             extra_pip_requirements=[\"scikit-learn==1.3.2\"],",
          "82:         )",
          "83:         run_id = run.info.run_id",
          "85:     requirements_override = [\"xgboost==1.7.3\", \"scikit-learn==1.3.0\"]",
          "86:     if env_manager == CONDA:",
          "87:         # Install charset-normalizer with conda-forge to work around pip-vs-conda issue during",
          "88:         # CI tests. At the beginning of the CI test, it installs MLflow dependencies via pip,",
          "89:         # which includes charset-normalizer. Then when it runs this test case, the conda env",
          "90:         # is created but charset-normalizer is installed via the default channel, which is one",
          "91:         # major version behind the version installed via pip (as of 2024 Jan). As a result,",
          "92:         # Python env confuses pip and conda versions and cause errors like \"ImportError: cannot",
          "93:         # import name 'COMMON_SAFE_ASCII_CHARACTERS' from 'charset_normalizer.constant'\".",
          "94:         # To work around this, we install the latest cversion from the conda-forge.",
          "95:         # TODO: Implement better isolation approach for pip and conda environments during testing.",
          "96:         requirements_override.append(\"conda-forge::charset-normalizer\")",
          "98:     mlflow.models.predict(",
          "99:         model_uri=f\"runs:/{run_id}/model\",",
          "100:         input_data={\"inputs\": [1, 2, 3]},",
          "101:         content_type=_CONTENT_TYPE_JSON,",
          "102:         pip_requirements_override=requirements_override,",
          "103:         env_manager=env_manager,",
          "104:     )",
          "107: @pytest.fixture",
          "108: def mock_backend():",
          "109:     mock_backend = mock.MagicMock()",
          "110:     with mock.patch(\"mlflow.models.python_api.get_flavor_backend\", return_value=mock_backend):",
          "111:         yield mock_backend",
          "114: def test_predict_with_both_input_data_and_path_raise(mock_backend):",
          "115:     with pytest.raises(MlflowException, match=r\"Both input_data and input_path are provided\"):",
          "116:         mlflow.models.predict(",
          "117:             model_uri=\"runs:/test/Model\",",
          "118:             input_data={\"inputs\": [1, 2, 3]},",
          "119:             input_path=\"input.csv\",",
          "120:             content_type=_CONTENT_TYPE_CSV,",
          "121:         )",
          "124: def test_predict_invalid_content_type(mock_backend):",
          "125:     with pytest.raises(MlflowException, match=r\"Content type must be one of\"):",
          "126:         mlflow.models.predict(",
          "127:             model_uri=\"runs:/test/Model\",",
          "128:             input_data={\"inputs\": [1, 2, 3]},",
          "129:             content_type=\"any\",",
          "130:         )",
          "133: def test_predict_with_input_none(mock_backend):",
          "134:     mlflow.models.predict(",
          "135:         model_uri=\"runs:/test/Model\",",
          "136:         content_type=_CONTENT_TYPE_CSV,",
          "137:     )",
          "139:     mock_backend.predict.assert_called_once_with(",
          "140:         model_uri=\"runs:/test/Model\",",
          "141:         input_path=None,",
          "142:         output_path=None,",
          "143:         content_type=_CONTENT_TYPE_CSV,",
          "144:         pip_requirements_override=None,",
          "145:     )",
          "148: @pytest.mark.parametrize(",
          "149:     (\"input_data\", \"content_type\", \"expected\"),",
          "150:     [",
          "151:         # String (no change)",
          "152:         ('{\"inputs\": [1, 2, 3]}', _CONTENT_TYPE_JSON, '{\"inputs\": [1, 2, 3]}'),",
          "153:         (\"x,y,z\\n1,2,3\\n4,5,6\", _CONTENT_TYPE_CSV, \"x,y,z\\n1,2,3\\n4,5,6\"),",
          "154:         # List",
          "155:         ([1, 2, 3], _CONTENT_TYPE_CSV, \"0\\n1\\n2\\n3\\n\"),  # a header '0' is added by pandas",
          "156:         ([[1, 2, 3], [4, 5, 6]], _CONTENT_TYPE_CSV, \"0,1,2\\n1,2,3\\n4,5,6\\n\"),",
          "157:         # Dict (pandas)",
          "158:         (",
          "159:             {",
          "160:                 \"x\": [",
          "161:                     1,",
          "162:                     2,",
          "163:                 ],",
          "164:                 \"y\": [3, 4],",
          "165:             },",
          "166:             _CONTENT_TYPE_CSV,",
          "167:             \"x,y\\n1,3\\n2,4\\n\",",
          "168:         ),",
          "169:         # Dict (json)",
          "170:         ({\"inputs\": [1, 2, 3]}, _CONTENT_TYPE_JSON, '{\"inputs\": [1, 2, 3]}'),",
          "171:         # Pandas DataFrame",
          "172:         (pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6]}), _CONTENT_TYPE_CSV, \"x,y\\n1,4\\n2,5\\n3,6\\n\"),",
          "173:     ],",
          "174: )",
          "175: def test_serialize_input_data(input_data, content_type, expected):",
          "176:     assert _serialize_input_data(input_data, content_type) == expected",
          "179: @pytest.mark.parametrize(",
          "180:     (\"input_data\", \"content_type\"),",
          "181:     [",
          "182:         # Invalid input datatype for the content type",
          "183:         (1, _CONTENT_TYPE_CSV),",
          "184:         ({1, 2, 3}, _CONTENT_TYPE_CSV),",
          "185:         (1, _CONTENT_TYPE_JSON),",
          "186:         (True, _CONTENT_TYPE_JSON),",
          "187:         ([1, 2, 3], _CONTENT_TYPE_JSON),",
          "188:         # Invalid string",
          "189:         (\"{inputs: [1, 2, 3]}\", _CONTENT_TYPE_JSON),",
          "190:         (\"x,y\\n1,2\\n3,4,5\\n\", _CONTENT_TYPE_CSV),",
          "191:         # Invalid list",
          "192:         ([[1, 2], [3, 4], 5], _CONTENT_TYPE_CSV),",
          "193:         # Invalid dict (unserealizable)",
          "194:         ({\"x\": 1, \"y\": {1, 2, 3}}, _CONTENT_TYPE_JSON),",
          "195:     ],",
          "196: )",
          "197: def test_serialize_input_data_invalid_format(input_data, content_type):",
          "198:     with pytest.raises(MlflowException):  # noqa: PT011",
          "199:         _serialize_input_data(input_data, content_type)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c16a2d1acc448e939c532367379d5c11973b93df",
      "candidate_info": {
        "commit_hash": "c16a2d1acc448e939c532367379d5c11973b93df",
        "repo": "mlflow/mlflow",
        "commit_url": "https://github.com/mlflow/mlflow/commit/c16a2d1acc448e939c532367379d5c11973b93df",
        "files": [
          ".github/workflows/master.yml",
          "mlflow/models/cli.py",
          "mlflow/models/container/__init__.py",
          "mlflow/models/docker_utils.py",
          "mlflow/models/python_api.py",
          "mlflow/pyfunc/backend.py",
          "mlflow/sagemaker/cli.py",
          "mlflow/utils/cli_args.py",
          "mlflow/utils/model_utils.py",
          "tests/diviner/test_diviner_model_export.py",
          "tests/pyfunc/docker/conftest.py",
          "tests/pyfunc/docker/test_docker.py",
          "tests/pyfunc/docker/test_docker_flavors.py",
          "tests/pyfunc/test_docker.py",
          "tests/resources/dockerfile/Dockerfile_conda",
          "tests/resources/dockerfile/Dockerfile_default",
          "tests/resources/dockerfile/Dockerfile_enable_mlserver",
          "tests/resources/dockerfile/Dockerfile_install_mlflow",
          "tests/resources/dockerfile/Dockerfile_install_mlflow_from_mlflow_home",
          "tests/resources/dockerfile/Dockerfile_java_flavor",
          "tests/resources/dockerfile/Dockerfile_no_model_uri",
          "tests/resources/dockerfile/Dockerfile_sagemaker_conda",
          "tests/resources/dockerfile/Dockerfile_sagemaker_virtualenv",
          "tests/resources/dockerfile/Dockerfile_with_mlflow_home",
          "tests/sagemaker/test_cli.py"
        ],
        "message": "Optimize Docker image for model serving (#10954)\n\nSigned-off-by: B-Step62 <yuki.watanabe@databricks.com>",
        "before_after_code_files": [
          "mlflow/models/cli.py||mlflow/models/cli.py",
          "mlflow/models/container/__init__.py||mlflow/models/container/__init__.py",
          "mlflow/models/docker_utils.py||mlflow/models/docker_utils.py",
          "mlflow/models/python_api.py||mlflow/models/python_api.py",
          "mlflow/pyfunc/backend.py||mlflow/pyfunc/backend.py",
          "mlflow/sagemaker/cli.py||mlflow/sagemaker/cli.py",
          "mlflow/utils/cli_args.py||mlflow/utils/cli_args.py",
          "mlflow/utils/model_utils.py||mlflow/utils/model_utils.py",
          "tests/diviner/test_diviner_model_export.py||tests/diviner/test_diviner_model_export.py",
          "tests/pyfunc/docker/conftest.py||tests/pyfunc/docker/conftest.py",
          "tests/pyfunc/docker/test_docker.py||tests/pyfunc/docker/test_docker.py",
          "tests/pyfunc/docker/test_docker_flavors.py||tests/pyfunc/docker/test_docker_flavors.py",
          "tests/pyfunc/test_docker.py||tests/pyfunc/test_docker.py",
          "tests/sagemaker/test_cli.py||tests/sagemaker/test_cli.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "mlflow/pyfunc/backend.py||mlflow/pyfunc/backend.py"
          ],
          "candidate": [
            "mlflow/pyfunc/backend.py||mlflow/pyfunc/backend.py"
          ]
        }
      },
      "candidate_diff": {
        "mlflow/models/cli.py||mlflow/models/cli.py": [
          "File: mlflow/models/cli.py -> mlflow/models/cli.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "170: )",
          "171: @cli_args.ENV_MANAGER",
          "172: @cli_args.MLFLOW_HOME",
          "173: @cli_args.INSTALL_MLFLOW",
          "174: @cli_args.ENABLE_MLSERVER",
          "175: def generate_dockerfile(",
          "177: ):",
          "178:     \"\"\"",
          "179:     Generates a directory with Dockerfile whose default entrypoint serves an MLflow model at port",
          "",
          "[Removed Lines]",
          "176:     model_uri, output_directory, env_manager, mlflow_home, install_mlflow, enable_mlserver",
          "",
          "[Added Lines]",
          "173: @cli_args.INSTALL_JAVA",
          "177:     model_uri,",
          "178:     output_directory,",
          "179:     env_manager,",
          "180:     mlflow_home,",
          "181:     install_java,",
          "182:     install_mlflow,",
          "183:     enable_mlserver,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "191:             model_uri,",
          "192:             output_directory,",
          "193:             mlflow_home=mlflow_home,",
          "194:             install_mlflow=install_mlflow,",
          "195:             enable_mlserver=enable_mlserver,",
          "196:         )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "201:             install_java=install_java,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "208: @click.option(\"--name\", \"-n\", default=\"mlflow-pyfunc-servable\", help=\"Name to use for built image\")",
          "209: @cli_args.ENV_MANAGER",
          "210: @cli_args.MLFLOW_HOME",
          "211: @cli_args.INSTALL_MLFLOW",
          "212: @cli_args.ENABLE_MLSERVER",
          "213: def build_docker(**kwargs):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "219: @cli_args.INSTALL_JAVA",
          "",
          "---------------"
        ],
        "mlflow/models/container/__init__.py||mlflow/models/container/__init__.py": [
          "File: mlflow/models/container/__init__.py -> mlflow/models/container/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "12: import sys",
          "13: from pathlib import Path",
          "14: from subprocess import Popen, check_call",
          "16: import mlflow",
          "17: import mlflow.version",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "15: from typing import List",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "22: from mlflow.pyfunc import _extract_conda_env, mlserver, scoring_server",
          "23: from mlflow.store.artifact.models_artifact_repo import REGISTERED_MODEL_META_FILE_NAME",
          "24: from mlflow.utils import env_manager as em",
          "25: from mlflow.utils.file_utils import read_yaml",
          "26: from mlflow.utils.virtualenv import _get_or_create_virtualenv",
          "27: from mlflow.version import VERSION as MLFLOW_VERSION",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: from mlflow.utils.environment import _PythonEnv",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "86:     Creates a conda env for serving the model at the specified path and installs almost all serving",
          "87:     dependencies into the environment - MLflow is not installed as it's not available via conda.",
          "88:     \"\"\"",
          "118:     # NB: install gunicorn[gevent] from pip rather than from conda because gunicorn is already",
          "119:     # dependency of mlflow on pip and we expect mlflow to be part of the environment.",
          "",
          "[Removed Lines]",
          "89:     # If model is a pyfunc model, create its conda env (even if it also has mleap flavor)",
          "90:     activate_cmd = []",
          "91:     if model_path:",
          "92:         model_config_path = os.path.join(model_path, MLMODEL_FILE_NAME)",
          "93:         model = Model.load(model_config_path)",
          "94:         # NOTE: this differs from _serve cause we always activate the env even if you're serving",
          "95:         # an mleap model",
          "96:         if pyfunc.FLAVOR_NAME not in model.flavors:",
          "97:             return",
          "98:         conf = model.flavors[pyfunc.FLAVOR_NAME]",
          "99:         if pyfunc.ENV in conf:",
          "100:             _logger.info(\"creating and activating custom environment\")",
          "101:             env = _extract_conda_env(conf[pyfunc.ENV])",
          "102:             env_path_dst = os.path.join(\"/opt/mlflow/\", env)",
          "103:             env_path_dst_dir = os.path.dirname(env_path_dst)",
          "104:             if not os.path.exists(env_path_dst_dir):",
          "105:                 os.makedirs(env_path_dst_dir)",
          "106:             shutil.copy2(os.path.join(MODEL_PATH, env), env_path_dst)",
          "107:             if env_manager == em.CONDA:",
          "108:                 conda_create_model_env = f\"conda env create -n custom_env -f {env_path_dst}\"",
          "109:                 if Popen([\"bash\", \"-c\", conda_create_model_env]).wait() != 0:",
          "110:                     raise Exception(\"Failed to create model environment.\")",
          "111:                 activate_cmd = [\"source /miniconda/bin/activate custom_env\"]",
          "112:             elif env_manager == em.VIRTUALENV:",
          "113:                 env_activate_cmd = _get_or_create_virtualenv(model_path)",
          "114:                 path = env_activate_cmd.split(\" \")[-1]",
          "115:                 os.symlink(path, \"/opt/activate\")",
          "116:                 activate_cmd = [env_activate_cmd]",
          "",
          "[Added Lines]",
          "91:     activate_cmd = _install_model_dependencies_to_env(model_path, env_manager) if model_path else []",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "125:     if Popen([\"bash\", \"-c\", \" && \".join(activate_cmd + install_server_deps)]).wait() != 0:",
          "126:         raise Exception(\"Failed to install serving dependencies into the model environment.\")",
          "128:     if len(activate_cmd) and install_mlflow:",
          "129:         install_mlflow_cmd = [",
          "130:             \"pip install /opt/mlflow/.\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "103:     # NB: If we don't use virtualenv or conda env, we don't need to install mlflow here as",
          "104:     # it's already installed in the container.",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "136:     return activate_cmd",
          "139: def _serve_pyfunc(model, env_manager):",
          "140:     # option to disable manually nginx. The default behavior is to enable nginx.",
          "141:     disable_nginx = os.getenv(DISABLE_NGINX, \"false\").lower() == \"true\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "116: def _install_model_dependencies_to_env(model_path, env_manager) -> List[str]:",
          "117:     \"\"\":",
          "118:     Installs model dependencies to the specified environment, which can be either a local",
          "119:     environment, a conda environment, or a virtualenv.",
          "121:     Returns:",
          "122:         Empty list if local environment, otherwise a list of bash commands to activate the",
          "123:         virtualenv or conda environment.",
          "124:     \"\"\"",
          "125:     model_config_path = os.path.join(model_path, MLMODEL_FILE_NAME)",
          "126:     model = Model.load(model_config_path)",
          "128:     conf = model.flavors.get(pyfunc.FLAVOR_NAME, {})",
          "129:     if pyfunc.ENV not in conf:",
          "130:         return []",
          "131:     env_conf = conf[mlflow.pyfunc.ENV]",
          "133:     if env_manager == em.LOCAL:",
          "134:         # Install pip dependencies directly into the local environment",
          "135:         python_env_config_path = os.path.join(model_path, env_conf[em.VIRTUALENV])",
          "136:         python_env = _PythonEnv.from_yaml(python_env_config_path)",
          "137:         deps = \" \".join(python_env.build_dependencies + python_env.dependencies)",
          "138:         deps = deps.replace(\"requirements.txt\", os.path.join(model_path, \"requirements.txt\"))",
          "139:         if Popen([\"bash\", \"-c\", f\"python -m pip install {deps}\"]).wait() != 0:",
          "140:             raise Exception(\"Failed to install model dependencies.\")",
          "141:         return []",
          "143:     _logger.info(\"creating and activating custom environment\")",
          "145:     env = _extract_conda_env(env_conf)",
          "146:     env_path_dst = os.path.join(\"/opt/mlflow/\", env)",
          "147:     env_path_dst_dir = os.path.dirname(env_path_dst)",
          "148:     if not os.path.exists(env_path_dst_dir):",
          "149:         os.makedirs(env_path_dst_dir)",
          "150:     shutil.copy2(os.path.join(MODEL_PATH, env), env_path_dst)",
          "152:     if env_manager == em.CONDA:",
          "153:         conda_create_model_env = f\"conda env create -n custom_env -f {env_path_dst}\"",
          "154:         if Popen([\"bash\", \"-c\", conda_create_model_env]).wait() != 0:",
          "155:             raise Exception(\"Failed to create model environment.\")",
          "156:         activate_cmd = [\"source /miniconda/bin/activate custom_env\"]",
          "158:     elif env_manager == em.VIRTUALENV:",
          "159:         env_activate_cmd = _get_or_create_virtualenv(model_path)",
          "160:         path = env_activate_cmd.split(\" \")[-1]",
          "161:         os.symlink(path, \"/opt/activate\")",
          "162:         activate_cmd = [env_activate_cmd]",
          "164:     return activate_cmd",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "145:     conf = model.flavors[pyfunc.FLAVOR_NAME]",
          "146:     bash_cmds = []",
          "147:     if pyfunc.ENV in conf:",
          "148:         if not disable_env_creation:",
          "149:             _install_pyfunc_deps(",
          "150:                 MODEL_PATH,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "176:         # NB: MLFLOW_DISABLE_ENV_CREATION is False only for SageMaker deployment, where the model",
          "177:         # files are loaded into the container at runtime rather than build time. In this case,",
          "178:         # we need to create a virtual environment and install the model dependencies into it when",
          "179:         # starting the container.",
          "",
          "---------------"
        ],
        "mlflow/models/docker_utils.py||mlflow/models/docker_utils.py": [
          "File: mlflow/models/docker_utils.py -> mlflow/models/docker_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2: import os",
          "3: from subprocess import PIPE, STDOUT, Popen",
          "4: from urllib.parse import urlparse",
          "6: from mlflow.utils import env_manager as em",
          "",
          "[Removed Lines]",
          "1: import logging",
          "",
          "[Added Lines]",
          "3: from typing import Optional, Union",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "8: from mlflow.utils.logging_utils import eprint",
          "9: from mlflow.version import VERSION",
          "25:     libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm \\",
          "26:     libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev",
          "27: RUN git clone \\",
          "",
          "[Removed Lines]",
          "11: _logger = logging.getLogger(__name__)",
          "13: SETUP_MINICONDA = \"\"\"",
          "14: # Setup miniconda",
          "15: RUN curl -L https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh >> miniconda.sh",
          "16: RUN bash ./miniconda.sh -b -p /miniconda && rm ./miniconda.sh",
          "17: ENV PATH=\"/miniconda/bin:$PATH\"",
          "18: \"\"\"",
          "20: SETUP_PYENV_AND_VIRTUALENV = r\"\"\"",
          "21: # Setup pyenv",
          "22: RUN apt -y update",
          "23: RUN DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC apt-get -y install tzdata",
          "24: RUN apt-get install -y \\",
          "",
          "[Added Lines]",
          "11: UBUNTU_BASE_IMAGE = \"ubuntu:20.04\"",
          "12: PYTHON_SLIM_BASE_IMAGE = \"python:{version}-slim\"",
          "15: SETUP_PYENV_AND_VIRTUALENV = r\"\"\"# Setup pyenv",
          "16: RUN DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC apt-get -y install tzdata \\",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "30:     https://github.com/pyenv/pyenv.git /root/.pyenv",
          "31: ENV PYENV_ROOT=\"/root/.pyenv\"",
          "32: ENV PATH=\"$PYENV_ROOT/bin:$PATH\"",
          "37: RUN pip install virtualenv",
          "38: \"\"\"",
          "101: WORKDIR /opt/mlflow",
          "103: {install_mlflow}",
          "107: ENV MLFLOW_DISABLE_ENV_CREATION={disable_env_creation}",
          "108: ENV ENABLE_MLSERVER={enable_mlserver}",
          "111: # granting read/write access and conditional execution authority to all child directories",
          "112: # and files to allow for deployment to AWS Sagemaker Serverless Endpoints",
          "113: # (see https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html)",
          "114: RUN chmod o+rwX /opt/mlflow/",
          "117: \"\"\"",
          "121:     \"\"\"",
          "124:     \"\"\"",
          "127:     maven_proxy = _get_maven_proxy()",
          "128:     if mlflow_home:",
          "130:         return (",
          "133:             \"RUN cd /opt/mlflow/mlflow/java/scoring && \"",
          "134:             f\"mvn --batch-mode package -DskipTests {maven_proxy} && \"",
          "135:             \"mkdir -p /opt/java/jars && \"",
          "",
          "[Removed Lines]",
          "33: RUN apt install -y python3.8 python3.8-distutils",
          "34: RUN ln -s -f $(which python3.8) /usr/bin/python",
          "35: RUN wget https://bootstrap.pypa.io/get-pip.py -O /tmp/get-pip.py",
          "36: RUN python /tmp/get-pip.py",
          "41: def _get_maven_proxy():",
          "42:     http_proxy = os.getenv(\"http_proxy\")",
          "43:     https_proxy = os.getenv(\"https_proxy\")",
          "44:     if not http_proxy or not https_proxy:",
          "45:         return \"\"",
          "47:     # Expects proxies as either PROTOCOL://{USER}:{PASSWORD}@HOSTNAME:PORT",
          "48:     # or PROTOCOL://HOSTNAME:PORT",
          "49:     parsed_http_proxy = urlparse(http_proxy)",
          "50:     assert parsed_http_proxy.hostname is not None, \"Invalid `http_proxy` hostname.\"",
          "51:     assert parsed_http_proxy.port is not None, f\"Invalid proxy port: {parsed_http_proxy.port}\"",
          "53:     parsed_https_proxy = urlparse(https_proxy)",
          "54:     assert parsed_https_proxy.hostname is not None, \"Invalid `https_proxy` hostname.\"",
          "55:     assert parsed_https_proxy.port is not None, f\"Invalid proxy port: {parsed_https_proxy.port}\"",
          "57:     maven_proxy_options = (",
          "58:         \"-DproxySet=true\",",
          "59:         f\"-Dhttp.proxyHost={parsed_http_proxy.hostname}\",",
          "60:         f\"-Dhttp.proxyPort={parsed_http_proxy.port}\",",
          "61:         f\"-Dhttps.proxyHost={parsed_https_proxy.hostname}\",",
          "62:         f\"-Dhttps.proxyPort={parsed_https_proxy.port}\",",
          "63:         \"-Dhttps.nonProxyHosts=repo.maven.apache.org\",",
          "64:     )",
          "66:     if parsed_http_proxy.username is None or parsed_http_proxy.password is None:",
          "67:         return \" \".join(maven_proxy_options)",
          "69:     return \" \".join(",
          "70:         (",
          "72:             f\"-Dhttp.proxyUser={parsed_http_proxy.username}\",",
          "73:             f\"-Dhttp.proxyPassword={parsed_http_proxy.password}\",",
          "74:         )",
          "75:     )",
          "78: _DOCKERFILE_TEMPLATE = \"\"\"",
          "79: # Build an image that can serve mlflow models.",
          "80: FROM ubuntu:20.04",
          "82: RUN apt-get -y update",
          "83: RUN DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC apt-get install -y --no-install-recommends \\",
          "84:          wget \\",
          "85:          curl \\",
          "86:          nginx \\",
          "87:          ca-certificates \\",
          "88:          bzip2 \\",
          "89:          build-essential \\",
          "90:          cmake \\",
          "91:          openjdk-8-jdk \\",
          "92:          git-core \\",
          "93:          maven \\",
          "94:     && rm -rf /var/lib/apt/lists/*",
          "96: {setup_python_env}",
          "98: ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64",
          "99: ENV GUNICORN_CMD_ARGS=\"--timeout 60 -k gevent\"",
          "100: # Set up the program in the image",
          "105: {custom_setup_steps}",
          "116: {entrypoint}",
          "120: def _get_mlflow_install_step(dockerfile_context_dir, mlflow_home):",
          "122:     Get docker build commands for installing MLflow given a Docker context dir and optional source",
          "123:     directory",
          "125:     mlflow_home = os.path.abspath(mlflow_home) if mlflow_home else None",
          "129:         mlflow_dir = _copy_project(src_path=mlflow_home, dst_path=dockerfile_context_dir)",
          "131:             f\"COPY {mlflow_dir} /opt/mlflow\\n\"",
          "132:             \"RUN pip install /opt/mlflow\\n\"",
          "",
          "[Added Lines]",
          "25: RUN apt install -y python3.8 python3.8-distutils \\",
          "26:     && ln -s -f $(which python3.8) /usr/bin/python \\",
          "27:     && wget https://bootstrap.pypa.io/get-pip.py -O /tmp/get-pip.py \\",
          "28:     && python /tmp/get-pip.py",
          "32: _DOCKERFILE_TEMPLATE = \"\"\"# Build an image that can serve mlflow models.",
          "33: FROM {base_image}",
          "35: {setup_python_venv}",
          "37: {setup_java}",
          "43: {install_model_and_deps}",
          "47: ENV GUNICORN_CMD_ARGS=\"--timeout 60 -k gevent\"",
          "54: # clean up apt cache to reduce image size",
          "55: RUN rm -rf /var/lib/apt/lists/*",
          "57: ENTRYPOINT [\"python\", \"-c\", \"{entrypoint}\"]",
          "61: SETUP_MINICONDA = \"\"\"# Setup miniconda",
          "62: RUN curl -L https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh >> miniconda.sh",
          "63: RUN bash ./miniconda.sh -b -p /miniconda && rm ./miniconda.sh",
          "64: ENV PATH=\"/miniconda/bin:$PATH\"",
          "65: \"\"\"",
          "68: def generate_dockerfile(",
          "69:     output_dir: str,",
          "70:     base_image: str,",
          "71:     model_install_steps: Optional[str],",
          "72:     entrypoint: str,",
          "73:     env_manager: Union[em.CONDA, em.LOCAL, em.VIRTUALENV],",
          "74:     mlflow_home: Optional[str] = None,",
          "75:     enable_mlserver: bool = False,",
          "76:     disable_env_creation_at_runtime: bool = True,",
          "77: ):",
          "79:     Generates a Dockerfile that can be used to build a docker image, that serves ML model",
          "80:     stored and tracked in MLflow.",
          "82:     if base_image.startswith(\"python:\"):",
          "83:         setup_python_venv_steps = (",
          "84:             \"RUN apt-get -y update && apt-get install -y --no-install-recommends nginx\"",
          "85:         )",
          "86:         setup_java_steps = \"\"",
          "87:         install_mlflow_steps = _pip_mlflow_install_step(output_dir, mlflow_home)",
          "89:     elif base_image == UBUNTU_BASE_IMAGE:",
          "90:         setup_python_venv_steps = (",
          "91:             \"RUN apt-get -y update && DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC apt-get install -y \"",
          "92:             \"--no-install-recommends wget curl nginx ca-certificates bzip2 build-essential cmake \"",
          "93:             \"git-core\\n\\n\"",
          "94:         )",
          "95:         setup_python_venv_steps += (",
          "96:             SETUP_MINICONDA if env_manager == em.CONDA else SETUP_PYENV_AND_VIRTUALENV",
          "97:         )",
          "99:         setup_java_steps = (",
          "100:             \"# Setup Java\\n\"",
          "101:             \"RUN apt-get install -y --no-install-recommends openjdk-8-jdk maven\\n\"",
          "102:             \"ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\"",
          "103:         )",
          "105:         install_mlflow_steps = _pip_mlflow_install_step(output_dir, mlflow_home)",
          "106:         install_mlflow_steps += \"\\n\\n\" + _java_mlflow_install_step(mlflow_home)",
          "108:     else:",
          "109:         raise ValueError(f\"Unsupported base image: {base_image}\")",
          "111:     with open(os.path.join(output_dir, \"Dockerfile\"), \"w\") as f:",
          "112:         f.write(",
          "113:             _DOCKERFILE_TEMPLATE.format(",
          "114:                 base_image=base_image,",
          "115:                 setup_python_venv=setup_python_venv_steps,",
          "116:                 setup_java=setup_java_steps,",
          "117:                 install_mlflow=install_mlflow_steps,",
          "118:                 install_model_and_deps=model_install_steps,",
          "119:                 entrypoint=entrypoint,",
          "120:                 enable_mlserver=enable_mlserver,",
          "121:                 disable_env_creation=disable_env_creation_at_runtime,",
          "122:             )",
          "123:         )",
          "126: def _java_mlflow_install_step(mlflow_home):",
          "130:             \"# Install Java mlflow-scoring from local source\\n\"",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "138:         )",
          "139:     else:",
          "140:         return (",
          "142:             \"RUN mvn\"",
          "143:             \" --batch-mode dependency:copy\"",
          "144:             f\" -Dartifact=org.mlflow:mlflow-scoring:{VERSION}:pom\"",
          "",
          "[Removed Lines]",
          "141:             f\"RUN pip install mlflow=={VERSION}\\n\"",
          "",
          "[Added Lines]",
          "139:             \"# Install Java mlflow-scoring from Maven Central\\n\"",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "154:         )",
          "166:     \"\"\"",
          "169:     \"\"\"",
          "184:         )",
          "187: def build_image_from_context(context_dir: str, image_name: str):",
          "",
          "[Removed Lines]",
          "157: def generate_dockerfile(",
          "158:     output_dir,",
          "159:     entrypoint,",
          "160:     env_manager,",
          "161:     mlflow_home=None,",
          "162:     custom_setup_steps=None,",
          "163:     enable_mlserver=False,",
          "164:     disable_env_creation=False,",
          "165: ):",
          "167:     Generates a Dockerfile that can be used to build a docker image, that serves ML model",
          "168:     stored and tracked in MLflow.",
          "170:     install_mlflow_steps = _get_mlflow_install_step(output_dir, mlflow_home)",
          "171:     setup_python_env_steps = (",
          "172:         SETUP_MINICONDA if env_manager == em.CONDA else SETUP_PYENV_AND_VIRTUALENV",
          "173:     )",
          "174:     with open(os.path.join(output_dir, \"Dockerfile\"), \"w\") as f:",
          "175:         f.write(",
          "176:             _DOCKERFILE_TEMPLATE.format(",
          "177:                 setup_python_env=setup_python_env_steps,",
          "178:                 install_mlflow=install_mlflow_steps,",
          "179:                 custom_setup_steps=custom_setup_steps,",
          "180:                 entrypoint=entrypoint,",
          "181:                 enable_mlserver=enable_mlserver,",
          "182:                 disable_env_creation=disable_env_creation,",
          "183:             )",
          "",
          "[Added Lines]",
          "155: def _get_maven_proxy():",
          "156:     http_proxy = os.getenv(\"http_proxy\")",
          "157:     https_proxy = os.getenv(\"https_proxy\")",
          "158:     if not http_proxy or not https_proxy:",
          "159:         return \"\"",
          "161:     # Expects proxies as either PROTOCOL://{USER}:{PASSWORD}@HOSTNAME:PORT",
          "162:     # or PROTOCOL://HOSTNAME:PORT",
          "163:     parsed_http_proxy = urlparse(http_proxy)",
          "164:     assert parsed_http_proxy.hostname is not None, \"Invalid `http_proxy` hostname.\"",
          "165:     assert parsed_http_proxy.port is not None, f\"Invalid proxy port: {parsed_http_proxy.port}\"",
          "167:     parsed_https_proxy = urlparse(https_proxy)",
          "168:     assert parsed_https_proxy.hostname is not None, \"Invalid `https_proxy` hostname.\"",
          "169:     assert parsed_https_proxy.port is not None, f\"Invalid proxy port: {parsed_https_proxy.port}\"",
          "171:     maven_proxy_options = (",
          "172:         \"-DproxySet=true\",",
          "173:         f\"-Dhttp.proxyHost={parsed_http_proxy.hostname}\",",
          "174:         f\"-Dhttp.proxyPort={parsed_http_proxy.port}\",",
          "175:         f\"-Dhttps.proxyHost={parsed_https_proxy.hostname}\",",
          "176:         f\"-Dhttps.proxyPort={parsed_https_proxy.port}\",",
          "177:         \"-Dhttps.nonProxyHosts=repo.maven.apache.org\",",
          "178:     )",
          "180:     if parsed_http_proxy.username is None or parsed_http_proxy.password is None:",
          "181:         return \" \".join(maven_proxy_options)",
          "183:     return \" \".join(",
          "184:         (",
          "186:             f\"-Dhttp.proxyUser={parsed_http_proxy.username}\",",
          "187:             f\"-Dhttp.proxyPassword={parsed_http_proxy.password}\",",
          "188:         )",
          "189:     )",
          "192: def _pip_mlflow_install_step(dockerfile_context_dir, mlflow_home):",
          "194:     Get docker build commands for installing MLflow given a Docker context dir and optional source",
          "195:     directory",
          "197:     if mlflow_home:",
          "198:         mlflow_dir = _copy_project(",
          "199:             src_path=os.path.abspath(mlflow_home), dst_path=dockerfile_context_dir",
          "201:         return (",
          "202:             \"# Install MLflow from local source\\n\"",
          "203:             f\"COPY {mlflow_dir} /opt/mlflow\\n\"",
          "204:             \"RUN pip install /opt/mlflow\"",
          "205:         )",
          "206:     else:",
          "207:         return f\"# Install MLflow\\nRUN pip install mlflow=={VERSION}\"",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "203:         \".\",",
          "204:     ]",
          "206:     for x in iter(proc.stdout.readline, \"\"):",
          "207:         eprint(x, end=\"\")",
          "",
          "[Removed Lines]",
          "205:     proc = Popen(commands, cwd=context_dir, stdout=PIPE, stderr=STDOUT, text=True)",
          "",
          "[Added Lines]",
          "228:     proc = Popen(commands, cwd=context_dir, stdout=PIPE, stderr=STDOUT, text=True, encoding=\"utf-8\")",
          "",
          "---------------"
        ],
        "mlflow/models/python_api.py||mlflow/models/python_api.py": [
          "File: mlflow/models/python_api.py -> mlflow/models/python_api.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19:     name=\"mlflow-pyfunc\",",
          "20:     env_manager=_EnvManager.VIRTUALENV,",
          "21:     mlflow_home=None,",
          "22:     install_mlflow=False,",
          "23:     enable_mlserver=False,",
          "24: ):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22:     install_java=False,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "48:         model_uri,",
          "49:         name,",
          "50:         mlflow_home=mlflow_home,",
          "51:         install_mlflow=install_mlflow,",
          "52:         enable_mlserver=enable_mlserver,",
          "53:     )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "52:         install_java=install_java,",
          "",
          "---------------"
        ],
        "mlflow/pyfunc/backend.py||mlflow/pyfunc/backend.py": [
          "File: mlflow/pyfunc/backend.py -> mlflow/pyfunc/backend.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "10: import warnings",
          "11: from pathlib import Path",
          "13: from mlflow.exceptions import MlflowException",
          "14: from mlflow.models import FlavorBackend, docker_utils",
          "15: from mlflow.pyfunc import (",
          "16:     ENV,",
          "17:     _extract_conda_env,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "13: from mlflow import pyfunc",
          "16: from mlflow.models.docker_utils import PYTHON_SLIM_BASE_IMAGE, UBUNTU_BASE_IMAGE",
          "17: from mlflow.models.model import MLMODEL_FILE_NAME, Model",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "22: from mlflow.tracking.artifact_utils import _download_artifact_from_uri",
          "23: from mlflow.utils import env_manager as _EnvManager",
          "24: from mlflow.utils.conda import get_conda_bin_executable, get_or_create_conda_env",
          "26: from mlflow.utils.file_utils import (",
          "27:     TempDir,",
          "28:     get_or_create_nfs_tmp_dir,",
          "29:     get_or_create_tmp_dir,",
          "30:     path_to_local_file_uri,",
          "31: )",
          "32: from mlflow.utils.nfs_on_spark import get_nfs_cache_root_dir",
          "33: from mlflow.utils.process import ShellCommandException, cache_return_value_per_process",
          "34: from mlflow.utils.virtualenv import (",
          "",
          "[Removed Lines]",
          "25: from mlflow.utils.environment import Environment",
          "",
          "[Added Lines]",
          "28: from mlflow.utils.environment import Environment, _PythonEnv",
          "35: from mlflow.utils.model_utils import _get_all_flavor_configurations",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "42: _IS_UNIX = os.name != \"nt\"",
          "43: _STDIN_SERVER_SCRIPT = Path(__file__).parent.joinpath(\"stdin_server.py\")",
          "46: class PyFuncBackend(FlavorBackend):",
          "47:     \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "49: # Flavors that require Java to be installed in the environment",
          "50: JAVA_FLAVORS = {\"johnsnowlabs\", \"h2o\", \"mleap\", \"spark\"}",
          "52: # Some flavor requires additional packages to be installed in the environment",
          "53: FLAVOR_SPECIFIC_APT_PACKAGES = {",
          "54:     \"lightgbm\": [\"libgomp1\"],",
          "55:     \"paddle\": [\"libgomp1\"],",
          "56: }",
          "58: # Directory to store loaded model inside the Docker context directory",
          "59: _MODEL_DIR_NAME = \"model_dir\"",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "318:             # Can not find conda",
          "319:             return False",
          "321:     def generate_dockerfile(",
          "323:     ):",
          "324:         os.makedirs(output_dir, exist_ok=True)",
          "325:         _logger.debug(\"Created all folders in path\", extra={\"output_directory\": output_dir})",
          "336:         dockerfile_text = docker_utils.generate_dockerfile(",
          "337:             output_dir=output_dir,",
          "340:             env_manager=self._env_manager,",
          "341:             mlflow_home=mlflow_home,",
          "342:             enable_mlserver=enable_mlserver,",
          "344:         )",
          "345:         _logger.debug(\"generated dockerfile at {output_dir}\", extra={\"dockerfile\": dockerfile_text})",
          "382:         return (",
          "386:         )",
          "",
          "[Removed Lines]",
          "322:         self, model_uri, output_dir, install_mlflow=False, mlflow_home=None, enable_mlserver=False",
          "327:         # Copy model to image if model_uri is specified",
          "328:         custom_setup_steps = (",
          "329:             self._get_copy_model_steps(output_dir, model_uri, install_mlflow, enable_mlserver)",
          "330:             if model_uri",
          "331:             else \"\"",
          "332:         )",
          "334:         pyfunc_entrypoint = self._pyfunc_entrypoint(model_uri, install_mlflow, enable_mlserver)",
          "338:             custom_setup_steps=custom_setup_steps,",
          "339:             entrypoint=pyfunc_entrypoint,",
          "343:             disable_env_creation=True,  # Always disable env creation for pyfunc",
          "347:     def build_image(",
          "348:         self, model_uri, image_name, install_mlflow=False, mlflow_home=None, enable_mlserver=False",
          "349:     ):",
          "350:         with TempDir() as tmp:",
          "351:             cwd = tmp.path()",
          "352:             self.generate_dockerfile(model_uri, cwd, install_mlflow, mlflow_home, enable_mlserver)",
          "354:             _logger.info(\"Building docker image with name %s\", image_name)",
          "355:             docker_utils.build_image_from_context(context_dir=cwd, image_name=image_name)",
          "357:     def _get_copy_model_steps(self, output_dir, model_uri, install_mlflow, enable_mlserver):",
          "358:         model_cwd = os.path.join(output_dir, \"model_dir\")",
          "359:         pathlib.Path(model_cwd).mkdir(parents=True, exist_ok=True)",
          "361:         # If model_uri is specified, copy the model to the image and install its dependencies",
          "362:         model_path = _download_artifact_from_uri(model_uri, output_path=model_cwd)",
          "363:         model_dir = str(posixpath.join(\"model_dir\", os.path.basename(model_path)))",
          "365:         install_deps_cmd = self._get_install_pyfunc_deps_cmd(install_mlflow, enable_mlserver)",
          "366:         return f'COPY {model_dir} /opt/ml/model\\nRUN python -c \"{install_deps_cmd}\"'",
          "368:     def _pyfunc_entrypoint(self, model_uri, install_mlflow, enable_mlserver):",
          "369:         if model_uri:",
          "370:             # If model_uri is specified, dependencies are installed at build time so we don't",
          "371:             # need to run the install command at runtime",
          "372:             install_deps_cmd = \"\"",
          "373:         else:",
          "374:             install_deps_cmd = self._get_install_pyfunc_deps_cmd(install_mlflow, enable_mlserver)",
          "375:         entrypoint = (",
          "376:             f\"from mlflow.models import container as C;{install_deps_cmd} \"",
          "377:             f\"C._serve('{self._env_manager}')\"",
          "378:         )",
          "379:         return f'ENTRYPOINT [\"python\", \"-c\", \"{entrypoint}\"]'",
          "381:     def _get_install_pyfunc_deps_cmd(self, install_mlflow, enable_mlserver):",
          "383:             \"from mlflow.models.container import _install_pyfunc_deps; \"",
          "384:             f\"_install_pyfunc_deps('/opt/ml/model', install_mlflow={install_mlflow}, \"",
          "385:             f\"enable_mlserver={enable_mlserver}, env_manager='{self._env_manager}');\"",
          "",
          "[Added Lines]",
          "337:     def build_image(",
          "338:         self,",
          "339:         model_uri,",
          "340:         image_name,",
          "341:         install_java=False,",
          "342:         install_mlflow=False,",
          "343:         mlflow_home=None,",
          "344:         enable_mlserver=False,",
          "345:     ):",
          "346:         with TempDir() as tmp:",
          "347:             cwd = tmp.path()",
          "348:             self.generate_dockerfile(",
          "349:                 model_uri=model_uri,",
          "350:                 output_dir=cwd,",
          "351:                 install_java=install_java,",
          "352:                 install_mlflow=install_mlflow,",
          "353:                 mlflow_home=mlflow_home,",
          "354:                 enable_mlserver=enable_mlserver,",
          "355:             )",
          "357:             _logger.info(\"Building docker image with name %s\", image_name)",
          "358:             docker_utils.build_image_from_context(context_dir=cwd, image_name=image_name)",
          "361:         self,",
          "362:         model_uri,",
          "363:         output_dir,",
          "364:         install_java=False,",
          "365:         install_mlflow=False,",
          "366:         mlflow_home=None,",
          "367:         enable_mlserver=False,",
          "372:         if model_uri:",
          "373:             model_cwd = os.path.join(output_dir, _MODEL_DIR_NAME)",
          "374:             pathlib.Path(model_cwd).mkdir(parents=True, exist_ok=True)",
          "375:             model_path = _download_artifact_from_uri(model_uri, output_path=model_cwd)",
          "376:             base_image = self._get_base_image(model_path, install_java)",
          "378:             # We don't need virtualenv or conda if base image is python",
          "379:             env_manager = (",
          "380:                 _EnvManager.LOCAL if base_image.startswith(\"python\") else self._env_manager",
          "381:             )",
          "383:             model_install_steps = self._model_installation_steps(",
          "384:                 model_path, env_manager, install_mlflow, enable_mlserver",
          "385:             )",
          "386:             entrypoint = f\"from mlflow.models import container as C; C._serve('{env_manager}')\"",
          "388:         else:",
          "389:             base_image = UBUNTU_BASE_IMAGE",
          "390:             model_install_steps = \"\"",
          "391:             # If model_uri is not specified, dependencies are installed at runtime",
          "392:             entrypoint = (",
          "393:                 self._get_install_pyfunc_deps_cmd(",
          "394:                     self._env_manager, install_mlflow, enable_mlserver",
          "395:                 )",
          "396:                 + f\" C._serve('{self._env_manager}')\"",
          "397:             )",
          "401:             base_image=base_image,",
          "402:             model_install_steps=model_install_steps,",
          "403:             entrypoint=entrypoint,",
          "407:             # always disable env creation at runtime for pyfunc",
          "408:             disable_env_creation_at_runtime=True,",
          "412:     def _get_base_image(self, model_path: str, install_java: bool) -> str:",
          "413:         \"\"\"",
          "414:         Determine the base image to use for the Dockerfile.",
          "416:         We use Python slim base image when all of the following conditions are met:",
          "417:           1. Model URI is specified by the user",
          "418:           2. Model flavor does not require Java",
          "419:           3. Python version is specified in the model",
          "421:         Returns:",
          "422:             Either the Ubuntu base image or the Python slim base image.",
          "423:         \"\"\"",
          "424:         # Check if the model requires Java",
          "425:         if not install_java:",
          "426:             flavors = _get_all_flavor_configurations(model_path).keys()",
          "427:             if java_flavors := JAVA_FLAVORS & flavors:",
          "428:                 _logger.info(f\"Detected java flavors {java_flavors}, installing Java in the image\")",
          "429:                 install_java = True",
          "431:         # Use ubuntu base image if Java is required",
          "432:         if install_java:",
          "433:             return UBUNTU_BASE_IMAGE",
          "435:         # Get Python version from MLmodel",
          "436:         try:",
          "437:             model_config_path = os.path.join(model_path, MLMODEL_FILE_NAME)",
          "438:             model = Model.load(model_config_path)",
          "440:             conf = model.flavors[pyfunc.FLAVOR_NAME]",
          "441:             env_conf = conf[pyfunc.ENV]",
          "442:             python_env_config_path = os.path.join(model_path, env_conf[_EnvManager.VIRTUALENV])",
          "444:             python_env = _PythonEnv.from_yaml(python_env_config_path)",
          "445:             return PYTHON_SLIM_BASE_IMAGE.format(version=python_env.python)",
          "446:         except Exception as e:",
          "447:             _logger.warning(",
          "448:                 f\"Failed to determine Python version from {model_config_path}. \"",
          "449:                 f\"Defaulting to {UBUNTU_BASE_IMAGE}. Error: {e}\"",
          "450:             )",
          "451:             return UBUNTU_BASE_IMAGE",
          "453:     def _model_installation_steps(self, model_path, env_manager, install_mlflow, enable_mlserver):",
          "454:         model_dir = str(posixpath.join(_MODEL_DIR_NAME, os.path.basename(model_path)))",
          "455:         # Copy model to image if model_uri is specified",
          "456:         steps = (",
          "457:             \"# Copy model to image and install dependencies\\n\"",
          "458:             f\"COPY {model_dir} /opt/ml/model\\nRUN python -c \"",
          "459:         )",
          "460:         steps += (",
          "461:             f'\"{self._get_install_pyfunc_deps_cmd(env_manager, install_mlflow, enable_mlserver)}\"'",
          "462:         )",
          "464:         # Install flavor-specific dependencies if needed",
          "465:         flavors = _get_all_flavor_configurations(model_path).keys()",
          "466:         for flavor in flavors:",
          "467:             if flavor in FLAVOR_SPECIFIC_APT_PACKAGES:",
          "468:                 packages = \" \".join(FLAVOR_SPECIFIC_APT_PACKAGES[flavor])",
          "469:                 steps += f\"\\nRUN apt-get install -y --no-install-recommends {packages}\"",
          "471:         return steps",
          "473:     def _get_install_pyfunc_deps_cmd(",
          "474:         self, env_manager: _EnvManager, install_mlflow: bool, enable_mlserver: bool",
          "475:     ):",
          "477:             \"from mlflow.models import container as C; \"",
          "478:             f\"C._install_pyfunc_deps('/opt/ml/model', install_mlflow={install_mlflow}, \"",
          "479:             f\"enable_mlserver={enable_mlserver}, env_manager='{env_manager}');\"",
          "",
          "---------------"
        ],
        "mlflow/sagemaker/cli.py||mlflow/sagemaker/cli.py": [
          "File: mlflow/sagemaker/cli.py -> mlflow/sagemaker/cli.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "358:         click.echo(\"skipping both build and push, have nothing to do!\")",
          "359:     if build:",
          "360:         sagemaker_image_entrypoint = (",
          "363:         )",
          "365:         setup_container = (",
          "366:             'RUN python -c \"from mlflow.models.container import _install_pyfunc_deps;'",
          "367:             '_install_pyfunc_deps(None, False)\"'",
          "368:         )",
          "370:         with tempfile.TemporaryDirectory() as tmp:",
          "371:             docker_utils.generate_dockerfile(",
          "372:                 output_dir=tmp,",
          "373:                 entrypoint=sagemaker_image_entrypoint,",
          "374:                 env_manager=env_manager,",
          "375:                 mlflow_home=os.path.abspath(mlflow_home) if mlflow_home else None,",
          "377:             )",
          "379:             docker_utils.build_image_from_context(tmp, image_name=container)",
          "",
          "[Removed Lines]",
          "361:             'ENTRYPOINT [\"python\", \"-c\", \"import sys; from mlflow.models import container as C; '",
          "362:             f\"C._init(sys.argv[1], '{env_manager}')\\\"]\"",
          "376:                 custom_setup_steps=setup_container,",
          "",
          "[Added Lines]",
          "361:             \"import sys; from mlflow.models import container as C; \"",
          "362:             f\"C._init(sys.argv[1], '{env_manager}')\"",
          "366:             \"# Install minimal serving dependencies\\n\"",
          "373:                 base_image=mlflow.models.docker_utils.UBUNTU_BASE_IMAGE,",
          "378:                 model_install_steps=setup_container,",
          "379:                 # Create a conda env or virtualenv at runtime after the model is loaded",
          "380:                 disable_env_creation_at_runtime=False,",
          "",
          "---------------"
        ],
        "mlflow/utils/cli_args.py||mlflow/utils/cli_args.py": [
          "File: mlflow/utils/cli_args.py -> mlflow/utils/cli_args.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "212:     is_flag=True,",
          "213:     help=\"If specified, use local environment.\",",
          "214: )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "216: INSTALL_JAVA = click.option(",
          "217:     \"--install-java\",",
          "218:     is_flag=True,",
          "219:     help=\"Install Java in the image. Default is False in order to reduce both the \"",
          "220:     \"image size and the build time. Model flavors requiring Java will enable this \"",
          "221:     \"setting automatically, such as the Spark flavor.\",",
          "222: )",
          "",
          "---------------"
        ],
        "mlflow/utils/model_utils.py||mlflow/utils/model_utils.py": [
          "File: mlflow/utils/model_utils.py -> mlflow/utils/model_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: FLAVOR_CONFIG_CODE = \"code\"",
          "30:     Args:",
          "31:         model_path: The path to the root directory of the MLflow model for which to load",
          "32:             the specified flavor configuration.",
          "35:     Returns:",
          "38:     \"\"\"",
          "39:     model_configuration_path = os.path.join(model_path, MLMODEL_FILE_NAME)",
          "",
          "[Removed Lines]",
          "25: def _get_flavor_configuration(model_path, flavor_name):",
          "26:     \"\"\"Obtains the configuration for the specified flavor from the specified",
          "27:     MLflow model path. If the model does not contain the specified flavor,",
          "28:     an exception will be thrown.",
          "33:         flavor_name: The name of the flavor configuration to load.",
          "36:         The flavor configuration as a dictionary.",
          "",
          "[Added Lines]",
          "25: def _get_all_flavor_configurations(model_path):",
          "26:     \"\"\"Obtains all the flavor configurations from the specified MLflow model path.",
          "33:         The dictionary contains all flavor configurations with flavor name as key.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "44:         )",
          "46:     model_conf = Model.load(model_configuration_path)",
          "48:         raise MlflowException(",
          "49:             f'Model does not have the \"{flavor_name}\" flavor',",
          "50:             RESOURCE_DOES_NOT_EXIST,",
          "51:         )",
          "55: def _get_flavor_configuration_from_uri(model_uri, flavor_name, logger):",
          "",
          "[Removed Lines]",
          "47:     if flavor_name not in model_conf.flavors:",
          "52:     return model_conf.flavors[flavor_name]",
          "",
          "[Added Lines]",
          "44:     return model_conf.flavors",
          "47: def _get_flavor_configuration(model_path, flavor_name):",
          "48:     \"\"\"Obtains the configuration for the specified flavor from the specified",
          "49:     MLflow model path. If the model does not contain the specified flavor,",
          "50:     an exception will be thrown.",
          "52:     Args:",
          "53:         model_path: The path to the root directory of the MLflow model for which to load",
          "54:             the specified flavor configuration.",
          "55:         flavor_name: The name of the flavor configuration to load.",
          "57:     Returns:",
          "58:         The flavor configuration as a dictionary.",
          "60:     \"\"\"",
          "61:     flavors = _get_all_flavor_configurations(model_path)",
          "62:     if flavor_name not in flavors:",
          "67:     return flavors[flavor_name]",
          "",
          "---------------"
        ],
        "tests/diviner/test_diviner_model_export.py||tests/diviner/test_diviner_model_export.py": [
          "File: tests/diviner/test_diviner_model_export.py -> tests/diviner/test_diviner_model_export.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "79:     return conda_env",
          "82: def test_diviner_native_save_and_load(grouped_prophet, model_path):",
          "83:     mlflow.diviner.save_model(diviner_model=grouped_prophet, path=model_path)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "82: @pytest.fixture",
          "83: def diviner_groups(diviner_data):",
          "84:     groups = []",
          "85:     for i in [0, -1]:",
          "86:         key_entries = []",
          "87:         for value in diviner_data.df[diviner_data.key_columns].iloc[[i]].to_dict().values():",
          "88:             key_entries.append(list(value.values())[0])",
          "89:         groups.append(tuple(key_entries))",
          "90:     return groups",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "137:         loaded_pyfunc_model.predict(bad_conf)",
          "148:     mlflow.diviner.save_model(diviner_model=grouped_prophet, path=model_path)",
          "149:     loaded_pyfunc_model = pyfunc.load_pyfunc(model_uri=model_path)",
          "153:     pyfunc_group_predict = loaded_pyfunc_model.predict(pyfunc_conf)",
          "155:     pd.testing.assert_frame_equal(local_group_pred, pyfunc_group_predict)",
          "166:     mlflow.diviner.save_model(diviner_model=grouped_pmdarima, path=model_path)",
          "167:     loaded_pyfunc_model = pyfunc.load_pyfunc(model_uri=model_path)",
          "169:     local_group_pred = grouped_pmdarima.predict_groups(",
          "171:         n_periods=10,",
          "172:         predict_col=\"prediction\",",
          "173:         alpha=0.1,",
          "",
          "[Removed Lines]",
          "140: def test_diviner_pyfunc_group_predict_prophet(grouped_prophet, model_path, diviner_data):",
          "141:     groups = []",
          "142:     for i in [0, -1]:",
          "143:         key_entries = []",
          "144:         for value in diviner_data.df[diviner_data.key_columns].iloc[[i]].to_dict().values():",
          "145:             key_entries.append(list(value.values())[0])",
          "146:         groups.append(tuple(key_entries))",
          "151:     local_group_pred = grouped_prophet.predict_groups(groups=groups, horizon=10, frequency=\"D\")",
          "152:     pyfunc_conf = pd.DataFrame({\"groups\": [groups], \"horizon\": 10, \"frequency\": \"D\"}, index=[0])",
          "158: def test_diviner_pyfunc_group_predict_pmdarima(grouped_pmdarima, model_path, diviner_data):",
          "159:     groups = []",
          "160:     for i in [0, -1]:",
          "161:         key_entries = []",
          "162:         for value in diviner_data.df[diviner_data.key_columns].iloc[[i]].to_dict().values():",
          "163:             key_entries.append(list(value.values())[0])",
          "164:         groups.append(tuple(key_entries))",
          "170:         groups=groups,",
          "",
          "[Added Lines]",
          "151: def test_diviner_pyfunc_group_predict_prophet(grouped_prophet, model_path, diviner_groups):",
          "155:     local_group_pred = grouped_prophet.predict_groups(",
          "156:         groups=diviner_groups, horizon=10, frequency=\"D\"",
          "157:     )",
          "158:     pyfunc_conf = pd.DataFrame(",
          "159:         {\"groups\": [diviner_groups], \"horizon\": 10, \"frequency\": \"D\"}, index=[0]",
          "160:     )",
          "166: def test_diviner_pyfunc_group_predict_pmdarima(grouped_pmdarima, model_path, diviner_groups):",
          "171:         groups=diviner_groups,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "176:     )",
          "177:     pyfunc_conf = pd.DataFrame(",
          "178:         {",
          "180:             \"n_periods\": 10,",
          "181:             \"predict_col\": \"prediction\",",
          "182:             \"alpha\": 0.1,",
          "",
          "[Removed Lines]",
          "179:             \"groups\": [groups],",
          "",
          "[Added Lines]",
          "180:             \"groups\": [diviner_groups],",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "419:     pd.testing.assert_frame_equal(local_predict, scores)",
          "423:     artifact_path = \"model\"",
          "424:     with mlflow.start_run():",
          "425:         mlflow.diviner.log_model(",
          "",
          "[Removed Lines]",
          "422: def test_pmdarima_pyfunc_serve_and_score_groups(grouped_prophet, diviner_data):",
          "",
          "[Added Lines]",
          "423: def test_pmdarima_pyfunc_serve_and_score_groups(grouped_prophet, diviner_groups):",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "428:         )",
          "429:         model_uri = mlflow.get_artifact_uri(artifact_path)",
          "442:     from mlflow.deployments import PredictionsResponse",
          "",
          "[Removed Lines]",
          "431:     groups = []",
          "432:     for i in [0, -1]:",
          "433:         key_entries = []",
          "434:         for value in diviner_data.df[diviner_data.key_columns].iloc[[i]].to_dict().values():",
          "435:             key_entries.append(list(value.values())[0])",
          "436:         groups.append(tuple(key_entries))",
          "438:     local_predict = grouped_prophet.predict_groups(groups=groups, horizon=10, frequency=\"W\")",
          "440:     inference_data = pd.DataFrame({\"groups\": [groups], \"horizon\": 10, \"frequency\": \"W\"}, index=[0])",
          "",
          "[Added Lines]",
          "432:     local_predict = grouped_prophet.predict_groups(groups=diviner_groups, horizon=10, frequency=\"W\")",
          "434:     inference_data = pd.DataFrame(",
          "435:         {\"groups\": [diviner_groups], \"horizon\": 10, \"frequency\": \"W\"}, index=[0]",
          "436:     )",
          "",
          "---------------"
        ],
        "tests/pyfunc/docker/conftest.py||tests/pyfunc/docker/conftest.py": [
          "File: tests/pyfunc/docker/conftest.py -> tests/pyfunc/docker/conftest.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: import os",
          "2: from functools import lru_cache",
          "4: import pytest",
          "5: import requests",
          "6: from packaging.version import Version",
          "8: import docker",
          "9: import mlflow",
          "11: TEST_IMAGE_NAME = \"test_image\"",
          "12: MLFLOW_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"..\"))",
          "13: RESOURCE_DIR = os.path.join(MLFLOW_ROOT, \"tests\", \"resources\", \"dockerfile\")",
          "15: docker_client = docker.from_env()",
          "18: @pytest.fixture(autouse=True)",
          "19: def clean_up_docker_image():",
          "20:     # Get all containers using the test image",
          "21:     containers = docker_client.containers.list(filters={\"ancestor\": TEST_IMAGE_NAME})",
          "22:     for container in containers:",
          "23:         container.remove(force=True)",
          "25:     # Clean up the image",
          "26:     try:",
          "27:         docker_client.images.remove(TEST_IMAGE_NAME, force=True)",
          "28:     except docker.errors.ImageNotFound:",
          "29:         pass",
          "32: @lru_cache(maxsize=1)",
          "33: def get_released_mlflow_version():",
          "34:     url = \"https://pypi.org/pypi/mlflow/json\"",
          "35:     response = requests.get(url)",
          "36:     response.raise_for_status()",
          "37:     data = response.json()",
          "38:     versions = [",
          "39:         v for v in map(Version, data[\"releases\"]) if not (v.is_devrelease or v.is_prerelease)",
          "40:     ]",
          "41:     return str(sorted(versions, reverse=True)[0])",
          "44: def save_model_with_latest_mlflow_version(flavor, **kwargs):",
          "45:     \"\"\"",
          "46:     Save a model with overriding MLflow version from dev version to the latest released version.",
          "47:     By default a model is saved with the dev version of MLflow, which is not available on PyPI.",
          "48:     Usually we can be workaround this by adding --serve-wheel flag that starts local PyPI server,",
          "49:     however, this doesn't work when installing dependencies inside Docker container. Hence, this",
          "50:     function uses `extra_pip_requirements` to save the model with the latest released MLflow.",
          "51:     \"\"\"",
          "52:     latest_mlflow_version = get_released_mlflow_version()",
          "53:     if flavor == \"langchain\":",
          "54:         kwargs[\"pip_requirements\"] = [f\"mlflow[gateway]=={latest_mlflow_version}\", \"langchain\"]",
          "55:     else:",
          "56:         kwargs[\"extra_pip_requirements\"] = [f\"mlflow=={latest_mlflow_version}\"]",
          "57:     flavor_module = getattr(mlflow, flavor)",
          "58:     flavor_module.save_model(**kwargs)",
          "",
          "---------------"
        ],
        "tests/pyfunc/docker/test_docker.py||tests/pyfunc/docker/test_docker.py": [
          "File: tests/pyfunc/docker/test_docker.py -> tests/pyfunc/docker/test_docker.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: import difflib",
          "2: import os",
          "3: import shutil",
          "4: from dataclasses import dataclass",
          "5: from pathlib import Path",
          "6: from typing import Optional",
          "7: from unittest import mock",
          "9: import pytest",
          "10: import sklearn",
          "11: import sklearn.neighbors",
          "13: import mlflow",
          "14: from mlflow.models import Model",
          "15: from mlflow.models.docker_utils import build_image_from_context",
          "16: from mlflow.models.flavor_backend_registry import get_flavor_backend",
          "17: from mlflow.utils import PYTHON_VERSION",
          "18: from mlflow.utils.env_manager import VIRTUALENV",
          "19: from mlflow.version import VERSION",
          "21: from tests.pyfunc.docker.conftest import RESOURCE_DIR, get_released_mlflow_version",
          "24: def assert_dockerfiles_equal(actual_dockerfile_path: Path, expected_dockerfile_path: Path):",
          "25:     actual_dockerfile = actual_dockerfile_path.read_text().replace(",
          "26:         VERSION, get_released_mlflow_version()",
          "27:     )",
          "28:     expected_dockerfile = (",
          "29:         expected_dockerfile_path.read_text()",
          "30:         .replace(\"${{ MLFLOW_VERSION }}\", get_released_mlflow_version())",
          "31:         .replace(\"${{ PYTHON_VERSION }}\", PYTHON_VERSION)",
          "32:     )",
          "33:     assert (",
          "34:         actual_dockerfile == expected_dockerfile",
          "35:     ), \"Generated Dockerfile does not match expected one. Diff:\\n\" + \"\\n\".join(",
          "36:         difflib.unified_diff(expected_dockerfile.splitlines(), actual_dockerfile.splitlines())",
          "37:     )",
          "40: def save_model(tmp_path):",
          "41:     knn_model = sklearn.neighbors.KNeighborsClassifier()",
          "42:     model_path = os.path.join(tmp_path, \"model\")",
          "43:     mlflow.sklearn.save_model(",
          "44:         knn_model,",
          "45:         path=model_path,",
          "46:         pip_requirements=[",
          "47:             f\"mlflow=={get_released_mlflow_version()}\",",
          "48:             f\"scikit-learn=={sklearn.__version__}\",",
          "49:         ],  # Skip requirements inference for speed up",
          "50:     )",
          "51:     return model_path",
          "54: def add_spark_flavor_to_model(model_path):",
          "55:     model_config_path = os.path.join(model_path, \"MLmodel\")",
          "56:     model = Model.load(model_config_path)",
          "57:     model.add_flavor(\"spark\", spark_version=\"3.5.0\")",
          "58:     model.save(model_config_path)",
          "61: @dataclass",
          "62: class Param:",
          "63:     expected_dockerfile: str",
          "64:     env_manager: str = VIRTUALENV",
          "65:     mlflow_home: Optional[str] = None",
          "66:     install_mlflow: bool = False",
          "67:     enable_mlserver: bool = False",
          "68:     # If True, image is built with --model-uri param",
          "69:     specify_model_uri: bool = True",
          "72: @pytest.mark.parametrize(",
          "73:     \"params\",",
          "74:     [",
          "75:         Param(expected_dockerfile=\"Dockerfile_default\"),",
          "76:         Param(install_mlflow=True, expected_dockerfile=\"Dockerfile_install_mlflow\"),",
          "77:         Param(enable_mlserver=True, expected_dockerfile=\"Dockerfile_enable_mlserver\"),",
          "78:         Param(mlflow_home=\".\", expected_dockerfile=\"Dockerfile_with_mlflow_home\"),",
          "79:         Param(specify_model_uri=False, expected_dockerfile=\"Dockerfile_no_model_uri\"),",
          "80:     ],",
          "81: )",
          "82: def test_build_image(tmp_path, params):",
          "83:     model_uri = save_model(tmp_path) if params.specify_model_uri else None",
          "85:     backend = get_flavor_backend(model_uri, docker_build=True, env_manager=params.env_manager)",
          "87:     # Copy the context dir to a temp dir so we can verify the generated Dockerfile",
          "88:     def _build_image_with_copy(context_dir, image_name):",
          "89:         # Replace mlflow dev version in Dockerfile with the latest released one",
          "90:         dockerfile = Path(context_dir) / \"Dockerfile\"",
          "91:         content = dockerfile.read_text()",
          "92:         content = content.replace(VERSION, get_released_mlflow_version())",
          "93:         dockerfile.write_text(content)",
          "95:         shutil.copytree(context_dir, dst_dir)",
          "96:         build_image_from_context(context_dir, image_name)",
          "98:     dst_dir = tmp_path / \"context\"",
          "99:     with mock.patch(",
          "100:         \"mlflow.models.docker_utils.build_image_from_context\",",
          "101:         side_effect=_build_image_with_copy,",
          "102:     ):",
          "103:         backend.build_image(",
          "104:             model_uri=model_uri,",
          "105:             image_name=\"test_image\",",
          "106:             mlflow_home=params.mlflow_home,",
          "107:             install_mlflow=params.install_mlflow,",
          "108:             enable_mlserver=params.enable_mlserver,",
          "109:         )",
          "111:     actual = dst_dir / \"Dockerfile\"",
          "112:     expected = Path(RESOURCE_DIR) / params.expected_dockerfile",
          "113:     assert_dockerfiles_equal(actual, expected)",
          "116: def test_generate_dockerfile_for_java_flavor(tmp_path):",
          "117:     model_path = save_model(tmp_path)",
          "118:     add_spark_flavor_to_model(model_path)",
          "120:     backend = get_flavor_backend(model_path, docker_build=True)",
          "122:     backend.generate_dockerfile(",
          "123:         model_uri=model_path,",
          "124:         output_dir=tmp_path,",
          "125:     )",
          "127:     actual = tmp_path / \"Dockerfile\"",
          "128:     expected = Path(RESOURCE_DIR) / \"Dockerfile_java_flavor\"",
          "129:     assert_dockerfiles_equal(actual, expected)",
          "",
          "---------------"
        ],
        "tests/pyfunc/docker/test_docker_flavors.py||tests/pyfunc/docker/test_docker_flavors.py": [
          "File: tests/pyfunc/docker/test_docker_flavors.py -> tests/pyfunc/docker/test_docker_flavors.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: \"\"\"",
          "2: This test class is used for comprehensive testing of serving docker images for all MLflow flavors.",
          "3: As such, it is not intended to be run on a regular basis and is skipped by default. Rather, it",
          "4: should be run manually when making changes to the core docker logic.",
          "6: To run this test, run the following command manually",
          "8:     $ pytest tests/pyfunc/test_docker_flavors.py",
          "10: \"\"\"",
          "12: import json",
          "13: import os",
          "14: import time",
          "15: from operator import itemgetter",
          "17: import pandas as pd",
          "18: import pytest",
          "19: import requests",
          "21: import mlflow",
          "22: from mlflow.models.flavor_backend_registry import get_flavor_backend",
          "24: from tests.catboost.test_catboost_model_export import reg_model  # noqa: F401",
          "25: from tests.diviner.test_diviner_model_export import (  # noqa: F401",
          "26:     diviner_data,",
          "27:     diviner_groups,",
          "28:     grouped_prophet,",
          "29: )",
          "30: from tests.fastai.test_fastai_model_export import fastai_model as fastai_model_raw  # noqa: F401",
          "31: from tests.h2o.test_h2o_model_export import h2o_iris_model  # noqa: F401",
          "32: from tests.helper_functions import get_safe_port",
          "33: from tests.langchain.test_langchain_model_export import fake_chat_model  # noqa: F401",
          "34: from tests.lightgbm.test_lightgbm_model_export import lgb_model  # noqa: F401",
          "35: from tests.models.test_model import iris_data, sklearn_knn_model  # noqa: F401",
          "36: from tests.paddle.test_paddle_model_export import pd_model  # noqa: F401",
          "37: from tests.pmdarima.test_pmdarima_model_export import (  # noqa: F401",
          "38:     auto_arima_object_model,",
          "39:     test_data,",
          "40: )",
          "41: from tests.prophet.test_prophet_model_export import prophet_model as prophet_raw_model  # noqa: F401",
          "42: from tests.pyfunc.docker.conftest import (  # noqa: F401",
          "43:     MLFLOW_ROOT,",
          "44:     TEST_IMAGE_NAME,",
          "45:     docker_client,",
          "46:     save_model_with_latest_mlflow_version,",
          "47: )",
          "48: from tests.spacy.test_spacy_model_export import spacy_model_with_data  # noqa: F401",
          "49: from tests.spark.test_spark_model_export import (  # noqa: F401",
          "50:     iris_df,",
          "51:     spark,",
          "52:     spark_model_iris,",
          "53: )",
          "54: from tests.statsmodels.model_fixtures import ols_model",
          "55: from tests.tensorflow.test_tensorflow2_core_model_export import tf2_toy_model  # noqa: F401",
          "56: from tests.transformers.helper import load_small_seq2seq_pipeline",
          "59: @pytest.mark.skipif(os.getenv(\"GITHUB_ACTIONS\") == \"true\", reason=\"Time consuming tests\")",
          "60: @pytest.mark.parametrize(",
          "61:     (\"flavor\"),",
          "62:     [",
          "63:         \"catboost\",",
          "64:         \"diviner\",",
          "65:         \"fastai\",",
          "66:         \"h2o\",",
          "67:         # \"johnsnowlabs\", # Couldn't test JohnSnowLab locally due to license issue",
          "68:         \"keras\",",
          "69:         \"langchain\",",
          "70:         \"lightgbm\",",
          "71:         # \"mleap\", # Mleap model logging is deprecated since 2.6.1",
          "72:         \"onnx\",",
          "73:         # \"openai\", # OPENAI API KEY is not necessarily available for everyone",
          "74:         \"paddle\",",
          "75:         \"pmdarima\",",
          "76:         \"prophet\",",
          "77:         \"pyfunc\",",
          "78:         \"pytorch\",",
          "79:         \"sklearn\",",
          "80:         \"spacy\",",
          "81:         \"spark\",",
          "82:         \"statsmodels\",",
          "83:         \"tensorflow\",",
          "84:         \"transformers\",",
          "85:     ],",
          "86: )",
          "87: def test_build_image_and_serve(flavor, request):",
          "88:     model_path = request.getfixturevalue(f\"{flavor}_model\")",
          "90:     # Build an image",
          "91:     backend = get_flavor_backend(model_uri=model_path, docker_build=True)",
          "92:     backend.build_image(",
          "93:         model_uri=model_path,",
          "94:         image_name=TEST_IMAGE_NAME,",
          "95:         mlflow_home=MLFLOW_ROOT,  # Required to prevent installing dev version of MLflow from PyPI",
          "96:     )",
          "98:     # Run a container",
          "99:     port = get_safe_port()",
          "100:     docker_client.containers.run(",
          "101:         image=TEST_IMAGE_NAME,",
          "102:         ports={8080: port},",
          "103:         detach=True,",
          "104:     )",
          "106:     # Wait for the container to start",
          "107:     for _ in range(30):",
          "108:         try:",
          "109:             response = requests.get(url=f\"http://localhost:{port}/ping\")",
          "110:             if response.ok:",
          "111:                 break",
          "112:         except requests.exceptions.ConnectionError:",
          "113:             time.sleep(5)",
          "114:     else:",
          "115:         raise TimeoutError(\"Failed to start server.\")",
          "117:     # Make a scoring request with a saved input example",
          "118:     with open(os.path.join(model_path, \"input_example.json\")) as f:",
          "119:         input_example = json.load(f)",
          "121:     # Wrap Pandas dataframe in a proper payload format",
          "122:     if \"columns\" in input_example or \"data\" in input_example:",
          "123:         input_example = {\"dataframe_split\": input_example}",
          "125:     response = requests.post(",
          "126:         url=f\"http://localhost:{port}/invocations\",",
          "127:         data=json.dumps(input_example),",
          "128:         headers={\"Content-Type\": \"application/json\"},",
          "129:     )",
          "131:     assert response.status_code == 200, f\"Response: {response.text}\"",
          "132:     assert \"predictions\" in response.json(), f\"Response: {response.text}\"",
          "135: @pytest.fixture",
          "136: def catboost_model(tmp_path, reg_model):",
          "137:     model_path = str(tmp_path / \"catboost_model\")",
          "138:     save_model_with_latest_mlflow_version(",
          "139:         flavor=\"catboost\",",
          "140:         cb_model=reg_model.model,",
          "141:         path=model_path,",
          "142:         input_example=reg_model.inference_dataframe[:1],",
          "143:     )",
          "144:     return model_path",
          "147: @pytest.fixture",
          "148: def diviner_model(tmp_path, grouped_prophet, diviner_groups):",
          "149:     model_path = str(tmp_path / \"diviner_model\")",
          "151:     save_model_with_latest_mlflow_version(",
          "152:         flavor=\"diviner\",",
          "153:         diviner_model=grouped_prophet,",
          "154:         path=model_path,",
          "155:         input_example=pd.DataFrame(",
          "156:             {\"groups\": [diviner_groups], \"horizon\": 10, \"frequency\": \"D\"}, index=[0]",
          "157:         ),",
          "158:     )",
          "159:     return model_path",
          "162: @pytest.fixture",
          "163: def fastai_model(tmp_path, fastai_model_raw):",
          "164:     model_path = str(tmp_path / \"fastai_model\")",
          "165:     save_model_with_latest_mlflow_version(",
          "166:         flavor=\"fastai\",",
          "167:         fastai_learner=fastai_model_raw.model,",
          "168:         path=model_path,",
          "169:         input_example=fastai_model_raw.inference_dataframe[:1],",
          "170:     )",
          "171:     return model_path",
          "174: @pytest.fixture",
          "175: def h2o_model(tmp_path, h2o_iris_model):",
          "176:     model_path = str(tmp_path / \"h2o_model\")",
          "177:     save_model_with_latest_mlflow_version(",
          "178:         flavor=\"h2o\",",
          "179:         h2o_model=h2o_iris_model.model,",
          "180:         path=model_path,",
          "181:         input_example=h2o_iris_model.inference_data.as_data_frame()[:1],",
          "182:     )",
          "183:     return model_path",
          "186: @pytest.fixture",
          "187: def keras_model(tmp_path, iris_data):",
          "188:     from sklearn import datasets",
          "189:     from tensorflow.keras.layers import Dense",
          "190:     from tensorflow.keras.models import Sequential",
          "192:     model_path = str(tmp_path / \"keras_model\")",
          "194:     model = Sequential()",
          "195:     model.add(Dense(3, input_dim=4))",
          "196:     model.add(Dense(1))",
          "198:     X, y = datasets.load_iris(return_X_y=True)",
          "199:     save_model_with_latest_mlflow_version(",
          "200:         flavor=\"tensorflow\",",
          "201:         model=model,",
          "202:         path=model_path,",
          "203:         input_example=X[:3, :],",
          "204:     )",
          "205:     return model_path",
          "208: @pytest.fixture",
          "209: def langchain_model(tmp_path):",
          "210:     from langchain.schema.runnable import RunnablePassthrough",
          "212:     chain = RunnablePassthrough() | itemgetter(\"messages\")",
          "213:     model_path = str(tmp_path / \"langchain_model\")",
          "214:     save_model_with_latest_mlflow_version(",
          "215:         flavor=\"langchain\", lc_model=chain, path=model_path, input_example={\"messages\": \"Hi\"}",
          "216:     )",
          "217:     return model_path",
          "220: @pytest.fixture",
          "221: def lightgbm_model(tmp_path, lgb_model):",
          "222:     model_path = str(tmp_path / \"lightgbm_model\")",
          "223:     save_model_with_latest_mlflow_version(",
          "224:         flavor=\"lightgbm\",",
          "225:         lgb_model=lgb_model.model,",
          "226:         path=model_path,",
          "227:         input_example=lgb_model.inference_dataframe.to_numpy()[:1],",
          "228:     )",
          "229:     return model_path",
          "232: @pytest.fixture",
          "233: def onnx_model(tmp_path):",
          "234:     import numpy as np",
          "235:     import onnx",
          "236:     import torch",
          "237:     from torch import nn",
          "239:     model = torch.nn.Sequential(nn.Linear(4, 3), nn.ReLU(), nn.Linear(3, 1))",
          "240:     onnx_model_path = os.path.join(tmp_path, \"torch_onnx\")",
          "241:     torch.onnx.export(",
          "242:         model,",
          "243:         torch.randn(1, 4),",
          "244:         onnx_model_path,",
          "245:         dynamic_axes={\"input\": {0: \"batch\"}},",
          "246:         input_names=[\"input\"],",
          "247:     )",
          "248:     onnx_model = onnx.load(onnx_model_path)",
          "250:     model_path = str(tmp_path / \"onnx_model\")",
          "251:     save_model_with_latest_mlflow_version(",
          "252:         flavor=\"onnx\",",
          "253:         onnx_model=onnx_model,",
          "254:         path=model_path,",
          "255:         input_example=np.random.rand(1, 4).astype(np.float32),",
          "256:     )",
          "257:     return model_path",
          "260: @pytest.fixture",
          "261: def paddle_model(tmp_path, pd_model):",
          "262:     model_path = str(tmp_path / \"paddle_model\")",
          "263:     save_model_with_latest_mlflow_version(",
          "264:         flavor=\"paddle\",",
          "265:         pd_model=pd_model.model,",
          "266:         path=model_path,",
          "267:         input_example=pd_model.inference_dataframe[:1],",
          "268:     )",
          "269:     return model_path",
          "272: @pytest.fixture",
          "273: def pmdarima_model(tmp_path, auto_arima_object_model):",
          "274:     model_path = str(tmp_path / \"pmdarima_model\")",
          "275:     save_model_with_latest_mlflow_version(",
          "276:         flavor=\"pmdarima\",",
          "277:         pmdarima_model=auto_arima_object_model,",
          "278:         path=model_path,",
          "279:         input_example=pd.DataFrame({\"n_periods\": [30]}),",
          "280:     )",
          "281:     return model_path",
          "284: @pytest.fixture",
          "285: def prophet_model(tmp_path, prophet_raw_model):",
          "286:     model_path = str(tmp_path / \"prophet_model\")",
          "287:     save_model_with_latest_mlflow_version(",
          "288:         flavor=\"prophet\",",
          "289:         pr_model=prophet_raw_model.model,",
          "290:         path=model_path,",
          "291:         input_example=prophet_raw_model.data[:1],",
          "292:     )",
          "293:     return model_path",
          "296: @pytest.fixture",
          "297: def pyfunc_model(tmp_path):",
          "298:     class CustomModel(mlflow.pyfunc.PythonModel):",
          "299:         def __init__(self):",
          "300:             pass",
          "302:         def predict(self, context, model_input):",
          "303:             return model_input",
          "305:     model_path = str(tmp_path / \"pyfunc_model\")",
          "306:     save_model_with_latest_mlflow_version(",
          "307:         flavor=\"pyfunc\",",
          "308:         python_model=CustomModel(),",
          "309:         path=model_path,",
          "310:         input_example=[1, 2, 3],",
          "311:     )",
          "312:     return model_path",
          "315: @pytest.fixture",
          "316: def pytorch_model(tmp_path):",
          "317:     from torch import nn, randn",
          "319:     model = nn.Sequential(nn.Linear(4, 3), nn.ReLU(), nn.Linear(3, 1))",
          "320:     model_path = str(tmp_path / \"pytorch_model\")",
          "321:     save_model_with_latest_mlflow_version(",
          "322:         flavor=\"pytorch\",",
          "323:         pytorch_model=model,",
          "324:         path=model_path,",
          "325:         input_example=randn(1, 4).numpy(),",
          "326:     )",
          "327:     return model_path",
          "330: @pytest.fixture",
          "331: def sklearn_model(tmp_path, sklearn_knn_model, iris_data):",
          "332:     model_path = str(tmp_path / \"sklearn_model\")",
          "333:     save_model_with_latest_mlflow_version(",
          "334:         flavor=\"sklearn\",",
          "335:         sk_model=sklearn_knn_model,",
          "336:         path=model_path,",
          "337:         input_example=iris_data[0][:1],",
          "338:     )",
          "339:     return model_path",
          "342: @pytest.fixture",
          "343: def spacy_model(tmp_path, spacy_model_with_data):",
          "344:     model_path = str(tmp_path / \"spacy_model\")",
          "345:     save_model_with_latest_mlflow_version(",
          "346:         flavor=\"spacy\",",
          "347:         spacy_model=spacy_model_with_data.model,",
          "348:         path=model_path,",
          "349:         input_example=spacy_model_with_data.inference_data[:1],",
          "350:     )",
          "351:     return model_path",
          "354: @pytest.fixture",
          "355: def spark_model(tmp_path, spark_model_iris):",
          "356:     model_path = str(tmp_path / \"spark_model\")",
          "357:     save_model_with_latest_mlflow_version(",
          "358:         flavor=\"spark\",",
          "359:         spark_model=spark_model_iris.model,",
          "360:         path=model_path,",
          "361:         input_example=spark_model_iris.spark_df.toPandas()[:1],",
          "362:     )",
          "363:     return model_path",
          "366: @pytest.fixture",
          "367: def statsmodels_model(tmp_path):",
          "368:     model = ols_model()",
          "369:     model_path = str(tmp_path / \"statsmodels_model\")",
          "370:     save_model_with_latest_mlflow_version(",
          "371:         flavor=\"statsmodels\",",
          "372:         statsmodels_model=model.model,",
          "373:         path=model_path,",
          "374:         input_example=model.inference_dataframe[:1],",
          "375:     )",
          "376:     return model_path",
          "379: @pytest.fixture",
          "380: def tensorflow_model(tmp_path, tf2_toy_model):",
          "381:     model_path = str(tmp_path / \"tensorflow_model\")",
          "382:     save_model_with_latest_mlflow_version(",
          "383:         flavor=\"tensorflow\",",
          "384:         model=tf2_toy_model.model,",
          "385:         path=model_path,",
          "386:         input_example=tf2_toy_model.inference_data[:1],",
          "387:     )",
          "388:     return model_path",
          "391: @pytest.fixture",
          "392: def transformers_model(tmp_path):",
          "393:     pipeline = load_small_seq2seq_pipeline()",
          "394:     model_path = str(tmp_path / \"transformers_model\")",
          "395:     save_model_with_latest_mlflow_version(",
          "396:         flavor=\"transformers\",",
          "397:         transformers_model=pipeline,",
          "398:         path=model_path,",
          "399:         input_example=\"hi\",",
          "400:     )",
          "401:     return model_path",
          "",
          "---------------"
        ],
        "tests/pyfunc/test_docker.py||tests/pyfunc/test_docker.py": [
          "File: tests/pyfunc/test_docker.py -> tests/pyfunc/test_docker.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/sagemaker/test_cli.py||tests/sagemaker/test_cli.py": [
          "File: tests/sagemaker/test_cli.py -> tests/sagemaker/test_cli.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "6: import pytest",
          "7: from click.testing import CliRunner",
          "9: import mlflow",
          "10: from mlflow.models.docker_utils import build_image_from_context",
          "11: from mlflow.sagemaker.cli import build_and_push_container",
          "15: _MLFLOW_ROOT = Path(mlflow.__file__).parent.parent",
          "16: _RESOURCE_DIR = os.path.join(_MLFLOW_ROOT, \"tests\", \"resources\", \"dockerfile\")",
          "19: @pytest.mark.parametrize(\"env_manager\", [\"conda\", \"virtualenv\"])",
          "",
          "[Removed Lines]",
          "13: from tests.pyfunc.test_docker import assert_dockerfiles_equal",
          "",
          "[Added Lines]",
          "9: import docker",
          "14: from tests.pyfunc.docker.test_docker import assert_dockerfiles_equal",
          "18: _TEST_IMAGE_NAME = \"test-sagemaker-image\"",
          "20: _docker_client = docker.from_env()",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "36:                 \".\",",
          "37:                 \"--env-manager\",",
          "38:                 env_manager,",
          "39:             ],",
          "40:             catch_exceptions=False,",
          "41:         )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "43:                 \"--container\",",
          "44:                 _TEST_IMAGE_NAME,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "44:     actual = dst_dir / \"Dockerfile\"",
          "45:     expected = Path(_RESOURCE_DIR) / f\"Dockerfile_sagemaker_{env_manager}\"",
          "46:     assert_dockerfiles_equal(actual, expected)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "54:     # Clean up generated image",
          "55:     _docker_client.images.remove(_TEST_IMAGE_NAME, force=True)",
          "",
          "---------------"
        ]
      }
    }
  ]
}