{
  "cve_id": "CVE-2022-23595",
  "cve_desc": "Tensorflow is an Open Source Machine Learning Framework. When building an XLA compilation cache, if default settings are used, TensorFlow triggers a null pointer dereference. In the default scenario, all devices are allowed, so `flr->config_proto` is `nullptr`. The fix will be included in TensorFlow 2.8.0. We will also cherrypick this commit on TensorFlow 2.7.1, TensorFlow 2.6.3, and TensorFlow 2.5.3, as these are also affected and still in supported range.",
  "repo": "tensorflow/tensorflow",
  "patch_hash": "e21af685e1828f7ca65038307df5cc06de4479e8",
  "patch_info": {
    "commit_hash": "e21af685e1828f7ca65038307df5cc06de4479e8",
    "repo": "tensorflow/tensorflow",
    "commit_url": "https://github.com/tensorflow/tensorflow/commit/e21af685e1828f7ca65038307df5cc06de4479e8",
    "files": [
      "tensorflow/compiler/jit/xla_platform_info.cc"
    ],
    "message": "Fix Null-pointer dereference in BuildXlaCompilationCache\n\nIf ConfigProto is not used, then use the default settings which is to allow all devices.\n\nPiperOrigin-RevId: 420391800\nChange-Id: I88161ad7042990aef678e77b597a2fb2c8f815be",
    "before_after_code_files": [
      "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
    ]
  },
  "patch_diff": {
    "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc": [
      "File: tensorflow/compiler/jit/xla_platform_info.cc -> tensorflow/compiler/jit/xla_platform_info.cc",
      "--- Hunk 1 ---",
      "[Context before]",
      "82:   client_options.set_intra_op_parallelism_threads(",
      "83:       device->tensorflow_cpu_worker_threads()->num_threads);",
      "91:   auto client = xla::ClientLibrary::GetOrCreateLocalClient(client_options);",
      "92:   if (!client.ok()) {",
      "",
      "[Removed Lines]",
      "85:   string allowed_gpus =",
      "86:       flr->config_proto()->gpu_options().visible_device_list();",
      "87:   TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,",
      "88:                       ParseVisibleDeviceList(allowed_gpus));",
      "89:   client_options.set_allowed_devices(gpu_ids);",
      "",
      "[Added Lines]",
      "85:   if (flr->config_proto()) {",
      "86:     string allowed_gpus =",
      "87:         flr->config_proto()->gpu_options().visible_device_list();",
      "88:     TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,",
      "89:                         ParseVisibleDeviceList(allowed_gpus));",
      "90:     client_options.set_allowed_devices(gpu_ids);",
      "91:   }",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "c83aeca106be58680521e7fa02a8bd27b61d48f5",
      "candidate_info": {
        "commit_hash": "c83aeca106be58680521e7fa02a8bd27b61d48f5",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/c83aeca106be58680521e7fa02a8bd27b61d48f5",
        "files": [
          "tensorflow/compiler/jit/xla_platform_info.cc"
        ],
        "message": "Fix Null-pointer dereference in BuildXlaCompilationCache\n\nIf ConfigProto is not used, then use the default settings which is to allow all devices.\n\nPiperOrigin-RevId: 420391800\nChange-Id: I88161ad7042990aef678e77b597a2fb2c8f815be",
        "before_after_code_files": [
          "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
          ],
          "candidate": [
            "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc": [
          "File: tensorflow/compiler/jit/xla_platform_info.cc -> tensorflow/compiler/jit/xla_platform_info.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "82:   client_options.set_intra_op_parallelism_threads(",
          "83:       device->tensorflow_cpu_worker_threads()->num_threads);",
          "91:   auto client = xla::ClientLibrary::GetOrCreateLocalClient(client_options);",
          "92:   if (!client.ok()) {",
          "",
          "[Removed Lines]",
          "85:   string allowed_gpus =",
          "86:       flr->config_proto()->gpu_options().visible_device_list();",
          "87:   TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,",
          "88:                       ParseVisibleDeviceList(allowed_gpus));",
          "89:   client_options.set_allowed_devices(gpu_ids);",
          "",
          "[Added Lines]",
          "85:   if (flr->config_proto()) {",
          "86:     string allowed_gpus =",
          "87:         flr->config_proto()->gpu_options().visible_device_list();",
          "88:     TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,",
          "89:                         ParseVisibleDeviceList(allowed_gpus));",
          "90:     client_options.set_allowed_devices(gpu_ids);",
          "91:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d22277271c747d6de421387c7b38425db7cfbb74",
      "candidate_info": {
        "commit_hash": "d22277271c747d6de421387c7b38425db7cfbb74",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/d22277271c747d6de421387c7b38425db7cfbb74",
        "files": [
          "tensorflow/compiler/jit/xla_platform_info.cc"
        ],
        "message": "Fix Null-pointer dereference in BuildXlaCompilationCache\n\nIf ConfigProto is not used, then use the default settings which is to allow all devices.\n\nPiperOrigin-RevId: 420391800\nChange-Id: I88161ad7042990aef678e77b597a2fb2c8f815be",
        "before_after_code_files": [
          "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
          ],
          "candidate": [
            "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc": [
          "File: tensorflow/compiler/jit/xla_platform_info.cc -> tensorflow/compiler/jit/xla_platform_info.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "60:   client_options.set_platform(platform.ValueOrDie());",
          "61:   client_options.set_intra_op_parallelism_threads(",
          "62:       device->tensorflow_cpu_worker_threads()->num_threads);",
          "63:   auto client = xla::ClientLibrary::GetOrCreateLocalClient(client_options);",
          "64:   if (!client.ok()) {",
          "65:     return client.status();",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "64:   if (flr->config_proto()) {",
          "65:     string allowed_gpus =",
          "66:         flr->config_proto()->gpu_options().visible_device_list();",
          "67:     TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,",
          "68:                         ParseVisibleDeviceList(allowed_gpus));",
          "69:     client_options.set_allowed_devices(gpu_ids);",
          "70:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "6e00a6efc6fadf3fe1a70e4dc11a5c2297723110",
      "candidate_info": {
        "commit_hash": "6e00a6efc6fadf3fe1a70e4dc11a5c2297723110",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/6e00a6efc6fadf3fe1a70e4dc11a5c2297723110",
        "files": [
          "tensorflow/compiler/jit/xla_platform_info.cc"
        ],
        "message": "Fix Null-pointer dereference in BuildXlaCompilationCache\n\nIf ConfigProto is not used, then use the default settings which is to allow all devices.\n\nPiperOrigin-RevId: 420391800\nChange-Id: I88161ad7042990aef678e77b597a2fb2c8f815be",
        "before_after_code_files": [
          "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
          ],
          "candidate": [
            "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc": [
          "File: tensorflow/compiler/jit/xla_platform_info.cc -> tensorflow/compiler/jit/xla_platform_info.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "82:   client_options.set_intra_op_parallelism_threads(",
          "83:       device->tensorflow_cpu_worker_threads()->num_threads);",
          "91:   auto client = xla::ClientLibrary::GetOrCreateLocalClient(client_options);",
          "92:   if (!client.ok()) {",
          "",
          "[Removed Lines]",
          "85:   string allowed_gpus =",
          "86:       flr->config_proto()->gpu_options().visible_device_list();",
          "87:   TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,",
          "88:                       ParseVisibleDeviceList(allowed_gpus));",
          "89:   client_options.set_allowed_devices(gpu_ids);",
          "",
          "[Added Lines]",
          "85:   if (flr->config_proto()) {",
          "86:     string allowed_gpus =",
          "87:         flr->config_proto()->gpu_options().visible_device_list();",
          "88:     TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,",
          "89:                         ParseVisibleDeviceList(allowed_gpus));",
          "90:     client_options.set_allowed_devices(gpu_ids);",
          "91:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "23ca6e0d815fae1856daea51e7f052a185532a23",
      "candidate_info": {
        "commit_hash": "23ca6e0d815fae1856daea51e7f052a185532a23",
        "repo": "tensorflow/tensorflow",
        "commit_url": "https://github.com/tensorflow/tensorflow/commit/23ca6e0d815fae1856daea51e7f052a185532a23",
        "files": [
          "tensorflow/compiler/jit/xla_platform_info.cc"
        ],
        "message": "Fix Null-pointer dereference in BuildXlaCompilationCache\n\nIf ConfigProto is not used, then use the default settings which is to allow all devices.\n\nPiperOrigin-RevId: 420391800\nChange-Id: I88161ad7042990aef678e77b597a2fb2c8f815be",
        "before_after_code_files": [
          "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
          ],
          "candidate": [
            "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc"
          ]
        }
      },
      "candidate_diff": {
        "tensorflow/compiler/jit/xla_platform_info.cc||tensorflow/compiler/jit/xla_platform_info.cc": [
          "File: tensorflow/compiler/jit/xla_platform_info.cc -> tensorflow/compiler/jit/xla_platform_info.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "82:   client_options.set_intra_op_parallelism_threads(",
          "83:       device->tensorflow_cpu_worker_threads()->num_threads);",
          "91:   auto client = xla::ClientLibrary::GetOrCreateLocalClient(client_options);",
          "92:   if (!client.ok()) {",
          "",
          "[Removed Lines]",
          "85:   string allowed_gpus =",
          "86:       flr->config_proto()->gpu_options().visible_device_list();",
          "87:   TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,",
          "88:                       ParseVisibleDeviceList(allowed_gpus));",
          "89:   client_options.set_allowed_devices(gpu_ids);",
          "",
          "[Added Lines]",
          "85:   if (flr->config_proto()) {",
          "86:     string allowed_gpus =",
          "87:         flr->config_proto()->gpu_options().visible_device_list();",
          "88:     TF_ASSIGN_OR_RETURN(absl::optional<std::set<int>> gpu_ids,",
          "89:                         ParseVisibleDeviceList(allowed_gpus));",
          "90:     client_options.set_allowed_devices(gpu_ids);",
          "91:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}