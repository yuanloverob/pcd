{
  "cve_id": "CVE-2023-45813",
  "cve_desc": "Torbot is an open source tor network intelligence tool. In affected versions the `torbot.modules.validators.validate_link function` uses the python-validators URL validation regex. This particular regular expression has an exponential complexity which allows an attacker to cause an application crash using a well-crafted argument. An attacker can use a well-crafted URL argument to exploit the vulnerability in the regular expression and cause a Denial of Service on the system. The validators file has been removed in version 4.0.0. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
  "repo": "DedSecInside/TorBot",
  "patch_hash": "ef6e06bc7785355b1701d5524eb4550441086ac4",
  "patch_info": {
    "commit_hash": "ef6e06bc7785355b1701d5524eb4550441086ac4",
    "repo": "DedSecInside/TorBot",
    "commit_url": "https://github.com/DedSecInside/TorBot/commit/ef6e06bc7785355b1701d5524eb4550441086ac4",
    "files": [
      "torbot/modules/validators.py"
    ],
    "message": "remove unused validators file",
    "before_after_code_files": [
      "torbot/modules/validators.py||torbot/modules/validators.py"
    ]
  },
  "patch_diff": {
    "torbot/modules/validators.py||torbot/modules/validators.py": [
      "File: torbot/modules/validators.py -> torbot/modules/validators.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "[No context available]",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "461d67ea935cba683fa251b42f5f155a4113d0c5",
      "candidate_info": {
        "commit_hash": "461d67ea935cba683fa251b42f5f155a4113d0c5",
        "repo": "DedSecInside/TorBot",
        "commit_url": "https://github.com/DedSecInside/TorBot/commit/461d67ea935cba683fa251b42f5f155a4113d0c5",
        "files": [
          "torbot/__init__.py",
          "torbot/__main__.py",
          "torbot/main.py",
          "torbot/modules/api.py",
          "torbot/modules/link_io.py",
          "torbot/modules/linktree.py",
          "torbot/version.py"
        ],
        "message": "flake8 fixes",
        "before_after_code_files": [
          "torbot/__init__.py||torbot/__init__.py",
          "torbot/__main__.py||torbot/__main__.py",
          "torbot/main.py||torbot/main.py",
          "torbot/modules/api.py||torbot/modules/api.py",
          "torbot/modules/link_io.py||torbot/modules/link_io.py",
          "torbot/modules/linktree.py||torbot/modules/linktree.py",
          "torbot/version.py||torbot/version.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/DedSecInside/TorBot/pull/307"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "torbot/__init__.py||torbot/__init__.py": [
          "File: torbot/__init__.py -> torbot/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "torbot/__main__.py||torbot/__main__.py": [
          "File: torbot/__main__.py -> torbot/__main__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "torbot/main.py||torbot/main.py": [
          "File: torbot/main.py -> torbot/main.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: from .modules.collect_data import collect_data",
          "16: from .modules.nlp import main",
          "21: # TorBot CLI class",
          "",
          "[Removed Lines]",
          "18: from . import version",
          "",
          "[Added Lines]",
          "18: VERSION = '3.1.2'",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "24:     def __init__(self, args):",
          "25:         self.args = args",
          "28:     def get_header(self):",
          "29:         license_msg = color(\"LICENSE: GNU Public License v3\", \"red\")",
          "",
          "[Removed Lines]",
          "26:         self.__version__ = version",
          "",
          "[Added Lines]",
          "26:         self.__version__ = VERSION",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "33:                             / __/ / / / /_/ / __ \\/ __ \\/ /",
          "34:                            / /_/ /_/ / _, _/ /_/ / /_/ / /",
          "35:                            \\__/\\____/_/ |_/_____/\\____/_/  V{VERSION}",
          "37:         banner = color(banner, \"red\")",
          "39:         title = r\"\"\"",
          "",
          "[Removed Lines]",
          "36:                 \"\"\".format(VERSION=version.__version__)",
          "",
          "[Added Lines]",
          "36:                 \"\"\".format(VERSION=self.__version__)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "111:         print_tor_ip_address()",
          "115:         if args.classify:",
          "116:             result = main.classify(args.url)",
          "",
          "[Removed Lines]",
          "113:         tree = get_node(args.url, args.depth)",
          "",
          "[Added Lines]",
          "113:         tree = get_node(args.url, args.depth)",
          "",
          "---------------"
        ],
        "torbot/modules/api.py||torbot/modules/api.py": [
          "File: torbot/modules/api.py -> torbot/modules/api.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "85:     \"\"\"",
          "86:     endpoint = f'/content?link={link}'",
          "87:     url = base_url + endpoint",
          "89:     resp = httpx.get(url)",
          "91:     return resp.text",
          "",
          "[Removed Lines]",
          "88:     debug(f'requesting {url}')",
          "90:     debug(f'retrieved {resp.text}')",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "torbot/modules/link_io.py||torbot/modules/link_io.py": [
          "File: torbot/modules/link_io.py -> torbot/modules/link_io.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "48:             insert(node, 'yellow')",
          "49:         else:",
          "50:             insert(node, 'red')",
          "52:     headers = [\"Title\", \"URL\", \"Status\", \"Category\"]",
          "54:     print(table)",
          "",
          "[Removed Lines]",
          "53:     table = tabulate.tabulate(table_data, headers=headers)",
          "",
          "[Added Lines]",
          "53:     table = tabulate.tabulate(table_data, headers=headers)",
          "",
          "---------------"
        ],
        "torbot/modules/linktree.py||torbot/modules/linktree.py": [
          "File: torbot/modules/linktree.py -> torbot/modules/linktree.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "53:     if depth > 0:",
          "54:         depth -= 1",
          "55:         resp = httpx.get(url, proxies='socks5://127.0.0.1:9050')",
          "57:         for child in children:",
          "58:             append_node(tree, id=child, parent_id=url)",
          "59:             build_tree(tree, child, depth)",
          "",
          "[Removed Lines]",
          "56:         children = parse_links(resp.text)",
          "",
          "[Added Lines]",
          "56:         children = parse_links(resp.text)",
          "",
          "---------------"
        ],
        "torbot/version.py||torbot/version.py": [
          "File: torbot/version.py -> torbot/version.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "182f51abb5596410e8eb875885d61dc0b9b19096",
      "candidate_info": {
        "commit_hash": "182f51abb5596410e8eb875885d61dc0b9b19096",
        "repo": "DedSecInside/TorBot",
        "commit_url": "https://github.com/DedSecInside/TorBot/commit/182f51abb5596410e8eb875885d61dc0b9b19096",
        "files": [
          ".env",
          "README.md",
          "torbot/main.py",
          "torbot/modules/config.py",
          "torbot/modules/io.py",
          "torbot/modules/linktree.py",
          "torbot/modules/savefile.py",
          "torbot/modules/tests/test_savetofile.py"
        ],
        "message": "more major changes",
        "before_after_code_files": [
          "torbot/main.py||torbot/main.py",
          "torbot/modules/config.py||torbot/modules/config.py",
          "torbot/modules/io.py||torbot/modules/io.py",
          "torbot/modules/linktree.py||torbot/modules/linktree.py",
          "torbot/modules/savefile.py||torbot/modules/savefile.py",
          "torbot/modules/tests/test_savetofile.py||torbot/modules/tests/test_savetofile.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/DedSecInside/TorBot/pull/307"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "torbot/main.py||torbot/main.py": [
          "File: torbot/main.py -> torbot/main.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: \"\"\"",
          "2: Core",
          "3: \"\"\"",
          "4: import argparse",
          "5: import sys",
          "6: import logging",
          "9: from modules.color import color",
          "10: from modules.updater import check_version",
          "11: from modules.info import execute_all",
          "12: from modules.linktree import LinkTree",
          "18:     \"\"\"",
          "19:     Prints the TorBot banner including version and license.",
          "20:     \"\"\"",
          "",
          "[Removed Lines]",
          "8: from modules.io import pprint_tree, print_tor_ip_address",
          "14: VERSION = '3.1.2'",
          "17: def print_header() -> None:",
          "",
          "[Added Lines]",
          "4: import os",
          "8: import tomllib",
          "10: from modules.api import get_ip",
          "15: from modules.config import project_root_directory",
          "18: def print_tor_ip_address() -> None:",
          "19:     \"\"\"",
          "20:     https://check.torproject.org/ tells you if you are using tor and it",
          "21:     displays your IP address which we scape and display",
          "22:     \"\"\"",
          "23:     resp = get_ip()",
          "24:     print(resp[\"header\"])",
          "25:     print(color(resp[\"body\"], \"yellow\"))",
          "28: def print_header(version: str) -> None:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "24:                             / /_/ __ \\/ __ \\/ /_  ____/_  __/",
          "25:                         / __/ / / / /_/ / __ \\/ __ \\/ /",
          "26:                         / /_/ /_/ / _, _/ /_/ / /_/ / /",
          "29:     banner = color(banner, \"red\")",
          "31:     title = r\"\"\"",
          "",
          "[Removed Lines]",
          "27:                         \\__/\\____/_/ |_/_____/\\____/_/  V{VERSION}",
          "28:             \"\"\".format(VERSION=VERSION)",
          "",
          "[Added Lines]",
          "38:                         \\__/\\____/_/ |_/_____/\\____/_/  v{VERSION}",
          "39:             \"\"\".format(VERSION=version)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "42:     print(title)",
          "46:     args = arg_parser.parse_args()",
          "48:     # setup logging",
          "",
          "[Removed Lines]",
          "45: def run(arg_parser: argparse.ArgumentParser) -> None:",
          "",
          "[Added Lines]",
          "56: def run(arg_parser: argparse.ArgumentParser, version: str) -> None:",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "59:     # Print verison then exit",
          "60:     if args.version:",
          "62:         sys.exit()",
          "64:     # check version and update if necessary",
          "",
          "[Removed Lines]",
          "61:         print(f\"TorBot Version: {VERSION}\")",
          "",
          "[Added Lines]",
          "72:         print(f\"TorBot Version: {version}\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "69:     # print header and IP address if not set to quiet",
          "70:     if not args.quiet:",
          "72:         print_tor_ip_address()",
          "74:     if args.info:",
          "",
          "[Removed Lines]",
          "71:         print_header()",
          "",
          "[Added Lines]",
          "82:         print_header(version)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "77:     tree = LinkTree(url=args.url, depth=args.depth)",
          "78:     tree.load()",
          "81:         tree.save()",
          "84:         tree.show()",
          "91:     print(\"\\n\\n\")",
          "",
          "[Removed Lines]",
          "79:     # save tree and continue",
          "80:     if args.save:",
          "83:     if args.visualize:",
          "86:     pprint_tree(tree)",
          "87:     '''",
          "88:     elif args.save or args.mail or args.phone:",
          "89:         self.handle_json_args(args)",
          "90:     '''",
          "",
          "[Added Lines]",
          "91:     # save data if desired",
          "92:     if args.save == 'tree':",
          "94:     elif args.save == 'json':",
          "95:         tree.saveJSON()",
          "97:     # always print something, table is the default",
          "98:     if args.visualize == 'table' or not args.visualize:",
          "99:         tree.showTable()",
          "100:     elif args.visualize == 'tree':",
          "102:     elif args.visualize == 'json':",
          "103:         tree.showJSON()",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "98:     parser = argparse.ArgumentParser(prog=\"TorBot\", usage=\"Gather and analayze data from Tor sites.\")",
          "99:     parser.add_argument(\"-u\", \"--url\", type=str, required=True, help=\"Specifiy a website link to crawl\")",
          "100:     parser.add_argument(\"--depth\", type=int, help=\"Specifiy max depth of crawler (default 1)\", default=1)",
          "101:     parser.add_argument(\"-q\", \"--quiet\", action=\"store_true\")",
          "104:     parser.add_argument(\"--version\", action=\"store_true\", help=\"Show current version of TorBot.\")",
          "105:     parser.add_argument(\"--update\", action=\"store_true\", help=\"Update TorBot to the latest stable version\")",
          "107:     parser.add_argument(\"--info\", action=\"store_true\", help=\"Info displays basic info of the scanned site. Only supports a single URL at a time.\")",
          "109:     parser.add_argument(\"-v\", action=\"store_true\", help=\"verbose logging\")",
          "111:     return parser",
          "",
          "[Removed Lines]",
          "102:     parser.add_argument(\"-m\", \"--mail\", action=\"store_true\", help=\"Get e-mail addresses from the crawled sites\")",
          "103:     parser.add_argument(\"-p\", \"--phone\", action=\"store_true\", help=\"Get phone numbers from the crawled sites\")",
          "106:     parser.add_argument(\"--save\", action=\"store_true\", help=\"Save results in a file\")",
          "108:     parser.add_argument(\"--visualize\", action=\"store_true\", help=\"Visualizes tree of data gathered.\")",
          "",
          "[Added Lines]",
          "115:     parser.add_argument(\"--save\", type=str, choices=['tree', 'json'], help=\"Save results in a file\")",
          "116:     parser.add_argument(\"--visualize\", type=str, choices=['table', 'tree', 'json'], help=\"Visualizes data collection.\")",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "114: if __name__ == '__main__':",
          "115:     try:",
          "116:         arg_parser = set_arguments()",
          "118:     except KeyboardInterrupt:",
          "119:         print(\"Interrupt received! Exiting cleanly...\")",
          "",
          "[Removed Lines]",
          "117:         run(arg_parser)",
          "",
          "[Added Lines]",
          "129:         config_file_path = os.path.join(project_root_directory, \"pyproject.toml\")",
          "130:         try:",
          "131:             version = None",
          "132:             with open(config_file_path, \"rb\") as f:",
          "133:                 data = tomllib.load(f)",
          "134:                 version = data['tool']['poetry']['version']",
          "135:         except Exception as e:",
          "136:             raise Exception(\"unable to find version from pyprojec.toml.\\n\", e)",
          "138:         run(arg_parser, version)",
          "",
          "---------------"
        ],
        "torbot/modules/config.py||torbot/modules/config.py": [
          "File: torbot/modules/config.py -> torbot/modules/config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: import os",
          "4: from dotenv import load_dotenv",
          "5: from inspect import getsourcefile",
          "",
          "[Removed Lines]",
          "2: import logging",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "torbot/modules/io.py||torbot/modules/io.py": [
          "File: torbot/modules/io.py -> torbot/modules/io.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "torbot/modules/linktree.py||torbot/modules/linktree.py": [
          "File: torbot/modules/linktree.py -> torbot/modules/linktree.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: \"\"\"",
          "2: Module is used for analyzing link relationships",
          "3: \"\"\"",
          "4: import os",
          "5: import httpx",
          "6: import validators",
          "7: import logging",
          "9: from treelib import Tree, exceptions, Node",
          "10: from bs4 import BeautifulSoup",
          "12: from .config import project_root_directory",
          "13: from .nlp.main import classify",
          "16: class LinkNode(Node):",
          "19:         super().__init__()",
          "20:         self.identifier = url",
          "21:         self.tag = title",
          "22:         self.status = status",
          "23:         self.classification = classification",
          "24:         self.accuracy = accuracy",
          "26: class LinkTree(Tree):",
          "27:     def __init__(self, url: str, depth: int) -> None:",
          "",
          "[Removed Lines]",
          "17:     def __init__(self, title: str, url: str, status: int,",
          "18:                  classification: str, accuracy: float):",
          "",
          "[Added Lines]",
          "4: import http.client",
          "9: import phonenumbers",
          "11: from urllib import parse",
          "12: from tabulate import tabulate",
          "16: from .color import color",
          "22:     def __init__(self, title: str, url: str, status: int, classification: str, accuracy: float,",
          "23:                  numbers: list[str], emails: list[str]):",
          "30:         self.numbers = numbers",
          "31:         self.emails = emails",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "38:         Creates a node for a tree using the given ID which corresponds to a URL.",
          "39:         If the parent_id is None, this will be considered a root node.",
          "40:         \"\"\"",
          "42:         soup = BeautifulSoup(resp.text, 'html.parser')",
          "44:         try:",
          "45:             [classification, accuracy] = classify(resp.text)",
          "47:             self.create_node(title, identifier=id, parent=parent_id, data=data)",
          "48:         except exceptions.DuplicatedNodeIdError:",
          "49:             logging.debug(f\"found a duplicate URL {id}\")",
          "",
          "[Removed Lines]",
          "41:         resp = httpx.get(id, proxies='socks5://127.0.0.1:9050')",
          "43:         title = soup.title.text.strip() if soup.title is not None else id",
          "46:             data = LinkNode(title, id, resp.status_code, classification, accuracy)",
          "",
          "[Added Lines]",
          "49:         resp = httpx.get(id, timeout=60, proxies='socks5://127.0.0.1:9050')",
          "51:         title = soup.title.text.strip() if soup.title is not None else parse_hostname(id)",
          "54:             numbers = parse_phone_numbers(soup)",
          "55:             emails = parse_emails(soup)",
          "56:             data = LinkNode(title, id, resp.status_code, classification, accuracy, numbers, emails)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "54:         \"\"\"",
          "55:         if depth > 0:",
          "56:             depth -= 1",
          "58:             children = parse_links(resp.text)",
          "59:             for child in children:",
          "60:                 self._append_node(id=child, parent_id=url)",
          "61:                 self._build_tree(url=child, depth=depth)",
          "63:     def save(self) -> None:",
          "64:         \"\"\"",
          "65:         Saves the tree to the current working directory under the given file name.",
          "66:         \"\"\"",
          "72: def parse_links(html: str) -> list[str]:",
          "",
          "[Removed Lines]",
          "57:             resp = httpx.get(url, proxies='socks5://127.0.0.1:9050')",
          "67:         root_id = self.root",
          "68:         root_node = self.get_node(root_id)",
          "69:         self.save2file(os.path.join(project_root_directory, root_node.tag))",
          "",
          "[Added Lines]",
          "67:             resp = httpx.get(url, timeout=60, proxies='socks5://127.0.0.1:9050')",
          "73:     def _get_tree_file_name(self) -> str:",
          "74:         root_id = self.root",
          "75:         root_node = self.get_node(root_id)",
          "76:         if root_node is None:",
          "77:             raise Exception('no root node can be found.')",
          "79:         return os.path.join(project_root_directory, f'{root_node.tag} - Depth {self._depth}')",
          "85:         file_name = self._get_tree_file_name()",
          "86:         self.save2file(file_name)",
          "88:     def saveJSON(self) -> None:",
          "89:         \"\"\"",
          "90:         Saves the tree to the current working directory under the given file name in JSON.",
          "91:         \"\"\"",
          "92:         json_data = self.to_json()",
          "93:         with open(self._get_tree_file_name(), 'w+') as f:",
          "94:             f.write(json_data)",
          "96:     def showJSON(self) -> None:",
          "97:         \"\"\"",
          "98:         Prints tree to console as JSON",
          "99:         \"\"\"",
          "100:         json_data = self.to_json()",
          "101:         print(json_data)",
          "103:     def showTable(self) -> None:",
          "104:         \"\"\"",
          "105:         Prints the status of a link based on it's connection status",
          "106:         \"\"\"",
          "107:         nodes = self.all_nodes_itr()",
          "108:         table_data = []",
          "110:         def insert(node, color_code):",
          "111:             status = str(node.data.status)",
          "112:             code = http.client.responses[node.data.status]",
          "113:             status_message = f'{status} {code}'",
          "114:             table_data.append([",
          "115:                 node.tag,",
          "116:                 node.identifier,",
          "117:                 color(status_message, color_code),",
          "118:                 node.data.numbers,",
          "119:                 node.data.emails,",
          "120:                 node.data.classification,",
          "121:             ])",
          "123:         for node in nodes:",
          "124:             status_code = node.data.status",
          "125:             if status_code >= 200 and status_code < 300:",
          "126:                 insert(node, 'green')",
          "127:             elif status_code >= 300 and status_code < 400:",
          "128:                 insert(node, 'yellow')",
          "129:             else:",
          "130:                 insert(node, 'red')",
          "132:         headers = [\"Title\", \"URL\", \"Status\", \"Phone Numbers\", \"Emails\", \"Category\"]",
          "133:         table = tabulate(table_data, headers=headers)",
          "134:         print(table)",
          "137: def parse_hostname(url: str) -> str:",
          "138:     hostname = parse.urlsplit(url).hostname",
          "139:     if hostname is not None:",
          "140:         return hostname",
          "142:     raise Exception('unable to parse hostname from URL')",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "77:     tags = soup.find_all('a')",
          "78:     return [tag['href'] for tag in tags if tag.has_attr('href') and validators.url(tag['href'])]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "154: def parse_emails(soup: BeautifulSoup) -> list[str]:",
          "155:     \"\"\"",
          "156:     Finds all anchor tags and parses the email href attributes.",
          "157:     example attribute: `mailto:example@example.com`",
          "158:     \"\"\"",
          "159:     tags = soup.find_all('a')",
          "161:     emails = set()",
          "162:     for tag in tags:",
          "163:         if tag.has_attr('href') and 'mailto:' in tag['href']:",
          "164:             email = tag['href'].split('mailto:', 1)[1]",
          "165:             if validators.email(email):",
          "166:                 emails.add(set)",
          "168:     return list(emails)",
          "171: def parse_phone_numbers(soup: BeautifulSoup) -> list[str]:",
          "172:     \"\"\"",
          "173:     Finds all anchor tags and parses the href attribute.",
          "174:     \"\"\"",
          "175:     tags = soup.find_all('a')",
          "176:     numbers = set()",
          "177:     for tag in tags:",
          "178:         if tag.has_attr('href') and 'tel:' in tag['href']:",
          "179:             number = tag['href'].split('tel:', 1)[1]",
          "180:             try:",
          "181:                 if phonenumbers.is_valid_number(number):",
          "182:                     numbers.add(number)",
          "183:             except:",
          "184:                 pass",
          "186:             try:",
          "187:                 if phonenumbers.is_valid_number(tag['href']):",
          "188:                     numbers.add(tag['href'])",
          "189:             except:",
          "190:                 pass",
          "192:     return list(numbers)",
          "",
          "---------------"
        ],
        "torbot/modules/savefile.py||torbot/modules/savefile.py": [
          "File: torbot/modules/savefile.py -> torbot/modules/savefile.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "torbot/modules/tests/test_savetofile.py||torbot/modules/tests/test_savetofile.py": [
          "File: torbot/modules/tests/test_savetofile.py -> torbot/modules/tests/test_savetofile.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d9de8d3dd8ad27a4da739725331f71537b0f2144",
      "candidate_info": {
        "commit_hash": "d9de8d3dd8ad27a4da739725331f71537b0f2144",
        "repo": "DedSecInside/TorBot",
        "commit_url": "https://github.com/DedSecInside/TorBot/commit/d9de8d3dd8ad27a4da739725331f71537b0f2144",
        "files": [
          "torbot/main.py",
          "torbot/modules/config.py",
          "torbot/modules/info.py",
          "torbot/modules/linktree.py"
        ],
        "message": "flake8 fixes",
        "before_after_code_files": [
          "torbot/main.py||torbot/main.py",
          "torbot/modules/config.py||torbot/modules/config.py",
          "torbot/modules/info.py||torbot/modules/info.py",
          "torbot/modules/linktree.py||torbot/modules/linktree.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/DedSecInside/TorBot/pull/307"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "torbot/main.py||torbot/main.py": [
          "File: torbot/main.py -> torbot/main.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "121:     parser.add_argument(\"-q\", \"--quiet\", action=\"store_true\")",
          "122:     parser.add_argument(\"--version\", action=\"store_true\", help=\"Show current version of TorBot.\")",
          "123:     parser.add_argument(\"--update\", action=\"store_true\", help=\"Update TorBot to the latest stable version\")",
          "125:     parser.add_argument(\"-v\", action=\"store_true\", help=\"verbose logging\")",
          "127:     return parser",
          "",
          "[Removed Lines]",
          "124:     parser.add_argument(\"--info\", action=\"store_true\", help=\"Info displays basic info of the scanned site. Only supports a single URL at a time.\")",
          "",
          "[Added Lines]",
          "123:     parser.add_argument(\"--info\", action=\"store_true\",",
          "124:                         help=\"Info displays basic info of the scanned site. Only supports a single URL at a time.\")",
          "",
          "---------------"
        ],
        "torbot/modules/config.py||torbot/modules/config.py": [
          "File: torbot/modules/config.py -> torbot/modules/config.py"
        ],
        "torbot/modules/info.py||torbot/modules/info.py": [
          "File: torbot/modules/info.py -> torbot/modules/info.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "4: \"\"\"",
          "5: import re",
          "6: import httpx",
          "8: from urllib.parse import urlsplit",
          "9: from bs4 import BeautifulSoup",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "7: import logging",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "41:             attempts to terminal.",
          "42:     \"\"\"",
          "45:     soup = BeautifulSoup(resp.text, 'html.parser')",
          "46:     validation_functions = [",
          "47:         get_robots_txt, get_dot_git, get_dot_svn, get_dot_git, get_intel, get_dot_htaccess, get_bitcoin_address",
          "",
          "[Removed Lines]",
          "44:     resp = client.get(link)",
          "",
          "[Added Lines]",
          "45:     resp = client.get(url=link)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "49:     for validate_func in validation_functions:",
          "50:         try:",
          "51:             validate_func(client, link, resp)",
          "53:             cprint('Error', 'red')",
          "55:     display_webpage_description(soup)",
          "",
          "[Removed Lines]",
          "52:         except:",
          "",
          "[Added Lines]",
          "53:         except Exception as e:",
          "54:             logging.debug(e)",
          "",
          "---------------"
        ],
        "torbot/modules/linktree.py||torbot/modules/linktree.py": [
          "File: torbot/modules/linktree.py -> torbot/modules/linktree.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "183:             try:",
          "184:                 if phonenumbers.is_valid_number(number):",
          "185:                     numbers.add(number)",
          "187:                 pass",
          "189:             try:",
          "190:                 if phonenumbers.is_valid_number(tag['href']):",
          "191:                     numbers.add(tag['href'])",
          "193:                 pass",
          "195:     return list(numbers)",
          "",
          "[Removed Lines]",
          "186:             except:",
          "192:             except:",
          "",
          "[Added Lines]",
          "186:             except Exception as e:",
          "187:                 logging.debug(e)",
          "193:             except Exception as e:",
          "194:                 logging.debug(e)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8dd2b81c1480140c7e6ade2ec157dc59a3316217",
      "candidate_info": {
        "commit_hash": "8dd2b81c1480140c7e6ade2ec157dc59a3316217",
        "repo": "DedSecInside/TorBot",
        "commit_url": "https://github.com/DedSecInside/TorBot/commit/8dd2b81c1480140c7e6ade2ec157dc59a3316217",
        "files": [
          "README.md",
          "poetry.lock",
          "requirements.txt",
          "scripts/install.sh"
        ],
        "message": "Update README, scripts and dependency managers to reflect gotor changes",
        "before_after_code_files": [
          "poetry.lock||poetry.lock",
          "scripts/install.sh||scripts/install.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/DedSecInside/TorBot/pull/307"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "poetry.lock||poetry.lock": [
          "File: poetry.lock -> poetry.lock",
          "--- Hunk 1 ---",
          "[Context before]",
          "505: [[package]]",
          "506: name = \"setuptools\"",
          "508: description = \"Easily download, build, install, upgrade, and uninstall Python packages\"",
          "509: optional = false",
          "510: python-versions = \">=3.8\"",
          "511: files = [",
          "514: ]",
          "516: [package.extras]",
          "518: testing = [\"build[virtualenv]\", \"filelock (>=3.4.0)\", \"flake8-2020\", \"ini2toml[lite] (>=0.9)\", \"jaraco.develop (>=7.21)\", \"jaraco.envs (>=2.2)\", \"jaraco.path (>=3.2.0)\", \"pip (>=19.1)\", \"pytest (>=6)\", \"pytest-black (>=0.3.7)\", \"pytest-checkdocs (>=2.4)\", \"pytest-cov\", \"pytest-enabler (>=2.2)\", \"pytest-mypy (>=0.9.1)\", \"pytest-perf\", \"pytest-ruff\", \"pytest-timeout\", \"pytest-xdist\", \"tomli-w (>=1.0.0)\", \"virtualenv (>=13.0.0)\", \"wheel\"]",
          "521: [[package]]",
          "522: name = \"six\"",
          "",
          "[Removed Lines]",
          "507: version = \"68.1.2\"",
          "512:     {file = \"setuptools-68.1.2-py3-none-any.whl\", hash = \"sha256:3d8083eed2d13afc9426f227b24fd1659489ec107c0e86cec2ffdde5c92e790b\"},",
          "513:     {file = \"setuptools-68.1.2.tar.gz\", hash = \"sha256:3d4dfa6d95f1b101d695a6160a7626e15583af71a5f52176efa5d39a054d475d\"},",
          "517: docs = [\"furo\", \"jaraco.packaging (>=9.3)\", \"jaraco.tidelift (>=1.4)\", \"pygments-github-lexers (==0.0.5)\", \"rst.linker (>=1.9)\", \"sphinx (>=3.5,<=7.1.2)\", \"sphinx-favicon\", \"sphinx-hoverxref (<2)\", \"sphinx-inline-tabs\", \"sphinx-lint\", \"sphinx-notfound-page (==0.8.3)\", \"sphinx-reredirects\", \"sphinxcontrib-towncrier\"]",
          "519: testing-integration = [\"build[virtualenv]\", \"filelock (>=3.4.0)\", \"jaraco.envs (>=2.2)\", \"jaraco.path (>=3.2.0)\", \"pytest\", \"pytest-enabler\", \"pytest-xdist\", \"tomli\", \"virtualenv (>=13.0.0)\", \"wheel\"]",
          "",
          "[Added Lines]",
          "507: version = \"68.2.2\"",
          "512:     {file = \"setuptools-68.2.2-py3-none-any.whl\", hash = \"sha256:b454a35605876da60632df1a60f736524eb73cc47bbc9f3f1ef1b644de74fd2a\"},",
          "513:     {file = \"setuptools-68.2.2.tar.gz\", hash = \"sha256:4ac1475276d2f1c48684874089fefcd83bd7162ddaafb81fac866ba0db282a87\"},",
          "517: docs = [\"furo\", \"jaraco.packaging (>=9.3)\", \"jaraco.tidelift (>=1.4)\", \"pygments-github-lexers (==0.0.5)\", \"rst.linker (>=1.9)\", \"sphinx (>=3.5)\", \"sphinx-favicon\", \"sphinx-hoverxref (<2)\", \"sphinx-inline-tabs\", \"sphinx-lint\", \"sphinx-notfound-page (>=1,<2)\", \"sphinx-reredirects\", \"sphinxcontrib-towncrier\"]",
          "519: testing-integration = [\"build[virtualenv] (>=1.0.3)\", \"filelock (>=3.4.0)\", \"jaraco.envs (>=2.2)\", \"jaraco.path (>=3.2.0)\", \"packaging (>=23.1)\", \"pytest\", \"pytest-enabler\", \"pytest-xdist\", \"tomli\", \"virtualenv (>=13.0.0)\", \"wheel\"]",
          "",
          "---------------"
        ],
        "scripts/install.sh||scripts/install.sh": [
          "File: scripts/install.sh -> scripts/install.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "5: python -m pip install torbot",
          "6: echo",
          "7: echo \"TorBot installed. Run with 'python -m torbot'\"",
          "",
          "[Removed Lines]",
          "8: echo",
          "10: echo \"Setting GOPATH to access executable\"",
          "11: export PATH=${PATH}:`go env GOPATH`/bin",
          "12: echo \"New Path ${PATH}\"",
          "13: echo",
          "15: echo \"Installing gotor\"",
          "16: echo",
          "17: cd gotor/cmd/main",
          "18: go install gotor.go",
          "19: echo \"Gotor installed. Run with 'gotor'.\"",
          "21: cd ../../..",
          "",
          "[Added Lines]",
          "8: echo",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1fe7a7b0fc9b3d304705eb83636048cce5690000",
      "candidate_info": {
        "commit_hash": "1fe7a7b0fc9b3d304705eb83636048cce5690000",
        "repo": "DedSecInside/TorBot",
        "commit_url": "https://github.com/DedSecInside/TorBot/commit/1fe7a7b0fc9b3d304705eb83636048cce5690000",
        "files": [
          "poetry.lock",
          "pyproject.toml",
          "requirements.txt",
          "run.py",
          "torbot/main.py",
          "torbot/modules/api.py",
          "torbot/modules/collect_data.py",
          "torbot/modules/config.py",
          "torbot/modules/info.py",
          "torbot/modules/io.py",
          "torbot/modules/linktree.py",
          "torbot/modules/log.py",
          "torbot/modules/tests/test_pagereader.py"
        ],
        "message": "many changes",
        "before_after_code_files": [
          "poetry.lock||poetry.lock",
          "run.py||run.py",
          "torbot/main.py||torbot/main.py",
          "torbot/modules/api.py||torbot/modules/api.py",
          "torbot/modules/collect_data.py||torbot/modules/collect_data.py",
          "torbot/modules/config.py||torbot/modules/config.py",
          "torbot/modules/info.py||torbot/modules/info.py",
          "torbot/modules/link_io.py||torbot/modules/io.py",
          "torbot/modules/linktree.py||torbot/modules/linktree.py",
          "torbot/modules/log.py||torbot/modules/log.py",
          "torbot/modules/tests/test_pagereader.py||torbot/modules/tests/test_pagereader.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/DedSecInside/TorBot/pull/307"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "poetry.lock||poetry.lock": [
          "File: poetry.lock -> poetry.lock",
          "--- Hunk 1 ---",
          "[Context before]",
          "311:     {file = \"pefile-2023.2.7.tar.gz\", hash = \"sha256:82e6114004b3d6911c77c3953e3838654b04511b8b66e8583db70c65998017dc\"},",
          "312: ]",
          "314: [[package]]",
          "315: name = \"progress\"",
          "316: version = \"1.6\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "314: [[package]]",
          "315: name = \"phonenumbers\"",
          "316: version = \"8.13.22\"",
          "317: description = \"Python version of Google's common library for parsing, formatting, storing and validating international phone numbers.\"",
          "318: optional = false",
          "319: python-versions = \"*\"",
          "320: files = [",
          "321:     {file = \"phonenumbers-8.13.22-py2.py3-none-any.whl\", hash = \"sha256:85ceeba9e67984ba98182c77e8e4c70093d38c0c6a0cb2bd392e0694ddaeb1f6\"},",
          "322:     {file = \"phonenumbers-8.13.22.tar.gz\", hash = \"sha256:001664c90f59b8954766c2db85adafc8dbc96177efeb49607ca4e64a7acaf569\"},",
          "323: ]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "703: [metadata]",
          "704: lock-version = \"2.0\"",
          "705: python-versions = \">=3.9,<=3.11.4\"",
          "",
          "[Removed Lines]",
          "706: content-hash = \"bc665d85d8bb2537f084f64260e0b84212b7917a530ff79d8c8c9dd896c015d5\"",
          "",
          "[Added Lines]",
          "717: content-hash = \"7b3ae36389472ec97dd5aacc437381b5c7f13f3d08e4ab738ef699b46c85a17a\"",
          "",
          "---------------"
        ],
        "run.py||run.py": [
          "File: run.py -> run.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "torbot/main.py||torbot/main.py": [
          "File: torbot/main.py -> torbot/main.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "3: \"\"\"",
          "4: import argparse",
          "5: import sys",
          "18: VERSION = '3.1.2'",
          "111:         print_tor_ip_address()",
          "133:     \"\"\"",
          "134:     Parses user flags passed to TorBot",
          "135:     \"\"\"",
          "136:     parser = argparse.ArgumentParser(prog=\"TorBot\", usage=\"Gather and analayze data from Tor sites.\")",
          "139:     parser.add_argument(\"-q\", \"--quiet\", action=\"store_true\")",
          "142:     parser.add_argument(\"-m\", \"--mail\", action=\"store_true\", help=\"Get e-mail addresses from the crawled sites\")",
          "143:     parser.add_argument(\"-p\", \"--phone\", action=\"store_true\", help=\"Get phone numbers from the crawled sites\")",
          "166: if __name__ == '__main__':",
          "167:     try:",
          "171:     except KeyboardInterrupt:",
          "172:         print(\"Interrupt received! Exiting cleanly...\")",
          "",
          "[Removed Lines]",
          "7: from .modules import link_io",
          "9: from .modules.link_io import pprint_tree, print_tor_ip_address",
          "10: from .modules.api import get_node",
          "11: from .modules.color import color",
          "12: from .modules.updater import check_version",
          "13: from .modules.savefile import saveJson",
          "14: from .modules.info import execute_all",
          "15: from .modules.collect_data import collect_data",
          "16: from .modules.nlp import main",
          "21: # TorBot CLI class",
          "22: class TorBot:",
          "24:     def __init__(self, args):",
          "25:         self.args = args",
          "26:         self.__version__ = VERSION",
          "28:     def get_header(self):",
          "29:         license_msg = color(\"LICENSE: GNU Public License v3\", \"red\")",
          "30:         banner = r\"\"\"",
          "31:                               __  ____  ____  __        ______",
          "32:                              / /_/ __ \\/ __ \\/ /_  ____/_  __/",
          "33:                             / __/ / / / /_/ / __ \\/ __ \\/ /",
          "34:                            / /_/ /_/ / _, _/ /_/ / /_/ / /",
          "35:                            \\__/\\____/_/ |_/_____/\\____/_/  V{VERSION}",
          "36:                 \"\"\".format(VERSION=self.__version__)",
          "37:         banner = color(banner, \"red\")",
          "39:         title = r\"\"\"",
          "40:                                         {banner}",
          "41:                         #######################################################",
          "42:                         #  TorBot - Dark Web OSINT Tool                       #",
          "43:                         #  GitHub : https://github.com/DedsecInside/TorBot    #",
          "44:                         #  Help : use -h for help text                        #",
          "45:                         #######################################################",
          "46:                                     {license_msg}",
          "47:                 \"\"\"",
          "49:         title = title.format(license_msg=license_msg, banner=banner)",
          "50:         print(title)",
          "52:     def handle_json_args(self, args):",
          "53:         \"\"\"",
          "54:         Outputs JSON file for data",
          "55:         \"\"\"",
          "57:         # -m/--mail",
          "58:         if args.mail:",
          "59:             email_json = link_io.print_emails(args.url)",
          "60:             if args.save:",
          "61:                 saveJson('Emails', email_json)",
          "62:         # -p/--phone",
          "63:         if args.phone:",
          "64:             phone_json = link_io.print_phones(args.url)",
          "65:             if args.save:",
          "66:                 saveJson('Phones', phone_json)",
          "67:         # -s/--save",
          "68:         else:",
          "69:             node_json = link_io.print_json(args.url, args.depth)",
          "70:             saveJson(\"Links\", node_json)",
          "72:     def handle_tree_args(self, args):",
          "73:         \"\"\"",
          "74:         Outputs tree visual for data",
          "75:         \"\"\"",
          "76:         '''",
          "77:         # -v/--visualize",
          "78:         if args.visualize:",
          "79:             tree.show()",
          "81:         # -d/--download",
          "82:         if args.download:",
          "83:             file_name = str(input(\"File Name (.txt): \"))",
          "84:             tree.save(file_name)",
          "85:             '''",
          "87:     def perform_action(self):",
          "88:         args = self.args",
          "90:         # If url flag is set then check for accompanying flag set. Only one",
          "91:         # additional flag can be set with -u/--url flag",
          "92:         if not args.url:",
          "93:             print(\"usage: See run.py -h for possible arguments.\")",
          "94:             sys.exit()",
          "96:         if args.gather:",
          "97:             collect_data(args.url)",
          "98:             sys.exit()",
          "100:         # If flag is -v, --update, -q/--quiet then user only runs that operation",
          "101:         # because these are single flags only",
          "102:         if args.version:",
          "103:             print(f\"TorBot Version: {self.__version__}\")",
          "104:             sys.exit()",
          "105:         if args.update:",
          "106:             check_version()",
          "107:             sys.exit()",
          "108:         if not args.quiet:",
          "109:             self.get_header()",
          "113:         tree = get_node(args.url, args.depth)",
          "115:         if args.classify:",
          "116:             result = main.classify(args.url)",
          "117:             print(\"Website Classification: \" + result[0], \"| Accuracy: \" + str(result[1]))",
          "118:         if args.visualize or args.download:",
          "119:             self.handle_tree_args(args)",
          "120:             # raise NotImplementedError(\"Tree visualization and download is not available yet.\")",
          "121:         elif args.save or args.mail or args.phone:",
          "122:             self.handle_json_args(args)",
          "123:         # -i/--info",
          "124:         elif args.info:",
          "125:             execute_all(args.url)",
          "126:         else:",
          "127:             if args.url:",
          "128:                 pprint_tree(tree)",
          "129:         print(\"\\n\\n\")",
          "132: def get_args():",
          "137:     parser.add_argument(\"--version\", action=\"store_true\", help=\"Show current version of TorBot.\")",
          "138:     parser.add_argument(\"--update\", action=\"store_true\", help=\"Update TorBot to the latest stable version\")",
          "140:     parser.add_argument(\"-u\", \"--url\", help=\"Specifiy a website link to crawl\")",
          "141:     parser.add_argument(\"-s\", \"--save\", action=\"store_true\", help=\"Save results in a file\")",
          "144:     parser.add_argument(\"--depth\", type=int, help=\"Specifiy max depth of crawler (default 1)\", default=1)",
          "145:     parser.add_argument(\"--gather\", action=\"store_true\", help=\"Gather data for analysis\")",
          "146:     parser.add_argument(\"-v\", \"--visualize\", action=\"store_true\", help=\"Visualizes tree of data gathered.\")",
          "147:     parser.add_argument(\"-d\", \"--download\", action=\"store_true\", help=\"Downloads tree of data gathered.\")",
          "148:     parser.add_argument(",
          "149:         \"-e\",",
          "150:         \"--extension\",",
          "151:         action='append',",
          "152:         dest='extension',",
          "153:         default=[],",
          "154:         help=' '.join((\"Specifiy additional website\", \"extensions to the list(.com , .org, .etc)\"))",
          "155:     )",
          "156:     parser.add_argument(\"-c\", \"--classify\", action=\"store_true\", help=\"Classify the webpage using NLP module\")",
          "157:     parser.add_argument(",
          "158:         \"-cAll\", \"--classifyAll\", action=\"store_true\", help=\"Classify all the obtained webpages using NLP module\"",
          "159:     )",
          "160:     parser.add_argument(",
          "161:         \"-i\", \"--info\", action=\"store_true\", help=' '.join((\"Info displays basic info of the scanned site\"))",
          "162:     )",
          "163:     return parser.parse_args()",
          "168:         args = get_args()",
          "169:         torbot = TorBot(args)",
          "170:         torbot.perform_action()",
          "",
          "[Added Lines]",
          "6: import logging",
          "8: from modules.io import pprint_tree, print_tor_ip_address",
          "9: from modules.color import color",
          "10: from modules.updater import check_version",
          "11: from modules.info import execute_all",
          "12: from modules.linktree import LinkTree",
          "17: def print_header() -> None:",
          "18:     \"\"\"",
          "19:     Prints the TorBot banner including version and license.",
          "20:     \"\"\"",
          "21:     license_msg = color(\"LICENSE: GNU Public License v3\", \"red\")",
          "22:     banner = r\"\"\"",
          "23:                             __  ____  ____  __        ______",
          "24:                             / /_/ __ \\/ __ \\/ /_  ____/_  __/",
          "25:                         / __/ / / / /_/ / __ \\/ __ \\/ /",
          "26:                         / /_/ /_/ / _, _/ /_/ / /_/ / /",
          "27:                         \\__/\\____/_/ |_/_____/\\____/_/  V{VERSION}",
          "28:             \"\"\".format(VERSION=VERSION)",
          "29:     banner = color(banner, \"red\")",
          "31:     title = r\"\"\"",
          "32:                                     {banner}",
          "33:                     #######################################################",
          "34:                     #  TorBot - Dark Web OSINT Tool                       #",
          "35:                     #  GitHub : https://github.com/DedsecInside/TorBot    #",
          "36:                     #  Help : use -h for help text                        #",
          "37:                     #######################################################",
          "38:                                 {license_msg}",
          "39:             \"\"\"",
          "41:     title = title.format(license_msg=license_msg, banner=banner)",
          "42:     print(title)",
          "45: def run(arg_parser: argparse.ArgumentParser) -> None:",
          "46:     args = arg_parser.parse_args()",
          "48:     # setup logging",
          "49:     date_fmt = '%d-%b-%y %H:%M:%S'",
          "50:     logging_fmt = '%(asctime)s - %(levelname)s - %(message)s'",
          "51:     logging_lvl = logging.DEBUG if args.v else logging.INFO",
          "52:     logging.basicConfig(level=logging_lvl, format=logging_fmt, datefmt=date_fmt)",
          "54:     # URL is a required argument",
          "55:     if not args.url:",
          "56:         arg_parser.print_help()",
          "57:         sys.exit()",
          "59:     # Print verison then exit",
          "60:     if args.version:",
          "61:         print(f\"TorBot Version: {VERSION}\")",
          "62:         sys.exit()",
          "64:     # check version and update if necessary",
          "65:     if args.update:",
          "66:         check_version()",
          "67:         sys.exit()",
          "69:     # print header and IP address if not set to quiet",
          "70:     if not args.quiet:",
          "71:         print_header()",
          "74:     if args.info:",
          "75:         execute_all(args.url)",
          "77:     tree = LinkTree(url=args.url, depth=args.depth)",
          "78:     tree.load()",
          "79:     # save tree and continue",
          "80:     if args.save:",
          "81:         tree.save()",
          "83:     if args.visualize:",
          "84:         tree.show()",
          "86:     pprint_tree(tree)",
          "87:     '''",
          "88:     elif args.save or args.mail or args.phone:",
          "89:         self.handle_json_args(args)",
          "90:     '''",
          "91:     print(\"\\n\\n\")",
          "94: def set_arguments() -> argparse.ArgumentParser:",
          "99:     parser.add_argument(\"-u\", \"--url\", type=str, required=True, help=\"Specifiy a website link to crawl\")",
          "100:     parser.add_argument(\"--depth\", type=int, help=\"Specifiy max depth of crawler (default 1)\", default=1)",
          "104:     parser.add_argument(\"--version\", action=\"store_true\", help=\"Show current version of TorBot.\")",
          "105:     parser.add_argument(\"--update\", action=\"store_true\", help=\"Update TorBot to the latest stable version\")",
          "106:     parser.add_argument(\"--save\", action=\"store_true\", help=\"Save results in a file\")",
          "107:     parser.add_argument(\"--info\", action=\"store_true\", help=\"Info displays basic info of the scanned site. Only supports a single URL at a time.\")",
          "108:     parser.add_argument(\"--visualize\", action=\"store_true\", help=\"Visualizes tree of data gathered.\")",
          "109:     parser.add_argument(\"-v\", action=\"store_true\", help=\"verbose logging\")",
          "111:     return parser",
          "116:         arg_parser = set_arguments()",
          "117:         run(arg_parser)",
          "",
          "---------------"
        ],
        "torbot/modules/api.py||torbot/modules/api.py": [
          "File: torbot/modules/api.py -> torbot/modules/api.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "6: import httpx",
          "7: import logging",
          "10: from bs4 import BeautifulSoup, Tag",
          "17: logging.getLogger(\"httpx\").setLevel(logging.WARNING)",
          "30: def get_ip() -> dict:",
          "31:     \"\"\"",
          "32:     Returns the IP address of the current Tor client the service is using.",
          "33:     \"\"\"",
          "34:     resp = httpx.get(\"https://check.torproject.org/\", proxies='socks5://127.0.0.1:9050')",
          "37:     # Get the content of check tor project, this contains the header and body",
          "38:     content = soup.find(\"div\", {\"class\": \"content\"})",
          "",
          "[Removed Lines]",
          "9: from treelib import Tree",
          "12: from .config import host, port",
          "13: from .linktree import append_node, build_tree",
          "15: base_url: str = f'http://{host}:{port}'",
          "20: def get_node(url: str, depth: int):",
          "21:     \"\"\"",
          "22:     Returns the LinkTree for the given link at the specified depth.",
          "23:     \"\"\"",
          "24:     tree = Tree()",
          "25:     append_node(tree, id=url, parent_id=None)",
          "26:     build_tree(tree, url, depth)",
          "27:     return tree",
          "35:     soup = BeautifulSoup(resp.text, features='html.parser')",
          "",
          "[Added Lines]",
          "20:     soup = BeautifulSoup(resp.text, 'html.parser')",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "56:     body = body_tag.get_text().strip()",
          "58:     return {\"header\": header, \"body\": body}",
          "",
          "[Removed Lines]",
          "61: def get_emails(link: str):",
          "62:     \"\"\"",
          "63:     Returns the mailto links found on the page.",
          "64:     \"\"\"",
          "65:     endpoint = f'/emails?link={link}'",
          "66:     url = base_url + endpoint",
          "67:     resp = httpx.get(url)",
          "68:     data = resp.json()",
          "69:     return data",
          "72: def get_phone(link: str):",
          "73:     \"\"\"",
          "74:     Returns the tel links found on the page.",
          "75:     \"\"\"",
          "76:     endpoint = f'/phone?link={link}'",
          "77:     url = base_url + endpoint",
          "78:     resp = httpx.get(url)",
          "79:     data = resp.json()",
          "80:     return data",
          "83: def get_web_content(link: str):",
          "84:     \"\"\"",
          "85:     Returns the HTML content of the page.",
          "86:     \"\"\"",
          "87:     endpoint = f'/content?link={link}'",
          "88:     url = base_url + endpoint",
          "89:     resp = httpx.get(url)",
          "90:     return resp.text",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "torbot/modules/collect_data.py||torbot/modules/collect_data.py": [
          "File: torbot/modules/collect_data.py -> torbot/modules/collect_data.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "torbot/modules/config.py||torbot/modules/config.py": [
          "File: torbot/modules/config.py -> torbot/modules/config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "5: from inspect import getsourcefile",
          "6: from unipath import Path",
          "10: modules_directory = Path(config_file_path).parent",
          "11: torbot_directory = modules_directory.parent",
          "12: project_root_directory = torbot_directory.parent",
          "13: dotenv_path = os.path.join(project_root_directory, '.env')",
          "14: load_dotenv(dotenv_path=dotenv_path, verbose=True)",
          "30: def get_data_directory():",
          "31:     data_directory = os.getenv('TORBOT_DATA_DIR')",
          "32:     # if a path is not set, write data to the config directory",
          "",
          "[Removed Lines]",
          "9: config_file_path = (os.path.abspath(getsourcefile(lambda: 0)))",
          "16: port = os.getenv(\"PORT\")",
          "17: host = os.getenv(\"HOST\")",
          "20: def get_log_level() -> int:",
          "21:     log_level_str = os.getenv('LOG_LEVEL')",
          "22:     if log_level_str:",
          "23:         log_level_str = log_level_str.lower()",
          "24:         mapping = logging.getLevelNamesMapping()",
          "25:         if log_level_str in mapping:",
          "26:             return mapping[log_level_str]",
          "27:     return logging.INFO",
          "",
          "[Added Lines]",
          "8: source_file = getsourcefile(lambda: 0)",
          "9: config_file_path = None",
          "10: if isinstance(source_file, str):",
          "11:     config_file_path = (os.path.abspath(source_file))",
          "13: if not config_file_path:",
          "14:     raise Exception('Unable to load environment.')",
          "",
          "---------------"
        ],
        "torbot/modules/info.py||torbot/modules/info.py": [
          "File: torbot/modules/info.py -> torbot/modules/info.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "3: and saving data to file.",
          "4: \"\"\"",
          "5: import re",
          "7: from urllib.parse import urlsplit",
          "8: from bs4 import BeautifulSoup",
          "9: from termcolor import cprint",
          "10: from requests.exceptions import HTTPError",
          "14: keys = set()  # high entropy strings, prolly secret keys",
          "",
          "[Removed Lines]",
          "11: from .api import get_web_content",
          "",
          "[Added Lines]",
          "6: import httpx",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "42:             attempts to terminal.",
          "43:     \"\"\"",
          "47:     validation_functions = [",
          "48:         get_robots_txt, get_dot_git, get_dot_svn, get_dot_git, get_intel, get_dot_htaccess, get_bitcoin_address",
          "49:     ]",
          "50:     for validate_func in validation_functions:",
          "51:         try:",
          "53:         except (ConnectionError, HTTPError):",
          "54:             cprint('Error', 'red')",
          "",
          "[Removed Lines]",
          "45:     response = get_web_content(link)",
          "46:     soup = BeautifulSoup(response, 'html.parser')",
          "52:             validate_func(link, response)",
          "",
          "[Added Lines]",
          "45:     resp = httpx.get(link, proxies='socks5://127.0.0.1:9050')",
          "46:     soup = BeautifulSoup(resp.text, 'html.parser')",
          "52:             validate_func(link, resp)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "81:     cprint(\"[*]Checking for Robots.txt\", 'yellow')",
          "82:     url = target",
          "83:     target = \"{0.scheme}://{0.netloc}/\".format(urlsplit(url))",
          "85:     print(target + \"robots.txt\")",
          "86:     matches = re.findall(r'Allow: (.*)|Disallow: (.*)', response)",
          "87:     for match in matches:",
          "",
          "[Removed Lines]",
          "84:     get_web_content(target + \"robots.txt\")",
          "",
          "[Added Lines]",
          "84:     httpx.get(target + \"robots.txt\", proxies='socks5://127.0.0.1:9050')",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "119:     cprint(\"[*]Checking for .git folder\", 'yellow')",
          "120:     url = target",
          "121:     target = \"{0.scheme}://{0.netloc}/\".format(urlsplit(url))",
          "124:         cprint(\"Alert!\", 'red')",
          "125:         cprint(\".git folder exposed publicly\", 'red')",
          "126:     else:",
          "",
          "[Removed Lines]",
          "122:     resp = get_web_content(target + \"/.git/config\")",
          "123:     if not resp.__contains__(\"404\"):",
          "",
          "[Added Lines]",
          "122:     resp = httpx.get(target + \"/.git/config\", proxies='socks5://127.0.0.1:9050')",
          "123:     if not resp.text.__contains__(\"404\"):",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "150:     cprint(\"[*]Checking for .svn folder\", 'yellow')",
          "151:     url = target",
          "152:     target = \"{0.scheme}://{0.netloc}/\".format(urlsplit(url))",
          "155:         cprint(\"Alert!\", 'red')",
          "156:         cprint(\".SVN folder exposed publicly\", 'red')",
          "157:     else:",
          "",
          "[Removed Lines]",
          "153:     resp = get_web_content(target + \"/.svn/entries\")",
          "154:     if not resp.__contains__(\"404\"):",
          "",
          "[Added Lines]",
          "153:     resp = httpx.get(target + \"/.svn/entries\", proxies='socks5://127.0.0.1:9050')",
          "154:     if not resp.text.__contains__(\"404\"):",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "168:     cprint(\"[*]Checking for .htaccess\", 'yellow')",
          "169:     url = target",
          "170:     target = \"{0.scheme}://{0.netloc}/\".format(urlsplit(url))",
          "173:         cprint(\"403 Forbidden\", 'blue')",
          "175:         cprint(\"Alert!!\", 'blue')",
          "176:         cprint(\".htaccess file found!\", 'blue')",
          "177:     else:",
          "",
          "[Removed Lines]",
          "171:     resp = get_web_content(target + \"/.htaccess\")",
          "172:     if resp.__contains__(\"403\"):",
          "174:     elif not resp.__contains__(\"404\") or resp.__contains__(\"500\"):",
          "",
          "[Added Lines]",
          "171:     resp = httpx.get(target + \"/.htaccess\", proxies='socks5://127.0.0.1:9050')",
          "172:     if resp.text.__contains__(\"403\"):",
          "174:     elif not resp.text.__contains__(\"404\") or resp.text.__contains__(\"500\"):",
          "",
          "---------------"
        ],
        "torbot/modules/link_io.py||torbot/modules/io.py": [
          "File: torbot/modules/link_io.py -> torbot/modules/io.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "6: import tabulate",
          "8: from pprint import pprint",
          "12: from .color import color",
          "",
          "[Removed Lines]",
          "9: from treelib import Tree",
          "11: from .api import get_node, get_emails, get_phone, get_ip",
          "",
          "[Added Lines]",
          "9: from .linktree import LinkTree",
          "11: from .api import get_ip",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "22:     print(color(resp[\"body\"], \"yellow\"))",
          "26:     \"\"\"",
          "27:     Prints the status of a link based on it's connection status",
          "28:     \"\"\"",
          "",
          "[Removed Lines]",
          "25: def pprint_tree(tree: Tree) -> None:",
          "",
          "[Added Lines]",
          "25: def pprint_tree(tree: LinkTree) -> None:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "54:     print(table)",
          "58:     \"\"\"",
          "59:     Prints the JSON representation of a Link node.",
          "61:     Returns:",
          "62:         root (dict): Dictionary containing the root node and it's children",
          "63:     \"\"\"",
          "",
          "[Removed Lines]",
          "57: def print_json(url: str, depth: int = 1):",
          "64:     root = get_node(url, depth)",
          "65:     print(root.to_json())",
          "68: def print_emails(url: str):",
          "69:     \"\"\"",
          "70:     Prints any emails found within the HTML content of this url.",
          "72:     Returns:",
          "73:         emails (list): list of emails",
          "74:     \"\"\"",
          "75:     email_list = get_emails(url)",
          "76:     pprint(email_list)",
          "77:     return email_list",
          "80: def print_phones(url: str):",
          "81:     \"\"\"",
          "82:     Prints any phones found within the HTML content of this url.",
          "84:     Returns:",
          "85:         phones (list): list of phones",
          "86:     \"\"\"",
          "87:     phone_list = get_phone(url)",
          "88:     pprint(phone_list)",
          "89:     return phone_list",
          "",
          "[Added Lines]",
          "57: def print_json(url: str, depth: int = 1) -> None:",
          "64:     tree = LinkTree(url=url, depth=depth)",
          "65:     tree.load()",
          "66:     print(tree.to_json())",
          "",
          "---------------"
        ],
        "torbot/modules/linktree.py||torbot/modules/linktree.py": [
          "File: torbot/modules/linktree.py -> torbot/modules/linktree.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "9: from treelib import Tree, exceptions, Node",
          "10: from bs4 import BeautifulSoup",
          "12: from .nlp.main import classify",
          "17:         self.identifier = url",
          "18:         self.tag = title",
          "19:         self.status = status",
          "20:         self.classification = classification",
          "21:         self.accuracy = accuracy",
          "24: def parse_links(html: str) -> list[str]:",
          "25:     \"\"\"",
          "",
          "[Removed Lines]",
          "15: class Link(Node):",
          "16:     def __init__(self, title: str, url: str, status: int, classification: str, accuracy: float):",
          "",
          "[Added Lines]",
          "12: from .config import project_root_directory",
          "16: class LinkNode(Node):",
          "17:     def __init__(self, title: str, url: str, status: int,",
          "18:                  classification: str, accuracy: float):",
          "19:         super().__init__()",
          "26: class LinkTree(Tree):",
          "27:     def __init__(self, url: str, depth: int) -> None:",
          "28:         super().__init__()",
          "29:         self._url = url",
          "30:         self._depth = depth",
          "32:     def load(self) -> None:",
          "33:         self._append_node(id=self._url, parent_id=None)",
          "34:         self._build_tree(url=self._url, depth=self._depth)",
          "36:     def _append_node(self, id: str, parent_id: str | None) -> None:",
          "37:         \"\"\"",
          "38:         Creates a node for a tree using the given ID which corresponds to a URL.",
          "39:         If the parent_id is None, this will be considered a root node.",
          "40:         \"\"\"",
          "41:         resp = httpx.get(id, proxies='socks5://127.0.0.1:9050')",
          "42:         soup = BeautifulSoup(resp.text, 'html.parser')",
          "43:         title = soup.title.text.strip() if soup.title is not None else id",
          "44:         try:",
          "45:             [classification, accuracy] = classify(resp.text)",
          "46:             data = LinkNode(title, id, resp.status_code, classification, accuracy)",
          "47:             self.create_node(title, identifier=id, parent=parent_id, data=data)",
          "48:         except exceptions.DuplicatedNodeIdError:",
          "49:             logging.debug(f\"found a duplicate URL {id}\")",
          "51:     def _build_tree(self,  url: str, depth: int) -> None:",
          "52:         \"\"\"",
          "53:         Builds a tree from the root to the given depth.",
          "54:         \"\"\"",
          "55:         if depth > 0:",
          "56:             depth -= 1",
          "57:             resp = httpx.get(url, proxies='socks5://127.0.0.1:9050')",
          "58:             children = parse_links(resp.text)",
          "59:             for child in children:",
          "60:                 self._append_node(id=child, parent_id=url)",
          "61:                 self._build_tree(url=child, depth=depth)",
          "63:     def save(self) -> None:",
          "64:         \"\"\"",
          "65:         Saves the tree to the current working directory under the given file name.",
          "66:         \"\"\"",
          "67:         root_id = self.root",
          "68:         root_node = self.get_node(root_id)",
          "69:         self.save2file(os.path.join(project_root_directory, root_node.tag))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "29:     tags = soup.find_all('a')",
          "30:     return [tag['href'] for tag in tags if tag.has_attr('href') and validators.url(tag['href'])]",
          "",
          "[Removed Lines]",
          "33: def append_node(tree: Tree, id: str, parent_id: str | None) -> None:",
          "34:     \"\"\"",
          "35:     Creates a node for a tree using the given ID which corresponds to a URL.",
          "36:     If the parent_id is None, this will be considered a root node.",
          "37:     \"\"\"",
          "38:     resp = httpx.get(id, proxies='socks5://127.0.0.1:9050')",
          "39:     soup = BeautifulSoup(resp.text, 'html.parser')",
          "40:     title = soup.title.text.strip() if soup.title is not None else id",
          "41:     try:",
          "42:         [classification, accuracy] = classify(resp.text)",
          "43:         data = Link(title, id, resp.status_code, classification, accuracy)",
          "44:         tree.create_node(title, identifier=id, parent=parent_id, data=data)",
          "45:     except exceptions.DuplicatedNodeIdError:",
          "46:         logging.debug(f\"found a duplicate URL {id}\")",
          "49: def build_tree(tree: Tree, url: str, depth: int) -> None:",
          "50:     \"\"\"",
          "51:     Builds a tree from the root to the given depth.",
          "52:     \"\"\"",
          "53:     if depth > 0:",
          "54:         depth -= 1",
          "55:         resp = httpx.get(url, proxies='socks5://127.0.0.1:9050')",
          "56:         children = parse_links(resp.text)",
          "57:         for child in children:",
          "58:             append_node(tree, id=child, parent_id=url)",
          "59:             build_tree(tree, child, depth)",
          "62: def save(tree: Tree, file_name: str) -> None:",
          "63:     \"\"\"",
          "64:     Saves the tree to the current working directory under the given file name.",
          "65:     \"\"\"",
          "66:     tree.save2file(os.path.join(os.getcwd(), file_name))",
          "69: def show(tree: Tree) -> None:",
          "70:     \"\"\"",
          "71:     Prints the tree",
          "72:     \"\"\"",
          "73:     tree.show()",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "torbot/modules/log.py||torbot/modules/log.py": [
          "File: torbot/modules/log.py -> torbot/modules/log.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "torbot/modules/tests/test_pagereader.py||torbot/modules/tests/test_pagereader.py": [
          "File: torbot/modules/tests/test_pagereader.py -> torbot/modules/tests/test_pagereader.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "5: import requests_mock",
          "7: from yattag import Doc",
          "11: @pytest.fixture",
          "",
          "[Removed Lines]",
          "8: from ..link_io import LinkIO",
          "",
          "[Added Lines]",
          "8: from ..io import LinkIO",
          "",
          "---------------"
        ]
      }
    }
  ]
}