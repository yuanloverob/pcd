{
  "cve_id": "CVE-2023-45813",
  "cve_desc": "Torbot is an open source tor network intelligence tool. In affected versions the `torbot.modules.validators.validate_link function` uses the python-validators URL validation regex. This particular regular expression has an exponential complexity which allows an attacker to cause an application crash using a well-crafted argument. An attacker can use a well-crafted URL argument to exploit the vulnerability in the regular expression and cause a Denial of Service on the system. The validators file has been removed in version 4.0.0. Users are advised to upgrade. There are no known workarounds for this vulnerability.",
  "repo": "DedSecInside/TorBot",
  "patch_hash": "ef6e06bc7785355b1701d5524eb4550441086ac4",
  "patch_info": {
    "commit_hash": "ef6e06bc7785355b1701d5524eb4550441086ac4",
    "repo": "DedSecInside/TorBot",
    "commit_url": "https://github.com/DedSecInside/TorBot/commit/ef6e06bc7785355b1701d5524eb4550441086ac4",
    "files": [
      "torbot/modules/validators.py"
    ],
    "message": "remove unused validators file",
    "before_after_code_files": [
      "torbot/modules/validators.py||torbot/modules/validators.py"
    ]
  },
  "patch_diff": {
    "torbot/modules/validators.py||torbot/modules/validators.py": [
      "File: torbot/modules/validators.py -> torbot/modules/validators.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "[No context available]",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "6f3f4dbf2de2ac4ea34e82a7436a0cf3af8722af",
      "candidate_info": {
        "commit_hash": "6f3f4dbf2de2ac4ea34e82a7436a0cf3af8722af",
        "repo": "DedSecInside/TorBot",
        "commit_url": "https://github.com/DedSecInside/TorBot/commit/6f3f4dbf2de2ac4ea34e82a7436a0cf3af8722af",
        "files": [
          "torbot/main.py"
        ],
        "message": "Add option to disable socks5",
        "before_after_code_files": [
          "torbot/main.py||torbot/main.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/DedSecInside/TorBot/pull/307"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "torbot/main.py||torbot/main.py": [
          "File: torbot/main.py -> torbot/main.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "79:         sys.exit()",
          "81:     socks5_proxy = f'socks5://{socks5_host}:{socks5_port}'",
          "83:         # print header and IP address if not set to quiet",
          "84:         if not args.quiet:",
          "85:             print_header(version)",
          "",
          "[Removed Lines]",
          "82:     with httpx.Client(timeout=60, proxies=socks5_proxy) as client:",
          "",
          "[Added Lines]",
          "82:     with httpx.Client(timeout=60, proxies=socks5_proxy if not args.disable_socks5 else None) as client:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "123:     parser.add_argument(\"--info\", action=\"store_true\",",
          "124:                         help=\"Info displays basic info of the scanned site. Only supports a single URL at a time.\")",
          "125:     parser.add_argument(\"-v\", action=\"store_true\", help=\"verbose logging\")",
          "127:     return parser",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "126:     parser.add_argument(\"--disable-socks5\", action=\"store_true\", help=\"Executes HTTP requests without using SOCKS5 proxy\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "3a967b960a0ff987bfd69cdb7decc5a4d3092565",
      "candidate_info": {
        "commit_hash": "3a967b960a0ff987bfd69cdb7decc5a4d3092565",
        "repo": "DedSecInside/TorBot",
        "commit_url": "https://github.com/DedSecInside/TorBot/commit/3a967b960a0ff987bfd69cdb7decc5a4d3092565",
        "files": [
          "torbot/main.py"
        ],
        "message": "flake8",
        "before_after_code_files": [
          "torbot/main.py||torbot/main.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/DedSecInside/TorBot/pull/307"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "torbot/main.py||torbot/main.py": [
          "File: torbot/main.py -> torbot/main.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "123:     parser.add_argument(\"--info\", action=\"store_true\",",
          "124:                         help=\"Info displays basic info of the scanned site. Only supports a single URL at a time.\")",
          "125:     parser.add_argument(\"-v\", action=\"store_true\", help=\"verbose logging\")",
          "128:     return parser",
          "",
          "[Removed Lines]",
          "126:     parser.add_argument(\"--disable-socks5\", action=\"store_true\", help=\"Executes HTTP requests without using SOCKS5 proxy\")",
          "",
          "[Added Lines]",
          "126:     parser.add_argument(\"--disable-socks5\", action=\"store_true\",",
          "127:                         help=\"Executes HTTP requests without using SOCKS5 proxy\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d727061f73859219b9b5da7c1f8feeb7a4cc98b4",
      "candidate_info": {
        "commit_hash": "d727061f73859219b9b5da7c1f8feeb7a4cc98b4",
        "repo": "DedSecInside/TorBot",
        "commit_url": "https://github.com/DedSecInside/TorBot/commit/d727061f73859219b9b5da7c1f8feeb7a4cc98b4",
        "files": [
          "torbot/modules/linktree.py"
        ],
        "message": "Remove LinkTree class and use treelib strcuture for hosting nodes",
        "before_after_code_files": [
          "torbot/modules/linktree.py||torbot/modules/linktree.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/DedSecInside/TorBot/pull/307"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "torbot/modules/linktree.py||torbot/modules/linktree.py": [
          "File: torbot/modules/linktree.py -> torbot/modules/linktree.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2: Module is used for analyzing link relationships",
          "3: \"\"\"",
          "4: import os",
          "43:     \"\"\"",
          "48:     \"\"\"",
          "",
          "[Removed Lines]",
          "6: from treelib import Tree, exceptions",
          "8: from .api import get_node",
          "9: from .config import get_data_directory",
          "10: from .log import debug",
          "13: def formatNode(n):",
          "14:     return f\"{n['url']} {n['status_code']} {n['status']}\"",
          "17: def build_tree_recursive(t, n):",
          "19:     # this will only be ran on the root node since others will exist before being passed",
          "20:     parent_id = n[\"url\"]",
          "21:     if not t.contains(parent_id):",
          "22:         debug(f\"adding id {parent_id}\")",
          "23:         t.create_node(formatNode(n), parent_id)",
          "25:     # if there are no children, there's nothing to process",
          "26:     children = n[\"children\"]",
          "27:     if not children:",
          "28:         return",
          "30:     for child in children:",
          "31:         try:",
          "32:             child_id = child[\"url\"]",
          "33:             debug(f\"adding child_id {child_id} to parent_id {parent_id}\")",
          "34:             t.create_node(formatNode(child), child_id, parent=parent_id)",
          "35:         except exceptions.DuplicatedNodeIdError:",
          "36:             debug(f\"found a duplicate url {child_id}\")",
          "37:             continue  # this node has already been processed somewhere else",
          "39:         build_tree_recursive(t, child)",
          "42: class LinkTree:",
          "44:     This is a class that represents a tree of links within TorBot. This can",
          "45:     be used to build a tree, examine the number of nodes, check if a node",
          "46:     exists within a tree, displaying the tree, and downloading the tree. It",
          "47:     will be expanded in the future to meet further needs.",
          "50:     def __init__(self, root: str, depth: int):",
          "51:         self.__build_tree(root, depth)",
          "53:     def __build_tree(self, url: str, depth: int = 1):",
          "54:         \"\"\"",
          "55:         Builds link tree by traversing through children nodes.",
          "57:         Returns:",
          "58:             tree (ete3.Tree): Built tree.",
          "59:         \"\"\"",
          "60:         debug(f\"building tree for {url} at {depth} depth\")",
          "61:         n = get_node(url, depth)",
          "62:         t = Tree()",
          "63:         build_tree_recursive(t, n)",
          "64:         self._tree = t",
          "65:         debug(\"tree built successfully\")",
          "67:     def save(self, file_name: str):",
          "68:         \"\"\"",
          "69:         Saves LinkTree to file with given file_name",
          "70:         Current file types supported are .txt",
          "71:         \"\"\"",
          "72:         print(f\"saving link tree as {file_name}\")",
          "73:         data_directory = get_data_directory()",
          "74:         file_path = os.path.join(data_directory, file_name)",
          "75:         try:",
          "76:             self._tree.save2file(file_path)",
          "77:         except Exception as e:",
          "78:             print(f\"failed to save link tree to {file_path}\")",
          "79:             debug(e)",
          "80:             raise e",
          "82:         print(f\"file saved successfully to {file_path}\")",
          "84:     def show(self):",
          "85:         \"\"\"",
          "86:         Displays image of LinkTree",
          "87:         \"\"\"",
          "88:         self._tree.show()",
          "",
          "[Added Lines]",
          "5: import re",
          "6: import httpx",
          "7: import validators",
          "8: import logging",
          "10: from treelib import Tree, exceptions, Node",
          "11: from bs4 import BeautifulSoup",
          "13: from .nlp.main import classify",
          "15: class Link(Node):",
          "16:     def __init__(self, title: str, url: str, status: int, classification: str, accuracy: float):",
          "17:         self.identifier = url",
          "18:         self.tag = title",
          "19:         self.status = status",
          "20:         self.classification = classification",
          "21:         self.accuracy = accuracy",
          "24: def parse_links(html: str) -> list[str]:",
          "26:     Finds all anchor tags and parses the href attribute.",
          "28:     soup = BeautifulSoup(html, 'html.parser')",
          "29:     tags = soup.find_all('a')",
          "30:     return [tag['href'] for tag in tags if tag.has_attr('href') and validators.url(tag['href'])]",
          "33: def append_node(tree: Tree, id: str, parent_id: str | None) -> None:",
          "34:     \"\"\"",
          "35:     Creates a node for a tree using the given ID which corresponds to a URL.",
          "36:     If the parent_id is None, this will be considered a root node.",
          "37:     \"\"\"",
          "38:     resp = httpx.get(id)",
          "39:     soup = BeautifulSoup(resp.text, 'html.parser')",
          "40:     title = soup.title.text.strip() if soup.title is not None else id",
          "41:     try:",
          "42:         [classification, accuracy] = classify(resp.text)",
          "43:         data = Link(title, id, resp.status_code, classification, accuracy)",
          "44:         tree.create_node(title, identifier=id, parent=parent_id, data=data)",
          "45:     except exceptions.DuplicatedNodeIdError:",
          "46:         logging.debug(f\"found a duplicate URL {id}\")",
          "49: def build_tree(tree: Tree, url: str, depth: int) -> None:",
          "50:     \"\"\"",
          "51:     Builds a tree from the root to the given depth.",
          "52:     \"\"\"",
          "53:     if depth > 0:",
          "54:         depth -= 1",
          "55:         resp = httpx.get(url)",
          "56:         children = parse_links(resp.text)",
          "57:         for child in children:",
          "58:             append_node(tree, id=child, parent_id=url)",
          "59:             build_tree(tree, child, depth)",
          "62: def save(tree: Tree, file_name: str) -> None:",
          "63:     \"\"\"",
          "64:     Saves the tree to the current working directory under the given file name.",
          "65:     \"\"\"",
          "66:     tree.save2file(os.path.join(os.getcwd(), file_name))",
          "69: def show(tree: Tree) -> None:",
          "70:     \"\"\"",
          "71:     Prints the tree",
          "72:     \"\"\"",
          "73:     tree.show()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ddabe8a0dc676ffe1663e479464c764517331a8a",
      "candidate_info": {
        "commit_hash": "ddabe8a0dc676ffe1663e479464c764517331a8a",
        "repo": "DedSecInside/TorBot",
        "commit_url": "https://github.com/DedSecInside/TorBot/commit/ddabe8a0dc676ffe1663e479464c764517331a8a",
        "files": [
          "poetry.lock",
          "pyproject.toml",
          "requirements.txt",
          "torbot/modules/linktree.py"
        ],
        "message": "syntax fix and removing threadsafe",
        "before_after_code_files": [
          "poetry.lock||poetry.lock",
          "torbot/modules/linktree.py||torbot/modules/linktree.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/DedSecInside/TorBot/pull/307"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "poetry.lock||poetry.lock": [
          "File: poetry.lock -> poetry.lock",
          "--- Hunk 1 ---",
          "[Context before]",
          "612:     {file = \"threadpoolctl-3.1.0.tar.gz\", hash = \"sha256:a335baacfaa4400ae1f0d8e3a58d6674d2f8828e3716bb2802c44955ad391380\"},",
          "613: ]",
          "626: [[package]]",
          "627: name = \"treelib\"",
          "628: version = \"1.7.0\"",
          "",
          "[Removed Lines]",
          "615: [[package]]",
          "616: name = \"threadsafe\"",
          "617: version = \"1.0.0\"",
          "618: description = \"Thread-safe data structures\"",
          "619: optional = false",
          "620: python-versions = \"*\"",
          "621: files = [",
          "622:     {file = \"threadsafe-1.0.0-py3-none-any.whl\", hash = \"sha256:acbd59278ca8221dc3a8051443fe24c647ee9ac81808058e280ef6f75dd4387b\"},",
          "623:     {file = \"threadsafe-1.0.0.tar.gz\", hash = \"sha256:7c61f9fdd0b3cd6c07b427de355dafcd337578d30871634cb1e8985ee4955edc\"},",
          "624: ]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "693: [metadata]",
          "694: lock-version = \"2.0\"",
          "695: python-versions = \">=3.9,<=3.11.4\"",
          "",
          "[Removed Lines]",
          "696: content-hash = \"fa048130f884a71b33d42a8dd2940a2c17365309afe56ae1c6abc2dfc6ee5a40\"",
          "",
          "[Added Lines]",
          "685: content-hash = \"1e6d83812ac5be9a550b998795ee28f76bb788e972c933e497e68b20f548a0ea\"",
          "",
          "---------------"
        ],
        "torbot/modules/linktree.py||torbot/modules/linktree.py": [
          "File: torbot/modules/linktree.py -> torbot/modules/linktree.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "60:         except exceptions.DuplicatedNodeIdError:",
          "61:             logging.debug(f\"found a duplicate URL {id}\")",
          "64:         \"\"\"",
          "65:         Builds a tree from the root to the given depth.",
          "66:         \"\"\"",
          "",
          "[Removed Lines]",
          "63:     def _build_tree(self,  url: str, depth: int) -> None:",
          "",
          "[Added Lines]",
          "63:     def _build_tree(self, url: str, depth: int) -> None:",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "71f8cc887853d50d45fd0eec938ecfe9f3177a3f",
      "candidate_info": {
        "commit_hash": "71f8cc887853d50d45fd0eec938ecfe9f3177a3f",
        "repo": "DedSecInside/TorBot",
        "commit_url": "https://github.com/DedSecInside/TorBot/commit/71f8cc887853d50d45fd0eec938ecfe9f3177a3f",
        "files": [
          "torbot/modules/link_io.py"
        ],
        "message": "Utilize new api in IO module",
        "before_after_code_files": [
          "torbot/modules/link_io.py||torbot/modules/link_io.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/DedSecInside/TorBot/pull/307"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "torbot/modules/link_io.py||torbot/modules/link_io.py": [
          "File: torbot/modules/link_io.py -> torbot/modules/link_io.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2: This module is used for reading HTML pages using either bs4.BeautifulSoup",
          "3: objects or url strings",
          "4: \"\"\"",
          "6: from pprint import pprint",
          "11: from .color import color",
          "16:     \"\"\"",
          "17:     https://check.torproject.org/ tells you if you are using tor and it",
          "18:     displays your IP address which we scape and display",
          "19:     \"\"\"",
          "26:     \"\"\"",
          "27:     Prints the status of a link based on it's connection status",
          "28:     \"\"\"",
          "39:         else:",
          "64: def print_json(url: str, depth: int = 1):",
          "",
          "[Removed Lines]",
          "7: from typing import Any",
          "9: from .linktree import LinkTree",
          "10: from .api import get_web_content, get_node, get_emails, get_phone, get_ip",
          "12: from .nlp.main import classify",
          "15: def print_tor_ip_address():",
          "20:     print('Attempting to connect to https://check.torproject.org/')",
          "21:     ip_string = color(get_ip(), 'yellow')",
          "22:     print(f'Tor IP Address: {ip_string}')",
          "25: def print_node(node: LinkTree, classify_page: bool):",
          "29:     try:",
          "30:         title = node['url']",
          "31:         status_text = f\"{node['status_code']} {node['status']}\"",
          "32:         if classify_page:",
          "33:             classification = classify(get_web_content(node['url']))",
          "34:             status_text += f\" {classification}\"",
          "35:         if node['status_code'] >= 200 and node['status_code'] < 300:",
          "36:             status = color(status_text, 'green')",
          "37:         elif node['status_code'] >= 300 and node['status_code'] < 400:",
          "38:             status = color(status_text, 'yellow')",
          "40:             status = color(status_text, 'red')",
          "41:     except Exception:",
          "42:         title = \"NOT FOUND\"",
          "43:         status = color('Unable to reach destination.', 'red')",
          "45:     status_msg = \"%-60s %-20s\" % (title, status)",
          "46:     print(status_msg)",
          "49: def cascade(node: LinkTree, work: Any, classify_page: bool):",
          "50:     work(node, classify_page)",
          "51:     if node['children']:",
          "52:         for child in node['children']:",
          "53:             cascade(child, work, classify_page)",
          "56: def print_tree(url: str, depth: int = 1, classify_page: bool = False):",
          "57:     \"\"\"",
          "58:     Prints the entire tree in a user friendly fashion",
          "59:     \"\"\"",
          "60:     root = get_node(url, depth)",
          "61:     cascade(root, print_node, classify_page)",
          "",
          "[Added Lines]",
          "5: import http.client",
          "6: import tabulate",
          "9: from treelib import Tree",
          "11: from .api import get_node, get_emails, get_phone, get_ip",
          "15: def print_tor_ip_address() -> None:",
          "20:     resp = get_ip()",
          "21:     print(resp[\"header\"])",
          "22:     print(color(resp[\"body\"], \"yellow\"))",
          "25: def pprint_tree(tree: Tree) -> None:",
          "29:     nodes = tree.all_nodes_itr()",
          "30:     table_data = []",
          "32:     def insert(node, color_code):",
          "33:         status = str(node.data.status)",
          "34:         code = http.client.responses[node.data.status]",
          "35:         status_message = f'{status} {code}'",
          "36:         table_data.append([",
          "37:             node.tag,",
          "38:             node.identifier,",
          "39:             color(status_message, color_code),",
          "40:             node.data.classification,",
          "41:         ])",
          "43:     for node in nodes:",
          "44:         status_code = node.data.status",
          "45:         if status_code >= 200 and status_code < 300:",
          "46:             insert(node, 'green')",
          "47:         elif status_code >= 300 and status_code < 400:",
          "48:             insert(node, 'yellow')",
          "50:             insert(node, 'red')",
          "52:     headers = [\"Title\", \"URL\", \"Status\", \"Category\"]",
          "53:     table = tabulate.tabulate(table_data, headers=headers)",
          "54:     print(table)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "69:         root (dict): Dictionary containing the root node and it's children",
          "70:     \"\"\"",
          "71:     root = get_node(url, depth)",
          "76: def print_emails(url: str):",
          "",
          "[Removed Lines]",
          "72:     pprint(root)",
          "73:     return root",
          "",
          "[Added Lines]",
          "65:     print(root.to_json())",
          "",
          "---------------"
        ]
      }
    }
  ]
}