{
  "cve_id": "CVE-2024-3574",
  "cve_desc": "In scrapy version 2.10.1, an issue was identified where the Authorization header, containing credentials for server authentication, is leaked to a third-party site during a cross-domain redirect. This vulnerability arises from the failure to remove the Authorization header when redirecting across domains. The exposure of the Authorization header to unauthorized actors could potentially allow for account hijacking.",
  "repo": "scrapy/scrapy",
  "patch_hash": "5bcb8fd5019c72d05c4a96da78a7fcb6ecb55b75",
  "patch_info": {
    "commit_hash": "5bcb8fd5019c72d05c4a96da78a7fcb6ecb55b75",
    "repo": "scrapy/scrapy",
    "commit_url": "https://github.com/scrapy/scrapy/commit/5bcb8fd5019c72d05c4a96da78a7fcb6ecb55b75",
    "files": [
      "docs/news.rst",
      "scrapy/downloadermiddlewares/redirect.py",
      "tests/test_downloadermiddleware_redirect.py"
    ],
    "message": "Merge branch '2.11-authorization' into 2.11",
    "before_after_code_files": [
      "scrapy/downloadermiddlewares/redirect.py||scrapy/downloadermiddlewares/redirect.py",
      "tests/test_downloadermiddleware_redirect.py||tests/test_downloadermiddleware_redirect.py"
    ]
  },
  "patch_diff": {
    "scrapy/downloadermiddlewares/redirect.py||scrapy/downloadermiddlewares/redirect.py": [
      "File: scrapy/downloadermiddlewares/redirect.py -> scrapy/downloadermiddlewares/redirect.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "18:         cookies=None,",
      "19:     )",
      "21:         source_request_netloc = urlparse_cached(source_request).netloc",
      "22:         redirect_request_netloc = urlparse_cached(redirect_request).netloc",
      "23:         if source_request_netloc != redirect_request_netloc:",
      "25:     return redirect_request",
      "",
      "[Removed Lines]",
      "20:     if \"Cookie\" in redirect_request.headers:",
      "24:             del redirect_request.headers[\"Cookie\"]",
      "",
      "[Added Lines]",
      "20:     has_cookie_header = \"Cookie\" in redirect_request.headers",
      "21:     has_authorization_header = \"Authorization\" in redirect_request.headers",
      "22:     if has_cookie_header or has_authorization_header:",
      "26:             if has_cookie_header:",
      "27:                 del redirect_request.headers[\"Cookie\"]",
      "28:             # https://fetch.spec.whatwg.org/#ref-for-cors-non-wildcard-request-header-name",
      "29:             if has_authorization_header:",
      "30:                 del redirect_request.headers[\"Authorization\"]",
      "",
      "---------------"
    ],
    "tests/test_downloadermiddleware_redirect.py||tests/test_downloadermiddleware_redirect.py": [
      "File: tests/test_downloadermiddleware_redirect.py -> tests/test_downloadermiddleware_redirect.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "247:         perc_encoded_utf8_url = \"http://scrapytest.org/a%C3%A7%C3%A3o\"",
      "248:         self.assertEqual(perc_encoded_utf8_url, req_result.url)",
      "251: class MetaRefreshMiddlewareTest(unittest.TestCase):",
      "252:     def setUp(self):",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "250:     def test_cross_domain_header_dropping(self):",
      "251:         safe_headers = {\"A\": \"B\"}",
      "252:         original_request = Request(",
      "253:             \"https://example.com\",",
      "254:             headers={\"Cookie\": \"a=b\", \"Authorization\": \"a\", **safe_headers},",
      "255:         )",
      "257:         internal_response = Response(",
      "258:             \"https://example.com\",",
      "259:             headers={\"Location\": \"https://example.com/a\"},",
      "260:             status=301,",
      "261:         )",
      "262:         internal_redirect_request = self.mw.process_response(",
      "263:             original_request, internal_response, self.spider",
      "264:         )",
      "265:         self.assertIsInstance(internal_redirect_request, Request)",
      "266:         self.assertEqual(original_request.headers, internal_redirect_request.headers)",
      "268:         external_response = Response(",
      "269:             \"https://example.com\",",
      "270:             headers={\"Location\": \"https://example.org/a\"},",
      "271:             status=301,",
      "272:         )",
      "273:         external_redirect_request = self.mw.process_response(",
      "274:             original_request, external_response, self.spider",
      "275:         )",
      "276:         self.assertIsInstance(external_redirect_request, Request)",
      "277:         self.assertEqual(",
      "278:             safe_headers, external_redirect_request.headers.to_unicode_dict()",
      "279:         )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "2f1d345e74d19e33016f9e69fcda0bda9afb568d",
      "candidate_info": {
        "commit_hash": "2f1d345e74d19e33016f9e69fcda0bda9afb568d",
        "repo": "scrapy/scrapy",
        "commit_url": "https://github.com/scrapy/scrapy/commit/2f1d345e74d19e33016f9e69fcda0bda9afb568d",
        "files": [
          "tests/test_downloadermiddleware_httpcompression.py",
          "tests/test_utils_response.py"
        ],
        "message": "Solve test issues",
        "before_after_code_files": [
          "tests/test_downloadermiddleware_httpcompression.py||tests/test_downloadermiddleware_httpcompression.py",
          "tests/test_utils_response.py||tests/test_utils_response.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/scrapy/scrapy/pull/6222"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/test_downloadermiddleware_httpcompression.py||tests/test_downloadermiddleware_httpcompression.py": [
          "File: tests/test_downloadermiddleware_httpcompression.py -> tests/test_downloadermiddleware_httpcompression.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "402:         self._test_compression_bomb_setting(\"gzip\")",
          "404:     def test_compression_bomb_setting_zstd(self):",
          "405:         self._test_compression_bomb_setting(\"zstd\")",
          "407:     def _test_compression_bomb_spider_attr(self, compression_id):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "405:         try:",
          "406:             import zstandard  # noqa: F401",
          "407:         except ImportError:",
          "408:             raise SkipTest(\"no zstd support (zstandard)\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "436:         self._test_compression_bomb_spider_attr(\"gzip\")",
          "438:     def test_compression_bomb_spider_attr_zstd(self):",
          "439:         self._test_compression_bomb_spider_attr(\"zstd\")",
          "441:     def _test_compression_bomb_request_meta(self, compression_id):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "443:         try:",
          "444:             import zstandard  # noqa: F401",
          "445:         except ImportError:",
          "446:             raise SkipTest(\"no zstd support (zstandard)\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "468:         self._test_compression_bomb_request_meta(\"gzip\")",
          "470:     def test_compression_bomb_request_meta_zstd(self):",
          "471:         self._test_compression_bomb_request_meta(\"zstd\")",
          "473:     def _test_download_warnsize_setting(self, compression_id):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "479:         try:",
          "480:             import zstandard  # noqa: F401",
          "481:         except ImportError:",
          "482:             raise SkipTest(\"no zstd support (zstandard)\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "510:         self._test_download_warnsize_setting(\"gzip\")",
          "512:     def test_download_warnsize_setting_zstd(self):",
          "513:         self._test_download_warnsize_setting(\"zstd\")",
          "515:     def _test_download_warnsize_spider_attr(self, compression_id):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "525:         try:",
          "526:             import zstandard  # noqa: F401",
          "527:         except ImportError:",
          "528:             raise SkipTest(\"no zstd support (zstandard)\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "554:         self._test_download_warnsize_spider_attr(\"gzip\")",
          "556:     def test_download_warnsize_spider_attr_zstd(self):",
          "557:         self._test_download_warnsize_spider_attr(\"zstd\")",
          "559:     def _test_download_warnsize_request_meta(self, compression_id):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "573:         try:",
          "574:             import zstandard  # noqa: F401",
          "575:         except ImportError:",
          "576:             raise SkipTest(\"no zstd support (zstandard)\")",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "596:         self._test_download_warnsize_request_meta(\"gzip\")",
          "598:     def test_download_warnsize_request_meta_zstd(self):",
          "599:         self._test_download_warnsize_request_meta(\"zstd\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "619:         try:",
          "620:             import zstandard  # noqa: F401",
          "621:         except ImportError:",
          "622:             raise SkipTest(\"no zstd support (zstandard)\")",
          "",
          "---------------"
        ],
        "tests/test_utils_response.py||tests/test_utils_response.py": [
          "File: tests/test_utils_response.py -> tests/test_utils_response.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "204:         ), \"Inject unique base url with conditional comment\"",
          "206:     def test_open_in_browser_redos_comment(self):",
          "209:         # Exploit input from",
          "210:         # https://makenowjust-labs.github.io/recheck/playground/",
          "",
          "[Removed Lines]",
          "207:         MAX_CPU_TIME = 0.001",
          "",
          "[Added Lines]",
          "207:         MAX_CPU_TIME = 0.02",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "221:         self.assertLess(end_time - start_time, MAX_CPU_TIME)",
          "223:     def test_open_in_browser_redos_head(self):",
          "226:         # Exploit input from",
          "227:         # https://makenowjust-labs.github.io/recheck/playground/",
          "",
          "[Removed Lines]",
          "224:         MAX_CPU_TIME = 0.001",
          "",
          "[Added Lines]",
          "224:         MAX_CPU_TIME = 0.02",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "502addc717b6b971425a9385359a382b8d0187a1",
      "candidate_info": {
        "commit_hash": "502addc717b6b971425a9385359a382b8d0187a1",
        "repo": "scrapy/scrapy",
        "commit_url": "https://github.com/scrapy/scrapy/commit/502addc717b6b971425a9385359a382b8d0187a1",
        "files": [
          ".bumpversion.cfg",
          "scrapy/VERSION"
        ],
        "message": "Bump version: 2.11.0 \u2192 2.11.1",
        "before_after_code_files": [
          ".bumpversion.cfg||.bumpversion.cfg"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/scrapy/scrapy/pull/6222"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        ".bumpversion.cfg||.bumpversion.cfg": [
          "File: .bumpversion.cfg -> .bumpversion.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: [bumpversion]",
          "3: commit = True",
          "4: tag = True",
          "5: tag_name = {new_version}",
          "",
          "[Removed Lines]",
          "2: current_version = 2.11.0",
          "",
          "[Added Lines]",
          "2: current_version = 2.11.1",
          "",
          "---------------"
        ]
      }
    }
  ]
}