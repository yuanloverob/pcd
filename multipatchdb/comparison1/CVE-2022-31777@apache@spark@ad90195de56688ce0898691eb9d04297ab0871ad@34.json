{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "d725d9c20e33e3c68d9c7ec84b74fea2952814b6",
      "candidate_info": {
        "commit_hash": "d725d9c20e33e3c68d9c7ec84b74fea2952814b6",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/d725d9c20e33e3c68d9c7ec84b74fea2952814b6",
        "files": [
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q32.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q32.sf100/simplified.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q32/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q32/simplified.txt",
          "sql/core/src/test/resources/tpcds-query-results/v1_4/q32.sql.out",
          "sql/core/src/test/resources/tpcds/q32.sql"
        ],
        "message": "[SPARK-40124][SQL][TEST][3.3] Update TPCDS v1.4 q32 for Plan Stability tests\n\n### What changes were proposed in this pull request?\nThis is port of SPARK-40124 to Spark 3.3. Fix query 32 for TPCDS v1.4\n\n### Why are the changes needed?\nCurrent q32.sql seems to be wrong. It is just selection `1`.\nReference for query template: https://github.com/databricks/tpcds-kit/blob/eff5de2c30337b71cc0dc1976147742d2c65d378/query_templates/query32.tpl#L41\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nTest change only\n\nCloses #37615 from mskapilks/change-q32-3.3.\n\nAuthored-by: Kapil Kumar Singh <kapilsingh@microsoft.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/resources/tpcds/q32.sql||sql/core/src/test/resources/tpcds/q32.sql"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/resources/tpcds/q32.sql||sql/core/src/test/resources/tpcds/q32.sql": [
          "File: sql/core/src/test/resources/tpcds/q32.sql -> sql/core/src/test/resources/tpcds/q32.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "2: FROM",
          "3:   catalog_sales, item, date_dim",
          "4: WHERE",
          "",
          "[Removed Lines]",
          "1: SELECT 1 AS `excess discount amount `",
          "",
          "[Added Lines]",
          "1: SELECT sum(cs_ext_discount_amt) AS `excess discount amount`",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "83ae6f4084f1ebbff96df27f076a283515424c9c",
      "candidate_info": {
        "commit_hash": "83ae6f4084f1ebbff96df27f076a283515424c9c",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/83ae6f4084f1ebbff96df27f076a283515424c9c",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/numberFormatExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/NumberFormatter.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/NumberFormatterSuite.scala",
          "sql/core/src/test/resources/sql-functions/sql-expression-schema.md",
          "sql/core/src/test/resources/sql-tests/inputs/string-functions.sql",
          "sql/core/src/test/resources/sql-tests/results/ansi/string-functions.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/numeric.sql.out",
          "sql/core/src/test/resources/sql-tests/results/string-functions.sql.out"
        ],
        "message": "[SPARK-38796][SQL] Implement the to_number and try_to_number SQL functions according to a new specification\n\n### What changes were proposed in this pull request?\n\nThis PR implements the `to_number` and `try_to_number` SQL function expressions according to new semantics described below. The former is equivalent to the latter except that it throws an exception instead of returning NULL for cases where the input string does not match the format string.\n\n-----------\n\n# `try_to_number` function\n\nReturns `expr` cast to DECIMAL using formatting `fmt`, or `NULL` if `expr` is not a valid match for the given format.\n\n## Syntax\n\n```\ntry_to_number(expr, fmt)\nfmt\n  { ' [ S ] [ L | $ ]\n      [ 0 | 9 | G | , ] [...]\n      [ . | D ]\n      [ 0 | 9 ] [...]\n      [ L | $ ] [ PR | MI | S ] ' }\n```\n\n## Arguments\n\n- `expr`: A STRING expression representing a number. `expr` may include leading or trailing spaces.\n- `fmt`: An STRING literal, specifying the expected format of `expr`.\n\n## Returns\n\nA DECIMAL(p, s) where `p` is the total number of digits (`0` or `9`) and `s` is the number of digits after the decimal point, or 0 if there is none.\n\n`fmt` can contain the following elements (case insensitive):\n\n- **`0`** or **`9`**\n\n  Specifies an expected digit between `0` and `9`.\n  A `0` to the left of the decimal points indicates that `expr` must have at least as many digits.\n  A leading `9` indicates that `expr` may omit these digits.\n\n  `expr` must not be larger than the number of digits to the left of the decimal point allowed by the format string.\n\n  Digits to the right of the decimal point in the format string indicate the most digits that `expr` may have to the right of the decimal point.\n\n- **`.`** or **`D`**\n\n  Specifies the position of the decimal point.\n\n  `expr` does not need to include a decimal point.\n\n- **`,`** or **`G`**\n\n  Specifies the position of the `,` grouping (thousands) separator.\n  There must be a `0` or `9` to the left of the rightmost grouping separator.\n  `expr` must match the grouping separator relevant for the size of the number.\n\n- **`L`** or **`$`**\n\n  Specifies the location of the `$` currency sign. This character may only be specified once.\n\n- **`S`**\n\n  Specifies the position of an option '+' or '-' sign. This character may only be specified once.\n\n- **`MI`**\n\n  Specifies that `expr` has an optional `-` sign at the end, but no `+`.\n\n- **`PR`**\n\n  Specifies that `expr` indicates a negative number with wrapping angled brackets (`<1>`).\n\nIf `expr` contains any characters other than `0` through `9` and those permitted in `fmt` a `NULL` is returned.\n\n## Examples\n\n```sql\n-- The format expects:\n--  * an optional sign at the beginning,\n--  * followed by a dollar sign,\n--  * followed by a number between 3 and 6 digits long,\n--  * thousands separators,\n--  * up to two dights beyond the decimal point.\n> SELECT try_to_number('-$12,345.67', 'S$999,099.99');\n -12345.67\n-- The plus sign is optional, and so are fractional digits.\n> SELECT try_to_number('$345', 'S$999,099.99');\n 345.00\n-- The format requires at least three digits.\n> SELECT try_to_number('$45', 'S$999,099.99');\n NULL\n-- The format requires at least three digits.\n> SELECT try_to_number('$045', 'S$999,099.99');\n 45.00\n-- Using brackets to denote negative values\n> SELECT try_to_number('<1234>', '999999PR');\n -1234\n```\n\n### Why are the changes needed?\n\nThe new semantics bring Spark into consistency with other engines and grant the user flexibility about how to handle cases where inputs do not match the format string.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes.\n\n* The minus sign `-` is no longer supported in the format string (`S` replaces it).\n* `MI` and `PR` are new options in the format string.\n* `to_number` and `try_to_number` are separate functions with different error behavior.\n\n### How was this patch tested?\n\n* New positive and negative unit tests cover both `to_number` and `try_to_number` functions.\n* Query tests update as needed according to the behavior changes.\n\nCloses #36066 from dtenedor/to-number.\n\nAuthored-by: Daniel Tenedorio <daniel.tenedorio@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 7a6b98965bf40993ea2e7837ded1c79813bec5d8)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/numberFormatExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/numberFormatExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/NumberFormatter.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/NumberFormatter.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/NumberFormatterSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/NumberFormatterSuite.scala",
          "sql/core/src/test/resources/sql-tests/inputs/string-functions.sql||sql/core/src/test/resources/sql-tests/inputs/string-functions.sql"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "541:     expression[FormatNumber](\"format_number\"),",
          "542:     expression[FormatString](\"format_string\"),",
          "543:     expression[ToNumber](\"to_number\"),",
          "544:     expression[GetJsonObject](\"get_json_object\"),",
          "545:     expression[InitCap](\"initcap\"),",
          "546:     expression[StringInstr](\"instr\"),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "544:     expression[TryToNumber](\"try_to_number\"),",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/numberFormatExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/numberFormatExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/numberFormatExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/numberFormatExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import org.apache.spark.sql.catalyst.analysis.TypeCheckResult",
          "23: import org.apache.spark.sql.catalyst.expressions.codegen.{CodegenContext, CodeGenerator, ExprCode}",
          "24: import org.apache.spark.sql.catalyst.expressions.codegen.Block.BlockHelper",
          "26: import org.apache.spark.sql.types.{DataType, StringType}",
          "27: import org.apache.spark.unsafe.types.UTF8String",
          "32: @ExpressionDescription(",
          "33:   usage = \"\"\"",
          "41:   \"\"\",",
          "42:   examples = \"\"\"",
          "43:     Examples:",
          "44:       > SELECT _FUNC_('454', '999');",
          "45:        454",
          "47:        454.00",
          "49:        12454",
          "50:       > SELECT _FUNC_('$78.12', '$99.99');",
          "51:        78.12",
          "53:        -12454.8",
          "54:   \"\"\",",
          "55:   since = \"3.3.0\",",
          "56:   group = \"string_funcs\")",
          "57: case class ToNumber(left: Expression, right: Expression)",
          "58:   extends BinaryExpression with ImplicitCastInputTypes with NullIntolerant {",
          "60:   private lazy val numberFormat = right.eval().toString.toUpperCase(Locale.ROOT)",
          "63:   override def dataType: DataType = numberFormatter.parsedDecimalType",
          "65:   override def inputTypes: Seq[DataType] = Seq(StringType, StringType)",
          "67:   override def checkInputDataTypes(): TypeCheckResult = {",
          "68:     val inputTypeCheck = super.checkInputDataTypes()",
          "69:     if (inputTypeCheck.isSuccess) {",
          "",
          "[Removed Lines]",
          "25: import org.apache.spark.sql.catalyst.util.NumberFormatter",
          "34:      _FUNC_(strExpr, formatExpr) - Convert `strExpr` to a number based on the `formatExpr`.",
          "35:        The format can consist of the following characters:",
          "36:          '0' or '9':  digit position",
          "37:          '.' or 'D':  decimal point (only allowed once)",
          "38:          ',' or 'G':  group (thousands) separator",
          "39:          '-' or 'S':  sign anchored to number (only allowed once)",
          "40:          '$':  value with a leading dollar sign (only allowed once)",
          "46:       > SELECT _FUNC_('454.00', '000D00');",
          "48:       > SELECT _FUNC_('12,454', '99G999');",
          "52:       > SELECT _FUNC_('12,454.8-', '99G999D9S');",
          "61:   private lazy val numberFormatter = new NumberFormatter(numberFormat)",
          "",
          "[Added Lines]",
          "25: import org.apache.spark.sql.catalyst.util.ToNumberParser",
          "35:      _FUNC_(expr, fmt) - Convert string 'expr' to a number based on the string format 'fmt'.",
          "36:        Throws an exception if the conversion fails. The format can consist of the following",
          "37:        characters, case insensitive:",
          "38:          '0' or '9': Specifies an expected digit between 0 and 9. A sequence of 0 or 9 in the format",
          "39:            string matches a sequence of digits in the input string. If the 0/9 sequence starts with",
          "40:            0 and is before the decimal point, it can only match a digit sequence of the same size.",
          "41:            Otherwise, if the sequence starts with 9 or is after the decimal poin, it can match a",
          "42:            digit sequence that has the same or smaller size.",
          "43:          '.' or 'D': Specifies the position of the decimal point (optional, only allowed once).",
          "44:          ',' or 'G': Specifies the position of the grouping (thousands) separator (,). There must be",
          "45:            one or more 0 or 9 to the left of the rightmost grouping separator. 'expr' must match the",
          "46:            grouping separator relevant for the size of the number.",
          "47:          '$': Specifies the location of the $ currency sign. This character may only be specified",
          "48:            once.",
          "49:          'S': Specifies the position of a '-' or '+' sign (optional, only allowed once).",
          "50:          'MI': Specifies that 'expr' has an optional '-' sign, but no '+' (only allowed once).",
          "51:          'PR': Only allowed at the end of the format string; specifies that 'expr' indicates a",
          "52:            negative number with wrapping angled brackets.",
          "53:            ('<1>').",
          "59:       > SELECT _FUNC_('454.00', '000.00');",
          "61:       > SELECT _FUNC_('12,454', '99,999');",
          "65:       > SELECT _FUNC_('12,454.8-', '99,999.9S');",
          "73:   private lazy val numberFormatter = new ToNumberParser(numberFormat, true)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "76:       inputTypeCheck",
          "77:     }",
          "78:   }",
          "80:   override def prettyName: String = \"to_number\"",
          "82:   override def nullSafeEval(string: Any, format: Any): Any = {",
          "83:     val input = string.asInstanceOf[UTF8String]",
          "84:     numberFormatter.parse(input)",
          "85:   }",
          "87:   override def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {",
          "88:     val builder =",
          "90:     val eval = left.genCode(ctx)",
          "91:     ev.copy(code =",
          "92:       code\"\"\"",
          "",
          "[Removed Lines]",
          "89:       ctx.addReferenceObj(\"builder\", numberFormatter, classOf[NumberFormatter].getName)",
          "",
          "[Added Lines]",
          "96:       ctx.addReferenceObj(\"builder\", numberFormatter, classOf[ToNumberParser].getName)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "98:         |}",
          "99:       \"\"\".stripMargin)",
          "100:   }",
          "102:   override protected def withNewChildrenInternal(",
          "104: }",
          "",
          "[Removed Lines]",
          "103:       newLeft: Expression, newRight: Expression): ToNumber = copy(left = newLeft, right = newRight)",
          "",
          "[Added Lines]",
          "109:       newLeft: Expression, newRight: Expression): ToNumber =",
          "110:     copy(left = newLeft, right = newRight)",
          "117: @ExpressionDescription(",
          "118:   usage = \"\"\"",
          "119:      _FUNC_(expr, fmt) - Convert string 'expr' to a number based on the string format `fmt`.",
          "120:        Returns NULL if the string 'expr' does not match the expected format. The format follows the",
          "121:        same semantics as the to_number function.",
          "122:   \"\"\",",
          "123:   examples = \"\"\"",
          "124:     Examples:",
          "125:       > SELECT _FUNC_('454', '999');",
          "126:        454",
          "127:       > SELECT _FUNC_('454.00', '000.00');",
          "128:        454.00",
          "129:       > SELECT _FUNC_('12,454', '99,999');",
          "130:        12454",
          "131:       > SELECT _FUNC_('$78.12', '$99.99');",
          "132:        78.12",
          "133:       > SELECT _FUNC_('12,454.8-', '99,999.9S');",
          "134:        -12454.8",
          "135:   \"\"\",",
          "136:   since = \"3.3.0\",",
          "137:   group = \"string_funcs\")",
          "138: case class TryToNumber(left: Expression, right: Expression)",
          "139:   extends BinaryExpression with ImplicitCastInputTypes with NullIntolerant {",
          "140:   private lazy val numberFormat = right.eval().toString.toUpperCase(Locale.ROOT)",
          "141:   private lazy val numberFormatter = new ToNumberParser(numberFormat, false)",
          "143:   override def dataType: DataType = numberFormatter.parsedDecimalType",
          "144:   override def inputTypes: Seq[DataType] = Seq(StringType, StringType)",
          "145:   override def nullable: Boolean = true",
          "146:   override def checkInputDataTypes(): TypeCheckResult = ToNumber(left, right).checkInputDataTypes()",
          "147:   override def prettyName: String = \"try_to_number\"",
          "148:   override def nullSafeEval(string: Any, format: Any): Any = {",
          "149:     val input = string.asInstanceOf[UTF8String]",
          "150:     numberFormatter.parse(input)",
          "151:   }",
          "152:   override def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {",
          "153:     val builder =",
          "154:       ctx.addReferenceObj(\"builder\", numberFormatter, classOf[ToNumberParser].getName)",
          "155:     val eval = left.genCode(ctx)",
          "156:     ev.copy(code =",
          "157:       code\"\"\"",
          "158:         |${eval.code}",
          "159:         |boolean ${ev.isNull} = ${eval.isNull};",
          "160:         |${CodeGenerator.javaType(dataType)} ${ev.value} = ${CodeGenerator.defaultValue(dataType)};",
          "161:         |if (!${ev.isNull}) {",
          "162:         |  ${ev.value} = $builder.parse(${eval.value});",
          "163:         |}",
          "164:       \"\"\".stripMargin)",
          "165:   }",
          "166:   override protected def withNewChildrenInternal(",
          "167:       newLeft: Expression,",
          "168:       newRight: Expression): TryToNumber =",
          "169:     copy(left = newLeft, right = newRight)",
          "170: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/NumberFormatter.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/NumberFormatter.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/NumberFormatter.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/NumberFormatter.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.sql.catalyst.util",
          "20: import scala.collection.mutable",
          "22: import org.apache.spark.sql.catalyst.analysis.TypeCheckResult",
          "23: import org.apache.spark.sql.errors.QueryExecutionErrors",
          "24: import org.apache.spark.sql.types.{Decimal, DecimalType}",
          "25: import org.apache.spark.unsafe.types.UTF8String",
          "28: object ToNumberParser {",
          "29:   final val ANGLE_BRACKET_CLOSE = '>'",
          "30:   final val ANGLE_BRACKET_OPEN = '<'",
          "31:   final val COMMA_LETTER = 'G'",
          "32:   final val COMMA_SIGN = ','",
          "33:   final val DOLLAR_SIGN = '$'",
          "34:   final val MINUS_SIGN = '-'",
          "35:   final val NINE_DIGIT = '9'",
          "36:   final val OPTIONAL_PLUS_OR_MINUS_LETTER = 'S'",
          "37:   final val PLUS_SIGN = '+'",
          "38:   final val POINT_LETTER = 'D'",
          "39:   final val POINT_SIGN = '.'",
          "40:   final val ZERO_DIGIT = '0'",
          "42:   final val OPTIONAL_MINUS_STRING = \"MI\"",
          "43:   final val WRAPPING_ANGLE_BRACKETS_TO_NEGATIVE_NUMBER = \"PR\"",
          "45:   final val OPTIONAL_MINUS_STRING_START = 'M'",
          "46:   final val OPTIONAL_MINUS_STRING_END = 'I'",
          "48:   final val WRAPPING_ANGLE_BRACKETS_TO_NEGATIVE_NUMBER_START = 'P'",
          "49:   final val WRAPPING_ANGLE_BRACKETS_TO_NEGATIVE_NUMBER_END = 'R'",
          "53:   abstract class InputToken()",
          "55:   abstract class Digits extends InputToken",
          "57:   case class ExactlyAsManyDigits(num: Int) extends Digits",
          "59:   case class AtMostAsManyDigits(num: Int) extends Digits",
          "61:   case class DecimalPoint() extends InputToken",
          "63:   case class ThousandsSeparator() extends InputToken",
          "66:   case class DigitGroups(tokens: Seq[InputToken], digits: Seq[Digits]) extends InputToken",
          "68:   case class DollarSign() extends InputToken",
          "70:   case class OptionalPlusOrMinusSign() extends InputToken",
          "72:   case class OptionalMinusSign() extends InputToken",
          "74:   case class OpeningAngleBracket() extends InputToken",
          "76:   case class ClosingAngleBracket() extends InputToken",
          "78:   case class InvalidUnrecognizedCharacter(char: Char) extends InputToken",
          "79: }",
          "94: class ToNumberParser(numberFormat: String, errorOnFail: Boolean) extends Serializable {",
          "95:   import ToNumberParser._",
          "99:   private lazy val formatTokens: Seq[InputToken] = {",
          "100:     val tokens = mutable.Buffer.empty[InputToken]",
          "101:     var i = 0",
          "102:     var reachedDecimalPoint = false",
          "103:     val len = numberFormat.length",
          "104:     while (i < len) {",
          "105:       val char: Char = numberFormat(i)",
          "106:       char match {",
          "107:         case ZERO_DIGIT =>",
          "108:           val prevI = i",
          "109:           do {",
          "110:             i += 1",
          "111:           } while (i < len && (numberFormat(i) == ZERO_DIGIT || numberFormat(i) == NINE_DIGIT))",
          "112:           if (reachedDecimalPoint) {",
          "113:             tokens.append(AtMostAsManyDigits(i - prevI))",
          "114:           } else {",
          "115:             tokens.append(ExactlyAsManyDigits(i - prevI))",
          "116:           }",
          "117:         case NINE_DIGIT =>",
          "118:           val prevI = i",
          "119:           do {",
          "120:             i += 1",
          "121:           } while (i < len && (numberFormat(i) == ZERO_DIGIT || numberFormat(i) == NINE_DIGIT))",
          "122:           tokens.append(AtMostAsManyDigits(i - prevI))",
          "123:         case POINT_SIGN | POINT_LETTER =>",
          "124:           tokens.append(DecimalPoint())",
          "125:           reachedDecimalPoint = true",
          "126:           i += 1",
          "127:         case COMMA_SIGN | COMMA_LETTER =>",
          "128:           tokens.append(ThousandsSeparator())",
          "129:           i += 1",
          "130:         case DOLLAR_SIGN =>",
          "131:           tokens.append(DollarSign())",
          "132:           i += 1",
          "133:         case OPTIONAL_PLUS_OR_MINUS_LETTER =>",
          "134:           tokens.append(OptionalPlusOrMinusSign())",
          "135:           i += 1",
          "136:         case OPTIONAL_MINUS_STRING_START if i < len - 1 &&",
          "137:           OPTIONAL_MINUS_STRING_END == numberFormat(i + 1) =>",
          "138:           tokens.append(OptionalMinusSign())",
          "139:           i += 2",
          "140:         case WRAPPING_ANGLE_BRACKETS_TO_NEGATIVE_NUMBER_START if i < len - 1 &&",
          "141:           WRAPPING_ANGLE_BRACKETS_TO_NEGATIVE_NUMBER_END == numberFormat(i + 1) =>",
          "142:           tokens.prepend(OpeningAngleBracket())",
          "143:           tokens.append(ClosingAngleBracket())",
          "144:           i += 2",
          "145:         case c: Char =>",
          "146:           tokens.append(InvalidUnrecognizedCharacter(c))",
          "147:           i += 1",
          "148:       }",
          "149:     }",
          "152:     val groupedTokens = mutable.Buffer.empty[InputToken]",
          "153:     var currentGroup = mutable.Buffer.empty[InputToken]",
          "154:     var currentDigits = mutable.Buffer.empty[Digits]",
          "155:     for (token <- tokens) {",
          "156:       token match {",
          "157:         case digits: Digits =>",
          "158:           currentGroup.append(token)",
          "159:           currentDigits.append(digits)",
          "160:         case _: ThousandsSeparator =>",
          "161:           currentGroup.append(token)",
          "162:         case other =>",
          "163:           if (currentGroup.nonEmpty) {",
          "167:             groupedTokens.append(",
          "168:               DigitGroups(currentGroup.reverse.toSeq, currentDigits.reverse.toSeq))",
          "169:             currentGroup = mutable.Buffer.empty[InputToken]",
          "170:             currentDigits = mutable.Buffer.empty[Digits]",
          "171:           }",
          "172:           groupedTokens.append(other)",
          "173:       }",
          "174:     }",
          "175:     if (currentGroup.nonEmpty) {",
          "176:       groupedTokens.append(DigitGroups(currentGroup.reverse.toSeq, currentDigits.reverse.toSeq))",
          "177:     }",
          "178:     groupedTokens.toSeq",
          "179:   }",
          "186:   private lazy val precision: Int = {",
          "187:     val lengths = formatTokens.map {",
          "188:       case DigitGroups(_, digits) => digits.map {",
          "189:         case ExactlyAsManyDigits(num) => num",
          "190:         case AtMostAsManyDigits(num) => num",
          "191:       }.sum",
          "192:       case _ => 0",
          "193:     }",
          "194:     lengths.sum",
          "195:   }",
          "197:   private lazy val scale: Int = {",
          "198:     val index = formatTokens.indexOf(DecimalPoint())",
          "199:     if (index != -1) {",
          "200:       val suffix: Seq[InputToken] = formatTokens.drop(index)",
          "201:       val lengths: Seq[Int] = suffix.map {",
          "202:         case DigitGroups(_, digits) => digits.map {",
          "203:           case ExactlyAsManyDigits(num) => num",
          "204:           case AtMostAsManyDigits(num) => num",
          "205:         }.sum",
          "206:         case _ => 0",
          "207:       }",
          "208:       lengths.sum",
          "209:     } else {",
          "210:       0",
          "211:     }",
          "212:   }",
          "215:   private lazy val beforeDecimalPoint = new StringBuilder(precision)",
          "217:   private lazy val afterDecimalPoint = new StringBuilder(scale)",
          "219:   private lazy val parsedDigitGroupSizes = mutable.Buffer.empty[Int]",
          "221:   private var numDigitsInCurrentGroup: Int = 0",
          "226:   def parsedDecimalType: DecimalType = DecimalType(precision, scale)",
          "231:   def check(): TypeCheckResult = {",
          "232:     val validateResult: String = validateFormatString",
          "233:     if (validateResult.nonEmpty) {",
          "234:       TypeCheckResult.TypeCheckFailure(validateResult)",
          "235:     } else {",
          "236:       TypeCheckResult.TypeCheckSuccess",
          "237:     }",
          "238:   }",
          "243:   private def validateFormatString: String = {",
          "244:     def multipleSignInNumberFormatError(message: String) = {",
          "245:       s\"At most one $message is allowed in the number format: '$numberFormat'\"",
          "246:     }",
          "248:     def notAtEndOfNumberFormatError(message: String) = {",
          "249:       s\"$message must be at the end of the number format: '$numberFormat'\"",
          "250:     }",
          "252:     val inputTokenCounts = formatTokens.groupBy(identity).mapValues(_.size)",
          "254:     val firstDollarSignIndex: Int = formatTokens.indexOf(DollarSign())",
          "255:     val firstDigitIndex: Int = formatTokens.indexWhere {",
          "256:       case _: DigitGroups => true",
          "257:       case _ => false",
          "258:     }",
          "259:     val firstDecimalPointIndex: Int = formatTokens.indexOf(DecimalPoint())",
          "260:     val digitGroupsBeforeDecimalPoint: Seq[DigitGroups] =",
          "261:       formatTokens.zipWithIndex.flatMap {",
          "262:         case (d@DigitGroups(_, _), i)",
          "263:           if firstDecimalPointIndex == -1 ||",
          "264:             i < firstDecimalPointIndex =>",
          "265:           Seq(d)",
          "266:         case _ => Seq()",
          "267:       }",
          "268:     val digitGroupsAfterDecimalPoint: Seq[DigitGroups] =",
          "269:       formatTokens.zipWithIndex.flatMap {",
          "270:         case (d@DigitGroups(_, _), i)",
          "271:           if firstDecimalPointIndex != -1 &&",
          "272:             i > firstDecimalPointIndex =>",
          "273:           Seq(d)",
          "274:         case _ => Seq()",
          "275:       }",
          "278:     if (numberFormat.isEmpty) {",
          "279:       \"The format string cannot be empty\"",
          "280:     }",
          "282:     else if (formatTokens.exists(_.isInstanceOf[InvalidUnrecognizedCharacter])) {",
          "283:       val unrecognizedChars =",
          "284:         formatTokens.filter {",
          "285:           _.isInstanceOf[InvalidUnrecognizedCharacter]",
          "286:         }.map {",
          "287:           case i: InvalidUnrecognizedCharacter => i.char",
          "288:         }",
          "289:       val char: Char = unrecognizedChars.head",
          "290:       s\"Encountered invalid character $char in the number format: '$numberFormat'\"",
          "291:     }",
          "293:     else if (!formatTokens.exists(",
          "294:       token => token.isInstanceOf[DigitGroups])) {",
          "295:       \"The format string requires at least one number digit\"",
          "296:     }",
          "298:     else if (inputTokenCounts.getOrElse(DecimalPoint(), 0) > 1) {",
          "299:       multipleSignInNumberFormatError(s\"'$POINT_LETTER' or '$POINT_SIGN'\")",
          "300:     }",
          "302:     else if (inputTokenCounts.getOrElse(OptionalPlusOrMinusSign(), 0) > 1) {",
          "303:       multipleSignInNumberFormatError(s\"'$OPTIONAL_PLUS_OR_MINUS_LETTER'\")",
          "304:     }",
          "306:     else if (inputTokenCounts.getOrElse(DollarSign(), 0) > 1) {",
          "307:       multipleSignInNumberFormatError(s\"'$DOLLAR_SIGN'\")",
          "308:     }",
          "310:     else if (inputTokenCounts.getOrElse(OptionalMinusSign(), 0) > 1) {",
          "311:       multipleSignInNumberFormatError(s\"'$OPTIONAL_MINUS_STRING'\")",
          "312:     }",
          "314:     else if (inputTokenCounts.getOrElse(ClosingAngleBracket(), 0) > 1 ||",
          "315:       (inputTokenCounts.getOrElse(ClosingAngleBracket(), 0) == 1 &&",
          "316:         formatTokens.last != ClosingAngleBracket())) {",
          "317:       notAtEndOfNumberFormatError(s\"'$WRAPPING_ANGLE_BRACKETS_TO_NEGATIVE_NUMBER'\")",
          "318:     }",
          "320:     else if (firstDigitIndex < firstDollarSignIndex) {",
          "321:       s\"Currency characters must appear before digits in the number format: '$numberFormat'\"",
          "322:     }",
          "324:     else if (firstDecimalPointIndex != -1 &&",
          "325:       firstDecimalPointIndex < firstDollarSignIndex) {",
          "326:       \"Currency characters must appear before any decimal point in the \" +",
          "327:         s\"number format: '$numberFormat'\"",
          "328:     }",
          "330:     else if (digitGroupsBeforeDecimalPoint.exists {",
          "331:       case DigitGroups(tokens, _) =>",
          "332:         tokens.zipWithIndex.exists({",
          "333:           case (_: ThousandsSeparator, j: Int) if j == 0 || j == tokens.length - 1 =>",
          "334:             true",
          "335:           case (_: ThousandsSeparator, j: Int) if tokens(j - 1).isInstanceOf[ThousandsSeparator] =>",
          "336:             true",
          "337:           case (_: ThousandsSeparator, j: Int) if tokens(j + 1).isInstanceOf[ThousandsSeparator] =>",
          "338:             true",
          "339:           case _ =>",
          "340:             false",
          "341:         })",
          "342:     }) {",
          "343:       \"Thousands separators (,) must have digits in between them \" +",
          "344:         s\"in the number format: '$numberFormat'\"",
          "345:     }",
          "347:     else if (digitGroupsAfterDecimalPoint.exists {",
          "348:       case DigitGroups(tokens, digits) =>",
          "349:         tokens.length > digits.length",
          "350:     }) {",
          "351:       \"Thousands separators (,) may not appear after the decimal point \" +",
          "352:         s\"in the number format: '$numberFormat'\"",
          "353:     }",
          "355:     else {",
          "356:       \"\"",
          "357:     }",
          "358:   }",
          "370:   def parse(input: UTF8String): Decimal = {",
          "371:     val inputString = input.toString",
          "372:     val inputLength = inputString.length",
          "374:     beforeDecimalPoint.clear()",
          "375:     afterDecimalPoint.clear()",
          "376:     var reachedDecimalPoint = false",
          "378:     var negateResult = false",
          "380:     var inputIndex = 0",
          "382:     var formatIndex = 0",
          "385:     while (formatIndex < formatTokens.size) {",
          "386:       val token: InputToken = formatTokens(formatIndex)",
          "387:       token match {",
          "388:         case d: DigitGroups =>",
          "389:           inputIndex = parseDigitGroups(d, inputString, inputIndex, reachedDecimalPoint).getOrElse(",
          "390:             return formatMatchFailure(input, numberFormat))",
          "391:         case DecimalPoint() =>",
          "392:           if (inputIndex < inputLength &&",
          "393:             inputString(inputIndex) == POINT_SIGN) {",
          "394:             reachedDecimalPoint = true",
          "395:             inputIndex += 1",
          "396:           } else {",
          "399:           }",
          "400:         case DollarSign() =>",
          "401:           if (inputIndex >= inputLength ||",
          "402:             inputString(inputIndex) != DOLLAR_SIGN) {",
          "404:             return formatMatchFailure(input, numberFormat)",
          "405:           }",
          "406:           inputIndex += 1",
          "407:         case OptionalPlusOrMinusSign() =>",
          "408:           if (inputIndex < inputLength &&",
          "409:             inputString(inputIndex) == PLUS_SIGN) {",
          "410:             inputIndex += 1",
          "411:           } else if (inputIndex < inputLength &&",
          "412:             inputString(inputIndex) == MINUS_SIGN) {",
          "413:             negateResult = !negateResult",
          "414:             inputIndex += 1",
          "415:           } else {",
          "418:           }",
          "419:         case OptionalMinusSign() =>",
          "420:           if (inputIndex < inputLength &&",
          "421:             inputString(inputIndex) == MINUS_SIGN) {",
          "422:             negateResult = !negateResult",
          "423:             inputIndex += 1",
          "424:           } else {",
          "427:           }",
          "428:         case OpeningAngleBracket() =>",
          "429:           if (inputIndex >= inputLength ||",
          "430:             inputString(inputIndex) != ANGLE_BRACKET_OPEN) {",
          "432:             return formatMatchFailure(input, numberFormat)",
          "433:           }",
          "434:           inputIndex += 1",
          "435:         case ClosingAngleBracket() =>",
          "436:           if (inputIndex >= inputLength ||",
          "437:             inputString(inputIndex) != ANGLE_BRACKET_CLOSE) {",
          "439:             return formatMatchFailure(input, numberFormat)",
          "440:           }",
          "441:           negateResult = !negateResult",
          "442:           inputIndex += 1",
          "443:       }",
          "444:       formatIndex += 1",
          "445:     }",
          "446:     if (inputIndex < inputLength) {",
          "449:       formatMatchFailure(input, numberFormat)",
          "450:     } else {",
          "451:       getDecimal(negateResult)",
          "452:     }",
          "453:   }",
          "464:   private def parseDigitGroups(",
          "465:       digitGroups: DigitGroups,",
          "466:       inputString: String,",
          "467:       startingInputIndex: Int,",
          "468:       reachedDecimalPoint: Boolean): Option[Int] = {",
          "469:     val expectedDigits: Seq[Digits] = digitGroups.digits",
          "470:     val inputLength = inputString.length",
          "473:     numDigitsInCurrentGroup = 0",
          "474:     var inputIndex = startingInputIndex",
          "475:     parsedDigitGroupSizes.clear()",
          "477:     while (inputIndex < inputLength &&",
          "478:       matchesDigitOrComma(inputString(inputIndex), reachedDecimalPoint)) {",
          "479:       inputIndex += 1",
          "480:     }",
          "481:     if (inputIndex == inputLength) {",
          "482:       parsedDigitGroupSizes.prepend(numDigitsInCurrentGroup)",
          "483:     }",
          "486:     if (parsedDigitGroupSizes.length > expectedDigits.length) {",
          "488:       return None",
          "489:     }",
          "490:     for (i <- 0 until expectedDigits.length) {",
          "491:       val expectedToken: Digits = expectedDigits(i)",
          "492:       val actualNumDigits: Int =",
          "493:         if (i < parsedDigitGroupSizes.length) {",
          "494:           parsedDigitGroupSizes(i)",
          "495:         } else {",
          "496:           0",
          "497:         }",
          "498:       expectedToken match {",
          "499:         case ExactlyAsManyDigits(expectedNumDigits)",
          "500:           if actualNumDigits != expectedNumDigits =>",
          "502:           return None",
          "503:         case AtMostAsManyDigits(expectedMaxDigits)",
          "504:           if actualNumDigits > expectedMaxDigits =>",
          "506:           return None",
          "507:         case _ =>",
          "508:       }",
          "509:     }",
          "510:     Some(inputIndex)",
          "511:   }",
          "517:   private def matchesDigitOrComma(char: Char, reachedDecimalPoint: Boolean): Boolean = {",
          "518:     char match {",
          "519:       case _ if char.isWhitespace =>",
          "521:         true",
          "522:       case _ if char >= ZERO_DIGIT && char <= NINE_DIGIT =>",
          "523:         numDigitsInCurrentGroup += 1",
          "526:         if (reachedDecimalPoint) {",
          "527:           afterDecimalPoint.append(char)",
          "528:         } else {",
          "529:           beforeDecimalPoint.append(char)",
          "530:         }",
          "531:         true",
          "532:       case COMMA_SIGN =>",
          "533:         parsedDigitGroupSizes.prepend(numDigitsInCurrentGroup)",
          "534:         numDigitsInCurrentGroup = 0",
          "535:         true",
          "536:       case _ =>",
          "537:         parsedDigitGroupSizes.prepend(numDigitsInCurrentGroup)",
          "538:         false",
          "539:     }",
          "540:   }",
          "546:   private def formatMatchFailure(input: UTF8String, originNumberFormat: String): Decimal = {",
          "547:     if (errorOnFail) {",
          "548:       throw QueryExecutionErrors.invalidNumberFormatError(input, originNumberFormat)",
          "549:     }",
          "550:     null",
          "551:   }",
          "561:   private def getDecimal(negateResult: Boolean): Decimal = {",
          "565:     val extraZeros = \"0\" * (scale - afterDecimalPoint.length)",
          "566:     val afterDecimalPadded = afterDecimalPoint.toString + extraZeros",
          "567:     val prefix = if (negateResult) \"-\" else \"\"",
          "568:     val suffix = if (afterDecimalPadded.nonEmpty) \".\" + afterDecimalPadded else \"\"",
          "569:     val numStr = s\"$prefix$beforeDecimalPoint$suffix\"",
          "570:     val javaDecimal = new java.math.BigDecimal(numStr)",
          "571:     if (precision <= Decimal.MAX_LONG_DIGITS) {",
          "573:       Decimal(javaDecimal.unscaledValue().longValue(), precision, scale)",
          "574:     } else {",
          "576:       Decimal(javaDecimal, precision, scale)",
          "577:     }",
          "578:   }",
          "579: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.catalyst.expressions",
          "20: import org.apache.spark.SparkFunSuite",
          "21: import org.apache.spark.sql.catalyst.analysis.TypeCheckResult",
          "22: import org.apache.spark.sql.catalyst.dsl.expressions._",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import java.math.{BigDecimal => JavaBigDecimal}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "892:       Literal.create(null, IntegerType), Literal.create(null, IntegerType)), null)",
          "893:   }",
          "946:     Seq(",
          "962:     }",
          "968:       }",
          "969:     }",
          "1013:     Seq(",
          "1029:     }",
          "1032:     Seq(",
          "1058:     }",
          "1059:   }",
          "",
          "[Removed Lines]",
          "895:   test(\"ToNumber\") {",
          "896:     ToNumber(Literal(\"454\"), Literal(\"\")).checkInputDataTypes() match {",
          "897:       case TypeCheckResult.TypeCheckFailure(msg) =>",
          "898:         assert(msg.contains(\"Number format cannot be empty\"))",
          "899:     }",
          "900:     ToNumber(Literal(\"454\"), NonFoldableLiteral.create(\"999\", StringType))",
          "901:       .checkInputDataTypes() match {",
          "902:       case TypeCheckResult.TypeCheckFailure(msg) =>",
          "903:         assert(msg.contains(\"Format expression must be foldable\"))",
          "904:     }",
          "908:     Seq(\"454\", \"054\", \"54\", \"450\").foreach { input =>",
          "909:       val invalidFormat1 = 0.until(input.length - 1).map(_ => '0').mkString",
          "910:       val invalidFormat2 = 0.until(input.length - 2).map(_ => '0').mkString",
          "911:       val invalidFormat3 = 0.until(input.length - 1).map(_ => '9').mkString",
          "912:       val invalidFormat4 = 0.until(input.length - 2).map(_ => '9').mkString",
          "913:       Seq(invalidFormat1, invalidFormat2, invalidFormat3, invalidFormat4)",
          "914:         .filter(_.nonEmpty).foreach { format =>",
          "915:         checkExceptionInExpression[IllegalArgumentException](",
          "916:           ToNumber(Literal(input), Literal(format)),",
          "917:           s\"The input string '$input' does not match the given number format: '$format'\")",
          "918:       }",
          "920:       val format1 = 0.until(input.length).map(_ => '0').mkString",
          "921:       val format2 = 0.until(input.length).map(_ => '9').mkString",
          "922:       val format3 = 0.until(input.length).map(i => i % 2 * 9).mkString",
          "923:       val format4 = 0.until(input.length + 1).map(_ => '0').mkString",
          "924:       val format5 = 0.until(input.length + 1).map(_ => '9').mkString",
          "925:       val format6 = 0.until(input.length + 1).map(i => i % 2 * 9).mkString",
          "926:       Seq(format1, format2, format3, format4, format5, format6).foreach { format =>",
          "927:         checkEvaluation(ToNumber(Literal(input), Literal(format)), Decimal(input))",
          "928:       }",
          "929:     }",
          "932:     checkExceptionInExpression[IllegalArgumentException](",
          "933:       ToNumber(Literal(\"454.2\"), Literal(\"999\")),",
          "934:       \"The input string '454.2' does not match the given number format: '999'\")",
          "935:     Seq(\"999.9\", \"000.0\", \"99.99\", \"00.00\", \"0000.0\", \"9999.9\", \"00.000\", \"99.999\")",
          "936:       .foreach { format =>",
          "937:         checkExceptionInExpression[IllegalArgumentException](",
          "938:           ToNumber(Literal(\"454.23\"), Literal(format)),",
          "939:           s\"The input string '454.23' does not match the given number format: '$format'\")",
          "940:         val format2 = format.replace('.', 'D')",
          "941:         checkExceptionInExpression[IllegalArgumentException](",
          "942:           ToNumber(Literal(\"454.23\"), Literal(format2)),",
          "943:           s\"The input string '454.23' does not match the given number format: '$format2'\")",
          "944:     }",
          "947:       (\"454.2\", \"000.0\") -> Decimal(454.2),",
          "948:       (\"454.23\", \"000.00\") -> Decimal(454.23),",
          "949:       (\"454.2\", \"000.00\") -> Decimal(454.2),",
          "950:       (\"454.0\", \"000.0\") -> Decimal(454),",
          "951:       (\"454.00\", \"000.00\") -> Decimal(454),",
          "952:       (\".4542\", \".0000\") -> Decimal(0.4542),",
          "953:       (\"4542.\", \"0000.\") -> Decimal(4542)",
          "954:     ).foreach { case ((str, format), expected) =>",
          "955:       checkEvaluation(ToNumber(Literal(str), Literal(format)), expected)",
          "956:       val format2 = format.replace('.', 'D')",
          "957:       checkEvaluation(ToNumber(Literal(str), Literal(format2)), expected)",
          "958:       val format3 = format.replace('0', '9')",
          "959:       checkEvaluation(ToNumber(Literal(str), Literal(format3)), expected)",
          "960:       val format4 = format3.replace('.', 'D')",
          "961:       checkEvaluation(ToNumber(Literal(str), Literal(format4)), expected)",
          "964:     Seq(\"999.9.9\", \"999D9D9\", \"999.9D9\", \"999D9.9\").foreach { str =>",
          "965:       ToNumber(Literal(\"454.3.2\"), Literal(str)).checkInputDataTypes() match {",
          "966:         case TypeCheckResult.TypeCheckFailure(msg) =>",
          "967:           assert(msg.contains(s\"At most one 'D' or '.' is allowed in the number format: '$str'\"))",
          "972:     checkExceptionInExpression[IllegalArgumentException](",
          "973:       ToNumber(Literal(\"123,456\"), Literal(\"9G9\")),",
          "974:       \"The input string '123,456' does not match the given number format: '9G9'\")",
          "975:     checkExceptionInExpression[IllegalArgumentException](",
          "976:       ToNumber(Literal(\"123,456,789\"), Literal(\"999,999\")),",
          "977:       \"The input string '123,456,789' does not match the given number format: '999,999'\")",
          "979:     Seq(",
          "980:       (\"12,454\", \"99,999\") -> Decimal(12454),",
          "981:       (\"12,454\", \"99,999,999\") -> Decimal(12454),",
          "982:       (\"12,454,367\", \"99,999,999\") -> Decimal(12454367),",
          "983:       (\"12,454,\", \"99,999,\") -> Decimal(12454),",
          "984:       (\",454,367\", \",999,999\") -> Decimal(454367),",
          "985:       (\",454,367\", \"999,999\") -> Decimal(454367)",
          "986:     ).foreach { case ((str, format), expected) =>",
          "987:       checkEvaluation(ToNumber(Literal(str), Literal(format)), expected)",
          "988:       val format2 = format.replace(',', 'G')",
          "989:       checkEvaluation(ToNumber(Literal(str), Literal(format2)), expected)",
          "990:       val format3 = format.replace('9', '0')",
          "991:       checkEvaluation(ToNumber(Literal(str), Literal(format3)), expected)",
          "992:       val format4 = format3.replace(',', 'G')",
          "993:       checkEvaluation(ToNumber(Literal(str), Literal(format4)), expected)",
          "994:       val format5 = s\"${format}9\"",
          "995:       checkEvaluation(ToNumber(Literal(str), Literal(format5)), expected)",
          "996:       val format6 = s\"${format}0\"",
          "997:       checkEvaluation(ToNumber(Literal(str), Literal(format6)), expected)",
          "998:       val format7 = s\"9${format}9\"",
          "999:       checkEvaluation(ToNumber(Literal(str), Literal(format7)), expected)",
          "1000:       val format8 = s\"0${format}0\"",
          "1001:       checkEvaluation(ToNumber(Literal(str), Literal(format8)), expected)",
          "1002:       val format9 = s\"${format3}9\"",
          "1003:       checkEvaluation(ToNumber(Literal(str), Literal(format9)), expected)",
          "1004:       val format10 = s\"${format3}0\"",
          "1005:       checkEvaluation(ToNumber(Literal(str), Literal(format10)), expected)",
          "1006:       val format11 = s\"9${format3}9\"",
          "1007:       checkEvaluation(ToNumber(Literal(str), Literal(format11)), expected)",
          "1008:       val format12 = s\"0${format3}0\"",
          "1009:       checkEvaluation(ToNumber(Literal(str), Literal(format12)), expected)",
          "1010:     }",
          "1014:       (\"$78.12\", \"$99.99\") -> Decimal(78.12),",
          "1015:       (\"$78.12\", \"$00.00\") -> Decimal(78.12),",
          "1016:       (\"78.12$\", \"99.99$\") -> Decimal(78.12),",
          "1017:       (\"78.12$\", \"00.00$\") -> Decimal(78.12)",
          "1018:     ).foreach { case ((str, format), expected) =>",
          "1019:       checkEvaluation(ToNumber(Literal(str), Literal(format)), expected)",
          "1020:     }",
          "1022:     ToNumber(Literal(\"$78$.12\"), Literal(\"$99$.99\")).checkInputDataTypes() match {",
          "1023:       case TypeCheckResult.TypeCheckFailure(msg) =>",
          "1024:         assert(msg.contains(\"At most one '$' is allowed in the number format: '$99$.99'\"))",
          "1025:     }",
          "1026:     ToNumber(Literal(\"78$.12\"), Literal(\"99$.99\")).checkInputDataTypes() match {",
          "1027:       case TypeCheckResult.TypeCheckFailure(msg) =>",
          "1028:         assert(msg.contains(\"'$' must be the first or last char in the number format: '99$.99'\"))",
          "1033:       (\"454-\", \"999-\") -> Decimal(-454),",
          "1034:       (\"-454\", \"-999\") -> Decimal(-454),",
          "1035:       (\"12,454.8-\", \"99G999D9-\") -> Decimal(-12454.8),",
          "1036:       (\"00,454.8-\", \"99G999.9-\") -> Decimal(-454.8)",
          "1037:     ).foreach { case ((str, format), expected) =>",
          "1038:       checkEvaluation(ToNumber(Literal(str), Literal(format)), expected)",
          "1039:       val format2 = format.replace('9', '0')",
          "1040:       checkEvaluation(ToNumber(Literal(str), Literal(format2)), expected)",
          "1041:       val format3 = format.replace('-', 'S')",
          "1042:       checkEvaluation(ToNumber(Literal(str), Literal(format3)), expected)",
          "1043:       val format4 = format2.replace('-', 'S')",
          "1044:       checkEvaluation(ToNumber(Literal(str), Literal(format4)), expected)",
          "1045:     }",
          "1047:     ToNumber(Literal(\"454.3--\"), Literal(\"999D9SS\")).checkInputDataTypes() match {",
          "1048:       case TypeCheckResult.TypeCheckFailure(msg) =>",
          "1049:         assert(msg.contains(\"At most one 'S' or '-' is allowed in the number format: '999D9SS'\"))",
          "1050:     }",
          "1052:     Seq(\"9S99\", \"9-99\").foreach { str =>",
          "1053:       ToNumber(Literal(\"-454\"), Literal(str)).checkInputDataTypes() match {",
          "1054:         case TypeCheckResult.TypeCheckFailure(msg) =>",
          "1055:           assert(msg.contains(",
          "1056:             s\"'S' or '-' must be the first or last char in the number format: '$str'\"))",
          "1057:       }",
          "",
          "[Added Lines]",
          "897:   test(\"ToNumber: positive tests\") {",
          "899:       (\"$345\", \"S$999,099.99\") -> Decimal(345),",
          "900:       (\"-$12,345.67\", \"S$999,099.99\") -> Decimal(-12345.67),",
          "901:       (\"454,123\", \"999,099\") -> Decimal(454123),",
          "902:       (\"$045\", \"S$999,099.99\") -> Decimal(45),",
          "903:       (\"454\", \"099\") -> Decimal(454),",
          "904:       (\"454.\", \"099.99\") -> Decimal(454.0),",
          "905:       (\"454.6\", \"099D99\") -> Decimal(454.6),",
          "906:       (\"454.67\", \"099.00\") -> Decimal(454.67),",
          "907:       (\"454\", \"000\") -> Decimal(454),",
          "908:       (\"  454 \", \"9099\") -> Decimal(454),",
          "909:       (\"454\", \"099\") -> Decimal(454),",
          "910:       (\"454.67\", \"099.99\") -> Decimal(454.67),",
          "911:       (\"$454\", \"$999\") -> Decimal(454),",
          "912:       (\"  454,123 \", \"999G099\") -> Decimal(454123),",
          "913:       (\"$454,123\", \"$999G099\") -> Decimal(454123),",
          "914:       (\"+$89,1,2,3,45.123\", \"S$999,0,0,0,999.00000\") -> Decimal(8912345.123),",
          "915:       (\"-454\", \"S999\") -> Decimal(-454),",
          "916:       (\"+454\", \"S999\") -> Decimal(454),",
          "917:       (\"<454>\", \"999PR\") -> Decimal(-454),",
          "918:       (\"454-\", \"999MI\") -> Decimal(-454),",
          "919:       (\"-$54\", \"MI$99\") -> Decimal(-54),",
          "920:       (\"$4-4\", \"$9MI9\") -> Decimal(-44),",
          "922:       (\"123,456,789,123,456,789,123\", \"999,999,999,999,999,999,999\") ->",
          "923:         Decimal(new JavaBigDecimal(\"123456789123456789123\"))",
          "924:     ).foreach { case ((str: String, format: String), expected: Decimal) =>",
          "925:       val toNumberExpr = ToNumber(Literal(str), Literal(format))",
          "926:       assert(toNumberExpr.checkInputDataTypes() == TypeCheckResult.TypeCheckSuccess)",
          "927:       checkEvaluation(toNumberExpr, expected)",
          "929:       val tryToNumberExpr = TryToNumber(Literal(str), Literal(format))",
          "930:       assert(tryToNumberExpr.checkInputDataTypes() == TypeCheckResult.TypeCheckSuccess)",
          "931:       checkEvaluation(tryToNumberExpr, expected)",
          "934:     for (i <- 0 to 2) {",
          "935:       for (j <- 3 to 5) {",
          "936:         for (k <- 6 to 9) {",
          "937:           Seq(",
          "938:             (s\"$i$j$k\", \"999\") -> Decimal(s\"$i$j$k\".toInt),",
          "939:             (s\"$i$j$k\", \"S099.\") -> Decimal(s\"$i$j$k\".toInt),",
          "940:             (s\"$i$j.$k\", \"99.9\") -> Decimal(s\"$i$j.$k\".toDouble),",
          "941:             (s\"$i,$j,$k\", \"999,999,0\") -> Decimal(s\"$i$j$k\".toInt)",
          "942:           ).foreach { case ((str: String, format: String), expected: Decimal) =>",
          "943:             val toNumberExpr = ToNumber(Literal(str), Literal(format))",
          "944:             assert(toNumberExpr.checkInputDataTypes() == TypeCheckResult.TypeCheckSuccess)",
          "945:             checkEvaluation(toNumberExpr, expected)",
          "947:             val tryToNumberExpr = TryToNumber(Literal(str), Literal(format))",
          "948:             assert(tryToNumberExpr.checkInputDataTypes() == TypeCheckResult.TypeCheckSuccess)",
          "949:             checkEvaluation(tryToNumberExpr, expected)",
          "950:           }",
          "951:         }",
          "954:   }",
          "956:   test(\"ToNumber: negative tests (the format string is invalid)\") {",
          "957:     val invalidCharacter = \"Encountered invalid character\"",
          "958:     val thousandsSeparatorDigitsBetween =",
          "959:       \"Thousands separators (,) must have digits in between them\"",
          "960:     val mustBeAtEnd = \"must be at the end of the number format\"",
          "961:     val atMostOne = \"At most one\"",
          "964:       (\"454\", \"\") -> \"The format string cannot be empty\",",
          "966:       (\"454\", \"999@\") -> invalidCharacter,",
          "967:       (\"454\", \"999M\") -> invalidCharacter,",
          "968:       (\"454\", \"999P\") -> invalidCharacter,",
          "970:       (\"454\", \"$\") -> \"The format string requires at least one number digit\",",
          "972:       (\"454\", \"99.99.99\") -> atMostOne,",
          "974:       (\"454\", \"$$99\") -> atMostOne,",
          "976:       (\"--$54\", \"SS$99\") -> atMostOne,",
          "977:       (\"-$54\", \"MI$99MI\") -> atMostOne,",
          "978:       (\"$4-4\", \"$9MI9MI\") -> atMostOne,",
          "980:       (\"<$45>\", \"PR$99\") -> mustBeAtEnd,",
          "981:       (\"$4<4>\", \"$9PR9\") -> mustBeAtEnd,",
          "982:       (\"<<454>>\", \"999PRPR\") -> mustBeAtEnd,",
          "984:       (\"4$54\", \"9$99\") -> \"Currency characters must appear before digits\",",
          "986:       (\".$99\", \".$99\") -> \"Currency characters must appear before any decimal point\",",
          "988:       (\",123\", \",099\") -> thousandsSeparatorDigitsBetween,",
          "989:       (\",123,456\", \",999,099\") -> thousandsSeparatorDigitsBetween,",
          "990:       (\",,345\", \"9,,09.99\") -> thousandsSeparatorDigitsBetween,",
          "991:       (\",,345\", \"9,99,.99\") -> thousandsSeparatorDigitsBetween,",
          "992:       (\",,345\", \"9,99,\") -> thousandsSeparatorDigitsBetween,",
          "993:       (\",,345\", \",,999,099.99\") -> thousandsSeparatorDigitsBetween,",
          "995:       (\"123.45,6\", \"099.99,9\") -> \"Thousands separators (,) may not appear after the decimal point\"",
          "996:     ).foreach { case ((str: String, format: String), expectedErrMsg: String) =>",
          "997:       val toNumberResult = ToNumber(Literal(str), Literal(format)).checkInputDataTypes()",
          "998:       assert(toNumberResult != TypeCheckResult.TypeCheckSuccess,",
          "999:         s\"The format string should have been invalid: $format\")",
          "1000:       toNumberResult match {",
          "1001:         case TypeCheckResult.TypeCheckFailure(message) =>",
          "1002:           assert(message.contains(expectedErrMsg))",
          "1003:       }",
          "1005:       val tryToNumberResult = TryToNumber(Literal(str), Literal(format)).checkInputDataTypes()",
          "1006:       assert(tryToNumberResult != TypeCheckResult.TypeCheckSuccess,",
          "1007:         s\"The format string should have been invalid: $format\")",
          "1008:       tryToNumberResult match {",
          "1009:         case TypeCheckResult.TypeCheckFailure(message) =>",
          "1010:           assert(message.contains(expectedErrMsg))",
          "1011:       }",
          "1013:   }",
          "1015:   test(\"ToNumber: negative tests (the input string does not match the format string)\") {",
          "1018:       (\"45\", \"0,9\"),",
          "1020:       (\"4\", \"09\"),",
          "1021:       (\"454\", \"09\"),",
          "1023:       (\"454\", \"99\"),",
          "1025:       (\"45\", \"$99\"),",
          "1027:       (\"45>\", \"99PR\"),",
          "1029:       (\"<45\", \"99PR\"),",
          "1031:       (\"454+\", \"999MI\"),",
          "1033:       (\"<454\", \"999PR\"),",
          "1034:       (\"454>\", \"999PR\"),",
          "1035:       (\"<<454>>\", \"999PR\"),",
          "1037:       (\"45\", \"S$999,099.99\"),",
          "1039:       (\"$345\", \"S$099,099.99\"),",
          "1041:       (\"4D5\", \"0D9\")",
          "1042:     ).foreach { case (str: String, format: String) =>",
          "1043:       val toNumberExpr = ToNumber(Literal(str), Literal(format))",
          "1044:       assert(toNumberExpr.checkInputDataTypes() == TypeCheckResult.TypeCheckSuccess)",
          "1045:       checkExceptionInExpression[IllegalArgumentException](",
          "1046:         toNumberExpr, \"does not match the given number format\")",
          "1048:       val tryToNumberExpr = TryToNumber(Literal(str), Literal(format))",
          "1049:       assert(tryToNumberExpr.checkInputDataTypes() == TypeCheckResult.TypeCheckSuccess)",
          "1050:       checkEvaluation(tryToNumberExpr, null)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/NumberFormatterSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/NumberFormatterSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/NumberFormatterSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/NumberFormatterSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/string-functions.sql||sql/core/src/test/resources/sql-tests/inputs/string-functions.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/string-functions.sql -> sql/core/src/test/resources/sql-tests/inputs/string-functions.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "162: select to_number('454.2', '000.0');",
          "163: select to_number('12,454', '00,000');",
          "164: select to_number('$78.12', '$00.00');",
          "166: select to_number('-454', 'S000');",
          "170: -- to_binary",
          "171: select to_binary('abc');",
          "",
          "[Removed Lines]",
          "165: select to_number('-454', '-000');",
          "167: select to_number('12,454.8-', '00,000.9-');",
          "168: select to_number('00,454.8-', '00,000.9-');",
          "",
          "[Added Lines]",
          "165: select to_number('+454', 'S000');",
          "167: select to_number('12,454.8-', '00,000.9MI');",
          "168: select to_number('00,454.8-', '00,000.9MI');",
          "169: select to_number('<00,454.8>', '00,000.9PR');",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "9926b093056a8896e84aedb42ad62e08a3f4950c",
      "candidate_info": {
        "commit_hash": "9926b093056a8896e84aedb42ad62e08a3f4950c",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/9926b093056a8896e84aedb42ad62e08a3f4950c",
        "files": [
          "core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala",
          "python/pyspark/tests/test_rdd.py"
        ],
        "message": "[SPARK-38677][PYSPARK] Python MonitorThread should detect deadlock due to blocking I/O\n\n### What changes were proposed in this pull request?\nWhen calling a Python UDF on a DataFrame with large rows, a deadlock can occur involving the following three threads:\n\n1. The Scala task executor thread. During task execution, this is responsible for reading output produced by the Python process. However, in this case the task has finished early, and this thread is no longer reading output produced by the Python process. Instead, it is waiting for the Scala WriterThread to exit so that it can finish the task.\n2. The Scala WriterThread. This is trying to send a large row to the Python process, and is waiting for the Python process to read that row.\n3. The Python process. This is trying to send a large output to the Scala task executor thread, and is waiting for that thread to read that output, which will never happen.\n\nWe considered the following three solutions for the deadlock:\n\n1. When the task completes, make the Scala task executor thread close the socket before waiting for the Scala WriterThread to exit. If the WriterThread is blocked on a large write, this would interrupt that write and allow the WriterThread to exit. However, it would prevent Python worker reuse.\n2. Modify PythonWorkerFactory to use interruptible I/O. [java.nio.channels.SocketChannel](https://docs.oracle.com/javase/6/docs/api/java/nio/channels/SocketChannel.html#write(java.nio.ByteBuffer)) supports interruptible blocking operations. The goal is that when the WriterThread is interrupted, it should exit even if it was blocked on a large write. However, this would be invasive.\n3. Add a watchdog thread similar to the existing PythonRunner.MonitorThread to detect this deadlock and kill the Python worker. The MonitorThread currently kills the Python worker only if the task itself is interrupted. In this case, the task completes normally, so the MonitorThread does not take action. We want the new watchdog thread (WriterMonitorThread) to detect that the task is completed but the Python writer thread has not stopped, indicating a deadlock.\n\nThis PR implements Option 3.\n\n### Why are the changes needed?\nTo fix a deadlock that can cause PySpark queries to hang.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nAdded a test that previously encountered the deadlock and timed out, and now succeeds.\n\nCloses #36065 from ankurdave/SPARK-38677.\n\nAuthored-by: Ankur Dave <ankurdave@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 088e05d2518883aa27d0b8144107e45f41dd6b90)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala||core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala",
          "python/pyspark/tests/test_rdd.py||python/pyspark/tests/test_rdd.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala||core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala": [
          "File: core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala -> core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "183:     }",
          "185:     writerThread.start()",
          "186:     if (reuseWorker) {",
          "187:       val key = (worker, context.taskAttemptId)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "186:     new WriterMonitorThread(SparkEnv.get, worker, writerThread, context).start()",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "646:       }",
          "647:     }",
          "648:   }",
          "649: }",
          "651: private[spark] object PythonRunner {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "659:   class WriterMonitorThread(",
          "660:       env: SparkEnv, worker: Socket, writerThread: WriterThread, context: TaskContext)",
          "661:     extends Thread(s\"Writer Monitor for $pythonExec (writer thread id ${writerThread.getId})\") {",
          "667:     private val taskKillTimeout = env.conf.get(PYTHON_TASK_KILL_TIMEOUT)",
          "669:     setDaemon(true)",
          "671:     override def run(): Unit = {",
          "674:       while (!context.isCompleted && writerThread.isAlive) {",
          "675:         Thread.sleep(2000)",
          "676:       }",
          "677:       if (writerThread.isAlive) {",
          "678:         Thread.sleep(taskKillTimeout)",
          "681:         if (writerThread.isAlive) {",
          "682:           try {",
          "684:             val taskName = s\"${context.partitionId}.${context.attemptNumber} \" +",
          "685:               s\"in stage ${context.stageId} (TID ${context.taskAttemptId})\"",
          "686:             logWarning(",
          "687:               s\"Detected deadlock while completing task $taskName: \" +",
          "688:                 \"Attempting to kill Python Worker\")",
          "689:             env.destroyPythonWorker(pythonExec, envVars.asScala.toMap, worker)",
          "690:           } catch {",
          "691:             case e: Exception =>",
          "692:               logError(\"Exception when trying to kill worker\", e)",
          "693:           }",
          "694:         }",
          "695:       }",
          "696:     }",
          "697:   }",
          "",
          "---------------"
        ],
        "python/pyspark/tests/test_rdd.py||python/pyspark/tests/test_rdd.py": [
          "File: python/pyspark/tests/test_rdd.py -> python/pyspark/tests/test_rdd.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "34:     UTF8Deserializer,",
          "35:     NoOpSerializer,",
          "36: )",
          "37: from pyspark.testing.utils import ReusedPySparkTestCase, SPARK_HOME, QuietTest",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "37: from pyspark.sql import SparkSession",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "697:         rdd = self.sc.parallelize(range(1 << 20)).map(lambda x: str(x))",
          "698:         rdd._jrdd.first()",
          "700:     def test_sortByKey_uses_all_partitions_not_only_first_and_last(self):",
          "701:         # Regression test for SPARK-5969",
          "702:         seq = [(i * 59 % 101, i) for i in range(101)]  # unsorted sequence",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "701:     def test_take_on_jrdd_with_large_rows_should_not_cause_deadlock(self):",
          "702:         # Regression test for SPARK-38677.",
          "703:         #",
          "704:         # Create a DataFrame with many columns, call a Python function on each row, and take only",
          "705:         # the first result row.",
          "706:         #",
          "707:         # This produces large rows that trigger a deadlock involving the following three threads:",
          "708:         #",
          "709:         # 1. The Scala task executor thread. During task execution, this is responsible for reading",
          "710:         #    output produced by the Python process. However, in this case the task has finished",
          "711:         #    early, and this thread is no longer reading output produced by the Python process.",
          "712:         #    Instead, it is waiting for the Scala WriterThread to exit so that it can finish the",
          "713:         #    task.",
          "714:         #",
          "715:         # 2. The Scala WriterThread. This is trying to send a large row to the Python process, and",
          "716:         #    is waiting for the Python process to read that row.",
          "717:         #",
          "718:         # 3. The Python process. This is trying to send a large output to the Scala task executor",
          "719:         #    thread, and is waiting for that thread to read that output.",
          "720:         #",
          "721:         # For this test to succeed rather than hanging, the Scala MonitorThread must detect this",
          "722:         # deadlock and kill the Python worker.",
          "723:         import numpy as np",
          "724:         import pandas as pd",
          "726:         num_rows = 100000",
          "727:         num_columns = 134",
          "728:         data = np.zeros((num_rows, num_columns))",
          "729:         columns = map(str, range(num_columns))",
          "730:         df = SparkSession(self.sc).createDataFrame(pd.DataFrame(data, columns=columns))",
          "731:         actual = CPickleSerializer().loads(df.rdd.map(list)._jrdd.first())",
          "732:         expected = [list(data[0])]",
          "733:         self.assertEqual(expected, actual)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c759151838b7515a3d5fc5abb33d0d93e067cd75",
      "candidate_info": {
        "commit_hash": "c759151838b7515a3d5fc5abb33d0d93e067cd75",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/c759151838b7515a3d5fc5abb33d0d93e067cd75",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/RegexpExpressionsSuite.scala"
        ],
        "message": "[SPARK-39107][SQL] Account for empty string input in regex replace\n\n### What changes were proposed in this pull request?\n\nWhen trying to perform a regex replace, account for the possibility of having empty strings as input.\n\n### Why are the changes needed?\n\nhttps://github.com/apache/spark/pull/29891 was merged to address https://issues.apache.org/jira/browse/SPARK-30796 and introduced a bug that would not allow regex matching on empty strings, as it would account for position within substring but not consider the case where input string has length 0 (empty string)\n\nFrom https://issues.apache.org/jira/browse/SPARK-39107 there is a change in behavior between spark versions.\n3.0.2\n```\nscala> val df = spark.sql(\"SELECT '' AS col\")\ndf: org.apache.spark.sql.DataFrame = [col: string]\n\nscala> df.withColumn(\"replaced\", regexp_replace(col(\"col\"), \"^$\", \"<empty>\")).show\n+---+--------+\n|col|replaced|\n+---+--------+\n|   | <empty>|\n+---+--------+\n```\n3.1.2\n```\nscala> val df = spark.sql(\"SELECT '' AS col\")\ndf: org.apache.spark.sql.DataFrame = [col: string]\n\nscala> df.withColumn(\"replaced\", regexp_replace(col(\"col\"), \"^$\", \"<empty>\")).show\n+---+--------+\n|col|replaced|\n+---+--------+\n|   |        |\n+---+--------+\n```\n\nThe 3.0.2 outcome is the expected and correct one\n\n### Does this PR introduce _any_ user-facing change?\n\nYes compared to spark 3.2.1, as it brings back the correct behavior when trying to regex match empty strings, as shown in the example above.\n\n### How was this patch tested?\n\nAdded special casing test in `RegexpExpressionsSuite.RegexReplace` with empty string replacement.\n\nCloses #36457 from LorenzoMartini/lmartini/fix-empty-string-replace.\n\nAuthored-by: Lorenzo Martini <lmartini@palantir.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n(cherry picked from commit 731aa2cdf8a78835621fbf3de2d3492b27711d1a)\nSigned-off-by: Sean Owen <srowen@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/RegexpExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/RegexpExpressionsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "642:     }",
          "643:     val source = s.toString()",
          "644:     val position = i.asInstanceOf[Int] - 1",
          "646:       val m = pattern.matcher(source)",
          "647:       m.region(position, source.length)",
          "648:       result.delete(0, result.length())",
          "",
          "[Removed Lines]",
          "645:     if (position < source.length) {",
          "",
          "[Added Lines]",
          "645:     if (position == 0 || position < source.length) {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "696:       }",
          "697:       String $source = $subject.toString();",
          "698:       int $position = $pos - 1;",
          "700:         $classNameStringBuffer $termResult = new $classNameStringBuffer();",
          "701:         java.util.regex.Matcher $matcher = $termPattern.matcher($source);",
          "702:         $matcher.region($position, $source.length());",
          "",
          "[Removed Lines]",
          "699:       if ($position < $source.length()) {",
          "",
          "[Added Lines]",
          "699:       if ($position == 0 || $position < $source.length()) {",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/RegexpExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/RegexpExpressionsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/RegexpExpressionsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/RegexpExpressionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "293:     val row4 = create_row(null, \"(\\\\d+)\", \"###\")",
          "294:     val row5 = create_row(\"100-200\", null, \"###\")",
          "295:     val row6 = create_row(\"100-200\", \"(-)\", null)",
          "297:     val s = 's.string.at(0)",
          "298:     val p = 'p.string.at(1)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "296:     val row7 = create_row(\"\", \"^$\", \"<empty string>\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "305:     checkEvaluation(expr, null, row4)",
          "306:     checkEvaluation(expr, null, row5)",
          "307:     checkEvaluation(expr, null, row6)",
          "309:     val exprWithPos = RegExpReplace(s, p, r, 4)",
          "310:     checkEvaluation(exprWithPos, \"100-num\", row1)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "309:     checkEvaluation(expr, \"<empty string>\", row7)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "313:     checkEvaluation(exprWithPos, null, row4)",
          "314:     checkEvaluation(exprWithPos, null, row5)",
          "315:     checkEvaluation(exprWithPos, null, row6)",
          "316:     val exprWithLargePos = RegExpReplace(s, p, r, 7)",
          "317:     checkEvaluation(exprWithLargePos, \"100-20num\", row1)",
          "318:     checkEvaluation(exprWithLargePos, \"100-20###\", row2)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "318:     checkEvaluation(exprWithPos, \"\", row7)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "9fdd097aa6c05e7ecfd33dccad876a00d96b6ddf",
      "candidate_info": {
        "commit_hash": "9fdd097aa6c05e7ecfd33dccad876a00d96b6ddf",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/9fdd097aa6c05e7ecfd33dccad876a00d96b6ddf",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/joins/BroadcastJoinSuite.scala",
          "sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveSparkSubmitSuite.scala"
        ],
        "message": "[SPARK-39879][SQL][TESTS] Reduce local-cluster maximum memory size in `BroadcastJoinSuite*` and `HiveSparkSubmitSuite`\n\n### What changes were proposed in this pull request?\nThis pr change `local-cluster[2, 1, 1024]` in `BroadcastJoinSuite*` and `HiveSparkSubmitSuite` to `local-cluster[2, 1, 512]` to reduce test maximum memory usage.\n\n### Why are the changes needed?\nReduce the maximum memory usage of test cases.\n\n### Does this PR introduce _any_ user-facing change?\nNo, test-only.\n\n### How was this patch tested?\nShould monitor CI\n\nCloses #37298 from LuciferYang/reduce-local-cluster-memory.\n\nAuthored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 01d41e7de418d0a40db7b16ddd0d8546f0794d17)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/joins/BroadcastJoinSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/joins/BroadcastJoinSuite.scala",
          "sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveSparkSubmitSuite.scala||sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveSparkSubmitSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/execution/joins/BroadcastJoinSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/joins/BroadcastJoinSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/joins/BroadcastJoinSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/joins/BroadcastJoinSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import scala.reflect.ClassTag",
          "22: import org.apache.spark.AccumulatorSuite",
          "23: import org.apache.spark.sql.{Dataset, QueryTest, Row, SparkSession}",
          "24: import org.apache.spark.sql.catalyst.expressions.{AttributeReference, BitwiseAnd, BitwiseOr, Cast, Expression, Literal, ShiftLeft}",
          "25: import org.apache.spark.sql.catalyst.optimizer.{BuildLeft, BuildRight, BuildSide}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "23: import org.apache.spark.internal.config.EXECUTOR_MEMORY",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "56:   override def beforeAll(): Unit = {",
          "57:     super.beforeAll()",
          "58:     spark = SparkSession.builder()",
          "60:       .appName(\"testing\")",
          "61:       .getOrCreate()",
          "62:   }",
          "",
          "[Removed Lines]",
          "59:       .master(\"local-cluster[2,1,1024]\")",
          "",
          "[Added Lines]",
          "60:       .master(\"local-cluster[2,1,512]\")",
          "61:       .config(EXECUTOR_MEMORY.key, \"512m\")",
          "",
          "---------------"
        ],
        "sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveSparkSubmitSuite.scala||sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveSparkSubmitSuite.scala": [
          "File: sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveSparkSubmitSuite.scala -> sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveSparkSubmitSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "33: import org.apache.spark._",
          "34: import org.apache.spark.deploy.SparkSubmitTestUtils",
          "35: import org.apache.spark.internal.Logging",
          "36: import org.apache.spark.internal.config.UI.UI_ENABLED",
          "37: import org.apache.spark.sql.{QueryTest, Row, SparkSession}",
          "38: import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "36: import org.apache.spark.internal.config.EXECUTOR_MEMORY",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "73:     val args = Seq(",
          "74:       \"--class\", TemporaryHiveUDFTest.getClass.getName.stripSuffix(\"$\"),",
          "75:       \"--name\", \"TemporaryHiveUDFTest\",",
          "77:       \"--conf\", \"spark.ui.enabled=false\",",
          "78:       \"--conf\", \"spark.master.rest.enabled=false\",",
          "79:       \"--driver-java-options\", \"-Dderby.system.durability=test\",",
          "",
          "[Removed Lines]",
          "76:       \"--master\", \"local-cluster[2,1,1024]\",",
          "",
          "[Added Lines]",
          "77:       \"--master\", \"local-cluster[2,1,512]\",",
          "78:       \"--conf\", s\"${EXECUTOR_MEMORY.key}=512m\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "90:     val args = Seq(",
          "91:       \"--class\", PermanentHiveUDFTest1.getClass.getName.stripSuffix(\"$\"),",
          "92:       \"--name\", \"PermanentHiveUDFTest1\",",
          "94:       \"--conf\", \"spark.ui.enabled=false\",",
          "95:       \"--conf\", \"spark.master.rest.enabled=false\",",
          "96:       \"--driver-java-options\", \"-Dderby.system.durability=test\",",
          "",
          "[Removed Lines]",
          "93:       \"--master\", \"local-cluster[2,1,1024]\",",
          "",
          "[Added Lines]",
          "95:       \"--master\", \"local-cluster[2,1,512]\",",
          "96:       \"--conf\", s\"${EXECUTOR_MEMORY.key}=512m\",",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "107:     val args = Seq(",
          "108:       \"--class\", PermanentHiveUDFTest2.getClass.getName.stripSuffix(\"$\"),",
          "109:       \"--name\", \"PermanentHiveUDFTest2\",",
          "111:       \"--conf\", \"spark.ui.enabled=false\",",
          "112:       \"--conf\", \"spark.master.rest.enabled=false\",",
          "113:       \"--driver-java-options\", \"-Dderby.system.durability=test\",",
          "",
          "[Removed Lines]",
          "110:       \"--master\", \"local-cluster[2,1,1024]\",",
          "",
          "[Added Lines]",
          "113:       \"--master\", \"local-cluster[2,1,512]\",",
          "114:       \"--conf\", s\"${EXECUTOR_MEMORY.key}=512m\",",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "126:     val args = Seq(",
          "127:       \"--class\", SparkSubmitClassLoaderTest.getClass.getName.stripSuffix(\"$\"),",
          "128:       \"--name\", \"SparkSubmitClassLoaderTest\",",
          "130:       \"--conf\", \"spark.ui.enabled=false\",",
          "131:       \"--conf\", \"spark.master.rest.enabled=false\",",
          "132:       \"--driver-java-options\", \"-Dderby.system.durability=test\",",
          "",
          "[Removed Lines]",
          "129:       \"--master\", \"local-cluster[2,1,1024]\",",
          "",
          "[Added Lines]",
          "133:       \"--master\", \"local-cluster[2,1,512]\",",
          "134:       \"--conf\", s\"${EXECUTOR_MEMORY.key}=512m\",",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "141:     val args = Seq(",
          "142:       \"--class\", SparkSQLConfTest.getClass.getName.stripSuffix(\"$\"),",
          "143:       \"--name\", \"SparkSQLConfTest\",",
          "145:       \"--conf\", \"spark.ui.enabled=false\",",
          "146:       \"--conf\", \"spark.master.rest.enabled=false\",",
          "147:       \"--conf\", \"spark.sql.hive.metastore.version=0.12\",",
          "",
          "[Removed Lines]",
          "144:       \"--master\", \"local-cluster[2,1,1024]\",",
          "",
          "[Added Lines]",
          "149:       \"--master\", \"local-cluster[2,1,512]\",",
          "150:       \"--conf\", s\"${EXECUTOR_MEMORY.key}=512m\",",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "179:     val args = Seq(",
          "180:       \"--class\", SPARK_9757.getClass.getName.stripSuffix(\"$\"),",
          "181:       \"--name\", \"SparkSQLConfTest\",",
          "183:       \"--conf\", \"spark.ui.enabled=false\",",
          "184:       \"--conf\", \"spark.master.rest.enabled=false\",",
          "185:       \"--driver-java-options\", \"-Dderby.system.durability=test\",",
          "",
          "[Removed Lines]",
          "182:       \"--master\", \"local-cluster[2,1,1024]\",",
          "",
          "[Added Lines]",
          "188:       \"--master\", \"local-cluster[2,1,512]\",",
          "189:       \"--conf\", s\"${EXECUTOR_MEMORY.key}=512m\",",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "192:     val args = Seq(",
          "193:       \"--class\", SPARK_11009.getClass.getName.stripSuffix(\"$\"),",
          "194:       \"--name\", \"SparkSQLConfTest\",",
          "196:       \"--conf\", \"spark.ui.enabled=false\",",
          "197:       \"--conf\", \"spark.master.rest.enabled=false\",",
          "198:       \"--driver-java-options\", \"-Dderby.system.durability=test\",",
          "",
          "[Removed Lines]",
          "195:       \"--master\", \"local-cluster[2,1,1024]\",",
          "",
          "[Added Lines]",
          "202:       \"--master\", \"local-cluster[2,1,512]\",",
          "203:       \"--conf\", s\"${EXECUTOR_MEMORY.key}=512m\",",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "205:     val args = Seq(",
          "206:       \"--class\", SPARK_14244.getClass.getName.stripSuffix(\"$\"),",
          "207:       \"--name\", \"SparkSQLConfTest\",",
          "209:       \"--conf\", \"spark.ui.enabled=false\",",
          "210:       \"--conf\", \"spark.master.rest.enabled=false\",",
          "211:       \"--driver-java-options\", \"-Dderby.system.durability=test\",",
          "",
          "[Removed Lines]",
          "208:       \"--master\", \"local-cluster[2,1,1024]\",",
          "",
          "[Added Lines]",
          "216:       \"--master\", \"local-cluster[2,1,512]\",",
          "217:       \"--conf\", s\"${EXECUTOR_MEMORY.key}=512m\",",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "218:     val args = Seq(",
          "219:       \"--class\", SetWarehouseLocationTest.getClass.getName.stripSuffix(\"$\"),",
          "220:       \"--name\", \"SetSparkWarehouseLocationTest\",",
          "222:       \"--conf\", \"spark.ui.enabled=false\",",
          "223:       \"--conf\", \"spark.master.rest.enabled=false\",",
          "224:       \"--driver-java-options\", \"-Dderby.system.durability=test\",",
          "",
          "[Removed Lines]",
          "221:       \"--master\", \"local-cluster[2,1,1024]\",",
          "",
          "[Added Lines]",
          "230:       \"--master\", \"local-cluster[2,1,512]\",",
          "231:       \"--conf\", s\"${EXECUTOR_MEMORY.key}=512m\",",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "255:     val args = Seq(",
          "256:       \"--class\", SetWarehouseLocationTest.getClass.getName.stripSuffix(\"$\"),",
          "257:       \"--name\", \"SetHiveWarehouseLocationTest\",",
          "259:       \"--conf\", \"spark.ui.enabled=false\",",
          "260:       \"--conf\", \"spark.master.rest.enabled=false\",",
          "261:       \"--conf\", s\"spark.sql.test.expectedWarehouseDir=$hiveWarehouseLocation\",",
          "",
          "[Removed Lines]",
          "258:       \"--master\", \"local-cluster[2,1,1024]\",",
          "",
          "[Added Lines]",
          "268:       \"--master\", \"local-cluster[2,1,512]\",",
          "269:       \"--conf\", s\"${EXECUTOR_MEMORY.key}=512m\",",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "313:     val args = Seq(",
          "314:       \"--class\", SPARK_18360.getClass.getName.stripSuffix(\"$\"),",
          "315:       \"--name\", \"SPARK-18360\",",
          "317:       \"--conf\", \"spark.ui.enabled=false\",",
          "318:       \"--conf\", \"spark.master.rest.enabled=false\",",
          "319:       \"--driver-java-options\", \"-Dderby.system.durability=test\",",
          "",
          "[Removed Lines]",
          "316:       \"--master\", \"local-cluster[2,1,1024]\",",
          "",
          "[Added Lines]",
          "327:       \"--master\", \"local-cluster[2,1,512]\",",
          "328:       \"--conf\", s\"${EXECUTOR_MEMORY.key}=512m\",",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "327:     val argsForCreateTable = Seq(",
          "328:       \"--class\", SPARK_18989_CREATE_TABLE.getClass.getName.stripSuffix(\"$\"),",
          "329:       \"--name\", \"SPARK-18947\",",
          "331:       \"--conf\", \"spark.ui.enabled=false\",",
          "332:       \"--conf\", \"spark.master.rest.enabled=false\",",
          "333:       \"--jars\", HiveTestJars.getHiveContribJar().getCanonicalPath,",
          "",
          "[Removed Lines]",
          "330:       \"--master\", \"local-cluster[2,1,1024]\",",
          "",
          "[Added Lines]",
          "342:       \"--master\", \"local-cluster[2,1,512]\",",
          "343:       \"--conf\", s\"${EXECUTOR_MEMORY.key}=512m\",",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "337:     val argsForShowTables = Seq(",
          "338:       \"--class\", SPARK_18989_DESC_TABLE.getClass.getName.stripSuffix(\"$\"),",
          "339:       \"--name\", \"SPARK-18947\",",
          "341:       \"--conf\", \"spark.ui.enabled=false\",",
          "342:       \"--conf\", \"spark.master.rest.enabled=false\",",
          "343:       unusedJar.toString)",
          "",
          "[Removed Lines]",
          "340:       \"--master\", \"local-cluster[2,1,1024]\",",
          "",
          "[Added Lines]",
          "353:       \"--master\", \"local-cluster[2,1,512]\",",
          "354:       \"--conf\", s\"${EXECUTOR_MEMORY.key}=512m\",",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "358:       val args = Seq(",
          "359:         \"--class\", SPARK_34772.getClass.getName.stripSuffix(\"$\"),",
          "360:         \"--name\", \"SPARK-34772\",",
          "362:         \"--conf\", s\"${LEGACY_TIME_PARSER_POLICY.key}=LEGACY\",",
          "363:         \"--conf\", s\"${HiveUtils.HIVE_METASTORE_VERSION.key}=1.2.1\",",
          "364:         \"--conf\", s\"${HiveUtils.HIVE_METASTORE_JARS.key}=maven\",",
          "",
          "[Removed Lines]",
          "361:         \"--master\", \"local-cluster[2,1,1024]\",",
          "",
          "[Added Lines]",
          "375:         \"--master\", \"local-cluster[2,1,512]\",",
          "376:         \"--conf\", s\"${EXECUTOR_MEMORY.key}=512m\",",
          "",
          "---------------"
        ]
      }
    }
  ]
}