{
  "cve_id": "CVE-2023-50944",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows an authenticated user to access the source code of a DAG to which they don't have access.\u00a0This vulnerability is considered low since it requires an authenticated user to exploit it. Users are recommended to upgrade to version 2.8.1, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "8d76538d6e105947272b000581c6fabec20146b1",
  "patch_info": {
    "commit_hash": "8d76538d6e105947272b000581c6fabec20146b1",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/8d76538d6e105947272b000581c6fabec20146b1",
    "files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ],
    "message": "Check DAG read permission before accessing DAG code (#36257)\n\n(cherry picked from commit 30ea37e0d247ce54c2d25b115e807fdb0074d795)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py||airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_source_endpoint.py -> airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "17: from __future__ import annotations",
      "19: from http import HTTPStatus",
      "21: from flask import Response, current_app, request",
      "22: from itsdangerous import BadSignature, URLSafeSerializer",
      "24: from airflow.api_connexion import security",
      "26: from airflow.api_connexion.schemas.dag_source_schema import dag_source_schema",
      "27: from airflow.auth.managers.models.resource_details import DagAccessEntity",
      "28: from airflow.models.dagcode import DagCode",
      "31: @security.requires_access_dag(\"GET\", DagAccessEntity.CODE)",
      "33:     \"\"\"Get source code using file token.\"\"\"",
      "34:     secret_key = current_app.config[\"SECRET_KEY\"]",
      "35:     auth_s = URLSafeSerializer(secret_key)",
      "36:     try:",
      "37:         path = auth_s.loads(file_token)",
      "39:     except (BadSignature, FileNotFoundError):",
      "40:         raise NotFound(\"Dag source not found\")",
      "",
      "[Removed Lines]",
      "25: from airflow.api_connexion.exceptions import NotFound",
      "32: def get_dag_source(*, file_token: str) -> Response:",
      "38:         dag_source = DagCode.code(path)",
      "",
      "[Added Lines]",
      "20: from typing import TYPE_CHECKING",
      "26: from airflow.api_connexion.exceptions import NotFound, PermissionDenied",
      "28: from airflow.api_connexion.security import get_readable_dags",
      "30: from airflow.models.dag import DagModel",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "34: if TYPE_CHECKING:",
      "35:     from sqlalchemy.orm import Session",
      "39: @provide_session",
      "40: def get_dag_source(*, file_token: str, session: Session = NEW_SESSION) -> Response:",
      "46:         dag_ids = session.query(DagModel.dag_id).filter(DagModel.fileloc == path).all()",
      "47:         readable_dags = get_readable_dags()",
      "48:         # Check if user has read access to all the DAGs defined in the file",
      "49:         if any(dag_id[0] not in readable_dags for dag_id in dag_ids):",
      "50:             raise PermissionDenied()",
      "51:         dag_source = DagCode.code(path, session=session)",
      "",
      "---------------"
    ],
    "airflow/models/dagcode.py||airflow/models/dagcode.py": [
      "File: airflow/models/dagcode.py -> airflow/models/dagcode.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "177:         return cls.code(fileloc)",
      "179:     @classmethod",
      "181:         \"\"\"Return source code for this DagCode object.",
      "183:         :return: source code as string",
      "184:         \"\"\"",
      "187:     @staticmethod",
      "188:     def _get_code_from_file(fileloc):",
      "",
      "[Removed Lines]",
      "180:     def code(cls, fileloc) -> str:",
      "185:         return cls._get_code_from_db(fileloc)",
      "",
      "[Added Lines]",
      "180:     @provide_session",
      "181:     def code(cls, fileloc, session: Session = NEW_SESSION) -> str:",
      "186:         return cls._get_code_from_db(fileloc, session)",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_source_endpoint.py -> tests/api_connexion/endpoints/test_dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35: ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))",
      "36: EXAMPLE_DAG_FILE = os.path.join(\"airflow\", \"example_dags\", \"example_bash_operator.py\")",
      "39: @pytest.fixture(scope=\"module\")",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "37: EXAMPLE_DAG_ID = \"example_bash_operator\"",
      "38: TEST_DAG_ID = \"latest_only\"",
      "39: NOT_READABLE_DAG_ID = \"latest_only_with_trigger\"",
      "40: TEST_MULTIPLE_DAGS_ID = \"dataset_produces_1\"",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "45:         role_name=\"Test\",",
      "46:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_CODE)],  # type: ignore",
      "47:     )",
      "48:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "50:     yield app",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "52:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "53:         TEST_DAG_ID,",
      "54:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "55:     )",
      "56:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "57:         EXAMPLE_DAG_ID,",
      "58:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "59:     )",
      "60:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "61:         TEST_MULTIPLE_DAGS_ID,",
      "62:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "63:     )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "80:     def test_should_respond_200_text(self, url_safe_serializer):",
      "81:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "82:         dagbag.sync_to_db()",
      "87:         response = self.client.get(",
      "88:             url, headers={\"Accept\": \"text/plain\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "89:         )",
      "",
      "[Removed Lines]",
      "83:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "84:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "86:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "99:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "100:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "102:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "95:     def test_should_respond_200_json(self, url_safe_serializer):",
      "96:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "97:         dagbag.sync_to_db()",
      "102:         response = self.client.get(",
      "103:             url, headers={\"Accept\": \"application/json\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "104:         )",
      "",
      "[Removed Lines]",
      "98:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "99:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "101:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "114:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "115:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "117:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "110:     def test_should_respond_406(self, url_safe_serializer):",
      "111:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "112:         dagbag.sync_to_db()",
      "116:         response = self.client.get(",
      "117:             url, headers={\"Accept\": \"image/webp\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "118:         )",
      "",
      "[Removed Lines]",
      "113:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "115:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "129:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "131:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "151:             environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"},",
      "152:         )",
      "153:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "171:     def test_should_respond_403_not_readable(self, url_safe_serializer):",
      "172:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "173:         dagbag.sync_to_db()",
      "174:         dag: DAG = dagbag.dags[NOT_READABLE_DAG_ID]",
      "176:         response = self.client.get(",
      "177:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "178:             headers={\"Accept\": \"text/plain\"},",
      "179:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "180:         )",
      "181:         read_dag = self.client.get(",
      "182:             f\"/api/v1/dags/{NOT_READABLE_DAG_ID}\",",
      "183:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "184:         )",
      "185:         assert response.status_code == 403",
      "186:         assert read_dag.status_code == 403",
      "188:     def test_should_respond_403_some_dags_not_readable_in_the_file(self, url_safe_serializer):",
      "189:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "190:         dagbag.sync_to_db()",
      "191:         dag: DAG = dagbag.dags[TEST_MULTIPLE_DAGS_ID]",
      "193:         response = self.client.get(",
      "194:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "195:             headers={\"Accept\": \"text/plain\"},",
      "196:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "197:         )",
      "199:         read_dag = self.client.get(",
      "200:             f\"/api/v1/dags/{TEST_MULTIPLE_DAGS_ID}\",",
      "201:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "202:         )",
      "203:         assert response.status_code == 403",
      "204:         assert read_dag.status_code == 200",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "36422edc3738784394f5faa2452d7bc3b1801a33",
      "candidate_info": {
        "commit_hash": "36422edc3738784394f5faa2452d7bc3b1801a33",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/36422edc3738784394f5faa2452d7bc3b1801a33",
        "files": [
          "README.md",
          "RELEASE_NOTES.rst",
          "airflow/__init__.py",
          "airflow/api_connexion/openapi/v1.yaml",
          "airflow/reproducible_build.yaml",
          "docs/apache-airflow/installation/supported-versions.rst",
          "docs/docker-stack/README.md",
          "docs/docker-stack/docker-examples/extending/add-airflow-configuration/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-apt-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-build-essential-extend/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-providers/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-pypi-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-requirement-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/custom-providers/Dockerfile",
          "docs/docker-stack/docker-examples/extending/embedding-dags/Dockerfile",
          "docs/docker-stack/docker-examples/extending/writable-directory/Dockerfile",
          "docs/docker-stack/entrypoint.rst",
          "generated/PYPI_README.md",
          "scripts/ci/pre_commit/pre_commit_supported_versions.py"
        ],
        "message": "Update version to 2.8.1",
        "before_after_code_files": [
          "airflow/__init__.py||airflow/__init__.py",
          "scripts/ci/pre_commit/pre_commit_supported_versions.py||scripts/ci/pre_commit/pre_commit_supported_versions.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/__init__.py||airflow/__init__.py": [
          "File: airflow/__init__.py -> airflow/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: \"\"\"",
          "27: from __future__ import annotations",
          "31: # flake8: noqa: F401",
          "",
          "[Removed Lines]",
          "29: __version__ = \"2.8.1.dev0\"",
          "",
          "[Added Lines]",
          "29: __version__ = \"2.8.1\"",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_supported_versions.py||scripts/ci/pre_commit/pre_commit_supported_versions.py": [
          "File: scripts/ci/pre_commit/pre_commit_supported_versions.py -> scripts/ci/pre_commit/pre_commit_supported_versions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: HEADERS = (\"Version\", \"Current Patch/Minor\", \"State\", \"First Release\", \"Limited Support\", \"EOL/Terminated\")",
          "29: SUPPORTED_VERSIONS = (",
          "31:     (\"1.10\", \"1.10.15\", \"EOL\", \"Aug 27, 2018\", \"Dec 17, 2020\", \"June 17, 2021\"),",
          "32:     (\"1.9\", \"1.9.0\", \"EOL\", \"Jan 03, 2018\", \"Aug 27, 2018\", \"Aug 27, 2018\"),",
          "33:     (\"1.8\", \"1.8.2\", \"EOL\", \"Mar 19, 2017\", \"Jan 03, 2018\", \"Jan 03, 2018\"),",
          "",
          "[Removed Lines]",
          "30:     (\"2\", \"2.8.0\", \"Supported\", \"Dec 17, 2020\", \"TBD\", \"TBD\"),",
          "",
          "[Added Lines]",
          "30:     (\"2\", \"2.8.1\", \"Supported\", \"Dec 17, 2020\", \"TBD\", \"TBD\"),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "21b58ecac36bcf81d24f9a79e02d88e4d04cc94d",
      "candidate_info": {
        "commit_hash": "21b58ecac36bcf81d24f9a79e02d88e4d04cc94d",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/21b58ecac36bcf81d24f9a79e02d88e4d04cc94d",
        "files": [
          "airflow/cli/commands/dag_command.py",
          "tests/cli/commands/test_dag_command.py"
        ],
        "message": "Raise error when ``DagRun`` fails while running ``dag test`` (#36517)\n\n**Motivation**:\n\nCurrently, when using `airflow dags test`, there is no easy way to know programmatically if a DagRun fails since the state is not stored in DB. The way to do know relies on log lines as below:\n\n```bash\nstate=$(airflow dags test exception_dag | grep \"DagRun Finished\" | awk -F, '{for(i=1;i<=NF;i++) if ($i ~ / state=/) print $i}' | awk -F= '{print $2}') if [[ $state == \"failed\" ]]; then exit 1 else exit 0 fi\n```\n\nThis PR adds will return an exit code 1 when `airflow dags test` command if DagRun fails and makes it easy to integrate in CI for testing.\n\n(cherry picked from commit 383ad31c76411fb0a9f7d4243729d7bb0640ff0c)",
        "before_after_code_files": [
          "airflow/cli/commands/dag_command.py||airflow/cli/commands/dag_command.py",
          "tests/cli/commands/test_dag_command.py||tests/cli/commands/test_dag_command.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/commands/dag_command.py||airflow/cli/commands/dag_command.py": [
          "File: airflow/cli/commands/dag_command.py -> airflow/cli/commands/dag_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "515:             raise SystemExit(f\"Configuration {args.conf!r} is not valid JSON. Error: {e}\")",
          "516:     execution_date = args.execution_date or timezone.utcnow()",
          "517:     dag = dag or get_dag(subdir=args.subdir, dag_id=args.dag_id)",
          "519:     show_dagrun = args.show_dagrun",
          "520:     imgcat = args.imgcat_dagrun",
          "521:     filename = args.save_dagrun",
          "",
          "[Removed Lines]",
          "518:     dag.test(execution_date=execution_date, run_conf=run_conf, session=session)",
          "",
          "[Added Lines]",
          "518:     dr: DagRun = dag.test(execution_date=execution_date, run_conf=run_conf, session=session)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "536:         if show_dagrun:",
          "537:             print(dot_graph.source)",
          "540: @cli_utils.action_cli",
          "541: @providers_configuration_loaded",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "539:     if dr and dr.state == DagRunState.FAILED:",
          "540:         raise SystemExit(\"DagRun failed\")",
          "",
          "---------------"
        ],
        "tests/cli/commands/test_dag_command.py||tests/cli/commands/test_dag_command.py": [
          "File: tests/cli/commands/test_dag_command.py -> tests/cli/commands/test_dag_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "43: from airflow.triggers.temporal import DateTimeTrigger, TimeDeltaTrigger",
          "44: from airflow.utils import timezone",
          "45: from airflow.utils.session import create_session",
          "46: from airflow.utils.types import DagRunType",
          "47: from tests.models import TEST_DAGS_FOLDER",
          "48: from tests.test_utils.config import conf_vars",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "46: from airflow.utils.state import DagRunState",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "747:             ]",
          "748:         )",
          "750:     @mock.patch(\"airflow.cli.commands.dag_command.get_dag\")",
          "751:     @mock.patch(\"airflow.utils.timezone.utcnow\")",
          "752:     def test_dag_test_no_execution_date(self, mock_utcnow, mock_get_dag):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "751:     @mock.patch(\"airflow.cli.commands.dag_command.get_dag\")",
          "752:     def test_dag_test_fail_raise_error(self, mock_get_dag):",
          "753:         execution_date_str = DEFAULT_DATE.isoformat()",
          "754:         mock_get_dag.return_value.test.return_value = DagRun(",
          "755:             dag_id=\"example_bash_operator\", execution_date=DEFAULT_DATE, state=DagRunState.FAILED",
          "756:         )",
          "757:         cli_args = self.parser.parse_args([\"dags\", \"test\", \"example_bash_operator\", execution_date_str])",
          "758:         with pytest.raises(SystemExit, match=r\"DagRun failed\"):",
          "759:             dag_command.dag_test(cli_args)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a6a305f0dc5f49b60ac2e0153262b9741b6b503c",
      "candidate_info": {
        "commit_hash": "a6a305f0dc5f49b60ac2e0153262b9741b6b503c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a6a305f0dc5f49b60ac2e0153262b9741b6b503c",
        "files": [
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py",
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py",
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py",
          "tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py"
        ],
        "message": "Kubernetes executor running slots leak fix (#36240)\n\n---------\n\nCo-authored-by: gopal <gopal_dirisala@apple.com>\n(cherry picked from commit 49108e15eb2eb30e2ccb95c9332db7b38d35f2de)",
        "before_after_code_files": [
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py",
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py",
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py",
          "tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py||tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py": [
          "File: airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py -> airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "74:     raise",
          "75: from airflow.configuration import conf",
          "76: from airflow.executors.base_executor import BaseExecutor",
          "78: from airflow.providers.cncf.kubernetes.kube_config import KubeConfig",
          "79: from airflow.providers.cncf.kubernetes.kubernetes_helper_functions import annotations_to_key",
          "80: from airflow.utils.event_scheduler import EventScheduler",
          "",
          "[Removed Lines]",
          "77: from airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types import POD_EXECUTOR_DONE_KEY",
          "",
          "[Added Lines]",
          "77: from airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types import (",
          "78:     ADOPTED,",
          "79:     POD_EXECUTOR_DONE_KEY,",
          "80: )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "450:     def _change_state(",
          "451:         self,",
          "452:         key: TaskInstanceKey,",
          "454:         pod_name: str,",
          "455:         namespace: str,",
          "456:         session: Session = NEW_SESSION,",
          "",
          "[Removed Lines]",
          "453:         state: TaskInstanceState | None,",
          "",
          "[Added Lines]",
          "456:         state: TaskInstanceState | str | None,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "458:         if TYPE_CHECKING:",
          "459:             assert self.kube_scheduler",
          "461:         if state == TaskInstanceState.RUNNING:",
          "462:             self.event_buffer[key] = state, None",
          "463:             return",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "464:         if state == ADOPTED:",
          "465:             # When the task pod is adopted by another executor,",
          "466:             # then remove the task from the current executor running queue.",
          "467:             try:",
          "468:                 self.running.remove(key)",
          "469:             except KeyError:",
          "470:                 self.log.debug(\"TI key not in running: %s\", key)",
          "471:             return",
          "",
          "---------------"
        ],
        "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py": [
          "File: airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py -> airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "21: if TYPE_CHECKING:",
          "22:     from airflow.executors.base_executor import CommandType",
          "23:     from airflow.models.taskinstance import TaskInstanceKey",
          "",
          "[Removed Lines]",
          "19: from typing import TYPE_CHECKING, Any, Dict, Optional, Tuple",
          "",
          "[Added Lines]",
          "19: from typing import TYPE_CHECKING, Any, Dict, Optional, Tuple, Union",
          "21: ADOPTED = \"adopted\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "27:     KubernetesJobType = Tuple[TaskInstanceKey, CommandType, Any, Optional[str]]",
          "29:     # key, pod state, pod_name, namespace, resource_version",
          "32:     # pod_name, namespace, pod state, annotations, resource_version",
          "35: ALL_NAMESPACES = \"ALL_NAMESPACES\"",
          "36: POD_EXECUTOR_DONE_KEY = \"airflow_executor_done\"",
          "",
          "[Removed Lines]",
          "30:     KubernetesResultsType = Tuple[TaskInstanceKey, Optional[TaskInstanceState], str, str, str]",
          "33:     KubernetesWatchType = Tuple[str, str, Optional[TaskInstanceState], Dict[str, str], str]",
          "",
          "[Added Lines]",
          "31:     KubernetesResultsType = Tuple[TaskInstanceKey, Optional[Union[TaskInstanceState, str]], str, str, str]",
          "34:     KubernetesWatchType = Tuple[str, str, Optional[Union[TaskInstanceState, str]], Dict[str, str], str]",
          "",
          "---------------"
        ],
        "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py": [
          "File: airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py -> airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "41: try:",
          "42:     from airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types import (",
          "43:         ALL_NAMESPACES,",
          "44:         POD_EXECUTOR_DONE_KEY,",
          "45:     )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "43:         ADOPTED,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "220:         pod = event[\"object\"]",
          "221:         annotations_string = annotations_for_logging_task_metadata(annotations)",
          "222:         \"\"\"Process status response.\"\"\"",
          "224:             # deletion_timestamp is set by kube server when a graceful deletion is requested.",
          "225:             # since kube server have received request to delete pod set TI state failed",
          "226:             if event[\"type\"] == \"DELETED\" and pod.metadata.deletion_timestamp:",
          "",
          "[Removed Lines]",
          "223:         if status == \"Pending\":",
          "",
          "[Added Lines]",
          "224:         if event[\"type\"] == \"DELETED\" and not pod.metadata.deletion_timestamp:",
          "225:             # This will happen only when the task pods are adopted by another executor.",
          "226:             # So, there is no change in the pod state.",
          "227:             # However, need to free the executor slot from the current executor.",
          "228:             self.log.info(\"Event: pod %s adopted, annotations: %s\", pod_name, annotations_string)",
          "229:             self.watcher_queue.put((pod_name, namespace, ADOPTED, annotations, resource_version))",
          "230:         elif status == \"Pending\":",
          "",
          "---------------"
        ],
        "tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py||tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py": [
          "File: tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py -> tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:         KubernetesExecutor,",
          "45:         PodReconciliationError,",
          "46:     )",
          "48:     from airflow.providers.cncf.kubernetes.executors.kubernetes_executor_utils import (",
          "49:         AirflowKubernetesScheduler,",
          "50:         KubernetesJobWatcher,",
          "",
          "[Removed Lines]",
          "47:     from airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types import POD_EXECUTOR_DONE_KEY",
          "",
          "[Added Lines]",
          "47:     from airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types import (",
          "48:         ADOPTED,",
          "49:         POD_EXECUTOR_DONE_KEY,",
          "50:     )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "644:         finally:",
          "645:             executor.end()",
          "647:     @pytest.mark.db_test",
          "648:     @pytest.mark.parametrize(",
          "649:         \"multi_namespace_mode_namespace_list, watchers_keys\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "650:     @pytest.mark.db_test",
          "651:     @mock.patch(\"airflow.providers.cncf.kubernetes.executors.kubernetes_executor_utils.KubernetesJobWatcher\")",
          "652:     @mock.patch(\"airflow.providers.cncf.kubernetes.kube_client.get_kube_client\")",
          "653:     @mock.patch(",
          "654:         \"airflow.providers.cncf.kubernetes.executors.kubernetes_executor_utils.AirflowKubernetesScheduler.delete_pod\"",
          "655:     )",
          "656:     def test_change_state_adopted(self, mock_delete_pod, mock_get_kube_client, mock_kubernetes_job_watcher):",
          "657:         executor = self.kubernetes_executor",
          "658:         executor.start()",
          "659:         try:",
          "660:             key = (\"dag_id\", \"task_id\", \"run_id\", \"try_number2\")",
          "661:             executor.running = {key}",
          "662:             executor._change_state(key, ADOPTED, \"pod_name\", \"default\")",
          "663:             assert len(executor.event_buffer) == 0",
          "664:             assert len(executor.running) == 0",
          "665:             mock_delete_pod.assert_not_called()",
          "666:         finally:",
          "667:             executor.end()",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1372:         self._run()",
          "1373:         self.watcher.watcher_queue.put.assert_not_called()",
          "1377:         self.events.append({\"type\": \"DELETED\", \"object\": self.pod})",
          "1379:         self._run()",
          "1382:     def test_process_status_running_deleted(self):",
          "1383:         self.pod.status.phase = \"Running\"",
          "",
          "[Removed Lines]",
          "1375:     def test_process_status_succeeded_type_delete(self):",
          "1376:         self.pod.status.phase = \"Succeeded\"",
          "1380:         self.watcher.watcher_queue.put.assert_not_called()",
          "",
          "[Added Lines]",
          "1397:     @pytest.mark.parametrize(",
          "1398:         \"ti_state\",",
          "1399:         [",
          "1400:             TaskInstanceState.SUCCESS,",
          "1401:             TaskInstanceState.FAILED,",
          "1402:             TaskInstanceState.RUNNING,",
          "1403:             TaskInstanceState.QUEUED,",
          "1404:             TaskInstanceState.UP_FOR_RETRY,",
          "1405:         ],",
          "1406:     )",
          "1407:     def test_process_status_pod_adopted(self, ti_state):",
          "1408:         self.pod.status.phase = ti_state",
          "1410:         self.pod.metadata.deletion_timestamp = None",
          "1413:         self.watcher.watcher_queue.put.assert_called_once_with(",
          "1414:             (",
          "1415:                 self.pod.metadata.name,",
          "1416:                 self.watcher.namespace,",
          "1417:                 ADOPTED,",
          "1418:                 self.core_annotations,",
          "1419:                 self.pod.metadata.resource_version,",
          "1420:             )",
          "1421:         )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "808ed02476385c8c670ae3e64c70f19954496075",
      "candidate_info": {
        "commit_hash": "808ed02476385c8c670ae3e64c70f19954496075",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/808ed02476385c8c670ae3e64c70f19954496075",
        "files": [
          "airflow/utils/task_group.py",
          "tests/utils/test_task_group.py"
        ],
        "message": "Fix get_leaves calculation for teardown in nested group (#36456)\n\nWhen arrowing `group` >> `task`, the \"leaves\" of `group` are connected to `task`. When calculating leaves in the group, teardown tasks are ignored, and we recurse upstream to find non-teardowns.\n\nWhat was happening, and what this fixes, is you might recurse to a work task that already has another non-teardown downstream in the group.  In that case you should ignore the work task (because it already has a non-teardown descendent).\n\nResolves #36345\n\n(cherry picked from commit 949fc5788ec7a7a1e4f6bc850d2615ec0f79a57d)",
        "before_after_code_files": [
          "airflow/utils/task_group.py||airflow/utils/task_group.py",
          "tests/utils/test_task_group.py||tests/utils/test_task_group.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/task_group.py||airflow/utils/task_group.py": [
          "File: airflow/utils/task_group.py -> airflow/utils/task_group.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "371:         tasks = list(self)",
          "372:         ids = {x.task_id for x in tasks}",
          "374:         def recurse_for_first_non_teardown(task):",
          "375:             for upstream_task in task.upstream_list:",
          "376:                 if upstream_task.task_id not in ids:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "374:         def has_non_teardown_downstream(task, exclude: str):",
          "375:             for down_task in task.downstream_list:",
          "376:                 if down_task.task_id == exclude:",
          "377:                     continue",
          "378:                 elif down_task.task_id not in ids:",
          "379:                     continue",
          "380:                 elif not down_task.is_teardown:",
          "381:                     return True",
          "382:             return False",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "381:                 elif task.is_teardown and upstream_task.is_setup:",
          "382:                     # don't go through the teardown-to-setup path",
          "383:                     continue",
          "385:                     yield upstream_task",
          "387:         for task in tasks:",
          "",
          "[Removed Lines]",
          "384:                 else:",
          "",
          "[Added Lines]",
          "394:                 # return unless upstream task already has non-teardown downstream in group",
          "395:                 elif not has_non_teardown_downstream(upstream_task, exclude=task.task_id):",
          "",
          "---------------"
        ],
        "tests/utils/test_task_group.py||tests/utils/test_task_group.py": [
          "File: tests/utils/test_task_group.py -> tests/utils/test_task_group.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "584:     ]",
          "587: def test_duplicate_group_id():",
          "588:     from airflow.exceptions import DuplicateTaskIdFound",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "587: def test_dag_edges_setup_teardown_nested():",
          "588:     from airflow.decorators import task, task_group",
          "589:     from airflow.models.dag import DAG",
          "590:     from airflow.operators.empty import EmptyOperator",
          "592:     execution_date = pendulum.parse(\"20200101\")",
          "594:     with DAG(dag_id=\"s_t_dag\", start_date=execution_date) as dag:",
          "596:         @task",
          "597:         def test_task():",
          "598:             print(\"Hello world!\")",
          "600:         @task_group",
          "601:         def inner():",
          "602:             inner_start = EmptyOperator(task_id=\"start\")",
          "603:             inner_end = EmptyOperator(task_id=\"end\")",
          "605:             test_task_r = test_task.override(task_id=\"work\")()",
          "606:             inner_start >> test_task_r >> inner_end.as_teardown(setups=inner_start)",
          "608:         @task_group",
          "609:         def outer():",
          "610:             outer_work = EmptyOperator(task_id=\"work\")",
          "611:             inner_group = inner()",
          "612:             inner_group >> outer_work",
          "614:         dag_start = EmptyOperator(task_id=\"dag_start\")",
          "615:         dag_end = EmptyOperator(task_id=\"dag_end\")",
          "616:         dag_start >> outer() >> dag_end",
          "618:     edges = dag_edges(dag)",
          "620:     actual = sorted((e[\"source_id\"], e[\"target_id\"], e.get(\"is_setup_teardown\")) for e in edges)",
          "621:     assert actual == [",
          "622:         (\"dag_start\", \"outer.upstream_join_id\", None),",
          "623:         (\"outer.downstream_join_id\", \"dag_end\", None),",
          "624:         (\"outer.inner.downstream_join_id\", \"outer.work\", None),",
          "625:         (\"outer.inner.start\", \"outer.inner.end\", True),",
          "626:         (\"outer.inner.start\", \"outer.inner.work\", None),",
          "627:         (\"outer.inner.work\", \"outer.inner.downstream_join_id\", None),",
          "628:         (\"outer.inner.work\", \"outer.inner.end\", None),",
          "629:         (\"outer.upstream_join_id\", \"outer.inner.start\", None),",
          "630:         (\"outer.work\", \"outer.downstream_join_id\", None),",
          "631:     ]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0db3bc6b53cdb86720900418b63f515b76db46f8",
      "candidate_info": {
        "commit_hash": "0db3bc6b53cdb86720900418b63f515b76db46f8",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0db3bc6b53cdb86720900418b63f515b76db46f8",
        "files": [
          ".github/workflows/build-images.yml",
          ".github/workflows/ci.yml",
          "airflow/provider.yaml.schema.json",
          "dev/README_RELEASE_PROVIDER_PACKAGES.md",
          "dev/breeze/src/airflow_breeze/commands/common_options.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands_config.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py",
          "dev/breeze/src/airflow_breeze/utils/packages.py",
          "images/breeze/output_build-docs.svg",
          "images/breeze/output_build-docs.txt",
          "images/breeze/output_release-management_add-back-references.svg",
          "images/breeze/output_release-management_add-back-references.txt",
          "images/breeze/output_release-management_install-provider-packages.txt",
          "images/breeze/output_release-management_prepare-provider-documentation.svg",
          "images/breeze/output_release-management_prepare-provider-documentation.txt",
          "images/breeze/output_release-management_prepare-provider-packages.svg",
          "images/breeze/output_release-management_prepare-provider-packages.txt",
          "images/breeze/output_release-management_publish-docs.svg",
          "images/breeze/output_release-management_publish-docs.txt",
          "images/breeze/output_release-management_verify-provider-packages.txt",
          "images/breeze/output_shell.txt",
          "images/breeze/output_start-airflow.txt"
        ],
        "message": "Add feture of \"not-ready\" provider. (#36391)\n\nThis PR adds possibility of marking the provider as \"not ready\" in the\nprovider.yaml (by setting optional field as \"not-ready\" to `true\".\n\nSetting provider as \"not-ready\", removes it by default from all the\nrelease management commands - preparing documentation files preparing\nprovider packages, publishing docs.\n\nYou can include such providers via `--include-not-ready-providers`\nflag (or setting INCLUDE_NOT_READY_PROVIDERS environment variable to\ntrue).\n\nThis flag is set to True in our CI, so that we can make sure the\nproviders in-progress are also being tested and verified, but when\nrelease manager prepares packages, those providers are not prepared.\n\nThat will help in early stage of a lifecycle of a provider when we\nalready want to iterate and test it continuously, but - for example\nthe API of such provider is not yet stable or when we are in progress\nof moving functionality for such provider from core.\n\nThis PR also marks `fab` providers as \"not-ready\" as it is still\nearly days and we want to exclude it for now from any kind of release\nprocess.\n\n(cherry picked from commit 341d5b747db78b9be00d5d5dc491e37d413570da)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/common_options.py||dev/breeze/src/airflow_breeze/commands/common_options.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands_config.py||dev/breeze/src/airflow_breeze/commands/developer_commands_config.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py||dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py",
          "dev/breeze/src/airflow_breeze/utils/packages.py||dev/breeze/src/airflow_breeze/utils/packages.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/common_options.py||dev/breeze/src/airflow_breeze/commands/common_options.py": [
          "File: dev/breeze/src/airflow_breeze/commands/common_options.py -> dev/breeze/src/airflow_breeze/commands/common_options.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "74:     required=False,",
          "75:     type=NotVerifiedBetterChoice(",
          "76:         get_available_packages(",
          "78:         )",
          "79:     ),",
          "80: )",
          "",
          "[Removed Lines]",
          "77:             include_non_provider_doc_packages=True, include_all_providers=True, include_removed=True",
          "",
          "[Added Lines]",
          "77:             include_non_provider_doc_packages=True,",
          "78:             include_all_providers=True,",
          "79:             include_removed=True,",
          "80:             include_not_ready=True,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "189:     is_flag=True,",
          "190:     envvar=\"INCLUDE_REMOVED_PROVIDERS\",",
          "191: )",
          "192: option_include_success_outputs = click.option(",
          "193:     \"--include-success-outputs\",",
          "194:     help=\"Whether to include outputs of successful parallel runs (skipped by default).\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "195: option_include_not_ready_providers = click.option(",
          "196:     \"--include-not-ready-providers\",",
          "197:     help=\"Whether to include providers that are not yet ready to be released.\",",
          "198:     is_flag=True,",
          "199:     envvar=\"INCLUDE_NOT_READY_PROVIDERS\",",
          "200: )",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/developer_commands.py -> dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:     option_forward_credentials,",
          "45:     option_github_repository,",
          "46:     option_image_tag_for_running,",
          "47:     option_include_removed_providers,",
          "48:     option_installation_package_format,",
          "49:     option_integration,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "47:     option_include_not_ready_providers,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "590: @click.option(\"-d\", \"--docs-only\", help=\"Only build documentation.\", is_flag=True)",
          "591: @option_dry_run",
          "592: @option_github_repository",
          "593: @option_include_removed_providers",
          "594: @click.option(",
          "595:     \"--one-pass-only\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "594: @option_include_not_ready_providers",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "612:     clean_build: bool,",
          "613:     docs_only: bool,",
          "614:     github_repository: str,",
          "615:     include_removed_providers: bool,",
          "616:     one_pass_only: bool,",
          "617:     package_filter: tuple[str, ...],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "617:     include_not_ready_providers: bool,",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "640:         spellcheck_only=spellcheck_only,",
          "641:         one_pass_only=one_pass_only,",
          "642:         short_doc_packages=expand_all_provider_packages(",
          "644:         ),",
          "645:     )",
          "646:     cmd = \"/opt/airflow/scripts/in_container/run_docs_build.sh \" + \" \".join(",
          "",
          "[Removed Lines]",
          "643:             doc_packages, include_removed=include_removed_providers",
          "",
          "[Added Lines]",
          "646:             short_doc_packages=doc_packages,",
          "647:             include_removed=include_removed_providers,",
          "648:             include_not_ready=include_not_ready_providers,",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/developer_commands_config.py||dev/breeze/src/airflow_breeze/commands/developer_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/developer_commands_config.py -> dev/breeze/src/airflow_breeze/commands/developer_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "294:                 \"--clean-build\",",
          "295:                 \"--one-pass-only\",",
          "296:                 \"--package-filter\",",
          "297:                 \"--include-removed-providers\",",
          "298:                 \"--github-repository\",",
          "299:                 \"--builder\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "297:                 \"--include-not-ready-providers\",",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "46:     option_github_repository,",
          "47:     option_historical_python_version,",
          "48:     option_image_tag_for_running,",
          "49:     option_include_removed_providers,",
          "50:     option_include_success_outputs,",
          "51:     option_installation_package_format,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "49:     option_include_not_ready_providers,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "153:     \"provider_packages\",",
          "154:     nargs=-1,",
          "155:     required=False,",
          "157: )",
          "158: option_airflow_site_directory = click.option(",
          "159:     \"-a\",",
          "",
          "[Removed Lines]",
          "156:     type=NotVerifiedBetterChoice(get_available_packages(include_removed=False)),",
          "",
          "[Added Lines]",
          "157:     type=NotVerifiedBetterChoice(get_available_packages(include_removed=False, include_not_ready=False)),",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "346: @argument_provider_packages",
          "347: @option_answer",
          "348: @option_dry_run",
          "349: @option_include_removed_providers",
          "350: @click.option(",
          "351:     \"--non-interactive\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "350: @option_include_not_ready_providers",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "368: def prepare_provider_documentation(",
          "369:     base_branch: str,",
          "370:     github_repository: str,",
          "371:     include_removed_providers: bool,",
          "372:     non_interactive: bool,",
          "373:     only_min_version_update: bool,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "373:     include_not_ready_providers: bool,",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "390:     fix_ownership_using_docker()",
          "391:     cleanup_python_generated_files()",
          "392:     if not provider_packages:",
          "395:     if not skip_git_fetch:",
          "396:         run_command([\"git\", \"remote\", \"rm\", \"apache-https-for-providers\"], check=False, stderr=DEVNULL)",
          "",
          "[Removed Lines]",
          "393:         provider_packages = get_available_packages(include_removed=include_removed_providers)",
          "",
          "[Added Lines]",
          "396:         provider_packages = get_available_packages(",
          "397:             include_removed=include_removed_providers, include_not_ready=include_not_ready_providers",
          "398:         )",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "520: )",
          "521: @option_dry_run",
          "522: @option_github_repository",
          "523: @option_include_removed_providers",
          "524: @argument_provider_packages",
          "525: @option_verbose",
          "526: def prepare_provider_packages(",
          "527:     clean_dist: bool,",
          "528:     github_repository: str,",
          "529:     include_removed_providers: bool,",
          "530:     package_format: str,",
          "531:     package_list_file: IO | None,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "528: @option_include_not_ready_providers",
          "535:     include_not_ready_providers: bool,",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "538:     fix_ownership_using_docker()",
          "539:     cleanup_python_generated_files()",
          "540:     packages_list = get_packages_list_to_act_on(",
          "542:     )",
          "543:     if not skip_tag_check:",
          "544:         run_command([\"git\", \"remote\", \"rm\", \"apache-https-for-providers\"], check=False, stderr=DEVNULL)",
          "",
          "[Removed Lines]",
          "541:         package_list_file, provider_packages, include_removed_providers",
          "",
          "[Added Lines]",
          "548:         package_list_file=package_list_file,",
          "549:         provider_packages=provider_packages,",
          "550:         include_removed=include_removed_providers,",
          "551:         include_not_ready=include_not_ready_providers,",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "1145: @option_airflow_site_directory",
          "1146: @option_debug_resources",
          "1147: @option_dry_run",
          "1148: @option_include_removed_providers",
          "1149: @option_include_success_outputs",
          "1150: @click.option(\"-s\", \"--override-versioned\", help=\"Overrides versioned directories.\", is_flag=True)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1158: @option_include_not_ready_providers",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "1165:     debug_resources: bool,",
          "1166:     doc_packages: tuple[str, ...],",
          "1167:     include_success_outputs: bool,",
          "1168:     include_removed_providers: bool,",
          "1169:     override_versioned: bool,",
          "1170:     package_filter: tuple[str, ...],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1179:     include_not_ready_providers: bool,",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1180:         )",
          "1182:     current_packages = find_matching_long_package_names(",
          "1184:         filters=package_filter,",
          "1185:     )",
          "1186:     print(f\"Publishing docs for {len(current_packages)} package(s)\")",
          "",
          "[Removed Lines]",
          "1183:         short_packages=expand_all_provider_packages(doc_packages, include_removed=include_removed_providers),",
          "",
          "[Added Lines]",
          "1195:         short_packages=expand_all_provider_packages(",
          "1196:             short_doc_packages=doc_packages,",
          "1197:             include_removed=include_removed_providers,",
          "1198:             include_not_ready=include_not_ready_providers,",
          "1199:         ),",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "1209:     help=\"Command to add back references for documentation to make it backward compatible.\",",
          "1210: )",
          "1211: @option_airflow_site_directory",
          "1212: @option_include_removed_providers",
          "1213: @argument_doc_packages",
          "1214: @option_dry_run",
          "1215: @option_verbose",
          "1216: def add_back_references(",
          "1217:     airflow_site_directory: str,",
          "1218:     include_removed_providers: bool,",
          "1219:     doc_packages: tuple[str, ...],",
          "1220: ):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1228: @option_include_not_ready_providers",
          "1235:     include_not_ready_providers: bool,",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "1232:         )",
          "1233:         sys.exit(1)",
          "1234:     start_generating_back_references(",
          "1236:     )",
          "",
          "[Removed Lines]",
          "1235:         site_path, list(expand_all_provider_packages(doc_packages, include_removed=include_removed_providers))",
          "",
          "[Added Lines]",
          "1253:         site_path,",
          "1254:         list(",
          "1255:             expand_all_provider_packages(",
          "1256:                 short_doc_packages=doc_packages,",
          "1257:                 include_removed=include_removed_providers,",
          "1258:                 include_not_ready=include_not_ready_providers,",
          "1259:             )",
          "1260:         ),",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "132:             \"options\": [",
          "133:                 \"--clean-dist\",",
          "134:                 \"--github-repository\",",
          "135:                 \"--include-removed-providers\",",
          "136:                 \"--package-format\",",
          "137:                 \"--package-list-file\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "135:                 \"--include-not-ready-providers\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "147:             \"options\": [",
          "148:                 \"--base-branch\",",
          "149:                 \"--github-repository\",",
          "150:                 \"--include-removed-providers\",",
          "151:                 \"--non-interactive\",",
          "152:                 \"--only-min-version-update\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "151:                 \"--include-not-ready-providers\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "197:             \"name\": \"Publish Docs\",",
          "198:             \"options\": [",
          "199:                 \"--airflow-site-directory\",",
          "200:                 \"--include-removed-providers\",",
          "201:                 \"--override-versioned\",",
          "202:                 \"--package-filter\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "202:                 \"--include-not-ready-providers\",",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "218:             \"name\": \"Add Back References to Docs\",",
          "219:             \"options\": [",
          "220:                 \"--airflow-site-directory\",",
          "221:                 \"--include-removed-providers\",",
          "222:             ],",
          "223:         },",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "224:                 \"--include-not-ready-providers\",",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py||dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py": [
          "File: dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py -> dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import sys",
          "23: from pathlib import Path",
          "24: from shutil import copytree, rmtree",
          "27: from airflow_breeze.utils.console import get_console",
          "28: from airflow_breeze.utils.packages import (",
          "29:     get_available_packages,",
          "30:     get_latest_provider_tag,",
          "31:     get_provider_details,",
          "32:     get_provider_jinja_context,",
          "33:     get_removed_provider_ids,",
          "",
          "[Removed Lines]",
          "25: from typing import IO, Any",
          "",
          "[Added Lines]",
          "25: from typing import Any, TextIO",
          "31:     get_not_ready_provider_ids,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "230: def get_packages_list_to_act_on(",
          "232: ) -> list[str]:",
          "233:     if package_list_file and provider_packages:",
          "234:         get_console().print(",
          "",
          "[Removed Lines]",
          "231:     package_list_file: IO | None, provider_packages: tuple[str, ...], include_removed: bool = False",
          "",
          "[Added Lines]",
          "232:     package_list_file: TextIO | None,",
          "233:     provider_packages: tuple[str, ...],",
          "234:     include_not_ready: bool = False,",
          "235:     include_removed: bool = False,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "237:         sys.exit(1)",
          "238:     if package_list_file:",
          "239:         removed_provider_ids = get_removed_provider_ids()",
          "240:         return [",
          "241:             package.strip()",
          "242:             for package in package_list_file.readlines()",
          "244:         ]",
          "245:     elif provider_packages:",
          "246:         return list(provider_packages)",
          "",
          "[Removed Lines]",
          "243:             if package.strip() not in removed_provider_ids",
          "247:     return get_available_packages(include_removed=include_removed)",
          "",
          "[Added Lines]",
          "244:         not_ready_provider_ids = get_not_ready_provider_ids()",
          "248:             if (package.strip() not in removed_provider_ids or include_removed)",
          "249:             and (package.strip() not in not_ready_provider_ids or include_not_ready)",
          "253:     return get_available_packages(include_removed=include_removed, include_not_ready=include_not_ready)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/packages.py||dev/breeze/src/airflow_breeze/utils/packages.py": [
          "File: dev/breeze/src/airflow_breeze/utils/packages.py -> dev/breeze/src/airflow_breeze/utils/packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "228:     ]",
          "231: def get_provider_requirements(provider_id: str) -> list[str]:",
          "232:     package_metadata = get_provider_packages_metadata().get(provider_id)",
          "233:     return package_metadata[\"dependencies\"] if package_metadata else []",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "231: @lru_cache",
          "232: def get_not_ready_provider_ids() -> list[str]:",
          "233:     return [",
          "234:         provider_id",
          "235:         for provider_id, provider_metadata in get_provider_packages_metadata().items()",
          "236:         if provider_metadata.get(\"not-ready\", False)",
          "237:     ]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "239:     include_all_providers: bool = False,",
          "240:     include_suspended: bool = False,",
          "241:     include_removed: bool = False,",
          "242: ) -> list[str]:",
          "243:     \"\"\"",
          "244:     Return provider ids for all packages that are available currently (not suspended).",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "251:     include_not_ready: bool = False,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "246:     :rtype: object",
          "247:     :param include_suspended: whether the suspended packages should be included",
          "248:     :param include_removed: whether the removed packages should be included",
          "249:     :param include_non_provider_doc_packages: whether the non-provider doc packages should be included",
          "250:            (packages like apache-airflow, helm-chart, docker-stack)",
          "251:     :param include_all_providers: whether \"all-providers\" should be included ni the list.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "259:     :param include_not_ready: whether the not-ready ppackages should be included",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "253:     \"\"\"",
          "254:     provider_ids: list[str] = list(json.loads(PROVIDER_DEPENDENCIES_JSON_FILE_PATH.read_text()).keys())",
          "255:     available_packages = []",
          "256:     if include_non_provider_doc_packages:",
          "257:         available_packages.extend(REGULAR_DOC_PACKAGES)",
          "258:     if include_all_providers:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "267:     not_ready_provider_ids = get_not_ready_provider_ids()",
          "268:     if not include_not_ready:",
          "269:         provider_ids = [",
          "270:             provider_id for provider_id in provider_ids if provider_id not in not_ready_provider_ids",
          "271:         ]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "268: def expand_all_provider_packages(",
          "270: ) -> tuple[str, ...]:",
          "271:     \"\"\"In case there are \"all-providers\" in the list, expand the list with all providers.\"\"\"",
          "272:     if \"all-providers\" in short_doc_packages:",
          "273:         packages = [package for package in short_doc_packages if package != \"all-providers\"]",
          "275:         short_doc_packages = tuple(set(packages))",
          "276:     return short_doc_packages",
          "",
          "[Removed Lines]",
          "269:     short_doc_packages: tuple[str, ...], include_removed: bool = False",
          "274:         packages.extend(get_available_packages(include_removed=include_removed))",
          "",
          "[Added Lines]",
          "285:     short_doc_packages: tuple[str, ...],",
          "286:     include_removed: bool = False,",
          "287:     include_not_ready: bool = False,",
          "292:         packages.extend(",
          "293:             get_available_packages(include_removed=include_removed, include_not_ready=include_not_ready)",
          "294:         )",
          "",
          "---------------"
        ]
      }
    }
  ]
}