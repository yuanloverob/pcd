{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "47b8eeeb61657f293d132f29ac5859e39e98f9d6",
      "candidate_info": {
        "commit_hash": "47b8eeeb61657f293d132f29ac5859e39e98f9d6",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/47b8eeeb61657f293d132f29ac5859e39e98f9d6",
        "files": [
          "core/src/main/resources/error/error-classes.json",
          "core/src/main/scala/org/apache/spark/ErrorInfo.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala"
        ],
        "message": "[SPARK-38700][SQL][3.3] Use error classes in the execution errors of save mode\n\n### What changes were proposed in this pull request?\nMigrate the following errors in QueryExecutionErrors:\n\n* unsupportedSaveModeError -> UNSUPPORTED_SAVE_MODE\n\nThis is a backport of https://github.com/apache/spark/pull/36350.\n\n### Why are the changes needed?\nPorting execution errors of unsupported saveMode to new error framework.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nAdd new UT.\n\nCloses #36852 from panbingkun/branch-3.3-SPARK-38700-new.\n\nLead-authored-by: panbingkun <pbk1982@gmail.com>\nCo-authored-by: panbingkun <84731559@qq.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/ErrorInfo.scala||core/src/main/scala/org/apache/spark/ErrorInfo.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/ErrorInfo.scala||core/src/main/scala/org/apache/spark/ErrorInfo.scala": [
          "File: core/src/main/scala/org/apache/spark/ErrorInfo.scala -> core/src/main/scala/org/apache/spark/ErrorInfo.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: import org.apache.spark.util.Utils",
          "40:   @JsonIgnore",
          "41:   val messageFormat: String = message.mkString(\"\\n\")",
          "",
          "[Removed Lines]",
          "38: private[spark] case class ErrorInfo(message: Seq[String], sqlState: Option[String]) {",
          "",
          "[Added Lines]",
          "37: private[spark] case class ErrorSubInfo(message: Seq[String]) {",
          "39:   @JsonIgnore",
          "40:   val messageFormat: String = message.mkString(\"\\n\")",
          "41: }",
          "51: private[spark] case class ErrorInfo(",
          "52:     message: Seq[String],",
          "53:     subClass: Option[Map[String, ErrorSubInfo]],",
          "54:     sqlState: Option[String]) {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "61:       queryContext: String = \"\"): String = {",
          "62:     val errorInfo = errorClassToInfoMap.getOrElse(errorClass,",
          "63:       throw new IllegalArgumentException(s\"Cannot find error class '$errorClass'\"))",
          "64:     val displayQueryContext = if (queryContext.isEmpty) {",
          "65:       \"\"",
          "66:     } else {",
          "67:       s\"\\n$queryContext\"",
          "68:     }",
          "71:   }",
          "73:   def getSqlState(errorClass: String): String = {",
          "",
          "[Removed Lines]",
          "69:     String.format(errorInfo.messageFormat.replaceAll(\"<[a-zA-Z0-9_-]+>\", \"%s\"),",
          "70:       messageParameters: _*) + displayQueryContext",
          "",
          "[Added Lines]",
          "80:     val (displayMessageParameters, displayFormat) = if (errorInfo.subClass.isDefined) {",
          "81:       val subClass = errorInfo.subClass.get",
          "82:       val subErrorClass = messageParameters.head",
          "83:       val errorSubInfo = subClass.getOrElse(subErrorClass,",
          "84:         throw new IllegalArgumentException(s\"Cannot find sub error class '$subErrorClass'\"))",
          "85:       val subMessageParameters = messageParameters.tail",
          "86:       (subMessageParameters, errorInfo.messageFormat + errorSubInfo.messageFormat)",
          "87:     } else {",
          "88:       (messageParameters, errorInfo.messageFormat)",
          "89:     }",
          "90:     val displayMessage = String.format(",
          "91:       displayFormat.replaceAll(\"<[a-zA-Z0-9_-]+>\", \"%s\"),",
          "92:       displayMessageParameters : _*)",
          "98:     s\"$displayMessage$displayQueryContext\"",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "652:        \"\"\".stripMargin)",
          "653:   }",
          "657:   }",
          "659:   def cannotClearOutputDirectoryError(staticPrefixPath: Path): Throwable = {",
          "",
          "[Removed Lines]",
          "655:   def unsupportedSaveModeError(saveMode: String, pathExists: Boolean): Throwable = {",
          "656:     new IllegalStateException(s\"unsupported save mode $saveMode ($pathExists)\")",
          "",
          "[Added Lines]",
          "655:   def saveModeUnsupportedError(saveMode: Any, pathExists: Boolean): Throwable = {",
          "656:     pathExists match {",
          "657:       case true => new SparkIllegalArgumentException(errorClass = \"UNSUPPORTED_SAVE_MODE\",",
          "658:         messageParameters = Array(\"EXISTENT_PATH\", toSQLValue(saveMode, StringType)))",
          "659:       case _ => new SparkIllegalArgumentException(errorClass = \"UNSUPPORTED_SAVE_MODE\",",
          "660:         messageParameters = Array(\"NON_EXISTENT_PATH\", toSQLValue(saveMode, StringType)))",
          "661:     }",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "133:         case (SaveMode.Ignore, exists) =>",
          "134:           !exists",
          "135:         case (s, exists) =>",
          "137:       }",
          "138:     }",
          "",
          "[Removed Lines]",
          "136:           throw QueryExecutionErrors.unsupportedSaveModeError(s.toString, exists)",
          "",
          "[Added Lines]",
          "136:           throw QueryExecutionErrors.saveModeUnsupportedError(s, exists)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.errors",
          "22: import org.apache.spark.sql.execution.datasources.orc.OrcTest",
          "23: import org.apache.spark.sql.execution.datasources.parquet.ParquetTest",
          "24: import org.apache.spark.sql.functions.{lit, lower, struct, sum}",
          "25: import org.apache.spark.sql.internal.SQLConf",
          "26: import org.apache.spark.sql.internal.SQLConf.LegacyBehaviorPolicy.EXCEPTION",
          "27: import org.apache.spark.sql.test.SharedSparkSession",
          "29: class QueryExecutionErrorsSuite extends QueryTest",
          "30:   with ParquetTest with OrcTest with SharedSparkSession {",
          "",
          "[Removed Lines]",
          "20: import org.apache.spark.{SparkArithmeticException, SparkException, SparkRuntimeException, SparkUnsupportedOperationException, SparkUpgradeException}",
          "21: import org.apache.spark.sql.{DataFrame, QueryTest}",
          "",
          "[Added Lines]",
          "20: import org.apache.spark.{SparkArithmeticException, SparkException, SparkIllegalArgumentException, SparkRuntimeException, SparkUnsupportedOperationException, SparkUpgradeException}",
          "21: import org.apache.spark.sql.{DataFrame, QueryTest, SaveMode}",
          "28: import org.apache.spark.util.Utils",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "265:     assert(e.getMessage ===",
          "266:       \"Datetime operation overflow: add 1000000 YEAR to TIMESTAMP '2022-03-09 01:02:03'.\")",
          "267:   }",
          "268: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "270:   test(\"UNSUPPORTED_SAVE_MODE: unsupported null saveMode whether the path exists or not\") {",
          "271:     withTempPath { path =>",
          "272:       val e1 = intercept[SparkIllegalArgumentException] {",
          "273:         val saveMode: SaveMode = null",
          "274:         Seq(1, 2).toDS().write.mode(saveMode).parquet(path.getAbsolutePath)",
          "275:       }",
          "276:       assert(e1.getErrorClass === \"UNSUPPORTED_SAVE_MODE\")",
          "277:       assert(e1.getMessage === \"The save mode NULL is not supported for: a non-existent path.\")",
          "279:       Utils.createDirectory(path)",
          "281:       val e2 = intercept[SparkIllegalArgumentException] {",
          "282:         val saveMode: SaveMode = null",
          "283:         Seq(1, 2).toDS().write.mode(saveMode).parquet(path.getAbsolutePath)",
          "284:       }",
          "285:       assert(e2.getErrorClass === \"UNSUPPORTED_SAVE_MODE\")",
          "286:       assert(e2.getMessage === \"The save mode NULL is not supported for: an existent path.\")",
          "287:     }",
          "288:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2b3df38b430b92e4a8392854988f071b795d543c",
      "candidate_info": {
        "commit_hash": "2b3df38b430b92e4a8392854988f071b795d543c",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/2b3df38b430b92e4a8392854988f071b795d543c",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/linearRegression.scala"
        ],
        "message": "[SPARK-37613][SQL][FOLLOWUP] Supplement docs for regr_count\n\n### What changes were proposed in this pull request?\nhttps://github.com/apache/spark/pull/34880 supported ANSI Aggregate Function: regr_count.\nBut the docs of regr_count is not good enough.\n\n### Why are the changes needed?\nMake the docs of regr_count more detailed.\n\n### Does this PR introduce _any_ user-facing change?\n'No'.\nNew feature.\n\n### How was this patch tested?\nN/A\n\nCloses #36258 from beliefer/SPARK-37613_followup.\n\nAuthored-by: Jiaan Geng <beliefer@163.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 1b106ea32d567dd32ac697ed0d6cfd40ea7e6e08)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/linearRegression.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/linearRegression.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/linearRegression.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/linearRegression.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/linearRegression.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/linearRegression.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import org.apache.spark.sql.catalyst.trees.BinaryLike",
          "23: import org.apache.spark.sql.types.{AbstractDataType, DoubleType, NumericType}",
          "25: @ExpressionDescription(",
          "29:   examples = \"\"\"",
          "30:     Examples:",
          "31:       > SELECT _FUNC_(y, x) FROM VALUES (1, 2), (2, 2), (2, 3), (2, 4) AS tab(y, x);",
          "",
          "[Removed Lines]",
          "26:   usage = \"\"\"",
          "27:     _FUNC_(expr) - Returns the number of non-null number pairs in a group.",
          "28:   \"\"\",",
          "",
          "[Added Lines]",
          "27:   usage = \"_FUNC_(y, x) - Returns the number of non-null number pairs in a group, where `y` is the dependent variable and `x` is the independent variable.\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "194ed0c74415700e13ec8c4fade053c523542efc",
      "candidate_info": {
        "commit_hash": "194ed0c74415700e13ec8c4fade053c523542efc",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/194ed0c74415700e13ec8c4fade053c523542efc",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala"
        ],
        "message": "[SPARK-38892][SQL][TESTS] Fix a test case schema assertion of ParquetPartitionDiscoverySuite\n\n### What changes were proposed in this pull request?\nin ParquetPartitionDiscoverySuite, thare are some assert have no parctical significance.\n`assert(input.schema.sameType(input.schema))`\n\n### Why are the changes needed?\nfix this to assert the actual result.\n\n### Does this PR introduce _any_ user-facing change?\nno\n\n### How was this patch tested?\nupdated testsuites\n\nCloses #36189 from fhygh/assertutfix.\n\nAuthored-by: fhygh <283452027@qq.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 4835946de2ef71b176da5106e9b6c2706e182722)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1076:       val input = spark.read.parquet(path.getAbsolutePath).select(\"id\",",
          "1077:         \"date_month\", \"date_hour\", \"date_t_hour\", \"data\")",
          "1080:       checkAnswer(input, data)",
          "1081:     }",
          "1082:   }",
          "",
          "[Removed Lines]",
          "1079:       assert(input.schema.sameType(input.schema))",
          "",
          "[Added Lines]",
          "1079:       assert(data.schema.sameType(input.schema))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e7c9d1a8c8e3604347aa969b66b52fac6f58be97",
      "candidate_info": {
        "commit_hash": "e7c9d1a8c8e3604347aa969b66b52fac6f58be97",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/e7c9d1a8c8e3604347aa969b66b52fac6f58be97",
        "files": [
          "R/pkg/DESCRIPTION",
          "assembly/pom.xml",
          "common/kvstore/pom.xml",
          "common/network-common/pom.xml",
          "common/network-shuffle/pom.xml",
          "common/network-yarn/pom.xml",
          "common/sketch/pom.xml",
          "common/tags/pom.xml",
          "common/unsafe/pom.xml",
          "core/pom.xml",
          "docs/_config.yml",
          "examples/pom.xml",
          "external/avro/pom.xml",
          "external/docker-integration-tests/pom.xml",
          "external/kafka-0-10-assembly/pom.xml",
          "external/kafka-0-10-sql/pom.xml",
          "external/kafka-0-10-token-provider/pom.xml",
          "external/kafka-0-10/pom.xml",
          "external/kinesis-asl-assembly/pom.xml",
          "external/kinesis-asl/pom.xml",
          "external/spark-ganglia-lgpl/pom.xml",
          "graphx/pom.xml",
          "hadoop-cloud/pom.xml",
          "launcher/pom.xml",
          "mllib-local/pom.xml",
          "mllib/pom.xml",
          "pom.xml",
          "python/pyspark/version.py",
          "repl/pom.xml",
          "resource-managers/kubernetes/core/pom.xml",
          "resource-managers/kubernetes/integration-tests/pom.xml",
          "resource-managers/mesos/pom.xml",
          "resource-managers/yarn/pom.xml",
          "sql/catalyst/pom.xml",
          "sql/core/pom.xml",
          "sql/hive-thriftserver/pom.xml",
          "sql/hive/pom.xml",
          "streaming/pom.xml",
          "tools/pom.xml"
        ],
        "message": "Preparing development version 3.3.2-SNAPSHOT",
        "before_after_code_files": [
          "python/pyspark/version.py||python/pyspark/version.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/version.py||python/pyspark/version.py": [
          "File: python/pyspark/version.py -> python/pyspark/version.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # See the License for the specific language governing permissions and",
          "17: # limitations under the License.",
          "",
          "[Removed Lines]",
          "19: __version__: str = \"3.3.1\"",
          "",
          "[Added Lines]",
          "19: __version__: str = \"3.3.2.dev0\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f0564354f1207a8b84d251ae323a9e9733bf66de",
      "candidate_info": {
        "commit_hash": "f0564354f1207a8b84d251ae323a9e9733bf66de",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/f0564354f1207a8b84d251ae323a9e9733bf66de",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/core/src/test/resources/sql-tests/results/ansi/array.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/cast.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/decimalArithmeticOperations.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/timestamp.sql.out",
          "sql/core/src/test/resources/sql-tests/results/interval.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/case.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/float4.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/float8.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/int8.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/select_having.sql.out",
          "sql/core/src/test/resources/sql-tests/results/timestampNTZ/timestamp-ansi.sql.out",
          "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-case.sql.out",
          "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-select_having.sql.out",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala"
        ],
        "message": "[SPARK-39007][SQL][3.3] Use double quotes for SQL configs in error messages\n\n### What changes were proposed in this pull request?\nWrap SQL configs in error messages by double quotes. Added the `toSQLConf()` method to `QueryErrorsBase` to invoke it from `Query.*Errors`.\n\nThis is a backport of https://github.com/apache/spark/pull/36335.\n\n### Why are the changes needed?\n1. To highlight types and make them more visible for users.\n2. To be able to easily parse types from error text.\n3. To be consistent to other outputs of identifiers, sql statement and etc. where Spark uses quotes or ticks.\n\n### Does this PR introduce _any_ user-facing change?\nYes, it changes user-facing error messages.\n\n### How was this patch tested?\nBy running the modified test suites:\n```\n$ build/sbt \"testOnly *QueryCompilationErrorsSuite\"\n$ build/sbt \"testOnly *QueryExecutionAnsiErrorsSuite\"\n$ build/sbt \"testOnly *QueryExecutionErrorsSuite\"\n```\n\nAuthored-by: Max Gekk <max.gekkgmail.com>\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n(cherry picked from commit f01bff971e36870e101b2f76195e0d380db64e0c)\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n\nCloses #36340 from MaxGekk/output-conf-error-class-3.3.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "48:     litToErrorValue(Literal.create(v, t))",
          "49:   }",
          "52:   def toSQLStmt(text: String): String = {",
          "54:   }",
          "56:   def toSQLId(parts: Seq[String]): String = {",
          "",
          "[Removed Lines]",
          "53:     \"\\\"\" + text.toUpperCase(Locale.ROOT) + \"\\\"\"",
          "",
          "[Added Lines]",
          "51:   private def quoteByDefault(elem: String): String = {",
          "52:     \"\\\"\" + elem + \"\\\"\"",
          "53:   }",
          "57:     quoteByDefault(text.toUpperCase(Locale.ROOT))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "62:   }",
          "64:   def toSQLType(t: DataType): String = {",
          "66:   }",
          "67: }",
          "",
          "[Removed Lines]",
          "65:     \"\\\"\" + t.sql + \"\\\"\"",
          "",
          "[Added Lines]",
          "69:     quoteByDefault(t.sql)",
          "70:   }",
          "72:   def toSQLConf(conf: String): String = {",
          "73:     quoteByDefault(conf)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "92:   def castingCauseOverflowError(t: Any, dataType: DataType): ArithmeticException = {",
          "93:     new SparkArithmeticException(errorClass = \"CAST_CAUSES_OVERFLOW\",",
          "95:   }",
          "97:   def cannotChangeDecimalPrecisionError(",
          "",
          "[Removed Lines]",
          "94:       messageParameters = Array(toSQLValue(t), toSQLType(dataType), SQLConf.ANSI_ENABLED.key))",
          "",
          "[Added Lines]",
          "94:       messageParameters = Array(",
          "95:         toSQLValue(t), toSQLType(dataType), toSQLConf(SQLConf.ANSI_ENABLED.key)))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "99:       decimalPrecision: Int,",
          "100:       decimalScale: Int,",
          "101:       context: String): ArithmeticException = {",
          "105:   }",
          "107:   def invalidInputSyntaxForNumericError(",
          "",
          "[Removed Lines]",
          "102:     new SparkArithmeticException(errorClass = \"CANNOT_CHANGE_DECIMAL_PRECISION\",",
          "103:       messageParameters = Array(value.toDebugString,",
          "104:         decimalPrecision.toString, decimalScale.toString, SQLConf.ANSI_ENABLED.key, context))",
          "",
          "[Added Lines]",
          "103:     new SparkArithmeticException(",
          "104:       errorClass = \"CANNOT_CHANGE_DECIMAL_PRECISION\",",
          "105:       messageParameters = Array(",
          "106:         value.toDebugString,",
          "107:         decimalPrecision.toString,",
          "108:         decimalScale.toString,",
          "109:         toSQLConf(SQLConf.ANSI_ENABLED.key),",
          "110:         context))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "149:   def divideByZeroError(context: String): ArithmeticException = {",
          "150:     new SparkArithmeticException(",
          "152:   }",
          "154:   def invalidArrayIndexError(index: Int, numElements: Int): ArrayIndexOutOfBoundsException = {",
          "",
          "[Removed Lines]",
          "151:       errorClass = \"DIVIDE_BY_ZERO\", messageParameters = Array(SQLConf.ANSI_ENABLED.key, context))",
          "",
          "[Added Lines]",
          "157:       errorClass = \"DIVIDE_BY_ZERO\",",
          "158:       messageParameters = Array(toSQLConf(SQLConf.ANSI_ENABLED.key), context))",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "163:       index: Int,",
          "164:       numElements: Int,",
          "165:       key: String): ArrayIndexOutOfBoundsException = {",
          "168:   }",
          "170:   def invalidElementAtIndexError(",
          "",
          "[Removed Lines]",
          "166:     new SparkArrayIndexOutOfBoundsException(errorClass = \"INVALID_ARRAY_INDEX\",",
          "167:       messageParameters = Array(toSQLValue(index), toSQLValue(numElements), key))",
          "",
          "[Added Lines]",
          "173:     new SparkArrayIndexOutOfBoundsException(",
          "174:       errorClass = \"INVALID_ARRAY_INDEX\",",
          "175:       messageParameters = Array(toSQLValue(index), toSQLValue(numElements), toSQLConf(key)))",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "173:     new SparkArrayIndexOutOfBoundsException(",
          "174:       errorClass = \"INVALID_ARRAY_INDEX_IN_ELEMENT_AT\",",
          "175:       messageParameters =",
          "177:   }",
          "179:   def mapKeyNotExistError(key: Any, context: String): NoSuchElementException = {",
          "",
          "[Removed Lines]",
          "176:         Array(toSQLValue(index), toSQLValue(numElements), SQLConf.ANSI_ENABLED.key))",
          "",
          "[Added Lines]",
          "184:         Array(toSQLValue(index), toSQLValue(numElements), toSQLConf(SQLConf.ANSI_ENABLED.key)))",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "186:   }",
          "188:   def invalidFractionOfSecondError(): DateTimeException = {",
          "191:   }",
          "193:   def ansiDateTimeParseError(e: DateTimeParseException): DateTimeParseException = {",
          "",
          "[Removed Lines]",
          "189:     new SparkDateTimeException(errorClass = \"INVALID_FRACTION_OF_SECOND\",",
          "190:       Array(SQLConf.ANSI_ENABLED.key))",
          "",
          "[Added Lines]",
          "197:     new SparkDateTimeException(",
          "198:       errorClass = \"INVALID_FRACTION_OF_SECOND\",",
          "199:       Array(toSQLConf(SQLConf.ANSI_ENABLED.key)))",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "550:            |from $format files can be ambiguous, as the files may be written by",
          "551:            |Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar",
          "552:            |that is different from Spark 3.0+'s Proleptic Gregorian calendar.",
          "554:            |the datasource option '$option' to 'LEGACY' to rebase the datetime values",
          "555:            |w.r.t. the calendar difference during reading. To read the datetime values",
          "557:            |to 'CORRECTED'.",
          "558:            |\"\"\".stripMargin),",
          "559:       cause = null",
          "",
          "[Removed Lines]",
          "553:            |See more details in SPARK-31404. You can set the SQL config '$config' or",
          "556:            |as it is, set the SQL config '$config' or the datasource option '$option'",
          "",
          "[Added Lines]",
          "562:            |See more details in SPARK-31404. You can set the SQL config ${toSQLConf(config)} or",
          "565:            |as it is, set the SQL config ${toSQLConf(config)} or the datasource option '$option'",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "566:       messageParameters = Array(",
          "567:         \"3.0\",",
          "568:         s\"\"\"",
          "579:       cause = null",
          "580:     )",
          "581:   }",
          "",
          "[Removed Lines]",
          "569:            |writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z",
          "570:            |into $format files can be dangerous, as the files may be read by Spark 2.x",
          "571:            |or legacy versions of Hive later, which uses a legacy hybrid calendar that",
          "572:            |is different from Spark 3.0+'s Proleptic Gregorian calendar. See more",
          "573:            |details in SPARK-31404. You can set $config to 'LEGACY' to rebase the",
          "574:            |datetime values w.r.t. the calendar difference during writing, to get maximum",
          "575:            |interoperability. Or set $config to 'CORRECTED' to write the datetime values",
          "576:            |as it is, if you are 100% sure that the written files will only be read by",
          "577:            |Spark 3.0+ or other systems that use Proleptic Gregorian calendar.",
          "578:            |\"\"\".stripMargin),",
          "",
          "[Added Lines]",
          "578:           |writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z",
          "579:           |into $format files can be dangerous, as the files may be read by Spark 2.x",
          "580:           |or legacy versions of Hive later, which uses a legacy hybrid calendar that",
          "581:           |is different from Spark 3.0+'s Proleptic Gregorian calendar. See more",
          "582:           |details in SPARK-31404. You can set ${toSQLConf(config)} to 'LEGACY' to rebase the",
          "583:           |datetime values w.r.t. the calendar difference during writing, to get maximum",
          "584:           |interoperability. Or set ${toSQLConf(config)} to 'CORRECTED' to write the datetime",
          "585:           |values as it is, if you are 100% sure that the written files will only be read by",
          "586:           |Spark 3.0+ or other systems that use Proleptic Gregorian calendar.",
          "587:           |\"\"\".stripMargin),",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "182:       }.getCause.asInstanceOf[SparkUpgradeException]",
          "184:       val format = \"Parquet\"",
          "186:       val option = \"datetimeRebaseMode\"",
          "187:       assert(e.getErrorClass === \"INCONSISTENT_BEHAVIOR_CROSS_VERSION\")",
          "188:       assert(e.getMessage ===",
          "189:         \"You may get a different result due to the upgrading to Spark >= 3.0: \" +",
          "201:     }",
          "",
          "[Removed Lines]",
          "185:       val config = SQLConf.PARQUET_REBASE_MODE_IN_READ.key",
          "190:           s\"\"\"",
          "191:              |reading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z",
          "192:              |from $format files can be ambiguous, as the files may be written by",
          "193:              |Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar",
          "194:              |that is different from Spark 3.0+'s Proleptic Gregorian calendar.",
          "195:              |See more details in SPARK-31404. You can set the SQL config '$config' or",
          "196:              |the datasource option '$option' to 'LEGACY' to rebase the datetime values",
          "197:              |w.r.t. the calendar difference during reading. To read the datetime values",
          "198:              |as it is, set the SQL config '$config' or the datasource option '$option'",
          "199:              |to 'CORRECTED'.",
          "200:              |\"\"\".stripMargin)",
          "",
          "[Added Lines]",
          "185:       val config = \"\\\"\" + SQLConf.PARQUET_REBASE_MODE_IN_READ.key + \"\\\"\"",
          "190:         s\"\"\"",
          "191:           |reading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z",
          "192:           |from $format files can be ambiguous, as the files may be written by",
          "193:           |Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar",
          "194:           |that is different from Spark 3.0+'s Proleptic Gregorian calendar.",
          "195:           |See more details in SPARK-31404. You can set the SQL config $config or",
          "196:           |the datasource option '$option' to 'LEGACY' to rebase the datetime values",
          "197:           |w.r.t. the calendar difference during reading. To read the datetime values",
          "198:           |as it is, set the SQL config $config or the datasource option '$option'",
          "199:           |to 'CORRECTED'.",
          "200:           |\"\"\".stripMargin)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "209:         }.getCause.getCause.getCause.asInstanceOf[SparkUpgradeException]",
          "211:         val format = \"Parquet\"",
          "213:         assert(e.getErrorClass === \"INCONSISTENT_BEHAVIOR_CROSS_VERSION\")",
          "214:         assert(e.getMessage ===",
          "215:           \"You may get a different result due to the upgrading to Spark >= 3.0: \" +",
          "227:       }",
          "228:     }",
          "229:   }",
          "",
          "[Removed Lines]",
          "212:         val config = SQLConf.PARQUET_REBASE_MODE_IN_WRITE.key",
          "216:             s\"\"\"",
          "217:                |writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z",
          "218:                |into $format files can be dangerous, as the files may be read by Spark 2.x",
          "219:                |or legacy versions of Hive later, which uses a legacy hybrid calendar that",
          "220:                |is different from Spark 3.0+'s Proleptic Gregorian calendar. See more",
          "221:                |details in SPARK-31404. You can set $config to 'LEGACY' to rebase the",
          "222:                |datetime values w.r.t. the calendar difference during writing, to get maximum",
          "223:                |interoperability. Or set $config to 'CORRECTED' to write the datetime values",
          "224:                |as it is, if you are 100% sure that the written files will only be read by",
          "225:                |Spark 3.0+ or other systems that use Proleptic Gregorian calendar.",
          "226:                |\"\"\".stripMargin)",
          "",
          "[Added Lines]",
          "212:         val config = \"\\\"\" + SQLConf.PARQUET_REBASE_MODE_IN_WRITE.key + \"\\\"\"",
          "216:           s\"\"\"",
          "217:             |writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z",
          "218:             |into $format files can be dangerous, as the files may be read by Spark 2.x",
          "219:             |or legacy versions of Hive later, which uses a legacy hybrid calendar that",
          "220:             |is different from Spark 3.0+'s Proleptic Gregorian calendar. See more",
          "221:             |details in SPARK-31404. You can set $config to 'LEGACY' to rebase the",
          "222:             |datetime values w.r.t. the calendar difference during writing, to get maximum",
          "223:             |interoperability. Or set $config to 'CORRECTED' to write the datetime",
          "224:             |values as it is, if you are 100% sure that the written files will only be read by",
          "225:             |Spark 3.0+ or other systems that use Proleptic Gregorian calendar.",
          "226:             |\"\"\".stripMargin)",
          "",
          "---------------"
        ]
      }
    }
  ]
}