{
  "cve_id": "CVE-2023-39441",
  "cve_desc": "Apache Airflow SMTP Provider before 1.3.0, Apache Airflow IMAP Provider before 3.3.0, and\u00a0Apache Airflow before 2.7.0 are affected by the\u00a0Validation of OpenSSL Certificate vulnerability.\n\nThe default SSL context with SSL library did not check a server's X.509\u00a0certificate.\u00a0 Instead, the code accepted any certificate, which could\u00a0result in the disclosure of mail server credentials or mail contents\u00a0when the client connects to an attacker in a MITM position.\n\nUsers are strongly advised to upgrade to Apache Airflow version 2.7.0 or newer, Apache Airflow IMAP Provider version 3.3.0 or newer, and Apache Airflow SMTP Provider version 1.3.0 or newer to mitigate the risk associated with this vulnerability",
  "repo": "apache/airflow",
  "patch_hash": "38fc9cd823feafd8ec61d5d5c7eddb9e9162f755",
  "patch_info": {
    "commit_hash": "38fc9cd823feafd8ec61d5d5c7eddb9e9162f755",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/38fc9cd823feafd8ec61d5d5c7eddb9e9162f755",
    "files": [
      "airflow/providers/imap/CHANGELOG.rst",
      "airflow/providers/imap/hooks/imap.py",
      "airflow/providers/imap/provider.yaml",
      "docs/apache-airflow-providers-imap/configurations-ref.rst",
      "docs/apache-airflow-providers-imap/index.rst",
      "docs/apache-airflow/configurations-ref.rst",
      "tests/providers/imap/hooks/test_imap.py"
    ],
    "message": "Allows to choose SSL context for IMAP provider (#33108)\n\n* Allows to choose SSL context for IMAP provider\n\nThis change add two options to choose from when SSL IMAP connection is created:\n\n* default - for balance between compatibility and security\n* none - in case compatibility with existing infrastructure is preferred\n\nThe fallback is:\n\n* The Airflow \"email\", \"ssl_context\"\n* \"default\"\n\nCo-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>\n(cherry picked from commit 52ca7bfc988f4c9b608f544bc3e9524fd6564639)",
    "before_after_code_files": [
      "airflow/providers/imap/hooks/imap.py||airflow/providers/imap/hooks/imap.py",
      "tests/providers/imap/hooks/test_imap.py||tests/providers/imap/hooks/test_imap.py"
    ]
  },
  "patch_diff": {
    "airflow/providers/imap/hooks/imap.py||airflow/providers/imap/hooks/imap.py": [
      "File: airflow/providers/imap/hooks/imap.py -> airflow/providers/imap/hooks/imap.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "26: import imaplib",
      "27: import os",
      "28: import re",
      "29: from typing import Any, Iterable",
      "31: from airflow.exceptions import AirflowException",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "29: import ssl",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "78:         return self",
      "80:     def _build_client(self, conn: Connection) -> imaplib.IMAP4_SSL | imaplib.IMAP4:",
      "84:         else:",
      "92:         return mail_client",
      "",
      "[Removed Lines]",
      "81:         IMAP: type[imaplib.IMAP4_SSL] | type[imaplib.IMAP4]",
      "82:         if conn.extra_dejson.get(\"use_ssl\", True):",
      "83:             IMAP = imaplib.IMAP4_SSL",
      "85:             IMAP = imaplib.IMAP4",
      "87:         if conn.port:",
      "88:             mail_client = IMAP(conn.host, conn.port)",
      "89:         else:",
      "90:             mail_client = IMAP(conn.host)",
      "",
      "[Added Lines]",
      "82:         mail_client: imaplib.IMAP4_SSL | imaplib.IMAP4",
      "83:         use_ssl = conn.extra_dejson.get(\"use_ssl\", True)",
      "84:         if use_ssl:",
      "85:             from airflow.configuration import conf",
      "87:             ssl_context_string = conf.get(\"imap\", \"SSL_CONTEXT\", fallback=None)",
      "88:             if ssl_context_string is None:",
      "89:                 ssl_context_string = conf.get(\"email\", \"SSL_CONTEXT\", fallback=None)",
      "90:             if ssl_context_string is None:",
      "91:                 ssl_context_string = \"default\"",
      "92:             if ssl_context_string == \"default\":",
      "93:                 ssl_context = ssl.create_default_context()",
      "94:             elif ssl_context_string == \"none\":",
      "95:                 ssl_context = None",
      "96:             else:",
      "97:                 raise RuntimeError(",
      "98:                     f\"The email.ssl_context configuration variable must \"",
      "99:                     f\"be set to 'default' or 'none' and is '{ssl_context_string}'.\"",
      "100:                 )",
      "101:             if conn.port:",
      "102:                 mail_client = imaplib.IMAP4_SSL(conn.host, conn.port, ssl_context=ssl_context)",
      "103:             else:",
      "104:                 mail_client = imaplib.IMAP4_SSL(conn.host, ssl_context=ssl_context)",
      "106:             if conn.port:",
      "107:                 mail_client = imaplib.IMAP4(conn.host, conn.port)",
      "108:             else:",
      "109:                 mail_client = imaplib.IMAP4(conn.host)",
      "",
      "---------------"
    ],
    "tests/providers/imap/hooks/test_imap.py||tests/providers/imap/hooks/test_imap.py": [
      "File: tests/providers/imap/hooks/test_imap.py -> tests/providers/imap/hooks/test_imap.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "27: from airflow.models import Connection",
      "28: from airflow.providers.imap.hooks.imap import ImapHook",
      "29: from airflow.utils import db",
      "31: imaplib_string = \"airflow.providers.imap.hooks.imap.imaplib\"",
      "32: open_string = \"airflow.providers.imap.hooks.imap.open\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "30: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "85:         )",
      "87:     @patch(imaplib_string)",
      "89:         mock_conn = _create_fake_imap(mock_imaplib)",
      "91:         with ImapHook():",
      "92:             pass",
      "95:         mock_conn.login.assert_called_once_with(\"imap_user\", \"imap_password\")",
      "96:         assert mock_conn.logout.call_count == 1",
      "",
      "[Removed Lines]",
      "88:     def test_connect_and_disconnect(self, mock_imaplib):",
      "94:         mock_imaplib.IMAP4_SSL.assert_called_once_with(\"imap_server_address\", 1993)",
      "",
      "[Added Lines]",
      "89:     @patch(\"ssl.create_default_context\")",
      "90:     def test_connect_and_disconnect(self, create_default_context, mock_imaplib):",
      "96:         assert create_default_context.called",
      "97:         mock_imaplib.IMAP4_SSL.assert_called_once_with(",
      "98:             \"imap_server_address\", 1993, ssl_context=create_default_context.return_value",
      "99:         )",
      "100:         mock_conn.login.assert_called_once_with(\"imap_user\", \"imap_password\")",
      "101:         assert mock_conn.logout.call_count == 1",
      "103:     @patch(imaplib_string)",
      "104:     @patch(\"ssl.create_default_context\")",
      "105:     def test_connect_and_disconnect_imap_ssl_context_none(self, create_default_context, mock_imaplib):",
      "106:         mock_conn = _create_fake_imap(mock_imaplib)",
      "108:         with conf_vars({(\"imap\", \"ssl_context\"): \"none\"}):",
      "109:             with ImapHook():",
      "110:                 pass",
      "112:         assert not create_default_context.called",
      "113:         mock_imaplib.IMAP4_SSL.assert_called_once_with(\"imap_server_address\", 1993, ssl_context=None)",
      "114:         mock_conn.login.assert_called_once_with(\"imap_user\", \"imap_password\")",
      "115:         assert mock_conn.logout.call_count == 1",
      "117:     @patch(imaplib_string)",
      "118:     @patch(\"ssl.create_default_context\")",
      "119:     def test_connect_and_disconnect_imap_ssl_context_default(self, create_default_context, mock_imaplib):",
      "120:         mock_conn = _create_fake_imap(mock_imaplib)",
      "122:         with conf_vars({(\"imap\", \"ssl_context\"): \"default\"}):",
      "123:             with ImapHook():",
      "124:                 pass",
      "126:         assert create_default_context.called",
      "127:         mock_imaplib.IMAP4_SSL.assert_called_once_with(",
      "128:             \"imap_server_address\", 1993, ssl_context=create_default_context.return_value",
      "129:         )",
      "130:         mock_conn.login.assert_called_once_with(\"imap_user\", \"imap_password\")",
      "131:         assert mock_conn.logout.call_count == 1",
      "133:     @patch(imaplib_string)",
      "134:     @patch(\"ssl.create_default_context\")",
      "135:     def test_connect_and_disconnect_email_ssl_context_none(self, create_default_context, mock_imaplib):",
      "136:         mock_conn = _create_fake_imap(mock_imaplib)",
      "138:         with conf_vars({(\"email\", \"ssl_context\"): \"none\"}):",
      "139:             with ImapHook():",
      "140:                 pass",
      "142:         assert not create_default_context.called",
      "143:         mock_imaplib.IMAP4_SSL.assert_called_once_with(\"imap_server_address\", 1993, ssl_context=None)",
      "144:         mock_conn.login.assert_called_once_with(\"imap_user\", \"imap_password\")",
      "145:         assert mock_conn.logout.call_count == 1",
      "147:     @patch(imaplib_string)",
      "148:     @patch(\"ssl.create_default_context\")",
      "149:     def test_connect_and_disconnect_imap_ssl_context_override(self, create_default_context, mock_imaplib):",
      "150:         mock_conn = _create_fake_imap(mock_imaplib)",
      "152:         with conf_vars({(\"email\", \"ssl_context\"): \"none\", (\"imap\", \"ssl_context\"): \"default\"}):",
      "153:             with ImapHook():",
      "154:                 pass",
      "156:         assert create_default_context.called",
      "157:         mock_imaplib.IMAP4_SSL.assert_called_once_with(",
      "158:             \"imap_server_address\", 1993, ssl_context=create_default_context.return_value",
      "159:         )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "cf9e3fcdddd1674025b97a3f80ee4d2dbe6b219b",
      "candidate_info": {
        "commit_hash": "cf9e3fcdddd1674025b97a3f80ee4d2dbe6b219b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cf9e3fcdddd1674025b97a3f80ee4d2dbe6b219b",
        "files": [
          "airflow/api_connexion/endpoints/pool_endpoint.py"
        ],
        "message": "Remove redundant dict.keys() call (#33158)\n\n(cherry picked from commit ba9d4e470de13031fc306772900c73b9642bb0cb)",
        "before_after_code_files": [
          "airflow/api_connexion/endpoints/pool_endpoint.py||airflow/api_connexion/endpoints/pool_endpoint.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api_connexion/endpoints/pool_endpoint.py||airflow/api_connexion/endpoints/pool_endpoint.py": [
          "File: airflow/api_connexion/endpoints/pool_endpoint.py -> airflow/api_connexion/endpoints/pool_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "125:     else:",
          "126:         required_fields = {\"name\", \"slots\"}",
          "128:         if fields_diff:",
          "129:             raise BadRequest(detail=f\"Missing required property(ies): {sorted(fields_diff)}\")",
          "",
          "[Removed Lines]",
          "127:         fields_diff = required_fields - set(get_json_request_dict().keys())",
          "",
          "[Added Lines]",
          "127:         fields_diff = required_fields.difference(get_json_request_dict())",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "139: def post_pool(*, session: Session = NEW_SESSION) -> APIResponse:",
          "140:     \"\"\"Create a pool.\"\"\"",
          "141:     required_fields = {\"name\", \"slots\"}  # Pool would require both fields in the post request",
          "143:     if fields_diff:",
          "144:         raise BadRequest(detail=f\"Missing required property(ies): {sorted(fields_diff)}\")",
          "",
          "[Removed Lines]",
          "142:     fields_diff = required_fields - set(get_json_request_dict().keys())",
          "",
          "[Added Lines]",
          "142:     fields_diff = required_fields.difference(get_json_request_dict())",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "99aaacbd64c40dbbb825cb2cf9d1db911d5ce180",
      "candidate_info": {
        "commit_hash": "99aaacbd64c40dbbb825cb2cf9d1db911d5ce180",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/99aaacbd64c40dbbb825cb2cf9d1db911d5ce180",
        "files": [
          "airflow/cli/cli_parser.py",
          "airflow/cli/commands/provider_command.py",
          "airflow/cli/commands/role_command.py",
          "airflow/cli/simple_table.py"
        ],
        "message": "Refactor: Simplify dict manipulation in airflow/cli (#33159)\n\n(cherry picked from commit c074a1ecd946b2ad0f85be62aa977c269e2ffb83)",
        "before_after_code_files": [
          "airflow/cli/cli_parser.py||airflow/cli/cli_parser.py",
          "airflow/cli/commands/provider_command.py||airflow/cli/commands/provider_command.py",
          "airflow/cli/commands/role_command.py||airflow/cli/commands/role_command.py",
          "airflow/cli/simple_table.py||airflow/cli/simple_table.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/cli_parser.py||airflow/cli/cli_parser.py": [
          "File: airflow/cli/cli_parser.py -> airflow/cli/cli_parser.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "116:     subparsers.required = True",
          "118:     command_dict = DAG_CLI_DICT if dag_parser else ALL_COMMANDS_DICT",
          "123:         _add_command(subparsers, sub)",
          "124:     return parser",
          "",
          "[Removed Lines]",
          "119:     subparser_list = command_dict.keys()",
          "120:     sub_name: str",
          "121:     for sub_name in sorted(subparser_list):",
          "122:         sub: CLICommand = command_dict[sub_name]",
          "",
          "[Added Lines]",
          "119:     for _, sub in sorted(command_dict.items()):",
          "",
          "---------------"
        ],
        "airflow/cli/commands/provider_command.py||airflow/cli/commands/provider_command.py": [
          "File: airflow/cli/commands/provider_command.py -> airflow/cli/commands/provider_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "106: def connection_form_widget_list(args):",
          "107:     \"\"\"Lists all custom connection form fields at the command line.\"\"\"",
          "108:     AirflowConsole().print_as(",
          "110:         output=args.output,",
          "111:         mapper=lambda x: {",
          "112:             \"connection_parameter_name\": x[0],",
          "",
          "[Removed Lines]",
          "109:         data=list(sorted(ProvidersManager().connection_form_widgets.items())),",
          "",
          "[Added Lines]",
          "109:         data=sorted(ProvidersManager().connection_form_widgets.items()),",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "122: def connection_field_behaviours(args):",
          "123:     \"\"\"Lists field behaviours.\"\"\"",
          "124:     AirflowConsole().print_as(",
          "126:         output=args.output,",
          "127:         mapper=lambda x: {",
          "128:             \"field_behaviours\": x,",
          "",
          "[Removed Lines]",
          "125:         data=list(ProvidersManager().field_behaviours.keys()),",
          "",
          "[Added Lines]",
          "125:         data=list(ProvidersManager().field_behaviours),",
          "",
          "---------------"
        ],
        "airflow/cli/commands/role_command.py||airflow/cli/commands/role_command.py": [
          "File: airflow/cli/commands/role_command.py -> airflow/cli/commands/role_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "121:                 exit(1)",
          "123:         permission_count = 0",
          "126:         ):",
          "127:             res_key = (role_name, resource_name)",
          "128:             if is_add and action_name not in perm_map[res_key]:",
          "",
          "[Removed Lines]",
          "124:         for (role_name, resource_name, action_name) in list(",
          "125:             itertools.product(args.role, args.resource, args.action or [None])",
          "",
          "[Added Lines]",
          "124:         for role_name, resource_name, action_name in itertools.product(",
          "125:             args.role, args.resource, args.action or [None]",
          "",
          "---------------"
        ],
        "airflow/cli/simple_table.py||airflow/cli/simple_table.py": [
          "File: airflow/cli/simple_table.py -> airflow/cli/simple_table.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "64:             return",
          "66:         table = SimpleTable(show_header=self.show_header)",
          "68:             table.add_column(col)",
          "70:         for row in data:",
          "",
          "[Removed Lines]",
          "67:         for col in data[0].keys():",
          "",
          "[Added Lines]",
          "67:         for col in data[0]:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "77:             self.print(\"No data found\")",
          "78:             return",
          "79:         rows = [d.values() for d in data]",
          "81:         print(output)",
          "83:     def _normalize_data(self, value: Any, output: str) -> list | str | dict | None:",
          "",
          "[Removed Lines]",
          "80:         output = tabulate(rows, tablefmt=\"plain\", headers=list(data[0].keys()))",
          "",
          "[Added Lines]",
          "80:         output = tabulate(rows, tablefmt=\"plain\", headers=list(data[0]))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "108:         }",
          "109:         renderer = output_to_renderer.get(output)",
          "110:         if not renderer:",
          "115:         if mapper:",
          "116:             dict_data: Sequence[dict] = [mapper(d) for d in data]",
          "",
          "[Removed Lines]",
          "111:             raise ValueError(",
          "112:                 f\"Unknown formatter: {output}. Allowed options: {list(output_to_renderer.keys())}\"",
          "113:             )",
          "",
          "[Added Lines]",
          "111:             raise ValueError(f\"Unknown formatter: {output}. Allowed options: {list(output_to_renderer)}\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "816d4ccfdecce9c7d8fa64bef9873ec963892d22",
      "candidate_info": {
        "commit_hash": "816d4ccfdecce9c7d8fa64bef9873ec963892d22",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/816d4ccfdecce9c7d8fa64bef9873ec963892d22",
        "files": [
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "scripts/in_container/_in_container_utils.sh",
          "scripts/in_container/check_environment.sh"
        ],
        "message": "Fix installing older airflow versions in Breeze with openssl fix (#33171)\n\nThe dnspython and Flask Application Builder have a weird transitive\ndependency to openssl. It does not require it - it is an optional\ndependnency of email validator that uses dnspython and the dnspython\nuses pyopenssl when installed. This is all fine. But when opendns\nis used with older FAB < 4.1.4) and NEWER opensl (>=23.0), just\ncreating the FAB applicatio in flask causes an exception than\nthere is a missing parameter in openssl.\n\nSince openssl is NOT declared as required dependency in neither FAB\nnor dnspython, installing airflow with constraints while\nhaving a newer version of pyopenssl does not downgrade pyopenssl\nit is just mentioned in a conflicting message resulting from conflicts\nwith other packages already installed (such as cryptography).\n\nThis caused a problem when installin older airflow version in breeze\nwith --use-airflow-version switch.\n\nAlso - during checking this issue, it turned out that we were not\n- by default - using the constraints for older version when installing\nit in breeze (you could do that by explicitly specifying the\nconstraints). This was because when you used anoter way to specify\nversio (commit hash, tag) it is quite complex to figure out which\nconstraints to use. However we can automatically derive constraints\nwhen you specify version, or branch, which are the most common\nscenarios.\n\nThis PR fixes the `--use-airflow-version` case for Airflow 2.4 and below\nby:\n\n* automatically deriving the right constraints when version or branch\n  are used in `--use-airflow-version`\n\n* adding pyopenssl specifically as dependency when\n  --use-airflow-version switch is used.\n\n* also a small quality of life improvment - when airflow db migrate\n  fails (because old versio of airflow does not support it) - we inform\n  the user that it happened and that we are attempting to run the\n  legacy airflow db init instead.\n\n(cherry picked from commit 5387c0c761fe161f28cee70a46009e1e67286679)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "scripts/in_container/_in_container_utils.sh||scripts/in_container/_in_container_utils.sh",
          "scripts/in_container/check_environment.sh||scripts/in_container/check_environment.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/developer_commands.py -> dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: from __future__ import annotations",
          "19: import os",
          "20: import shutil",
          "21: import sys",
          "22: import threading",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import re",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "27: import click",
          "29: from airflow_breeze.commands.ci_image_commands import rebuild_or_pull_ci_image_if_needed",
          "30: from airflow_breeze.commands.main_command import main",
          "31: from airflow_breeze.global_constants import (",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "30: from airflow_breeze.branch_defaults import DEFAULT_AIRFLOW_CONSTRAINTS_BRANCH",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "96: from airflow_breeze.utils.shared_options import get_dry_run, get_verbose, set_forced_answer",
          "97: from airflow_breeze.utils.visuals import ASCIIART, ASCIIART_STYLE, CHEATSHEET, CHEATSHEET_STYLE",
          "107: class TimerThread(threading.Thread):",
          "",
          "[Removed Lines]",
          "99: # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!",
          "100: # Make sure that whatever you add here as an option is also",
          "101: # Added in the \"main\" command in breeze.py. The min command above",
          "102: # Is used for a shorthand of shell and except the extra",
          "103: # Args it should have the same parameters.",
          "104: # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!",
          "",
          "[Added Lines]",
          "102: def _determine_constraint_branch_used(airflow_constraints_reference: str, use_airflow_version: str | None):",
          "103:     \"\"\"",
          "104:     Determine which constraints reference to use.",
          "106:     When use-airflow-version is branch or version, we derive the constraints branch from it, unless",
          "107:     someone specified the constraints branch explicitly.",
          "109:     :param airflow_constraints_reference: the constraint reference specified (or default)",
          "110:     :param use_airflow_version: which airflow version we are installing",
          "111:     :return: the actual constraints reference to use",
          "112:     \"\"\"",
          "113:     if (",
          "114:         use_airflow_version",
          "115:         and airflow_constraints_reference == DEFAULT_AIRFLOW_CONSTRAINTS_BRANCH",
          "116:         and re.match(r\"[0-9]+\\.[0-9]+\\.[0-9]+[0-9a-z\\.]*|main|v[0-9]_.*\", use_airflow_version)",
          "117:     ):",
          "118:         get_console().print(f\"[info]Using constraints {use_airflow_version} - matching airflow version used.\")",
          "119:         return use_airflow_version",
          "120:     return airflow_constraints_reference",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "116:         os.killpg(os.getpgid(0), SIGTERM)",
          "119: @main.command()",
          "120: @option_python",
          "121: @option_platform_single",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "135: # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!",
          "136: # Make sure that whatever you add here as an option is also",
          "137: # Added in the \"main\" command in breeze.py. The min command above",
          "138: # Is used for a shorthand of shell and except the extra",
          "139: # Args it should have the same parameters.",
          "140: # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "181:     if max_time:",
          "182:         TimerThread(max_time=max_time).start()",
          "183:         set_forced_answer(\"yes\")",
          "184:     result = enter_shell(",
          "185:         python=python,",
          "186:         github_repository=github_repository,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "208:     airflow_constraints_reference = _determine_constraint_branch_used(",
          "209:         airflow_constraints_reference, use_airflow_version",
          "210:     )",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "292:         skip_asset_compilation = True",
          "293:     if use_airflow_version is None and not skip_asset_compilation:",
          "294:         run_compile_www_assets(dev=dev_mode, run_in_background=True)",
          "295:     result = enter_shell(",
          "296:         python=python,",
          "297:         github_repository=github_repository,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "322:     airflow_constraints_reference = _determine_constraint_branch_used(",
          "323:         airflow_constraints_reference, use_airflow_version",
          "324:     )",
          "",
          "---------------"
        ],
        "scripts/in_container/_in_container_utils.sh||scripts/in_container/_in_container_utils.sh": [
          "File: scripts/in_container/_in_container_utils.sh -> scripts/in_container/_in_container_utils.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "262:     if [[ ${constraints_reference} == \"none\" ]]; then",
          "263:         pip install \"${airflow_package}${extras}\"",
          "264:     else",
          "266:             --constraint \"https://raw.githubusercontent.com/${CONSTRAINTS_GITHUB_REPOSITORY}/constraints-${version}/constraints-${PYTHON_MAJOR_MINOR_VERSION}.txt\"",
          "267:     fi",
          "268: }",
          "",
          "[Removed Lines]",
          "265:         pip install \"apache-airflow${BRACKETED_AIRFLOW_EXTRAS}==${version}\" \\",
          "",
          "[Added Lines]",
          "265:         local dependency_fix=\"\"",
          "266:         # The pyopenssl is needed to downgrade pyopenssl for older airflow versions when using constraints",
          "267:         # Flask app builder has an optional pyopenssl transitive dependency, that causes import error when",
          "268:         # Pyopenssl is installed in a wrong version for Flask App Builder 4.1 and older. Adding PyOpenSSL",
          "269:         # directly as the dependency, forces downgrading of pyopenssl to the right version. Our constraint",
          "270:         # version has it pinned to the right version, but since it is not directly required, it is not",
          "271:         # downgraded when installing airflow and it is already installed in a newer version",
          "272:         if [[ ${USE_AIRFLOW_VERSION=} != \"\" ]]; then",
          "273:             dependency_fix=\"pyopenssl\"",
          "274:         fi",
          "276:         pip install \"apache-airflow${BRACKETED_AIRFLOW_EXTRAS}==${version}\" ${dependency_fix} \\",
          "",
          "---------------"
        ],
        "scripts/in_container/check_environment.sh||scripts/in_container/check_environment.sh": [
          "File: scripts/in_container/check_environment.sh -> scripts/in_container/check_environment.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "123:                 airflow connections create-default-connections",
          "124:             fi",
          "125:         else",
          "126:             # For Airflow versions that do not support db migrate, we should run airflow db init and",
          "127:             # set the removed AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS",
          "128:             AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS=${LOAD_DEFAULT_CONNECTIONS} airflow db init",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "126:             echo \"${COLOR_YELLOW}Failed to run 'airflow db migrate'.${COLOR_RESET}\"",
          "127:             echo \"${COLOR_BLUE}This could be because you are installing old airflow version${COLOR_RESET}\"",
          "128:             echo \"${COLOR_BLUE}Attempting to run deprecated 'airflow db init' instead.${COLOR_RESET}\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "097d2bed4e372e4f4cc86b14c6b8a47dd8d65902",
      "candidate_info": {
        "commit_hash": "097d2bed4e372e4f4cc86b14c6b8a47dd8d65902",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/097d2bed4e372e4f4cc86b14c6b8a47dd8d65902",
        "files": [
          "airflow/executors/base_executor.py",
          "airflow/executors/debug_executor.py",
          "airflow/executors/sequential_executor.py",
          "airflow/providers/celery/executors/celery_executor.py",
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py",
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py",
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py"
        ],
        "message": "Replace State by TaskInstanceState in Airflow executors (#32627)\n\n* Replace State by TaskInstanceState in Airflow executors\n\n* chaneg state type in change_state method, KubernetesResultsType and KubernetesWatchType to TaskInstanceState\n\n* Fix change_state annotation in CeleryExecutor\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 9556d6d5f611428ac8a3a5891647b720d4498ace)",
        "before_after_code_files": [
          "airflow/executors/base_executor.py||airflow/executors/base_executor.py",
          "airflow/executors/debug_executor.py||airflow/executors/debug_executor.py",
          "airflow/executors/sequential_executor.py||airflow/executors/sequential_executor.py",
          "airflow/providers/celery/executors/celery_executor.py||airflow/providers/celery/executors/celery_executor.py",
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py",
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py",
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/executors/base_executor.py||airflow/executors/base_executor.py": [
          "File: airflow/executors/base_executor.py -> airflow/executors/base_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "32: from airflow.exceptions import RemovedInAirflow3Warning",
          "33: from airflow.stats import Stats",
          "34: from airflow.utils.log.logging_mixin import LoggingMixin",
          "37: PARALLELISM: int = conf.getint(\"core\", \"PARALLELISM\")",
          "",
          "[Removed Lines]",
          "35: from airflow.utils.state import State",
          "",
          "[Added Lines]",
          "35: from airflow.utils.state import TaskInstanceState",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "295:             self.execute_async(key=key, command=command, queue=queue, executor_config=executor_config)",
          "296:             self.running.add(key)",
          "299:         \"\"\"",
          "300:         Changes state of the task.",
          "",
          "[Removed Lines]",
          "298:     def change_state(self, key: TaskInstanceKey, state: str, info=None) -> None:",
          "",
          "[Added Lines]",
          "298:     def change_state(self, key: TaskInstanceKey, state: TaskInstanceState, info=None) -> None:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "317:         :param info: Executor information for the task instance",
          "318:         :param key: Unique key for the task instance",
          "319:         \"\"\"",
          "322:     def success(self, key: TaskInstanceKey, info=None) -> None:",
          "323:         \"\"\"",
          "",
          "[Removed Lines]",
          "320:         self.change_state(key, State.FAILED, info)",
          "",
          "[Added Lines]",
          "320:         self.change_state(key, TaskInstanceState.FAILED, info)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "326:         :param info: Executor information for the task instance",
          "327:         :param key: Unique key for the task instance",
          "328:         \"\"\"",
          "331:     def get_event_buffer(self, dag_ids=None) -> dict[TaskInstanceKey, EventBufferValueType]:",
          "332:         \"\"\"",
          "",
          "[Removed Lines]",
          "329:         self.change_state(key, State.SUCCESS, info)",
          "",
          "[Added Lines]",
          "329:         self.change_state(key, TaskInstanceState.SUCCESS, info)",
          "",
          "---------------"
        ],
        "airflow/executors/debug_executor.py||airflow/executors/debug_executor.py": [
          "File: airflow/executors/debug_executor.py -> airflow/executors/debug_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: from typing import TYPE_CHECKING, Any",
          "31: from airflow.executors.base_executor import BaseExecutor",
          "34: if TYPE_CHECKING:",
          "35:     from airflow.models.taskinstance import TaskInstance",
          "",
          "[Removed Lines]",
          "32: from airflow.utils.state import State",
          "",
          "[Added Lines]",
          "32: from airflow.utils.state import TaskInstanceState",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "68:         while self.tasks_to_run:",
          "69:             ti = self.tasks_to_run.pop(0)",
          "70:             if self.fail_fast and not task_succeeded:",
          "74:                 continue",
          "76:             if self._terminated.is_set():",
          "80:                 continue",
          "82:             task_succeeded = self._run_task(ti)",
          "",
          "[Removed Lines]",
          "71:                 self.log.info(\"Setting %s to %s\", ti.key, State.UPSTREAM_FAILED)",
          "72:                 ti.set_state(State.UPSTREAM_FAILED)",
          "73:                 self.change_state(ti.key, State.UPSTREAM_FAILED)",
          "77:                 self.log.info(\"Executor is terminated! Stopping %s to %s\", ti.key, State.FAILED)",
          "78:                 ti.set_state(State.FAILED)",
          "79:                 self.change_state(ti.key, State.FAILED)",
          "",
          "[Added Lines]",
          "71:                 self.log.info(\"Setting %s to %s\", ti.key, TaskInstanceState.UPSTREAM_FAILED)",
          "72:                 ti.set_state(TaskInstanceState.UPSTREAM_FAILED)",
          "73:                 self.change_state(ti.key, TaskInstanceState.UPSTREAM_FAILED)",
          "77:                 self.log.info(\"Executor is terminated! Stopping %s to %s\", ti.key, TaskInstanceState.FAILED)",
          "78:                 ti.set_state(TaskInstanceState.FAILED)",
          "79:                 self.change_state(ti.key, TaskInstanceState.FAILED)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "87:         try:",
          "88:             params = self.tasks_params.pop(ti.key, {})",
          "89:             ti.run(job_id=ti.job_id, **params)",
          "91:             return True",
          "92:         except Exception as e:",
          "95:             self.log.exception(\"Failed to execute task: %s.\", str(e))",
          "96:             return False",
          "",
          "[Removed Lines]",
          "90:             self.change_state(key, State.SUCCESS)",
          "93:             ti.set_state(State.FAILED)",
          "94:             self.change_state(key, State.FAILED)",
          "",
          "[Added Lines]",
          "90:             self.change_state(key, TaskInstanceState.SUCCESS)",
          "93:             ti.set_state(TaskInstanceState.FAILED)",
          "94:             self.change_state(key, TaskInstanceState.FAILED)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "148:     def end(self) -> None:",
          "149:         \"\"\"Set states of queued tasks to UPSTREAM_FAILED marking them as not executed.\"\"\"",
          "150:         for ti in self.tasks_to_run:",
          "155:     def terminate(self) -> None:",
          "156:         self._terminated.set()",
          "159:         self.log.debug(\"Popping %s from executor task queue.\", key)",
          "160:         self.running.remove(key)",
          "161:         self.event_buffer[key] = state, info",
          "",
          "[Removed Lines]",
          "151:             self.log.info(\"Setting %s to %s\", ti.key, State.UPSTREAM_FAILED)",
          "152:             ti.set_state(State.UPSTREAM_FAILED)",
          "153:             self.change_state(ti.key, State.UPSTREAM_FAILED)",
          "158:     def change_state(self, key: TaskInstanceKey, state: str, info=None) -> None:",
          "",
          "[Added Lines]",
          "151:             self.log.info(\"Setting %s to %s\", ti.key, TaskInstanceState.UPSTREAM_FAILED)",
          "152:             ti.set_state(TaskInstanceState.UPSTREAM_FAILED)",
          "153:             self.change_state(ti.key, TaskInstanceState.UPSTREAM_FAILED)",
          "158:     def change_state(self, key: TaskInstanceKey, state: TaskInstanceState, info=None) -> None:",
          "",
          "---------------"
        ],
        "airflow/executors/sequential_executor.py||airflow/executors/sequential_executor.py": [
          "File: airflow/executors/sequential_executor.py -> airflow/executors/sequential_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "28: from typing import TYPE_CHECKING, Any",
          "30: from airflow.executors.base_executor import BaseExecutor",
          "33: if TYPE_CHECKING:",
          "34:     from airflow.executors.base_executor import CommandType",
          "",
          "[Removed Lines]",
          "31: from airflow.utils.state import State",
          "",
          "[Added Lines]",
          "31: from airflow.utils.state import TaskInstanceState",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "76:             try:",
          "77:                 subprocess.check_call(command, close_fds=True)",
          "79:             except subprocess.CalledProcessError as e:",
          "81:                 self.log.error(\"Failed to execute task %s.\", str(e))",
          "83:         self.commands_to_run = []",
          "",
          "[Removed Lines]",
          "78:                 self.change_state(key, State.SUCCESS)",
          "80:                 self.change_state(key, State.FAILED)",
          "",
          "[Added Lines]",
          "78:                 self.change_state(key, TaskInstanceState.SUCCESS)",
          "80:                 self.change_state(key, TaskInstanceState.FAILED)",
          "",
          "---------------"
        ],
        "airflow/providers/celery/executors/celery_executor.py||airflow/providers/celery/executors/celery_executor.py": [
          "File: airflow/providers/celery/executors/celery_executor.py -> airflow/providers/celery/executors/celery_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "74: from airflow.exceptions import AirflowTaskTimeout",
          "75: from airflow.executors.base_executor import BaseExecutor",
          "76: from airflow.stats import Stats",
          "79: log = logging.getLogger(__name__)",
          "",
          "[Removed Lines]",
          "77: from airflow.utils.state import State",
          "",
          "[Added Lines]",
          "77: from airflow.utils.state import TaskInstanceState",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "299:             self.task_publish_retries.pop(key, None)",
          "300:             if isinstance(result, ExceptionWithTraceback):",
          "301:                 self.log.error(CELERY_SEND_ERR_MSG_HEADER + \": %s\\n%s\\n\", result.exception, result.traceback)",
          "303:             elif result is not None:",
          "304:                 result.backend = cached_celery_backend",
          "305:                 self.running.add(key)",
          "",
          "[Removed Lines]",
          "302:                 self.event_buffer[key] = (State.FAILED, None)",
          "",
          "[Added Lines]",
          "302:                 self.event_buffer[key] = (TaskInstanceState.FAILED, None)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "308:                 # Store the Celery task_id in the event buffer. This will get \"overwritten\" if the task",
          "309:                 # has another event, but that is fine, because the only other events are success/failed at",
          "310:                 # which point we don't need the ID anymore anyway",
          "313:                 # If the task runs _really quickly_ we may already have a result!",
          "314:                 self.update_task_state(key, result.state, getattr(result, \"info\", None))",
          "",
          "[Removed Lines]",
          "311:                 self.event_buffer[key] = (State.QUEUED, result.task_id)",
          "",
          "[Added Lines]",
          "311:                 self.event_buffer[key] = (TaskInstanceState.QUEUED, result.task_id)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "355:             if state:",
          "356:                 self.update_task_state(key, state, info)",
          "359:         super().change_state(key, state, info)",
          "360:         self.tasks.pop(key, None)",
          "",
          "[Removed Lines]",
          "358:     def change_state(self, key: TaskInstanceKey, state: str, info=None) -> None:",
          "",
          "[Added Lines]",
          "358:     def change_state(self, key: TaskInstanceKey, state: TaskInstanceState, info=None) -> None:",
          "",
          "---------------"
        ],
        "airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py": [
          "File: airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py -> airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "78: from airflow.utils.event_scheduler import EventScheduler",
          "79: from airflow.utils.log.logging_mixin import remove_escape_codes",
          "80: from airflow.utils.session import NEW_SESSION, provide_session",
          "83: if TYPE_CHECKING:",
          "84:     from kubernetes import client",
          "",
          "[Removed Lines]",
          "81: from airflow.utils.state import State, TaskInstanceState",
          "",
          "[Added Lines]",
          "81: from airflow.utils.state import TaskInstanceState",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "425:     def _change_state(",
          "426:         self,",
          "427:         key: TaskInstanceKey,",
          "429:         pod_name: str,",
          "430:         namespace: str,",
          "431:         session: Session = NEW_SESSION,",
          "",
          "[Removed Lines]",
          "428:         state: str | None,",
          "",
          "[Added Lines]",
          "428:         state: TaskInstanceState | None,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "433:         if TYPE_CHECKING:",
          "434:             assert self.kube_scheduler",
          "437:             self.event_buffer[key] = state, None",
          "438:             return",
          "440:         if self.kube_config.delete_worker_pods:",
          "442:                 self.kube_scheduler.delete_pod(pod_name=pod_name, namespace=namespace)",
          "443:                 self.log.info(\"Deleted pod: %s in namespace %s\", str(key), str(namespace))",
          "444:         else:",
          "",
          "[Removed Lines]",
          "436:         if state == State.RUNNING:",
          "441:             if state != State.FAILED or self.kube_config.delete_worker_pods_on_failure:",
          "",
          "[Added Lines]",
          "436:         if state == TaskInstanceState.RUNNING:",
          "441:             if state != TaskInstanceState.FAILED or self.kube_config.delete_worker_pods_on_failure:",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "455:             from airflow.models.taskinstance import TaskInstance",
          "457:             state = session.query(TaskInstance.state).filter(TaskInstance.filter_for_tis([key])).scalar()",
          "459:         self.event_buffer[key] = state, None",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "458:             state = TaskInstanceState(state)",
          "",
          "---------------"
        ],
        "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py": [
          "File: airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py -> airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: if TYPE_CHECKING:",
          "22:     from airflow.executors.base_executor import CommandType",
          "23:     from airflow.models.taskinstance import TaskInstanceKey",
          "25:     # TaskInstance key, command, configuration, pod_template_file",
          "26:     KubernetesJobType = Tuple[TaskInstanceKey, CommandType, Any, Optional[str]]",
          "28:     # key, pod state, pod_name, namespace, resource_version",
          "31:     # pod_name, namespace, pod state, annotations, resource_version",
          "34: ALL_NAMESPACES = \"ALL_NAMESPACES\"",
          "35: POD_EXECUTOR_DONE_KEY = \"airflow_executor_done\"",
          "",
          "[Removed Lines]",
          "29:     KubernetesResultsType = Tuple[TaskInstanceKey, Optional[str], str, str, str]",
          "32:     KubernetesWatchType = Tuple[str, str, Optional[str], Dict[str, str], str]",
          "",
          "[Added Lines]",
          "24:     from airflow.utils.state import TaskInstanceState",
          "30:     KubernetesResultsType = Tuple[TaskInstanceKey, Optional[TaskInstanceState], str, str, str]",
          "33:     KubernetesWatchType = Tuple[str, str, Optional[TaskInstanceState], Dict[str, str], str]",
          "",
          "---------------"
        ],
        "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py": [
          "File: airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py -> airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "36: )",
          "37: from airflow.providers.cncf.kubernetes.pod_generator import PodGenerator",
          "38: from airflow.utils.log.logging_mixin import LoggingMixin",
          "41: try:",
          "42:     from airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types import (",
          "",
          "[Removed Lines]",
          "39: from airflow.utils.state import State",
          "",
          "[Added Lines]",
          "39: from airflow.utils.state import TaskInstanceState",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "223:             # since kube server have received request to delete pod set TI state failed",
          "224:             if event[\"type\"] == \"DELETED\" and pod.metadata.deletion_timestamp:",
          "225:                 self.log.info(\"Event: Failed to start pod %s, annotations: %s\", pod_name, annotations_string)",
          "227:             else:",
          "228:                 self.log.debug(\"Event: %s Pending, annotations: %s\", pod_name, annotations_string)",
          "229:         elif status == \"Failed\":",
          "230:             self.log.error(\"Event: %s Failed, annotations: %s\", pod_name, annotations_string)",
          "232:         elif status == \"Succeeded\":",
          "233:             # We get multiple events once the pod hits a terminal state, and we only want to",
          "234:             # send it along to the scheduler once.",
          "",
          "[Removed Lines]",
          "226:                 self.watcher_queue.put((pod_name, namespace, State.FAILED, annotations, resource_version))",
          "231:             self.watcher_queue.put((pod_name, namespace, State.FAILED, annotations, resource_version))",
          "",
          "[Added Lines]",
          "226:                 self.watcher_queue.put(",
          "227:                     (pod_name, namespace, TaskInstanceState.FAILED, annotations, resource_version)",
          "228:                 )",
          "233:             self.watcher_queue.put(",
          "234:                 (pod_name, namespace, TaskInstanceState.FAILED, annotations, resource_version)",
          "235:             )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "256:                     pod_name,",
          "257:                     annotations_string,",
          "258:                 )",
          "260:             else:",
          "261:                 self.log.info(\"Event: %s is Running, annotations: %s\", pod_name, annotations_string)",
          "262:         else:",
          "",
          "[Removed Lines]",
          "259:                 self.watcher_queue.put((pod_name, namespace, State.FAILED, annotations, resource_version))",
          "",
          "[Added Lines]",
          "263:                 self.watcher_queue.put(",
          "264:                     (pod_name, namespace, TaskInstanceState.FAILED, annotations, resource_version)",
          "265:                 )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "448f4849c001626131efe7f0be744ebf02806b27",
      "candidate_info": {
        "commit_hash": "448f4849c001626131efe7f0be744ebf02806b27",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/448f4849c001626131efe7f0be744ebf02806b27",
        "files": [
          "airflow/configuration.py"
        ],
        "message": "Correctly log when there are problems trying to chmod airflow.cfg (#33118)\n\nWithout this change log gives us a big warning/stack trace about\n\"TypeError: not all arguments converted during string formatting\".\n\n(cherry picked from commit af0839200565dd31787632119aec0b6a751b0ca8)",
        "before_after_code_files": [
          "airflow/configuration.py||airflow/configuration.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/configuration.py||airflow/configuration.py": [
          "File: airflow/configuration.py -> airflow/configuration.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2054:     except Exception as e:",
          "2055:         log.warning(",
          "2056:             \"Could not change permissions of config file to be group/other inaccessible. \"",
          "2058:             e,",
          "2059:         )",
          "",
          "[Removed Lines]",
          "2057:             \"Continuing with original permissions:\",",
          "",
          "[Added Lines]",
          "2057:             \"Continuing with original permissions: %s\",",
          "",
          "---------------"
        ]
      }
    }
  ]
}