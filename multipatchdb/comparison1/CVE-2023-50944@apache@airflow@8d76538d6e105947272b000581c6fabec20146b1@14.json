{
  "cve_id": "CVE-2023-50944",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows an authenticated user to access the source code of a DAG to which they don't have access.\u00a0This vulnerability is considered low since it requires an authenticated user to exploit it. Users are recommended to upgrade to version 2.8.1, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "8d76538d6e105947272b000581c6fabec20146b1",
  "patch_info": {
    "commit_hash": "8d76538d6e105947272b000581c6fabec20146b1",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/8d76538d6e105947272b000581c6fabec20146b1",
    "files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ],
    "message": "Check DAG read permission before accessing DAG code (#36257)\n\n(cherry picked from commit 30ea37e0d247ce54c2d25b115e807fdb0074d795)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py||airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_source_endpoint.py -> airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "17: from __future__ import annotations",
      "19: from http import HTTPStatus",
      "21: from flask import Response, current_app, request",
      "22: from itsdangerous import BadSignature, URLSafeSerializer",
      "24: from airflow.api_connexion import security",
      "26: from airflow.api_connexion.schemas.dag_source_schema import dag_source_schema",
      "27: from airflow.auth.managers.models.resource_details import DagAccessEntity",
      "28: from airflow.models.dagcode import DagCode",
      "31: @security.requires_access_dag(\"GET\", DagAccessEntity.CODE)",
      "33:     \"\"\"Get source code using file token.\"\"\"",
      "34:     secret_key = current_app.config[\"SECRET_KEY\"]",
      "35:     auth_s = URLSafeSerializer(secret_key)",
      "36:     try:",
      "37:         path = auth_s.loads(file_token)",
      "39:     except (BadSignature, FileNotFoundError):",
      "40:         raise NotFound(\"Dag source not found\")",
      "",
      "[Removed Lines]",
      "25: from airflow.api_connexion.exceptions import NotFound",
      "32: def get_dag_source(*, file_token: str) -> Response:",
      "38:         dag_source = DagCode.code(path)",
      "",
      "[Added Lines]",
      "20: from typing import TYPE_CHECKING",
      "26: from airflow.api_connexion.exceptions import NotFound, PermissionDenied",
      "28: from airflow.api_connexion.security import get_readable_dags",
      "30: from airflow.models.dag import DagModel",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "34: if TYPE_CHECKING:",
      "35:     from sqlalchemy.orm import Session",
      "39: @provide_session",
      "40: def get_dag_source(*, file_token: str, session: Session = NEW_SESSION) -> Response:",
      "46:         dag_ids = session.query(DagModel.dag_id).filter(DagModel.fileloc == path).all()",
      "47:         readable_dags = get_readable_dags()",
      "48:         # Check if user has read access to all the DAGs defined in the file",
      "49:         if any(dag_id[0] not in readable_dags for dag_id in dag_ids):",
      "50:             raise PermissionDenied()",
      "51:         dag_source = DagCode.code(path, session=session)",
      "",
      "---------------"
    ],
    "airflow/models/dagcode.py||airflow/models/dagcode.py": [
      "File: airflow/models/dagcode.py -> airflow/models/dagcode.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "177:         return cls.code(fileloc)",
      "179:     @classmethod",
      "181:         \"\"\"Return source code for this DagCode object.",
      "183:         :return: source code as string",
      "184:         \"\"\"",
      "187:     @staticmethod",
      "188:     def _get_code_from_file(fileloc):",
      "",
      "[Removed Lines]",
      "180:     def code(cls, fileloc) -> str:",
      "185:         return cls._get_code_from_db(fileloc)",
      "",
      "[Added Lines]",
      "180:     @provide_session",
      "181:     def code(cls, fileloc, session: Session = NEW_SESSION) -> str:",
      "186:         return cls._get_code_from_db(fileloc, session)",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_source_endpoint.py -> tests/api_connexion/endpoints/test_dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35: ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))",
      "36: EXAMPLE_DAG_FILE = os.path.join(\"airflow\", \"example_dags\", \"example_bash_operator.py\")",
      "39: @pytest.fixture(scope=\"module\")",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "37: EXAMPLE_DAG_ID = \"example_bash_operator\"",
      "38: TEST_DAG_ID = \"latest_only\"",
      "39: NOT_READABLE_DAG_ID = \"latest_only_with_trigger\"",
      "40: TEST_MULTIPLE_DAGS_ID = \"dataset_produces_1\"",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "45:         role_name=\"Test\",",
      "46:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_CODE)],  # type: ignore",
      "47:     )",
      "48:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "50:     yield app",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "52:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "53:         TEST_DAG_ID,",
      "54:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "55:     )",
      "56:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "57:         EXAMPLE_DAG_ID,",
      "58:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "59:     )",
      "60:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "61:         TEST_MULTIPLE_DAGS_ID,",
      "62:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "63:     )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "80:     def test_should_respond_200_text(self, url_safe_serializer):",
      "81:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "82:         dagbag.sync_to_db()",
      "87:         response = self.client.get(",
      "88:             url, headers={\"Accept\": \"text/plain\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "89:         )",
      "",
      "[Removed Lines]",
      "83:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "84:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "86:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "99:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "100:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "102:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "95:     def test_should_respond_200_json(self, url_safe_serializer):",
      "96:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "97:         dagbag.sync_to_db()",
      "102:         response = self.client.get(",
      "103:             url, headers={\"Accept\": \"application/json\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "104:         )",
      "",
      "[Removed Lines]",
      "98:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "99:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "101:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "114:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "115:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "117:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "110:     def test_should_respond_406(self, url_safe_serializer):",
      "111:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "112:         dagbag.sync_to_db()",
      "116:         response = self.client.get(",
      "117:             url, headers={\"Accept\": \"image/webp\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "118:         )",
      "",
      "[Removed Lines]",
      "113:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "115:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "129:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "131:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "151:             environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"},",
      "152:         )",
      "153:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "171:     def test_should_respond_403_not_readable(self, url_safe_serializer):",
      "172:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "173:         dagbag.sync_to_db()",
      "174:         dag: DAG = dagbag.dags[NOT_READABLE_DAG_ID]",
      "176:         response = self.client.get(",
      "177:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "178:             headers={\"Accept\": \"text/plain\"},",
      "179:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "180:         )",
      "181:         read_dag = self.client.get(",
      "182:             f\"/api/v1/dags/{NOT_READABLE_DAG_ID}\",",
      "183:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "184:         )",
      "185:         assert response.status_code == 403",
      "186:         assert read_dag.status_code == 403",
      "188:     def test_should_respond_403_some_dags_not_readable_in_the_file(self, url_safe_serializer):",
      "189:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "190:         dagbag.sync_to_db()",
      "191:         dag: DAG = dagbag.dags[TEST_MULTIPLE_DAGS_ID]",
      "193:         response = self.client.get(",
      "194:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "195:             headers={\"Accept\": \"text/plain\"},",
      "196:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "197:         )",
      "199:         read_dag = self.client.get(",
      "200:             f\"/api/v1/dags/{TEST_MULTIPLE_DAGS_ID}\",",
      "201:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "202:         )",
      "203:         assert response.status_code == 403",
      "204:         assert read_dag.status_code == 200",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "ed1e49a1196d8bc68fc9f48f9ad5cc5b30cf4921",
      "candidate_info": {
        "commit_hash": "ed1e49a1196d8bc68fc9f48f9ad5cc5b30cf4921",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ed1e49a1196d8bc68fc9f48f9ad5cc5b30cf4921",
        "files": [
          ".github/workflows/ci.yml",
          ".pre-commit-config.yaml",
          "STATIC_CODE_CHECKS.rst",
          "dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "dev/breeze/tests/test_selective_checks.py",
          "images/breeze/output_static-checks.svg",
          "images/breeze/output_static-checks.txt",
          "scripts/ci/pre_commit/pre_commit_mypy.py",
          "scripts/ci/pre_commit/pre_commit_mypy_folder.py"
        ],
        "message": "Simplify how mypy \"folder\" checks are run (#36760)\n\nThe #36638 change introduced \"full package\" checks - where in\ncase of CI we run mypy checks separately from regular static checks,\nfor the whole folders.\n\nHowever it's been a little convoluted on how the checks were run,\nwith a separate env variable. Instead we can actually have multiple\nmypy-* checks (same as we have for local pre-commit runs) as mypy\nallows to have multiple checks with the same name in various stages.\n\nThis change simplifies the setup a bit:\n\n* we name the checks \"folder\" checks because this is what they are\n* we name the check names consistent (\"airflow\", \"providers\", \"docs\",\n  \"dev\") with mypy-folders output\n* we have separate small script to run the folder checks\n* we map \"providers\" into \"airflow/providers\" in the pre-commit\n\n(cherry picked from commit a912948b51cc50ae6c92496e11abebbba0c647e5)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/pre_commit_ids.py||dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "dev/breeze/tests/test_selective_checks.py||dev/breeze/tests/test_selective_checks.py",
          "scripts/ci/pre_commit/pre_commit_mypy.py||scripts/ci/pre_commit/pre_commit_mypy.py",
          "scripts/ci/pre_commit/pre_commit_mypy_folder.py||scripts/ci/pre_commit/pre_commit_mypy_folder.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/pre_commit_ids.py||dev/breeze/src/airflow_breeze/pre_commit_ids.py": [
          "File: dev/breeze/src/airflow_breeze/pre_commit_ids.py -> dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "103:     \"lint-markdown\",",
          "104:     \"lint-openapi\",",
          "105:     \"mixed-line-ending\",",
          "108:     \"mypy-dev\",",
          "109:     \"mypy-docs\",",
          "110:     \"mypy-providers\",",
          "",
          "[Removed Lines]",
          "106:     \"mypy\",",
          "107:     \"mypy-core\",",
          "",
          "[Added Lines]",
          "106:     \"mypy-airflow\",",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py": [
          "File: dev/breeze/src/airflow_breeze/utils/selective_checks.py -> dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "582:             return False",
          "584:     @cached_property",
          "587:         if (",
          "588:             self._matching_files(",
          "589:                 FileGroupForCi.ALL_AIRFLOW_PYTHON_FILES, CI_FILE_GROUP_MATCHES, CI_FILE_GROUP_EXCLUDES",
          "590:             )",
          "591:             or self.full_tests_needed",
          "592:         ):",
          "594:         if (",
          "595:             self._matching_files(",
          "596:                 FileGroupForCi.ALL_PROVIDERS_PYTHON_FILES, CI_FILE_GROUP_MATCHES, CI_FILE_GROUP_EXCLUDES",
          "597:             )",
          "598:             or self._are_all_providers_affected()",
          "599:         ) and self._default_branch == \"main\":",
          "601:         if (",
          "602:             self._matching_files(",
          "603:                 FileGroupForCi.ALL_DOCS_PYTHON_FILES, CI_FILE_GROUP_MATCHES, CI_FILE_GROUP_EXCLUDES",
          "604:             )",
          "605:             or self.full_tests_needed",
          "606:         ):",
          "608:         if (",
          "609:             self._matching_files(",
          "610:                 FileGroupForCi.ALL_DEV_PYTHON_FILES, CI_FILE_GROUP_MATCHES, CI_FILE_GROUP_EXCLUDES",
          "611:             )",
          "612:             or self.full_tests_needed",
          "613:         ):",
          "617:     @cached_property",
          "618:     def needs_mypy(self) -> bool:",
          "621:     @cached_property",
          "622:     def needs_python_scans(self) -> bool:",
          "",
          "[Removed Lines]",
          "585:     def mypy_packages(self) -> list[str]:",
          "586:         packages_to_run: list[str] = []",
          "593:             packages_to_run.append(\"airflow\")",
          "600:             packages_to_run.append(\"airflow/providers\")",
          "607:             packages_to_run.append(\"docs\")",
          "614:             packages_to_run.append(\"dev\")",
          "615:         return packages_to_run",
          "619:         return self.mypy_packages != []",
          "",
          "[Added Lines]",
          "585:     def mypy_folders(self) -> list[str]:",
          "586:         folders_to_check: list[str] = []",
          "593:             folders_to_check.append(\"airflow\")",
          "600:             folders_to_check.append(\"providers\")",
          "607:             folders_to_check.append(\"docs\")",
          "614:             folders_to_check.append(\"dev\")",
          "615:         return folders_to_check",
          "619:         return self.mypy_folders != []",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_selective_checks.py||dev/breeze/tests/test_selective_checks.py": [
          "File: dev/breeze/tests/test_selective_checks.py -> dev/breeze/tests/test_selective_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "112:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "113:                     \"parallel-test-types-list-as-string\": None,",
          "114:                     \"needs-mypy\": \"false\",",
          "116:                 },",
          "117:                 id=\"No tests on simple change\",",
          "118:             )",
          "",
          "[Removed Lines]",
          "115:                     \"mypy-packages\": \"[]\",",
          "",
          "[Added Lines]",
          "115:                     \"mypy-folders\": \"[]\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "137:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "138:                     \"parallel-test-types-list-as-string\": \"API Always\",",
          "139:                     \"needs-mypy\": \"true\",",
          "141:                 },",
          "142:                 id=\"Only API tests and DOCS should run\",",
          "143:             )",
          "",
          "[Removed Lines]",
          "140:                     \"mypy-packages\": \"['airflow']\",",
          "",
          "[Added Lines]",
          "140:                     \"mypy-folders\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "162:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "163:                     \"parallel-test-types-list-as-string\": \"Always Operators\",",
          "164:                     \"needs-mypy\": \"true\",",
          "166:                 },",
          "167:                 id=\"Only Operator tests and DOCS should run\",",
          "168:             )",
          "",
          "[Removed Lines]",
          "165:                     \"mypy-packages\": \"['airflow']\",",
          "",
          "[Added Lines]",
          "165:                     \"mypy-folders\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "188:                     \"parallel-test-types-list-as-string\": \"Always BranchExternalPython BranchPythonVenv \"",
          "189:                     \"ExternalPython Operators PythonVenv\",",
          "190:                     \"needs-mypy\": \"true\",",
          "192:                 },",
          "193:                 id=\"Only Python tests\",",
          "194:             )",
          "",
          "[Removed Lines]",
          "191:                     \"mypy-packages\": \"['airflow']\",",
          "",
          "[Added Lines]",
          "191:                     \"mypy-folders\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "213:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "214:                     \"parallel-test-types-list-as-string\": \"Always Serialization\",",
          "215:                     \"needs-mypy\": \"true\",",
          "217:                 },",
          "218:                 id=\"Only Serialization tests\",",
          "219:             )",
          "",
          "[Removed Lines]",
          "216:                     \"mypy-packages\": \"['airflow']\",",
          "",
          "[Added Lines]",
          "216:                     \"mypy-folders\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "243:                     \"parallel-test-types-list-as-string\": \"API Always Providers[amazon] \"",
          "244:                     \"Providers[common.sql,openlineage,pgvector,postgres] Providers[google]\",",
          "245:                     \"needs-mypy\": \"true\",",
          "247:                 },",
          "248:                 id=\"API and providers tests and docs should run\",",
          "249:             )",
          "",
          "[Removed Lines]",
          "246:                     \"mypy-packages\": \"['airflow', 'airflow/providers']\",",
          "",
          "[Added Lines]",
          "246:                     \"mypy-folders\": \"['airflow', 'providers']\",",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "269:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "270:                     \"parallel-test-types-list-as-string\": \"Always Providers[apache.beam] Providers[google]\",",
          "271:                     \"needs-mypy\": \"true\",",
          "273:                 },",
          "274:                 id=\"Selected Providers and docs should run\",",
          "275:             )",
          "",
          "[Removed Lines]",
          "272:                     \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "[Added Lines]",
          "272:                     \"mypy-folders\": \"['providers']\",",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "295:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "296:                     \"parallel-test-types-list-as-string\": None,",
          "297:                     \"needs-mypy\": \"false\",",
          "299:                 },",
          "300:                 id=\"Only docs builds should run - no tests needed\",",
          "301:             )",
          "",
          "[Removed Lines]",
          "298:                     \"mypy-packages\": \"[]\",",
          "",
          "[Added Lines]",
          "298:                     \"mypy-folders\": \"[]\",",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "325:                     \"parallel-test-types-list-as-string\": \"Always Providers[amazon] \"",
          "326:                     \"Providers[common.sql,openlineage,pgvector,postgres] Providers[google]\",",
          "327:                     \"needs-mypy\": \"true\",",
          "329:                 },",
          "330:                 id=\"Helm tests, providers (both upstream and downstream),\"",
          "331:                 \"kubernetes tests and docs should run\",",
          "",
          "[Removed Lines]",
          "328:                     \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "[Added Lines]",
          "328:                     \"mypy-folders\": \"['providers']\",",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "357:                     \"parallel-test-types-list-as-string\": \"Always \"",
          "358:                     \"Providers[airbyte,apache.livy,dbt.cloud,dingding,discord,http] Providers[amazon]\",",
          "359:                     \"needs-mypy\": \"true\",",
          "361:                 },",
          "362:                 id=\"Helm tests, http and all relevant providers, kubernetes tests and \"",
          "363:                 \"docs should run even if unimportant files were added\",",
          "",
          "[Removed Lines]",
          "360:                     \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "[Added Lines]",
          "360:                     \"mypy-folders\": \"['providers']\",",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "387:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "388:                     \"parallel-test-types-list-as-string\": \"Always Providers[airbyte,http]\",",
          "389:                     \"needs-mypy\": \"true\",",
          "391:                 },",
          "392:                 id=\"Helm tests, airbyte/http providers, kubernetes tests and \"",
          "393:                 \"docs should run even if unimportant files were added\",",
          "",
          "[Removed Lines]",
          "390:                     \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "[Added Lines]",
          "390:                     \"mypy-folders\": \"['providers']\",",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "418:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "419:                     \"parallel-test-types-list-as-string\": \"Always\",",
          "420:                     \"needs-mypy\": \"true\",",
          "422:                 },",
          "423:                 id=\"Docs should run even if unimportant files were added and prod image \"",
          "424:                 \"should be build for chart changes\",",
          "",
          "[Removed Lines]",
          "421:                     \"mypy-packages\": \"['airflow']\",",
          "",
          "[Added Lines]",
          "421:                     \"mypy-folders\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "444:                     \"upgrade-to-newer-dependencies\": \"true\",",
          "445:                     \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "446:                     \"needs-mypy\": \"true\",",
          "448:                 },",
          "449:                 id=\"Everything should run - including all providers and upgrading to \"",
          "450:                 \"newer requirements as pyproject.toml changed and all Python versions\",",
          "",
          "[Removed Lines]",
          "447:                     \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "447:                     \"mypy-folders\": \"['airflow', 'providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "470:                     \"upgrade-to-newer-dependencies\": \"true\",",
          "471:                     \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "472:                     \"needs-mypy\": \"true\",",
          "474:                 },",
          "475:                 id=\"Everything should run and upgrading to newer requirements as dependencies change\",",
          "476:             )",
          "",
          "[Removed Lines]",
          "473:                     \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "473:                     \"mypy-folders\": \"['airflow', 'providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "498:                 \"Providers[apache.hive,cncf.kubernetes,common.sql,exasol,ftp,http,\"",
          "499:                 \"imap,microsoft.azure,mongo,mysql,openlineage,postgres,salesforce,ssh] Providers[google]\",",
          "500:                 \"needs-mypy\": \"true\",",
          "502:             },",
          "503:             id=\"Providers tests run including amazon tests if amazon provider files changed\",",
          "504:         ),",
          "",
          "[Removed Lines]",
          "501:                 \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "[Added Lines]",
          "501:                 \"mypy-folders\": \"['providers']\",",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "521:                 \"upgrade-to-newer-dependencies\": \"false\",",
          "522:                 \"parallel-test-types-list-as-string\": \"Always Providers[airbyte,http]\",",
          "523:                 \"needs-mypy\": \"true\",",
          "525:             },",
          "526:             id=\"Providers tests run without amazon tests if no amazon file changed\",",
          "527:         ),",
          "",
          "[Removed Lines]",
          "524:                 \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "[Added Lines]",
          "524:                 \"mypy-folders\": \"['providers']\",",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "548:                 \"Providers[apache.hive,cncf.kubernetes,common.sql,exasol,ftp,http,\"",
          "549:                 \"imap,microsoft.azure,mongo,mysql,openlineage,postgres,salesforce,ssh] Providers[google]\",",
          "550:                 \"needs-mypy\": \"true\",",
          "552:             },",
          "553:             id=\"Providers tests run including amazon tests if amazon provider files changed\",",
          "554:         ),",
          "",
          "[Removed Lines]",
          "551:                 \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "[Added Lines]",
          "551:                 \"mypy-folders\": \"['providers']\",",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "575:                 \"upgrade-to-newer-dependencies\": \"false\",",
          "576:                 \"parallel-test-types-list-as-string\": \"Always Providers[common.io]\",",
          "577:                 \"needs-mypy\": \"true\",",
          "579:             },",
          "580:             id=\"Only Always and Common.IO tests should run when only common.io and tests/always changed\",",
          "581:         ),",
          "",
          "[Removed Lines]",
          "578:                 \"mypy-packages\": \"['airflow', 'airflow/providers']\",",
          "",
          "[Added Lines]",
          "578:                 \"mypy-folders\": \"['airflow', 'providers']\",",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "619:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "620:                     \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "621:                     \"needs-mypy\": \"true\",",
          "623:                 },",
          "624:                 id=\"Everything should run including all providers when full tests are needed\",",
          "625:             )",
          "",
          "[Removed Lines]",
          "622:                     \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "622:                     \"mypy-folders\": \"['airflow', 'providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "648:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "649:                     \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "650:                     \"needs-mypy\": \"true\",",
          "652:                 },",
          "653:                 id=\"Everything should run including full providers when full \"",
          "654:                 \"tests are needed even with different label set as well\",",
          "",
          "[Removed Lines]",
          "651:                     \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "651:                     \"mypy-folders\": \"['airflow', 'providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "675:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "676:                     \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "677:                     \"needs-mypy\": \"true\",",
          "679:                 },",
          "680:                 id=\"Everything should run including full providers when\"",
          "681:                 \"full tests are needed even if no files are changed\",",
          "",
          "[Removed Lines]",
          "678:                     \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "678:                     \"mypy-folders\": \"['airflow', 'providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "705:                     \"BranchPythonVenv CLI Core ExternalPython Operators Other PlainAsserts \"",
          "706:                     \"PythonVenv Serialization WWW\",",
          "707:                     \"needs-mypy\": \"true\",",
          "709:                 },",
          "710:                 id=\"Everything should run except Providers and lint pre-commit \"",
          "711:                 \"when full tests are needed for non-main branch\",",
          "",
          "[Removed Lines]",
          "708:                     \"mypy-packages\": \"['airflow', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "708:                     \"mypy-folders\": \"['airflow', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "751:                 \"skip-provider-tests\": \"true\",",
          "752:                 \"parallel-test-types-list-as-string\": None,",
          "753:                 \"needs-mypy\": \"false\",",
          "755:             },",
          "756:             id=\"Nothing should run if only non-important files changed\",",
          "757:         ),",
          "",
          "[Removed Lines]",
          "754:                 \"mypy-packages\": \"[]\",",
          "",
          "[Added Lines]",
          "754:                 \"mypy-folders\": \"[]\",",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "781:                 \"skip-provider-tests\": \"true\",",
          "782:                 \"parallel-test-types-list-as-string\": \"Always\",",
          "783:                 \"needs-mypy\": \"false\",",
          "785:             },",
          "786:             id=\"No Helm tests, No providers no lint charts, should run if \"",
          "787:             \"only chart/providers changed in non-main but PROD image should be built\",",
          "",
          "[Removed Lines]",
          "784:                 \"mypy-packages\": \"[]\",",
          "",
          "[Added Lines]",
          "784:                 \"mypy-folders\": \"[]\",",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "812:                 \"skip-provider-tests\": \"true\",",
          "813:                 \"parallel-test-types-list-as-string\": \"Always CLI\",",
          "814:                 \"needs-mypy\": \"true\",",
          "816:             },",
          "817:             id=\"Only CLI tests and Kubernetes tests should run if cli/chart files changed in non-main branch\",",
          "818:         ),",
          "",
          "[Removed Lines]",
          "815:                 \"mypy-packages\": \"['airflow']\",",
          "",
          "[Added Lines]",
          "815:                 \"mypy-folders\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 26 ---",
          "[Context before]",
          "839:                 \"parallel-test-types-list-as-string\": \"API Always BranchExternalPython BranchPythonVenv \"",
          "840:                 \"CLI Core ExternalPython Operators Other PlainAsserts PythonVenv Serialization WWW\",",
          "841:                 \"needs-mypy\": \"true\",",
          "843:             },",
          "844:             id=\"All tests except Providers and helm lint pre-commit \"",
          "845:             \"should run if core file changed in non-main branch\",",
          "",
          "[Removed Lines]",
          "842:                 \"mypy-packages\": \"['airflow']\",",
          "",
          "[Added Lines]",
          "842:                 \"mypy-folders\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 27 ---",
          "[Context before]",
          "880:                 \"skip-provider-tests\": \"true\",",
          "881:                 \"parallel-test-types-list-as-string\": None,",
          "882:                 \"needs-mypy\": \"false\",",
          "884:             },",
          "885:             id=\"Nothing should run if only non-important files changed\",",
          "886:         ),",
          "",
          "[Removed Lines]",
          "883:                 \"mypy-packages\": \"[]\",",
          "",
          "[Added Lines]",
          "883:                 \"mypy-folders\": \"[]\",",
          "",
          "---------------",
          "--- Hunk 28 ---",
          "[Context before]",
          "901:                 \"skip-provider-tests\": \"true\",",
          "902:                 \"parallel-test-types-list-as-string\": \"Always\",",
          "903:                 \"needs-mypy\": \"true\",",
          "905:             },",
          "906:             id=\"Only Always and docs build should run if only system tests changed\",",
          "907:         ),",
          "",
          "[Removed Lines]",
          "904:                 \"mypy-packages\": \"['airflow']\",",
          "",
          "[Added Lines]",
          "904:                 \"mypy-folders\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 29 ---",
          "[Context before]",
          "936:                 \"microsoft.azure,microsoft.mssql,mysql,openlineage,oracle,postgres,presto,salesforce,\"",
          "937:                 \"samba,sftp,ssh,trino] Providers[google]\",",
          "938:                 \"needs-mypy\": \"true\",",
          "940:             },",
          "941:             id=\"CLI tests and Google-related provider tests should run if cli/chart files changed but \"",
          "942:             \"prod image should be build too and k8s tests too\",",
          "",
          "[Removed Lines]",
          "939:                 \"mypy-packages\": \"['airflow', 'airflow/providers']\",",
          "",
          "[Added Lines]",
          "939:                 \"mypy-folders\": \"['airflow', 'providers']\",",
          "",
          "---------------",
          "--- Hunk 30 ---",
          "[Context before]",
          "964:                 \"skip-provider-tests\": \"true\",",
          "965:                 \"parallel-test-types-list-as-string\": \"API Always CLI Operators WWW\",",
          "966:                 \"needs-mypy\": \"true\",",
          "968:             },",
          "969:             id=\"No providers tests should run if only CLI/API/Operators/WWW file changed\",",
          "970:         ),",
          "",
          "[Removed Lines]",
          "967:                 \"mypy-packages\": \"['airflow']\",",
          "",
          "[Added Lines]",
          "967:                 \"mypy-folders\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 31 ---",
          "[Context before]",
          "986:                 \"skip-provider-tests\": \"false\",",
          "987:                 \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "988:                 \"needs-mypy\": \"true\",",
          "990:             },",
          "991:             id=\"Tests for all providers should run if model file changed\",",
          "992:         ),",
          "",
          "[Removed Lines]",
          "989:                 \"mypy-packages\": \"['airflow', 'airflow/providers']\",",
          "",
          "[Added Lines]",
          "989:                 \"mypy-folders\": \"['airflow', 'providers']\",",
          "",
          "---------------",
          "--- Hunk 32 ---",
          "[Context before]",
          "1008:                 \"skip-provider-tests\": \"false\",",
          "1009:                 \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "1010:                 \"needs-mypy\": \"true\",",
          "1012:             },",
          "1013:             id=\"Tests for all providers should run if any other than API/WWW/CLI/Operators file changed.\",",
          "1014:         ),",
          "",
          "[Removed Lines]",
          "1011:                 \"mypy-packages\": \"['airflow', 'airflow/providers']\",",
          "",
          "[Added Lines]",
          "1011:                 \"mypy-folders\": \"['airflow', 'providers']\",",
          "",
          "---------------",
          "--- Hunk 33 ---",
          "[Context before]",
          "1049:                 \"upgrade-to-newer-dependencies\": \"true\",",
          "1050:                 \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "1051:                 \"needs-mypy\": \"true\",",
          "1053:             },",
          "1054:             id=\"All tests run on push even if unimportant file changed\",",
          "1055:         ),",
          "",
          "[Removed Lines]",
          "1052:                 \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "1052:                 \"mypy-folders\": \"['airflow', 'providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 34 ---",
          "[Context before]",
          "1072:                 \"parallel-test-types-list-as-string\": \"API Always BranchExternalPython BranchPythonVenv \"",
          "1073:                 \"CLI Core ExternalPython Operators Other PlainAsserts PythonVenv Serialization WWW\",",
          "1074:                 \"needs-mypy\": \"true\",",
          "1076:             },",
          "1077:             id=\"All tests except Providers and Helm run on push\"",
          "1078:             \" even if unimportant file changed in non-main branch\",",
          "",
          "[Removed Lines]",
          "1075:                 \"mypy-packages\": \"['airflow', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "1075:                 \"mypy-folders\": \"['airflow', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 35 ---",
          "[Context before]",
          "1095:                 \"upgrade-to-newer-dependencies\": \"true\",",
          "1096:                 \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "1097:                 \"needs-mypy\": \"true\",",
          "1099:             },",
          "1100:             id=\"All tests run on push if core file changed\",",
          "1101:         ),",
          "",
          "[Removed Lines]",
          "1098:                 \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "1098:                 \"mypy-folders\": \"['airflow', 'providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 36 ---",
          "[Context before]",
          "1150:             else \"false\",",
          "1151:             \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "1152:             \"needs-mypy\": \"true\",",
          "1154:         },",
          "1155:         str(stderr),",
          "1156:     )",
          "",
          "[Removed Lines]",
          "1153:             \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "1153:             \"mypy-folders\": \"['airflow', 'providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 37 ---",
          "[Context before]",
          "1640:             (\"README.md\",),",
          "1641:             {",
          "1642:                 \"needs-mypy\": \"false\",",
          "1644:             },",
          "1645:             \"main\",",
          "1646:             (),",
          "",
          "[Removed Lines]",
          "1643:                 \"mypy-packages\": \"[]\",",
          "",
          "[Added Lines]",
          "1643:                 \"mypy-folders\": \"[]\",",
          "",
          "---------------",
          "--- Hunk 38 ---",
          "[Context before]",
          "1650:             (\"airflow/cli/file.py\",),",
          "1651:             {",
          "1652:                 \"needs-mypy\": \"true\",",
          "1654:             },",
          "1655:             \"main\",",
          "1656:             (),",
          "",
          "[Removed Lines]",
          "1653:                 \"mypy-packages\": \"['airflow']\",",
          "",
          "[Added Lines]",
          "1653:                 \"mypy-folders\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 39 ---",
          "[Context before]",
          "1660:             (\"airflow/models/file.py\",),",
          "1661:             {",
          "1662:                 \"needs-mypy\": \"true\",",
          "1664:             },",
          "1665:             \"main\",",
          "1666:             (),",
          "",
          "[Removed Lines]",
          "1663:                 \"mypy-packages\": \"['airflow', 'airflow/providers']\",",
          "",
          "[Added Lines]",
          "1663:                 \"mypy-folders\": \"['airflow', 'providers']\",",
          "",
          "---------------",
          "--- Hunk 40 ---",
          "[Context before]",
          "1670:             (\"airflow/providers/a_file.py\",),",
          "1671:             {",
          "1672:                 \"needs-mypy\": \"true\",",
          "1674:             },",
          "1675:             \"main\",",
          "1676:             (),",
          "",
          "[Removed Lines]",
          "1673:                 \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "[Added Lines]",
          "1673:                 \"mypy-folders\": \"['providers']\",",
          "",
          "---------------",
          "--- Hunk 41 ---",
          "[Context before]",
          "1680:             (\"docs/a_file.py\",),",
          "1681:             {",
          "1682:                 \"needs-mypy\": \"true\",",
          "1684:             },",
          "1685:             \"main\",",
          "1686:             (),",
          "",
          "[Removed Lines]",
          "1683:                 \"mypy-packages\": \"['docs']\",",
          "",
          "[Added Lines]",
          "1683:                 \"mypy-folders\": \"['docs']\",",
          "",
          "---------------",
          "--- Hunk 42 ---",
          "[Context before]",
          "1690:             (\"dev/a_package/a_file.py\",),",
          "1691:             {",
          "1692:                 \"needs-mypy\": \"true\",",
          "1694:             },",
          "1695:             \"main\",",
          "1696:             (),",
          "",
          "[Removed Lines]",
          "1693:                 \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "1693:                 \"mypy-folders\": \"['airflow', 'providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 43 ---",
          "[Context before]",
          "1700:             (\"readme.md\",),",
          "1701:             {",
          "1702:                 \"needs-mypy\": \"true\",",
          "1704:             },",
          "1705:             \"main\",",
          "1706:             (\"full tests needed\",),",
          "",
          "[Removed Lines]",
          "1703:                 \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "1703:                 \"mypy-folders\": \"['airflow', 'providers', 'docs', 'dev']\",",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_mypy.py||scripts/ci/pre_commit/pre_commit_mypy.py": [
          "File: scripts/ci/pre_commit/pre_commit_mypy.py -> scripts/ci/pre_commit/pre_commit_mypy.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import os",
          "22: import sys",
          "23: from pathlib import Path",
          "",
          "[Removed Lines]",
          "21: import shlex",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "34: initialize_breeze_precommit(__name__, __file__)",
          "36: files_to_test = pre_process_files(sys.argv[1:])",
          "40: if files_to_test == [\"--namespace-packages\"] or files_to_test == []:",
          "41:     print(\"No files to tests. Quitting\")",
          "42:     sys.exit(0)",
          "",
          "[Removed Lines]",
          "37: mypy_packages = os.environ.get(\"MYPY_PACKAGES\")",
          "38: if mypy_packages:",
          "39:     files_to_test += shlex.split(mypy_packages)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "57: )",
          "58: ci_environment = os.environ.get(\"CI\")",
          "59: if res.returncode != 0:",
          "68:     upgrading = os.environ.get(\"UPGRADE_TO_NEWER_DEPENDENCIES\", \"false\") != \"false\"",
          "69:     if upgrading:",
          "70:         console.print(",
          "",
          "[Removed Lines]",
          "60:     if mypy_packages and ci_environment:",
          "61:         console.print(",
          "62:             \"[yellow]You are running mypy with the packages selected. If you want to\"",
          "63:             \"reproduce it locally, you need to run the following command:\\n\"",
          "64:         )",
          "65:         console.print(",
          "66:             f'MYPY_PACKAGES=\"{mypy_packages}\" pre-commit run --hook-stage manual mypy --all-files\\n'",
          "67:         )",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_mypy_folder.py||scripts/ci/pre_commit/pre_commit_mypy_folder.py": [
          "File: scripts/ci/pre_commit/pre_commit_mypy_folder.py -> scripts/ci/pre_commit/pre_commit_mypy_folder.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: import os",
          "21: import sys",
          "22: from pathlib import Path",
          "24: sys.path.insert(0, str(Path(__file__).parent.resolve()))",
          "26: from common_precommit_utils import (",
          "27:     console,",
          "28:     initialize_breeze_precommit,",
          "29:     run_command_via_breeze_shell,",
          "30: )",
          "32: initialize_breeze_precommit(__name__, __file__)",
          "34: ALLOWED_FOLDERS = [\"airflow\", \"airflow/providers\", \"dev\", \"docs\"]",
          "36: if len(sys.argv) < 2:",
          "37:     console.print(f\"[yellow]You need to specify the folder to test as parameter: {ALLOWED_FOLDERS}\\n\")",
          "38:     sys.exit(1)",
          "40: mypy_folder = sys.argv[1]",
          "41: if mypy_folder not in ALLOWED_FOLDERS:",
          "42:     console.print(f\"[yellow]Wrong folder {mypy_folder}. It should be one of those: {ALLOWED_FOLDERS}\\n\")",
          "43:     sys.exit(1)",
          "45: arguments = [mypy_folder]",
          "46: if mypy_folder == \"airflow/providers\":",
          "47:     arguments.append(\"--namespace-packages\")",
          "49: res = run_command_via_breeze_shell(",
          "50:     [",
          "51:         \"/opt/airflow/scripts/in_container/run_mypy.sh\",",
          "53:     ],",
          "54:     warn_image_upgrade_needed=True,",
          "55:     extra_env={",
          "56:         \"INCLUDE_MYPY_VOLUME\": \"true\",",
          "57:         # Need to mount local sources when running it - to not have to rebuild the image",
          "58:         # and to let CI work on it when running on PRs from forks - because mypy-dev uses files",
          "59:         # that are not available at the time when image is built in CI",
          "60:         \"MOUNT_SOURCES\": \"selected\",",
          "61:     },",
          "62: )",
          "63: ci_environment = os.environ.get(\"CI\")",
          "64: if res.returncode != 0:",
          "65:     if ci_environment:",
          "66:         console.print(",
          "67:             \"[yellow]You are running mypy with the folders selected. If you want to\"",
          "68:             \"reproduce it locally, you need to run the following command:\\n\"",
          "69:         )",
          "70:         console.print(\"pre-commit run --hook-stage manual mypy-<folder> --all-files\\n\")",
          "71:     upgrading = os.environ.get(\"UPGRADE_TO_NEWER_DEPENDENCIES\", \"false\") != \"false\"",
          "72:     if upgrading:",
          "73:         console.print(",
          "74:             \"[yellow]You are running mypy with the image that has dependencies upgraded automatically.\\n\"",
          "75:         )",
          "76:     flag = \" --upgrade-to-newer-dependencies\" if upgrading else \"\"",
          "77:     console.print(",
          "78:         \"[yellow]If you see strange stacktraces above, and can't reproduce it, please run\"",
          "79:         \" this command and try again:\\n\"",
          "80:     )",
          "81:     console.print(f\"breeze ci-image build --python 3.8{flag}\\n\")",
          "82:     console.print(\"[yellow]You can also run `breeze down --cleanup-mypy-cache` to clean up the cache used.\\n\")",
          "83: sys.exit(res.returncode)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cf83862ccb502f22b4294a813b792cb8b068ddbd",
      "candidate_info": {
        "commit_hash": "cf83862ccb502f22b4294a813b792cb8b068ddbd",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cf83862ccb502f22b4294a813b792cb8b068ddbd",
        "files": [
          ".rat-excludes",
          "3rd-party-licenses/LICENSE-reproducible.txt",
          "BREEZE.rst",
          "dev/README_RELEASE_AIRFLOW.md",
          "dev/breeze/src/airflow_breeze/commands/release_candidate_command.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/utils/reproducible.py",
          "images/breeze/output_release-management.svg",
          "images/breeze/output_release-management.txt",
          "images/breeze/output_release-management_prepare-airflow-package.svg",
          "images/breeze/output_release-management_prepare-airflow-package.txt",
          "images/breeze/output_release-management_prepare-airflow-tarball.svg",
          "images/breeze/output_release-management_prepare-airflow-tarball.txt",
          "images/breeze/output_setup_check-all-params-in-groups.svg",
          "images/breeze/output_setup_check-all-params-in-groups.txt",
          "images/breeze/output_setup_regenerate-command-images.svg",
          "images/breeze/output_setup_regenerate-command-images.txt",
          "scripts/in_container/run_prepare_airflow_packages.py"
        ],
        "message": "Update Airflow release process to include reproducible tarballs (#36744)\n\nSource tarball is the main artifact produced by the release\nprocess - one that is the \"official\" release and named like that\nby the Apache Software Foundation.\n\nThis PR makes the source tarball generation reproducible - following\nreproducibility of the `.whl` and `sdist` packages.\n\nThis change adds:\n\n* vendors-in reproducible.py script that repacks .tar.gz package\n  in reproducible way using source-date-epoch as timestamps\n* breeze release-management prepare-airflow-tarball command\n* adds verification of the tarballs to PMC verification process\n* adds --use-local-hatch for package building command to allow for\n  faster / non-docker build of packages for PMC verification\n* improves diagnostic output of the release and build commands\n\n(cherry picked from commit 72a571dc6d21d90f92d5ce683a5d40c6a527fcb0)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/release_candidate_command.py||dev/breeze/src/airflow_breeze/commands/release_candidate_command.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/utils/reproducible.py||dev/breeze/src/airflow_breeze/utils/reproducible.py",
          "scripts/in_container/run_prepare_airflow_packages.py||scripts/in_container/run_prepare_airflow_packages.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/release_candidate_command.py||dev/breeze/src/airflow_breeze/commands/release_candidate_command.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_candidate_command.py -> dev/breeze/src/airflow_breeze/commands/release_candidate_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: from __future__ import annotations",
          "19: import os",
          "21: import click",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import shutil",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "25: from airflow_breeze.utils.confirm import confirm_action",
          "26: from airflow_breeze.utils.console import console_print",
          "27: from airflow_breeze.utils.path_utils import AIRFLOW_SOURCES_ROOT",
          "28: from airflow_breeze.utils.run_utils import run_command",
          "30: CI = os.environ.get(\"CI\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "29: from airflow_breeze.utils.reproducible import archive_deterministically, get_source_date_epoch",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "59: def git_tag(version):",
          "60:     if confirm_action(f\"Tag {version}?\"):",
          "61:         run_command([\"git\", \"tag\", \"-s\", f\"{version}\", \"-m\", f\"Apache Airflow {version}\"], check=True)",
          "65: def git_clean():",
          "66:     if confirm_action(\"Clean git repo?\"):",
          "67:         run_command([\"breeze\", \"ci\", \"fix-ownership\"], dry_run_override=DRY_RUN, check=True)",
          "68:         run_command([\"git\", \"clean\", \"-fxd\"], dry_run_override=DRY_RUN, check=True)",
          "77:         run_command(",
          "78:             [",
          "79:                 \"git\",",
          "",
          "[Removed Lines]",
          "62:         console_print(\"Tagged\")",
          "69:         console_print(\"Git repo cleaned\")",
          "72: def tarball_release(version, version_without_rc):",
          "73:     if confirm_action(\"Create tarball?\"):",
          "74:         run_command([\"rm\", \"-rf\", \"dist\"], check=True)",
          "76:         run_command([\"mkdir\", \"dist\"], check=True)",
          "",
          "[Added Lines]",
          "64:         console_print(\"[success]Tagged\")",
          "71:         console_print(\"[success]Git repo cleaned\")",
          "74: DIST_DIR = AIRFLOW_SOURCES_ROOT / \"dist\"",
          "75: OUT_DIR = AIRFLOW_SOURCES_ROOT / \"out\"",
          "76: REPRODUCIBLE_DIR = OUT_DIR / \"reproducible\"",
          "79: def tarball_release(version: str, version_without_rc: str, source_date_epoch: int):",
          "80:     if confirm_action(\"Create tarball?\"):",
          "81:         console_print(f\"[info]Creating tarball for Airflow {version}\")",
          "82:         shutil.rmtree(OUT_DIR, ignore_errors=True)",
          "83:         DIST_DIR.mkdir(exist_ok=True)",
          "84:         OUT_DIR.mkdir(exist_ok=True)",
          "85:         REPRODUCIBLE_DIR.mkdir(exist_ok=True)",
          "86:         archive_name = f\"apache-airflow-{version_without_rc}-source.tar.gz\"",
          "87:         temporary_archive = OUT_DIR / archive_name",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "82:                 f\"{version}\",",
          "83:                 f\"--prefix=apache-airflow-{version_without_rc}/\",",
          "84:                 \"-o\",",
          "86:             ],",
          "87:             check=True,",
          "88:         )",
          "98:     run_command(",
          "99:         [",
          "100:             \"breeze\",",
          "",
          "[Removed Lines]",
          "85:                 f\"dist/apache-airflow-{version_without_rc}-source.tar.gz\",",
          "89:         console_print(\"Tarball created\")",
          "92: def create_artifacts_with_sdist():",
          "93:     run_command([\"hatch\", \"build\", \"-t\", \"sdist\", \"-t\", \"wheel\"], check=True)",
          "94:     console_print(\"Artifacts created\")",
          "97: def create_artifacts_with_breeze():",
          "",
          "[Added Lines]",
          "96:                 temporary_archive.as_posix(),",
          "100:         run_command(",
          "101:             [",
          "102:                 \"tar\",",
          "103:                 \"-xf\",",
          "104:                 temporary_archive.as_posix(),",
          "105:                 \"-C\",",
          "106:                 REPRODUCIBLE_DIR.as_posix(),",
          "107:                 \"--strip\",",
          "108:                 \"1\",",
          "109:             ]",
          "110:         )",
          "111:         final_archive = DIST_DIR / archive_name",
          "112:         archive_deterministically(",
          "113:             dir_to_archive=REPRODUCIBLE_DIR.as_posix(),",
          "114:             dest_archive=final_archive.as_posix(),",
          "115:             prepend_path=None,",
          "116:             timestamp=source_date_epoch,",
          "117:         )",
          "118:         console_print(f\"[success]Tarball created in {final_archive}\")",
          "121: def create_artifacts_with_hatch(source_date_epoch: int):",
          "122:     console_print(\"[info]Creating artifacts with hatch\")",
          "123:     shutil.rmtree(DIST_DIR, ignore_errors=True)",
          "124:     DIST_DIR.mkdir(exist_ok=True)",
          "125:     env_copy = os.environ.copy()",
          "126:     env_copy[\"SOURCE_DATE_EPOCH\"] = str(source_date_epoch)",
          "127:     run_command(",
          "128:         [\"hatch\", \"build\", \"-c\", \"-t\", \"custom\", \"-t\", \"sdist\", \"-t\", \"wheel\"], check=True, env=env_copy",
          "129:     )",
          "130:     console_print(\"[success]Successfully prepared Airflow packages:\")",
          "131:     for file in sorted(DIST_DIR.glob(\"apache_airflow*\")):",
          "132:         console_print(print(file.name))",
          "133:     console_print()",
          "136: def create_artifacts_with_docker():",
          "137:     console_print(\"[info]Creating artifacts with docker\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "105:         ],",
          "106:         check=True,",
          "107:     )",
          "111: def sign_the_release(repo_root):",
          "112:     if confirm_action(\"Do you want to sign the release?\"):",
          "113:         os.chdir(f\"{repo_root}/dist\")",
          "114:         run_command(\"./../dev/sign.sh *\", dry_run_override=DRY_RUN, check=True, shell=True)",
          "118: def tag_and_push_constraints(version, version_branch):",
          "",
          "[Removed Lines]",
          "108:     console_print(\"Artifacts created\")",
          "115:         console_print(\"Release signed\")",
          "",
          "[Added Lines]",
          "148:     console_print(\"[success]Artifacts created\")",
          "155:         console_print(\"[success]Release signed\")",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "135:         run_command(",
          "136:             [\"git\", \"push\", \"origin\", \"tag\", f\"constraints-{version}\"], dry_run_override=DRY_RUN, check=True",
          "137:         )",
          "141: def clone_asf_repo(version, repo_root):",
          "",
          "[Removed Lines]",
          "138:         console_print(\"Constraints tagged and pushed\")",
          "",
          "[Added Lines]",
          "178:         console_print(\"[success]Constraints tagged and pushed\")",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "146:             check=True,",
          "147:         )",
          "148:         run_command([\"svn\", \"update\", \"--set-depth=infinity\", \"asf-dist/dev/airflow\"], check=True)",
          "152: def move_artifacts_to_svn(version, repo_root):",
          "",
          "[Removed Lines]",
          "149:         console_print(\"Cloned ASF repo successfully\")",
          "",
          "[Added Lines]",
          "189:         console_print(\"[success]Cloned ASF repo successfully\")",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "154:         os.chdir(f\"{repo_root}/asf-dist/dev/airflow\")",
          "155:         run_command([\"svn\", \"mkdir\", f\"{version}\"], dry_run_override=DRY_RUN, check=True)",
          "156:         run_command(f\"mv {repo_root}/dist/* {version}/\", dry_run_override=DRY_RUN, check=True, shell=True)",
          "158:         run_command([\"ls\"], dry_run_override=DRY_RUN)",
          "",
          "[Removed Lines]",
          "157:         console_print(\"Moved artifacts to SVN:\")",
          "",
          "[Added Lines]",
          "197:         console_print(\"[success]Moved artifacts to SVN:\")",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "171:             dry_run_override=DRY_RUN,",
          "172:             check=True,",
          "173:         )",
          "177: def delete_asf_repo(repo_root):",
          "",
          "[Removed Lines]",
          "174:         console_print(\"Files pushed to svn\")",
          "",
          "[Added Lines]",
          "214:         console_print(\"[success]Files pushed to svn\")",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "183: def prepare_pypi_packages(version, version_suffix, repo_root):",
          "184:     if confirm_action(\"Prepare pypi packages?\"):",
          "186:         os.chdir(repo_root)",
          "187:         run_command([\"git\", \"checkout\", f\"{version}\"], dry_run_override=DRY_RUN, check=True)",
          "188:         run_command(",
          "",
          "[Removed Lines]",
          "185:         console_print(\"Preparing PyPI packages\")",
          "",
          "[Added Lines]",
          "225:         console_print(\"[info]Preparing PyPI packages\")",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "198:             check=True,",
          "199:         )",
          "200:         run_command([\"twine\", \"check\", \"dist/*\"], check=True)",
          "204: def push_packages_to_pypi(version):",
          "205:     if confirm_action(\"Do you want to push packages to production PyPI?\"):",
          "206:         run_command([\"twine\", \"upload\", \"-r\", \"pypi\", \"dist/*\"], dry_run_override=DRY_RUN, check=True)",
          "208:         console_print(",
          "209:             \"Again, confirm that the package is available here: https://pypi.python.org/pypi/apache-airflow\"",
          "210:         )",
          "",
          "[Removed Lines]",
          "201:         console_print(\"PyPI packages prepared\")",
          "207:         console_print(\"Packages pushed to production PyPI\")",
          "",
          "[Added Lines]",
          "241:         console_print(\"[success]PyPI packages prepared\")",
          "247:         console_print(\"[success]Packages pushed to production PyPI\")",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "240:         )",
          "241:         confirm_action(f\"Confirm that {version} is pushed to PyPI(not PyPI test). Is it pushed?\", abort=True)",
          "242:         run_command([\"git\", \"push\", \"origin\", \"tag\", f\"{version}\"], dry_run_override=DRY_RUN, check=True)",
          "246: def create_issue_for_testing(version, previous_version, github_token):",
          "",
          "[Removed Lines]",
          "243:         console_print(\"Release candidate tag pushed to GitHub\")",
          "",
          "[Added Lines]",
          "283:         console_print(\"[success]Release candidate tag pushed to GitHub\")",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "293:                 dry_run_override=DRY_RUN,",
          "294:                 check=True,",
          "295:             )",
          "297:     os.chdir(repo_root)",
          "300: @release_management.command(",
          "301:     name=\"start-rc-process\",",
          "302:     short_help=\"Start RC process\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "336:     console_print(\"[success]Old releases removed\")",
          "340: @release_management.command(",
          "341:     name=\"prepare-airflow-tarball\",",
          "342:     help=\"Prepare airflow's source tarball.\",",
          "343: )",
          "344: @click.option(",
          "345:     \"--version\", required=True, help=\"The release candidate version e.g. 2.4.3rc1\", envvar=\"VERSION\"",
          "346: )",
          "347: def prepare_airflow_tarball(version: str):",
          "348:     from packaging.version import Version",
          "350:     airflow_version = Version(version)",
          "351:     if not airflow_version.is_prerelease:",
          "352:         exit(\"--version value must be a pre-release\")",
          "353:     source_date_epoch = get_source_date_epoch()",
          "354:     version_without_rc = airflow_version.base_version",
          "355:     # Create the tarball",
          "356:     tarball_release(",
          "357:         version=version, version_without_rc=version_without_rc, source_date_epoch=source_date_epoch",
          "358:     )",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "311: def publish_release_candidate(version, previous_version, github_token):",
          "312:     from packaging.version import Version",
          "315:         exit(\"--version value must be a pre-release\")",
          "316:     if Version(previous_version).is_prerelease:",
          "317:         exit(\"--previous-version value must be a release not a pre-release\")",
          "",
          "[Removed Lines]",
          "314:     if not Version(version).is_prerelease:",
          "",
          "[Added Lines]",
          "375:     airflow_version = Version(version)",
          "376:     if not airflow_version.is_prerelease:",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "320:         if not github_token:",
          "321:             console_print(\"GITHUB_TOKEN is not set! Issue generation will fail.\")",
          "322:             confirm_action(\"Do you want to continue?\", abort=True)",
          "326:     os.chdir(AIRFLOW_SOURCES_ROOT)",
          "327:     airflow_repo_root = os.getcwd()",
          "",
          "[Removed Lines]",
          "323:     version_suffix = version[5:]",
          "324:     version_branch = version[:3].replace(\".\", \"-\")",
          "325:     version_without_rc = version[:5]",
          "",
          "[Added Lines]",
          "386:     version_suffix = airflow_version.pre[0] + str(airflow_version.pre[1])",
          "387:     version_branch = str(airflow_version.release[0]) + \"-\" + str(airflow_version.release[1])",
          "388:     version_without_rc = airflow_version.base_version",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "343:     confirm_action(\"Pushes will be made to origin. Do you want to continue?\", abort=True)",
          "344:     # Merge the sync PR",
          "345:     merge_pr(version_branch)",
          "348:     git_tag(version)",
          "349:     git_clean()",
          "353:     # Create the tarball",
          "355:     # Create the artifacts",
          "358:     elif confirm_action(\"Use hatch to create artifacts?\"):",
          "360:     # Sign the release",
          "361:     sign_the_release(airflow_repo_root)",
          "362:     # Tag and push constraints",
          "",
          "[Removed Lines]",
          "347:     # Tag & clean the repo",
          "350:     # Build the latest image",
          "351:     if confirm_action(\"Build latest breeze image?\"):",
          "352:         run_command([\"breeze\", \"ci-image\", \"build\", \"--python\", \"3.8\"], dry_run_override=DRY_RUN, check=True)",
          "354:     tarball_release(version, version_without_rc)",
          "356:     if confirm_action(\"Use breeze to create artifacts?\"):",
          "357:         create_artifacts_with_breeze()",
          "359:         create_artifacts_with_sdist()",
          "",
          "[Added Lines]",
          "409:     #",
          "410:     # # Tag & clean the repo",
          "413:     source_date_epoch = get_source_date_epoch()",
          "414:     shutil.rmtree(DIST_DIR, ignore_errors=True)",
          "416:     tarball_release(",
          "417:         version=version, version_without_rc=version_without_rc, source_date_epoch=source_date_epoch",
          "418:     )",
          "420:     if confirm_action(\"Use docker to create artifacts?\"):",
          "421:         create_artifacts_with_docker()",
          "423:         create_artifacts_with_hatch()",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "142:     get_related_providers,",
          "143: )",
          "144: from airflow_breeze.utils.python_versions import get_python_version_list",
          "145: from airflow_breeze.utils.run_utils import (",
          "146:     run_command,",
          "147: )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "145: from airflow_breeze.utils.reproducible import get_source_date_epoch",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "282:     name=\"prepare-airflow-package\",",
          "283:     help=\"Prepare sdist/whl package of Airflow.\",",
          "284: )",
          "285: @option_package_format",
          "286: @option_version_suffix_for_pypi",
          "287: @option_verbose",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "286: @click.option(",
          "287:     \"--use-local-hatch\",",
          "288:     is_flag=True,",
          "289:     help=\"Use local hatch instead of docker to build the package. You need to have hatch installed.\",",
          "290: )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "289: def prepare_airflow_packages(",
          "290:     package_format: str,",
          "291:     version_suffix_for_pypi: str,",
          "292: ):",
          "293:     perform_environment_checks()",
          "294:     fix_ownership_using_docker()",
          "295:     cleanup_python_generated_files()",
          "296:     # This is security feature.",
          "297:     #",
          "298:     # Building the image needed to build airflow package including .git directory",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "298:     use_local_hatch: bool,",
          "303:     source_date_epoch = get_source_date_epoch()",
          "304:     if use_local_hatch:",
          "305:         hatch_build_command = [\"hatch\", \"build\", \"-c\", \"-t\", \"custom\"]",
          "306:         if package_format in [\"sdist\", \"both\"]:",
          "307:             hatch_build_command.extend([\"-t\", \"sdist\"])",
          "308:         if package_format in [\"wheel\", \"both\"]:",
          "309:             hatch_build_command.extend([\"-t\", \"wheel\"])",
          "310:         env_copy = os.environ.copy()",
          "311:         env_copy[\"SOURCE_DATE_EPOCH\"] = str(source_date_epoch)",
          "312:         run_command(",
          "313:             hatch_build_command,",
          "314:             check=True,",
          "315:             env=env_copy,",
          "316:         )",
          "317:         get_console().print(\"[success]Successfully prepared Airflow packages:\")",
          "318:         for file in sorted(DIST_DIR.glob(\"apache_airflow*\")):",
          "319:             get_console().print(file.name)",
          "320:         get_console().print()",
          "321:         return",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "350:     # Copy all files in the dist directory in container to the host dist directory (note '/.' in SRC)",
          "351:     run_command([\"docker\", \"cp\", f\"{container_id}:/opt/airflow/dist/.\", \"./dist\"], check=True)",
          "352:     run_command([\"docker\", \"rm\", \"--force\", container_id], check=True)",
          "356: def provider_action_summary(description: str, message_type: MessageType, packages: list[str]):",
          "",
          "[Removed Lines]",
          "353:     get_console().print(\"[success]Successfully prepared Airflow package!\\n\\n\")",
          "",
          "[Added Lines]",
          "379:     get_console().print(\"[success]Successfully prepared Airflow packages:\")",
          "380:     for file in sorted(DIST_DIR.glob(\"apache_airflow*\")):",
          "381:         get_console().print(file.name)",
          "382:     get_console().print()",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: RELEASE_AIRFLOW_COMMANDS: dict[str, str | list[str]] = {",
          "20:     \"name\": \"Airflow release commands\",",
          "21:     \"commands\": [",
          "23:         \"create-minor-branch\",",
          "24:         \"start-rc-process\",",
          "25:         \"start-release\",",
          "26:         \"release-prod-images\",",
          "",
          "[Removed Lines]",
          "22:         \"prepare-airflow-package\",",
          "",
          "[Added Lines]",
          "23:         \"prepare-airflow-package\",",
          "24:         \"prepare-airflow-tarball\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "43: RELEASE_OTHER_COMMANDS: dict[str, str | list[str]] = {",
          "44:     \"name\": \"Other release commands\",",
          "45:     \"commands\": [",
          "46:         \"publish-docs\",",
          "47:         \"generate-constraints\",",
          "49:     ],",
          "50: }",
          "",
          "[Removed Lines]",
          "48:         \"add-back-references\",",
          "",
          "[Added Lines]",
          "47:         \"add-back-references\",",
          "50:         \"update-constraints\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "55:             \"name\": \"Package flags\",",
          "56:             \"options\": [",
          "57:                 \"--package-format\",",
          "58:                 \"--version-suffix-for-pypi\",",
          "59:             ],",
          "60:         }",
          "61:     ],",
          "62:     \"breeze release-management verify-provider-packages\": [",
          "63:         {",
          "64:             \"name\": \"Provider verification flags\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "60:                 \"--use-local-hatch\",",
          "65:     \"breeze release-management prepare-airflow-tarball\": [",
          "66:         {",
          "67:             \"name\": \"Package flags\",",
          "68:             \"options\": [",
          "69:                 \"--version\",",
          "70:             ],",
          "71:         }",
          "72:     ],",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/reproducible.py||dev/breeze/src/airflow_breeze/utils/reproducible.py": [
          "File: dev/breeze/src/airflow_breeze/utils/reproducible.py -> dev/breeze/src/airflow_breeze/utils/reproducible.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python3",
          "4: # Licensed to the Apache Software Foundation (ASF) under one",
          "5: # or more contributor license agreements.  See the NOTICE file",
          "6: # distributed with this work for additional information",
          "7: # regarding copyright ownership.  The ASF licenses this file",
          "8: # to you under the Apache License, Version 2.0 (the",
          "9: # \"License\"); you may not use this file except in compliance",
          "10: # with the License.  You may obtain a copy of the License at",
          "11: #",
          "12: #   http://www.apache.org/licenses/LICENSE-2.0",
          "13: #",
          "14: # Unless required by applicable law or agreed to in writing,",
          "15: # software distributed under the License is distributed on an",
          "16: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "17: # KIND, either express or implied.  See the License for the",
          "18: # specific language governing permissions and limitations",
          "19: # under the License.",
          "21: # Copyright 2013 The Servo Project Developers.",
          "22: # Copyright 2017 zerolib Developers.",
          "23: #",
          "24: # Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or",
          "25: # http://www.apache.org/licenses/LICENSE-2.0> or the MIT license",
          "26: # <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your",
          "27: # option. This file may not be copied, modified, or distributed",
          "28: # except according to those terms.",
          "30: # This command is a largely vendored-in script from",
          "31: # https://github.com/MuxZeroNet/reproducible/blob/master/reproducible.py",
          "32: from __future__ import annotations",
          "34: import contextlib",
          "35: import gzip",
          "36: import itertools",
          "37: import locale",
          "38: import os",
          "39: import tarfile",
          "40: from argparse import ArgumentParser",
          "42: from airflow_breeze.utils.path_utils import AIRFLOW_SOURCES_ROOT",
          "45: def get_source_date_epoch():",
          "46:     import yaml",
          "48:     reproducible_build_yaml = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"reproducible_build.yaml\"",
          "49:     reproducible_build_dict = yaml.safe_load(reproducible_build_yaml.read_text())",
          "50:     source_date_epoch: int = reproducible_build_dict[\"source-date-epoch\"]",
          "51:     return source_date_epoch",
          "54: @contextlib.contextmanager",
          "55: def cd(new_path):",
          "56:     \"\"\"Context manager for changing the current working directory\"\"\"",
          "57:     previous_path = os.getcwd()",
          "58:     try:",
          "59:         os.chdir(new_path)",
          "60:         yield",
          "61:     finally:",
          "62:         os.chdir(previous_path)",
          "65: @contextlib.contextmanager",
          "66: def setlocale(name):",
          "67:     \"\"\"Context manager for changing the current locale\"\"\"",
          "68:     saved_locale = locale.setlocale(locale.LC_ALL)",
          "69:     try:",
          "70:         yield locale.setlocale(locale.LC_ALL, name)",
          "71:     finally:",
          "72:         locale.setlocale(locale.LC_ALL, saved_locale)",
          "75: def archive_deterministically(dir_to_archive, dest_archive, prepend_path=None, timestamp=0):",
          "76:     \"\"\"Create a .tar.gz archive in a deterministic (reproducible) manner.",
          "78:     See https://reproducible-builds.org/docs/archives/ for more details.\"\"\"",
          "80:     def reset(tarinfo):",
          "81:         \"\"\"Helper to reset owner/group and modification time for tar entries\"\"\"",
          "82:         tarinfo.uid = tarinfo.gid = 0",
          "83:         tarinfo.uname = tarinfo.gname = \"root\"",
          "84:         tarinfo.mtime = timestamp",
          "85:         return tarinfo",
          "87:     dest_archive = os.path.abspath(dest_archive)",
          "88:     with cd(dir_to_archive):",
          "89:         current_dir = \".\"",
          "90:         file_list = [current_dir]",
          "91:         for root, dirs, files in os.walk(current_dir):",
          "92:             for name in itertools.chain(dirs, files):",
          "93:                 file_list.append(os.path.join(root, name))",
          "95:         # Sort file entries with the fixed locale",
          "96:         with setlocale(\"C\"):",
          "97:             file_list.sort(key=locale.strxfrm)",
          "99:         # Use a temporary file and atomic rename to avoid partially-formed",
          "100:         # packaging (in case of exceptional situations like running out of disk space).",
          "101:         temp_file = f\"{dest_archive}.temp~\"",
          "102:         with os.fdopen(os.open(temp_file, os.O_WRONLY | os.O_CREAT, 0o644), \"wb\") as out_file:",
          "103:             with gzip.GzipFile(\"wb\", fileobj=out_file, mtime=0) as gzip_file:",
          "104:                 with tarfile.open(fileobj=gzip_file, mode=\"w:\") as tar_file:",
          "105:                     for entry in file_list:",
          "106:                         arcname = entry",
          "107:                         if prepend_path is not None:",
          "108:                             arcname = os.path.normpath(os.path.join(prepend_path, arcname))",
          "109:                         tar_file.add(entry, filter=reset, recursive=False, arcname=arcname)",
          "110:         os.rename(temp_file, dest_archive)",
          "113: def main():",
          "114:     parser = ArgumentParser()",
          "115:     parser.add_argument(\"-d\", \"--dir\", help=\"directory to archive\")",
          "116:     parser.add_argument(\"-o\", \"--out\", help=\"archive destination\")",
          "117:     parser.add_argument(\"-p\", \"--prepend\", help=\"prepend path\")",
          "118:     parser.add_argument(",
          "119:         \"-t\", \"--timestamp\", help=\"timestamp of files\", type=int, default=get_source_date_epoch()",
          "120:     )",
          "122:     args = parser.parse_args()",
          "124:     if not args.dir or not args.out:",
          "125:         error = (",
          "126:             \"You should provide a directory to archive, and the \"",
          "127:             f\"archive file name, not {repr((args.dir, args.out))}\"",
          "128:         )",
          "129:         raise ValueError(error)",
          "131:     archive_deterministically(args.dir, args.out, args.prepend, args.timestamp)",
          "134: if __name__ == \"__main__\":",
          "135:     main()",
          "",
          "---------------"
        ],
        "scripts/in_container/run_prepare_airflow_packages.py||scripts/in_container/run_prepare_airflow_packages.py": [
          "File: scripts/in_container/run_prepare_airflow_packages.py -> scripts/in_container/run_prepare_airflow_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "78: def build_airflow_packages(package_format: str):",
          "79:     build_command = [sys.executable, \"-m\", \"hatch\", \"build\", \"-t\", \"custom\"]",
          "83:     if package_format in [\"both\", \"sdist\"]:",
          "84:         build_command.extend([\"-t\", \"sdist\"])",
          "86:     reproducible_date = yaml.safe_load(REPRODUCIBLE_BUILD_FILE.read_text())[\"source-date-epoch\"]",
          "",
          "[Removed Lines]",
          "81:     if package_format in [\"both\", \"wheel\"]:",
          "82:         build_command.extend([\"-t\", \"wheel\"])",
          "",
          "[Added Lines]",
          "82:     if package_format in [\"both\", \"wheel\"]:",
          "83:         build_command.extend([\"-t\", \"wheel\"])",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "81b23317dde8ddf68295725131e4956c10146d5d",
      "candidate_info": {
        "commit_hash": "81b23317dde8ddf68295725131e4956c10146d5d",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/81b23317dde8ddf68295725131e4956c10146d5d",
        "files": [
          "airflow/jobs/backfill_job_runner.py"
        ],
        "message": "Refactor _manage_executor_state by refreshing TIs in batch (#36502)\n\nRefactor _manage_executor_state by refreshing TIs in batch (#36418)\" (#36500)\"\n\nHandle Microsoft SQL Server\n\n(cherry picked from commit 9cf5f6f08483ff141df51c07daa91a0aa34906ec)",
        "before_after_code_files": [
          "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py": [
          "File: airflow/jobs/backfill_job_runner.py -> airflow/jobs/backfill_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import attr",
          "24: import pendulum",
          "26: from sqlalchemy.exc import OperationalError",
          "27: from sqlalchemy.orm.session import make_transient",
          "28: from tabulate import tabulate",
          "",
          "[Removed Lines]",
          "25: from sqlalchemy import select, update",
          "",
          "[Added Lines]",
          "25: from sqlalchemy import select, tuple_, update",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "264:         :return: An iterable of expanded TaskInstance per MappedTask",
          "265:         \"\"\"",
          "266:         executor = self.job.executor",
          "270:             state, info = value",
          "272:                 self.log.warning(\"%s state %s not in running=%s\", key, state, running.values())",
          "273:                 continue",
          "278:             self.log.debug(\"Executor state: %s task %s\", state, ti)",
          "",
          "[Removed Lines]",
          "268:         # TODO: query all instead of refresh from db",
          "269:         for key, value in list(executor.get_event_buffer().items()):",
          "271:             if key not in running:",
          "275:             ti = running[key]",
          "276:             ti.refresh_from_db()",
          "",
          "[Added Lines]",
          "267:         # list of tuples (dag_id, task_id, execution_date, map_index) of running tasks in executor",
          "268:         buffered_events = list(executor.get_event_buffer().items())",
          "269:         if session.get_bind().dialect.name == \"mssql\":",
          "270:             # SQL Server doesn't support multiple column subqueries",
          "271:             # TODO: Remove this once we drop support for SQL Server (#35868)",
          "272:             need_refresh = True",
          "273:             running_dict = {(ti.dag_id, ti.task_id, ti.run_id, ti.map_index): ti for ti in running.values()}",
          "274:         else:",
          "275:             running_tis_ids = [",
          "276:                 (key.dag_id, key.task_id, key.run_id, key.map_index)",
          "277:                 for key, _ in buffered_events",
          "278:                 if key in running",
          "279:             ]",
          "280:             # list of TaskInstance of running tasks in executor (refreshed from db in batch)",
          "281:             refreshed_running_tis = session.scalars(",
          "282:                 select(TaskInstance).where(",
          "283:                     tuple_(",
          "284:                         TaskInstance.dag_id,",
          "285:                         TaskInstance.task_id,",
          "286:                         TaskInstance.run_id,",
          "287:                         TaskInstance.map_index,",
          "288:                     ).in_(running_tis_ids)",
          "289:                 )",
          "290:             ).all()",
          "291:             # dict of refreshed TaskInstance by key to easily find them",
          "292:             running_dict = {",
          "293:                 (ti.dag_id, ti.task_id, ti.run_id, ti.map_index): ti for ti in refreshed_running_tis",
          "294:             }",
          "295:             need_refresh = False",
          "297:         for key, value in buffered_events:",
          "299:             ti_key = (key.dag_id, key.task_id, key.run_id, key.map_index)",
          "300:             if ti_key not in running_dict:",
          "304:             ti = running_dict[ti_key]",
          "305:             if need_refresh:",
          "306:                 ti.refresh_from_db(session=session)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "48afea83e6c692258a266930cc76df8950e80f4e",
      "candidate_info": {
        "commit_hash": "48afea83e6c692258a266930cc76df8950e80f4e",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/48afea83e6c692258a266930cc76df8950e80f4e",
        "files": [
          "airflow/example_dags/plugins/workday.py",
          "tests/plugins/workday.py"
        ],
        "message": "Straighten typing in workday timetable (#36296)\n\n(cherry picked from commit 954bb60e876b7cbb491ec7542ecdbb6bb9b8ab03)",
        "before_after_code_files": [
          "airflow/example_dags/plugins/workday.py||airflow/example_dags/plugins/workday.py",
          "tests/plugins/workday.py||tests/plugins/workday.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/example_dags/plugins/workday.py||airflow/example_dags/plugins/workday.py": [
          "File: airflow/example_dags/plugins/workday.py -> airflow/example_dags/plugins/workday.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "73:     ) -> DagRunInfo | None:",
          "74:         if last_automated_data_interval is not None:  # There was a previous run on the regular schedule.",
          "75:             last_start = last_automated_data_interval.start",
          "91:         # Skip weekends and holidays",
          "94:         if restriction.latest is not None and next_start > restriction.latest:",
          "95:             return None  # Over the DAG's scheduled end; don't schedule.",
          "",
          "[Removed Lines]",
          "76:             next_start = DateTime.combine((last_start + timedelta(days=1)).date(), Time.min).replace(",
          "77:                 tzinfo=UTC",
          "78:             )",
          "79:         else:  # This is the first ever run on the regular schedule.",
          "80:             next_start = restriction.earliest",
          "81:             if next_start is None:  # No start_date. Don't schedule.",
          "82:                 return None",
          "83:             if not restriction.catchup:",
          "84:                 # If the DAG has catchup=False, today is the earliest to consider.",
          "85:                 next_start = max(next_start, DateTime.combine(Date.today(), Time.min).replace(tzinfo=UTC))",
          "86:             elif next_start.time() != Time.min:",
          "87:                 # If earliest does not fall on midnight, skip to the next day.",
          "88:                 next_start = DateTime.combine(next_start.date() + timedelta(days=1), Time.min).replace(",
          "89:                     tzinfo=UTC",
          "90:                 )",
          "92:         next_start = self.get_next_workday(next_start)",
          "",
          "[Added Lines]",
          "76:             next_start = DateTime.combine((last_start + timedelta(days=1)).date(), Time.min)",
          "77:         # Otherwise this is the first ever run on the regular schedule...",
          "78:         elif (earliest := restriction.earliest) is None:",
          "79:             return None  # No start_date. Don't schedule.",
          "80:         elif not restriction.catchup:",
          "81:             # If the DAG has catchup=False, today is the earliest to consider.",
          "82:             next_start = max(earliest, DateTime.combine(Date.today(), Time.min))",
          "83:         elif earliest.time() != Time.min:",
          "84:             # If earliest does not fall on midnight, skip to the next day.",
          "85:             next_start = DateTime.combine(earliest.date() + timedelta(days=1), Time.min)",
          "86:         else:",
          "87:             next_start = earliest",
          "89:         next_start = self.get_next_workday(next_start.replace(tzinfo=UTC))",
          "",
          "---------------"
        ],
        "tests/plugins/workday.py||tests/plugins/workday.py": [
          "File: tests/plugins/workday.py -> tests/plugins/workday.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "",
          "[Removed Lines]",
          "18: \"\"\"Plugin to demonstrate timetable registration and accommodate example DAGs.\"\"\"",
          "19: from __future__ import annotations",
          "21: import logging",
          "22: from datetime import timedelta",
          "24: # [START howto_timetable]",
          "25: from pendulum import UTC, Date, DateTime, Time",
          "27: from airflow.plugins_manager import AirflowPlugin",
          "28: from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction, Timetable",
          "30: log = logging.getLogger(__name__)",
          "31: try:",
          "32:     from pandas.tseries.holiday import USFederalHolidayCalendar",
          "34:     holiday_calendar = USFederalHolidayCalendar()",
          "35: except ImportError:",
          "36:     log.warning(\"Could not import pandas. Holidays will not be considered.\")",
          "37:     holiday_calendar = None  # type: ignore[assignment]",
          "40: class AfterWorkdayTimetable(Timetable):",
          "41:     def get_next_workday(self, d: DateTime, incr=1) -> DateTime:",
          "42:         next_start = d",
          "43:         while True:",
          "44:             if next_start.weekday() in (5, 6):  # If next start is in the weekend go to next day",
          "45:                 next_start = next_start + incr * timedelta(days=1)",
          "46:                 continue",
          "47:             if holiday_calendar is not None:",
          "48:                 holidays = holiday_calendar.holidays(start=next_start, end=next_start).to_pydatetime()",
          "49:                 if next_start in holidays:  # If next start is a holiday go to next day",
          "50:                     next_start = next_start + incr * timedelta(days=1)",
          "51:                     continue",
          "52:             break",
          "53:         return next_start",
          "55:     # [START howto_timetable_infer_manual_data_interval]",
          "56:     def infer_manual_data_interval(self, run_after: DateTime) -> DataInterval:",
          "57:         start = DateTime.combine((run_after - timedelta(days=1)).date(), Time.min).replace(tzinfo=UTC)",
          "58:         # Skip backwards over weekends and holidays to find last run",
          "59:         start = self.get_next_workday(start, incr=-1)",
          "60:         return DataInterval(start=start, end=(start + timedelta(days=1)))",
          "62:     # [END howto_timetable_infer_manual_data_interval]",
          "64:     # [START howto_timetable_next_dagrun_info]",
          "65:     def next_dagrun_info(",
          "66:         self,",
          "68:         last_automated_data_interval: DataInterval | None,",
          "69:         restriction: TimeRestriction,",
          "70:     ) -> DagRunInfo | None:",
          "71:         if last_automated_data_interval is not None:  # There was a previous run on the regular schedule.",
          "72:             last_start = last_automated_data_interval.start",
          "73:             next_start = DateTime.combine((last_start + timedelta(days=1)).date(), Time.min).replace(",
          "74:                 tzinfo=UTC",
          "75:             )",
          "76:         else:  # This is the first ever run on the regular schedule.",
          "77:             next_start = restriction.earliest",
          "78:             if next_start is None:  # No start_date. Don't schedule.",
          "79:                 return None",
          "80:             if not restriction.catchup:",
          "81:                 # If the DAG has catchup=False, today is the earliest to consider.",
          "82:                 next_start = max(next_start, DateTime.combine(Date.today(), Time.min).replace(tzinfo=UTC))",
          "83:             elif next_start.time() != Time.min:",
          "84:                 # If earliest does not fall on midnight, skip to the next day.",
          "85:                 next_start = DateTime.combine(next_start.date() + timedelta(days=1), Time.min).replace(",
          "86:                     tzinfo=UTC",
          "87:                 )",
          "88:         # Skip weekends and holidays",
          "89:         next_start = self.get_next_workday(next_start)",
          "91:         if restriction.latest is not None and next_start > restriction.latest:",
          "92:             return None  # Over the DAG's scheduled end; don't schedule.",
          "93:         return DagRunInfo.interval(start=next_start, end=(next_start + timedelta(days=1)))",
          "95:     # [END howto_timetable_next_dagrun_info]",
          "98: class WorkdayTimetablePlugin(AirflowPlugin):",
          "99:     name = \"workday_timetable_plugin\"",
          "100:     timetables = [AfterWorkdayTimetable]",
          "103: # [END howto_timetable]",
          "",
          "[Added Lines]",
          "18: \"\"\"Plugin to demonstrate timetable registration and accommodate example DAGs.",
          "20: This simply forwards the timetable from ``airflow.example_dags``, so we can make",
          "21: it discoverable to unit tests without exposing the entire subpackage.",
          "22: \"\"\"",
          "23: from __future__ import annotations",
          "25: from airflow.example_dags.plugins.workday import AfterWorkdayTimetable, WorkdayTimetablePlugin",
          "27: __all__ = [\"AfterWorkdayTimetable\", \"WorkdayTimetablePlugin\"]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5422a3612b03fe66b606e9cbc66480c3c18543a7",
      "candidate_info": {
        "commit_hash": "5422a3612b03fe66b606e9cbc66480c3c18543a7",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/5422a3612b03fe66b606e9cbc66480c3c18543a7",
        "files": [
          "airflow/api_connexion/endpoints/dag_warning_endpoint.py",
          "airflow/api_connexion/security.py"
        ],
        "message": "Replace deprecated get_accessible_dag_ids and use get_readable_dags in get_dag_warnings (#36256)\n\n(cherry picked from commit 9406f00c0cab795375973e84702824e685d53e04)",
        "before_after_code_files": [
          "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py",
          "airflow/api_connexion/security.py||airflow/api_connexion/security.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py": [
          "File: airflow/api_connexion/endpoints/dag_warning_endpoint.py -> airflow/api_connexion/endpoints/dag_warning_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: from typing import TYPE_CHECKING",
          "22: from sqlalchemy import select",
          "24: from airflow.api_connexion import security",
          "",
          "[Removed Lines]",
          "21: from flask import g",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "27:     DagWarningCollection,",
          "28:     dag_warning_collection_schema,",
          "29: )",
          "30: from airflow.auth.managers.models.resource_details import DagAccessEntity",
          "31: from airflow.models.dagwarning import DagWarning as DagWarningModel",
          "33: from airflow.utils.db import get_query_count",
          "34: from airflow.utils.session import NEW_SESSION, provide_session",
          "",
          "[Removed Lines]",
          "32: from airflow.utils.airflow_flask_app import get_airflow_app",
          "",
          "[Added Lines]",
          "29: from airflow.api_connexion.security import get_readable_dags",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "61:     if dag_id:",
          "62:         query = query.where(DagWarningModel.dag_id == dag_id)",
          "63:     else:",
          "65:         query = query.where(DagWarningModel.dag_id.in_(readable_dags))",
          "66:     if warning_type:",
          "67:         query = query.where(DagWarningModel.warning_type == warning_type)",
          "",
          "[Removed Lines]",
          "64:         readable_dags = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
          "",
          "[Added Lines]",
          "63:         readable_dags = get_readable_dags()",
          "",
          "---------------"
        ],
        "airflow/api_connexion/security.py||airflow/api_connexion/security.py": [
          "File: airflow/api_connexion/security.py -> airflow/api_connexion/security.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "267:     return requires_access_decorator",
          "",
          "[Removed Lines]",
          "270: def get_readable_dags() -> list[str]:",
          "271:     return get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
          "",
          "[Added Lines]",
          "270: def get_readable_dags() -> set[str]:",
          "271:     return get_auth_manager().get_permitted_dag_ids(user=g.user)",
          "",
          "---------------"
        ]
      }
    }
  ]
}