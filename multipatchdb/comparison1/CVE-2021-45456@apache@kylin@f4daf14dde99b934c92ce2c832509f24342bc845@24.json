{
  "cve_id": "CVE-2021-45456",
  "cve_desc": "Apache kylin checks the legitimacy of the project before executing some commands with the project name passed in by the user. There is a mismatch between what is being checked and what is being used as the shell command argument in DiagnosisService. This may cause an illegal project name to pass the check and perform the following steps, resulting in a command injection vulnerability. This issue affects Apache Kylin 4.0.0.",
  "repo": "apache/kylin",
  "patch_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
  "patch_info": {
    "commit_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
    "repo": "apache/kylin",
    "commit_url": "https://github.com/apache/kylin/commit/f4daf14dde99b934c92ce2c832509f24342bc845",
    "files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "server/src/main/webapp/WEB-INF/web.xml"
    ],
    "message": "test fix",
    "before_after_code_files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java"
    ]
  },
  "patch_diff": {
    "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "3403:     public String getKerberosPrincipal() {",
      "3404:         return getOptional(\"kylin.kerberos.principal\");",
      "3405:     }",
      "3406: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "3407:     public String getEncryptCipherIvSpec() {",
      "3408:         return getOptional(\"kylin.security.encrypt.cipher.ivSpec\", \"AAAAAAAAAAAAAAAA\");",
      "3409:     }",
      "",
      "---------------"
    ],
    "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java -> core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "25: import java.security.NoSuchAlgorithmException;",
      "27: import org.apache.commons.codec.binary.Base64;",
      "29: import javax.crypto.Cipher;",
      "30: import javax.crypto.NoSuchPaddingException;",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "28: import org.apache.kylin.common.KylinConfig;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "42:             InvalidKeyException, NoSuchPaddingException, NoSuchAlgorithmException, UnsupportedEncodingException {",
      "43:         Cipher cipher = Cipher.getInstance(\"AES/CFB/PKCS5Padding\");",
      "44:         final SecretKeySpec secretKey = new SecretKeySpec(key, \"AES\");",
      "46:         cipher.init(cipherMode, secretKey, ivSpec);",
      "47:         return cipher;",
      "48:     }",
      "",
      "[Removed Lines]",
      "45:         IvParameterSpec ivSpec = new IvParameterSpec(\"AAAAAAAAAAAAAAAA\".getBytes(\"UTF-8\"));",
      "",
      "[Added Lines]",
      "46:         IvParameterSpec ivSpec = new IvParameterSpec(KylinConfig.getInstanceFromEnv().getEncryptCipherIvSpec().getBytes(\"UTF-8\"));",
      "",
      "---------------"
    ],
    "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java": [
      "File: core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java -> core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: package org.apache.kylin.common.util;",
      "21: import org.junit.Assert;",
      "22: import org.junit.Test;",
      "26:     @Test",
      "27:     public void testAESEncrypt(){",
      "",
      "[Removed Lines]",
      "24: public class EncryptUtilTest {",
      "",
      "[Added Lines]",
      "21: import org.junit.After;",
      "23: import org.junit.Before;",
      "26: public class EncryptUtilTest extends LocalFileMetadataTestCase {",
      "27:     @Before",
      "28:     public void setUp() throws Exception {",
      "29:         this.createTestMetadata();",
      "30:     }",
      "32:     @After",
      "33:     public void after() throws Exception {",
      "34:         this.cleanupTestMetadata();",
      "35:     }",
      "",
      "---------------"
    ],
    "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java": [
      "File: server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java -> server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "87:     public String dumpProjectDiagnosisInfo(String project, File exportPath) throws IOException {",
      "88:         Message msg = MsgPicker.getMsg();",
      "89:         ProjectInstance projectInstance =",
      "90:                 ProjectManager.getInstance(KylinConfig.getInstanceFromEnv())",
      "92:         if (null == projectInstance) {",
      "93:             throw new BadRequestException(",
      "95:         }",
      "96:         aclEvaluate.checkProjectOperationPermission(projectInstance);",
      "98:         runDiagnosisCLI(args);",
      "99:         return getDiagnosisPackageName(exportPath);",
      "100:     }",
      "",
      "[Removed Lines]",
      "91:                         .getProject(ValidateUtil.convertStringToBeAlphanumericUnderscore(project));",
      "94:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), project));",
      "97:         String[] args = { project, exportPath.getAbsolutePath() };",
      "",
      "[Added Lines]",
      "89:         String projectName = ValidateUtil.convertStringToBeAlphanumericUnderscore(project);",
      "92:                         .getProject(projectName);",
      "95:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), projectName));",
      "98:         String[] args = { projectName, exportPath.getAbsolutePath() };",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "666cce471a4bb8077c29bb9d7c03a15888e25a4b",
      "candidate_info": {
        "commit_hash": "666cce471a4bb8077c29bb9d7c03a15888e25a4b",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/666cce471a4bb8077c29bb9d7c03a15888e25a4b",
        "files": [
          "webapp/app/js/controllers/query.js",
          "webapp/app/partials/query/query.html"
        ],
        "message": "KYLIN-4527 Beautify the drop-down list of the cube on query page\n\n(cherry picked from commit a4cf4b2f6d1b18738b7970e16e2f50b8518530c4)",
        "before_after_code_files": [
          "webapp/app/js/controllers/query.js||webapp/app/js/controllers/query.js",
          "webapp/app/partials/query/query.html||webapp/app/partials/query/query.html"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "webapp/app/js/controllers/query.js||webapp/app/js/controllers/query.js": [
          "File: webapp/app/js/controllers/query.js -> webapp/app/js/controllers/query.js",
          "--- Hunk 1 ---",
          "[Context before]",
          "57:         };",
          "59:         $scope.locationChangeConfirmed = false;",
          "61:         $scope.cubeItems = [];",
          "63:         var Query = {",
          "",
          "[Removed Lines]",
          "60:         $scope.selectDirective = null;",
          "",
          "[Added Lines]",
          "60:         $scope.specifyCube = null;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "253:         $scope.query = function (query) {",
          "254:             scrollToButton();",
          "255:             var backdoorToggles = null;",
          "258:             }",
          "259:             QueryService.query({}, {sql: query.sql, offset: 0, limit: $scope.rowsPerPage, acceptPartial: query.acceptPartial, project: query.project, backdoorToggles: backdoorToggles}, function (result) {",
          "260:                 scrollToButton();",
          "",
          "[Removed Lines]",
          "256:             if ($scope.selectDirective.selected != null && $scope.selectDirective.selected !== \"-- All cubes in current project --\") {",
          "257:               backdoorToggles = {\"DEBUG_TOGGLE_HIT_CUBE\": $scope.selectDirective.selected};",
          "",
          "[Added Lines]",
          "256:             if ($scope.specifyCube != null) {",
          "257:               backdoorToggles = {\"DEBUG_TOGGLE_HIT_CUBE\": $scope.specifyCube};",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "403:             });",
          "404:         }",
          "410:         var saveQueryController = function ($scope, $modalInstance, curQuery, QueryService) {",
          "411:             $scope.curQuery = curQuery;",
          "",
          "[Removed Lines]",
          "406:         $scope.getSelected = function (selected) {",
          "407:           $scope.selectDirective = selected;",
          "408:         }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "426:           $scope.listCachedQueries();",
          "427:           $scope.listSavedQueries();",
          "428:           $scope.cubeItems = [];",
          "433:           CubeService.list({projectName:newValue}, function (_cubes) {",
          "434:             if (_cubes !== 0) {",
          "435:               angular.forEach(_cubes,function(cube){",
          "",
          "[Removed Lines]",
          "429:           $scope.cubeItems.push(\"-- All cubes in current project --\")",
          "430:           if ($scope.selectDirective != null) {",
          "431:             $scope.selectDirective.clearSelected();",
          "432:           }",
          "",
          "[Added Lines]",
          "425:           $scope.specifyCube = null;",
          "",
          "---------------"
        ],
        "webapp/app/partials/query/query.html||webapp/app/partials/query/query.html": [
          "File: webapp/app/partials/query/query.html -> webapp/app/partials/query/query.html",
          "--- Hunk 1 ---",
          "[Context before]",
          "65:                         <h4>",
          "66:                             Project: <span class=\"label label-info\">{{projectModel.getSelectedProject()}}</span>",
          "67:                             &nbsp&nbsp Cube:",
          "74:                             <i class=\"fa fa-info-circle\" title=\"Specify cube to query\"  kylinpopover template=\"specifyCubeTip.html\" placement=\"right\"></i>",
          "75:                         </h4>",
          "76:                     </div>",
          "",
          "[Removed Lines]",
          "68:                             <ui-select style=\"display: inline-flex; width: 250px\" ng-model=\"mandatary_cube\" ng-init=\"getSelected($select)\">",
          "69:                               <ui-select-match placeholder=\"Filter ...\" style=\"overflow: auto;width: 250px\">{{$select.selected}}</ui-select-match>",
          "70:                               <ui-select-choices repeat=\"cube in cubeItems | filter:$select.search\" style=\"word-break:break-all;\">",
          "71:                                 <span ng-bind-html=\"cube | highlight: $select.search\"></span>",
          "72:                               </ui-select-choices>",
          "73:                             </ui-select>",
          "",
          "[Added Lines]",
          "68:                             <select chosen ng-model=\"specifyCube\"",
          "69:                                     ng-options=\"cube for cube in cubeItems\"",
          "70:                                     style=\"display: inline-flex; width: 250px\"",
          "71:                                     data-placeholder=\"Filter ...\"",
          "72:                                     class=\"chosen-select\">",
          "73:                               <option value=\"\"> -- Choose Cube -- </option>",
          "74:                             </select>",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e1f7157dd5dd52b8f6e3e76d7dc873fe9e973f67",
      "candidate_info": {
        "commit_hash": "e1f7157dd5dd52b8f6e3e76d7dc873fe9e973f67",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/e1f7157dd5dd52b8f6e3e76d7dc873fe9e973f67",
        "files": [
          "docker/build_standalone_image.sh",
          "docker/dockerfile/standalone/Dockerfile",
          "docker/dockerfile/standalone/build_standalone_image.sh",
          "docker/dockerfile/standalone/conf/bin/kylin.sh",
          "docker/dockerfile/standalone/conf/hadoop/capacity-scheduler.xml",
          "docker/dockerfile/standalone/conf/hive/hive-site.xml",
          "docker/dockerfile/standalone/conf/kylin/kylin.properties",
          "docker/dockerfile/standalone/conf/spark/spark-defaults.conf",
          "docker/dockerfile/standalone/conf/spark/spark-env.sh",
          "docker/dockerfile/standalone/conf/zk/zoo.cfg",
          "docker/setup_standalone.sh"
        ],
        "message": "KYLIN-4913 Update docker image for Kylin 4.0 Beta",
        "before_after_code_files": [
          "docker/build_standalone_image.sh||docker/build_standalone_image.sh",
          "docker/dockerfile/standalone/build_standalone_image.sh||docker/dockerfile/standalone/build_standalone_image.sh",
          "docker/dockerfile/standalone/conf/bin/kylin.sh||docker/dockerfile/standalone/conf/bin/kylin.sh",
          "docker/dockerfile/standalone/conf/kylin/kylin.properties||docker/dockerfile/standalone/conf/kylin/kylin.properties",
          "docker/dockerfile/standalone/conf/spark/spark-defaults.conf||docker/dockerfile/standalone/conf/spark/spark-defaults.conf",
          "docker/dockerfile/standalone/conf/spark/spark-env.sh||docker/dockerfile/standalone/conf/spark/spark-env.sh",
          "docker/dockerfile/standalone/conf/zk/zoo.cfg||docker/dockerfile/standalone/conf/zk/zoo.cfg",
          "docker/setup_standalone.sh||docker/setup_standalone.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "docker/build_standalone_image.sh||docker/build_standalone_image.sh": [
          "File: docker/build_standalone_image.sh -> docker/build_standalone_image.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: echo \"start build Hadoop docker image\"",
          "",
          "[Removed Lines]",
          "26: docker build -f Dockerfile_hadoop -t hadoop2.7-all-in-one-for-kylin4 .",
          "27: docker build -f Dockerfile -t apachekylin/apache-kylin-standalone:4.0.0-alpha .",
          "",
          "[Added Lines]",
          "26: docker build -f Dockerfile_hadoop -t hadoop2.7-all-in-one-for-kylin4-beta .",
          "27: docker build -f Dockerfile -t apachekylin/apache-kylin-standalone:4.0.0-beta .",
          "",
          "---------------"
        ],
        "docker/dockerfile/standalone/build_standalone_image.sh||docker/dockerfile/standalone/build_standalone_image.sh": [
          "File: docker/dockerfile/standalone/build_standalone_image.sh -> docker/dockerfile/standalone/build_standalone_image.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env bash",
          "3: #",
          "4: # Licensed to the Apache Software Foundation (ASF) under one or more",
          "5: # contributor license agreements.  See the NOTICE file distributed with",
          "6: # this work for additional information regarding copyright ownership.",
          "7: # The ASF licenses this file to You under the Apache License, Version 2.0",
          "8: # (the \"License\"); you may not use this file except in compliance with",
          "9: # the License.  You may obtain a copy of the License at",
          "10: #",
          "11: #    http://www.apache.org/licenses/LICENSE-2.0",
          "12: #",
          "13: # Unless required by applicable law or agreed to in writing, software",
          "14: # distributed under the License is distributed on an \"AS IS\" BASIS,",
          "15: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
          "16: # See the License for the specific language governing permissions and",
          "17: # limitations under the License.",
          "18: #",
          "20: DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"",
          "21: cd ${DIR}",
          "22: echo \"build image in dir \"${DIR}",
          "25: echo \"start build Hadoop docker image\"",
          "26: docker build -f Dockerfile_hadoop -t hadoop2.7-all-in-one-for-kylin4-beta .",
          "27: docker build -f Dockerfile -t apachekylin/apache-kylin-standalone:4.0.0-beta .",
          "",
          "---------------"
        ],
        "docker/dockerfile/standalone/conf/bin/kylin.sh||docker/dockerfile/standalone/conf/bin/kylin.sh": [
          "File: docker/dockerfile/standalone/conf/bin/kylin.sh -> docker/dockerfile/standalone/conf/bin/kylin.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/bin/bash",
          "3: #",
          "4: # Licensed to the Apache Software Foundation (ASF) under one or more",
          "5: # contributor license agreements.  See the NOTICE file distributed with",
          "6: # this work for additional information regarding copyright ownership.",
          "7: # The ASF licenses this file to You under the Apache License, Version 2.0",
          "8: # (the \"License\"); you may not use this file except in compliance with",
          "9: # the License.  You may obtain a copy of the License at",
          "10: #",
          "11: #    http://www.apache.org/licenses/LICENSE-2.0",
          "12: #",
          "13: # Unless required by applicable law or agreed to in writing, software",
          "14: # distributed under the License is distributed on an \"AS IS\" BASIS,",
          "15: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
          "16: # See the License for the specific language governing permissions and",
          "17: # limitations under the License.",
          "18: #",
          "20: # set verbose=true to print more logs during start up",
          "25: source ${KYLIN_HOME:-\"$(cd -P -- \"$(dirname -- \"$0\")\" && pwd -P)/../\"}/bin/header.sh $@",
          "26: if [ \"$verbose\" = true ]; then",
          "27:     shift",
          "28: fi",
          "30: mkdir -p ${KYLIN_HOME}/logs",
          "31: mkdir -p ${KYLIN_HOME}/ext",
          "33: source ${dir}/set-java-home.sh",
          "35: function retrieveDependency() {",
          "36:     #retrive $hive_dependency and $hbase_dependency",
          "37:     if [[ -z $reload_dependency && `ls -1 ${dir}/cached-* 2>/dev/null | wc -l` -eq 6 ]]",
          "38:     then",
          "39:         echo \"Using cached dependency...\"",
          "40:         source ${dir}/cached-hive-dependency.sh",
          "41:         #retrive $hbase_dependency",
          "42:         metadataUrl=`${dir}/get-properties.sh kylin.metadata.url`",
          "43:         if [[ \"${metadataUrl##*@}\" == \"hbase\" ]]",
          "44:         then",
          "45:             source ${dir}/cached-hbase-dependency.sh",
          "46:         fi",
          "47:         source ${dir}/cached-hadoop-conf-dir.sh",
          "48:         # source ${dir}/cached-kafka-dependency.sh",
          "49:         source ${dir}/cached-spark-dependency.sh",
          "50:         # source ${dir}/cached-flink-dependency.sh",
          "51:     else",
          "52:         source ${dir}/find-hive-dependency.sh",
          "53:         #retrive $hbase_dependency",
          "54:         metadataUrl=`${dir}/get-properties.sh kylin.metadata.url`",
          "55:         if [[ \"${metadataUrl##*@}\" == \"hbase\" ]]",
          "56:         then",
          "57:             source ${dir}/find-hbase-dependency.sh",
          "58:         fi",
          "59:         source ${dir}/find-hadoop-conf-dir.sh",
          "60:         # source ${dir}/find-kafka-dependency.sh",
          "61:         source ${dir}/find-spark-dependency.sh",
          "62:         # source ${dir}/find-flink-dependency.sh",
          "63:     fi",
          "65:     # Replace jars for different hadoop dist",
          "66:     bash ${dir}/replace-jars-under-spark.sh",
          "68:     # get hdp_version",
          "69:     if [ -z \"${hdp_version}\" ]; then",
          "70:         hdp_version=`/bin/bash -x hadoop 2>&1 | sed -n \"s/\\(.*\\)export HDP_VERSION=\\(.*\\)/\\2/\"p`",
          "71:         verbose \"hdp_version is ${hdp_version}\"",
          "72:     fi",
          "74:     # Replace jars for HDI",
          "75:     KYLIN_SPARK_JARS_HOME=\"${KYLIN_HOME}/spark/jars\"",
          "76:     if [[ -d \"/usr/hdp/current/hdinsight-zookeeper\" && $hdp_version == \"2\"* ]]",
          "77:     then",
          "78:        echo \"The current Hadoop environment is HDI3, will replace some jars package for ${KYLIN_HOME}/spark/jars\"",
          "79:        if [[ -f ${KYLIN_HOME}/tomcat/webapps/kylin.war ]]",
          "80:        then",
          "81:           if [[ ! -d ${KYLIN_HOME}/tomcat/webapps/kylin ]]",
          "82:           then",
          "83:              mkdir ${KYLIN_HOME}/tomcat/webapps/kylin",
          "84:           fi",
          "85:           mv ${KYLIN_HOME}/tomcat/webapps/kylin.war ${KYLIN_HOME}/tomcat/webapps/kylin",
          "86:           cd ${KYLIN_HOME}/tomcat/webapps/kylin",
          "87:           jar -xf ${KYLIN_HOME}/tomcat/webapps/kylin/kylin.war",
          "88:           if [[ -f ${KYLIN_HOME}/tomcat/webapps/kylin/WEB-INF/lib/guava-14.0.jar ]]",
          "89:           then",
          "90:              echo \"Remove ${KYLIN_HOME}/tomcat/webapps/kylin/WEB-INF/lib/guava-14.0.jar to avoid version conflicts\"",
          "91:              rm -rf ${KYLIN_HOME}/tomcat/webapps/kylin/WEB-INF/lib/guava-14.0.jar",
          "92:              rm -rf ${KYLIN_HOME}/tomcat/webapps/kylin/kylin.war",
          "93:              cd ${KYLIN_HOME}/",
          "94:           fi",
          "95:        fi",
          "97:        if [[ -d \"${KYLIN_SPARK_JARS_HOME}\" ]]",
          "98:        then",
          "99:           if [[ -f ${KYLIN_HOME}/hdi3_spark_jars_flag ]]",
          "100:           then",
          "101:           echo \"Required jars have been added to ${KYLIN_HOME}/spark/jars, skip this step.\"",
          "102:           else",
          "103:              rm -rf ${KYLIN_HOME}/spark/jars/hadoop-*",
          "104:              cp /usr/hdp/current/spark2-client/jars/hadoop-* $KYLIN_SPARK_JARS_HOME",
          "105:              cp /usr/hdp/current/spark2-client/jars/azure-* $KYLIN_SPARK_JARS_HOME",
          "106:              cp /usr/hdp/current/hadoop-client/lib/microsoft-log4j-etwappender-1.0.jar $KYLIN_SPARK_JARS_HOME",
          "107:              cp /usr/hdp/current/hadoop-client/lib/hadoop-lzo-0.6.0.${hdp_version}.jar $KYLIN_SPARK_JARS_HOME",
          "109:              rm -rf $KYLIN_HOME/spark/jars/guava-14.0.1.jar",
          "110:              cp /usr/hdp/current/spark2-client/jars/guava-24.1.1-jre.jar $KYLIN_SPARK_JARS_HOME",
          "112:              echo \"Upload spark jars to HDFS\"",
          "113:              hdfs dfs -test -d /spark2_jars",
          "114:              if [ $? -eq 1 ]",
          "115:              then",
          "116:                 hdfs dfs -mkdir /spark2_jars",
          "117:              fi",
          "118:              hdfs dfs -put $KYLIN_SPARK_JARS_HOME/* /spark2_jars",
          "120:              touch ${KYLIN_HOME}/hdi3_spark_jars_flag",
          "121:           fi",
          "122:        else",
          "123:           echo \"${KYLIN_HOME}/spark/jars dose not exist. You can run ${KYLIN_HOME}/download-spark.sh to download spark.\"",
          "124:        fi",
          "125:     fi",
          "127:     tomcat_root=${dir}/../tomcat",
          "128:     export tomcat_root",
          "130:     # get KYLIN_REST_ADDRESS",
          "131:     if [ -z \"$KYLIN_REST_ADDRESS\" ]",
          "132:     then",
          "133:         KYLIN_REST_ADDRESS=`hostname -f`\":\"`grep \"<Connector port=\" ${tomcat_root}/conf/server.xml |grep protocol=\\\"HTTP/1.1\\\" | cut -d '=' -f 2 | cut -d \\\" -f 2`",
          "134:         export KYLIN_REST_ADDRESS",
          "135:         verbose \"KYLIN_REST_ADDRESS is ${KYLIN_REST_ADDRESS}\"",
          "136:     fi",
          "137:     # the number of Spring active profiles can be greater than 1. Additional profiles",
          "138:     # can be added by setting kylin.security.additional-profiles",
          "139:     additional_security_profiles=`bash ${dir}/get-properties.sh kylin.security.additional-profiles`",
          "140:     if [[ \"x${additional_security_profiles}\" != \"x\" ]]; then",
          "141:         spring_profile=\"${spring_profile},${additional_security_profiles}\"",
          "142:     fi",
          "144:     # compose hadoop_dependencies",
          "145:     hadoop_dependencies=${hadoop_dependencies}:`hadoop classpath`",
          "146: #    if [ -n \"${hbase_dependency}\" ]; then",
          "147: #        hadoop_dependencies=${hadoop_dependencies}:${hbase_dependency}",
          "148: #    fi",
          "149:     if [ -n \"${hive_dependency}\" ]; then",
          "150:         #hadoop_dependencies=${hadoop_dependencies}:${hive_dependency}",
          "151:         hadoop_dependencies=${hive_dependency}:${hadoop_dependencies}",
          "152:     fi",
          "153:     if [ -n \"${kafka_dependency}\" ]; then",
          "154:         hadoop_dependencies=${hadoop_dependencies}:${kafka_dependency}",
          "155:     fi",
          "156:     if [ -n \"${spark_dependency}\" ]; then",
          "157:         #hadoop_dependencies=${hadoop_dependencies}:${spark_dependency}",
          "158:         hadoop_dependencies=${spark_dependency}:${hadoop_dependencies}",
          "159:     fi",
          "161:     # compose KYLIN_TOMCAT_CLASSPATH",
          "162:     tomcat_classpath=${tomcat_root}/bin/bootstrap.jar:${tomcat_root}/bin/tomcat-juli.jar:${tomcat_root}/lib/*",
          "163:     export KYLIN_TOMCAT_CLASSPATH=${tomcat_classpath}:${KYLIN_HOME}/conf:${KYLIN_HOME}/lib/*:${KYLIN_HOME}/ext/*:${hadoop_dependencies}:${flink_dependency}",
          "165:     # compose KYLIN_TOOL_CLASSPATH",
          "166:     export KYLIN_TOOL_CLASSPATH=${KYLIN_HOME}/conf:${KYLIN_HOME}/tool/*:${KYLIN_HOME}/ext/*:${hadoop_dependencies}",
          "168:     # compose kylin_common_opts",
          "169:     kylin_common_opts=\"-Dkylin.hive.dependency=${hive_dependency} \\",
          "170:     -Dkylin.kafka.dependency=${kafka_dependency} \\",
          "171:     -Dkylin.hadoop.conf.dir=${kylin_hadoop_conf_dir} \\",
          "172:     -Dkylin.server.host-address=${KYLIN_REST_ADDRESS} \\",
          "173:     -Dspring.profiles.active=${spring_profile} \\",
          "174:     -Dhdp.version=${hdp_version}\"",
          "176:     # compose KYLIN_TOMCAT_OPTS",
          "177:     KYLIN_TOMCAT_OPTS=\"-Dlog4j.configuration=file:${KYLIN_HOME}/conf/kylin-server-log4j.properties \\",
          "178:     -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager \\",
          "179:     -Dorg.apache.tomcat.util.buf.UDecoder.ALLOW_ENCODED_SLASH=true \\",
          "180:     -Dorg.apache.catalina.connector.CoyoteAdapter.ALLOW_BACKSLASH=true \\",
          "181:     -Djava.endorsed.dirs=${tomcat_root}/endorsed  \\",
          "182:     -Dcatalina.base=${tomcat_root} \\",
          "183:     -Dcatalina.home=${tomcat_root} \\",
          "184:     -Djava.io.tmpdir=${tomcat_root}/temp ${kylin_common_opts}\"",
          "185:     export KYLIN_TOMCAT_OPTS",
          "187:     # compose KYLIN_TOOL_OPTS",
          "188:     KYLIN_TOOL_OPTS=\"-Dlog4j.configuration=file:${KYLIN_HOME}/conf/kylin-tools-log4j.properties ${kylin_common_opts}\"",
          "189:     export KYLIN_TOOL_OPTS",
          "190: }",
          "192: function checkBasicKylinProps() {",
          "193:     spring_profile=`${dir}/get-properties.sh kylin.security.profile`",
          "194:     if [ -z \"$spring_profile\" ]",
          "195:     then",
          "196:         quit 'Please set kylin.security.profile in kylin.properties, options are: testing, ldap, saml.'",
          "197:     else",
          "198:         verbose \"kylin.security.profile is $spring_profile\"",
          "199:     fi",
          "200: }",
          "202: function prepareFairScheduler() {",
          "203:     cat > ${KYLIN_HOME}/conf/fairscheduler.xml <<EOL",
          "204: <?xml version=\"1.0\"?>",
          "205: <!--",
          "206:    Licensed to the Apache Software Foundation (ASF) under one or more",
          "207:    contributor license agreements.  See the NOTICE file distributed with",
          "208:    this work for additional information regarding copyright ownership.",
          "209:    The ASF licenses this file to You under the Apache License, Version 2.0",
          "210:    (the \"License\"); you may not use this file except in compliance with",
          "211:    the License.  You may obtain a copy of the License at",
          "212:        http://www.apache.org/licenses/LICENSE-2.0",
          "213:    Unless required by applicable law or agreed to in writing, software",
          "214:    distributed under the License is distributed on an \"AS IS\" BASIS,",
          "215:    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
          "216:    See the License for the specific language governing permissions and",
          "217:    limitations under the License.",
          "218: -->",
          "219: <allocations>",
          "220:   <pool name=\"query_pushdown\">",
          "221:     <schedulingMode>FAIR</schedulingMode>",
          "222:     <weight>1</weight>",
          "223:     <minShare>1</minShare>",
          "224:   </pool>",
          "225:   <pool name=\"heavy_tasks\">",
          "226:     <schedulingMode>FAIR</schedulingMode>",
          "227:     <weight>5</weight>",
          "228:     <minShare>1</minShare>",
          "229:   </pool>",
          "230:   <pool name=\"lightweight_tasks\">",
          "231:     <schedulingMode>FAIR</schedulingMode>",
          "232:     <weight>10</weight>",
          "233:     <minShare>1</minShare>",
          "234:   </pool>",
          "235:   <pool name=\"vip_tasks\">",
          "236:     <schedulingMode>FAIR</schedulingMode>",
          "237:     <weight>15</weight>",
          "238:     <minShare>1</minShare>",
          "239:   </pool>",
          "240: </allocations>",
          "241: EOL",
          "242: }",
          "244: function checkRestPort() {",
          "245:     kylin_rest_address_arr=(${KYLIN_REST_ADDRESS//:/ })",
          "246:     inuse=`netstat -tlpn | grep \"\\b${kylin_rest_address_arr[1]}\\b\"`",
          "247:     [[ -z ${inuse} ]] || quit \"Port ${kylin_rest_address_arr[1]} is not available. Another kylin server is running?\"",
          "248: }",
          "251: function classpathDebug() {",
          "252:     if [ \"${KYLIN_CLASSPATH_DEBUG}\" != \"\" ]; then",
          "253:         echo \"Finding ${KYLIN_CLASSPATH_DEBUG} on classpath\" $@",
          "254:         $JAVA -classpath $@ org.apache.kylin.common.util.ClasspathScanner ${KYLIN_CLASSPATH_DEBUG}",
          "255:     fi",
          "256: }",
          "258: function runTool() {",
          "260:     retrieveDependency",
          "262:     # get KYLIN_EXTRA_START_OPTS",
          "263:     if [ -f \"${KYLIN_HOME}/conf/setenv-tool.sh\" ]; then",
          "264:         source ${KYLIN_HOME}/conf/setenv-tool.sh",
          "265:     fi",
          "267:     verbose \"java opts for tool is ${KYLIN_EXTRA_START_OPTS} ${KYLIN_TOOL_OPTS}\"",
          "268:     verbose \"java classpath for tool is ${KYLIN_TOOL_CLASSPATH}\"",
          "269:     classpathDebug ${KYLIN_TOOL_CLASSPATH}",
          "271:     exec $JAVA ${KYLIN_EXTRA_START_OPTS} ${KYLIN_TOOL_OPTS} -classpath ${KYLIN_TOOL_CLASSPATH}  \"$@\"",
          "272: }",
          "274: if [ \"$2\" == \"--reload-dependency\" ]",
          "275: then",
          "276:     reload_dependency=1",
          "277: fi",
          "279: # start command",
          "280: if [ \"$1\" == \"start\" ]",
          "281: then",
          "282:     if [ -f \"${KYLIN_HOME}/pid\" ]",
          "283:     then",
          "284:         PID=`cat $KYLIN_HOME/pid`",
          "285:         if ps -p $PID > /dev/null",
          "286:         then",
          "287:           quit \"Kylin is running, stop it first\"",
          "288:         fi",
          "289:     fi",
          "291:     checkBasicKylinProps",
          "293:     source ${dir}/check-env.sh",
          "295:     retrieveDependency",
          "297:     checkRestPort",
          "299:     prepareFairScheduler",
          "301:     ${KYLIN_HOME}/bin/check-migration-acl.sh || { exit 1; }",
          "303:     # get KYLIN_EXTRA_START_OPTS",
          "304:     if [ -f \"${KYLIN_HOME}/conf/setenv.sh\" ]; then",
          "305:         source ${KYLIN_HOME}/conf/setenv.sh",
          "306:     fi",
          "308:     security_ldap_truststore=`bash ${dir}/get-properties.sh kylin.security.ldap.connection-truststore`",
          "309:     if [ -f \"${security_ldap_truststore}\" ]; then",
          "310:         KYLIN_EXTRA_START_OPTS=\"$KYLIN_EXTRA_START_OPTS -Djavax.net.ssl.trustStore=$security_ldap_truststore\"",
          "311:     fi",
          "313:     verbose \"java opts is ${KYLIN_EXTRA_START_OPTS} ${KYLIN_TOMCAT_OPTS}\"",
          "314:     verbose \"java classpath is ${KYLIN_TOMCAT_CLASSPATH}\"",
          "315:     classpathDebug ${KYLIN_TOMCAT_CLASSPATH}",
          "316:     $JAVA ${KYLIN_EXTRA_START_OPTS} ${KYLIN_TOMCAT_OPTS} -classpath ${KYLIN_TOMCAT_CLASSPATH}  org.apache.catalina.startup.Bootstrap start >> ${KYLIN_HOME}/logs/kylin.out 2>&1 & echo $! > ${KYLIN_HOME}/pid &",
          "318:     echo \"\"",
          "319:     echo \"A new Kylin instance is started by $USER. To stop it, run 'kylin.sh stop'\"",
          "320:     echo \"Check the log at ${KYLIN_HOME}/logs/kylin.log\"",
          "321:     echo \"Web UI is at http://${KYLIN_REST_ADDRESS}/kylin\"",
          "322:     exit 0",
          "324: # run command",
          "325: elif [ \"$1\" == \"run\" ]",
          "326: then",
          "327:     retrieveStartCommand",
          "328:     ${start_command}",
          "330: # stop command",
          "331: elif [ \"$1\" == \"stop\" ]",
          "332: then",
          "333:     if [ -f \"${KYLIN_HOME}/pid\" ]",
          "334:     then",
          "335:         PID=`cat $KYLIN_HOME/pid`",
          "336:         WAIT_TIME=2",
          "337:         LOOP_COUNTER=10",
          "338:         if ps -p $PID > /dev/null",
          "339:         then",
          "340:             echo \"Stopping Kylin: $PID\"",
          "341:             kill $PID",
          "343:             for ((i=0; i<$LOOP_COUNTER; i++))",
          "344:             do",
          "345:                 # wait to process stopped",
          "346:                 sleep $WAIT_TIME",
          "347:                 if ps -p $PID > /dev/null ; then",
          "348:                     echo \"Stopping in progress. Will check after $WAIT_TIME secs again...\"",
          "349:                     continue;",
          "350:                 else",
          "351:                     break;",
          "352:                 fi",
          "353:             done",
          "355:             # if process is still around, use kill -9",
          "356:             if ps -p $PID > /dev/null",
          "357:             then",
          "358:                 echo \"Initial kill failed, getting serious now...\"",
          "359:                 kill -9 $PID",
          "360:                 sleep 1 #give kill -9  sometime to \"kill\"",
          "361:                 if ps -p $PID > /dev/null",
          "362:                 then",
          "363:                    quit \"Warning, even kill -9 failed, giving up! Sorry...\"",
          "364:                 fi",
          "365:             fi",
          "367:             # process is killed , remove pid file",
          "368:             rm -rf ${KYLIN_HOME}/pid",
          "369:             echo \"Kylin with pid ${PID} has been stopped.\"",
          "370:             exit 0",
          "371:         else",
          "372:            quit \"Kylin with pid ${PID} is not running\"",
          "373:         fi",
          "374:     else",
          "375:         quit \"Kylin is not running\"",
          "376:     fi",
          "378: # streaming command",
          "379: elif [ \"$1\" == \"streaming\" ]",
          "380: then",
          "381:     if [ $# -lt 2 ]",
          "382:     then",
          "383:         echo \"Invalid input args $@\"",
          "384:         exit -1",
          "385:     fi",
          "386:     if [ \"$2\" == \"start\" ]",
          "387:     then",
          "388:         if [ -f \"${KYLIN_HOME}/streaming_receiver_pid\" ]",
          "389:         then",
          "390:             PID=`cat $KYLIN_HOME/streaming_receiver_pid`",
          "391:             if ps -p $PID > /dev/null",
          "392:             then",
          "393:               echo \"Kylin streaming receiver is running, stop it first\"",
          "394:             exit 1",
          "395:             fi",
          "396:         fi",
          "397:         #retrive $hbase_dependency",
          "398:         metadataUrl=`${dir}/get-properties.sh kylin.metadata.url`",
          "399:         if [[ \"${metadataUrl##*@}\" == \"hbase\" ]]",
          "400:         then",
          "401:             source ${dir}/find-hbase-dependency.sh",
          "402:         fi",
          "403:         #retrive $KYLIN_EXTRA_START_OPTS",
          "404:         if [ -f \"${KYLIN_HOME}/conf/setenv.sh\" ]",
          "405:             then source ${KYLIN_HOME}/conf/setenv.sh",
          "406:         fi",
          "408:         mkdir -p ${KYLIN_HOME}/ext",
          "409:         HBASE_CLASSPATH=`hbase classpath`",
          "410:         #echo \"hbase class path:\"$HBASE_CLASSPATH",
          "411:         STREAM_CLASSPATH=${KYLIN_HOME}/lib/streaming/*:${KYLIN_HOME}/ext/*:${HBASE_CLASSPATH}",
          "413:         # KYLIN_EXTRA_START_OPTS is for customized settings, checkout bin/setenv.sh",
          "414:         $JAVA -cp $STREAM_CLASSPATH ${KYLIN_EXTRA_START_OPTS} \\",
          "415:         -Dlog4j.configuration=stream-receiver-log4j.properties\\",
          "416:         -DKYLIN_HOME=${KYLIN_HOME}\\",
          "417:         -Dkylin.hbase.dependency=${hbase_dependency} \\",
          "418:         org.apache.kylin.stream.server.StreamingReceiver $@ > ${KYLIN_HOME}/logs/streaming_receiver.out 2>&1 & echo $! > ${KYLIN_HOME}/streaming_receiver_pid &",
          "419:         exit 0",
          "420:     elif [ \"$2\" == \"stop\" ]",
          "421:     then",
          "422:         if [ ! -f \"${KYLIN_HOME}/streaming_receiver_pid\" ]",
          "423:         then",
          "424:             echo \"Streaming receiver is not running, please check\"",
          "425:             exit 1",
          "426:         fi",
          "427:         PID=`cat ${KYLIN_HOME}/streaming_receiver_pid`",
          "428:         if [ \"$PID\" = \"\" ]",
          "429:         then",
          "430:             echo \"Streaming receiver is not running, please check\"",
          "431:             exit 1",
          "432:         else",
          "433:             echo \"Stopping streaming receiver: $PID\"",
          "434:             WAIT_TIME=2",
          "435:             LOOP_COUNTER=20",
          "436:             if ps -p $PID > /dev/null",
          "437:             then",
          "438:                 kill $PID",
          "440:                 for ((i=0; i<$LOOP_COUNTER; i++))",
          "441:                 do",
          "442:                     # wait to process stopped",
          "443:                     sleep $WAIT_TIME",
          "444:                     if ps -p $PID > /dev/null ; then",
          "445:                         echo \"Stopping in progress. Will check after $WAIT_TIME secs again...\"",
          "446:                         continue;",
          "447:                     else",
          "448:                         break;",
          "449:                     fi",
          "450:                 done",
          "452:                 # if process is still around, use kill -9",
          "453:                 if ps -p $PID > /dev/null",
          "454:                 then",
          "455:                     echo \"Initial kill failed, getting serious now...\"",
          "456:                     kill -9 $PID",
          "457:                     sleep 1 #give kill -9  sometime to \"kill\"",
          "458:                     if ps -p $PID > /dev/null",
          "459:                     then",
          "460:                        quit \"Warning, even kill -9 failed, giving up! Sorry...\"",
          "461:                     fi",
          "462:                 fi",
          "464:                 # process is killed , remove pid file",
          "465:                 rm -rf ${KYLIN_HOME}/streaming_receiver_pid",
          "466:                 echo \"Kylin streaming receiver with pid ${PID} has been stopped.\"",
          "467:                 exit 0",
          "468:             else",
          "469:                quit \"Kylin streaming receiver with pid ${PID} is not running\"",
          "470:             fi",
          "471:         fi",
          "472:     elif [[ \"$2\" = org.apache.kylin.* ]]",
          "473:     then",
          "474:         source ${KYLIN_HOME}/conf/setenv.sh",
          "475:         HBASE_CLASSPATH=`hbase classpath`",
          "476:         #echo \"hbase class path:\"$HBASE_CLASSPATH",
          "477:         STREAM_CLASSPATH=${KYLIN_HOME}/lib/streaming/*:${KYLIN_HOME}/ext/*:${HBASE_CLASSPATH}",
          "479:         shift",
          "480:         # KYLIN_EXTRA_START_OPTS is for customized settings, checkout bin/setenv.sh",
          "481:         $JAVA -cp $STREAM_CLASSPATH ${KYLIN_EXTRA_START_OPTS} \\",
          "482:         -Dlog4j.configuration=stream-receiver-log4j.properties\\",
          "483:         -DKYLIN_HOME=${KYLIN_HOME}\\",
          "484:         -Dkylin.hbase.dependency=${hbase_dependency} \\",
          "485:         \"$@\"",
          "486:         exit 0",
          "487:     fi",
          "489: elif [ \"$1\" = \"version\" ]",
          "490: then",
          "491:     runTool org.apache.kylin.common.KylinVersion",
          "493: elif [ \"$1\" = \"diag\" ]",
          "494: then",
          "495:     echo \"'kylin.sh diag' no longer supported, use diag.sh instead\"",
          "496:     exit 0",
          "498: # tool command",
          "499: elif [[ \"$1\" = org.apache.kylin.* ]]",
          "500: then",
          "501:     runTool \"$@\"",
          "502: else",
          "503:     quit \"Usage: 'kylin.sh [-v] start' or 'kylin.sh [-v] stop'\"",
          "504: fi",
          "",
          "---------------"
        ],
        "docker/dockerfile/standalone/conf/kylin/kylin.properties||docker/dockerfile/standalone/conf/kylin/kylin.properties": [
          "File: docker/dockerfile/standalone/conf/kylin/kylin.properties -> docker/dockerfile/standalone/conf/kylin/kylin.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one or more",
          "3: # contributor license agreements.  See the NOTICE file distributed with",
          "4: # this work for additional information regarding copyright ownership.",
          "5: # The ASF licenses this file to You under the Apache License, Version 2.0",
          "6: # (the \"License\"); you may not use this file except in compliance with",
          "7: # the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #    http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing, software",
          "12: # distributed under the License is distributed on an \"AS IS\" BASIS,",
          "13: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
          "14: # See the License for the specific language governing permissions and",
          "15: # limitations under the License.",
          "16: #",
          "21: # The below commented values will effect as default settings",
          "22: # Uncomment and override them if necessary",
          "26: #",
          "27: #### METADATA | ENV ###",
          "28: #",
          "29: ## The metadata store has two implementations(RDBMS/HBase), while RDBMS is recommended in Kylin 4.X",
          "30: ## Please refer to https://cwiki.apache.org/confluence/display/KYLIN/How+to+use+HBase+metastore+in+Kylin+4.0 if you prefer HBase",
          "31: #kylin.metadata.url=kylin_metadata@jdbc,url=jdbc:mysql://localhost:3306/kylin,username=XXXX,password=XXXXXX,maxActive=10,maxIdle=10",
          "32: #",
          "33: ## metadata cache sync retry times",
          "34: #kylin.metadata.sync-retries=3",
          "35: #",
          "36: ## Working folder in HDFS, better be qualified absolute path, make sure user has the right permission to this directory",
          "37: #kylin.env.hdfs-working-dir=/kylin",
          "38: #",
          "39: ## DEV|QA|PROD. DEV will turn on some dev features, QA and PROD has no difference in terms of functions.",
          "40: #kylin.env=QA",
          "41: #",
          "42: ## kylin zk base path",
          "43: #kylin.env.zookeeper-base-path=/kylin",
          "44: #",
          "45: ## Run a TestingServer for curator locally",
          "46: #kylin.env.zookeeper-is-local=false",
          "47: #",
          "48: ## Connect to a remote zookeeper with the url, should set kylin.env.zookeeper-is-local to false",
          "49: #kylin.env.zookeeper-connect-string=sandbox.hortonworks.com",
          "50: #",
          "51: #### SERVER | WEB | RESTCLIENT ###",
          "52: #",
          "53: ## Kylin server mode, valid value [all, query, job]",
          "54: #kylin.server.mode=all",
          "55: #",
          "56: ### Kylin server port",
          "57: #server.port=7070",
          "58: #",
          "59: ## List of web servers in use, this enables one web server instance to sync up with other servers.",
          "60: #kylin.server.cluster-servers=localhost:7070",
          "61: #",
          "62: ## Display timezone on UI,format like[GMT+N or GMT-N]",
          "63: #kylin.web.timezone=",
          "64: #",
          "65: ## Timeout value for the queries submitted through the Web UI, in milliseconds",
          "66: #kylin.web.query-timeout=300000",
          "67: #",
          "68: #kylin.web.cross-domain-enabled=true",
          "69: #",
          "70: ##allow user to export query result",
          "71: #kylin.web.export-allow-admin=true",
          "72: #kylin.web.export-allow-other=true",
          "73: #",
          "74: ## Hide measures in measure list of cube designer, separate by comma",
          "75: #kylin.web.hide-measures=RAW",
          "76: #",
          "77: ##max connections of one route",
          "78: #kylin.restclient.connection.default-max-per-route=20",
          "79: #",
          "80: ##max connections of one rest-client",
          "81: #kylin.restclient.connection.max-total=200",
          "82: #",
          "83: #### PUBLIC CONFIG ###",
          "84: #kylin.engine.default=6",
          "85: #kylin.storage.default=4",
          "86: #kylin.web.hive-limit=20",
          "87: #kylin.web.help.length=4",
          "88: #kylin.web.help.0=start|Getting Started|http://kylin.apache.org/docs/tutorial/kylin_sample.html",
          "89: #kylin.web.help.1=odbc|ODBC Driver|http://kylin.apache.org/docs/tutorial/odbc.html",
          "90: #kylin.web.help.2=tableau|Tableau Guide|http://kylin.apache.org/docs/tutorial/tableau_91.html",
          "91: #kylin.web.help.3=onboard|Cube Design Tutorial|http://kylin.apache.org/docs/howto/howto_optimize_cubes.html",
          "92: #kylin.web.link-streaming-guide=http://kylin.apache.org/",
          "93: #kylin.htrace.show-gui-trace-toggle=false",
          "94: #kylin.web.link-hadoop=",
          "95: #kylin.web.link-diagnostic=",
          "96: #kylin.web.contact-mail=",
          "97: #kylin.server.external-acl-provider=",
          "98: #",
          "99: ## Default time filter for job list, 0->current day, 1->last one day, 2->last one week, 3->last one year, 4->all",
          "100: #kylin.web.default-time-filter=1",
          "101: #",
          "102: #### SOURCE ###",
          "103: ## Define how to access to hive metadata",
          "104: ## When user deploy kylin on AWS EMR and Glue is used as external metadata, use gluecatalog instead",
          "105: #kylin.source.hive.metadata-type=hcatalog",
          "106: #",
          "107: ## Hive client, valid value [cli, beeline]",
          "108: #kylin.source.hive.client=cli",
          "109: #",
          "110: ## Absolute path to beeline shell, can be set to spark beeline instead of the default hive beeline on PATH",
          "111: #kylin.source.hive.beeline-shell=beeline",
          "112: #",
          "113: ## Hive database name for putting the intermediate flat tables",
          "114: #kylin.source.hive.database-for-flat-table=default",
          "115: #",
          "116: #### STORAGE ###",
          "117: #",
          "118: ## The storage for final cube file in hbase",
          "119: #kylin.storage.url=hbase",
          "120: #",
          "121: ## clean real storage after delete operation",
          "122: ## if you want to delete the real storage like htable of deleting segment, you can set it to true",
          "123: #kylin.storage.clean-after-delete-operation=false",
          "124: #",
          "125: #### JOB ###",
          "126: #",
          "127: ## Max job retry on error, default 0: no retry",
          "128: #kylin.job.retry=0",
          "129: #",
          "130: ## Max count of concurrent jobs running",
          "131: #kylin.job.max-concurrent-jobs=10",
          "132: #",
          "133: ## The percentage of the sampling, default 100%",
          "134: #kylin.job.sampling-percentage=100",
          "135: #",
          "136: ## If true, will send email notification on job complete",
          "137: ##kylin.job.notification-enabled=true",
          "138: ##kylin.job.notification-mail-enable-starttls=true",
          "139: ##kylin.job.notification-mail-host=smtp.office365.com",
          "140: ##kylin.job.notification-mail-port=587",
          "141: ##kylin.job.notification-mail-username=kylin@example.com",
          "142: ##kylin.job.notification-mail-password=mypassword",
          "143: ##kylin.job.notification-mail-sender=kylin@example.com",
          "144: #kylin.job.scheduler.provider.100=org.apache.kylin.job.impl.curator.CuratorScheduler",
          "145: #kylin.job.scheduler.default=0",
          "146: #",
          "147: #### CUBE | DICTIONARY ###",
          "148: #",
          "149: #kylin.cube.cuboid-scheduler=org.apache.kylin.cube.cuboid.DefaultCuboidScheduler",
          "150: #kylin.cube.segment-advisor=org.apache.kylin.cube.CubeSegmentAdvisor",
          "151: #",
          "152: ## 'auto', 'inmem', 'layer' or 'random' for testing",
          "153: #kylin.cube.algorithm=layer",
          "154: #",
          "155: ## A smaller threshold prefers layer, a larger threshold prefers in-mem",
          "156: #kylin.cube.algorithm.layer-or-inmem-threshold=7",
          "157: #",
          "158: ## auto use inmem algorithm:",
          "159: ## 1, cube planner optimize job",
          "160: ## 2, no source record",
          "161: #kylin.cube.algorithm.inmem-auto-optimize=true",
          "162: #",
          "163: #kylin.cube.aggrgroup.max-combination=32768",
          "164: #",
          "165: #kylin.cube.cubeplanner.enabled=false",
          "166: #kylin.cube.cubeplanner.enabled-for-existing-cube=false",
          "167: #kylin.cube.cubeplanner.expansion-threshold=15.0",
          "168: #kylin.cube.cubeplanner.recommend-cache-max-size=200",
          "169: #kylin.cube.cubeplanner.mandatory-rollup-threshold=1000",
          "170: #kylin.cube.cubeplanner.algorithm-threshold-greedy=8",
          "171: #kylin.cube.cubeplanner.algorithm-threshold-genetic=23",
          "172: #",
          "173: #### QUERY ###",
          "174: #",
          "175: ## Controls the maximum number of bytes a query is allowed to scan storage.",
          "176: ## The default value 0 means no limit.",
          "177: ## The counterpart kylin.storage.partition.max-scan-bytes sets the maximum per coprocessor.",
          "178: #kylin.query.max-scan-bytes=0",
          "179: #",
          "180: #kylin.query.cache-enabled=true",
          "181: #kylin.query.cache-threshold-scan-count=10240",
          "182: #kylin.query.cache-threshold-duration=2000",
          "183: #kylin.query.cache-threshold-scan-bytes=1048576",
          "184: #kylin.query.large-query-threshold=1000000",
          "185: #",
          "186: ## Controls extras properties for Calcite jdbc driver",
          "187: ## all extras properties should undder prefix \"kylin.query.calcite.extras-props.\"",
          "188: ## case sensitive, default: true, to enable case insensitive set it to false",
          "189: ## @see org.apache.calcite.config.CalciteConnectionProperty.CASE_SENSITIVE",
          "190: #kylin.query.calcite.extras-props.caseSensitive=true",
          "191: ## how to handle unquoted identity, defualt: TO_UPPER, available options: UNCHANGED, TO_UPPER, TO_LOWER",
          "192: ## @see org.apache.calcite.config.CalciteConnectionProperty.UNQUOTED_CASING",
          "193: #kylin.query.calcite.extras-props.unquotedCasing=TO_UPPER",
          "194: ## quoting method, default: DOUBLE_QUOTE, available options: DOUBLE_QUOTE, BACK_TICK, BRACKET",
          "195: ## @see org.apache.calcite.config.CalciteConnectionProperty.QUOTING",
          "196: #kylin.query.calcite.extras-props.quoting=DOUBLE_QUOTE",
          "197: ## change SqlConformance from DEFAULT to LENIENT to enable group by ordinal",
          "198: ## @see org.apache.calcite.sql.validate.SqlConformance.SqlConformanceEnum",
          "199: #kylin.query.calcite.extras-props.conformance=LENIENT",
          "200: #",
          "201: ## TABLE ACL",
          "202: #kylin.query.security.table-acl-enabled=true",
          "203: #",
          "204: ## Usually should not modify this",
          "205: #kylin.query.interceptors=org.apache.kylin.rest.security.TableInterceptor",
          "206: #",
          "207: #kylin.query.escape-default-keyword=false",
          "208: #",
          "209: ## Usually should not modify this",
          "210: #kylin.query.transformers=org.apache.kylin.query.util.DefaultQueryTransformer,org.apache.kylin.query.util.KeywordDefaultDirtyHack",
          "211: #",
          "212: #### SECURITY ###",
          "213: #",
          "214: ## Spring security profile, options: testing, ldap, saml",
          "215: ## with \"testing\" profile, user can use pre-defined name/pwd like KYLIN/ADMIN to login",
          "216: #kylin.security.profile=testing",
          "217: #",
          "218: ## Admin roles in LDAP, for ldap and saml",
          "219: #kylin.security.acl.admin-role=admin",
          "220: #",
          "221: ## LDAP authentication configuration",
          "222: #kylin.security.ldap.connection-server=ldap://ldap_server:389",
          "223: #kylin.security.ldap.connection-username=",
          "224: #kylin.security.ldap.connection-password=",
          "225: ## When you use the customized CA certificate library for user authentication based on LDAPs, you need to configure this item.",
          "226: ## The value of this item will be added to the JVM parameter javax.net.ssl.trustStore.",
          "227: #kylin.security.ldap.connection-truststore=",
          "228: #",
          "229: ## LDAP user account directory;",
          "230: #kylin.security.ldap.user-search-base=",
          "231: #kylin.security.ldap.user-search-pattern=",
          "232: #kylin.security.ldap.user-group-search-base=",
          "233: #kylin.security.ldap.user-group-search-filter=(|(member={0})(memberUid={1}))",
          "234: #",
          "235: ## LDAP service account directory",
          "236: #kylin.security.ldap.service-search-base=",
          "237: #kylin.security.ldap.service-search-pattern=",
          "238: #kylin.security.ldap.service-group-search-base=",
          "239: #",
          "240: ### SAML configurations for SSO",
          "241: ## SAML IDP metadata file location",
          "242: #kylin.security.saml.metadata-file=classpath:sso_metadata.xml",
          "243: #kylin.security.saml.metadata-entity-base-url=https://hostname/kylin",
          "244: #kylin.security.saml.keystore-file=classpath:samlKeystore.jks",
          "245: #kylin.security.saml.context-scheme=https",
          "246: #kylin.security.saml.context-server-name=hostname",
          "247: #kylin.security.saml.context-server-port=443",
          "248: #kylin.security.saml.context-path=/kylin",
          "249: #",
          "250: #### SPARK BUILD ENGINE CONFIGS ###",
          "251: #",
          "252: ## Hadoop conf folder, will export this as \"HADOOP_CONF_DIR\" to run spark-submit",
          "253: ## This must contain site xmls of core, yarn, hive, and hbase in one folder",
          "254: ##kylin.env.hadoop-conf-dir=/etc/hadoop/conf",
          "255: #",
          "256: ## Spark conf (default is in spark/conf/spark-defaults.conf)",
          "257: #kylin.engine.spark-conf.spark.master=yarn",
          "258: ##kylin.engine.spark-conf.spark.submit.deployMode=client",
          "259: #kylin.engine.spark-conf.spark.yarn.queue=default",
          "260: ##kylin.engine.spark-conf.spark.executor.cores=1",
          "261: ##kylin.engine.spark-conf.spark.executor.memory=4G",
          "262: ##kylin.engine.spark-conf.spark.executor.instances=1",
          "263: ##kylin.engine.spark-conf.spark.executor.memoryOverhead=1024M",
          "264: #kylin.engine.spark-conf.spark.driver.cores=1",
          "265: #kylin.engine.spark-conf.spark.driver.memory=1G",
          "266: #kylin.engine.spark-conf.spark.shuffle.service.enabled=true",
          "267: #kylin.engine.spark-conf.spark.eventLog.enabled=true",
          "268: #kylin.engine.spark-conf.spark.eventLog.dir=hdfs\\:///kylin/spark-history",
          "269: #kylin.engine.spark-conf.spark.history.fs.logDirectory=hdfs\\:///kylin/spark-history",
          "270: #kylin.engine.spark-conf.spark.hadoop.yarn.timeline-service.enabled=false",
          "271: #kylin.engine.spark-conf.spark.executor.extraJavaOptions=-Dfile.encoding=UTF-8 -Dhdp.version=current -Dlog4j.configuration=spark-executor-log4j.properties -Dlog4j.debug -Dkylin.hdfs.working.dir=${hdfs.working.dir} -Dkylin.metadata.identifier=${kylin.metadata.url.identifier} -Dkylin.spark.category=job -Dkylin.spark.project=${job.project} -Dkylin.spark.identifier=${job.id} -Dkylin.spark.jobName=${job.stepId} -Duser.timezone=${user.timezone}",
          "272: ##kylin.engine.spark-conf.spark.sql.shuffle.partitions=1",
          "273: #",
          "274: ## manually upload spark-assembly jar to HDFS and then set this property will avoid repeatedly uploading jar at runtime",
          "275: ##kylin.engine.spark-conf.spark.yarn.jars=hdfs://localhost:9000/spark2_jars/*",
          "276: ##kylin.engine.spark-conf.spark.io.compression.codec=org.apache.spark.io.SnappyCompressionCodec",
          "277: #",
          "278: ## uncomment for HDP",
          "279: ##kylin.engine.spark-conf.spark.driver.extraJavaOptions=-Dhdp.version=current",
          "280: ##kylin.engine.spark-conf.spark.yarn.am.extraJavaOptions=-Dhdp.version=current",
          "281: #",
          "282: #### SPARK QUERY ENGINE CONFIGS (a.k.a. Sparder Context) ###",
          "283: ## Enlarge cores and memory to improve query performance in production env, please check https://cwiki.apache.org/confluence/display/KYLIN/User+Manual+4.X",
          "284: #",
          "285: #kylin.query.spark-conf.spark.master=yarn",
          "286: ##kylin.query.spark-conf.spark.submit.deployMode=client",
          "287: #kylin.query.spark-conf.spark.driver.cores=1",
          "288: #kylin.query.spark-conf.spark.driver.memory=4G",
          "289: #kylin.query.spark-conf.spark.driver.memoryOverhead=1G",
          "290: #kylin.query.spark-conf.spark.executor.cores=1",
          "291: #kylin.query.spark-conf.spark.executor.instances=1",
          "292: #kylin.query.spark-conf.spark.executor.memory=4G",
          "293: #kylin.query.spark-conf.spark.executor.memoryOverhead=1G",
          "294: #kylin.query.spark-conf.spark.serializer=org.apache.spark.serializer.JavaSerializer",
          "295: ##kylin.query.spark-conf.spark.sql.shuffle.partitions=40",
          "296: ##kylin.query.spark-conf.spark.yarn.jars=hdfs://localhost:9000/spark2_jars/*",
          "297: #",
          "298: #kylin.query.spark-conf.spark.executor.extraJavaOptions=-Dhdp.version=current -Dlog4j.configuration=spark-executor-log4j.properties -Dlog4j.debug -Dkylin.hdfs.working.dir=${kylin.env.hdfs-working-dir} -Dkylin.metadata.identifier=${kylin.metadata.url.identifier} -Dkylin.spark.category=sparder -Dkylin.spark.project=${job.project}",
          "299: ## uncomment for HDP",
          "300: ##kylin.query.spark-conf.spark.driver.extraJavaOptions=-Dhdp.version=current",
          "301: ##kylin.query.spark-conf.spark.yarn.am.extraJavaOptions=-Dhdp.version=current",
          "302: #",
          "303: #### QUERY PUSH DOWN ###",
          "304: #",
          "305: ##kylin.query.pushdown.runner-class-name=org.apache.kylin.query.pushdown.PushDownRunnerSparkImpl",
          "306: ##kylin.query.pushdown.update-enabled=false",
          "308: kylin.env=QA",
          "309: kylin.server.mode=all",
          "310: kylin.server.host-address=127.0.0.1:7070",
          "311: server.port=7070",
          "312: # Display timezone on UI,format like[GMT+N or GMT-N]",
          "313: kylin.web.timezone=GMT+8",
          "315: kylin.source.hive.client=cli",
          "316: kylin.source.hive.database-for-flat-table=kylin4",
          "318: kylin.engine.spark-conf.spark.eventLog.enabled=true",
          "319: kylin.engine.spark-conf.spark.history.fs.logDirectory=hdfs://localhost:9000/kylin4/spark-history",
          "320: kylin.engine.spark-conf.spark.eventLog.dir=hdfs://localhost:9000/kylin4/spark-history",
          "321: kylin.engine.spark-conf.spark.hadoop.yarn.timeline-service.enabled=false",
          "323: kylin.engine.spark-conf.spark.yarn.submit.file.replication=1",
          "324: kylin.engine.spark-conf.spark.master=yarn",
          "325: kylin.engine.spark-conf.spark.driver.memory=512M",
          "326: kylin.engine.spark-conf.spark.driver.memoryOverhead=512M",
          "327: kylin.engine.spark-conf.spark.executor.memory=1G",
          "328: kylin.engine.spark-conf.spark.executor.instances=1",
          "329: kylin.engine.spark-conf.spark.executor.memoryOverhead=512M",
          "330: kylin.engine.spark-conf.spark.executor.cores=1",
          "331: kylin.engine.spark-conf.spark.sql.shuffle.partitions=1",
          "332: kylin.engine.spark-conf.spark.yarn.jars=hdfs://localhost:9000/spark2_jars/*",
          "334: kylin.storage.columnar.shard-rowcount=2500000",
          "335: kylin.storage.columnar.shard-countdistinct-rowcount=1000000",
          "336: kylin.storage.columnar.repartition-threshold-size-mb=128",
          "337: kylin.storage.columnar.shard-size-mb=128",
          "339: kylin.query.auto-sparder-context=true",
          "340: kylin.query.sparder-context.app-name=sparder_on_docker",
          "341: kylin.query.spark-conf.spark.master=yarn",
          "342: kylin.query.spark-conf.spark.driver.memory=512M",
          "343: kylin.query.spark-conf.spark.driver.memoryOverhead=512M",
          "344: kylin.query.spark-conf.spark.executor.memory=1G",
          "345: kylin.query.spark-conf.spark.executor.instances=1",
          "346: kylin.query.spark-conf.spark.executor.memoryOverhead=512M",
          "347: kylin.query.spark-conf.spark.executor.cores=1",
          "348: kylin.query.spark-conf.spark.serializer=org.apache.spark.serializer.JavaSerializer",
          "349: kylin.query.spark-conf.spark.sql.shuffle.partitions=1",
          "350: kylin.query.spark-conf.spark.yarn.jars=hdfs://localhost:9000/spark2_jars/*",
          "351: kylin.query.spark-conf.spark.eventLog.enabled=true",
          "352: kylin.query.spark-conf.spark.history.fs.logDirectory=hdfs://localhost:9000/kylin4/spark-history",
          "353: kylin.query.spark-conf.spark.eventLog.dir=hdfs://localhost:9000/kylin4/spark-history",
          "355: # for local cache",
          "356: kylin.query.cache-enabled=false",
          "358: # for pushdown query",
          "359: kylin.query.pushdown.update-enabled=false",
          "360: kylin.query.pushdown.enabled=true",
          "361: kylin.query.pushdown.runner-class-name=org.apache.kylin.query.pushdown.PushDownRunnerSparkImpl",
          "363: # for Cube Planner",
          "364: kylin.cube.cubeplanner.enabled=true",
          "365: kylin.server.query-metrics2-enabled=false",
          "366: kylin.metrics.reporter-query-enabled=false",
          "367: kylin.metrics.reporter-job-enabled=false",
          "368: kylin.metrics.monitor-enabled=false",
          "369: kylin.web.dashboard-enabled=false",
          "371: # metadata for mysql",
          "372: kylin.metadata.url=kylin4@jdbc,url=jdbc:mysql://localhost:3306/kylin4,username=root,password=123456,maxActive=10,maxIdle=10",
          "373: kylin.env.hdfs-working-dir=/kylin4_metadata",
          "374: kylin.env.zookeeper-base-path=/kylin4",
          "375: kylin.env.zookeeper-connect-string=127.0.0.1",
          "377: kylin.storage.clean-after-delete-operation=true",
          "",
          "---------------"
        ],
        "docker/dockerfile/standalone/conf/spark/spark-defaults.conf||docker/dockerfile/standalone/conf/spark/spark-defaults.conf": [
          "File: docker/dockerfile/standalone/conf/spark/spark-defaults.conf -> docker/dockerfile/standalone/conf/spark/spark-defaults.conf",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one or more",
          "3: # contributor license agreements.  See the NOTICE file distributed with",
          "4: # this work for additional information regarding copyright ownership.",
          "5: # The ASF licenses this file to You under the Apache License, Version 2.0",
          "6: # (the \"License\"); you may not use this file except in compliance with",
          "7: # the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #    http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing, software",
          "12: # distributed under the License is distributed on an \"AS IS\" BASIS,",
          "13: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
          "14: # See the License for the specific language governing permissions and",
          "15: # limitations under the License.",
          "16: #",
          "18: # Default system properties included when running spark-submit.",
          "19: # This is useful for setting default environmental settings.",
          "21: # Example:",
          "22: # spark.master                     spark://master:7077",
          "23: # spark.eventLog.enabled           true",
          "24: # spark.eventLog.dir               hdfs://namenode:8021/directory",
          "25: # spark.serializer                 org.apache.spark.serializer.KryoSerializer",
          "26: # spark.driver.memory              5g",
          "27: # spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=\"one two three\"",
          "29: spark.sql.catalogImplementation         hive",
          "30: spark.driver.maxResultSize              1g",
          "31: spark.sql.hive.thriftServer.singleSession  false",
          "33: spark.serializer                      org.apache.spark.serializer.JavaSerializer",
          "35: spark.memory.useLegacyMode            false",
          "36: spark.memory.fraction                 0.3",
          "37: spark.memory.storageFraction          0.3",
          "39: spark.rdd.compress                    true",
          "40: spark.io.compression.codec            snappy",
          "42: spark.locality.wait                     100ms",
          "43: spark.speculation                       false",
          "45: spark.task.maxFailures                  4",
          "47: spark.scheduler.minRegisteredResourcesRatio         1.0",
          "48: spark.scheduler.maxRegisteredResourcesWaitingTime 60s",
          "50: spark.yarn.jars                        hdfs://localhost:9000/spark2_jars/*",
          "",
          "---------------"
        ],
        "docker/dockerfile/standalone/conf/spark/spark-env.sh||docker/dockerfile/standalone/conf/spark/spark-env.sh": [
          "File: docker/dockerfile/standalone/conf/spark/spark-env.sh -> docker/dockerfile/standalone/conf/spark/spark-env.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env bash",
          "3: #",
          "4: # Licensed to the Apache Software Foundation (ASF) under one or more",
          "5: # contributor license agreements.  See the NOTICE file distributed with",
          "6: # this work for additional information regarding copyright ownership.",
          "7: # The ASF licenses this file to You under the Apache License, Version 2.0",
          "8: # (the \"License\"); you may not use this file except in compliance with",
          "9: # the License.  You may obtain a copy of the License at",
          "10: #",
          "11: #    http://www.apache.org/licenses/LICENSE-2.0",
          "12: #",
          "13: # Unless required by applicable law or agreed to in writing, software",
          "14: # distributed under the License is distributed on an \"AS IS\" BASIS,",
          "15: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
          "16: # See the License for the specific language governing permissions and",
          "17: # limitations under the License.",
          "18: #",
          "20: # This file is sourced when running various Spark programs.",
          "21: # Copy it as spark-env.sh and edit that to configure Spark for your site.",
          "23: # Options read when launching programs locally with",
          "24: # ./bin/run-example or ./bin/spark-submit",
          "25: # - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files",
          "26: # - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node",
          "27: # - SPARK_PUBLIC_DNS, to set the public dns name of the driver program",
          "29: # Options read by executors and drivers running inside the cluster",
          "30: # - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node",
          "31: # - SPARK_PUBLIC_DNS, to set the public DNS name of the driver program",
          "32: # - SPARK_LOCAL_DIRS, storage directories to use on this node for shuffle and RDD data",
          "33: # - MESOS_NATIVE_JAVA_LIBRARY, to point to your libmesos.so if you use Mesos",
          "35: # Options read in YARN client/cluster mode",
          "36: # - SPARK_CONF_DIR, Alternate conf dir. (Default: ${SPARK_HOME}/conf)",
          "37: # - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files",
          "38: # - YARN_CONF_DIR, to point Spark towards YARN configuration files when you use YARN",
          "39: # - SPARK_EXECUTOR_CORES, Number of cores for the executors (Default: 1).",
          "40: # - SPARK_EXECUTOR_MEMORY, Memory per Executor (e.g. 1000M, 2G) (Default: 1G)",
          "41: # - SPARK_DRIVER_MEMORY, Memory for Driver (e.g. 1000M, 2G) (Default: 1G)",
          "43: # Options for the daemons used in the standalone deploy mode",
          "44: # - SPARK_MASTER_HOST, to bind the master to a different IP address or hostname",
          "45: # - SPARK_MASTER_PORT / SPARK_MASTER_WEBUI_PORT, to use non-default ports for the master",
          "46: # - SPARK_MASTER_OPTS, to set config properties only for the master (e.g. \"-Dx=y\")",
          "47: # - SPARK_WORKER_CORES, to set the number of cores to use on this machine",
          "48: # - SPARK_WORKER_MEMORY, to set how much total memory workers have to give executors (e.g. 1000m, 2g)",
          "49: # - SPARK_WORKER_PORT / SPARK_WORKER_WEBUI_PORT, to use non-default ports for the worker",
          "50: # - SPARK_WORKER_DIR, to set the working directory of worker processes",
          "51: # - SPARK_WORKER_OPTS, to set config properties only for the worker (e.g. \"-Dx=y\")",
          "52: # - SPARK_DAEMON_MEMORY, to allocate to the master, worker and history server themselves (default: 1g).",
          "53: # - SPARK_HISTORY_OPTS, to set config properties only for the history server (e.g. \"-Dx=y\")",
          "54: # - SPARK_SHUFFLE_OPTS, to set config properties only for the external shuffle service (e.g. \"-Dx=y\")",
          "55: # - SPARK_DAEMON_JAVA_OPTS, to set config properties for all daemons (e.g. \"-Dx=y\")",
          "56: # - SPARK_DAEMON_CLASSPATH, to set the classpath for all daemons",
          "57: # - SPARK_PUBLIC_DNS, to set the public dns name of the master or workers",
          "59: # Generic options for the daemons used in the standalone deploy mode",
          "60: # - SPARK_CONF_DIR      Alternate conf dir. (Default: ${SPARK_HOME}/conf)",
          "61: # - SPARK_LOG_DIR       Where log files are stored.  (Default: ${SPARK_HOME}/logs)",
          "62: # - SPARK_PID_DIR       Where the pid file is stored. (Default: /tmp)",
          "63: # - SPARK_IDENT_STRING  A string representing this instance of spark. (Default: $USER)",
          "64: # - SPARK_NICENESS      The scheduling priority for daemons. (Default: 0)",
          "65: # - SPARK_NO_DAEMONIZE  Run the proposed command in the foreground. It will not output a PID file.",
          "66: # Options for native BLAS, like Intel MKL, OpenBLAS, and so on.",
          "67: # You might get better performance to enable these options if using native BLAS (see SPARK-21305).",
          "68: # - MKL_NUM_THREADS=1        Disable multi-threading of Intel MKL",
          "69: # - OPENBLAS_NUM_THREADS=1   Disable multi-threading of OpenBLAS",
          "71: export JAVA_HOME=/home/admin/jdk1.8.0_141",
          "72: export CLASSPATH=.:$JAVA_HOME/lib",
          "73: export JAVA_LIBRARY_PATH=$JAVA_LIBRARY_PATH:/home/admin/hadoop-2.7.0/lib/native",
          "74: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/admin/hadoop-2.7.0/lib/native",
          "76: export SPARK_PID_DIR=${SPARK_HOME}/",
          "",
          "---------------"
        ],
        "docker/dockerfile/standalone/conf/zk/zoo.cfg||docker/dockerfile/standalone/conf/zk/zoo.cfg": [
          "File: docker/dockerfile/standalone/conf/zk/zoo.cfg -> docker/dockerfile/standalone/conf/zk/zoo.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one or more",
          "3: # contributor license agreements.  See the NOTICE file distributed with",
          "4: # this work for additional information regarding copyright ownership.",
          "5: # The ASF licenses this file to You under the Apache License, Version 2.0",
          "6: # (the \"License\"); you may not use this file except in compliance with",
          "7: # the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #    http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing, software",
          "12: # distributed under the License is distributed on an \"AS IS\" BASIS,",
          "13: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
          "14: # See the License for the specific language governing permissions and",
          "15: # limitations under the License.",
          "16: #",
          "18: # The number of milliseconds of each tick",
          "19: tickTime=2000",
          "20: # The number of ticks that the initial",
          "21: # synchronization phase can take",
          "22: initLimit=10",
          "23: # The number of ticks that can pass between",
          "24: # sending a request and getting an acknowledgement",
          "25: syncLimit=5",
          "26: # the directory where the snapshot is stored.",
          "27: # do not use /tmp for storage, /tmp here is just",
          "28: # example sakes.",
          "29: dataDir=/data/zookeeper",
          "30: # the port at which the clients will connect",
          "31: clientPort=2181",
          "32: # the maximum number of client connections.",
          "33: # increase this if you need to handle more clients",
          "34: #maxClientCnxns=60",
          "35: #",
          "36: # Be sure to read the maintenance section of the",
          "37: # administrator guide before turning on autopurge.",
          "38: #",
          "39: # http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance",
          "40: #",
          "41: # The number of snapshots to retain in dataDir",
          "42: #autopurge.snapRetainCount=3",
          "43: # Purge task interval in hours",
          "44: # Set to \"0\" to disable auto purge feature",
          "45: #autopurge.purgeInterval=1",
          "",
          "---------------"
        ],
        "docker/setup_standalone.sh||docker/setup_standalone.sh": [
          "File: docker/setup_standalone.sh -> docker/setup_standalone.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: -p 8032:8032 \\",
          "24: -p 8042:8042 \\",
          "25: -p 2181:2181 \\",
          "",
          "[Removed Lines]",
          "26: apachekylin/apache-kylin-standalone:4.0.0-alpha",
          "",
          "[Added Lines]",
          "26: apachekylin/apache-kylin-standalone:4.0.0-beta",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4c523b13b90477f143fcc20178736a7a2367ebf0",
      "candidate_info": {
        "commit_hash": "4c523b13b90477f143fcc20178736a7a2367ebf0",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/4c523b13b90477f143fcc20178736a7a2367ebf0",
        "files": [
          "webapp/app/js/controllers/cubeSchema.js"
        ],
        "message": "[KYLIN-4955] fix typo in KYLIN UI when not set dictionary for count_distinct measure.\n\n(cherry picked from commit 49210310859bee203dfb8f975c3f630b211f45dc)",
        "before_after_code_files": [
          "webapp/app/js/controllers/cubeSchema.js||webapp/app/js/controllers/cubeSchema.js"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "webapp/app/js/controllers/cubeSchema.js||webapp/app/js/controllers/cubeSchema.js": [
          "File: webapp/app/js/controllers/cubeSchema.js -> webapp/app/js/controllers/cubeSchema.js",
          "--- Hunk 1 ---",
          "[Context before]",
          "436:                 });",
          "438:                 if (!isColumnExit) {",
          "440:                 }",
          "441:             }",
          "442:         });",
          "",
          "[Removed Lines]",
          "439:                     errors.push(\"The non-Int type precise count distinct measure must set advanced cict: \" + measureColumn);",
          "",
          "[Added Lines]",
          "439:                     errors.push(\"The non-Int type precise count distinct measure must set advanced dictionary: \" + measureColumn);",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8ed6d5d69a4b589d183bb679f511f8230daa09bf",
      "candidate_info": {
        "commit_hash": "8ed6d5d69a4b589d183bb679f511f8230daa09bf",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/8ed6d5d69a4b589d183bb679f511f8230daa09bf",
        "files": [
          "server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java",
          "server/src/test/java/org/apache/kylin/rest/service/QueryServiceTest.java"
        ],
        "message": "fix",
        "before_after_code_files": [
          "server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java||server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java",
          "server/src/test/java/org/apache/kylin/rest/service/QueryServiceTest.java||server/src/test/java/org/apache/kylin/rest/service/QueryServiceTest.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java||server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java -> server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "1263:             throws SQLException {",
          "1264:         boolean isNull = (null == param.getValue());",
          "1273:         Rep rep = Rep.of(clazz);",
          "",
          "[Removed Lines]",
          "1266:         Class<?> clazz;",
          "1267:         try {",
          "1268:             clazz = Class.forName(param.getClassName());",
          "1269:         } catch (ClassNotFoundException e) {",
          "1270:             throw new InternalErrorException(e);",
          "1271:         }",
          "",
          "[Added Lines]",
          "1266:         Class<?> clazz = getValidClass(param.getClassName());",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1337:         }",
          "1338:     }",
          "1340:     public void setCacheManager(CacheManager cacheManager) {",
          "1341:         this.cacheManager = cacheManager;",
          "1342:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1335:     public Class<?> getValidClass(String className) {",
          "1336:         Class<?> clazz;",
          "1337:         try {",
          "1338:             List<String> classList = new ArrayList<>();",
          "1339:             Rep.VALUE_MAP.keySet().forEach(key -> {",
          "1340:                 classList.add(key.getName());",
          "1341:             });",
          "1342:             if (classList.contains(className)) {",
          "1343:                 clazz = Class.forName(className);",
          "1344:             } else {",
          "1345:                 clazz = Class.forName(\"java.lang.Object\");",
          "1346:             }",
          "1347:             logger.debug(\"Class parameter for sql is: \" + clazz.getName());",
          "1348:         } catch (ClassNotFoundException e) {",
          "1349:             throw new InternalErrorException(e);",
          "1350:         }",
          "1351:         return clazz;",
          "1352:     }",
          "",
          "---------------"
        ],
        "server/src/test/java/org/apache/kylin/rest/service/QueryServiceTest.java||server/src/test/java/org/apache/kylin/rest/service/QueryServiceTest.java": [
          "File: server/src/test/java/org/apache/kylin/rest/service/QueryServiceTest.java -> server/src/test/java/org/apache/kylin/rest/service/QueryServiceTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "120:             Assert.assertTrue(wrapper == null || wrapper.get() == null);",
          "121:         }",
          "122:     }",
          "123: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "124:     @Test",
          "125:     public void testClassName() throws ClassNotFoundException {",
          "126:         Assert.assertEquals(Class.forName(\"java.lang.Object\"), queryService.getValidClass(\"java.io.DataInputStream\"));",
          "127:         Assert.assertEquals(Class.forName(\"java.lang.String\"), queryService.getValidClass(\"java.lang.String\"));",
          "128:     }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e9e12a5edcfc9cbb25ab334ec160f9a6f2c656eb",
      "candidate_info": {
        "commit_hash": "e9e12a5edcfc9cbb25ab334ec160f9a6f2c656eb",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/e9e12a5edcfc9cbb25ab334ec160f9a6f2c656eb",
        "files": [
          "build/bin/prepare_hadoop_dependency.sh"
        ],
        "message": "KYLIN-5112, Use 'return' instead of 'exit' in script prepare_hadoop_dependency.sh",
        "before_after_code_files": [
          "build/bin/prepare_hadoop_dependency.sh||build/bin/prepare_hadoop_dependency.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "build/bin/prepare_hadoop_dependency.sh||build/bin/prepare_hadoop_dependency.sh": [
          "File: build/bin/prepare_hadoop_dependency.sh -> build/bin/prepare_hadoop_dependency.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: if [ ! -d \"$KYLIN_HOME/spark\" ]; then",
          "31:   echo \"Skip spark which not owned by kylin. SPARK_HOME is $SPARK_HOME and KYLIN_HOME is $KYLIN_HOME .\"",
          "33: fi",
          "35: echo \"Start replace hadoop jars under ${KYLIN_HOME}/spark/jars.\"",
          "",
          "[Removed Lines]",
          "32:   exit 0",
          "",
          "[Added Lines]",
          "32:   return",
          "",
          "---------------"
        ]
      }
    }
  ]
}