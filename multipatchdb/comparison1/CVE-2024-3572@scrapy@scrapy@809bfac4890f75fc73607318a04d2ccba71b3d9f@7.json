{
  "cve_id": "CVE-2024-3572",
  "cve_desc": "The scrapy/scrapy project is vulnerable to XML External Entity (XXE) attacks due to the use of lxml.etree.fromstring for parsing untrusted XML data without proper validation. This vulnerability allows attackers to perform denial of service attacks, access local files, generate network connections, or circumvent firewalls by submitting specially crafted XML data. ",
  "repo": "scrapy/scrapy",
  "patch_hash": "809bfac4890f75fc73607318a04d2ccba71b3d9f",
  "patch_info": {
    "commit_hash": "809bfac4890f75fc73607318a04d2ccba71b3d9f",
    "repo": "scrapy/scrapy",
    "commit_url": "https://github.com/scrapy/scrapy/commit/809bfac4890f75fc73607318a04d2ccba71b3d9f",
    "files": [
      "docs/news.rst",
      "docs/topics/request-response.rst",
      "docs/topics/settings.rst",
      "scrapy/downloadermiddlewares/decompression.py",
      "scrapy/downloadermiddlewares/httpcompression.py",
      "scrapy/spiders/sitemap.py",
      "scrapy/utils/_compression.py",
      "scrapy/utils/gz.py",
      "tests/sample_data/compressed/bomb-br.bin",
      "tests/sample_data/compressed/bomb-deflate.bin",
      "tests/sample_data/compressed/bomb-gzip.bin",
      "tests/sample_data/compressed/bomb-zstd.bin",
      "tests/test_downloadermiddleware_decompression.py",
      "tests/test_downloadermiddleware_httpcompression.py",
      "tests/test_spider.py"
    ],
    "message": "Merge branch '2.11-compression-bomb' into 2.11",
    "before_after_code_files": [
      "scrapy/downloadermiddlewares/decompression.py||scrapy/downloadermiddlewares/decompression.py",
      "scrapy/downloadermiddlewares/httpcompression.py||scrapy/downloadermiddlewares/httpcompression.py",
      "scrapy/spiders/sitemap.py||scrapy/spiders/sitemap.py",
      "scrapy/utils/_compression.py||scrapy/utils/_compression.py",
      "scrapy/utils/gz.py||scrapy/utils/gz.py",
      "tests/test_downloadermiddleware_decompression.py||tests/test_downloadermiddleware_decompression.py",
      "tests/test_downloadermiddleware_httpcompression.py||tests/test_downloadermiddleware_httpcompression.py",
      "tests/test_spider.py||tests/test_spider.py"
    ]
  },
  "patch_diff": {
    "scrapy/downloadermiddlewares/decompression.py||scrapy/downloadermiddlewares/decompression.py": [
      "File: scrapy/downloadermiddlewares/decompression.py -> scrapy/downloadermiddlewares/decompression.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "[No context available]",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------"
    ],
    "scrapy/downloadermiddlewares/httpcompression.py||scrapy/downloadermiddlewares/httpcompression.py": [
      "File: scrapy/downloadermiddlewares/httpcompression.py -> scrapy/downloadermiddlewares/httpcompression.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "2: import warnings",
      "6: from scrapy.http import Response, TextResponse",
      "7: from scrapy.responsetypes import responsetypes",
      "8: from scrapy.utils.deprecate import ScrapyDeprecationWarning",
      "9: from scrapy.utils.gz import gunzip",
      "11: ACCEPTED_ENCODINGS = [b\"gzip\", b\"deflate\"]",
      "13: try:",
      "17: except ImportError:",
      "18:     pass",
      "20: try:",
      "24: except ImportError:",
      "25:     pass",
      "28: class HttpCompressionMiddleware:",
      "29:     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be",
      "30:     sent/received from web sites\"\"\"",
      "35:     @classmethod",
      "36:     def from_crawler(cls, crawler):",
      "37:         if not crawler.settings.getbool(\"COMPRESSION_ENABLED\"):",
      "38:             raise NotConfigured",
      "39:         try:",
      "41:         except TypeError:",
      "42:             warnings.warn(",
      "43:                 \"HttpCompressionMiddleware subclasses must either modify \"",
      "46:                 ScrapyDeprecationWarning,",
      "47:             )",
      "52:     def process_request(self, request, spider):",
      "53:         request.headers.setdefault(\"Accept-Encoding\", b\", \".join(ACCEPTED_ENCODINGS))",
      "",
      "[Removed Lines]",
      "1: import io",
      "3: import zlib",
      "5: from scrapy.exceptions import NotConfigured",
      "14:     import brotli",
      "16:     ACCEPTED_ENCODINGS.append(b\"br\")",
      "21:     import zstandard",
      "23:     ACCEPTED_ENCODINGS.append(b\"zstd\")",
      "32:     def __init__(self, stats=None):",
      "33:         self.stats = stats",
      "40:             return cls(stats=crawler.stats)",
      "44:                 \"their '__init__' method to support a 'stats' parameter or \"",
      "45:                 \"reimplement the 'from_crawler' method.\",",
      "48:             result = cls()",
      "49:             result.stats = crawler.stats",
      "50:             return result",
      "",
      "[Added Lines]",
      "2: from logging import getLogger",
      "4: from scrapy import signals",
      "5: from scrapy.exceptions import IgnoreRequest, NotConfigured",
      "8: from scrapy.utils._compression import (",
      "9:     _DecompressionMaxSizeExceeded,",
      "10:     _inflate,",
      "11:     _unbrotli,",
      "12:     _unzstd,",
      "13: )",
      "17: logger = getLogger(__name__)",
      "22:     import brotli  # noqa: F401",
      "25: else:",
      "26:     ACCEPTED_ENCODINGS.append(b\"br\")",
      "29:     import zstandard  # noqa: F401",
      "32: else:",
      "33:     ACCEPTED_ENCODINGS.append(b\"zstd\")",
      "40:     def __init__(self, stats=None, *, crawler=None):",
      "41:         if not crawler:",
      "42:             self.stats = stats",
      "43:             self._max_size = 1073741824",
      "44:             self._warn_size = 33554432",
      "45:             return",
      "46:         self.stats = crawler.stats",
      "47:         self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")",
      "48:         self._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")",
      "49:         crawler.signals.connect(self.open_spider, signals.spider_opened)",
      "56:             return cls(crawler=crawler)",
      "60:                 \"their '__init__' method to support a 'crawler' parameter or \"",
      "61:                 \"reimplement their 'from_crawler' method.\",",
      "64:             mw = cls()",
      "65:             mw.stats = crawler.stats",
      "66:             mw._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")",
      "67:             mw._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")",
      "68:             crawler.signals.connect(mw.open_spider, signals.spider_opened)",
      "69:             return mw",
      "71:     def open_spider(self, spider):",
      "72:         if hasattr(spider, \"download_maxsize\"):",
      "73:             self._max_size = spider.download_maxsize",
      "74:         if hasattr(spider, \"download_warnsize\"):",
      "75:             self._warn_size = spider.download_warnsize",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "59:             content_encoding = response.headers.getlist(\"Content-Encoding\")",
      "60:             if content_encoding:",
      "61:                 encoding = content_encoding.pop()",
      "63:                 if self.stats:",
      "64:                     self.stats.inc_value(",
      "65:                         \"httpcompression/response_bytes\",",
      "",
      "[Removed Lines]",
      "62:                 decoded_body = self._decode(response.body, encoding.lower())",
      "",
      "[Added Lines]",
      "87:                 max_size = request.meta.get(\"download_maxsize\", self._max_size)",
      "88:                 warn_size = request.meta.get(\"download_warnsize\", self._warn_size)",
      "89:                 try:",
      "90:                     decoded_body = self._decode(",
      "91:                         response.body, encoding.lower(), max_size",
      "92:                     )",
      "93:                 except _DecompressionMaxSizeExceeded:",
      "94:                     raise IgnoreRequest(",
      "95:                         f\"Ignored response {response} because its body \"",
      "96:                         f\"({len(response.body)} B) exceeded DOWNLOAD_MAXSIZE \"",
      "97:                         f\"({max_size} B) during decompression.\"",
      "98:                     )",
      "99:                 if len(response.body) < warn_size <= len(decoded_body):",
      "100:                     logger.warning(",
      "101:                         f\"{response} body size after decompression \"",
      "102:                         f\"({len(decoded_body)} B) is larger than the \"",
      "103:                         f\"download warning size ({warn_size} B).\"",
      "104:                     )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "84:         return response",
      "87:         if encoding == b\"gzip\" or encoding == b\"x-gzip\":",
      "90:         if encoding == b\"deflate\":",
      "100:         if encoding == b\"br\" and b\"br\" in ACCEPTED_ENCODINGS:",
      "102:         if encoding == b\"zstd\" and b\"zstd\" in ACCEPTED_ENCODINGS:",
      "107:         return body",
      "",
      "[Removed Lines]",
      "86:     def _decode(self, body, encoding):",
      "88:             body = gunzip(body)",
      "91:             try:",
      "92:                 body = zlib.decompress(body)",
      "93:             except zlib.error:",
      "94:                 # ugly hack to work with raw deflate content that may",
      "95:                 # be sent by microsoft servers. For more information, see:",
      "96:                 # http://carsten.codimi.de/gzip.yaws/",
      "97:                 # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx",
      "98:                 # http://www.gzip.org/zlib/zlib_faq.html#faq38",
      "99:                 body = zlib.decompress(body, -15)",
      "101:             body = brotli.decompress(body)",
      "103:             # Using its streaming API since its simple API could handle only cases",
      "104:             # where there is content size data embedded in the frame",
      "105:             reader = zstandard.ZstdDecompressor().stream_reader(io.BytesIO(body))",
      "106:             body = reader.read()",
      "",
      "[Added Lines]",
      "128:     def _decode(self, body, encoding, max_size):",
      "130:             return gunzip(body, max_size=max_size)",
      "132:             return _inflate(body, max_size=max_size)",
      "134:             return _unbrotli(body, max_size=max_size)",
      "136:             return _unzstd(body, max_size=max_size)",
      "",
      "---------------"
    ],
    "scrapy/spiders/sitemap.py||scrapy/spiders/sitemap.py": [
      "File: scrapy/spiders/sitemap.py -> scrapy/spiders/sitemap.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: import logging",
      "2: import re",
      "4: from scrapy.http import Request, XmlResponse",
      "5: from scrapy.spiders import Spider",
      "6: from scrapy.utils.gz import gunzip, gzip_magic_number",
      "7: from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots",
      "9: logger = logging.getLogger(__name__)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "3: from typing import TYPE_CHECKING, Any",
      "7: from scrapy.utils._compression import _DecompressionMaxSizeExceeded",
      "11: if TYPE_CHECKING:",
      "12:     # typing.Self requires Python 3.11",
      "13:     from typing_extensions import Self",
      "15:     from scrapy.crawler import Crawler",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "14:     sitemap_rules = [(\"\", \"parse\")]",
      "15:     sitemap_follow = [\"\"]",
      "16:     sitemap_alternate_links = False",
      "18:     def __init__(self, *a, **kw):",
      "19:         super().__init__(*a, **kw)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "25:     _max_size: int",
      "26:     _warn_size: int",
      "28:     @classmethod",
      "29:     def from_crawler(cls, crawler: \"Crawler\", *args: Any, **kwargs: Any) -> \"Self\":",
      "30:         spider = super().from_crawler(crawler, *args, **kwargs)",
      "31:         spider._max_size = getattr(",
      "32:             spider, \"download_maxsize\", spider.settings.getint(\"DOWNLOAD_MAXSIZE\")",
      "33:         )",
      "34:         spider._warn_size = getattr(",
      "35:             spider, \"download_warnsize\", spider.settings.getint(\"DOWNLOAD_WARNSIZE\")",
      "36:         )",
      "37:         return spider",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "71:         if isinstance(response, XmlResponse):",
      "72:             return response.body",
      "73:         if gzip_magic_number(response):",
      "75:         # actual gzipped sitemap files are decompressed above ;",
      "76:         # if we are here (response body is not gzipped)",
      "77:         # and have a response for .xml.gz,",
      "",
      "[Removed Lines]",
      "74:             return gunzip(response.body)",
      "",
      "[Added Lines]",
      "95:             uncompressed_size = len(response.body)",
      "96:             max_size = response.meta.get(\"download_maxsize\", self._max_size)",
      "97:             warn_size = response.meta.get(\"download_warnsize\", self._warn_size)",
      "98:             try:",
      "99:                 body = gunzip(response.body, max_size=max_size)",
      "100:             except _DecompressionMaxSizeExceeded:",
      "101:                 return None",
      "102:             if uncompressed_size < warn_size <= len(body):",
      "103:                 logger.warning(",
      "104:                     f\"{response} body size after decompression ({len(body)} B) \"",
      "105:                     f\"is larger than the download warning size ({warn_size} B).\"",
      "106:                 )",
      "107:             return body",
      "",
      "---------------"
    ],
    "scrapy/utils/_compression.py||scrapy/utils/_compression.py": [
      "File: scrapy/utils/_compression.py -> scrapy/utils/_compression.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "[No context available]",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1: import zlib",
      "2: from io import BytesIO",
      "4: try:",
      "5:     import brotli",
      "6: except ImportError:",
      "7:     pass",
      "9: try:",
      "10:     import zstandard",
      "11: except ImportError:",
      "12:     pass",
      "15: _CHUNK_SIZE = 65536  # 64 KiB",
      "18: class _DecompressionMaxSizeExceeded(ValueError):",
      "19:     pass",
      "22: def _inflate(data: bytes, *, max_size: int = 0) -> bytes:",
      "23:     decompressor = zlib.decompressobj()",
      "24:     raw_decompressor = zlib.decompressobj(wbits=-15)",
      "25:     input_stream = BytesIO(data)",
      "26:     output_stream = BytesIO()",
      "27:     output_chunk = b\".\"",
      "28:     decompressed_size = 0",
      "29:     while output_chunk:",
      "30:         input_chunk = input_stream.read(_CHUNK_SIZE)",
      "31:         try:",
      "32:             output_chunk = decompressor.decompress(input_chunk)",
      "33:         except zlib.error:",
      "34:             if decompressor != raw_decompressor:",
      "35:                 # ugly hack to work with raw deflate content that may",
      "36:                 # be sent by microsoft servers. For more information, see:",
      "37:                 # http://carsten.codimi.de/gzip.yaws/",
      "38:                 # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx",
      "39:                 # http://www.gzip.org/zlib/zlib_faq.html#faq38",
      "40:                 decompressor = raw_decompressor",
      "41:                 output_chunk = decompressor.decompress(input_chunk)",
      "42:             else:",
      "43:                 raise",
      "44:         decompressed_size += len(output_chunk)",
      "45:         if max_size and decompressed_size > max_size:",
      "46:             raise _DecompressionMaxSizeExceeded(",
      "47:                 f\"The number of bytes decompressed so far \"",
      "48:                 f\"({decompressed_size} B) exceed the specified maximum \"",
      "49:                 f\"({max_size} B).\"",
      "50:             )",
      "51:         output_stream.write(output_chunk)",
      "52:     output_stream.seek(0)",
      "53:     return output_stream.read()",
      "56: def _unbrotli(data: bytes, *, max_size: int = 0) -> bytes:",
      "57:     decompressor = brotli.Decompressor()",
      "58:     input_stream = BytesIO(data)",
      "59:     output_stream = BytesIO()",
      "60:     output_chunk = b\".\"",
      "61:     decompressed_size = 0",
      "62:     while output_chunk:",
      "63:         input_chunk = input_stream.read(_CHUNK_SIZE)",
      "64:         output_chunk = decompressor.process(input_chunk)",
      "65:         decompressed_size += len(output_chunk)",
      "66:         if max_size and decompressed_size > max_size:",
      "67:             raise _DecompressionMaxSizeExceeded(",
      "68:                 f\"The number of bytes decompressed so far \"",
      "69:                 f\"({decompressed_size} B) exceed the specified maximum \"",
      "70:                 f\"({max_size} B).\"",
      "71:             )",
      "72:         output_stream.write(output_chunk)",
      "73:     output_stream.seek(0)",
      "74:     return output_stream.read()",
      "77: def _unzstd(data: bytes, *, max_size: int = 0) -> bytes:",
      "78:     decompressor = zstandard.ZstdDecompressor()",
      "79:     stream_reader = decompressor.stream_reader(BytesIO(data))",
      "80:     output_stream = BytesIO()",
      "81:     output_chunk = b\".\"",
      "82:     decompressed_size = 0",
      "83:     while output_chunk:",
      "84:         output_chunk = stream_reader.read(_CHUNK_SIZE)",
      "85:         decompressed_size += len(output_chunk)",
      "86:         if max_size and decompressed_size > max_size:",
      "87:             raise _DecompressionMaxSizeExceeded(",
      "88:                 f\"The number of bytes decompressed so far \"",
      "89:                 f\"({decompressed_size} B) exceed the specified maximum \"",
      "90:                 f\"({max_size} B).\"",
      "91:             )",
      "92:         output_stream.write(output_chunk)",
      "93:     output_stream.seek(0)",
      "94:     return output_stream.read()",
      "",
      "---------------"
    ],
    "scrapy/utils/gz.py||scrapy/utils/gz.py": [
      "File: scrapy/utils/gz.py -> scrapy/utils/gz.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: import struct",
      "2: from gzip import GzipFile",
      "3: from io import BytesIO",
      "6: from scrapy.http import Response",
      "10:     \"\"\"Gunzip the given data and return as much data as possible.",
      "12:     This is resilient to CRC checksum errors.",
      "13:     \"\"\"",
      "14:     f = GzipFile(fileobj=BytesIO(data))",
      "16:     chunk = b\".\"",
      "17:     while chunk:",
      "18:         try:",
      "21:         except (OSError, EOFError, struct.error):",
      "22:             # complete only if there is some data, otherwise re-raise",
      "23:             # see issue 87 about catching struct.error",
      "26:                 break",
      "27:             raise",
      "31: def gzip_magic_number(response: Response) -> bool:",
      "",
      "[Removed Lines]",
      "4: from typing import List",
      "9: def gunzip(data: bytes) -> bytes:",
      "15:     output_list: List[bytes] = []",
      "19:             chunk = f.read1(8196)",
      "20:             output_list.append(chunk)",
      "24:             # some pages are quite small so output_list is empty",
      "25:             if output_list:",
      "28:     return b\"\".join(output_list)",
      "",
      "[Added Lines]",
      "7: from ._compression import _CHUNK_SIZE, _DecompressionMaxSizeExceeded",
      "10: def gunzip(data: bytes, *, max_size: int = 0) -> bytes:",
      "16:     output_stream = BytesIO()",
      "18:     decompressed_size = 0",
      "21:             chunk = f.read1(_CHUNK_SIZE)",
      "25:             # some pages are quite small so output_stream is empty",
      "26:             if output_stream.getbuffer().nbytes > 0:",
      "29:         decompressed_size += len(chunk)",
      "30:         if max_size and decompressed_size > max_size:",
      "31:             raise _DecompressionMaxSizeExceeded(",
      "32:                 f\"The number of bytes decompressed so far \"",
      "33:                 f\"({decompressed_size} B) exceed the specified maximum \"",
      "34:                 f\"({max_size} B).\"",
      "35:             )",
      "36:         output_stream.write(chunk)",
      "37:     output_stream.seek(0)",
      "38:     return output_stream.read()",
      "",
      "---------------"
    ],
    "tests/test_downloadermiddleware_decompression.py||tests/test_downloadermiddleware_decompression.py": [
      "File: tests/test_downloadermiddleware_decompression.py -> tests/test_downloadermiddleware_decompression.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "[No context available]",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------"
    ],
    "tests/test_downloadermiddleware_httpcompression.py||tests/test_downloadermiddleware_httpcompression.py": [
      "File: tests/test_downloadermiddleware_httpcompression.py -> tests/test_downloadermiddleware_httpcompression.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: from gzip import GzipFile",
      "2: from io import BytesIO",
      "3: from pathlib import Path",
      "4: from unittest import SkipTest, TestCase",
      "5: from warnings import catch_warnings",
      "7: from w3lib.encoding import resolve_encoding",
      "9: from scrapy.downloadermiddlewares.httpcompression import (",
      "10:     ACCEPTED_ENCODINGS,",
      "11:     HttpCompressionMiddleware,",
      "12: )",
      "14: from scrapy.http import HtmlResponse, Request, Response",
      "15: from scrapy.responsetypes import responsetypes",
      "16: from scrapy.spiders import Spider",
      "",
      "[Removed Lines]",
      "13: from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning",
      "",
      "[Added Lines]",
      "3: from logging import WARNING",
      "8: from testfixtures import LogCapture",
      "15: from scrapy.exceptions import IgnoreRequest, NotConfigured, ScrapyDeprecationWarning",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "35:         \"html-zstd-streaming-no-content-size.bin\",",
      "36:         \"zstd\",",
      "37:     ),",
      "38: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "41:         f\"bomb-{format_id}\": (f\"bomb-{format_id}.bin\", format_id)",
      "42:         for format_id in (",
      "43:             \"br\",  # 34 \u2192 11 511 612",
      "44:             \"deflate\",  # 27 968 \u2192 11 511 612",
      "45:             \"gzip\",  # 27 988 \u2192 11 511 612",
      "46:             \"zstd\",  # 1 096 \u2192 11 511 612",
      "47:         )",
      "48:     },",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "115:         self.assertStatsEqual(\"httpcompression/response_count\", 1)",
      "116:         self.assertStatsEqual(\"httpcompression/response_bytes\", 74837)",
      "130:     def test_process_response_br(self):",
      "131:         try:",
      "132:             import brotli  # noqa: F401",
      "",
      "[Removed Lines]",
      "118:     def test_process_response_gzip_no_stats(self):",
      "119:         mw = HttpCompressionMiddleware()",
      "120:         response = self._getresponse(\"gzip\")",
      "121:         request = response.request",
      "123:         self.assertEqual(response.headers[\"Content-Encoding\"], b\"gzip\")",
      "124:         newresponse = mw.process_response(request, response, self.spider)",
      "125:         self.assertEqual(mw.stats, None)",
      "126:         assert newresponse is not response",
      "127:         assert newresponse.body.startswith(b\"<!DOCTYPE\")",
      "128:         assert \"Content-Encoding\" not in newresponse.headers",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "373:         self.assertStatsEqual(\"httpcompression/response_count\", None)",
      "374:         self.assertStatsEqual(\"httpcompression/response_bytes\", None)",
      "377: class HttpCompressionSubclassTest(TestCase):",
      "378:     def test_init_missing_stats(self):",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "375:     def _test_compression_bomb_setting(self, compression_id):",
      "376:         settings = {\"DOWNLOAD_MAXSIZE\": 10_000_000}",
      "377:         crawler = get_crawler(Spider, settings_dict=settings)",
      "378:         spider = crawler._create_spider(\"scrapytest.org\")",
      "379:         mw = HttpCompressionMiddleware.from_crawler(crawler)",
      "380:         mw.open_spider(spider)",
      "382:         response = self._getresponse(f\"bomb-{compression_id}\")",
      "383:         self.assertRaises(",
      "384:             IgnoreRequest,",
      "385:             mw.process_response,",
      "386:             response.request,",
      "387:             response,",
      "388:             spider,",
      "389:         )",
      "391:     def test_compression_bomb_setting_br(self):",
      "392:         try:",
      "393:             import brotli  # noqa: F401",
      "394:         except ImportError:",
      "395:             raise SkipTest(\"no brotli\")",
      "396:         self._test_compression_bomb_setting(\"br\")",
      "398:     def test_compression_bomb_setting_deflate(self):",
      "399:         self._test_compression_bomb_setting(\"deflate\")",
      "401:     def test_compression_bomb_setting_gzip(self):",
      "402:         self._test_compression_bomb_setting(\"gzip\")",
      "404:     def test_compression_bomb_setting_zstd(self):",
      "405:         self._test_compression_bomb_setting(\"zstd\")",
      "407:     def _test_compression_bomb_spider_attr(self, compression_id):",
      "408:         class DownloadMaxSizeSpider(Spider):",
      "409:             download_maxsize = 10_000_000",
      "411:         crawler = get_crawler(DownloadMaxSizeSpider)",
      "412:         spider = crawler._create_spider(\"scrapytest.org\")",
      "413:         mw = HttpCompressionMiddleware.from_crawler(crawler)",
      "414:         mw.open_spider(spider)",
      "416:         response = self._getresponse(f\"bomb-{compression_id}\")",
      "417:         self.assertRaises(",
      "418:             IgnoreRequest,",
      "419:             mw.process_response,",
      "420:             response.request,",
      "421:             response,",
      "422:             spider,",
      "423:         )",
      "425:     def test_compression_bomb_spider_attr_br(self):",
      "426:         try:",
      "427:             import brotli  # noqa: F401",
      "428:         except ImportError:",
      "429:             raise SkipTest(\"no brotli\")",
      "430:         self._test_compression_bomb_spider_attr(\"br\")",
      "432:     def test_compression_bomb_spider_attr_deflate(self):",
      "433:         self._test_compression_bomb_spider_attr(\"deflate\")",
      "435:     def test_compression_bomb_spider_attr_gzip(self):",
      "436:         self._test_compression_bomb_spider_attr(\"gzip\")",
      "438:     def test_compression_bomb_spider_attr_zstd(self):",
      "439:         self._test_compression_bomb_spider_attr(\"zstd\")",
      "441:     def _test_compression_bomb_request_meta(self, compression_id):",
      "442:         crawler = get_crawler(Spider)",
      "443:         spider = crawler._create_spider(\"scrapytest.org\")",
      "444:         mw = HttpCompressionMiddleware.from_crawler(crawler)",
      "445:         mw.open_spider(spider)",
      "447:         response = self._getresponse(f\"bomb-{compression_id}\")",
      "448:         response.meta[\"download_maxsize\"] = 10_000_000",
      "449:         self.assertRaises(",
      "450:             IgnoreRequest,",
      "451:             mw.process_response,",
      "452:             response.request,",
      "453:             response,",
      "454:             spider,",
      "455:         )",
      "457:     def test_compression_bomb_request_meta_br(self):",
      "458:         try:",
      "459:             import brotli  # noqa: F401",
      "460:         except ImportError:",
      "461:             raise SkipTest(\"no brotli\")",
      "462:         self._test_compression_bomb_request_meta(\"br\")",
      "464:     def test_compression_bomb_request_meta_deflate(self):",
      "465:         self._test_compression_bomb_request_meta(\"deflate\")",
      "467:     def test_compression_bomb_request_meta_gzip(self):",
      "468:         self._test_compression_bomb_request_meta(\"gzip\")",
      "470:     def test_compression_bomb_request_meta_zstd(self):",
      "471:         self._test_compression_bomb_request_meta(\"zstd\")",
      "473:     def _test_download_warnsize_setting(self, compression_id):",
      "474:         settings = {\"DOWNLOAD_WARNSIZE\": 10_000_000}",
      "475:         crawler = get_crawler(Spider, settings_dict=settings)",
      "476:         spider = crawler._create_spider(\"scrapytest.org\")",
      "477:         mw = HttpCompressionMiddleware.from_crawler(crawler)",
      "478:         mw.open_spider(spider)",
      "479:         response = self._getresponse(f\"bomb-{compression_id}\")",
      "481:         with LogCapture(",
      "482:             \"scrapy.downloadermiddlewares.httpcompression\",",
      "483:             propagate=False,",
      "484:             level=WARNING,",
      "485:         ) as log:",
      "486:             mw.process_response(response.request, response, spider)",
      "487:         log.check(",
      "488:             (",
      "489:                 \"scrapy.downloadermiddlewares.httpcompression\",",
      "490:                 \"WARNING\",",
      "491:                 (",
      "492:                     \"<200 http://scrapytest.org/> body size after \"",
      "493:                     \"decompression (11511612 B) is larger than the download \"",
      "494:                     \"warning size (10000000 B).\"",
      "495:                 ),",
      "496:             ),",
      "497:         )",
      "499:     def test_download_warnsize_setting_br(self):",
      "500:         try:",
      "501:             import brotli  # noqa: F401",
      "502:         except ImportError:",
      "503:             raise SkipTest(\"no brotli\")",
      "504:         self._test_download_warnsize_setting(\"br\")",
      "506:     def test_download_warnsize_setting_deflate(self):",
      "507:         self._test_download_warnsize_setting(\"deflate\")",
      "509:     def test_download_warnsize_setting_gzip(self):",
      "510:         self._test_download_warnsize_setting(\"gzip\")",
      "512:     def test_download_warnsize_setting_zstd(self):",
      "513:         self._test_download_warnsize_setting(\"zstd\")",
      "515:     def _test_download_warnsize_spider_attr(self, compression_id):",
      "516:         class DownloadWarnSizeSpider(Spider):",
      "517:             download_warnsize = 10_000_000",
      "519:         crawler = get_crawler(DownloadWarnSizeSpider)",
      "520:         spider = crawler._create_spider(\"scrapytest.org\")",
      "521:         mw = HttpCompressionMiddleware.from_crawler(crawler)",
      "522:         mw.open_spider(spider)",
      "523:         response = self._getresponse(f\"bomb-{compression_id}\")",
      "525:         with LogCapture(",
      "526:             \"scrapy.downloadermiddlewares.httpcompression\",",
      "527:             propagate=False,",
      "528:             level=WARNING,",
      "529:         ) as log:",
      "530:             mw.process_response(response.request, response, spider)",
      "531:         log.check(",
      "532:             (",
      "533:                 \"scrapy.downloadermiddlewares.httpcompression\",",
      "534:                 \"WARNING\",",
      "535:                 (",
      "536:                     \"<200 http://scrapytest.org/> body size after \"",
      "537:                     \"decompression (11511612 B) is larger than the download \"",
      "538:                     \"warning size (10000000 B).\"",
      "539:                 ),",
      "540:             ),",
      "541:         )",
      "543:     def test_download_warnsize_spider_attr_br(self):",
      "544:         try:",
      "545:             import brotli  # noqa: F401",
      "546:         except ImportError:",
      "547:             raise SkipTest(\"no brotli\")",
      "548:         self._test_download_warnsize_spider_attr(\"br\")",
      "550:     def test_download_warnsize_spider_attr_deflate(self):",
      "551:         self._test_download_warnsize_spider_attr(\"deflate\")",
      "553:     def test_download_warnsize_spider_attr_gzip(self):",
      "554:         self._test_download_warnsize_spider_attr(\"gzip\")",
      "556:     def test_download_warnsize_spider_attr_zstd(self):",
      "557:         self._test_download_warnsize_spider_attr(\"zstd\")",
      "559:     def _test_download_warnsize_request_meta(self, compression_id):",
      "560:         crawler = get_crawler(Spider)",
      "561:         spider = crawler._create_spider(\"scrapytest.org\")",
      "562:         mw = HttpCompressionMiddleware.from_crawler(crawler)",
      "563:         mw.open_spider(spider)",
      "564:         response = self._getresponse(f\"bomb-{compression_id}\")",
      "565:         response.meta[\"download_warnsize\"] = 10_000_000",
      "567:         with LogCapture(",
      "568:             \"scrapy.downloadermiddlewares.httpcompression\",",
      "569:             propagate=False,",
      "570:             level=WARNING,",
      "571:         ) as log:",
      "572:             mw.process_response(response.request, response, spider)",
      "573:         log.check(",
      "574:             (",
      "575:                 \"scrapy.downloadermiddlewares.httpcompression\",",
      "576:                 \"WARNING\",",
      "577:                 (",
      "578:                     \"<200 http://scrapytest.org/> body size after \"",
      "579:                     \"decompression (11511612 B) is larger than the download \"",
      "580:                     \"warning size (10000000 B).\"",
      "581:                 ),",
      "582:             ),",
      "583:         )",
      "585:     def test_download_warnsize_request_meta_br(self):",
      "586:         try:",
      "587:             import brotli  # noqa: F401",
      "588:         except ImportError:",
      "589:             raise SkipTest(\"no brotli\")",
      "590:         self._test_download_warnsize_request_meta(\"br\")",
      "592:     def test_download_warnsize_request_meta_deflate(self):",
      "593:         self._test_download_warnsize_request_meta(\"deflate\")",
      "595:     def test_download_warnsize_request_meta_gzip(self):",
      "596:         self._test_download_warnsize_request_meta(\"gzip\")",
      "598:     def test_download_warnsize_request_meta_zstd(self):",
      "599:         self._test_download_warnsize_request_meta(\"zstd\")",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "393:             (",
      "394:                 (",
      "395:                     \"HttpCompressionMiddleware subclasses must either modify \"",
      "398:                 ),",
      "399:             ),",
      "400:         )",
      "",
      "[Removed Lines]",
      "396:                     \"their '__init__' method to support a 'stats' parameter \"",
      "397:                     \"or reimplement the 'from_crawler' method.\"",
      "",
      "[Added Lines]",
      "621:                     \"their '__init__' method to support a 'crawler' parameter \"",
      "622:                     \"or reimplement their 'from_crawler' method.\"",
      "",
      "---------------"
    ],
    "tests/test_spider.py||tests/test_spider.py": [
      "File: tests/test_spider.py -> tests/test_spider.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "2: import inspect",
      "3: import warnings",
      "4: from io import BytesIO",
      "5: from typing import Any",
      "6: from unittest import mock",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "5: from logging import WARNING",
      "6: from pathlib import Path",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "25: )",
      "26: from scrapy.spiders.init import InitSpider",
      "27: from scrapy.utils.test import get_crawler",
      "31: class SpiderTest(unittest.TestCase):",
      "",
      "[Removed Lines]",
      "28: from tests import get_testdata",
      "",
      "[Added Lines]",
      "30: from tests import get_testdata, tests_datadir",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "489:     GZBODY = f.getvalue()",
      "491:     def assertSitemapBody(self, response, body):",
      "493:         self.assertEqual(spider._get_sitemap_body(response), body)",
      "495:     def test_get_sitemap_body(self):",
      "",
      "[Removed Lines]",
      "492:         spider = self.spider_class(\"example.com\")",
      "",
      "[Added Lines]",
      "494:         crawler = get_crawler()",
      "495:         spider = self.spider_class.from_crawler(crawler, \"example.com\")",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "507:             url=\"http://www.example.com/sitemap\",",
      "508:             body=self.GZBODY,",
      "509:             headers={\"content-type\": \"application/gzip\"},",
      "510:         )",
      "511:         self.assertSitemapBody(r, self.BODY)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "513:             request=Request(\"http://www.example.com/sitemap\"),",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "515:         self.assertSitemapBody(r, self.BODY)",
      "517:     def test_get_sitemap_body_xml_url_compressed(self):",
      "519:         self.assertSitemapBody(r, self.BODY)",
      "521:         # .xml.gz but body decoded by HttpCompression middleware already",
      "",
      "[Removed Lines]",
      "518:         r = Response(url=\"http://www.example.com/sitemap.xml.gz\", body=self.GZBODY)",
      "",
      "[Added Lines]",
      "522:         r = Response(",
      "523:             url=\"http://www.example.com/sitemap.xml.gz\",",
      "524:             body=self.GZBODY,",
      "525:             request=Request(\"http://www.example.com/sitemap\"),",
      "526:         )",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "692:             [\"http://www.example.com/sitemap2.xml\"],",
      "693:         )",
      "696: class DeprecationTest(unittest.TestCase):",
      "697:     def test_crawl_spider(self):",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "703:     def test_compression_bomb_setting(self):",
      "704:         settings = {\"DOWNLOAD_MAXSIZE\": 10_000_000}",
      "705:         crawler = get_crawler(settings_dict=settings)",
      "706:         spider = self.spider_class.from_crawler(crawler, \"example.com\")",
      "707:         body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")",
      "708:         body = body_path.read_bytes()",
      "709:         request = Request(url=\"https://example.com\")",
      "710:         response = Response(url=\"https://example.com\", body=body, request=request)",
      "711:         self.assertIsNone(spider._get_sitemap_body(response))",
      "713:     def test_compression_bomb_spider_attr(self):",
      "714:         class DownloadMaxSizeSpider(self.spider_class):",
      "715:             download_maxsize = 10_000_000",
      "717:         crawler = get_crawler()",
      "718:         spider = DownloadMaxSizeSpider.from_crawler(crawler, \"example.com\")",
      "719:         body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")",
      "720:         body = body_path.read_bytes()",
      "721:         request = Request(url=\"https://example.com\")",
      "722:         response = Response(url=\"https://example.com\", body=body, request=request)",
      "723:         self.assertIsNone(spider._get_sitemap_body(response))",
      "725:     def test_compression_bomb_request_meta(self):",
      "726:         crawler = get_crawler()",
      "727:         spider = self.spider_class.from_crawler(crawler, \"example.com\")",
      "728:         body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")",
      "729:         body = body_path.read_bytes()",
      "730:         request = Request(",
      "731:             url=\"https://example.com\", meta={\"download_maxsize\": 10_000_000}",
      "732:         )",
      "733:         response = Response(url=\"https://example.com\", body=body, request=request)",
      "734:         self.assertIsNone(spider._get_sitemap_body(response))",
      "736:     def test_download_warnsize_setting(self):",
      "737:         settings = {\"DOWNLOAD_WARNSIZE\": 10_000_000}",
      "738:         crawler = get_crawler(settings_dict=settings)",
      "739:         spider = self.spider_class.from_crawler(crawler, \"example.com\")",
      "740:         body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")",
      "741:         body = body_path.read_bytes()",
      "742:         request = Request(url=\"https://example.com\")",
      "743:         response = Response(url=\"https://example.com\", body=body, request=request)",
      "744:         with LogCapture(",
      "745:             \"scrapy.spiders.sitemap\", propagate=False, level=WARNING",
      "746:         ) as log:",
      "747:             spider._get_sitemap_body(response)",
      "748:         log.check(",
      "749:             (",
      "750:                 \"scrapy.spiders.sitemap\",",
      "751:                 \"WARNING\",",
      "752:                 (",
      "753:                     \"<200 https://example.com> body size after decompression \"",
      "754:                     \"(11511612 B) is larger than the download warning size \"",
      "755:                     \"(10000000 B).\"",
      "756:                 ),",
      "757:             ),",
      "758:         )",
      "760:     def test_download_warnsize_spider_attr(self):",
      "761:         class DownloadWarnSizeSpider(self.spider_class):",
      "762:             download_warnsize = 10_000_000",
      "764:         crawler = get_crawler()",
      "765:         spider = DownloadWarnSizeSpider.from_crawler(crawler, \"example.com\")",
      "766:         body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")",
      "767:         body = body_path.read_bytes()",
      "768:         request = Request(",
      "769:             url=\"https://example.com\", meta={\"download_warnsize\": 10_000_000}",
      "770:         )",
      "771:         response = Response(url=\"https://example.com\", body=body, request=request)",
      "772:         with LogCapture(",
      "773:             \"scrapy.spiders.sitemap\", propagate=False, level=WARNING",
      "774:         ) as log:",
      "775:             spider._get_sitemap_body(response)",
      "776:         log.check(",
      "777:             (",
      "778:                 \"scrapy.spiders.sitemap\",",
      "779:                 \"WARNING\",",
      "780:                 (",
      "781:                     \"<200 https://example.com> body size after decompression \"",
      "782:                     \"(11511612 B) is larger than the download warning size \"",
      "783:                     \"(10000000 B).\"",
      "784:                 ),",
      "785:             ),",
      "786:         )",
      "788:     def test_download_warnsize_request_meta(self):",
      "789:         crawler = get_crawler()",
      "790:         spider = self.spider_class.from_crawler(crawler, \"example.com\")",
      "791:         body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")",
      "792:         body = body_path.read_bytes()",
      "793:         request = Request(",
      "794:             url=\"https://example.com\", meta={\"download_warnsize\": 10_000_000}",
      "795:         )",
      "796:         response = Response(url=\"https://example.com\", body=body, request=request)",
      "797:         with LogCapture(",
      "798:             \"scrapy.spiders.sitemap\", propagate=False, level=WARNING",
      "799:         ) as log:",
      "800:             spider._get_sitemap_body(response)",
      "801:         log.check(",
      "802:             (",
      "803:                 \"scrapy.spiders.sitemap\",",
      "804:                 \"WARNING\",",
      "805:                 (",
      "806:                     \"<200 https://example.com> body size after decompression \"",
      "807:                     \"(11511612 B) is larger than the download warning size \"",
      "808:                     \"(10000000 B).\"",
      "809:                 ),",
      "810:             ),",
      "811:         )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "c5dad41190551578c2973c34520952f26f75dc7b",
      "candidate_info": {
        "commit_hash": "c5dad41190551578c2973c34520952f26f75dc7b",
        "repo": "scrapy/scrapy",
        "commit_url": "https://github.com/scrapy/scrapy/commit/c5dad41190551578c2973c34520952f26f75dc7b",
        "files": [
          ".github/workflows/tests-ubuntu.yml",
          "scrapy/utils/response.py",
          "tests/test_utils_response.py",
          "tox.ini"
        ],
        "message": "Speed up tests, remove comments without regexps",
        "before_after_code_files": [
          "scrapy/utils/response.py||scrapy/utils/response.py",
          "tests/test_utils_response.py||tests/test_utils_response.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/scrapy/scrapy/pull/6222"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scrapy/utils/response.py||scrapy/utils/response.py": [
          "File: scrapy/utils/response.py -> scrapy/utils/response.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "74:     return b\"\".join(values)",
          "77: def open_in_browser(",
          "78:     response: Union[",
          "79:         \"scrapy.http.response.html.HtmlResponse\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "77: def _remove_html_comments(body):",
          "78:     start = body.find(b\"<!--\")",
          "79:     while start != -1:",
          "80:         end = body.find(b\"-->\", start + 1)",
          "81:         if end == -1:",
          "82:             return body[:start]",
          "83:         else:",
          "84:             body = body[:start] + body[end + 3 :]",
          "85:             start = body.find(b\"<!--\")",
          "86:     return body",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "103:     body = response.body",
          "104:     if isinstance(response, HtmlResponse):",
          "105:         if b\"<base\" not in body:",
          "106:             repl = rf'\\0<base href=\"{response.url}\">'",
          "108:             body = re.sub(rb\"<head(?:[^<>]*?>)\", to_bytes(repl), body, count=1)",
          "109:         ext = \".html\"",
          "110:     elif isinstance(response, TextResponse):",
          "",
          "[Removed Lines]",
          "107:             body = re.sub(b\"(?s)<!--.*?(?:-->|$)\", b\"\", body)",
          "",
          "[Added Lines]",
          "118:             _remove_html_comments(body)",
          "",
          "---------------"
        ],
        "tests/test_utils_response.py||tests/test_utils_response.py": [
          "File: tests/test_utils_response.py -> tests/test_utils_response.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "9: from scrapy.exceptions import ScrapyDeprecationWarning",
          "10: from scrapy.http import HtmlResponse, Response, TextResponse",
          "12: from scrapy.utils.python import to_bytes",
          "13: from scrapy.utils.response import (",
          "14:     get_base_url,",
          "15:     get_meta_refresh,",
          "16:     open_in_browser,",
          "",
          "[Removed Lines]",
          "11: from scrapy.settings.default_settings import DOWNLOAD_MAXSIZE",
          "",
          "[Added Lines]",
          "13:     _remove_html_comments,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "203:             r5, _openfunc=check_base_url",
          "204:         ), \"Inject unique base url with conditional comment\"",
          "207:     def test_open_in_browser_redos_comment(self):",
          "210:         # Exploit input from",
          "211:         # https://makenowjust-labs.github.io/recheck/playground/",
          "212:         # for /<!--.*?-->/ (old pattern to remove comments).",
          "215:         response = HtmlResponse(\"https://example.com\", body=body)",
          "",
          "[Removed Lines]",
          "206:     @pytest.mark.slow",
          "208:         MAX_CPU_TIME = 30",
          "213:         body = b\"-><!--\\x00\" * (int(DOWNLOAD_MAXSIZE / 7) - 10) + b\"->\\n<!---->\"",
          "",
          "[Added Lines]",
          "207:         MAX_CPU_TIME = 0.001",
          "212:         body = b\"-><!--\\x00\" * 25_000 + b\"->\\n<!---->\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "221:         end_time = process_time()",
          "222:         self.assertLess(end_time - start_time, MAX_CPU_TIME)",
          "225:     def test_open_in_browser_redos_head(self):",
          "228:         # Exploit input from",
          "229:         # https://makenowjust-labs.github.io/recheck/playground/",
          "230:         # for /(<head(?:>|\\s.*?>))/ (old pattern to find the head element).",
          "233:         response = HtmlResponse(\"https://example.com\", body=body)",
          "",
          "[Removed Lines]",
          "224:     @pytest.mark.slow",
          "226:         MAX_CPU_TIME = 15",
          "231:         body = b\"<head\\t\" * int(DOWNLOAD_MAXSIZE / 6)",
          "",
          "[Added Lines]",
          "224:         MAX_CPU_TIME = 0.001",
          "229:         body = b\"<head\\t\" * 8_000",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "239:         end_time = process_time()",
          "240:         self.assertLess(end_time - start_time, MAX_CPU_TIME)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "241: @pytest.mark.parametrize(",
          "242:     \"input_body,output_body\",",
          "243:     (",
          "244:         (",
          "245:             b\"a<!--\",",
          "246:             b\"a\",",
          "247:         ),",
          "248:         (",
          "249:             b\"a<!---->b\",",
          "250:             b\"ab\",",
          "251:         ),",
          "252:         (",
          "253:             b\"a<!--b-->c\",",
          "254:             b\"ac\",",
          "255:         ),",
          "256:         (",
          "257:             b\"a<!--b-->c<!--\",",
          "258:             b\"ac\",",
          "259:         ),",
          "260:         (",
          "261:             b\"a<!--b-->c<!--d\",",
          "262:             b\"ac\",",
          "263:         ),",
          "264:         (",
          "265:             b\"a<!--b-->c<!---->d\",",
          "266:             b\"acd\",",
          "267:         ),",
          "268:         (",
          "269:             b\"a<!--b--><!--c-->d\",",
          "270:             b\"ad\",",
          "271:         ),",
          "272:     ),",
          "273: )",
          "274: def test_remove_html_comments(input_body, output_body):",
          "275:     assert (",
          "276:         _remove_html_comments(input_body) == output_body",
          "277:     ), f\"{_remove_html_comments(input_body)=} == {output_body=}\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1045856a50d379d145e514ec9c7aeeed231aefd6",
      "candidate_info": {
        "commit_hash": "1045856a50d379d145e514ec9c7aeeed231aefd6",
        "repo": "scrapy/scrapy",
        "commit_url": "https://github.com/scrapy/scrapy/commit/1045856a50d379d145e514ec9c7aeeed231aefd6",
        "files": [
          "tests/CrawlerProcess/sleeping.py",
          "tests/test_command_shell.py",
          "tests/test_crawler.py"
        ],
        "message": "Merge pull request #6112 from wRAR/test-shutdown-forced\n\nMake shutdown tests more robust.",
        "before_after_code_files": [
          "tests/CrawlerProcess/sleeping.py||tests/CrawlerProcess/sleeping.py",
          "tests/test_command_shell.py||tests/test_command_shell.py",
          "tests/test_crawler.py||tests/test_crawler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/scrapy/scrapy/pull/6222"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/CrawlerProcess/sleeping.py||tests/CrawlerProcess/sleeping.py": [
          "File: tests/CrawlerProcess/sleeping.py -> tests/CrawlerProcess/sleeping.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "14:         from twisted.internet import reactor",
          "16:         d = Deferred()",
          "18:         await maybe_deferred_to_future(d)",
          "",
          "[Removed Lines]",
          "17:         reactor.callLater(3, d.callback, None)",
          "",
          "[Added Lines]",
          "17:         reactor.callLater(int(self.sleep), d.callback, None)",
          "",
          "---------------"
        ],
        "tests/test_command_shell.py||tests/test_command_shell.py": [
          "File: tests/test_command_shell.py -> tests/test_command_shell.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: import sys",
          "2: from io import BytesIO",
          "3: from pathlib import Path",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: import os",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "147:             \"scrapy.cmdline\",",
          "148:             \"shell\",",
          "149:         )",
          "150:         logfile = BytesIO()",
          "152:         p.logfile_read = logfile",
          "153:         p.expect_exact(\"Available Scrapy objects\")",
          "154:         with MockServer() as mockserver:",
          "",
          "[Removed Lines]",
          "151:         p = PopenSpawn(args, timeout=5)",
          "",
          "[Added Lines]",
          "151:         env = os.environ.copy()",
          "152:         env[\"SCRAPY_PYTHON_SHELL\"] = \"python\"",
          "154:         p = PopenSpawn(args, env=env, timeout=5)",
          "",
          "---------------"
        ],
        "tests/test_crawler.py||tests/test_crawler.py": [
          "File: tests/test_crawler.py -> tests/test_crawler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "526:     def test_shutdown_graceful(self):",
          "527:         sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK",
          "529:         p = PopenSpawn(args, timeout=5)",
          "530:         p.expect_exact(\"Spider opened\")",
          "531:         p.expect_exact(\"Crawled (200)\")",
          "",
          "[Removed Lines]",
          "528:         args = self.get_script_args(\"sleeping.py\")",
          "",
          "[Added Lines]",
          "528:         args = self.get_script_args(\"sleeping.py\", \"-a\", \"sleep=3\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "534:         p.expect_exact(\"Spider closed (shutdown)\")",
          "535:         p.wait()",
          "537:     def test_shutdown_forced(self):",
          "538:         sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK",
          "540:         p = PopenSpawn(args, timeout=5)",
          "541:         p.expect_exact(\"Spider opened\")",
          "542:         p.expect_exact(\"Crawled (200)\")",
          "543:         p.kill(sig)",
          "544:         p.expect_exact(\"shutting down gracefully\")",
          "545:         p.kill(sig)",
          "546:         p.expect_exact(\"forcing unclean shutdown\")",
          "547:         p.wait()",
          "",
          "[Removed Lines]",
          "539:         args = self.get_script_args(\"sleeping.py\")",
          "",
          "[Added Lines]",
          "537:     @defer.inlineCallbacks",
          "539:         from twisted.internet import reactor",
          "542:         args = self.get_script_args(\"sleeping.py\", \"-a\", \"sleep=10\")",
          "548:         # sending the second signal too fast often causes problems",
          "549:         d = defer.Deferred()",
          "550:         reactor.callLater(0.1, d.callback, None)",
          "551:         yield d",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2538c0e8629b46bd34bb0555aeca6714a6f9e571",
      "candidate_info": {
        "commit_hash": "2538c0e8629b46bd34bb0555aeca6714a6f9e571",
        "repo": "scrapy/scrapy",
        "commit_url": "https://github.com/scrapy/scrapy/commit/2538c0e8629b46bd34bb0555aeca6714a6f9e571",
        "files": [
          "scrapy/utils/iterators.py"
        ],
        "message": "Restore the implementation of xmliter",
        "before_after_code_files": [
          "scrapy/utils/iterators.py||scrapy/utils/iterators.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/scrapy/scrapy/pull/6222"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scrapy/utils/iterators.py||scrapy/utils/iterators.py": [
          "File: scrapy/utils/iterators.py -> scrapy/utils/iterators.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "43:     \"\"\"",
          "44:     nodename_patt = re.escape(nodename)",
          "47:     HEADER_END_RE = re.compile(rf\"<\\s*/{nodename_patt}\\s*>\", re.S)",
          "50:     text = _body_or_str(obj)",
          "52:     document_header_match = re.search(DOCUMENT_HEADER_RE, text)",
          "",
          "[Removed Lines]",
          "46:     DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]{1,1024}>\\s*\", re.S)",
          "48:     END_TAG_RE = re.compile(r\"<\\s*/([^\\s>]{1,1024})\\s*>\", re.S)",
          "49:     NAMESPACE_RE = re.compile(r\"((xmlns[:A-Za-z]{,1024})=[^>\\s]+)\", re.S)",
          "",
          "[Added Lines]",
          "46:     DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]+>\\s*\", re.S)",
          "48:     END_TAG_RE = re.compile(r\"<\\s*/([^\\s>]+)\\s*>\", re.S)",
          "49:     NAMESPACE_RE = re.compile(r\"((xmlns[:A-Za-z]*)=[^>\\s]+)\", re.S)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "60:         for tagname in reversed(re.findall(END_TAG_RE, header_end)):",
          "61:             assert header_end_idx",
          "62:             tag = re.search(",
          "66:             )",
          "67:             if tag:",
          "68:                 for x in re.findall(NAMESPACE_RE, tag.group()):",
          "69:                     namespaces[x[1]] = x[0]",
          "72:     for match in r.finditer(text):",
          "73:         nodetext = (",
          "74:             document_header",
          "",
          "[Removed Lines]",
          "63:                 rf\"<\\s*{tagname}.{{,1024}}?xmlns[:=][^>]{{,1024}}>\",",
          "64:                 text[: header_end_idx[1]],",
          "65:                 re.S,",
          "71:     r = re.compile(rf\"<{nodename_patt}[\\s>].{{,1024}}?</{nodename_patt}>\", re.DOTALL)",
          "",
          "[Added Lines]",
          "63:                 rf\"<\\s*{tagname}.*?xmlns[:=][^>]*>\", text[: header_end_idx[1]], re.S",
          "69:     r = re.compile(rf\"<{nodename_patt}[\\s>].*?</{nodename_patt}>\", re.DOTALL)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0bf29a7b1b9b6a641c486780b7b0fa455577bf39",
      "candidate_info": {
        "commit_hash": "0bf29a7b1b9b6a641c486780b7b0fa455577bf39",
        "repo": "scrapy/scrapy",
        "commit_url": "https://github.com/scrapy/scrapy/commit/0bf29a7b1b9b6a641c486780b7b0fa455577bf39",
        "files": [
          "scrapy/downloadermiddlewares/httpcompression.py",
          "tests/test_downloadermiddleware_httpcompression.py"
        ],
        "message": "Update test expectations",
        "before_after_code_files": [
          "scrapy/downloadermiddlewares/httpcompression.py||scrapy/downloadermiddlewares/httpcompression.py",
          "tests/test_downloadermiddleware_httpcompression.py||tests/test_downloadermiddleware_httpcompression.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/scrapy/scrapy/pull/6222"
        ],
        "olp_code_files": {
          "patch": [
            "scrapy/downloadermiddlewares/httpcompression.py||scrapy/downloadermiddlewares/httpcompression.py",
            "tests/test_downloadermiddleware_httpcompression.py||tests/test_downloadermiddleware_httpcompression.py"
          ],
          "candidate": [
            "scrapy/downloadermiddlewares/httpcompression.py||scrapy/downloadermiddlewares/httpcompression.py",
            "tests/test_downloadermiddleware_httpcompression.py||tests/test_downloadermiddleware_httpcompression.py"
          ]
        }
      },
      "candidate_diff": {
        "scrapy/downloadermiddlewares/httpcompression.py||scrapy/downloadermiddlewares/httpcompression.py": [
          "File: scrapy/downloadermiddlewares/httpcompression.py -> scrapy/downloadermiddlewares/httpcompression.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30:     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be",
          "31:     sent/received from web sites\"\"\"",
          "34:         self.stats = crawler.stats",
          "35:         self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")",
          "",
          "[Removed Lines]",
          "33:     def __init__(self, crawler=None):",
          "",
          "[Added Lines]",
          "33:     def __init__(self, *, crawler=None):",
          "34:         if not crawler:",
          "35:             return",
          "",
          "---------------"
        ],
        "tests/test_downloadermiddleware_httpcompression.py||tests/test_downloadermiddleware_httpcompression.py": [
          "File: tests/test_downloadermiddleware_httpcompression.py -> tests/test_downloadermiddleware_httpcompression.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "127:         self.assertStatsEqual(\"httpcompression/response_count\", 1)",
          "128:         self.assertStatsEqual(\"httpcompression/response_bytes\", 74837)",
          "142:     def test_process_response_br(self):",
          "143:         try:",
          "144:             import brotli  # noqa: F401",
          "",
          "[Removed Lines]",
          "130:     def test_process_response_gzip_no_stats(self):",
          "131:         mw = HttpCompressionMiddleware()",
          "132:         response = self._getresponse(\"gzip\")",
          "133:         request = response.request",
          "135:         self.assertEqual(response.headers[\"Content-Encoding\"], b\"gzip\")",
          "136:         newresponse = mw.process_response(request, response, self.spider)",
          "137:         self.assertEqual(mw.stats, None)",
          "138:         assert newresponse is not response",
          "139:         assert newresponse.body.startswith(b\"<!DOCTYPE\")",
          "140:         assert \"Content-Encoding\" not in newresponse.headers",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "418:             (",
          "419:                 (",
          "420:                     \"HttpCompressionMiddleware subclasses must either modify \"",
          "423:                 ),",
          "424:             ),",
          "425:         )",
          "",
          "[Removed Lines]",
          "421:                     \"their '__init__' method to support a 'stats' parameter \"",
          "422:                     \"or reimplement the 'from_crawler' method.\"",
          "",
          "[Added Lines]",
          "409:                     \"their '__init__' method to support a 'crawler' parameter \"",
          "410:                     \"or reimplement their 'from_crawler' method.\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1c3d28979852aa8370ffdb9187f5864a5dfb2b1c",
      "candidate_info": {
        "commit_hash": "1c3d28979852aa8370ffdb9187f5864a5dfb2b1c",
        "repo": "scrapy/scrapy",
        "commit_url": "https://github.com/scrapy/scrapy/commit/1c3d28979852aa8370ffdb9187f5864a5dfb2b1c",
        "files": [
          "docs/news.rst",
          "docs/topics/request-response.rst",
          "docs/topics/settings.rst",
          "scrapy/downloadermiddlewares/httpcompression.py",
          "scrapy/spiders/sitemap.py",
          "scrapy/utils/_compression.py",
          "scrapy/utils/gz.py",
          "tests/sample_data/compressed/bomb-br.bin",
          "tests/sample_data/compressed/bomb-deflate.bin",
          "tests/sample_data/compressed/bomb-gzip.bin",
          "tests/sample_data/compressed/bomb-zstd.bin",
          "tests/test_downloader_handlers.py",
          "tests/test_downloadermiddleware_httpcompression.py",
          "tests/test_spider.py"
        ],
        "message": "Protect against compression bombs",
        "before_after_code_files": [
          "scrapy/downloadermiddlewares/httpcompression.py||scrapy/downloadermiddlewares/httpcompression.py",
          "scrapy/spiders/sitemap.py||scrapy/spiders/sitemap.py",
          "scrapy/utils/_compression.py||scrapy/utils/_compression.py",
          "scrapy/utils/gz.py||scrapy/utils/gz.py",
          "tests/test_downloader_handlers.py||tests/test_downloader_handlers.py",
          "tests/test_downloadermiddleware_httpcompression.py||tests/test_downloadermiddleware_httpcompression.py",
          "tests/test_spider.py||tests/test_spider.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "diff_branch_olp_changes": 1,
        "olp_code_files": {
          "patch": [
            "scrapy/downloadermiddlewares/httpcompression.py||scrapy/downloadermiddlewares/httpcompression.py",
            "scrapy/spiders/sitemap.py||scrapy/spiders/sitemap.py",
            "scrapy/utils/_compression.py||scrapy/utils/_compression.py",
            "scrapy/utils/gz.py||scrapy/utils/gz.py",
            "tests/test_downloadermiddleware_httpcompression.py||tests/test_downloadermiddleware_httpcompression.py",
            "tests/test_spider.py||tests/test_spider.py"
          ],
          "candidate": [
            "scrapy/downloadermiddlewares/httpcompression.py||scrapy/downloadermiddlewares/httpcompression.py",
            "scrapy/spiders/sitemap.py||scrapy/spiders/sitemap.py",
            "scrapy/utils/_compression.py||scrapy/utils/_compression.py",
            "scrapy/utils/gz.py||scrapy/utils/gz.py",
            "tests/test_downloadermiddleware_httpcompression.py||tests/test_downloadermiddleware_httpcompression.py",
            "tests/test_spider.py||tests/test_spider.py"
          ]
        }
      },
      "candidate_diff": {
        "scrapy/downloadermiddlewares/httpcompression.py||scrapy/downloadermiddlewares/httpcompression.py": [
          "File: scrapy/downloadermiddlewares/httpcompression.py -> scrapy/downloadermiddlewares/httpcompression.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "4: from scrapy.http import Response, TextResponse",
          "5: from scrapy.responsetypes import responsetypes",
          "11: try:",
          "14: except ImportError:",
          "15:     pass",
          "18: class HttpCompressionMiddleware(object):",
          "19:     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be",
          "20:     sent/received from web sites\"\"\"",
          "21:     @classmethod",
          "22:     def from_crawler(cls, crawler):",
          "23:         if not crawler.settings.getbool('COMPRESSION_ENABLED'):",
          "24:             raise NotConfigured",
          "27:     def process_request(self, request, spider):",
          "28:         request.headers.setdefault('Accept-Encoding',",
          "",
          "[Removed Lines]",
          "1: import zlib",
          "3: from scrapy.utils.gz import gunzip",
          "6: from scrapy.exceptions import NotConfigured",
          "9: ACCEPTED_ENCODINGS = [b'gzip', b'deflate']",
          "12:     import brotli",
          "13:     ACCEPTED_ENCODINGS.append(b'br')",
          "25:         return cls()",
          "",
          "[Added Lines]",
          "1: import warnings",
          "2: from logging import getLogger",
          "4: from scrapy import signals",
          "5: from scrapy.exceptions import IgnoreRequest, NotConfigured",
          "8: from scrapy.utils._compression import (",
          "9:     _DecompressionMaxSizeExceeded,",
          "10:     _inflate,",
          "11:     _unbrotli,",
          "12:     _unzstd,",
          "13: )",
          "14: from scrapy.utils.deprecate import ScrapyDeprecationWarning",
          "15: from scrapy.utils.gz import gunzip",
          "17: logger = getLogger(__name__)",
          "19: ACCEPTED_ENCODINGS = [b\"gzip\", b\"deflate\"]",
          "21: try:",
          "22:     import brotli  # noqa: F401",
          "23: except ImportError:",
          "24:     pass",
          "25: else:",
          "26:     ACCEPTED_ENCODINGS.append(b\"br\")",
          "29:     import zstandard  # noqa: F401",
          "32: else:",
          "33:     ACCEPTED_ENCODINGS.append(b\"zstd\")",
          "40:     def __init__(self, crawler=None):",
          "41:         if not crawler:",
          "42:             return",
          "43:         self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")",
          "44:         self._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")",
          "45:         crawler.signals.connect(self.open_spider, signals.spider_opened)",
          "51:         try:",
          "52:             return cls(crawler=crawler)",
          "53:         except TypeError:",
          "54:             warnings.warn(",
          "55:                 \"HttpCompressionMiddleware subclasses must either modify \"",
          "56:                 \"their '__init__' method to support a 'crawler' parameter or \"",
          "57:                 \"reimplement their 'from_crawler' method.\",",
          "58:                 ScrapyDeprecationWarning,",
          "59:             )",
          "60:             spider = cls()",
          "61:             spider._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")",
          "62:             spider._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")",
          "63:             crawler.signals.connect(spider.open_spider, signals.spider_opened)",
          "64:             return spider",
          "66:     def open_spider(self, spider):",
          "67:         if hasattr(spider, \"download_maxsize\"):",
          "68:             self._max_size = spider.download_maxsize",
          "69:         if hasattr(spider, \"download_warnsize\"):",
          "70:             self._warn_size = spider.download_warnsize",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "36:             content_encoding = response.headers.getlist('Content-Encoding')",
          "37:             if content_encoding:",
          "38:                 encoding = content_encoding.pop()",
          "42:                 kwargs = dict(cls=respcls, body=decoded_body)",
          "43:                 if issubclass(respcls, TextResponse):",
          "44:                     # force recalculating the encoding until we make sure the",
          "",
          "[Removed Lines]",
          "39:                 decoded_body = self._decode(response.body, encoding.lower())",
          "40:                 respcls = responsetypes.from_args(headers=response.headers, \\",
          "41:                     url=response.url, body=decoded_body)",
          "",
          "[Added Lines]",
          "84:                 max_size = request.meta.get(\"download_maxsize\", self._max_size)",
          "85:                 warn_size = request.meta.get(\"download_warnsize\", self._warn_size)",
          "86:                 try:",
          "87:                     decoded_body = self._decode(",
          "88:                         response.body, encoding.lower(), max_size",
          "89:                     )",
          "90:                 except _DecompressionMaxSizeExceeded:",
          "91:                     raise IgnoreRequest(",
          "92:                         \"Ignored response {response} because its body \"",
          "93:                         \"({body_size} B) exceeded DOWNLOAD_MAXSIZE \"",
          "94:                         \"({max_size} B) during decompression.\".format(",
          "95:                             response=response,",
          "96:                             body_size=len(response.body),",
          "97:                             max_size=max_size,",
          "98:                         )",
          "99:                     )",
          "100:                 if len(response.body) < warn_size <= len(decoded_body):",
          "101:                     logger.warning(",
          "102:                         \"%(response)s body size after decompression \"",
          "103:                         \"(%(body_size)s B) is larger than the \"",
          "104:                         \"download warning size (%(warn_size)s B).\",",
          "105:                         {",
          "106:                             \"response\": response,",
          "107:                             \"body_size\": len(decoded_body),",
          "108:                             \"warn_size\": warn_size,",
          "109:                         },",
          "110:                     )",
          "111:                 respcls = responsetypes.from_args(",
          "112:                     headers=response.headers, url=response.url, body=decoded_body",
          "113:                 )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "51:         return response",
          "69:         return body",
          "",
          "[Removed Lines]",
          "53:     def _decode(self, body, encoding):",
          "54:         if encoding == b'gzip' or encoding == b'x-gzip':",
          "55:             body = gunzip(body)",
          "57:         if encoding == b'deflate':",
          "58:             try:",
          "59:                 body = zlib.decompress(body)",
          "60:             except zlib.error:",
          "61:                 # ugly hack to work with raw deflate content that may",
          "62:                 # be sent by microsoft servers. For more information, see:",
          "63:                 # http://carsten.codimi.de/gzip.yaws/",
          "64:                 # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx",
          "65:                 # http://www.gzip.org/zlib/zlib_faq.html#faq38",
          "66:                 body = zlib.decompress(body, -15)",
          "67:         if encoding == b'br' and b'br' in ACCEPTED_ENCODINGS:",
          "68:             body = brotli.decompress(body)",
          "",
          "[Added Lines]",
          "125:     def _decode(self, body, encoding, max_size):",
          "126:         if encoding == b\"gzip\" or encoding == b\"x-gzip\":",
          "127:             return gunzip(body, max_size=max_size)",
          "128:         if encoding == b\"deflate\":",
          "129:             return _inflate(body, max_size=max_size)",
          "130:         if encoding == b\"br\" and b\"br\" in ACCEPTED_ENCODINGS:",
          "131:             return _unbrotli(body, max_size=max_size)",
          "132:         if encoding == b\"zstd\" and b\"zstd\" in ACCEPTED_ENCODINGS:",
          "133:             return _unzstd(body, max_size=max_size)",
          "",
          "---------------"
        ],
        "scrapy/spiders/sitemap.py||scrapy/spiders/sitemap.py": [
          "File: scrapy/spiders/sitemap.py -> scrapy/spiders/sitemap.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2: import logging",
          "3: import six",
          "8: from scrapy.utils.gz import gunzip, gzip_magic_number",
          "11: logger = logging.getLogger(__name__)",
          "",
          "[Removed Lines]",
          "1: import re",
          "5: from scrapy.spiders import Spider",
          "6: from scrapy.http import Request, XmlResponse",
          "7: from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots",
          "",
          "[Added Lines]",
          "2: import re",
          "6: from scrapy.http.response.xml import XmlResponse",
          "7: from scrapy.spiders import Request, Spider",
          "8: from scrapy.utils._compression import _DecompressionMaxSizeExceeded",
          "10: from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "18:     sitemap_follow = ['']",
          "19:     sitemap_alternate_links = False",
          "21:     def __init__(self, *a, **kw):",
          "22:         super(SitemapSpider, self).__init__(*a, **kw)",
          "23:         self._cbs = []",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22:     @classmethod",
          "23:     def from_crawler(cls, crawler, *args, **kwargs):",
          "24:         spider = super(SitemapSpider, cls).from_crawler(crawler, *args, **kwargs)",
          "25:         spider._max_size = getattr(",
          "26:             spider, \"download_maxsize\", spider.settings.getint(\"DOWNLOAD_MAXSIZE\")",
          "27:         )",
          "28:         spider._warn_size = getattr(",
          "29:             spider, \"download_warnsize\", spider.settings.getint(\"DOWNLOAD_WARNSIZE\")",
          "30:         )",
          "31:         return spider",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "70:         \"\"\"",
          "71:         if isinstance(response, XmlResponse):",
          "72:             return response.body",
          "75:         # actual gzipped sitemap files are decompressed above ;",
          "76:         # if we are here (response body is not gzipped)",
          "77:         # and have a response for .xml.gz,",
          "",
          "[Removed Lines]",
          "73:         elif gzip_magic_number(response):",
          "74:             return gunzip(response.body)",
          "",
          "[Added Lines]",
          "85:         if gzip_magic_number(response):",
          "86:             uncompressed_size = len(response.body)",
          "87:             max_size = response.meta.get(\"download_maxsize\", self._max_size)",
          "88:             warn_size = response.meta.get(\"download_warnsize\", self._warn_size)",
          "89:             try:",
          "90:                 body = gunzip(response.body, max_size=max_size)",
          "91:             except _DecompressionMaxSizeExceeded:",
          "92:                 return None",
          "93:             if uncompressed_size < warn_size <= len(body):",
          "94:                 logger.warning(",
          "95:                     \"%(response)s body size after decompression (%(body_length)s B) \"",
          "96:                     \"is larger than the download warning size (%(warn_size)s B).\",",
          "97:                     {",
          "98:                         \"response\": response,",
          "99:                         \"body_length\": len(body),",
          "100:                         \"warn_size\": warn_size,",
          "101:                     },",
          "102:                 )",
          "103:             return body",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "81:         # without actually being a .xml.gz file in the first place,",
          "82:         # merely XML gzip-compressed on the fly,",
          "83:         # in other word, here, we have plain XML",
          "85:             return response.body",
          "",
          "[Removed Lines]",
          "84:         elif response.url.endswith('.xml') or response.url.endswith('.xml.gz'):",
          "",
          "[Added Lines]",
          "113:         if response.url.endswith('.xml') or response.url.endswith('.xml.gz'):",
          "",
          "---------------"
        ],
        "scrapy/utils/_compression.py||scrapy/utils/_compression.py": [
          "File: scrapy/utils/_compression.py -> scrapy/utils/_compression.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: import zlib",
          "2: from io import BytesIO",
          "4: try:",
          "5:     import brotli",
          "6: except ImportError:",
          "7:     pass",
          "9: try:",
          "10:     import zstandard",
          "11: except ImportError:",
          "12:     pass",
          "15: class _DecompressionMaxSizeExceeded(ValueError):",
          "16:     pass",
          "19: def _inflate(data, max_size=0):",
          "20:     decompressor = zlib.decompressobj()",
          "21:     raw_decompressor = zlib.decompressobj(-15)",
          "22:     input_stream = BytesIO(data)",
          "23:     output_list = []",
          "24:     output_chunk = b\".\"",
          "25:     decompressed_size = 0",
          "26:     CHUNK_SIZE = 8196",
          "27:     while output_chunk:",
          "28:         input_chunk = input_stream.read(CHUNK_SIZE)",
          "29:         try:",
          "30:             output_chunk = decompressor.decompress(input_chunk)",
          "31:         except zlib.error:",
          "32:             if decompressor != raw_decompressor:",
          "33:                 # ugly hack to work with raw deflate content that may",
          "34:                 # be sent by microsoft servers. For more information, see:",
          "35:                 # http://carsten.codimi.de/gzip.yaws/",
          "36:                 # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx",
          "37:                 # http://www.gzip.org/zlib/zlib_faq.html#faq38",
          "38:                 decompressor = raw_decompressor",
          "39:                 output_chunk = decompressor.decompress(input_chunk)",
          "40:             else:",
          "41:                 raise",
          "42:         decompressed_size += len(output_chunk)",
          "43:         if max_size and decompressed_size > max_size:",
          "44:             raise _DecompressionMaxSizeExceeded(",
          "45:                 \"The number of bytes decompressed so far \"",
          "46:                 \"({decompressed_size} B) exceed the specified maximum \"",
          "47:                 \"({max_size} B).\".format(",
          "48:                     decompressed_size=decompressed_size,",
          "49:                     max_size=max_size,",
          "50:                 )",
          "51:             )",
          "52:         output_list.append(output_chunk)",
          "53:     return b\"\".join(output_list)",
          "56: def _unbrotli(data, max_size=0):",
          "57:     decompressor = brotli.Decompressor()",
          "58:     input_stream = BytesIO(data)",
          "59:     output_list = []",
          "60:     output_chunk = b\".\"",
          "61:     decompressed_size = 0",
          "62:     CHUNK_SIZE = 8196",
          "63:     while output_chunk:",
          "64:         input_chunk = input_stream.read(CHUNK_SIZE)",
          "65:         output_chunk = decompressor.decompress(input_chunk)",
          "66:         decompressed_size += len(output_chunk)",
          "67:         if max_size and decompressed_size > max_size:",
          "68:             raise _DecompressionMaxSizeExceeded(",
          "69:                 \"The number of bytes decompressed so far \"",
          "70:                 \"({decompressed_size} B) exceed the specified maximum \"",
          "71:                 \"({max_size} B).\".format(",
          "72:                     decompressed_size=decompressed_size,",
          "73:                     max_size=max_size,",
          "74:                 )",
          "75:             )",
          "76:         output_list.append(output_chunk)",
          "77:     return b\"\".join(output_list)",
          "80: def _unzstd(data, max_size=0):",
          "81:     decompressor = zstandard.ZstdDecompressor()",
          "82:     stream_reader = decompressor.stream_reader(BytesIO(data))",
          "83:     output_list = []",
          "84:     output_chunk = b\".\"",
          "85:     decompressed_size = 0",
          "86:     CHUNK_SIZE = 8196",
          "87:     while output_chunk:",
          "88:         output_chunk = stream_reader.read(CHUNK_SIZE)",
          "89:         decompressed_size += len(output_chunk)",
          "90:         if max_size and decompressed_size > max_size:",
          "91:             raise _DecompressionMaxSizeExceeded(",
          "92:                 \"The number of bytes decompressed so far \"",
          "93:                 \"({decompressed_size} B) exceed the specified maximum \"",
          "94:                 \"({max_size} B).\".format(",
          "95:                     decompressed_size=decompressed_size,",
          "96:                     max_size=max_size,",
          "97:                 )",
          "98:             )",
          "99:         output_list.append(output_chunk)",
          "100:     return b\"\".join(output_list)",
          "",
          "---------------"
        ],
        "scrapy/utils/gz.py||scrapy/utils/gz.py": [
          "File: scrapy/utils/gz.py -> scrapy/utils/gz.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "4:     from cStringIO import StringIO as BytesIO",
          "5: except ImportError:",
          "6:     from io import BytesIO",
          "7: from gzip import GzipFile",
          "9: import six",
          "12: from scrapy.utils.decorators import deprecated",
          "15: # - Python>=3.5 GzipFile's read() has issues returning leftover",
          "16: #   uncompressed data when input is corrupted",
          "",
          "[Removed Lines]",
          "10: import re",
          "",
          "[Added Lines]",
          "8: import re",
          "15: from ._compression import _DecompressionMaxSizeExceeded",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "27:         return gzf.read1(size)",
          "31:     \"\"\"Gunzip the given data and return as much data as possible.",
          "33:     This is resilient to CRC checksum errors.",
          "34:     \"\"\"",
          "35:     f = GzipFile(fileobj=BytesIO(data))",
          "36:     output_list = []",
          "38:     while chunk:",
          "39:         try:",
          "40:             chunk = read1(f, 8196)",
          "42:         except (IOError, EOFError, struct.error):",
          "43:             # complete only if there is some data, otherwise re-raise",
          "44:             # see issue 87 about catching struct.error",
          "",
          "[Removed Lines]",
          "30: def gunzip(data):",
          "37:     chunk = b'.'",
          "41:             output_list.append(chunk)",
          "",
          "[Added Lines]",
          "32: def gunzip(data, max_size=0):",
          "39:     chunk = b\".\"",
          "40:     decompressed_size = 0",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "51:                     break",
          "52:             else:",
          "53:                 raise",
          "56: _is_gzipped = re.compile(br'^application/(x-)?gzip\\b', re.I).search",
          "57: _is_octetstream = re.compile(br'^(application|binary)/octet-stream\\b', re.I).search",
          "",
          "[Removed Lines]",
          "54:     return b''.join(output_list)",
          "",
          "[Added Lines]",
          "56:         decompressed_size += len(chunk)",
          "57:         if max_size and decompressed_size > max_size:",
          "58:             raise _DecompressionMaxSizeExceeded(",
          "59:                 \"The number of bytes decompressed so far \"",
          "60:                 \"({decompressed_size} B) exceed the specified maximum \"",
          "61:                 \"({max_size} B).\".format(",
          "62:                     decompressed_size=decompressed_size,",
          "63:                     max_size=max_size,",
          "64:                 )",
          "65:             )",
          "66:         output_list.append(chunk)",
          "67:     return b\"\".join(output_list)",
          "",
          "---------------"
        ],
        "tests/test_downloader_handlers.py||tests/test_downloader_handlers.py": [
          "File: tests/test_downloader_handlers.py -> tests/test_downloader_handlers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "37: from scrapy.settings import Settings",
          "38: from scrapy.utils.test import get_crawler, skip_if_no_boto",
          "39: from scrapy.utils.python import to_bytes",
          "42: from tests.mockserver import MockServer, ssl_context_factory, Echo",
          "43: from tests.spiders import SingleRequestSpider",
          "",
          "[Removed Lines]",
          "40: from scrapy.exceptions import NotConfigured",
          "",
          "[Added Lines]",
          "40: from scrapy.exceptions import IgnoreRequest, NotConfigured",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "637:             request.headers.setdefault(b'Accept-Encoding', b'gzip,deflate')",
          "638:             request = request.replace(url=self.mockserver.url('/xpayload'))",
          "639:             yield crawler.crawl(seed=request)",
          "641:             failure = crawler.spider.meta.get('failure')",
          "645:         else:",
          "646:             # See issue https://twistedmatrix.com/trac/ticket/8175",
          "647:             raise unittest.SkipTest(\"xpayload only enabled for PY2\")",
          "",
          "[Removed Lines]",
          "640:             # download_maxsize = 50 is enough for the gzipped response",
          "642:             self.assertTrue(failure == None)",
          "643:             reason = crawler.spider.meta['close_reason']",
          "644:             self.assertTrue(reason, 'finished')",
          "",
          "[Added Lines]",
          "640:             # The gzipped response passes the download_maxsize = 50 during",
          "641:             # download, but fails during decompression.",
          "643:             self.assertIsInstance(failure.value, IgnoreRequest)",
          "",
          "---------------"
        ],
        "tests/test_downloadermiddleware_httpcompression.py||tests/test_downloadermiddleware_httpcompression.py": [
          "File: tests/test_downloadermiddleware_httpcompression.py -> tests/test_downloadermiddleware_httpcompression.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: from io import BytesIO",
          "3: from os.path import join",
          "10: from scrapy.responsetypes import responsetypes",
          "11: from scrapy.utils.gz import gunzip",
          "12: from tests import tests_datadir",
          "16: SAMPLEDIR = join(tests_datadir, 'compressed')",
          "18: FORMAT = {",
          "27: class HttpCompressionTest(TestCase):",
          "29:     def setUp(self):",
          "30:         self.spider = Spider('foo')",
          "33:     def _getresponse(self, coding):",
          "34:         if coding not in FORMAT:",
          "",
          "[Removed Lines]",
          "2: from unittest import TestCase, SkipTest",
          "4: from gzip import GzipFile",
          "6: from scrapy.spiders import Spider",
          "7: from scrapy.http import Response, Request, HtmlResponse",
          "8: from scrapy.downloadermiddlewares.httpcompression import HttpCompressionMiddleware, \\",
          "9:     ACCEPTED_ENCODINGS",
          "13: from w3lib.encoding import resolve_encoding",
          "19:         'gzip': ('html-gzip.bin', 'gzip'),",
          "20:         'x-gzip': ('html-gzip.bin', 'gzip'),",
          "21:         'rawdeflate': ('html-rawdeflate.bin', 'deflate'),",
          "22:         'zlibdeflate': ('html-zlibdeflate.bin', 'deflate'),",
          "23:         'br': ('html-br.bin', 'br')",
          "24:         }",
          "31:         self.mw = HttpCompressionMiddleware()",
          "",
          "[Added Lines]",
          "1: from gzip import GzipFile",
          "3: from logging import WARNING",
          "5: from unittest import SkipTest, TestCase",
          "7: from testfixtures import LogCapture",
          "8: from w3lib.encoding import resolve_encoding",
          "10: from scrapy.downloadermiddlewares.httpcompression import (",
          "11:     ACCEPTED_ENCODINGS,",
          "12:     HttpCompressionMiddleware,",
          "13: )",
          "14: from scrapy.exceptions import IgnoreRequest",
          "15: from scrapy.http import HtmlResponse, Request, Response",
          "17: from scrapy.spiders import Spider",
          "19: from scrapy.utils.test import get_crawler",
          "25:     \"gzip\": (\"html-gzip.bin\", \"gzip\"),",
          "26:     \"x-gzip\": (\"html-gzip.bin\", \"gzip\"),",
          "27:     \"rawdeflate\": (\"html-rawdeflate.bin\", \"deflate\"),",
          "28:     \"zlibdeflate\": (\"html-zlibdeflate.bin\", \"deflate\"),",
          "29:     \"br\": (\"html-br.bin\", \"br\"),",
          "30:     # $ zstd raw.html --content-size -o html-zstd-static-content-size.bin",
          "31:     \"zstd-static-content-size\": (\"html-zstd-static-content-size.bin\", \"zstd\"),",
          "32:     # $ zstd raw.html --no-content-size -o html-zstd-static-no-content-size.bin",
          "33:     \"zstd-static-no-content-size\": (\"html-zstd-static-no-content-size.bin\", \"zstd\"),",
          "34:     # $ cat raw.html | zstd -o html-zstd-streaming-no-content-size.bin",
          "35:     \"zstd-streaming-no-content-size\": (",
          "36:         \"html-zstd-streaming-no-content-size.bin\",",
          "37:         \"zstd\",",
          "38:     ),",
          "39: }",
          "40: FORMAT.update(",
          "41:     {",
          "42:         \"bomb-{format_id}\".format(format_id=format_id): (\"bomb-{format_id}.bin\".format(format_id=format_id), format_id)",
          "43:         for format_id in (",
          "44:             \"br\",  # 34 -> 11 511 612",
          "45:             \"deflate\",  # 27 968 -> 11 511 612",
          "46:             \"gzip\",  # 27 988 -> 11 511 612",
          "47:             \"zstd\",  # 1 096 -> 11 511 612",
          "48:         )",
          "49:     }",
          "50: )",
          "56:         crawler = get_crawler()",
          "58:         self.mw = HttpCompressionMiddleware.from_crawler(crawler)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "65:         self.assertEqual(response.headers['Content-Encoding'], b'gzip')",
          "66:         newresponse = self.mw.process_response(request, response, self.spider)",
          "67:         assert newresponse is not response",
          "71:     def test_process_response_br(self):",
          "72:         try:",
          "",
          "[Removed Lines]",
          "68:         assert newresponse.body.startswith(b'<!DOCTYPE')",
          "69:         assert 'Content-Encoding' not in newresponse.headers",
          "",
          "[Added Lines]",
          "95:         assert newresponse.body.startswith(b\"<!DOCTYPE\")",
          "96:         assert \"Content-Encoding\" not in newresponse.headers",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "248:         response = response.replace(body = None)",
          "249:         newresponse = self.mw.process_response(request, response, self.spider)",
          "250:         self.assertIs(newresponse, response)",
          "",
          "[Removed Lines]",
          "251:         self.assertEqual(response.body, b'')",
          "",
          "[Added Lines]",
          "278:         self.assertEqual(response.body, b\"\")",
          "280:     def _test_compression_bomb_setting(self, compression_id):",
          "281:         settings = {\"DOWNLOAD_MAXSIZE\": 10000000}",
          "282:         crawler = get_crawler(Spider, settings_dict=settings)",
          "283:         spider = crawler._create_spider(\"scrapytest.org\")",
          "284:         mw = HttpCompressionMiddleware.from_crawler(crawler)",
          "285:         mw.open_spider(spider)",
          "287:         response = self._getresponse(\"bomb-{compression_id}\".format(compression_id=compression_id))",
          "288:         self.assertRaises(",
          "289:             IgnoreRequest,",
          "290:             mw.process_response,",
          "291:             response.request,",
          "292:             response,",
          "293:             spider,",
          "294:         )",
          "296:     def test_compression_bomb_setting_br(self):",
          "297:         try:",
          "298:             import brotli  # noqa: F401",
          "299:         except ImportError:",
          "300:             raise SkipTest(\"no brotli\")",
          "301:         self._test_compression_bomb_setting(\"br\")",
          "303:     def test_compression_bomb_setting_deflate(self):",
          "304:         self._test_compression_bomb_setting(\"deflate\")",
          "306:     def test_compression_bomb_setting_gzip(self):",
          "307:         self._test_compression_bomb_setting(\"gzip\")",
          "309:     def test_compression_bomb_setting_zstd(self):",
          "310:         try:",
          "311:             import zstandard",
          "312:         except ImportError:",
          "313:             raise SkipTest(\"no zstandard\")",
          "314:         self._test_compression_bomb_setting(\"zstd\")",
          "316:     def _test_compression_bomb_spider_attr(self, compression_id):",
          "317:         class DownloadMaxSizeSpider(Spider):",
          "318:             download_maxsize = 10000000",
          "320:         crawler = get_crawler(DownloadMaxSizeSpider)",
          "321:         spider = crawler._create_spider(\"scrapytest.org\")",
          "322:         mw = HttpCompressionMiddleware.from_crawler(crawler)",
          "323:         mw.open_spider(spider)",
          "325:         response = self._getresponse(\"bomb-{compression_id}\".format(compression_id=compression_id))",
          "326:         self.assertRaises(",
          "327:             IgnoreRequest,",
          "328:             mw.process_response,",
          "329:             response.request,",
          "330:             response,",
          "331:             spider,",
          "332:         )",
          "334:     def test_compression_bomb_spider_attr_br(self):",
          "335:         try:",
          "336:             import brotli  # noqa: F401",
          "337:         except ImportError:",
          "338:             raise SkipTest(\"no brotli\")",
          "339:         self._test_compression_bomb_spider_attr(\"br\")",
          "341:     def test_compression_bomb_spider_attr_deflate(self):",
          "342:         self._test_compression_bomb_spider_attr(\"deflate\")",
          "344:     def test_compression_bomb_spider_attr_gzip(self):",
          "345:         self._test_compression_bomb_spider_attr(\"gzip\")",
          "347:     def test_compression_bomb_spider_attr_zstd(self):",
          "348:         try:",
          "349:             import zstandard",
          "350:         except ImportError:",
          "351:             raise SkipTest(\"no zstandard\")",
          "352:         self._test_compression_bomb_spider_attr(\"zstd\")",
          "354:     def _test_compression_bomb_request_meta(self, compression_id):",
          "355:         crawler = get_crawler(Spider)",
          "356:         spider = crawler._create_spider(\"scrapytest.org\")",
          "357:         mw = HttpCompressionMiddleware.from_crawler(crawler)",
          "358:         mw.open_spider(spider)",
          "360:         response = self._getresponse(\"bomb-{compression_id}\".format(compression_id=compression_id))",
          "361:         response.meta[\"download_maxsize\"] = 10000000",
          "362:         self.assertRaises(",
          "363:             IgnoreRequest,",
          "364:             mw.process_response,",
          "365:             response.request,",
          "366:             response,",
          "367:             spider,",
          "368:         )",
          "370:     def test_compression_bomb_request_meta_br(self):",
          "371:         try:",
          "372:             import brotli  # noqa: F401",
          "373:         except ImportError:",
          "374:             raise SkipTest(\"no brotli\")",
          "375:         self._test_compression_bomb_request_meta(\"br\")",
          "377:     def test_compression_bomb_request_meta_deflate(self):",
          "378:         self._test_compression_bomb_request_meta(\"deflate\")",
          "380:     def test_compression_bomb_request_meta_gzip(self):",
          "381:         self._test_compression_bomb_request_meta(\"gzip\")",
          "383:     def test_compression_bomb_request_meta_zstd(self):",
          "384:         try:",
          "385:             import zstandard",
          "386:         except ImportError:",
          "387:             raise SkipTest(\"no zstandard\")",
          "388:         self._test_compression_bomb_request_meta(\"zstd\")",
          "390:     def _test_download_warnsize_setting(self, compression_id):",
          "391:         settings = {\"DOWNLOAD_WARNSIZE\": 10000000}",
          "392:         crawler = get_crawler(Spider, settings_dict=settings)",
          "393:         spider = crawler._create_spider(\"scrapytest.org\")",
          "394:         mw = HttpCompressionMiddleware.from_crawler(crawler)",
          "395:         mw.open_spider(spider)",
          "396:         response = self._getresponse(\"bomb-{compression_id}\".format(compression_id=compression_id))",
          "398:         with LogCapture(",
          "399:             \"scrapy.downloadermiddlewares.httpcompression\",",
          "400:             propagate=False,",
          "401:             level=WARNING,",
          "402:         ) as log:",
          "403:             mw.process_response(response.request, response, spider)",
          "404:         log.check(",
          "405:             (",
          "406:                 \"scrapy.downloadermiddlewares.httpcompression\",",
          "407:                 \"WARNING\",",
          "408:                 (",
          "409:                     \"<200 http://scrapytest.org/> body size after \"",
          "410:                     \"decompression (11511612 B) is larger than the download \"",
          "411:                     \"warning size (10000000 B).\"",
          "412:                 ),",
          "413:             ),",
          "414:         )",
          "416:     def test_download_warnsize_setting_br(self):",
          "417:         try:",
          "418:             import brotli  # noqa: F401",
          "419:         except ImportError:",
          "420:             raise SkipTest(\"no brotli\")",
          "421:         self._test_download_warnsize_setting(\"br\")",
          "423:     def test_download_warnsize_setting_deflate(self):",
          "424:         self._test_download_warnsize_setting(\"deflate\")",
          "426:     def test_download_warnsize_setting_gzip(self):",
          "427:         self._test_download_warnsize_setting(\"gzip\")",
          "429:     def test_download_warnsize_setting_zstd(self):",
          "430:         try:",
          "431:             import zstandard",
          "432:         except ImportError:",
          "433:             raise SkipTest(\"no zstandard\")",
          "434:         self._test_download_warnsize_setting(\"zstd\")",
          "436:     def _test_download_warnsize_spider_attr(self, compression_id):",
          "437:         class DownloadWarnSizeSpider(Spider):",
          "438:             download_warnsize = 10000000",
          "440:         crawler = get_crawler(DownloadWarnSizeSpider)",
          "441:         spider = crawler._create_spider(\"scrapytest.org\")",
          "442:         mw = HttpCompressionMiddleware.from_crawler(crawler)",
          "443:         mw.open_spider(spider)",
          "444:         response = self._getresponse(\"bomb-{compression_id}\".format(compression_id=compression_id))",
          "446:         with LogCapture(",
          "447:             \"scrapy.downloadermiddlewares.httpcompression\",",
          "448:             propagate=False,",
          "449:             level=WARNING,",
          "450:         ) as log:",
          "451:             mw.process_response(response.request, response, spider)",
          "452:         log.check(",
          "453:             (",
          "454:                 \"scrapy.downloadermiddlewares.httpcompression\",",
          "455:                 \"WARNING\",",
          "456:                 (",
          "457:                     \"<200 http://scrapytest.org/> body size after \"",
          "458:                     \"decompression (11511612 B) is larger than the download \"",
          "459:                     \"warning size (10000000 B).\"",
          "460:                 ),",
          "461:             ),",
          "462:         )",
          "464:     def test_download_warnsize_spider_attr_br(self):",
          "465:         try:",
          "466:             import brotli  # noqa: F401",
          "467:         except ImportError:",
          "468:             raise SkipTest(\"no brotli\")",
          "469:         self._test_download_warnsize_spider_attr(\"br\")",
          "471:     def test_download_warnsize_spider_attr_deflate(self):",
          "472:         self._test_download_warnsize_spider_attr(\"deflate\")",
          "474:     def test_download_warnsize_spider_attr_gzip(self):",
          "475:         self._test_download_warnsize_spider_attr(\"gzip\")",
          "477:     def test_download_warnsize_spider_attr_zstd(self):",
          "478:         try:",
          "479:             import zstandard",
          "480:         except ImportError:",
          "481:             raise SkipTest(\"no zstandard\")",
          "482:         self._test_download_warnsize_spider_attr(\"zstd\")",
          "484:     def _test_download_warnsize_request_meta(self, compression_id):",
          "485:         crawler = get_crawler(Spider)",
          "486:         spider = crawler._create_spider(\"scrapytest.org\")",
          "487:         mw = HttpCompressionMiddleware.from_crawler(crawler)",
          "488:         mw.open_spider(spider)",
          "489:         response = self._getresponse(\"bomb-{compression_id}\".format(compression_id=compression_id))",
          "490:         response.meta[\"download_warnsize\"] = 10000000",
          "492:         with LogCapture(",
          "493:             \"scrapy.downloadermiddlewares.httpcompression\",",
          "494:             propagate=False,",
          "495:             level=WARNING,",
          "496:         ) as log:",
          "497:             mw.process_response(response.request, response, spider)",
          "498:         log.check(",
          "499:             (",
          "500:                 \"scrapy.downloadermiddlewares.httpcompression\",",
          "501:                 \"WARNING\",",
          "502:                 (",
          "503:                     \"<200 http://scrapytest.org/> body size after \"",
          "504:                     \"decompression (11511612 B) is larger than the download \"",
          "505:                     \"warning size (10000000 B).\"",
          "506:                 ),",
          "507:             ),",
          "508:         )",
          "510:     def test_download_warnsize_request_meta_br(self):",
          "511:         try:",
          "512:             import brotli  # noqa: F401",
          "513:         except ImportError:",
          "514:             raise SkipTest(\"no brotli\")",
          "515:         self._test_download_warnsize_request_meta(\"br\")",
          "517:     def test_download_warnsize_request_meta_deflate(self):",
          "518:         self._test_download_warnsize_request_meta(\"deflate\")",
          "520:     def test_download_warnsize_request_meta_gzip(self):",
          "521:         self._test_download_warnsize_request_meta(\"gzip\")",
          "523:     def test_download_warnsize_request_meta_zstd(self):",
          "524:         try:",
          "525:             import zstandard",
          "526:         except ImportError:",
          "527:             raise SkipTest(\"no zstandard\")",
          "528:         self._test_download_warnsize_request_meta(\"zstd\")",
          "",
          "---------------"
        ],
        "tests/test_spider.py||tests/test_spider.py": [
          "File: tests/test_spider.py -> tests/test_spider.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: import gzip",
          "2: import inspect",
          "3: import warnings",
          "4: from io import BytesIO",
          "6: from testfixtures import LogCapture",
          "7: from twisted.trial import unittest",
          "9: from scrapy import signals",
          "10: from scrapy.settings import Settings",
          "12: from scrapy.spiders.init import InitSpider",
          "18: from scrapy.utils.test import get_crawler",
          "23: class SpiderTest(unittest.TestCase):",
          "",
          "[Removed Lines]",
          "11: from scrapy.http import Request, Response, TextResponse, XmlResponse, HtmlResponse",
          "13: from scrapy.spiders import Spider, CrawlSpider, Rule, XMLFeedSpider, \\",
          "14:     CSVFeedSpider, SitemapSpider",
          "15: from scrapy.linkextractors import LinkExtractor",
          "16: from scrapy.exceptions import ScrapyDeprecationWarning",
          "17: from scrapy.utils.trackref import object_ref",
          "20: from tests import mock",
          "",
          "[Added Lines]",
          "3: import os",
          "6: from logging import WARNING",
          "12: from scrapy.exceptions import ScrapyDeprecationWarning",
          "13: from scrapy.http import HtmlResponse, Request, Response, TextResponse, XmlResponse",
          "14: from scrapy.linkextractors import LinkExtractor",
          "16: from scrapy.spiders import (",
          "17:     CrawlSpider,",
          "18:     CSVFeedSpider,",
          "19:     Rule,",
          "20:     SitemapSpider,",
          "21:     Spider,",
          "22:     XMLFeedSpider,",
          "23: )",
          "26: from tests import mock, tests_datadir",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "399:     GZBODY = f.getvalue()",
          "401:     def assertSitemapBody(self, response, body):",
          "403:         self.assertEqual(spider._get_sitemap_body(response), body)",
          "405:     def test_get_sitemap_body(self):",
          "",
          "[Removed Lines]",
          "402:         spider = self.spider_class(\"example.com\")",
          "",
          "[Added Lines]",
          "408:         crawler = get_crawler()",
          "409:         spider = self.spider_class.from_crawler(crawler, \"example.com\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "413:         self.assertSitemapBody(r, None)",
          "415:     def test_get_sitemap_body_gzip_headers(self):",
          "418:         self.assertSitemapBody(r, self.BODY)",
          "420:     def test_get_sitemap_body_xml_url(self):",
          "",
          "[Removed Lines]",
          "416:         r = Response(url=\"http://www.example.com/sitemap\", body=self.GZBODY,",
          "417:                      headers={\"content-type\": \"application/gzip\"})",
          "",
          "[Added Lines]",
          "423:         r = Response(",
          "424:             url=\"http://www.example.com/sitemap\",",
          "425:             body=self.GZBODY,",
          "426:             headers={\"content-type\": \"application/gzip\"},",
          "427:             request=Request(\"http://www.example.com/sitemap\"),",
          "428:         )",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "422:         self.assertSitemapBody(r, self.BODY)",
          "424:     def test_get_sitemap_body_xml_url_compressed(self):",
          "426:         self.assertSitemapBody(r, self.BODY)",
          "428:         # .xml.gz but body decoded by HttpCompression middleware already",
          "",
          "[Removed Lines]",
          "425:         r = Response(url=\"http://www.example.com/sitemap.xml.gz\", body=self.GZBODY)",
          "",
          "[Added Lines]",
          "436:         r = Response(",
          "437:             url=\"http://www.example.com/sitemap.xml.gz\",",
          "438:             body=self.GZBODY,",
          "439:             request=Request(\"http://www.example.com/sitemap\"),",
          "440:         )",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "570:         self.assertEqual([req.url for req in spider._parse_sitemap(r)],",
          "571:                          ['http://www.example.com/sitemap2.xml'])",
          "574: class DeprecationTest(unittest.TestCase):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "588:     def test_compression_bomb_setting(self):",
          "589:         settings = {\"DOWNLOAD_MAXSIZE\": 10000000}",
          "590:         crawler = get_crawler(settings_dict=settings)",
          "591:         spider = self.spider_class.from_crawler(crawler, \"example.com\")",
          "592:         body_path = os.path.join(tests_datadir, \"compressed\", \"bomb-gzip.bin\")",
          "593:         body = open(body_path, \"rb\").read()",
          "594:         request = Request(url=\"https://example.com\")",
          "595:         response = Response(url=\"https://example.com\", body=body, request=request)",
          "596:         self.assertIsNone(spider._get_sitemap_body(response))",
          "598:     def test_compression_bomb_spider_attr(self):",
          "599:         class DownloadMaxSizeSpider(self.spider_class):",
          "600:             download_maxsize = 10000000",
          "602:         crawler = get_crawler()",
          "603:         spider = DownloadMaxSizeSpider.from_crawler(crawler, \"example.com\")",
          "604:         body_path = os.path.join(tests_datadir, \"compressed\", \"bomb-gzip.bin\")",
          "605:         body = open(body_path, \"rb\").read()",
          "606:         request = Request(url=\"https://example.com\")",
          "607:         response = Response(url=\"https://example.com\", body=body, request=request)",
          "608:         self.assertIsNone(spider._get_sitemap_body(response))",
          "610:     def test_compression_bomb_request_meta(self):",
          "611:         crawler = get_crawler()",
          "612:         spider = self.spider_class.from_crawler(crawler, \"example.com\")",
          "613:         body_path = os.path.join(tests_datadir, \"compressed\", \"bomb-gzip.bin\")",
          "614:         body = open(body_path, \"rb\").read()",
          "615:         request = Request(",
          "616:             url=\"https://example.com\", meta={\"download_maxsize\": 10000000}",
          "617:         )",
          "618:         response = Response(url=\"https://example.com\", body=body, request=request)",
          "619:         self.assertIsNone(spider._get_sitemap_body(response))",
          "621:     def test_download_warnsize_setting(self):",
          "622:         settings = {\"DOWNLOAD_WARNSIZE\": 10000000}",
          "623:         crawler = get_crawler(settings_dict=settings)",
          "624:         spider = self.spider_class.from_crawler(crawler, \"example.com\")",
          "625:         body_path = os.path.join(tests_datadir, \"compressed\", \"bomb-gzip.bin\")",
          "626:         body = open(body_path, \"rb\").read()",
          "627:         request = Request(url=\"https://example.com\")",
          "628:         response = Response(url=\"https://example.com\", body=body, request=request)",
          "629:         with LogCapture(",
          "630:             \"scrapy.spiders.sitemap\", propagate=False, level=WARNING",
          "631:         ) as log:",
          "632:             spider._get_sitemap_body(response)",
          "633:         log.check(",
          "634:             (",
          "635:                 \"scrapy.spiders.sitemap\",",
          "636:                 \"WARNING\",",
          "637:                 (",
          "638:                     \"<200 https://example.com> body size after decompression \"",
          "639:                     \"(11511612 B) is larger than the download warning size \"",
          "640:                     \"(10000000 B).\"",
          "641:                 ),",
          "642:             ),",
          "643:         )",
          "645:     def test_download_warnsize_spider_attr(self):",
          "646:         class DownloadWarnSizeSpider(self.spider_class):",
          "647:             download_warnsize = 10000000",
          "649:         crawler = get_crawler()",
          "650:         spider = DownloadWarnSizeSpider.from_crawler(crawler, \"example.com\")",
          "651:         body_path = os.path.join(tests_datadir, \"compressed\", \"bomb-gzip.bin\")",
          "652:         body = open(body_path, \"rb\").read()",
          "653:         request = Request(",
          "654:             url=\"https://example.com\", meta={\"download_warnsize\": 10000000}",
          "655:         )",
          "656:         response = Response(url=\"https://example.com\", body=body, request=request)",
          "657:         with LogCapture(",
          "658:             \"scrapy.spiders.sitemap\", propagate=False, level=WARNING",
          "659:         ) as log:",
          "660:             spider._get_sitemap_body(response)",
          "661:         log.check(",
          "662:             (",
          "663:                 \"scrapy.spiders.sitemap\",",
          "664:                 \"WARNING\",",
          "665:                 (",
          "666:                     \"<200 https://example.com> body size after decompression \"",
          "667:                     \"(11511612 B) is larger than the download warning size \"",
          "668:                     \"(10000000 B).\"",
          "669:                 ),",
          "670:             ),",
          "671:         )",
          "673:     def test_download_warnsize_request_meta(self):",
          "674:         crawler = get_crawler()",
          "675:         spider = self.spider_class.from_crawler(crawler, \"example.com\")",
          "676:         body_path = os.path.join(tests_datadir, \"compressed\", \"bomb-gzip.bin\")",
          "677:         body = open(body_path, \"rb\").read()",
          "678:         request = Request(",
          "679:             url=\"https://example.com\", meta={\"download_warnsize\": 10000000}",
          "680:         )",
          "681:         response = Response(url=\"https://example.com\", body=body, request=request)",
          "682:         with LogCapture(",
          "683:             \"scrapy.spiders.sitemap\", propagate=False, level=WARNING",
          "684:         ) as log:",
          "685:             spider._get_sitemap_body(response)",
          "686:         log.check(",
          "687:             (",
          "688:                 \"scrapy.spiders.sitemap\",",
          "689:                 \"WARNING\",",
          "690:                 (",
          "691:                     \"<200 https://example.com> body size after decompression \"",
          "692:                     \"(11511612 B) is larger than the download warning size \"",
          "693:                     \"(10000000 B).\"",
          "694:                 ),",
          "695:             ),",
          "696:         )",
          "",
          "---------------"
        ]
      }
    }
  ]
}