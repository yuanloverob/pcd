{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "bb0cce990c214d4ca9cf3828940a2ca5350acf79",
      "candidate_info": {
        "commit_hash": "bb0cce990c214d4ca9cf3828940a2ca5350acf79",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/bb0cce990c214d4ca9cf3828940a2ca5350acf79",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala",
          "sql/core/src/test/resources/sql-tests/inputs/non-excludable-rule.sql",
          "sql/core/src/test/resources/sql-tests/results/non-excludable-rule.sql.out"
        ],
        "message": "[SPARK-39448][SQL] Add `ReplaceCTERefWithRepartition` into `nonExcludableRules` list\n\n### What changes were proposed in this pull request?\n\nThis PR adds `ReplaceCTERefWithRepartition` into nonExcludableRules list.\n\n### Why are the changes needed?\n\nIt will throw exception if user `set spark.sql.optimizer.excludedRules=org.apache.spark.sql.catalyst.optimizer.ReplaceCTERefWithRepartition` before running this query:\n```sql\nSELECT\n  (SELECT avg(id) FROM range(10)),\n  (SELECT sum(id) FROM range(10)),\n  (SELECT count(distinct id) FROM range(10))\n```\nException:\n```\nCaused by: java.lang.AssertionError: assertion failed: No plan for WithCTE\n:- CTERelationDef 0, true\n:  +- Project [named_struct(min(id), min(id)#223L, sum(id), sum(id)#226L, count(DISTINCT id), count(DISTINCT id)#229L) AS mergedValue#240]\n:     +- Aggregate [min(id#221L) AS min(id)#223L, sum(id#221L) AS sum(id)#226L, count(distinct id#221L) AS count(DISTINCT id)#229L]\n:        +- Range (0, 10, step=1, splits=None)\n+- Project [scalar-subquery#218 [].min(id) AS scalarsubquery()#230L, scalar-subquery#219 [].sum(id) AS scalarsubquery()#231L, scalar-subquery#220 [].count(DISTINCT id) AS scalarsubquery()#232L]\n   :  :- CTERelationRef 0, true, [mergedValue#240]\n   :  :- CTERelationRef 0, true, [mergedValue#240]\n   :  +- CTERelationRef 0, true, [mergedValue#240]\n   +- OneRowRelation\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nUnit test.\n\nCloses #36847 from wangyum/SPARK-39448.\n\nAuthored-by: Yuming Wang <yumwang@ebay.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 0b785b3c77374fa7736f01bb55e87c796985ae14)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala",
          "sql/core/src/test/resources/sql-tests/inputs/non-excludable-rule.sql||sql/core/src/test/resources/sql-tests/inputs/non-excludable-rule.sql"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "88:     GroupBasedRowLevelOperationScanPlanning.ruleName :+",
          "89:     V2ScanRelationPushDown.ruleName :+",
          "90:     V2ScanPartitioning.ruleName :+",
          "",
          "[Removed Lines]",
          "91:     V2Writes.ruleName",
          "",
          "[Added Lines]",
          "91:     V2Writes.ruleName :+",
          "92:     ReplaceCTERefWithRepartition.ruleName",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/non-excludable-rule.sql||sql/core/src/test/resources/sql-tests/inputs/non-excludable-rule.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/non-excludable-rule.sql -> sql/core/src/test/resources/sql-tests/inputs/non-excludable-rule.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: -- SPARK-39448",
          "2: SET spark.sql.optimizer.excludedRules=org.apache.spark.sql.catalyst.optimizer.ReplaceCTERefWithRepartition;",
          "3: SELECT",
          "4:   (SELECT min(id) FROM range(10)),",
          "5:   (SELECT sum(id) FROM range(10)),",
          "6:   (SELECT count(distinct id) FROM range(10));",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a52a245d11c20b0360d463c973388f3ee05768ac",
      "candidate_info": {
        "commit_hash": "a52a245d11c20b0360d463c973388f3ee05768ac",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/a52a245d11c20b0360d463c973388f3ee05768ac",
        "files": [
          "sql/hive/src/test/scala/org/apache/spark/sql/hive/StatisticsSuite.scala"
        ],
        "message": "[SPARK-38786][SQL][TEST] Bug in StatisticsSuite 'change stats after add/drop partition command'\n\n### What changes were proposed in this pull request?\nhttps://github.com/apache/spark/blob/cbffc12f90e45d33e651e38cf886d7ab4bcf96da/sql/hive/src/test/scala/org/apache/spark/sql/hive/StatisticsSuite.scala#L979\nIt should be `partDir2` instead of `partDir1`. Looks like it is a copy paste bug.\n\n### Why are the changes needed?\nDue to this test bug, the drop command was dropping a wrong (`partDir1`) underlying file in the test.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nAdded extra underlying file location check.\n\nCloses #36075 from kazuyukitanimura/SPARK-38786.\n\nAuthored-by: Kazuyuki Tanimura <ktanimura@apple.com>\nSigned-off-by: Chao Sun <sunchao@apple.com>\n(cherry picked from commit a6b04f007c07fe00637aa8be33a56f247a494110)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/hive/src/test/scala/org/apache/spark/sql/hive/StatisticsSuite.scala||sql/hive/src/test/scala/org/apache/spark/sql/hive/StatisticsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/hive/src/test/scala/org/apache/spark/sql/hive/StatisticsSuite.scala||sql/hive/src/test/scala/org/apache/spark/sql/hive/StatisticsSuite.scala": [
          "File: sql/hive/src/test/scala/org/apache/spark/sql/hive/StatisticsSuite.scala -> sql/hive/src/test/scala/org/apache/spark/sql/hive/StatisticsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "976:               s\"\"\"",
          "977:                  |ALTER TABLE $table ADD",
          "978:                  |PARTITION (ds='2008-04-09', hr='11') LOCATION '${partDir1.toURI.toString}'",
          "980:             \"\"\".stripMargin)",
          "981:             if (autoUpdate) {",
          "982:               val fetched2 = checkTableStats(table, hasSizeInBytes = true, expectedRowCounts = None)",
          "",
          "[Removed Lines]",
          "979:                  |PARTITION (ds='2008-04-09', hr='12') LOCATION '${partDir1.toURI.toString}'",
          "",
          "[Added Lines]",
          "979:                  |PARTITION (ds='2008-04-09', hr='12') LOCATION '${partDir2.toURI.toString}'",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "999:             sql(s\"ALTER TABLE $table DROP PARTITION (ds='2008-04-08'), PARTITION (hr='12')\")",
          "1000:             assert(spark.sessionState.catalog.listPartitions(TableIdentifier(table))",
          "1001:               .map(_.spec).toSet == Set(Map(\"ds\" -> \"2008-04-09\", \"hr\" -> \"11\")))",
          "1003:             if (autoUpdate) {",
          "1004:               val fetched4 = checkTableStats(table, hasSizeInBytes = true, expectedRowCounts = None)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1002:             assert(partDir1.exists())",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c44020b961ffe44e30ee617af6ffb84effbd28fe",
      "candidate_info": {
        "commit_hash": "c44020b961ffe44e30ee617af6ffb84effbd28fe",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/c44020b961ffe44e30ee617af6ffb84effbd28fe",
        "files": [
          "python/pyspark/sql/tests/test_pandas_cogrouped_map.py",
          "python/pyspark/sql/tests/test_pandas_grouped_map.py",
          "python/pyspark/sql/tests/test_pandas_map.py",
          "python/pyspark/worker.py"
        ],
        "message": "[SPARK-38833][PYTHON][SQL] Allow applyInPandas to return empty DataFrame without columns\n\n### What changes were proposed in this pull request?\nMethods `wrap_cogrouped_map_pandas_udf` and `wrap_grouped_map_pandas_udf` in `python/pyspark/worker.py` do not need to reject `pd.DataFrame`s with no columns return by udf when that DataFrame is empty (zero rows). This allows to return empty DataFrames without the need to define columns. The DataFrame is empty after all!\n\n**The proposed behaviour is consistent with the current behaviour of `DataFrame.mapInPandas`.**\n\n### Why are the changes needed?\nReturning an empty DataFrame from the lambda given to `applyInPandas` should be as easy as this:\n\n```python\nreturn pd.DataFrame([])\n```\n\nHowever, PySpark requires that empty DataFrame to have the right _number_ of columns. This seems redundant as the schema is already defined in the `applyInPandas` call. Returning a non-empty DataFrame does not require defining columns.\n\nBehaviour of `applyInPandas` should be consistent with `mapInPandas`.\n\nHere is an example to reproduce:\n```python\nimport pandas as pd\n\nfrom pyspark.sql.functions import pandas_udf, ceil\n\ndf = spark.createDataFrame(\n    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n    (\"id\", \"v\"))\n\ndef mean_func(key, pdf):\n    if key == (1,):\n        return pd.DataFrame([])\n    else:\n        return pd.DataFrame([key + (pdf.v.mean(),)])\n\ndf.groupby(\"id\").applyInPandas(mean_func, schema=\"id long, v double\").show()\n```\n\n### Does this PR introduce _any_ user-facing change?\nIt changes the behaviour of the following calls to allow returning empty `pd.DataFrame` without defining columns. The PySpark DataFrame returned by `applyInPandas` is unchanged:\n\n- `df.groupby(\u2026).applyInPandas(\u2026)`\n- `df.cogroup(\u2026).applyInPandas(\u2026)`\n\n### How was this patch tested?\nTests are added that test `applyInPandas` and `mapInPandas` when returning\n\n- empty DataFrame with no columns\n- empty DataFrame with the wrong number of columns\n- non-empty DataFrame with wrong number of columns\n- something other than `pd.DataFrame`\n\nNOTE: It is not an error for `mapInPandas` to return DataFrames with more columns than specified in the `mapInPandas` schema.\n\nCloses #36120 from EnricoMi/branch-empty-pd-dataframes.\n\nAuthored-by: Enrico Minack <github@enrico.minack.dev>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 556c74578eb2379fc6e0ec8d147674d0b10e5a2c)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/sql/tests/test_pandas_cogrouped_map.py||python/pyspark/sql/tests/test_pandas_cogrouped_map.py",
          "python/pyspark/sql/tests/test_pandas_grouped_map.py||python/pyspark/sql/tests/test_pandas_grouped_map.py",
          "python/pyspark/sql/tests/test_pandas_map.py||python/pyspark/sql/tests/test_pandas_map.py",
          "python/pyspark/worker.py||python/pyspark/worker.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/sql/tests/test_pandas_cogrouped_map.py||python/pyspark/sql/tests/test_pandas_cogrouped_map.py": [
          "File: python/pyspark/sql/tests/test_pandas_cogrouped_map.py -> python/pyspark/sql/tests/test_pandas_cogrouped_map.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: from pyspark.sql.functions import array, explode, col, lit, udf, pandas_udf",
          "22: from pyspark.sql.types import DoubleType, StructType, StructField, Row",
          "23: from pyspark.testing.sqlutils import (",
          "24:     ReusedSQLTestCase,",
          "25:     have_pandas,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "23: from pyspark.sql.utils import PythonException",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "125:         assert_frame_equal(expected, result)",
          "127:     def test_mixed_scalar_udfs_followed_by_cogrouby_apply(self):",
          "128:         df = self.spark.range(0, 10).toDF(\"v1\")",
          "129:         df = df.withColumn(\"v2\", udf(lambda x: x + 1, \"int\")(df[\"v1\"])).withColumn(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "128:     def test_apply_in_pandas_not_returning_pandas_dataframe(self):",
          "129:         left = self.data1",
          "130:         right = self.data2",
          "132:         def merge_pandas(lft, rgt):",
          "133:             return lft.size + rgt.size",
          "135:         with QuietTest(self.sc):",
          "136:             with self.assertRaisesRegex(",
          "137:                 PythonException,",
          "138:                 \"Return type of the user-defined function should be pandas.DataFrame, \"",
          "139:                 \"but is <class 'numpy.int64'>\",",
          "140:             ):",
          "141:                 (",
          "142:                     left.groupby(\"id\")",
          "143:                     .cogroup(right.groupby(\"id\"))",
          "144:                     .applyInPandas(merge_pandas, \"id long, k int, v int, v2 int\")",
          "145:                     .collect()",
          "146:                 )",
          "148:     def test_apply_in_pandas_returning_wrong_number_of_columns(self):",
          "149:         left = self.data1",
          "150:         right = self.data2",
          "152:         def merge_pandas(lft, rgt):",
          "153:             if 0 in lft[\"id\"] and lft[\"id\"][0] % 2 == 0:",
          "154:                 lft[\"add\"] = 0",
          "155:             if 0 in rgt[\"id\"] and rgt[\"id\"][0] % 3 == 0:",
          "156:                 rgt[\"more\"] = 1",
          "157:             return pd.merge(lft, rgt, on=[\"id\", \"k\"])",
          "159:         with QuietTest(self.sc):",
          "160:             with self.assertRaisesRegex(",
          "161:                 PythonException,",
          "162:                 \"Number of columns of the returned pandas.DataFrame \"",
          "163:                 \"doesn't match specified schema. Expected: 4 Actual: 6\",",
          "164:             ):",
          "165:                 (",
          "166:                     # merge_pandas returns two columns for even keys while we set schema to four",
          "167:                     left.groupby(\"id\")",
          "168:                     .cogroup(right.groupby(\"id\"))",
          "169:                     .applyInPandas(merge_pandas, \"id long, k int, v int, v2 int\")",
          "170:                     .collect()",
          "171:                 )",
          "173:     def test_apply_in_pandas_returning_empty_dataframe(self):",
          "174:         left = self.data1",
          "175:         right = self.data2",
          "177:         def merge_pandas(lft, rgt):",
          "178:             if 0 in lft[\"id\"] and lft[\"id\"][0] % 2 == 0:",
          "179:                 return pd.DataFrame([])",
          "180:             if 0 in rgt[\"id\"] and rgt[\"id\"][0] % 3 == 0:",
          "181:                 return pd.DataFrame([])",
          "182:             return pd.merge(lft, rgt, on=[\"id\", \"k\"])",
          "184:         result = (",
          "185:             left.groupby(\"id\")",
          "186:             .cogroup(right.groupby(\"id\"))",
          "187:             .applyInPandas(merge_pandas, \"id long, k int, v int, v2 int\")",
          "188:             .sort([\"id\", \"k\"])",
          "189:             .toPandas()",
          "190:         )",
          "192:         left = left.toPandas()",
          "193:         right = right.toPandas()",
          "195:         expected = pd.merge(",
          "196:             left[left[\"id\"] % 2 != 0], right[right[\"id\"] % 3 != 0], on=[\"id\", \"k\"]",
          "197:         ).sort_values(by=[\"id\", \"k\"])",
          "199:         assert_frame_equal(expected, result)",
          "201:     def test_apply_in_pandas_returning_empty_dataframe_and_wrong_number_of_columns(self):",
          "202:         left = self.data1",
          "203:         right = self.data2",
          "205:         def merge_pandas(lft, rgt):",
          "206:             if 0 in lft[\"id\"] and lft[\"id\"][0] % 2 == 0:",
          "207:                 return pd.DataFrame([], columns=[\"id\", \"k\"])",
          "208:             return pd.merge(lft, rgt, on=[\"id\", \"k\"])",
          "210:         with QuietTest(self.sc):",
          "211:             with self.assertRaisesRegex(",
          "212:                 PythonException,",
          "213:                 \"Number of columns of the returned pandas.DataFrame doesn't \"",
          "214:                 \"match specified schema. Expected: 4 Actual: 2\",",
          "215:             ):",
          "216:                 (",
          "217:                     # merge_pandas returns two columns for even keys while we set schema to four",
          "218:                     left.groupby(\"id\")",
          "219:                     .cogroup(right.groupby(\"id\"))",
          "220:                     .applyInPandas(merge_pandas, \"id long, k int, v int, v2 int\")",
          "221:                     .collect()",
          "222:                 )",
          "",
          "---------------"
        ],
        "python/pyspark/sql/tests/test_pandas_grouped_map.py||python/pyspark/sql/tests/test_pandas_grouped_map.py": [
          "File: python/pyspark/sql/tests/test_pandas_grouped_map.py -> python/pyspark/sql/tests/test_pandas_grouped_map.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "51:     NullType,",
          "52:     TimestampType,",
          "53: )",
          "54: from pyspark.testing.sqlutils import (",
          "55:     ReusedSQLTestCase,",
          "56:     have_pandas,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "54: from pyspark.sql.utils import PythonException",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "268:         expected = expected.assign(norm=expected.norm.astype(\"float64\"))",
          "269:         assert_frame_equal(expected, result)",
          "271:     def test_datatype_string(self):",
          "272:         df = self.data",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "272:     def test_apply_in_pandas_not_returning_pandas_dataframe(self):",
          "273:         df = self.data",
          "275:         def stats(key, _):",
          "276:             return key",
          "278:         with QuietTest(self.sc):",
          "279:             with self.assertRaisesRegex(",
          "280:                 PythonException,",
          "281:                 \"Return type of the user-defined function should be pandas.DataFrame, \"",
          "282:                 \"but is <class 'tuple'>\",",
          "283:             ):",
          "284:                 df.groupby(\"id\").applyInPandas(stats, schema=\"id integer, m double\").collect()",
          "286:     def test_apply_in_pandas_returning_wrong_number_of_columns(self):",
          "287:         df = self.data",
          "289:         def stats(key, pdf):",
          "290:             v = pdf.v",
          "291:             # returning three columns",
          "292:             res = pd.DataFrame([key + (v.mean(), v.std())])",
          "293:             return res",
          "295:         with QuietTest(self.sc):",
          "296:             with self.assertRaisesRegex(",
          "297:                 PythonException,",
          "298:                 \"Number of columns of the returned pandas.DataFrame doesn't match \"",
          "299:                 \"specified schema. Expected: 2 Actual: 3\",",
          "300:             ):",
          "301:                 # stats returns three columns while here we set schema with two columns",
          "302:                 df.groupby(\"id\").applyInPandas(stats, schema=\"id integer, m double\").collect()",
          "304:     def test_apply_in_pandas_returning_empty_dataframe(self):",
          "305:         df = self.data",
          "307:         def odd_means(key, pdf):",
          "308:             if key[0] % 2 == 0:",
          "309:                 return pd.DataFrame([])",
          "310:             else:",
          "311:                 return pd.DataFrame([key + (pdf.v.mean(),)])",
          "313:         expected_ids = {row[0] for row in self.data.collect() if row[0] % 2 != 0}",
          "315:         result = (",
          "316:             df.groupby(\"id\")",
          "317:             .applyInPandas(odd_means, schema=\"id integer, m double\")",
          "318:             .sort(\"id\", \"m\")",
          "319:             .collect()",
          "320:         )",
          "322:         actual_ids = {row[0] for row in result}",
          "323:         self.assertSetEqual(expected_ids, actual_ids)",
          "325:         self.assertEqual(len(expected_ids), len(result))",
          "326:         for row in result:",
          "327:             self.assertEqual(24.5, row[1])",
          "329:     def test_apply_in_pandas_returning_empty_dataframe_and_wrong_number_of_columns(self):",
          "330:         df = self.data",
          "332:         def odd_means(key, pdf):",
          "333:             if key[0] % 2 == 0:",
          "334:                 return pd.DataFrame([], columns=[\"id\"])",
          "335:             else:",
          "336:                 return pd.DataFrame([key + (pdf.v.mean(),)])",
          "338:         with QuietTest(self.sc):",
          "339:             with self.assertRaisesRegex(",
          "340:                 PythonException,",
          "341:                 \"Number of columns of the returned pandas.DataFrame doesn't match \"",
          "342:                 \"specified schema. Expected: 2 Actual: 1\",",
          "343:             ):",
          "344:                 # stats returns one column for even keys while here we set schema with two columns",
          "345:                 df.groupby(\"id\").applyInPandas(odd_means, schema=\"id integer, m double\").collect()",
          "",
          "---------------"
        ],
        "python/pyspark/sql/tests/test_pandas_map.py||python/pyspark/sql/tests/test_pandas_map.py": [
          "File: python/pyspark/sql/tests/test_pandas_map.py -> python/pyspark/sql/tests/test_pandas_map.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: from typing import cast",
          "24: from pyspark.sql import Row",
          "25: from pyspark.testing.sqlutils import (",
          "26:     ReusedSQLTestCase,",
          "27:     have_pandas,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: from pyspark.sql.functions import lit",
          "26: from pyspark.sql.utils import PythonException",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "29:     pandas_requirement_message,",
          "30:     pyarrow_requirement_message,",
          "31: )",
          "33: if have_pandas:",
          "34:     import pandas as pd",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "34: from pyspark.testing.utils import QuietTest",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "60:         time.tzset()",
          "61:         ReusedSQLTestCase.tearDownClass()",
          "64:         def func(iterator):",
          "65:             for pdf in iterator:",
          "66:                 assert isinstance(pdf, pd.DataFrame)",
          "67:                 assert pdf.columns == [\"id\"]",
          "68:                 yield pdf",
          "71:         actual = df.mapInPandas(func, \"id long\").collect()",
          "72:         expected = df.collect()",
          "73:         self.assertEqual(actual, expected)",
          "",
          "[Removed Lines]",
          "63:     def test_map_partitions_in_pandas(self):",
          "70:         df = self.spark.range(10)",
          "",
          "[Added Lines]",
          "66:     def test_map_in_pandas(self):",
          "73:         df = self.spark.range(10, numPartitions=3)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "95:         actual = df.repartition(1).mapInPandas(func, \"a long\").collect()",
          "96:         self.assertEqual(set((r.a for r in actual)), set(range(100)))",
          "98:     def test_empty_iterator(self):",
          "99:         def empty_iter(_):",
          "100:             return iter([])",
          "106:             return iter([pd.DataFrame({\"a\": []})])",
          "110:     def test_chain_map_partitions_in_pandas(self):",
          "111:         def func(iterator):",
          "",
          "[Removed Lines]",
          "102:         self.assertEqual(self.spark.range(10).mapInPandas(empty_iter, \"a int, b string\").count(), 0)",
          "104:     def test_empty_rows(self):",
          "105:         def empty_rows(_):",
          "108:         self.assertEqual(self.spark.range(10).mapInPandas(empty_rows, \"a int\").count(), 0)",
          "",
          "[Added Lines]",
          "101:     def test_other_than_dataframe(self):",
          "102:         def bad_iter(_):",
          "103:             return iter([1])",
          "105:         with QuietTest(self.sc):",
          "106:             with self.assertRaisesRegex(",
          "107:                 PythonException,",
          "108:                 \"Return type of the user-defined function should be Pandas.DataFrame, \"",
          "109:                 \"but is <class 'int'>\",",
          "110:             ):",
          "111:                 (",
          "112:                     self.spark.range(10, numPartitions=3)",
          "113:                     .mapInPandas(bad_iter, \"a int, b string\")",
          "114:                     .count()",
          "115:                 )",
          "121:         mapped = self.spark.range(10, numPartitions=3).mapInPandas(empty_iter, \"a int, b string\")",
          "122:         self.assertEqual(mapped.count(), 0)",
          "124:     def test_empty_dataframes(self):",
          "125:         def empty_dataframes(_):",
          "128:         mapped = self.spark.range(10, numPartitions=3).mapInPandas(empty_dataframes, \"a int\")",
          "129:         self.assertEqual(mapped.count(), 0)",
          "131:     def test_empty_dataframes_without_columns(self):",
          "132:         def empty_dataframes_wo_columns(iterator):",
          "133:             for pdf in iterator:",
          "134:                 yield pdf",
          "135:             # after yielding all elements of the iterator, also yield one dataframe without columns",
          "136:             yield pd.DataFrame([])",
          "138:         mapped = (",
          "139:             self.spark.range(10, numPartitions=3)",
          "140:             .toDF(\"id\")",
          "141:             .mapInPandas(empty_dataframes_wo_columns, \"id int\")",
          "142:         )",
          "143:         self.assertEqual(mapped.count(), 10)",
          "145:     def test_empty_dataframes_with_less_columns(self):",
          "146:         def empty_dataframes_with_less_columns(iterator):",
          "147:             for pdf in iterator:",
          "148:                 yield pdf",
          "149:             # after yielding all elements of the iterator, also yield a dataframe with less columns",
          "150:             yield pd.DataFrame([(1,)], columns=[\"id\"])",
          "152:         with QuietTest(self.sc):",
          "153:             with self.assertRaisesRegex(",
          "154:                 PythonException,",
          "155:                 \"KeyError: 'value'\",",
          "156:             ):",
          "157:                 (",
          "158:                     self.spark.range(10, numPartitions=3)",
          "159:                     .withColumn(\"value\", lit(0))",
          "160:                     .toDF(\"id\", \"value\")",
          "161:                     .mapInPandas(empty_dataframes_with_less_columns, \"id int, value int\")",
          "162:                     .collect()",
          "163:                 )",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "114:                 assert pdf.columns == [\"id\"]",
          "115:                 yield pdf",
          "118:         actual = df.mapInPandas(func, \"id long\").mapInPandas(func, \"id long\").collect()",
          "119:         expected = df.collect()",
          "120:         self.assertEqual(actual, expected)",
          "122:     def test_self_join(self):",
          "123:         # SPARK-34319: self-join with MapInPandas",
          "125:         df2 = df1.mapInPandas(lambda iter: iter, \"id long\")",
          "126:         actual = df2.join(df2).collect()",
          "127:         expected = df1.join(df1).collect()",
          "",
          "[Removed Lines]",
          "117:         df = self.spark.range(10)",
          "124:         df1 = self.spark.range(10)",
          "",
          "[Added Lines]",
          "172:         df = self.spark.range(10, numPartitions=3)",
          "179:         df1 = self.spark.range(10, numPartitions=3)",
          "",
          "---------------"
        ],
        "python/pyspark/worker.py||python/pyspark/worker.py": [
          "File: python/pyspark/worker.py -> python/pyspark/worker.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "162:                 \"Return type of the user-defined function should be \"",
          "163:                 \"pandas.DataFrame, but is {}\".format(type(result))",
          "164:             )",
          "166:             raise RuntimeError(",
          "167:                 \"Number of columns of the returned pandas.DataFrame \"",
          "168:                 \"doesn't match specified schema. \"",
          "",
          "[Removed Lines]",
          "165:         if not len(result.columns) == len(return_type):",
          "",
          "[Added Lines]",
          "165:         # the number of columns of result have to match the return type",
          "166:         # but it is fine for result to have no columns at all if it is empty",
          "167:         if not (",
          "168:             len(result.columns) == len(return_type) or len(result.columns) == 0 and result.empty",
          "169:         ):",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "188:                 \"Return type of the user-defined function should be \"",
          "189:                 \"pandas.DataFrame, but is {}\".format(type(result))",
          "190:             )",
          "192:             raise RuntimeError(",
          "193:                 \"Number of columns of the returned pandas.DataFrame \"",
          "194:                 \"doesn't match specified schema. \"",
          "",
          "[Removed Lines]",
          "191:         if not len(result.columns) == len(return_type):",
          "",
          "[Added Lines]",
          "195:         # the number of columns of result have to match the return type",
          "196:         # but it is fine for result to have no columns at all if it is empty",
          "197:         if not (",
          "198:             len(result.columns) == len(return_type) or len(result.columns) == 0 and result.empty",
          "199:         ):",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "6572c66d01e3db00858f0b4743670a1243d3c44f",
      "candidate_info": {
        "commit_hash": "6572c66d01e3db00858f0b4743670a1243d3c44f",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/6572c66d01e3db00858f0b4743670a1243d3c44f",
        "files": [
          "mllib/src/test/scala/org/apache/spark/ml/source/image/ImageFileFormatSuite.scala"
        ],
        "message": "[SPARK-40172][ML][TESTS] Temporarily disable flaky test cases in ImageFileFormatSuite\n\n### What changes were proposed in this pull request?\n\n3 test cases in ImageFileFormatSuite become flaky in the GitHub action tests:\n\nhttps://github.com/apache/spark/runs/7941765326?check_suite_focus=true\nhttps://github.com/gengliangwang/spark/runs/7928658069\n\nBefore they are fixed(https://issues.apache.org/jira/browse/SPARK-40171), I suggest disabling them in OSS.\n\n### Why are the changes needed?\n\nDisable flaky tests before they are fixed. The test cases keep failing from time to time, while they always pass on local env.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n### How was this patch tested?\n\nExisting CI\n\nCloses #37605 from gengliangwang/disableFlakyTest.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 50f2f506327b7d51af9fb0ae1316135905d2f87d)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "mllib/src/test/scala/org/apache/spark/ml/source/image/ImageFileFormatSuite.scala||mllib/src/test/scala/org/apache/spark/ml/source/image/ImageFileFormatSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "mllib/src/test/scala/org/apache/spark/ml/source/image/ImageFileFormatSuite.scala||mllib/src/test/scala/org/apache/spark/ml/source/image/ImageFileFormatSuite.scala": [
          "File: mllib/src/test/scala/org/apache/spark/ml/source/image/ImageFileFormatSuite.scala -> mllib/src/test/scala/org/apache/spark/ml/source/image/ImageFileFormatSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "49:     assert(df.schema(\"image\").dataType == columnSchema, \"data do not fit ImageSchema\")",
          "50:   }",
          "53:     val df1 = spark.read.format(\"image\").load(imagePath)",
          "54:     assert(df1.count === 9)",
          "",
          "[Removed Lines]",
          "52:   test(\"image datasource count test\") {",
          "",
          "[Added Lines]",
          "53:   ignore(\"image datasource count test\") {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "87:     assert(result === invalidImageRow(resultOrigin))",
          "88:   }",
          "91:     val result = spark.read.format(\"image\")",
          "92:       .option(\"dropInvalid\", true).load(imagePath)",
          "93:       .select(substring_index(col(\"image.origin\"), \"/\", -1).as(\"origin\"), col(\"cls\"), col(\"date\"))",
          "",
          "[Removed Lines]",
          "90:   test(\"image datasource partition test\") {",
          "",
          "[Added Lines]",
          "92:   ignore(\"image datasource partition test\") {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "105:     ))",
          "106:   }",
          "110:     val images = spark.read.format(\"image\").option(\"dropInvalid\", true)",
          "111:       .load(imagePath + \"/cls=multichannel/\").collect()",
          "",
          "[Removed Lines]",
          "109:   test(\"readImages pixel values test\") {",
          "",
          "[Added Lines]",
          "112:   ignore(\"readImages pixel values test\") {",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f9e3668dbb1cafdac0d7c46fc65035a1f9262af1",
      "candidate_info": {
        "commit_hash": "f9e3668dbb1cafdac0d7c46fc65035a1f9262af1",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/f9e3668dbb1cafdac0d7c46fc65035a1f9262af1",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DescribeNamespaceExec.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v2/DescribeNamespaceSuite.scala"
        ],
        "message": "[SPARK-39656][SQL][3.3] Fix wrong namespace in DescribeNamespaceExec\n\nbackport https://github.com/apache/spark/pull/37049 for branch-3.3\n\n### What changes were proposed in this pull request?\n\nDescribeNamespaceExec change ns.last to ns.quoted\n\n### Why are the changes needed?\n\nDescribeNamespaceExec should show the whole namespace rather than last\n\n### Does this PR introduce _any_ user-facing change?\n\nyes, a small bug fix\n\n### How was this patch tested?\n\nfix test\n\nCloses #37071 from ulysses-you/desc-namespace-3.3.\n\nAuthored-by: ulysses-you <ulyssesyou18@gmail.com>\nSigned-off-by: huaxingao <huaxin_gao@apple.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DescribeNamespaceExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DescribeNamespaceExec.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v2/DescribeNamespaceSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/command/v2/DescribeNamespaceSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DescribeNamespaceExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DescribeNamespaceExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DescribeNamespaceExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DescribeNamespaceExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import org.apache.spark.sql.catalyst.InternalRow",
          "24: import org.apache.spark.sql.catalyst.expressions.Attribute",
          "25: import org.apache.spark.sql.connector.catalog.{CatalogV2Util, SupportsNamespaces}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "37:     val ns = namespace.toArray",
          "38:     val metadata = catalog.loadNamespaceMetadata(ns)",
          "42:     CatalogV2Util.NAMESPACE_RESERVED_PROPERTIES.foreach { p =>",
          "43:       rows ++= Option(metadata.get(p)).map(toCatalystRow(p.capitalize, _))",
          "",
          "[Removed Lines]",
          "40:     rows += toCatalystRow(\"Namespace Name\", ns.last)",
          "",
          "[Added Lines]",
          "41:     rows += toCatalystRow(\"Namespace Name\", ns.quoted)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v2/DescribeNamespaceSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/command/v2/DescribeNamespaceSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/command/v2/DescribeNamespaceSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/command/v2/DescribeNamespaceSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "41:         ))",
          "42:       val description = descriptionDf.collect()",
          "43:       assert(description === Seq(",
          "45:         Row(SupportsNamespaces.PROP_COMMENT.capitalize, \"test namespace\"),",
          "46:         Row(SupportsNamespaces.PROP_LOCATION.capitalize, \"file:/tmp/ns_test\"),",
          "47:         Row(SupportsNamespaces.PROP_OWNER.capitalize, Utils.getCurrentUserName()))",
          "",
          "[Removed Lines]",
          "44:         Row(\"Namespace Name\", \"ns2\"),",
          "",
          "[Added Lines]",
          "44:         Row(\"Namespace Name\", \"ns1.ns2\"),",
          "",
          "---------------"
        ]
      }
    }
  ]
}