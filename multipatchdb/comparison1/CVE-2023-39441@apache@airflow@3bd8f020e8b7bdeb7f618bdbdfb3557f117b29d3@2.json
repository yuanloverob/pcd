{
  "cve_id": "CVE-2023-39441",
  "cve_desc": "Apache Airflow SMTP Provider before 1.3.0, Apache Airflow IMAP Provider before 3.3.0, and\u00a0Apache Airflow before 2.7.0 are affected by the\u00a0Validation of OpenSSL Certificate vulnerability.\n\nThe default SSL context with SSL library did not check a server's X.509\u00a0certificate.\u00a0 Instead, the code accepted any certificate, which could\u00a0result in the disclosure of mail server credentials or mail contents\u00a0when the client connects to an attacker in a MITM position.\n\nUsers are strongly advised to upgrade to Apache Airflow version 2.7.0 or newer, Apache Airflow IMAP Provider version 3.3.0 or newer, and Apache Airflow SMTP Provider version 1.3.0 or newer to mitigate the risk associated with this vulnerability",
  "repo": "apache/airflow",
  "patch_hash": "3bd8f020e8b7bdeb7f618bdbdfb3557f117b29d3",
  "patch_info": {
    "commit_hash": "3bd8f020e8b7bdeb7f618bdbdfb3557f117b29d3",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/3bd8f020e8b7bdeb7f618bdbdfb3557f117b29d3",
    "files": [
      "airflow/config_templates/config.yml",
      "airflow/utils/email.py",
      "newsfragments/33070.significant.rst",
      "tests/utils/test_email.py"
    ],
    "message": "Allows to choose SSL context for SMTP connection (#33070)\n\nThis change add two options to choose from when SSL SMTP connection\nis created:\n\n* default - for balance between compatibility and security\n* none - in case compatibility with existing infrastructure is\n  preferred\n\n(cherry picked from commit 120efc186556b1e9498f90ad436c74e5f4e138e9)",
    "before_after_code_files": [
      "airflow/utils/email.py||airflow/utils/email.py",
      "tests/utils/test_email.py||tests/utils/test_email.py"
    ]
  },
  "patch_diff": {
    "airflow/utils/email.py||airflow/utils/email.py": [
      "File: airflow/utils/email.py -> airflow/utils/email.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "21: import logging",
      "22: import os",
      "23: import smtplib",
      "24: import warnings",
      "25: from email.mime.application import MIMEApplication",
      "26: from email.mime.multipart import MIMEMultipart",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "24: import ssl",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "312:     :param with_ssl: Whether to use SSL encryption for the connection.",
      "313:     :return: An SMTP connection to the specified host and port.",
      "314:     \"\"\"",
      "322: def _get_email_list_from_str(addresses: str) -> list[str]:",
      "",
      "[Removed Lines]",
      "315:     return (",
      "316:         smtplib.SMTP_SSL(host=host, port=port, timeout=timeout)",
      "317:         if with_ssl",
      "318:         else smtplib.SMTP(host=host, port=port, timeout=timeout)",
      "319:     )",
      "",
      "[Added Lines]",
      "316:     if not with_ssl:",
      "317:         return smtplib.SMTP(host=host, port=port, timeout=timeout)",
      "318:     else:",
      "319:         ssl_context_string = conf.get(\"email\", \"SSL_CONTEXT\")",
      "320:         if ssl_context_string == \"default\":",
      "321:             ssl_context = ssl.create_default_context()",
      "322:         elif ssl_context_string == \"none\":",
      "323:             ssl_context = None",
      "324:         else:",
      "325:             raise RuntimeError(",
      "326:                 f\"The email.ssl_context configuration variable must \"",
      "327:                 f\"be set to 'default' or 'none' and is '{ssl_context_string}.\"",
      "328:             )",
      "329:         return smtplib.SMTP_SSL(host=host, port=port, timeout=timeout, context=ssl_context)",
      "",
      "---------------"
    ],
    "tests/utils/test_email.py||tests/utils/test_email.py": [
      "File: tests/utils/test_email.py -> tests/utils/test_email.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "241:     @mock.patch(\"smtplib.SMTP_SSL\")",
      "242:     @mock.patch(\"smtplib.SMTP\")",
      "244:         mock_smtp_ssl.return_value = mock.Mock()",
      "245:         with conf_vars({(\"smtp\", \"smtp_ssl\"): \"True\"}):",
      "246:             email.send_mime_email(\"from\", \"to\", MIMEMultipart(), dryrun=False)",
      "247:         assert not mock_smtp.called",
      "248:         mock_smtp_ssl.assert_called_once_with(",
      "249:             host=conf.get(\"smtp\", \"SMTP_HOST\"),",
      "250:             port=conf.getint(\"smtp\", \"SMTP_PORT\"),",
      "251:             timeout=conf.getint(\"smtp\", \"SMTP_TIMEOUT\"),",
      "252:         )",
      "254:     @mock.patch(\"smtplib.SMTP_SSL\")",
      "",
      "[Removed Lines]",
      "243:     def test_send_mime_ssl(self, mock_smtp, mock_smtp_ssl):",
      "",
      "[Added Lines]",
      "243:     def test_send_mime_ssl_none_context(self, mock_smtp, mock_smtp_ssl):",
      "244:         mock_smtp_ssl.return_value = mock.Mock()",
      "245:         with conf_vars({(\"smtp\", \"smtp_ssl\"): \"True\", (\"email\", \"ssl_context\"): \"none\"}):",
      "246:             email.send_mime_email(\"from\", \"to\", MIMEMultipart(), dryrun=False)",
      "247:         assert not mock_smtp.called",
      "248:         mock_smtp_ssl.assert_called_once_with(",
      "249:             host=conf.get(\"smtp\", \"SMTP_HOST\"),",
      "250:             port=conf.getint(\"smtp\", \"SMTP_PORT\"),",
      "251:             timeout=conf.getint(\"smtp\", \"SMTP_TIMEOUT\"),",
      "252:             context=None,",
      "253:         )",
      "255:     @mock.patch(\"smtplib.SMTP_SSL\")",
      "256:     @mock.patch(\"smtplib.SMTP\")",
      "257:     @mock.patch(\"ssl.create_default_context\")",
      "258:     def test_send_mime_ssl_default_context_if_not_set(self, create_default_context, mock_smtp, mock_smtp_ssl):",
      "263:         assert create_default_context.called",
      "268:             context=create_default_context.return_value,",
      "269:         )",
      "271:     @mock.patch(\"smtplib.SMTP_SSL\")",
      "272:     @mock.patch(\"smtplib.SMTP\")",
      "273:     @mock.patch(\"ssl.create_default_context\")",
      "274:     def test_send_mime_ssl_default_context_with_value_set_to_default(",
      "275:         self, create_default_context, mock_smtp, mock_smtp_ssl",
      "276:     ):",
      "277:         mock_smtp_ssl.return_value = mock.Mock()",
      "278:         with conf_vars({(\"smtp\", \"smtp_ssl\"): \"True\", (\"email\", \"ssl_context\"): \"default\"}):",
      "279:             email.send_mime_email(\"from\", \"to\", MIMEMultipart(), dryrun=False)",
      "280:         assert not mock_smtp.called",
      "281:         assert create_default_context.called",
      "282:         mock_smtp_ssl.assert_called_once_with(",
      "283:             host=conf.get(\"smtp\", \"SMTP_HOST\"),",
      "284:             port=conf.getint(\"smtp\", \"SMTP_PORT\"),",
      "285:             timeout=conf.getint(\"smtp\", \"SMTP_TIMEOUT\"),",
      "286:             context=create_default_context.return_value,",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "300:     @mock.patch(\"smtplib.SMTP_SSL\")",
      "301:     @mock.patch(\"smtplib.SMTP\")",
      "303:         mock_smtp_ssl.side_effect = SMTPServerDisconnected()",
      "304:         msg = MIMEMultipart()",
      "305:         with conf_vars({(\"smtp\", \"smtp_ssl\"): \"True\"}):",
      "",
      "[Removed Lines]",
      "302:     def test_send_mime_ssl_complete_failure(self, mock_smtp, mock_smtp_ssl):",
      "",
      "[Added Lines]",
      "336:     @mock.patch(\"ssl.create_default_context\")",
      "337:     def test_send_mime_ssl_complete_failure(self, create_default_context, mock_smtp, mock_smtp_ssl):",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "310:             host=conf.get(\"smtp\", \"SMTP_HOST\"),",
      "311:             port=conf.getint(\"smtp\", \"SMTP_PORT\"),",
      "312:             timeout=conf.getint(\"smtp\", \"SMTP_TIMEOUT\"),",
      "313:         )",
      "314:         assert mock_smtp_ssl.call_count == conf.getint(\"smtp\", \"SMTP_RETRY_LIMIT\")",
      "315:         assert not mock_smtp.called",
      "316:         assert not mock_smtp_ssl.return_value.starttls.called",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "348:             context=create_default_context.return_value,",
      "350:         assert create_default_context.called",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "0e513d83f924d0798400c9b01303022077718494",
      "candidate_info": {
        "commit_hash": "0e513d83f924d0798400c9b01303022077718494",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0e513d83f924d0798400c9b01303022077718494",
        "files": [
          "airflow/utils/log/secrets_masker.py",
          "tests/utils/log/test_secrets_masker.py"
        ],
        "message": "Fix issue with using the various state enum value in logs (#33065)\n\n* Fix issue with using the various state enum value in logs\n\nThe secrets masker is unable to work on the various state enums: DagRunState, TaskInstanceState,\nJobState, and State enums in logs.\nThis PR fixes this by converting the enums to strings during the secrets mask search\n\n* fixup! Fix issue with using the various state enum value in logs\n\n* apply to all enum\n\n* test with custom enum\n\n(cherry picked from commit b0f61be2f9791b75da3bca0bc30fdbb88e1e0a8a)",
        "before_after_code_files": [
          "airflow/utils/log/secrets_masker.py||airflow/utils/log/secrets_masker.py",
          "tests/utils/log/test_secrets_masker.py||tests/utils/log/test_secrets_masker.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33038"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/log/secrets_masker.py||airflow/utils/log/secrets_masker.py": [
          "File: airflow/utils/log/secrets_masker.py -> airflow/utils/log/secrets_masker.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import collections.abc",
          "21: import logging",
          "22: import sys",
          "23: from functools import cached_property",
          "24: from typing import (",
          "25:     TYPE_CHECKING,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "23: from enum import Enum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "242:                     for dict_key, subval in item.items()",
          "243:                 }",
          "244:                 return to_return",
          "245:             elif _is_v1_env_var(item):",
          "246:                 tmp: dict = item.to_dict()",
          "247:                 if should_hide_value_for_key(tmp.get(\"name\", \"\")) and \"value\" in tmp:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "246:             elif isinstance(item, Enum):",
          "247:                 return self._redact(item=item.value, name=name, depth=depth, max_depth=max_depth)",
          "",
          "---------------"
        ],
        "tests/utils/log/test_secrets_masker.py||tests/utils/log/test_secrets_masker.py": [
          "File: tests/utils/log/test_secrets_masker.py -> tests/utils/log/test_secrets_masker.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import os",
          "25: import sys",
          "26: import textwrap",
          "27: from unittest.mock import patch",
          "29: import pytest",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: from enum import Enum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "36:     redact,",
          "37:     should_hide_value_for_key,",
          "38: )",
          "39: from tests.test_utils.config import conf_vars",
          "41: settings.MASK_SECRETS_IN_LOGS = True",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "40: from airflow.utils.state import DagRunState, JobState, State, TaskInstanceState",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "43: p = \"password\"",
          "46: @pytest.fixture",
          "47: def logger(caplog):",
          "48:     logging.config.dictConfig(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "48: class MyEnum(str, Enum):",
          "49:     testname = \"testvalue\"",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "299:             got = redact(val, max_depth=max_depth)",
          "300:             assert got == expected",
          "303: class TestShouldHideValueForKey:",
          "304:     @pytest.mark.parametrize(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "308:     @pytest.mark.parametrize(",
          "309:         \"state, expected\",",
          "310:         [",
          "311:             (DagRunState.SUCCESS, \"success\"),",
          "312:             (TaskInstanceState.FAILED, \"failed\"),",
          "313:             (JobState.RUNNING, \"running\"),",
          "314:             ([DagRunState.SUCCESS, DagRunState.RUNNING], [\"success\", \"running\"]),",
          "315:             ([TaskInstanceState.FAILED, TaskInstanceState.SUCCESS], [\"failed\", \"success\"]),",
          "316:             (State.failed_states, frozenset([TaskInstanceState.FAILED, TaskInstanceState.UPSTREAM_FAILED])),",
          "317:             (MyEnum.testname, \"testvalue\"),",
          "318:         ],",
          "319:     )",
          "320:     def test_redact_state_enum(self, logger, caplog, state, expected):",
          "321:         logger.info(\"State: %s\", state)",
          "322:         assert caplog.text == f\"INFO State: {expected}\\n\"",
          "323:         assert \"TypeError\" not in caplog.text",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7aa705d58b89e62dd027620b6413d5249169ba71",
      "candidate_info": {
        "commit_hash": "7aa705d58b89e62dd027620b6413d5249169ba71",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7aa705d58b89e62dd027620b6413d5249169ba71",
        "files": [
          "scripts/in_container/_in_container_utils.sh"
        ],
        "message": "Temporarily exclude openlineage from \"PyPI\" constraints generation (#33101)\n\nThe openlineage provider has apache-airflow >= 2.7.0 as dependency,\nand this is a problem for automated constraint generation. Since there\nis only one version of the provider released, it's the only one that\ncan be installed from PyPI, but until apache-airflow 2.7.0 is released\nit cannot be installed because of the dependency missing.\n\nThis makes calculating of the constraints by PyPI impossible and it\nloops in continuously trying to find a solution that cannot be found.\n\nIn order to unblock constraint generation we need to exclude the\nprovider temporarily and regenerate the constraints again when airflow\n2.7.0 gets released (which will generally only bring the open-lineage\nprovider as part of the constraints).\n\n(cherry picked from commit b08188e24bf7e52a0b700e083caa38347a4fa3fc)\n(cherry picked from commit f14430d4d34413ea7aae0378a5169e4a16597555)",
        "before_after_code_files": [
          "scripts/in_container/_in_container_utils.sh||scripts/in_container/_in_container_utils.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33038"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/in_container/_in_container_utils.sh||scripts/in_container/_in_container_utils.sh": [
          "File: scripts/in_container/_in_container_utils.sh -> scripts/in_container/_in_container_utils.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "287:     for provider_package in ${ALL_PROVIDERS_PACKAGES}",
          "288:     do",
          "289:         echo -n \"Checking if ${provider_package} is available in PyPI: \"",
          "290:         res=$(curl --head -s -o /dev/null -w \"%{http_code}\" \"https://pypi.org/project/${provider_package}/\")",
          "291:         if [[ ${res} == \"200\" ]]; then",
          "292:             packages_to_install+=( \"${provider_package}\" )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "290:         if [[ ${provider_package} == \"apache-airflow-providers-openlineage\" ]]; then",
          "291:             # The openlineage provider has 2.7.0 airflow dependency so it should be excluded for now in",
          "292:             # \"pypi\" dependency calculation",
          "293:             # We should remove it right after 2.7.0 is released to PyPI and regenerate the 2.7.0 constraints",
          "294:             echo \"${COLOR_YELLOW}Skipped until 2.7.0 is released${COLOR_RESET}\"",
          "295:             continue",
          "296:         fi",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "db0186a8039d408460cb526f2025d5d4ba914a0b",
      "candidate_info": {
        "commit_hash": "db0186a8039d408460cb526f2025d5d4ba914a0b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/db0186a8039d408460cb526f2025d5d4ba914a0b",
        "files": [
          "airflow/www/static/js/dag.js",
          "airflow/www/static/js/dag/details/dagCode/CodeBlock.tsx",
          "airflow/www/static/js/dag_code.js",
          "airflow/www/templates/airflow/dag.html",
          "airflow/www/templates/airflow/dag_code.html",
          "airflow/www/views.py",
          "airflow/www/webpack.config.js",
          "docs/apache-airflow/img/code.png"
        ],
        "message": "Remove legacy dag code (#33058)\n\n* Remove legacy dag code and redirect to grid view\n\n* update docs image\n\n(cherry picked from commit 5c384e12dea898b0bf8dee44df115e33942686a6)",
        "before_after_code_files": [
          "airflow/www/static/js/dag.js||airflow/www/static/js/dag.js",
          "airflow/www/static/js/dag/details/dagCode/CodeBlock.tsx||airflow/www/static/js/dag/details/dagCode/CodeBlock.tsx",
          "airflow/www/static/js/dag_code.js||airflow/www/static/js/dag_code.js",
          "airflow/www/templates/airflow/dag.html||airflow/www/templates/airflow/dag.html",
          "airflow/www/templates/airflow/dag_code.html||airflow/www/templates/airflow/dag_code.html",
          "airflow/www/views.py||airflow/www/views.py",
          "airflow/www/webpack.config.js||airflow/www/webpack.config.js"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33038"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/static/js/dag.js||airflow/www/static/js/dag.js": [
          "File: airflow/www/static/js/dag.js -> airflow/www/static/js/dag.js",
          "--- Hunk 1 ---",
          "[Context before]",
          "49:   const gridNav = document.getElementById(\"grid-nav\");",
          "50:   const graphNav = document.getElementById(\"graph-nav\");",
          "51:   const ganttNav = document.getElementById(\"gantt-nav\");",
          "52:   if (isGrid) {",
          "53:     if (tab === \"graph\") {",
          "54:       gridNav.classList.remove(\"active\");",
          "55:       ganttNav.classList.remove(\"active\");",
          "56:       graphNav.classList.add(\"active\");",
          "57:     } else if (tab === \"gantt\") {",
          "58:       gridNav.classList.remove(\"active\");",
          "59:       graphNav.classList.remove(\"active\");",
          "60:       ganttNav.classList.add(\"active\");",
          "61:     } else {",
          "62:       graphNav.classList.remove(\"active\");",
          "63:       ganttNav.classList.remove(\"active\");",
          "64:       gridNav.classList.add(\"active\");",
          "65:     }",
          "66:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "52:   const codeNav = document.getElementById(\"code-nav\");",
          "57:       codeNav.classList.remove(\"active\");",
          "62:       codeNav.classList.remove(\"active\");",
          "64:     } else if (tab === \"code\") {",
          "65:       gridNav.classList.remove(\"active\");",
          "66:       graphNav.classList.remove(\"active\");",
          "67:       ganttNav.classList.remove(\"active\");",
          "68:       codeNav.classList.add(\"active\");",
          "72:       codeNav.classList.remove(\"active\");",
          "",
          "---------------"
        ],
        "airflow/www/static/js/dag/details/dagCode/CodeBlock.tsx||airflow/www/static/js/dag/details/dagCode/CodeBlock.tsx": [
          "File: airflow/www/static/js/dag/details/dagCode/CodeBlock.tsx -> airflow/www/static/js/dag/details/dagCode/CodeBlock.tsx",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import python from \"react-syntax-highlighter/dist/esm/languages/prism/python\";",
          "23: import React, { useState } from \"react\";",
          "24: import { Box, Button } from \"@chakra-ui/react\";",
          "26: SyntaxHighlighter.registerLanguage(\"python\", python);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: import { getMetaValue } from \"src/utils\";",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "29:   code: string;",
          "30: }",
          "31: export default function CodeBlock({ code }: Props) {",
          "33:   const toggleCodeWrap = () => setCodeWrap(!codeWrap);",
          "35:   return (",
          "",
          "[Removed Lines]",
          "32:   const [codeWrap, setCodeWrap] = useState(false);",
          "",
          "[Added Lines]",
          "33:   const [codeWrap, setCodeWrap] = useState(",
          "34:     getMetaValue(\"default_wrap\") === \"True\"",
          "35:   );",
          "",
          "---------------"
        ],
        "airflow/www/static/js/dag_code.js||airflow/www/static/js/dag_code.js": [
          "File: airflow/www/static/js/dag_code.js -> airflow/www/static/js/dag_code.js",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/www/templates/airflow/dag.html||airflow/www/templates/airflow/dag.html": [
          "File: airflow/www/templates/airflow/dag.html -> airflow/www/templates/airflow/dag.html",
          "--- Hunk 1 ---",
          "[Context before]",
          "210:           <li><a href=\"{{ url_for('Airflow.dag_details', dag_id=dag.dag_id) }}\">",
          "211:             <span class=\"material-icons\" aria-hidden=\"true\">details</span>",
          "212:             Details</a></li>",
          "214:             <span class=\"material-icons\" aria-hidden=\"true\">code</span>",
          "215:             Code</a></li>",
          "216:           <li><a href=\"{{ url_for('Airflow.audit_log', dag_id=dag.dag_id, root=root) }}\">",
          "",
          "[Removed Lines]",
          "213:           <li><a href=\"{{ url_for('Airflow.code', dag_id=dag.dag_id, root=root) }}\">",
          "",
          "[Added Lines]",
          "213:           <li id=\"code-nav\"><a href=\"{{ url_for('Airflow.code', dag_id=dag.dag_id, root=root) }}\">",
          "",
          "---------------"
        ],
        "airflow/www/templates/airflow/dag_code.html||airflow/www/templates/airflow/dag_code.html": [
          "File: airflow/www/templates/airflow/dag_code.html -> airflow/www/templates/airflow/dag_code.html",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "67: from markupsafe import Markup, escape",
          "68: from pendulum.datetime import DateTime",
          "69: from pendulum.parsing.exceptions import ParserError",
          "72: from sqlalchemy import Date, and_, case, desc, func, inspect, or_, select, union_all",
          "73: from sqlalchemy.exc import IntegrityError",
          "74: from sqlalchemy.orm import Session, joinedload",
          "",
          "[Removed Lines]",
          "70: from pygments import highlight, lexers",
          "71: from pygments.formatters import HtmlFormatter",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "100: from airflow.models import Connection, DagModel, DagTag, Log, SlaMiss, TaskFail, Trigger, XCom, errors",
          "101: from airflow.models.abstractoperator import AbstractOperator",
          "102: from airflow.models.dag import DAG, get_dataset_triggered_next_run_info",
          "104: from airflow.models.dagrun import RUN_ID_REGEX, DagRun, DagRunType",
          "105: from airflow.models.dataset import DagScheduleDatasetReference, DatasetDagRunQueue, DatasetEvent, DatasetModel",
          "106: from airflow.models.mappedoperator import MappedOperator",
          "",
          "[Removed Lines]",
          "103: from airflow.models.dagcode import DagCode",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1320:     @provide_session",
          "1321:     def code(self, dag_id, session: Session = NEW_SESSION):",
          "1322:         \"\"\"Dag Code.\"\"\"",
          "1349:     @expose(\"/dag_details\")",
          "1350:     @auth.has_access(",
          "",
          "[Removed Lines]",
          "1323:         dag = get_airflow_app().dag_bag.get_dag(dag_id, session=session)",
          "1324:         dag_model = DagModel.get_dagmodel(dag_id, session=session)",
          "1325:         if not dag or not dag_model:",
          "1326:             flash(f'DAG \"{dag_id}\" seems to be missing.', \"error\")",
          "1327:             return redirect(url_for(\"Airflow.index\"))",
          "1329:         wwwutils.check_import_errors(dag_model.fileloc, session)",
          "1330:         wwwutils.check_dag_warnings(dag_model.dag_id, session)",
          "1332:         try:",
          "1333:             code = DagCode.get_code_by_fileloc(dag_model.fileloc)",
          "1334:             html_code = Markup(highlight(code, lexers.PythonLexer(), HtmlFormatter(linenos=True)))",
          "1335:         except Exception as e:",
          "1336:             error = f\"Exception encountered during dag code retrieval/code highlighting:\\n\\n{e}\\n\"",
          "1337:             html_code = Markup(\"<p>Failed to load DAG file Code.</p><p>Details: {}</p>\").format(escape(error))",
          "1339:         return self.render_template(",
          "1340:             \"airflow/dag_code.html\",",
          "1341:             html_code=html_code,",
          "1342:             dag=dag,",
          "1343:             dag_model=dag_model,",
          "1344:             title=dag_id,",
          "1345:             root=request.args.get(\"root\"),",
          "1346:             wrapped=conf.getboolean(\"webserver\", \"default_wrap\"),",
          "1347:         )",
          "",
          "[Added Lines]",
          "1320:         kwargs = {",
          "1322:             \"dag_id\": dag_id,",
          "1323:             \"tab\": \"code\",",
          "1324:         }",
          "1326:         return redirect(url_for(\"Airflow.grid\", **kwargs))",
          "",
          "---------------"
        ],
        "airflow/www/webpack.config.js||airflow/www/webpack.config.js": [
          "File: airflow/www/webpack.config.js -> airflow/www/webpack.config.js",
          "--- Hunk 1 ---",
          "[Context before]",
          "61:     connectionForm: `${JS_DIR}/connection_form.js`,",
          "62:     chart: [`${CSS_DIR}/chart.css`],",
          "63:     dag: `${JS_DIR}/dag.js`,",
          "65:     dagDependencies: `${JS_DIR}/dag_dependencies.js`,",
          "66:     dags: [`${CSS_DIR}/dags.css`, `${JS_DIR}/dags.js`],",
          "67:     flash: `${CSS_DIR}/flash.css`,",
          "",
          "[Removed Lines]",
          "64:     dagCode: `${JS_DIR}/dag_code.js`,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "74aeccd236a159ad60c89f7ef8530b805ce73f1c",
      "candidate_info": {
        "commit_hash": "74aeccd236a159ad60c89f7ef8530b805ce73f1c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/74aeccd236a159ad60c89f7ef8530b805ce73f1c",
        "files": [
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "dev/breeze/tests/test_selective_checks.py"
        ],
        "message": "Avoid upgrade-to-newer-dependencies when provider.yaml changes (#33082)\n\nThe \"upgrade-to-newer-dependencies\" has been triggered when\nprovider.yaml changed, but this was completely unnecessary. The\nreal reason for upgrade is only when\ngenerated/provider_dependencies.json changes - because that is\nwhat changes dependencies used.\n\nThe provider_dependencies.json is automatically updated during\npre-commit based on provider.yaml files, so only using it to\ntrigger --upgrade-to-newer-dependencies is enough.\n\n(cherry picked from commit 825a818eb556e8d265f3b04b9476be55a77c9689)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "dev/breeze/tests/test_selective_checks.py||dev/breeze/tests/test_selective_checks.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33038"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py": [
          "File: dev/breeze/src/airflow_breeze/utils/selective_checks.py -> dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "133:             r\"^setup.cfg\",",
          "134:             r\"^setup.py\",",
          "135:             r\"^generated/provider_dependencies.json$\",",
          "137:         ],",
          "138:         FileGroupForCi.DOC_FILES: [",
          "139:             r\"^docs\",",
          "",
          "[Removed Lines]",
          "136:             r\"^airflow/providers/.*/provider.yaml$\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_selective_checks.py||dev/breeze/tests/test_selective_checks.py": [
          "File: dev/breeze/tests/test_selective_checks.py -> dev/breeze/tests/test_selective_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "925:         pytest.param(",
          "926:             (\"airflow/providers/microsoft/azure/provider.yaml\",),",
          "927:             {",
          "929:             },",
          "930:             id=\"Provider.yaml changed\",",
          "931:         ),",
          "",
          "[Removed Lines]",
          "928:                 \"upgrade-to-newer-dependencies\": \"true\",",
          "",
          "[Added Lines]",
          "928:                 \"upgrade-to-newer-dependencies\": \"false\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "12a801e9a28435605483d2011a0fca121e3864e4",
      "candidate_info": {
        "commit_hash": "12a801e9a28435605483d2011a0fca121e3864e4",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/12a801e9a28435605483d2011a0fca121e3864e4",
        "files": [
          "airflow/models/taskmixin.py",
          "tests/decorators/test_setup_teardown.py",
          "tests/models/test_taskmixin.py",
          "tests/utils/test_task_group.py"
        ],
        "message": "Don't auto-add to context just by virtue of arrowing (#33102)\n\n* Don't auto-add to context just by virtue of arrowing\n\n* no add tasks\n\n* Add back removed import\n\n* Fixup tests\n\n* fixup! Fixup tests\n\n---------\n\nCo-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>\n(cherry picked from commit c18a5a9f8630d7b9ec464eb30d61f084d01ffcec)",
        "before_after_code_files": [
          "airflow/models/taskmixin.py||airflow/models/taskmixin.py",
          "tests/decorators/test_setup_teardown.py||tests/decorators/test_setup_teardown.py",
          "tests/models/test_taskmixin.py||tests/models/test_taskmixin.py",
          "tests/utils/test_task_group.py||tests/utils/test_task_group.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33038"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/taskmixin.py||airflow/models/taskmixin.py": [
          "File: airflow/models/taskmixin.py -> airflow/models/taskmixin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: from airflow.exceptions import AirflowException, RemovedInAirflow3Warning",
          "26: from airflow.serialization.enums import DagAttributeTypes",
          "28: from airflow.utils.types import NOTSET, ArgNotSet",
          "30: if TYPE_CHECKING:",
          "",
          "[Removed Lines]",
          "27: from airflow.utils.setup_teardown import SetupTeardownContext",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "97:     def __lshift__(self, other: DependencyMixin | Sequence[DependencyMixin]):",
          "98:         \"\"\"Implements Task << Task.\"\"\"",
          "99:         self.set_upstream(other)",
          "102:         return other",
          "104:     def __rshift__(self, other: DependencyMixin | Sequence[DependencyMixin]):",
          "105:         \"\"\"Implements Task >> Task.\"\"\"",
          "106:         self.set_downstream(other)",
          "109:         return other",
          "111:     def __rrshift__(self, other: DependencyMixin | Sequence[DependencyMixin]):",
          "",
          "[Removed Lines]",
          "100:         self.set_setup_teardown_ctx_dependencies(other)",
          "101:         self.set_taskgroup_ctx_dependencies(other)",
          "107:         self.set_setup_teardown_ctx_dependencies(other)",
          "108:         self.set_taskgroup_ctx_dependencies(other)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "136:             for o in obj:",
          "137:                 yield from cls._iter_references(o)",
          "156: class TaskMixin(DependencyMixin):",
          "157:     \"\"\"Mixin to provide task-related things.",
          "",
          "[Removed Lines]",
          "139:     def set_setup_teardown_ctx_dependencies(self, other: DependencyMixin | Sequence[DependencyMixin]):",
          "140:         if not SetupTeardownContext.active:",
          "141:             return",
          "142:         for op, _ in self._iter_references([self, other]):",
          "143:             SetupTeardownContext.update_context_map(op)",
          "145:     def set_taskgroup_ctx_dependencies(self, other: DependencyMixin | Sequence[DependencyMixin]):",
          "146:         from airflow.utils.task_group import TaskGroupContext",
          "148:         if not TaskGroupContext.active:",
          "149:             return",
          "150:         task_group = TaskGroupContext.get_current_task_group(None)",
          "151:         for op, _ in self._iter_references([self, other]):",
          "152:             if task_group:",
          "153:                 op.add_to_taskgroup(task_group)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/decorators/test_setup_teardown.py||tests/decorators/test_setup_teardown.py": [
          "File: tests/decorators/test_setup_teardown.py -> tests/decorators/test_setup_teardown.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1130:             \"mytask\",",
          "1131:         }",
          "1186:     def test_tasks_decorators_called_outside_context_manager_can_link_up_with_scope(self, dag_maker):",
          "1187:         @setup",
          "1188:         def setuptask():",
          "",
          "[Removed Lines]",
          "1133:     def test_tasks_decorators_called_outside_context_manager_can_link_up(self, dag_maker):",
          "1134:         @setup",
          "1135:         def setuptask():",
          "1136:             print(\"setup\")",
          "1138:         @task()",
          "1139:         def mytask():",
          "1140:             print(\"mytask\")",
          "1142:         @task()",
          "1143:         def mytask2():",
          "1144:             print(\"mytask 2\")",
          "1146:         @teardown",
          "1147:         def teardowntask():",
          "1148:             print(\"teardown\")",
          "1150:         with dag_maker() as dag:",
          "1151:             task1 = mytask()",
          "1152:             task2 = mytask2()",
          "1153:             with setuptask() >> teardowntask():",
          "1154:                 task1 >> task2",
          "1156:         assert len(dag.task_group.children) == 4",
          "1157:         assert not dag.task_group.children[\"setuptask\"].upstream_task_ids",
          "1158:         assert dag.task_group.children[\"setuptask\"].downstream_task_ids == {\"mytask\", \"teardowntask\"}",
          "1159:         assert dag.task_group.children[\"mytask\"].upstream_task_ids == {\"setuptask\"}",
          "1160:         assert dag.task_group.children[\"mytask\"].downstream_task_ids == {\"mytask2\"}",
          "1161:         assert dag.task_group.children[\"mytask2\"].upstream_task_ids == {\"mytask\"}",
          "1162:         assert dag.task_group.children[\"mytask2\"].downstream_task_ids == {\"teardowntask\"}",
          "1163:         assert dag.task_group.children[\"teardowntask\"].upstream_task_ids == {\"mytask2\", \"setuptask\"}",
          "1164:         assert not dag.task_group.children[\"teardowntask\"].downstream_task_ids",
          "1166:     def test_classic_tasks_called_outside_context_manager_can_link_up(self, dag_maker):",
          "1168:         with dag_maker() as dag:",
          "1169:             setuptask = BashOperator(task_id=\"setuptask\", bash_command=\"echo 1\").as_setup()",
          "1170:             teardowntask = BashOperator(task_id=\"teardowntask\", bash_command=\"echo 1\").as_teardown()",
          "1171:             mytask = BashOperator(task_id=\"mytask\", bash_command=\"echo 1\")",
          "1172:             mytask2 = BashOperator(task_id=\"mytask2\", bash_command=\"echo 1\")",
          "1173:             with setuptask >> teardowntask:",
          "1174:                 mytask >> mytask2",
          "1176:         assert len(dag.task_group.children) == 4",
          "1177:         assert not dag.task_group.children[\"setuptask\"].upstream_task_ids",
          "1178:         assert dag.task_group.children[\"setuptask\"].downstream_task_ids == {\"mytask\", \"teardowntask\"}",
          "1179:         assert dag.task_group.children[\"mytask\"].upstream_task_ids == {\"setuptask\"}",
          "1180:         assert dag.task_group.children[\"mytask\"].downstream_task_ids == {\"mytask2\"}",
          "1181:         assert dag.task_group.children[\"mytask2\"].upstream_task_ids == {\"mytask\"}",
          "1182:         assert dag.task_group.children[\"mytask2\"].downstream_task_ids == {\"teardowntask\"}",
          "1183:         assert dag.task_group.children[\"teardowntask\"].upstream_task_ids == {\"mytask2\", \"setuptask\"}",
          "1184:         assert not dag.task_group.children[\"teardowntask\"].downstream_task_ids",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/models/test_taskmixin.py||tests/models/test_taskmixin.py": [
          "File: tests/models/test_taskmixin.py -> tests/models/test_taskmixin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "209:             ValueError, match=\"Cannot mark task 'my_ok_task__2' as setup; task is already a teardown.\"",
          "210:         ):",
          "211:             m.operator.is_setup = True",
          "",
          "[Removed Lines]",
          "214: def test_set_setup_teardown_ctx_dependencies_using_decorated_tasks(dag_maker):",
          "216:     with dag_maker():",
          "217:         t1 = make_task(\"t1\", type_=\"decorated\")",
          "218:         setuptask = make_task(\"setuptask\", type_=\"decorated\", setup_=True)",
          "219:         teardowntask = make_task(\"teardowntask\", type_=\"decorated\", teardown_=True)",
          "220:         with setuptask >> teardowntask as scope:",
          "221:             scope.add_task(t1)",
          "223:     assert t1.operator.upstream_task_ids == {\"setuptask\"}",
          "224:     assert t1.operator.downstream_task_ids == {\"teardowntask\"}",
          "226:     with dag_maker():",
          "227:         t1 = make_task(\"t1\", type_=\"decorated\")",
          "228:         t2 = make_task(\"t2\", type_=\"decorated\")",
          "229:         setuptask = make_task(\"setuptask\", type_=\"decorated\", setup_=True)",
          "230:         teardowntask = make_task(\"teardowntask\", type_=\"decorated\", teardown_=True)",
          "231:         with setuptask >> teardowntask:",
          "232:             t1 >> t2",
          "233:     assert t1.operator.upstream_task_ids == {\"setuptask\"}",
          "234:     assert t2.operator.downstream_task_ids == {\"teardowntask\"}",
          "236:     with dag_maker():",
          "237:         t1 = make_task(\"t1\", type_=\"decorated\")",
          "238:         t2 = make_task(\"t2\", type_=\"decorated\")",
          "239:         t3 = make_task(\"t3\", type_=\"decorated\")",
          "240:         setuptask = make_task(\"setuptask\", type_=\"decorated\", setup_=True)",
          "241:         teardowntask = make_task(\"teardowntask\", type_=\"decorated\", teardown_=True)",
          "242:         with setuptask >> teardowntask:",
          "243:             t1 >> [t2, t3]",
          "245:     assert t1.operator.upstream_task_ids == {\"setuptask\"}",
          "246:     assert t2.operator.downstream_task_ids == {\"teardowntask\"}",
          "247:     assert t3.operator.downstream_task_ids == {\"teardowntask\"}",
          "249:     with dag_maker():",
          "250:         t1 = make_task(\"t1\", type_=\"decorated\")",
          "251:         t2 = make_task(\"t2\", type_=\"decorated\")",
          "252:         t3 = make_task(\"t3\", type_=\"decorated\")",
          "253:         setuptask = make_task(\"setuptask\", type_=\"decorated\", setup_=True)",
          "254:         teardowntask = make_task(\"teardowntask\", type_=\"decorated\", teardown_=True)",
          "255:         with setuptask >> teardowntask:",
          "256:             [t1, t2] >> t3",
          "258:     assert t1.operator.upstream_task_ids == {\"setuptask\"}",
          "259:     assert t2.operator.upstream_task_ids == {\"setuptask\"}",
          "260:     assert t3.operator.downstream_task_ids == {\"teardowntask\"}",
          "263: def test_set_setup_teardown_ctx_dependencies_using_classic_tasks(dag_maker):",
          "264:     with dag_maker():",
          "265:         t1 = make_task(\"t1\", type_=\"classic\")",
          "266:         setuptask = make_task(\"setuptask\", type_=\"classic\", setup_=True)",
          "267:         teardowntask = make_task(\"teardowntask\", type_=\"classic\", teardown_=True)",
          "268:         with setuptask >> teardowntask as scope:",
          "269:             scope.add_task(t1)",
          "271:     assert t1.upstream_task_ids == {\"setuptask\"}",
          "272:     assert t1.downstream_task_ids == {\"teardowntask\"}",
          "274:     with dag_maker():",
          "275:         t1 = make_task(\"t1\", type_=\"classic\")",
          "276:         t2 = make_task(\"t2\", type_=\"classic\")",
          "277:         setuptask = make_task(\"setuptask\", type_=\"classic\", setup_=True)",
          "278:         teardowntask = make_task(\"teardowntask\", type_=\"classic\", teardown_=True)",
          "279:         with setuptask >> teardowntask:",
          "280:             t1 >> t2",
          "281:     assert t1.upstream_task_ids == {\"setuptask\"}",
          "282:     assert t2.downstream_task_ids == {\"teardowntask\"}",
          "284:     with dag_maker():",
          "285:         t1 = make_task(\"t1\", type_=\"classic\")",
          "286:         t2 = make_task(\"t2\", type_=\"classic\")",
          "287:         t3 = make_task(\"t3\", type_=\"classic\")",
          "288:         setuptask = make_task(\"setuptask\", type_=\"classic\", setup_=True)",
          "289:         teardowntask = make_task(\"teardowntask\", type_=\"classic\", teardown_=True)",
          "290:         with setuptask >> teardowntask:",
          "291:             t1 >> [t2, t3]",
          "293:     assert t1.upstream_task_ids == {\"setuptask\"}",
          "294:     assert t2.downstream_task_ids == {\"teardowntask\"}",
          "295:     assert t3.downstream_task_ids == {\"teardowntask\"}",
          "297:     with dag_maker():",
          "298:         t1 = make_task(\"t1\", type_=\"classic\")",
          "299:         t2 = make_task(\"t2\", type_=\"classic\")",
          "300:         t3 = make_task(\"t3\", type_=\"classic\")",
          "301:         setuptask = make_task(\"setuptask\", type_=\"classic\", setup_=True)",
          "302:         teardowntask = make_task(\"teardowntask\", type_=\"classic\", teardown_=True)",
          "303:         with setuptask >> teardowntask:",
          "304:             [t1, t2] >> t3",
          "306:     assert t1.upstream_task_ids == {\"setuptask\"}",
          "307:     assert t2.upstream_task_ids == {\"setuptask\"}",
          "308:     assert t3.downstream_task_ids == {\"teardowntask\"}",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/utils/test_task_group.py||tests/utils/test_task_group.py": [
          "File: tests/utils/test_task_group.py -> tests/utils/test_task_group.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import pytest",
          "25: from airflow.decorators import dag, task as task_decorator, task_group as task_group_decorator",
          "27: from airflow.models.baseoperator import BaseOperator",
          "28: from airflow.models.dag import DAG",
          "29: from airflow.models.xcom_arg import XComArg",
          "",
          "[Removed Lines]",
          "26: from airflow.exceptions import AirflowException, TaskAlreadyInTaskGroup",
          "",
          "[Added Lines]",
          "26: from airflow.exceptions import TaskAlreadyInTaskGroup",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1479:         tg1 >> w2",
          "1480:     assert t1.downstream_task_ids == set()",
          "1481:     assert w1.downstream_task_ids == {\"tg1.t1\", \"w2\"}",
          "",
          "[Removed Lines]",
          "1484: def test_tasks_defined_outside_taskgrooup(dag_maker):",
          "1485:     # Test that classic tasks defined outside a task group are added to the root task group",
          "1486:     # when the relationships are defined inside the task group",
          "1487:     with dag_maker() as dag:",
          "1488:         t1 = make_task(\"t1\")",
          "1489:         t2 = make_task(\"t2\")",
          "1490:         t3 = make_task(\"t3\")",
          "1491:         with TaskGroup(group_id=\"tg1\"):",
          "1492:             t1 >> t2 >> t3",
          "1493:     dag.validate()",
          "1494:     assert dag.task_group.children.keys() == {\"tg1\"}",
          "1495:     assert dag.task_group.children[\"tg1\"].children.keys() == {\"t1\", \"t2\", \"t3\"}",
          "1496:     assert dag.task_group.children[\"tg1\"].children[\"t1\"].upstream_task_ids == set()",
          "1497:     assert dag.task_group.children[\"tg1\"].children[\"t1\"].downstream_task_ids == {\"t2\"}",
          "1498:     assert dag.task_group.children[\"tg1\"].children[\"t2\"].upstream_task_ids == {\"t1\"}",
          "1499:     assert dag.task_group.children[\"tg1\"].children[\"t2\"].downstream_task_ids == {\"t3\"}",
          "1500:     assert dag.task_group.children[\"tg1\"].children[\"t3\"].upstream_task_ids == {\"t2\"}",
          "1501:     assert dag.task_group.children[\"tg1\"].children[\"t3\"].downstream_task_ids == set()",
          "1503:     # Test that decorated tasks defined outside a task group are added to the root task group",
          "1504:     # when relationships are defined inside the task group",
          "1505:     with dag_maker() as dag:",
          "1506:         t1 = make_task(\"t1\", type_=\"decorated\")",
          "1507:         t2 = make_task(\"t2\", type_=\"decorated\")",
          "1508:         t3 = make_task(\"t3\", type_=\"decorated\")",
          "1509:         with TaskGroup(group_id=\"tg1\"):",
          "1510:             t1 >> t2 >> t3",
          "1511:     dag.validate()",
          "1512:     assert dag.task_group.children.keys() == {\"tg1\"}",
          "1513:     assert dag.task_group.children[\"tg1\"].children.keys() == {\"t1\", \"t2\", \"t3\"}",
          "1514:     assert dag.task_group.children[\"tg1\"].children[\"t1\"].upstream_task_ids == set()",
          "1515:     assert dag.task_group.children[\"tg1\"].children[\"t1\"].downstream_task_ids == {\"t2\"}",
          "1516:     assert dag.task_group.children[\"tg1\"].children[\"t2\"].upstream_task_ids == {\"t1\"}",
          "1517:     assert dag.task_group.children[\"tg1\"].children[\"t2\"].downstream_task_ids == {\"t3\"}",
          "1518:     assert dag.task_group.children[\"tg1\"].children[\"t3\"].upstream_task_ids == {\"t2\"}",
          "1519:     assert dag.task_group.children[\"tg1\"].children[\"t3\"].downstream_task_ids == set()",
          "1521:     # Test adding single decorated task defined outside a task group to a task group",
          "1522:     with dag_maker() as dag:",
          "1523:         t1 = make_task(\"t1\", type_=\"decorated\")",
          "1524:         with TaskGroup(group_id=\"tg1\") as tg1:",
          "1525:             tg1.add_task(t1)",
          "1526:     dag.validate()",
          "1527:     assert dag.task_group.children.keys() == {\"tg1\"}",
          "1528:     assert dag.task_group.children[\"tg1\"].children.keys() == {\"t1\"}",
          "1529:     assert dag.task_group.children[\"tg1\"].children[\"t1\"].upstream_task_ids == set()",
          "1530:     assert dag.task_group.children[\"tg1\"].children[\"t1\"].downstream_task_ids == set()",
          "1532:     # Test adding single classic task defined outside a task group to a task group",
          "1533:     with dag_maker() as dag:",
          "1534:         t1 = make_task(\"t1\")",
          "1535:         with TaskGroup(group_id=\"tg1\") as tg1:",
          "1536:             tg1.add_task(t1)",
          "1537:     dag.validate()",
          "1538:     assert dag.task_group.children.keys() == {\"tg1\"}",
          "1539:     assert dag.task_group.children[\"tg1\"].children.keys() == {\"t1\"}",
          "1540:     assert dag.task_group.children[\"tg1\"].children[\"t1\"].upstream_task_ids == set()",
          "1541:     assert dag.task_group.children[\"tg1\"].children[\"t1\"].downstream_task_ids == set()",
          "1543:     with pytest.raises(",
          "1544:         AirflowException,",
          "1545:         match=\"Using this method on a task group that's not a context manager is not supported.\",",
          "1546:     ):",
          "1547:         with dag_maker():",
          "1548:             t1 = make_task(\"t1\")",
          "1549:             tg1 = TaskGroup(group_id=\"tg1\")",
          "1550:             tg1.add_task(t1)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    }
  ]
}