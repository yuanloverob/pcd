{
  "cve_id": "CVE-2021-45229",
  "cve_desc": "It was discovered that the \"Trigger DAG with config\" screen was susceptible to XSS attacks via the `origin` query argument. This issue affects Apache Airflow versions 2.2.3 and below.",
  "repo": "apache/airflow",
  "patch_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
  "patch_info": {
    "commit_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "files": [
      "airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py"
    ],
    "message": "Simplify trigger cancel button (#21591)\n\nCo-authored-by: Jed Cunningham <jedcunningham@apache.org>\n(cherry picked from commit 65297673a318660fba76797e50d0c06804dfcafc)",
    "before_after_code_files": [
      "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
    ]
  },
  "patch_diff": {
    "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html": [
      "File: airflow/www/templates/airflow/trigger.html -> airflow/www/templates/airflow/trigger.html",
      "--- Hunk 1 ---",
      "[Context before]",
      "63:       </label>",
      "64:     </div>",
      "65:     <button type=\"submit\" class=\"btn btn-primary\">Trigger</button>",
      "67:   </form>",
      "68: {% endblock %}",
      "",
      "[Removed Lines]",
      "66:     <button type=\"button\" class=\"btn\" onclick=\"location.href = '{{ origin }}'; return false\">Cancel</button>",
      "",
      "[Added Lines]",
      "66:     <a class=\"btn\" href=\"{{ origin }}\">Cancel</a>",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
      "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "133:         (\"javascript:alert(1)\", \"/home\"),",
      "134:         (\"http://google.com\", \"/home\"),",
      "135:         (\"36539'%3balert(1)%2f%2f166\", \"/home\"),",
      "136:         (",
      "137:             \"%2Ftree%3Fdag_id%3Dexample_bash_operator';alert(33)//\",",
      "138:             \"/home\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "136:         (",
      "137:             '\"><script>alert(99)</script><a href=\"',",
      "138:             \"&#34;&gt;&lt;script&gt;alert(99)&lt;/script&gt;&lt;a href=&#34;\",",
      "139:         ),",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "145:     test_dag_id = \"example_bash_operator\"",
      "147:     resp = admin_client.get(f'trigger?dag_id={test_dag_id}&origin={test_origin}')",
      "156: @pytest.mark.parametrize(",
      "",
      "[Removed Lines]",
      "148:     check_content_in_response(",
      "149:         '<button type=\"button\" class=\"btn\" onclick=\"location.href = \\'{}\\'; return false\">'.format(",
      "150:             expected_origin",
      "151:         ),",
      "152:         resp,",
      "153:     )",
      "",
      "[Added Lines]",
      "152:     check_content_in_response(f'<a class=\"btn\" href=\"{expected_origin}\">Cancel</a>', resp)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "a2fb0f4e46ad6d2b8fc43f9a7da288f35fedae3b",
      "candidate_info": {
        "commit_hash": "a2fb0f4e46ad6d2b8fc43f9a7da288f35fedae3b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a2fb0f4e46ad6d2b8fc43f9a7da288f35fedae3b",
        "files": [
          "BREEZE.rst",
          "CI.rst",
          "PULL_REQUEST_WORKFLOW.rst",
          "airflow/dag_processing/manager.py",
          "breeze",
          "scripts/ci/libraries/_build_images.sh"
        ],
        "message": "Be build -> built, and a stray space (#20703)\n\n(cherry picked from commit 73472e0b7cab5030c69a43196e3b392722b3da58)",
        "before_after_code_files": [
          "airflow/dag_processing/manager.py||airflow/dag_processing/manager.py",
          "scripts/ci/libraries/_build_images.sh||scripts/ci/libraries/_build_images.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/dag_processing/manager.py||airflow/dag_processing/manager.py": [
          "File: airflow/dag_processing/manager.py -> airflow/dag_processing/manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "440:         if conf.get('core', 'sql_alchemy_conn').startswith('sqlite') and self._parallelism > 1:",
          "441:             self.log.warning(",
          "442:                 \"Because we cannot use more than 1 thread (parsing_processes = \"",
          "444:                 self._parallelism,",
          "445:             )",
          "446:             self._parallelism = 1",
          "",
          "[Removed Lines]",
          "443:                 \"%d ) when using sqlite. So we set parallelism to 1.\",",
          "",
          "[Added Lines]",
          "443:                 \"%d) when using sqlite. So we set parallelism to 1.\",",
          "",
          "---------------"
        ],
        "scripts/ci/libraries/_build_images.sh||scripts/ci/libraries/_build_images.sh": [
          "File: scripts/ci/libraries/_build_images.sh -> scripts/ci/libraries/_build_images.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "111:     echo >\"${DETECTED_TERMINAL}\"",
          "112:     set +u",
          "113:     if [[ ${#MODIFIED_FILES[@]} != \"\" ]]; then",
          "115:         echo \"${COLOR_YELLOW}The files were modified since last build: ${MODIFIED_FILES[*]}${COLOR_RESET}\" >\"${DETECTED_TERMINAL}\"",
          "116:     fi",
          "117:     if [[ ${ACTION} == \"pull and rebuild\" ]]; then",
          "",
          "[Removed Lines]",
          "114:         echo \"${COLOR_YELLOW}The CI image for Python ${PYTHON_BASE_IMAGE} image likely needs to be rebuild${COLOR_RESET}\" >\"${DETECTED_TERMINAL}\"",
          "",
          "[Added Lines]",
          "114:         echo \"${COLOR_YELLOW}The CI image for Python ${PYTHON_BASE_IMAGE} image likely needs to be rebuilt${COLOR_RESET}\" >\"${DETECTED_TERMINAL}\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "129:     RES=$?",
          "130: }",
          "134: # is needed and that the rebuild is not already forced. It asks the user using available terminals",
          "135: # So that the script works also from within pre-commit run via git hooks - where stdin is not",
          "136: # available - it tries to find usable terminal and ask the user via this terminal.",
          "",
          "[Removed Lines]",
          "132: # Confirms if the image should be rebuild and interactively checks it with the user.",
          "133: # In case iit needs to be rebuild. It only ask the user if it determines that the rebuild",
          "",
          "[Added Lines]",
          "132: # Confirms if the image should be rebuilt and interactively checks it with the user.",
          "133: # In case iit needs to be rebuilt. It only ask the user if it determines that the rebuild",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "172:         echo",
          "173:         set +u",
          "174:         if [[ ${#MODIFIED_FILES[@]} != \"\" ]]; then",
          "176:             echo \"${COLOR_YELLOW}The files were modified since last build: ${MODIFIED_FILES[*]}${COLOR_RESET}\"",
          "177:         fi",
          "178:         echo",
          "",
          "[Removed Lines]",
          "175:             echo \"${COLOR_YELLOW}The CI image for Python ${PYTHON_BASE_IMAGE} image likely needs to be rebuild${COLOR_RESET}\"",
          "",
          "[Added Lines]",
          "175:             echo \"${COLOR_YELLOW}The CI image for Python ${PYTHON_BASE_IMAGE} image likely needs to be rebuilt${COLOR_RESET}\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e2dcdfd29784926470b44e3a7a127e0aa5018063",
      "candidate_info": {
        "commit_hash": "e2dcdfd29784926470b44e3a7a127e0aa5018063",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e2dcdfd29784926470b44e3a7a127e0aa5018063",
        "files": [
          "setup.py",
          "tests/providers/amazon/aws/hooks/test_cloud_formation.py",
          "tests/providers/amazon/aws/hooks/test_logs.py",
          "tests/providers/amazon/aws/log/test_cloudwatch_task_handler.py"
        ],
        "message": "Fix failures with recent moto library 2.2.15 (#19693)\n\nThe recent moto library is more picky about parameters\npassed to it:\n* when you are sending too old logs they are rejected\n* when you are passing cloud formation template they are parsed\n  and validated for correctness\n\nOur tests had artifficial values for those, which caused failures\nwith the recent moto version.\n\nThis PR provides realistic values in tests to pass moto validation\n\n(cherry picked from commit 49b7e751eb3cb512d138a06237116c3aec6c4290)",
        "before_after_code_files": [
          "setup.py||setup.py",
          "tests/providers/amazon/aws/hooks/test_cloud_formation.py||tests/providers/amazon/aws/hooks/test_cloud_formation.py",
          "tests/providers/amazon/aws/hooks/test_logs.py||tests/providers/amazon/aws/hooks/test_logs.py",
          "tests/providers/amazon/aws/log/test_cloudwatch_task_handler.py||tests/providers/amazon/aws/log/test_cloudwatch_task_handler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "532:     'jira',",
          "533:     'jsondiff',",
          "534:     'mongomock',",
          "536:     'mypy==0.770',",
          "537:     'parameterized',",
          "538:     'paramiko',",
          "",
          "[Removed Lines]",
          "535:     'moto~=2.2, >=2.2.7',",
          "",
          "[Added Lines]",
          "535:     'moto~=2.2, >=2.2.12',",
          "",
          "---------------"
        ],
        "tests/providers/amazon/aws/hooks/test_cloud_formation.py||tests/providers/amazon/aws/hooks/test_cloud_formation.py": [
          "File: tests/providers/amazon/aws/hooks/test_cloud_formation.py -> tests/providers/amazon/aws/hooks/test_cloud_formation.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: try:",
          "25:     from moto import mock_cloudformation",
          "27: except ImportError:",
          "28:     mock_cloudformation = None",
          "",
          "[Removed Lines]",
          "26:     from moto.ec2.models import NetworkInterface as some_model",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39:             {",
          "40:                 'Resources': {",
          "41:                     \"myResource\": {",
          "44:                     }",
          "46:             }",
          "47:         )",
          "",
          "[Removed Lines]",
          "42:                         \"Type\": some_model.cloudformation_type(),",
          "43:                         \"Properties\": {\"myProperty\": \"myPropertyValue\"},",
          "45:                 }",
          "",
          "[Added Lines]",
          "41:                         \"Type\": \"AWS::EC2::VPC\",",
          "42:                         \"Properties\": {",
          "43:                             \"CidrBlock\": {\"Ref\": \"VPCCidr\"},",
          "44:                             \"Tags\": [{\"Key\": \"Name\", \"Value\": \"Primary_CF_VPC\"}],",
          "45:                         },",
          "47:                 },",
          "48:                 \"Parameters\": {",
          "49:                     \"VPCCidr\": {",
          "50:                         \"Type\": \"String\",",
          "51:                         \"Default\": \"10.0.0.0/16\",",
          "52:                         \"Description\": \"Enter the CIDR block for the VPC. Default is 10.0.0.0/16.\",",
          "53:                     }",
          "54:                 },",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "51:             params={",
          "52:                 'TimeoutInMinutes': timeout,",
          "53:                 'TemplateBody': template_body,",
          "55:             },",
          "56:         )",
          "",
          "[Removed Lines]",
          "54:                 'Parameters': [{'ParameterKey': 'myParam', 'ParameterValue': 'myParamValue'}],",
          "",
          "[Added Lines]",
          "63:                 'Parameters': [{'ParameterKey': \"VPCCidr\", 'ParameterValue': '10.0.0.0/16'}],",
          "",
          "---------------"
        ],
        "tests/providers/amazon/aws/hooks/test_logs.py||tests/providers/amazon/aws/hooks/test_logs.py": [
          "File: tests/providers/amazon/aws/hooks/test_logs.py -> tests/providers/amazon/aws/hooks/test_logs.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: #",
          "20: import unittest",
          "22: from airflow.providers.amazon.aws.hooks.logs import AwsLogsHook",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: import time",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "49:         conn.create_log_group(logGroupName=log_group_name)",
          "50:         conn.create_log_stream(logGroupName=log_group_name, logStreamName=log_stream_name)",
          "54:         conn.put_log_events(",
          "55:             logGroupName=log_group_name, logStreamName=log_stream_name, logEvents=input_events",
          "",
          "[Removed Lines]",
          "52:         input_events = [{'timestamp': 1, 'message': 'Test Message 1'}]",
          "",
          "[Added Lines]",
          "52:         input_events = [{'timestamp': int(time.time()) * 1000, 'message': 'Test Message 1'}]",
          "",
          "---------------"
        ],
        "tests/providers/amazon/aws/log/test_cloudwatch_task_handler.py||tests/providers/amazon/aws/log/test_cloudwatch_task_handler.py": [
          "File: tests/providers/amazon/aws/log/test_cloudwatch_task_handler.py -> tests/providers/amazon/aws/log/test_cloudwatch_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "19: import unittest",
          "20: from unittest import mock",
          "21: from unittest.mock import ANY, call",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: import time",
          "20: from datetime import datetime as dt",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "38:     mock_logs = None",
          "41: @unittest.skipIf(mock_logs is None, \"Skipping test because moto.mock_logs is not available\")",
          "42: @mock_logs",
          "43: class TestCloudwatchTaskHandler(unittest.TestCase):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42: def get_time_str(time_in_milliseconds):",
          "43:     dt_time = dt.utcfromtimestamp(time_in_milliseconds / 1000.0)",
          "44:     return dt_time.strftime(\"%Y-%m-%d %H:%M:%S,000\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "117:     def test_event_to_str(self):",
          "118:         handler = self.cloudwatch_task_handler",
          "119:         events = [",
          "123:         ]",
          "124:         assert [handler._event_to_str(event) for event in events] == (",
          "125:             [",
          "129:             ]",
          "130:         )",
          "",
          "[Removed Lines]",
          "120:             {'timestamp': 1617400267123, 'message': 'First'},",
          "121:             {'timestamp': 1617400367456, 'message': 'Second'},",
          "122:             {'timestamp': 1617400467789, 'message': 'Third'},",
          "126:                 '[2021-04-02 21:51:07,123] First',",
          "127:                 '[2021-04-02 21:52:47,456] Second',",
          "128:                 '[2021-04-02 21:54:27,789] Third',",
          "",
          "[Added Lines]",
          "125:         current_time = int(time.time()) * 1000",
          "127:             {'timestamp': current_time - 2000, 'message': 'First'},",
          "128:             {'timestamp': current_time - 1000, 'message': 'Second'},",
          "129:             {'timestamp': current_time, 'message': 'Third'},",
          "133:                 f'[{get_time_str(current_time-2000)}] First',",
          "134:                 f'[{get_time_str(current_time-1000)}] Second',",
          "135:                 f'[{get_time_str(current_time)}] Third',",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "134:         # CloudWatch events must be ordered chronologically otherwise",
          "135:         # boto3 put_log_event API throws InvalidParameterException",
          "136:         # (moto does not throw this exception)",
          "137:         generate_log_events(",
          "138:             self.conn,",
          "139:             self.remote_log_group,",
          "140:             self.remote_log_stream,",
          "141:             [",
          "145:             ],",
          "146:         )",
          "148:         msg_template = '*** Reading remote log from Cloudwatch log_group: {} log_stream: {}.\\n{}\\n'",
          "149:         events = '\\n'.join(",
          "150:             [",
          "154:             ]",
          "155:         )",
          "156:         assert self.cloudwatch_task_handler.read(self.ti) == (",
          "",
          "[Removed Lines]",
          "142:                 {'timestamp': 1617400267123, 'message': 'First'},",
          "143:                 {'timestamp': 1617400367456, 'message': 'Second'},",
          "144:                 {'timestamp': 1617400467789, 'message': 'Third'},",
          "151:                 '[2021-04-02 21:51:07,123] First',",
          "152:                 '[2021-04-02 21:52:47,456] Second',",
          "153:                 '[2021-04-02 21:54:27,789] Third',",
          "",
          "[Added Lines]",
          "144:         current_time = int(time.time()) * 1000",
          "150:                 {'timestamp': current_time - 2000, 'message': 'First'},",
          "151:                 {'timestamp': current_time - 1000, 'message': 'Second'},",
          "152:                 {'timestamp': current_time, 'message': 'Third'},",
          "159:                 f'[{get_time_str(current_time-2000)}] First',",
          "160:                 f'[{get_time_str(current_time-1000)}] Second',",
          "161:                 f'[{get_time_str(current_time)}] Third',",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "676e16002685698197212e756e691ddcdf527d38",
      "candidate_info": {
        "commit_hash": "676e16002685698197212e756e691ddcdf527d38",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/676e16002685698197212e756e691ddcdf527d38",
        "files": [
          "scripts/ci/images/ci_wait_for_and_verify_ci_image.sh"
        ],
        "message": "Update md5 information about image after waiting (#21000)\n\nWhen \"wait_for_image\" was called, the information that the image\nwas built (including the information about md5 hashes of important\nfiles) had not been stored locally. It was only stored when\nimage was pulled by \"prepare_image\". Constraints job uses\nwait for image, because it runs in parallel for all python versions.\n\nWith recent changes, the flag that image had never been built,\nautomatically generates image build - unnecessarily, because the\nimage is already pulled by \"wait_for_image\".\n\nIt made almost no difference for \"regular builds\" because the image\nis rebuilt from cache (which is now very quick) but in case of\nPRs that change setup.py it caused image rebuild. Additionally if\nthis iamge has conflicting requirement, it would cause build failure.\n\nThe change simply registers the fact that image is \"ok\" so that\nno attempt to rebuild image happens when constraints are generated.\n\n(cherry picked from commit e5da9bf6c985593b9aa624fbb89ed052d871debf)",
        "before_after_code_files": [
          "scripts/ci/images/ci_wait_for_and_verify_ci_image.sh||scripts/ci/images/ci_wait_for_and_verify_ci_image.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/images/ci_wait_for_and_verify_ci_image.sh||scripts/ci/images/ci_wait_for_and_verify_ci_image.sh": [
          "File: scripts/ci/images/ci_wait_for_and_verify_ci_image.sh -> scripts/ci/images/ci_wait_for_and_verify_ci_image.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "38:     verify_image::verify_ci_image \"${image_name_with_tag}\"",
          "39: fi",
          "41: docker_v tag  \"${image_name_with_tag}\" \"${AIRFLOW_CI_IMAGE}\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "41: md5sum::update_all_md5_with_group",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "663bb546e782748bdd315483ca2070a77046997a",
      "candidate_info": {
        "commit_hash": "663bb546e782748bdd315483ca2070a77046997a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/663bb546e782748bdd315483ca2070a77046997a",
        "files": [
          "airflow/api/client/local_client.py",
          "airflow/api/common/delete_dag.py",
          "airflow/api/common/experimental/delete_dag.py",
          "airflow/api/common/experimental/get_code.py",
          "airflow/api/common/experimental/get_dag_run_state.py",
          "airflow/api/common/experimental/get_task.py",
          "airflow/api/common/experimental/get_task_instance.py",
          "airflow/api/common/experimental/pool.py",
          "airflow/api/common/experimental/trigger_dag.py",
          "airflow/api/common/trigger_dag.py",
          "airflow/api_connexion/endpoints/dag_endpoint.py",
          "airflow/models/pool.py",
          "airflow/operators/trigger_dagrun.py",
          "airflow/utils/db.py",
          "airflow/www/views.py",
          "setup.cfg",
          "tests/api/client/test_local_client.py",
          "tests/api/common/test_delete_dag.py",
          "tests/api/common/test_trigger_dag.py",
          "tests/models/test_pool.py"
        ],
        "message": "Deprecate some functions in the experimental API (#19931)\n\nThis PR seeks to deprecate some functions in the experimental API.\nSome of the deprecated functions are only used in the experimental REST API,\nothers that are valid are being moved out of the experimental package.\n\n(cherry picked from commit 6239ae91a4c8bfb05f053a61cb8386f2d63b8b3a)",
        "before_after_code_files": [
          "airflow/api/client/local_client.py||airflow/api/client/local_client.py",
          "airflow/api/common/delete_dag.py||airflow/api/common/delete_dag.py",
          "airflow/api/common/experimental/delete_dag.py||airflow/api/common/experimental/delete_dag.py",
          "airflow/api/common/experimental/get_code.py||airflow/api/common/experimental/get_code.py",
          "airflow/api/common/experimental/get_dag_run_state.py||airflow/api/common/experimental/get_dag_run_state.py",
          "airflow/api/common/experimental/get_task.py||airflow/api/common/experimental/get_task.py",
          "airflow/api/common/experimental/get_task_instance.py||airflow/api/common/experimental/get_task_instance.py",
          "airflow/api/common/experimental/pool.py||airflow/api/common/experimental/pool.py",
          "airflow/api/common/experimental/trigger_dag.py||airflow/api/common/experimental/trigger_dag.py",
          "airflow/api/common/trigger_dag.py||airflow/api/common/trigger_dag.py",
          "airflow/api_connexion/endpoints/dag_endpoint.py||airflow/api_connexion/endpoints/dag_endpoint.py",
          "airflow/models/pool.py||airflow/models/pool.py",
          "airflow/operators/trigger_dagrun.py||airflow/operators/trigger_dagrun.py",
          "airflow/utils/db.py||airflow/utils/db.py",
          "airflow/www/views.py||airflow/www/views.py",
          "setup.cfg||setup.cfg",
          "tests/api/client/test_local_client.py||tests/api/client/test_local_client.py",
          "tests/api/common/experimental/test_delete_dag.py||tests/api/common/test_delete_dag.py",
          "tests/api/common/experimental/test_trigger_dag.py||tests/api/common/test_trigger_dag.py",
          "tests/models/test_pool.py||tests/models/test_pool.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api/client/local_client.py||airflow/api/client/local_client.py": [
          "File: airflow/api/client/local_client.py -> airflow/api/client/local_client.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: \"\"\"Local client API\"\"\"",
          "20: from airflow.api.client import api_client",
          "22: from airflow.api.common.experimental.get_lineage import get_lineage as get_lineage_api",
          "25: class Client(api_client.Client):",
          "",
          "[Removed Lines]",
          "21: from airflow.api.common.experimental import delete_dag, pool, trigger_dag",
          "",
          "[Added Lines]",
          "21: from airflow.api.common import delete_dag, trigger_dag",
          "23: from airflow.exceptions import AirflowBadRequest, PoolNotFound",
          "24: from airflow.models.pool import Pool",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "36:         return f\"Removed {count} record(s)\"",
          "38:     def get_pool(self, name):",
          "42:     def get_pools(self):",
          "45:     def create_pool(self, name, slots, description):",
          "49:     def delete_pool(self, name):",
          "53:     def get_lineage(self, dag_id, execution_date):",
          "54:         lineage = get_lineage_api(dag_id=dag_id, execution_date=execution_date)",
          "",
          "[Removed Lines]",
          "39:         the_pool = pool.get_pool(name=name)",
          "40:         return the_pool.pool, the_pool.slots, the_pool.description",
          "43:         return [(p.pool, p.slots, p.description) for p in pool.get_pools()]",
          "46:         the_pool = pool.create_pool(name=name, slots=slots, description=description)",
          "47:         return the_pool.pool, the_pool.slots, the_pool.description",
          "50:         the_pool = pool.delete_pool(name=name)",
          "51:         return the_pool.pool, the_pool.slots, the_pool.description",
          "",
          "[Added Lines]",
          "41:         pool = Pool.get_pool(pool_name=name)",
          "42:         if not pool:",
          "43:             raise PoolNotFound(f\"Pool {name} not found\")",
          "44:         return pool.pool, pool.slots, pool.description",
          "47:         return [(p.pool, p.slots, p.description) for p in Pool.get_pools()]",
          "50:         if not (name and name.strip()):",
          "51:             raise AirflowBadRequest(\"Pool name shouldn't be empty\")",
          "52:         pool_name_length = Pool.pool.property.columns[0].type.length",
          "53:         if len(name) > pool_name_length:",
          "54:             raise AirflowBadRequest(f\"pool name cannot be more than {pool_name_length} characters\")",
          "55:         try:",
          "56:             slots = int(slots)",
          "57:         except ValueError:",
          "58:             raise AirflowBadRequest(f\"Bad value for `slots`: {slots}\")",
          "59:         pool = Pool.create_or_update_pool(name=name, slots=slots, description=description)",
          "60:         return pool.pool, pool.slots, pool.description",
          "63:         pool = Pool.delete_pool(name=name)",
          "64:         return pool.pool, pool.slots, pool.description",
          "",
          "---------------"
        ],
        "airflow/api/common/delete_dag.py||airflow/api/common/delete_dag.py": [
          "File: airflow/api/common/delete_dag.py -> airflow/api/common/delete_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: \"\"\"Delete DAGs APIs.\"\"\"",
          "19: import logging",
          "21: from sqlalchemy import or_",
          "23: from airflow import models",
          "24: from airflow.exceptions import AirflowException, DagNotFound",
          "25: from airflow.models import DagModel, TaskFail",
          "26: from airflow.models.serialized_dag import SerializedDagModel",
          "27: from airflow.utils.db import get_sqla_model_classes",
          "28: from airflow.utils.session import provide_session",
          "29: from airflow.utils.state import State",
          "31: log = logging.getLogger(__name__)",
          "34: @provide_session",
          "35: def delete_dag(dag_id: str, keep_records_in_log: bool = True, session=None) -> int:",
          "36:     \"\"\"",
          "37:     :param dag_id: the dag_id of the DAG to delete",
          "38:     :param keep_records_in_log: whether keep records of the given dag_id",
          "39:         in the Log table in the backend database (for reasons like auditing).",
          "40:         The default value is True.",
          "41:     :param session: session used",
          "42:     :return count of deleted dags",
          "43:     \"\"\"",
          "44:     log.info(\"Deleting DAG: %s\", dag_id)",
          "45:     running_tis = (",
          "46:         session.query(models.TaskInstance.state)",
          "47:         .filter(models.TaskInstance.dag_id == dag_id)",
          "48:         .filter(models.TaskInstance.state == State.RUNNING)",
          "49:         .first()",
          "50:     )",
          "51:     if running_tis:",
          "52:         raise AirflowException(\"TaskInstances still running\")",
          "53:     dag = session.query(DagModel).filter(DagModel.dag_id == dag_id).first()",
          "54:     if dag is None:",
          "55:         raise DagNotFound(f\"Dag id {dag_id} not found\")",
          "57:     # Scheduler removes DAGs without files from serialized_dag table every dag_dir_list_interval.",
          "58:     # There may be a lag, so explicitly removes serialized DAG here.",
          "59:     if SerializedDagModel.has_dag(dag_id=dag_id, session=session):",
          "60:         SerializedDagModel.remove_dag(dag_id=dag_id, session=session)",
          "62:     count = 0",
          "64:     for model in get_sqla_model_classes():",
          "65:         if hasattr(model, \"dag_id\"):",
          "66:             if keep_records_in_log and model.__name__ == 'Log':",
          "67:                 continue",
          "68:             cond = or_(model.dag_id == dag_id, model.dag_id.like(dag_id + \".%\"))",
          "69:             count += session.query(model).filter(cond).delete(synchronize_session='fetch')",
          "70:     if dag.is_subdag:",
          "71:         parent_dag_id, task_id = dag_id.rsplit(\".\", 1)",
          "72:         for model in TaskFail, models.TaskInstance:",
          "73:             count += (",
          "74:                 session.query(model).filter(model.dag_id == parent_dag_id, model.task_id == task_id).delete()",
          "75:             )",
          "77:     # Delete entries in Import Errors table for a deleted DAG",
          "78:     # This handles the case when the dag_id is changed in the file",
          "79:     session.query(models.ImportError).filter(models.ImportError.filename == dag.fileloc).delete(",
          "80:         synchronize_session='fetch'",
          "81:     )",
          "83:     return count",
          "",
          "---------------"
        ],
        "airflow/api/common/experimental/delete_dag.py||airflow/api/common/experimental/delete_dag.py": [
          "File: airflow/api/common/experimental/delete_dag.py -> airflow/api/common/experimental/delete_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "",
          "[Removed Lines]",
          "18: \"\"\"Delete DAGs APIs.\"\"\"",
          "19: import logging",
          "21: from sqlalchemy import or_",
          "23: from airflow import models",
          "24: from airflow.exceptions import AirflowException, DagNotFound",
          "25: from airflow.models import DagModel, TaskFail",
          "26: from airflow.models.serialized_dag import SerializedDagModel",
          "27: from airflow.utils.session import provide_session",
          "28: from airflow.utils.state import State",
          "30: log = logging.getLogger(__name__)",
          "33: @provide_session",
          "34: def delete_dag(dag_id: str, keep_records_in_log: bool = True, session=None) -> int:",
          "35:     \"\"\"",
          "36:     :param dag_id: the dag_id of the DAG to delete",
          "37:     :param keep_records_in_log: whether keep records of the given dag_id",
          "38:         in the Log table in the backend database (for reasons like auditing).",
          "39:         The default value is True.",
          "40:     :param session: session used",
          "41:     :return count of deleted dags",
          "42:     \"\"\"",
          "43:     log.info(\"Deleting DAG: %s\", dag_id)",
          "44:     running_tis = (",
          "45:         session.query(models.TaskInstance.state)",
          "46:         .filter(models.TaskInstance.dag_id == dag_id)",
          "47:         .filter(models.TaskInstance.state == State.RUNNING)",
          "48:         .first()",
          "49:     )",
          "50:     if running_tis:",
          "51:         raise AirflowException(\"TaskInstances still running\")",
          "52:     dag = session.query(DagModel).filter(DagModel.dag_id == dag_id).first()",
          "53:     if dag is None:",
          "54:         raise DagNotFound(f\"Dag id {dag_id} not found\")",
          "56:     # Scheduler removes DAGs without files from serialized_dag table every dag_dir_list_interval.",
          "57:     # There may be a lag, so explicitly removes serialized DAG here.",
          "58:     if SerializedDagModel.has_dag(dag_id=dag_id, session=session):",
          "59:         SerializedDagModel.remove_dag(dag_id=dag_id, session=session)",
          "61:     count = 0",
          "63:     for model in models.base.Base._decl_class_registry.values():",
          "64:         if hasattr(model, \"dag_id\"):",
          "65:             if keep_records_in_log and model.__name__ == 'Log':",
          "66:                 continue",
          "67:             cond = or_(model.dag_id == dag_id, model.dag_id.like(dag_id + \".%\"))",
          "68:             count += session.query(model).filter(cond).delete(synchronize_session='fetch')",
          "69:     if dag.is_subdag:",
          "70:         parent_dag_id, task_id = dag_id.rsplit(\".\", 1)",
          "71:         for model in TaskFail, models.TaskInstance:",
          "72:             count += (",
          "73:                 session.query(model).filter(model.dag_id == parent_dag_id, model.task_id == task_id).delete()",
          "74:             )",
          "76:     # Delete entries in Import Errors table for a deleted DAG",
          "77:     # This handles the case when the dag_id is changed in the file",
          "78:     session.query(models.ImportError).filter(models.ImportError.filename == dag.fileloc).delete(",
          "79:         synchronize_session='fetch'",
          "80:     )",
          "82:     return count",
          "",
          "[Added Lines]",
          "18: import warnings",
          "20: from airflow.api.common.delete_dag import *  # noqa",
          "22: warnings.warn(",
          "23:     \"This module is deprecated. Please use `airflow.api.common.delete_dag` instead.\",",
          "24:     DeprecationWarning,",
          "25:     stacklevel=2,",
          "26: )",
          "",
          "---------------"
        ],
        "airflow/api/common/experimental/get_code.py||airflow/api/common/experimental/get_code.py": [
          "File: airflow/api/common/experimental/get_code.py -> airflow/api/common/experimental/get_code.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: \"\"\"Get code APIs.\"\"\"",
          "19: from airflow.api.common.experimental import check_and_get_dag",
          "20: from airflow.exceptions import AirflowException, DagCodeNotFound",
          "21: from airflow.models.dagcode import DagCode",
          "24: def get_code(dag_id: str) -> str:",
          "25:     \"\"\"Return python code of a given dag_id.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: from deprecated import deprecated",
          "26: @deprecated(reason=\"Use DagCode().get_code_by_fileloc() instead\", version=\"2.2.3\")",
          "",
          "---------------"
        ],
        "airflow/api/common/experimental/get_dag_run_state.py||airflow/api/common/experimental/get_dag_run_state.py": [
          "File: airflow/api/common/experimental/get_dag_run_state.py -> airflow/api/common/experimental/get_dag_run_state.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: from datetime import datetime",
          "20: from typing import Dict",
          "22: from airflow.api.common.experimental import check_and_get_dag, check_and_get_dagrun",
          "25: def get_dag_run_state(dag_id: str, execution_date: datetime) -> Dict[str, str]:",
          "26:     \"\"\"Return the Dag Run state identified by the given dag_id and execution_date.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: from deprecated import deprecated",
          "27: @deprecated(reason=\"Use DagRun().get_state() instead\", version=\"2.2.3\")",
          "",
          "---------------"
        ],
        "airflow/api/common/experimental/get_task.py||airflow/api/common/experimental/get_task.py": [
          "File: airflow/api/common/experimental/get_task.py -> airflow/api/common/experimental/get_task.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: \"\"\"Task APIs..\"\"\"",
          "19: from airflow.api.common.experimental import check_and_get_dag",
          "20: from airflow.models import TaskInstance",
          "23: def get_task(dag_id: str, task_id: str) -> TaskInstance:",
          "24:     \"\"\"Return the task object identified by the given dag_id and task_id.\"\"\"",
          "25:     dag = check_and_get_dag(dag_id, task_id)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: from deprecated import deprecated",
          "25: @deprecated(reason=\"Use DAG().get_task\", version=\"2.2.3\")",
          "",
          "---------------"
        ],
        "airflow/api/common/experimental/get_task_instance.py||airflow/api/common/experimental/get_task_instance.py": [
          "File: airflow/api/common/experimental/get_task_instance.py -> airflow/api/common/experimental/get_task_instance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: \"\"\"Task Instance APIs.\"\"\"",
          "19: from datetime import datetime",
          "21: from airflow.api.common.experimental import check_and_get_dag, check_and_get_dagrun",
          "22: from airflow.exceptions import TaskInstanceNotFound",
          "23: from airflow.models import TaskInstance",
          "26: def get_task_instance(dag_id: str, task_id: str, execution_date: datetime) -> TaskInstance:",
          "27:     \"\"\"Return the task instance identified by the given dag_id, task_id and execution_date.\"\"\"",
          "28:     dag = check_and_get_dag(dag_id, task_id)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: from deprecated import deprecated",
          "28: @deprecated(version=\"2.2.3\", reason=\"Use DagRun.get_task_instance instead\")",
          "",
          "---------------"
        ],
        "airflow/api/common/experimental/pool.py||airflow/api/common/experimental/pool.py": [
          "File: airflow/api/common/experimental/pool.py -> airflow/api/common/experimental/pool.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: \"\"\"Pool APIs.\"\"\"",
          "19: from airflow.exceptions import AirflowBadRequest, PoolNotFound",
          "20: from airflow.models import Pool",
          "21: from airflow.utils.session import provide_session",
          "24: @provide_session",
          "25: def get_pool(name, session=None):",
          "26:     \"\"\"Get pool by a given name.\"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: from deprecated import deprecated",
          "26: @deprecated(reason=\"Use Pool.get_pool() instead\", version=\"2.2.3\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "34:     return pool",
          "37: @provide_session",
          "38: def get_pools(session=None):",
          "39:     \"\"\"Get all pools.\"\"\"",
          "40:     return session.query(Pool).all()",
          "43: @provide_session",
          "44: def create_pool(name, slots, description, session=None):",
          "45:     \"\"\"Create a pool with a given parameters.\"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "40: @deprecated(reason=\"Use Pool.get_pools() instead\", version=\"2.2.3\")",
          "47: @deprecated(reason=\"Use Pool.create_pool() instead\", version=\"2.2.3\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "70:     return pool",
          "73: @provide_session",
          "74: def delete_pool(name, session=None):",
          "75:     \"\"\"Delete pool by a given name.\"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "78: @deprecated(reason=\"Use Pool.delete_pool() instead\", version=\"2.2.3\")",
          "",
          "---------------"
        ],
        "airflow/api/common/experimental/trigger_dag.py||airflow/api/common/experimental/trigger_dag.py": [
          "File: airflow/api/common/experimental/trigger_dag.py -> airflow/api/common/experimental/trigger_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "",
          "[Removed Lines]",
          "18: \"\"\"Triggering DAG runs APIs.\"\"\"",
          "19: import json",
          "20: from datetime import datetime",
          "21: from typing import List, Optional, Union",
          "23: from airflow.exceptions import DagNotFound, DagRunAlreadyExists",
          "24: from airflow.models import DagBag, DagModel, DagRun",
          "25: from airflow.utils import timezone",
          "26: from airflow.utils.state import State",
          "27: from airflow.utils.types import DagRunType",
          "30: def _trigger_dag(",
          "31:     dag_id: str,",
          "32:     dag_bag: DagBag,",
          "33:     run_id: Optional[str] = None,",
          "34:     conf: Optional[Union[dict, str]] = None,",
          "35:     execution_date: Optional[datetime] = None,",
          "36:     replace_microseconds: bool = True,",
          "37: ) -> List[DagRun]:",
          "38:     \"\"\"Triggers DAG run.",
          "40:     :param dag_id: DAG ID",
          "41:     :param dag_bag: DAG Bag model",
          "42:     :param run_id: ID of the dag_run",
          "43:     :param conf: configuration",
          "44:     :param execution_date: date of execution",
          "45:     :param replace_microseconds: whether microseconds should be zeroed",
          "46:     :return: list of triggered dags",
          "47:     \"\"\"",
          "48:     dag = dag_bag.get_dag(dag_id)  # prefetch dag if it is stored serialized",
          "50:     if dag_id not in dag_bag.dags:",
          "51:         raise DagNotFound(f\"Dag id {dag_id} not found\")",
          "53:     execution_date = execution_date if execution_date else timezone.utcnow()",
          "55:     if not timezone.is_localized(execution_date):",
          "56:         raise ValueError(\"The execution_date should be localized\")",
          "58:     if replace_microseconds:",
          "59:         execution_date = execution_date.replace(microsecond=0)",
          "61:     if dag.default_args and 'start_date' in dag.default_args:",
          "62:         min_dag_start_date = dag.default_args[\"start_date\"]",
          "63:         if min_dag_start_date and execution_date < min_dag_start_date:",
          "64:             raise ValueError(",
          "65:                 \"The execution_date [{}] should be >= start_date [{}] from DAG's default_args\".format(",
          "66:                     execution_date.isoformat(), min_dag_start_date.isoformat()",
          "67:                 )",
          "68:             )",
          "70:     run_id = run_id or DagRun.generate_run_id(DagRunType.MANUAL, execution_date)",
          "71:     dag_run = DagRun.find_duplicate(dag_id=dag_id, execution_date=execution_date, run_id=run_id)",
          "73:     if dag_run:",
          "74:         raise DagRunAlreadyExists(",
          "75:             f\"A Dag Run already exists for dag id {dag_id} at {execution_date} with run id {run_id}\"",
          "76:         )",
          "78:     run_conf = None",
          "79:     if conf:",
          "80:         run_conf = conf if isinstance(conf, dict) else json.loads(conf)",
          "82:     dag_runs = []",
          "83:     dags_to_run = [dag] + dag.subdags",
          "84:     for _dag in dags_to_run:",
          "85:         dag_run = _dag.create_dagrun(",
          "86:             run_id=run_id,",
          "87:             execution_date=execution_date,",
          "88:             state=State.QUEUED,",
          "89:             conf=run_conf,",
          "90:             external_trigger=True,",
          "91:             dag_hash=dag_bag.dags_hash.get(dag_id),",
          "92:         )",
          "93:         dag_runs.append(dag_run)",
          "95:     return dag_runs",
          "98: def trigger_dag(",
          "99:     dag_id: str,",
          "100:     run_id: Optional[str] = None,",
          "101:     conf: Optional[Union[dict, str]] = None,",
          "102:     execution_date: Optional[datetime] = None,",
          "103:     replace_microseconds: bool = True,",
          "104: ) -> Optional[DagRun]:",
          "105:     \"\"\"Triggers execution of DAG specified by dag_id",
          "107:     :param dag_id: DAG ID",
          "108:     :param run_id: ID of the dag_run",
          "109:     :param conf: configuration",
          "110:     :param execution_date: date of execution",
          "111:     :param replace_microseconds: whether microseconds should be zeroed",
          "112:     :return: first dag run triggered - even if more than one Dag Runs were triggered or None",
          "113:     \"\"\"",
          "114:     dag_model = DagModel.get_current(dag_id)",
          "115:     if dag_model is None:",
          "116:         raise DagNotFound(f\"Dag id {dag_id} not found in DagModel\")",
          "118:     dagbag = DagBag(dag_folder=dag_model.fileloc, read_dags_from_db=True)",
          "119:     triggers = _trigger_dag(",
          "120:         dag_id=dag_id,",
          "121:         dag_bag=dagbag,",
          "122:         run_id=run_id,",
          "123:         conf=conf,",
          "124:         execution_date=execution_date,",
          "125:         replace_microseconds=replace_microseconds,",
          "126:     )",
          "128:     return triggers[0] if triggers else None",
          "",
          "[Added Lines]",
          "19: import warnings",
          "21: from airflow.api.common.trigger_dag import *  # noqa",
          "23: warnings.warn(",
          "24:     \"This module is deprecated. Please use `airflow.api.common.trigger_dag` instead.\",",
          "25:     DeprecationWarning,",
          "26:     stacklevel=2,",
          "27: )",
          "",
          "---------------"
        ],
        "airflow/api/common/trigger_dag.py||airflow/api/common/trigger_dag.py": [
          "File: airflow/api/common/trigger_dag.py -> airflow/api/common/trigger_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: \"\"\"Triggering DAG runs APIs.\"\"\"",
          "19: import json",
          "20: from datetime import datetime",
          "21: from typing import List, Optional, Union",
          "23: from airflow.exceptions import DagNotFound, DagRunAlreadyExists",
          "24: from airflow.models import DagBag, DagModel, DagRun",
          "25: from airflow.utils import timezone",
          "26: from airflow.utils.state import State",
          "27: from airflow.utils.types import DagRunType",
          "30: def _trigger_dag(",
          "31:     dag_id: str,",
          "32:     dag_bag: DagBag,",
          "33:     run_id: Optional[str] = None,",
          "34:     conf: Optional[Union[dict, str]] = None,",
          "35:     execution_date: Optional[datetime] = None,",
          "36:     replace_microseconds: bool = True,",
          "37: ) -> List[DagRun]:",
          "38:     \"\"\"Triggers DAG run.",
          "40:     :param dag_id: DAG ID",
          "41:     :param dag_bag: DAG Bag model",
          "42:     :param run_id: ID of the dag_run",
          "43:     :param conf: configuration",
          "44:     :param execution_date: date of execution",
          "45:     :param replace_microseconds: whether microseconds should be zeroed",
          "46:     :return: list of triggered dags",
          "47:     \"\"\"",
          "48:     dag = dag_bag.get_dag(dag_id)  # prefetch dag if it is stored serialized",
          "50:     if dag_id not in dag_bag.dags:",
          "51:         raise DagNotFound(f\"Dag id {dag_id} not found\")",
          "53:     execution_date = execution_date if execution_date else timezone.utcnow()",
          "55:     if not timezone.is_localized(execution_date):",
          "56:         raise ValueError(\"The execution_date should be localized\")",
          "58:     if replace_microseconds:",
          "59:         execution_date = execution_date.replace(microsecond=0)",
          "61:     if dag.default_args and 'start_date' in dag.default_args:",
          "62:         min_dag_start_date = dag.default_args[\"start_date\"]",
          "63:         if min_dag_start_date and execution_date < min_dag_start_date:",
          "64:             raise ValueError(",
          "65:                 f\"The execution_date [{execution_date.isoformat()}] should be >= start_date \"",
          "66:                 f\"[{min_dag_start_date.isoformat()}] from DAG's default_args\"",
          "67:             )",
          "69:     run_id = run_id or DagRun.generate_run_id(DagRunType.MANUAL, execution_date)",
          "70:     dag_run = DagRun.find_duplicate(dag_id=dag_id, execution_date=execution_date, run_id=run_id)",
          "72:     if dag_run:",
          "73:         raise DagRunAlreadyExists(",
          "74:             f\"A Dag Run already exists for dag id {dag_id} at {execution_date} with run id {run_id}\"",
          "75:         )",
          "77:     run_conf = None",
          "78:     if conf:",
          "79:         run_conf = conf if isinstance(conf, dict) else json.loads(conf)",
          "81:     dag_runs = []",
          "82:     dags_to_run = [dag] + dag.subdags",
          "83:     for _dag in dags_to_run:",
          "84:         dag_run = _dag.create_dagrun(",
          "85:             run_id=run_id,",
          "86:             execution_date=execution_date,",
          "87:             state=State.QUEUED,",
          "88:             conf=run_conf,",
          "89:             external_trigger=True,",
          "90:             dag_hash=dag_bag.dags_hash.get(dag_id),",
          "91:         )",
          "92:         dag_runs.append(dag_run)",
          "94:     return dag_runs",
          "97: def trigger_dag(",
          "98:     dag_id: str,",
          "99:     run_id: Optional[str] = None,",
          "100:     conf: Optional[Union[dict, str]] = None,",
          "101:     execution_date: Optional[datetime] = None,",
          "102:     replace_microseconds: bool = True,",
          "103: ) -> Optional[DagRun]:",
          "104:     \"\"\"Triggers execution of DAG specified by dag_id",
          "106:     :param dag_id: DAG ID",
          "107:     :param run_id: ID of the dag_run",
          "108:     :param conf: configuration",
          "109:     :param execution_date: date of execution",
          "110:     :param replace_microseconds: whether microseconds should be zeroed",
          "111:     :return: first dag run triggered - even if more than one Dag Runs were triggered or None",
          "112:     \"\"\"",
          "113:     dag_model = DagModel.get_current(dag_id)",
          "114:     if dag_model is None:",
          "115:         raise DagNotFound(f\"Dag id {dag_id} not found in DagModel\")",
          "117:     dagbag = DagBag(dag_folder=dag_model.fileloc, read_dags_from_db=True)",
          "118:     triggers = _trigger_dag(",
          "119:         dag_id=dag_id,",
          "120:         dag_bag=dagbag,",
          "121:         run_id=run_id,",
          "122:         conf=conf,",
          "123:         execution_date=execution_date,",
          "124:         replace_microseconds=replace_microseconds,",
          "125:     )",
          "127:     return triggers[0] if triggers else None",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/dag_endpoint.py||airflow/api_connexion/endpoints/dag_endpoint.py": [
          "File: airflow/api_connexion/endpoints/dag_endpoint.py -> airflow/api_connexion/endpoints/dag_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "110: @provide_session",
          "111: def delete_dag(dag_id: str, session: Session):",
          "112:     \"\"\"Delete the specific DAG.\"\"\"",
          "118:     try:",
          "120:     except DagNotFound:",
          "121:         raise NotFound(f\"Dag with id: '{dag_id}' not found\")",
          "122:     except AirflowException:",
          "",
          "[Removed Lines]",
          "113:     # TODO: This function is shared with the /delete endpoint used by the web",
          "114:     # UI, so we're reusing it to simplify maintenance. Refactor the function to",
          "115:     # another place when the experimental/legacy API is removed.",
          "116:     from airflow.api.common.experimental import delete_dag",
          "119:         delete_dag.delete_dag(dag_id, session=session)",
          "",
          "[Added Lines]",
          "113:     from airflow.api.common import delete_dag as delete_dag_module",
          "116:         delete_dag_module.delete_dag(dag_id, session=session)",
          "",
          "---------------"
        ],
        "airflow/models/pool.py||airflow/models/pool.py": [
          "File: airflow/models/pool.py -> airflow/models/pool.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: from sqlalchemy import Column, Integer, String, Text, func",
          "22: from sqlalchemy.orm.session import Session",
          "25: from airflow.models.base import Base",
          "26: from airflow.ti_deps.dependencies_states import EXECUTION_STATES",
          "27: from airflow.typing_compat import TypedDict",
          "29: from airflow.utils.sqlalchemy import nowait, with_row_locks",
          "30: from airflow.utils.state import State",
          "",
          "[Removed Lines]",
          "24: from airflow.exceptions import AirflowException",
          "28: from airflow.utils.session import provide_session",
          "",
          "[Added Lines]",
          "24: from airflow.exceptions import AirflowException, PoolNotFound",
          "28: from airflow.utils.session import NEW_SESSION, provide_session",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "58:     @staticmethod",
          "59:     @provide_session",
          "61:         \"\"\"",
          "62:         Get the Pool with specific pool name from the Pools.",
          "",
          "[Removed Lines]",
          "60:     def get_pool(pool_name, session: Session = None):",
          "",
          "[Added Lines]",
          "60:     def get_pools(session: Session = NEW_SESSION):",
          "61:         \"\"\"Get all pools.\"\"\"",
          "62:         return session.query(Pool).all()",
          "64:     @staticmethod",
          "65:     @provide_session",
          "66:     def get_pool(pool_name: str, session: Session = NEW_SESSION):",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "70:     @staticmethod",
          "71:     @provide_session",
          "73:         \"\"\"",
          "74:         Get the Pool of the default_pool from the Pools.",
          "",
          "[Removed Lines]",
          "72:     def get_default_pool(session: Session = None):",
          "",
          "[Added Lines]",
          "78:     def get_default_pool(session: Session = NEW_SESSION):",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "78:         \"\"\"",
          "79:         return Pool.get_pool(Pool.DEFAULT_POOL_NAME, session=session)",
          "81:     @staticmethod",
          "82:     @provide_session",
          "83:     def slots_stats(",
          "85:         lock_rows: bool = False,",
          "87:     ) -> Dict[str, PoolStats]:",
          "88:         \"\"\"",
          "89:         Get Pool stats (Number of Running, Queued, Open & Total tasks)",
          "",
          "[Removed Lines]",
          "86:         session: Session = None,",
          "",
          "[Added Lines]",
          "87:     @staticmethod",
          "88:     @provide_session",
          "89:     def create_or_update_pool(name: str, slots: int, description: str, session: Session = NEW_SESSION):",
          "90:         \"\"\"Create a pool with given parameters or update it if it already exists.\"\"\"",
          "91:         if not name:",
          "92:             return",
          "93:         pool = session.query(Pool).filter_by(pool=name).first()",
          "94:         if pool is None:",
          "95:             pool = Pool(pool=name, slots=slots, description=description)",
          "96:             session.add(pool)",
          "97:         else:",
          "98:             pool.slots = slots",
          "99:             pool.description = description",
          "101:         session.commit()",
          "103:         return pool",
          "105:     @staticmethod",
          "106:     @provide_session",
          "107:     def delete_pool(name: str, session: Session = NEW_SESSION):",
          "108:         \"\"\"Delete pool by a given name.\"\"\"",
          "109:         if name == Pool.DEFAULT_POOL_NAME:",
          "110:             raise AirflowException(\"default_pool cannot be deleted\")",
          "112:         pool = session.query(Pool).filter_by(pool=name).first()",
          "113:         if pool is None:",
          "114:             raise PoolNotFound(f\"Pool '{name}' doesn't exist\")",
          "116:         session.delete(pool)",
          "117:         session.commit()",
          "119:         return pool",
          "126:         session: Session = NEW_SESSION,",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "210:         )",
          "212:     @provide_session",
          "214:         \"\"\"",
          "215:         Get the number of slots open at the moment.",
          "",
          "[Removed Lines]",
          "213:     def open_slots(self, session: Session) -> float:",
          "",
          "[Added Lines]",
          "253:     def open_slots(self, session: Session = NEW_SESSION) -> float:",
          "",
          "---------------"
        ],
        "airflow/operators/trigger_dagrun.py||airflow/operators/trigger_dagrun.py": [
          "File: airflow/operators/trigger_dagrun.py -> airflow/operators/trigger_dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import time",
          "22: from typing import Dict, List, Optional, Union",
          "25: from airflow.exceptions import AirflowException, DagNotFound, DagRunAlreadyExists",
          "26: from airflow.models import BaseOperator, BaseOperatorLink, DagBag, DagModel, DagRun",
          "27: from airflow.utils import timezone",
          "",
          "[Removed Lines]",
          "24: from airflow.api.common.experimental.trigger_dag import trigger_dag",
          "",
          "[Added Lines]",
          "24: from airflow.api.common.trigger_dag import trigger_dag",
          "",
          "---------------"
        ],
        "airflow/utils/db.py||airflow/utils/db.py": [
          "File: airflow/utils/db.py -> airflow/utils/db.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "991:     \"\"\"",
          "992:     session.execute('select 1 as is_alive;')",
          "993:     log.info(\"Connection successful.\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "996: def get_sqla_model_classes():",
          "997:     \"\"\"",
          "998:     Get all SQLAlchemy class mappers.",
          "1000:     SQLAlchemy < 1.4 does not support registry.mappers so we use",
          "1001:     try/except to handle it.",
          "1002:     \"\"\"",
          "1003:     from airflow.models.base import Base",
          "1005:     try:",
          "1006:         return [mapper.class_ for mapper in Base.registry.mappers]",
          "1007:     except AttributeError:",
          "1008:         return Base._decl_class_registry.values()",
          "",
          "---------------"
        ],
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1607:     @action_logging",
          "1608:     def delete(self):",
          "1609:         \"\"\"Deletes DAG.\"\"\"",
          "1611:         from airflow.exceptions import DagNotFound",
          "1613:         dag_id = request.values.get('dag_id')",
          "",
          "[Removed Lines]",
          "1610:         from airflow.api.common.experimental import delete_dag",
          "",
          "[Added Lines]",
          "1610:         from airflow.api.common import delete_dag",
          "",
          "---------------"
        ],
        "setup.cfg||setup.cfg": [
          "File: setup.cfg -> setup.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "95:     croniter>=0.3.17",
          "96:     cryptography>=0.9.3",
          "97:     dataclasses;python_version<\"3.7\"",
          "98:     dill>=0.2.2, <0.4",
          "99:     # Sphinx RTD theme 0.5.2. introduced limitation to docutils to account for some docutils markup",
          "100:     # change:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "98:     deprecated>=1.2.13",
          "",
          "---------------"
        ],
        "tests/api/client/test_local_client.py||tests/api/client/test_local_client.py": [
          "File: tests/api/client/test_local_client.py -> tests/api/client/test_local_client.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "19: import json",
          "20: import unittest",
          "21: from unittest.mock import ANY, patch",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import random",
          "21: import string",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "26: from airflow.api.client.local_client import Client",
          "27: from airflow.example_dags import example_bash_operator",
          "29: from airflow.models import DAG, DagBag, DagModel, DagRun, Pool",
          "30: from airflow.utils import timezone",
          "31: from airflow.utils.session import create_session",
          "",
          "[Removed Lines]",
          "28: from airflow.exceptions import AirflowException",
          "",
          "[Added Lines]",
          "30: from airflow.exceptions import AirflowBadRequest, AirflowException, PoolNotFound",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "133:         pool = self.client.get_pool(name='foo')",
          "134:         assert pool == ('foo', 1, '')",
          "136:     def test_get_pools(self):",
          "137:         self.client.create_pool(name='foo1', slots=1, description='')",
          "138:         self.client.create_pool(name='foo2', slots=2, description='')",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "138:     def test_get_pool_non_existing_raises(self):",
          "139:         with pytest.raises(PoolNotFound):",
          "140:             self.client.get_pool(name='foo')",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "145:         with create_session() as session:",
          "146:             assert session.query(Pool).count() == 2",
          "148:     def test_delete_pool(self):",
          "149:         self.client.create_pool(name='foo', slots=1, description='')",
          "150:         with create_session() as session:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "154:     def test_create_pool_bad_slots(self):",
          "155:         with pytest.raises(AirflowBadRequest, match=\"^Bad value for `slots`: foo$\"):",
          "156:             self.client.create_pool(",
          "157:                 name='foo',",
          "158:                 slots='foo',",
          "159:                 description='',",
          "160:             )",
          "162:     def test_create_pool_name_too_long(self):",
          "163:         long_name = ''.join(random.choices(string.ascii_lowercase, k=300))",
          "164:         pool_name_length = Pool.pool.property.columns[0].type.length",
          "165:         with pytest.raises(",
          "166:             AirflowBadRequest, match=f\"^pool name cannot be more than {pool_name_length} characters\"",
          "167:         ):",
          "168:             self.client.create_pool(",
          "169:                 name=long_name,",
          "170:                 slots=5,",
          "171:                 description='',",
          "172:             )",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "152:         self.client.delete_pool(name='foo')",
          "153:         with create_session() as session:",
          "154:             assert session.query(Pool).count() == 1",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "181:         for name in ('', '    '):",
          "182:             with pytest.raises(PoolNotFound, match=f\"^Pool {name!r} doesn't exist$\"):",
          "183:                 Pool.delete_pool(name=name)",
          "",
          "---------------"
        ],
        "tests/api/common/experimental/test_delete_dag.py||tests/api/common/test_delete_dag.py": [
          "File: tests/api/common/experimental/test_delete_dag.py -> tests/api/common/test_delete_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import pytest",
          "22: from airflow import models",
          "24: from airflow.exceptions import AirflowException, DagNotFound",
          "25: from airflow.operators.dummy import DummyOperator",
          "26: from airflow.utils.dates import days_ago",
          "",
          "[Removed Lines]",
          "23: from airflow.api.common.experimental.delete_dag import delete_dag",
          "",
          "[Added Lines]",
          "23: from airflow.api.common.delete_dag import delete_dag",
          "",
          "---------------"
        ],
        "tests/api/common/experimental/test_trigger_dag.py||tests/api/common/test_trigger_dag.py": [
          "File: tests/api/common/experimental/test_trigger_dag.py -> tests/api/common/test_trigger_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import pytest",
          "23: from parameterized import parameterized",
          "26: from airflow.exceptions import AirflowException",
          "27: from airflow.models import DAG, DagRun",
          "28: from airflow.utils import timezone",
          "",
          "[Removed Lines]",
          "25: from airflow.api.common.experimental.trigger_dag import _trigger_dag",
          "",
          "[Added Lines]",
          "25: from airflow.api.common.trigger_dag import _trigger_dag",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "42:         with pytest.raises(AirflowException):",
          "43:             _trigger_dag('dag_not_found', dag_bag_mock)",
          "46:     @mock.patch('airflow.models.DagBag')",
          "47:     def test_trigger_dag_dag_run_exist(self, dag_bag_mock, dag_run_mock):",
          "48:         dag_id = \"dag_run_exist\"",
          "",
          "[Removed Lines]",
          "45:     @mock.patch('airflow.api.common.experimental.trigger_dag.DagRun', spec=DagRun)",
          "",
          "[Added Lines]",
          "45:     @mock.patch('airflow.api.common.trigger_dag.DagRun', spec=DagRun)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "54:             _trigger_dag(dag_id, dag_bag_mock)",
          "56:     @mock.patch('airflow.models.DAG')",
          "58:     @mock.patch('airflow.models.DagBag')",
          "59:     def test_trigger_dag_include_subdags(self, dag_bag_mock, dag_run_mock, dag_mock):",
          "60:         dag_id = \"trigger_dag\"",
          "",
          "[Removed Lines]",
          "57:     @mock.patch('airflow.api.common.experimental.trigger_dag.DagRun', spec=DagRun)",
          "",
          "[Added Lines]",
          "57:     @mock.patch('airflow.api.common.trigger_dag.DagRun', spec=DagRun)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "70:         assert 3 == len(triggers)",
          "72:     @mock.patch('airflow.models.DAG')",
          "74:     @mock.patch('airflow.models.DagBag')",
          "75:     def test_trigger_dag_include_nested_subdags(self, dag_bag_mock, dag_run_mock, dag_mock):",
          "76:         dag_id = \"trigger_dag\"",
          "",
          "[Removed Lines]",
          "73:     @mock.patch('airflow.api.common.experimental.trigger_dag.DagRun', spec=DagRun)",
          "",
          "[Added Lines]",
          "73:     @mock.patch('airflow.api.common.trigger_dag.DagRun', spec=DagRun)",
          "",
          "---------------"
        ],
        "tests/models/test_pool.py||tests/models/test_pool.py": [
          "File: tests/models/test_pool.py -> tests/models/test_pool.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "19: from airflow import settings",
          "20: from airflow.models.pool import Pool",
          "21: from airflow.models.taskinstance import TaskInstance as TI",
          "22: from airflow.operators.dummy import DummyOperator",
          "23: from airflow.utils import timezone",
          "24: from airflow.utils.state import State",
          "25: from tests.test_utils.db import clear_db_dags, clear_db_pools, clear_db_runs, set_default_pool_slots",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: import pytest",
          "22: from airflow.exceptions import AirflowException, PoolNotFound",
          "27: from airflow.utils.session import create_session",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "30: class TestPool:",
          "31:     @staticmethod",
          "32:     def clean_db():",
          "33:         clear_db_dags()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "36:     USER_POOL_COUNT = 2",
          "37:     TOTAL_POOL_COUNT = USER_POOL_COUNT + 1  # including default_pool",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "37:     def setup_method(self):",
          "38:         self.clean_db()",
          "40:     def teardown_method(self):",
          "41:         self.clean_db()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "47:         self.pools = []",
          "49:     def add_pools(self):",
          "50:         self.pools = [Pool.get_default_pool()]",
          "51:         for i in range(self.USER_POOL_COUNT):",
          "52:             name = f'experimental_{i + 1}'",
          "53:             pool = Pool(",
          "54:                 pool=name,",
          "55:                 slots=i,",
          "56:                 description=name,",
          "57:             )",
          "58:             self.pools.append(pool)",
          "59:         with create_session() as session:",
          "60:             session.add_all(self.pools)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "149:                 \"running\": 1,",
          "150:             }",
          "151:         } == Pool.slots_stats()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "175:     def test_get_pool(self):",
          "176:         self.add_pools()",
          "177:         pool = Pool.get_pool(pool_name=self.pools[0].pool)",
          "178:         assert pool.pool == self.pools[0].pool",
          "180:     def test_get_pool_non_existing(self):",
          "181:         self.add_pools()",
          "182:         assert not Pool.get_pool(pool_name='test')",
          "184:     def test_get_pool_bad_name(self):",
          "185:         for name in ('', '    '):",
          "186:             assert not Pool.get_pool(pool_name=name)",
          "188:     def test_get_pools(self):",
          "189:         self.add_pools()",
          "190:         pools = sorted(Pool.get_pools(), key=lambda p: p.pool)",
          "191:         assert pools[0].pool == self.pools[0].pool",
          "192:         assert pools[1].pool == self.pools[1].pool",
          "194:     def test_create_pool(self, session):",
          "195:         self.add_pools()",
          "196:         pool = Pool.create_or_update_pool(name='foo', slots=5, description='')",
          "197:         assert pool.pool == 'foo'",
          "198:         assert pool.slots == 5",
          "199:         assert pool.description == ''",
          "200:         assert session.query(Pool).count() == self.TOTAL_POOL_COUNT + 1",
          "202:     def test_create_pool_existing(self, session):",
          "203:         self.add_pools()",
          "204:         pool = Pool.create_or_update_pool(name=self.pools[0].pool, slots=5, description='')",
          "205:         assert pool.pool == self.pools[0].pool",
          "206:         assert pool.slots == 5",
          "207:         assert pool.description == ''",
          "208:         assert session.query(Pool).count() == self.TOTAL_POOL_COUNT",
          "210:     def test_delete_pool(self, session):",
          "211:         self.add_pools()",
          "212:         pool = Pool.delete_pool(name=self.pools[-1].pool)",
          "213:         assert pool.pool == self.pools[-1].pool",
          "214:         assert session.query(Pool).count() == self.TOTAL_POOL_COUNT - 1",
          "216:     def test_delete_pool_non_existing(self):",
          "217:         with pytest.raises(PoolNotFound, match=\"^Pool 'test' doesn't exist$\"):",
          "218:             Pool.delete_pool(name='test')",
          "220:     def test_delete_default_pool_not_allowed(self):",
          "221:         with pytest.raises(AirflowException, match=\"^default_pool cannot be deleted$\"):",
          "222:             Pool.delete_pool(Pool.DEFAULT_POOL_NAME)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "016929f782022791557d0ec1f316b338dc210566",
      "candidate_info": {
        "commit_hash": "016929f782022791557d0ec1f316b338dc210566",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/016929f782022791557d0ec1f316b338dc210566",
        "files": [
          "airflow/models/skipmixin.py",
          "airflow/models/xcom.py"
        ],
        "message": "Type-annotate SkipMixin and BaseXCom (#20011)\n\n(cherry picked from commit 6dd0a0df7e6a2f025e9234bdbf97b41e9b8f6257)",
        "before_after_code_files": [
          "airflow/models/skipmixin.py||airflow/models/skipmixin.py",
          "airflow/models/xcom.py||airflow/models/xcom.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/skipmixin.py||airflow/models/skipmixin.py": [
          "File: airflow/models/skipmixin.py -> airflow/models/skipmixin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "19: import warnings",
          "22: from airflow.models.taskinstance import TaskInstance",
          "23: from airflow.utils import timezone",
          "",
          "[Removed Lines]",
          "20: from typing import TYPE_CHECKING, Iterable, Union",
          "",
          "[Added Lines]",
          "20: from typing import TYPE_CHECKING, Iterable, Optional, Sequence, Union",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "26: from airflow.utils.state import State",
          "28: if TYPE_CHECKING:",
          "29:     from sqlalchemy import Session",
          "31:     from airflow.models import DagRun",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "29:     from pendulum import DateTime",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "66:     def skip(",
          "67:         self,",
          "68:         dag_run: \"DagRun\",",
          "72:     ):",
          "73:         \"\"\"",
          "74:         Sets tasks instances to skipped from the same dag run.",
          "",
          "[Removed Lines]",
          "69:         execution_date: \"timezone.DateTime\",",
          "70:         tasks: \"Iterable[BaseOperator]\",",
          "71:         session: \"Session\" = None,",
          "",
          "[Added Lines]",
          "70:         execution_date: \"DateTime\",",
          "71:         tasks: Sequence[\"BaseOperator\"],",
          "72:         session: \"Session\",",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "114:         session.commit()",
          "116:         # SkipMixin may not necessarily have a task_id attribute. Only store to XCom if one is available.",
          "122:         if task_id is not None:",
          "123:             from airflow.models.xcom import XCom",
          "",
          "[Removed Lines]",
          "117:         try:",
          "118:             task_id = self.task_id",
          "119:         except AttributeError:",
          "120:             task_id = None",
          "",
          "[Added Lines]",
          "118:         task_id: Optional[str] = getattr(self, \"task_id\", None)",
          "",
          "---------------"
        ],
        "airflow/models/xcom.py||airflow/models/xcom.py": [
          "File: airflow/models/xcom.py -> airflow/models/xcom.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "19: import json",
          "20: import logging",
          "21: import pickle",
          "24: import pendulum",
          "25: from sqlalchemy import Column, LargeBinary, String",
          "",
          "[Removed Lines]",
          "22: from typing import Any, Iterable, Optional, Union",
          "",
          "[Added Lines]",
          "19: import datetime",
          "23: from typing import TYPE_CHECKING, Any, Iterable, Optional, Type, Union, cast, overload",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "79:     def __repr__(self):",
          "80:         return f'<XCom \"{self.key}\" ({self.task_id} @ {self.execution_date})>'",
          "82:     @classmethod",
          "85:         \"\"\"",
          "90:         if not (execution_date is None) ^ (run_id is None):",
          "91:             raise ValueError(\"Exactly one of execution_date or run_id must be passed\")",
          "",
          "[Removed Lines]",
          "83:     @provide_session",
          "84:     def set(cls, key, value, task_id, dag_id, execution_date=None, run_id=None, session=None):",
          "86:         Store an XCom value.",
          "88:         :return: None",
          "89:         \"\"\"",
          "",
          "[Added Lines]",
          "83:     @overload",
          "85:     def set(",
          "86:         cls,",
          "87:         key: str,",
          "88:         value: Any,",
          "90:         dag_id: str,",
          "91:         task_id: str,",
          "92:         run_id: str,",
          "93:         session: Optional[Session] = None,",
          "94:     ) -> None:",
          "95:         \"\"\"Store an XCom value.",
          "97:         A deprecated form of this function accepts ``execution_date`` instead of",
          "98:         ``run_id``. The two arguments are mutually exclusive.",
          "100:         :param key: Key to store the XCom.",
          "101:         :param value: XCom value to store.",
          "102:         :param dag_id: DAG ID.",
          "103:         :param task_id: Task ID.",
          "104:         :param run_id: DAG run ID for the task.",
          "105:         :param session: Database session. If not given, a new session will be",
          "106:             created for this function.",
          "107:         :type session: sqlalchemy.orm.session.Session",
          "110:     @overload",
          "111:     @classmethod",
          "112:     def set(",
          "113:         cls,",
          "114:         key: str,",
          "115:         value: Any,",
          "116:         task_id: str,",
          "117:         dag_id: str,",
          "118:         execution_date: datetime.datetime,",
          "119:         session: Optional[Session] = None,",
          "120:     ) -> None:",
          "121:         \"\"\":sphinx-autoapi-skip:\"\"\"",
          "123:     @classmethod",
          "124:     @provide_session",
          "125:     def set(",
          "126:         cls,",
          "127:         key: str,",
          "128:         value: Any,",
          "129:         task_id: str,",
          "130:         dag_id: str,",
          "131:         execution_date: Optional[datetime.datetime] = None,",
          "132:         session: Session = None,",
          "134:         run_id: Optional[str] = None,",
          "135:     ) -> None:",
          "136:         \"\"\":sphinx-autoapi-skip:\"\"\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "94:             from airflow.models.dagrun import DagRun",
          "96:             dag_run = session.query(DagRun).filter_by(dag_id=dag_id, run_id=run_id).one()",
          "98:             execution_date = dag_run.execution_date",
          "103:         session.query(cls).filter(",
          "105:         ).delete()",
          "107:         session.flush()",
          "114:     @classmethod",
          "115:     @provide_session",
          "116:     def get_one(",
          "117:         cls,",
          "118:         execution_date: Optional[pendulum.DateTime] = None,",
          "120:         key: Optional[str] = None,",
          "121:         task_id: Optional[Union[str, Iterable[str]]] = None,",
          "122:         dag_id: Optional[Union[str, Iterable[str]]] = None,",
          "123:         include_prior_dates: bool = False,",
          "124:         session: Session = None,",
          "125:     ) -> Optional[Any]:",
          "155:         if not (execution_date is None) ^ (run_id is None):",
          "156:             raise ValueError(\"Exactly one of execution_date or run_id must be passed\")",
          "161:                 run_id=run_id,",
          "162:                 key=key,",
          "163:                 task_ids=task_id,",
          "",
          "[Removed Lines]",
          "100:         value = XCom.serialize_value(value)",
          "102:         # remove any duplicate XComs",
          "104:             cls.key == key, cls.execution_date == execution_date, cls.task_id == task_id, cls.dag_id == dag_id",
          "109:         # insert new XCom",
          "110:         session.add(XCom(key=key, value=value, execution_date=execution_date, task_id=task_id, dag_id=dag_id))",
          "112:         session.flush()",
          "119:         run_id: Optional[str] = None,",
          "126:         \"\"\"",
          "127:         Retrieve an XCom value, optionally meeting certain criteria. Returns None",
          "128:         of there are no results.",
          "130:         ``run_id`` and ``execution_date`` are mutually exclusive.",
          "132:         This method returns \"full\" XCom values (i.e. it uses ``deserialize_value`` from the XCom backend).",
          "133:         Please use :meth:`get_many` if you want the \"shortened\" value via ``orm_deserialize_value``",
          "135:         :param execution_date: Execution date for the task",
          "136:         :type execution_date: pendulum.datetime",
          "137:         :param run_id: Dag run id for the task",
          "138:         :type run_id: str",
          "139:         :param key: A key for the XCom. If provided, only XComs with matching",
          "140:             keys will be returned. To remove the filter, pass key=None.",
          "141:         :type key: str",
          "142:         :param task_id: Only XComs from task with matching id will be",
          "143:             pulled. Can pass None to remove the filter.",
          "144:         :type task_id: str",
          "145:         :param dag_id: If provided, only pulls XCom from this DAG.",
          "146:             If None (default), the DAG of the calling task is used.",
          "147:         :type dag_id: str",
          "148:         :param include_prior_dates: If False, only XCom from the current",
          "149:             execution_date are returned. If True, XCom from previous dates",
          "150:             are returned as well.",
          "151:         :type include_prior_dates: bool",
          "152:         :param session: database session",
          "153:         :type session: sqlalchemy.orm.session.Session",
          "154:         \"\"\"",
          "158:         result = (",
          "159:             cls.get_many(",
          "160:                 execution_date=execution_date,",
          "",
          "[Added Lines]",
          "146:         # Remove duplicate XComs and insert a new one.",
          "148:             cls.key == key,",
          "149:             cls.execution_date == execution_date,",
          "150:             cls.task_id == task_id,",
          "151:             cls.dag_id == dag_id,",
          "153:         new = cast(Any, cls)(  # Work around Mypy complaining model not defining '__init__'.",
          "154:             key=key,",
          "155:             value=cls.serialize_value(value),",
          "156:             execution_date=execution_date,",
          "157:             task_id=task_id,",
          "158:             dag_id=dag_id,",
          "159:         )",
          "160:         session.add(new)",
          "163:     @overload",
          "164:     @classmethod",
          "165:     def get_one(",
          "166:         cls,",
          "168:         run_id: str,",
          "169:         key: Optional[str] = None,",
          "170:         task_id: Optional[str] = None,",
          "171:         dag_id: Optional[str] = None,",
          "172:         include_prior_dates: bool = False,",
          "173:         session: Optional[Session] = None,",
          "174:     ) -> Optional[Any]:",
          "175:         \"\"\"Retrieve an XCom value, optionally meeting certain criteria.",
          "177:         This method returns \"full\" XCom values (i.e. uses ``deserialize_value``",
          "178:         from the XCom backend). Use :meth:`get_many` if you want the \"shortened\"",
          "179:         value via ``orm_deserialize_value``.",
          "181:         If there are no results, *None* is returned.",
          "183:         A deprecated form of this function accepts ``execution_date`` instead of",
          "184:         ``run_id``. The two arguments are mutually exclusive.",
          "186:         :param run_id: DAG run ID for the task.",
          "187:         :param key: A key for the XCom. If provided, only XCom with matching",
          "188:             keys will be returned. Pass *None* (default) to remove the filter.",
          "189:         :param task_id: Only XCom from task with matching ID will be pulled.",
          "190:             Pass *None* (default) to remove the filter.",
          "191:         :param dag_id: Only pull XCom from this DAG. If *None* (default), the",
          "192:             DAG of the calling task is used.",
          "193:         :param include_prior_dates: If *False* (default), only XCom from the",
          "194:             specified DAG run is returned. If *True*, the latest matching XCom is",
          "195:             returned regardless of the run it belongs to.",
          "196:         :param session: Database session. If not given, a new session will be",
          "197:             created for this function.",
          "198:         :type session: sqlalchemy.orm.session.Session",
          "199:         \"\"\"",
          "201:     @overload",
          "202:     @classmethod",
          "203:     def get_one(",
          "204:         cls,",
          "205:         execution_date: pendulum.DateTime,",
          "206:         key: Optional[str] = None,",
          "207:         task_id: Optional[str] = None,",
          "208:         dag_id: Optional[str] = None,",
          "209:         include_prior_dates: bool = False,",
          "210:         session: Optional[Session] = None,",
          "211:     ) -> Optional[Any]:",
          "212:         \"\"\":sphinx-autoapi-skip:\"\"\"",
          "225:         run_id: Optional[str] = None,",
          "227:         \"\"\":sphinx-autoapi-skip:\"\"\"",
          "231:         if run_id is not None:",
          "232:             query = cls.get_many(",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "165:                 include_prior_dates=include_prior_dates,",
          "166:                 session=session,",
          "167:             )",
          "171:         if result:",
          "172:             return cls.deserialize_value(result)",
          "173:         return None",
          "175:     @classmethod",
          "176:     @provide_session",
          "177:     def get_many(",
          "178:         cls,",
          "179:         execution_date: Optional[pendulum.DateTime] = None,",
          "181:         key: Optional[str] = None,",
          "182:         task_ids: Optional[Union[str, Iterable[str]]] = None,",
          "183:         dag_ids: Optional[Union[str, Iterable[str]]] = None,",
          "184:         include_prior_dates: bool = False,",
          "185:         limit: Optional[int] = None,",
          "186:         session: Session = None,",
          "187:     ) -> Query:",
          "220:         if not (execution_date is None) ^ (run_id is None):",
          "221:             raise ValueError(\"Exactly one of execution_date or run_id must be passed\")",
          "",
          "[Removed Lines]",
          "168:             .with_entities(cls.value)",
          "169:             .first()",
          "170:         )",
          "180:         run_id: Optional[str] = None,",
          "188:         \"\"\"",
          "189:         Composes a query to get one or more values from the xcom table.",
          "191:         ``run_id`` and ``execution_date`` are mutually exclusive.",
          "193:         This function returns an SQLAlchemy query of full XCom objects. If you just want one stored value then",
          "194:         use :meth:`get_one`.",
          "196:         :param execution_date: Execution date for the task",
          "197:         :type execution_date: pendulum.datetime",
          "198:         :param run_id: Dag run id for the task",
          "199:         :type run_id: str",
          "200:         :param key: A key for the XCom. If provided, only XComs with matching",
          "201:             keys will be returned. To remove the filter, pass key=None.",
          "202:         :type key: str",
          "203:         :param task_ids: Only XComs from tasks with matching ids will be",
          "204:             pulled. Can pass None to remove the filter.",
          "205:         :type task_ids: str or iterable of strings (representing task_ids)",
          "206:         :param dag_ids: If provided, only pulls XComs from this DAG.",
          "207:             If None (default), the DAG of the calling task is used.",
          "208:         :type dag_ids: str",
          "209:         :param include_prior_dates: If False, only XComs from the current",
          "210:             execution_date are returned. If True, XComs from previous dates",
          "211:             are returned as well.",
          "212:         :type include_prior_dates: bool",
          "213:         :param limit: If required, limit the number of returned objects.",
          "214:             XCom objects can be quite big and you might want to limit the",
          "215:             number of rows.",
          "216:         :type limit: int",
          "217:         :param session: database session",
          "218:         :type session: sqlalchemy.orm.session.Session",
          "219:         \"\"\"",
          "",
          "[Added Lines]",
          "240:         elif execution_date is not None:",
          "241:             query = cls.get_many(",
          "242:                 execution_date=execution_date,",
          "243:                 key=key,",
          "244:                 task_ids=task_id,",
          "245:                 dag_ids=dag_id,",
          "246:                 include_prior_dates=include_prior_dates,",
          "247:                 session=session,",
          "248:             )",
          "249:         else:",
          "250:             raise RuntimeError(\"Should not happen?\")",
          "252:         result = query.with_entities(cls.value).first()",
          "257:     @overload",
          "258:     @classmethod",
          "259:     def get_many(",
          "260:         cls,",
          "262:         run_id: str,",
          "263:         key: Optional[str] = None,",
          "264:         task_ids: Union[str, Iterable[str], None] = None,",
          "265:         dag_ids: Union[str, Iterable[str], None] = None,",
          "266:         include_prior_dates: bool = False,",
          "267:         limit: Optional[int] = None,",
          "268:         session: Optional[Session] = None,",
          "269:     ) -> Query:",
          "270:         \"\"\"Composes a query to get one or more XCom entries.",
          "272:         This function returns an SQLAlchemy query of full XCom objects. If you",
          "273:         just want one stored value, use :meth:`get_one` instead.",
          "275:         A deprecated form of this function accepts ``execution_date`` instead of",
          "276:         ``run_id``. The two arguments are mutually exclusive.",
          "278:         :param run_id: DAG run ID for the task.",
          "279:         :param key: A key for the XComs. If provided, only XComs with matching",
          "280:             keys will be returned. Pass *None* (default) to remove the filter.",
          "281:         :param task_ids: Only XComs from task with matching IDs will be pulled.",
          "282:             Pass *None* (default) to remove the filter.",
          "283:         :param dag_id: Only pulls XComs from this DAG. If *None* (default), the",
          "284:             DAG of the calling task is used.",
          "285:         :param include_prior_dates: If *False* (default), only XComs from the",
          "286:             specified DAG run are returned. If *True*, all matching XComs are",
          "287:             returned regardless of the run it belongs to.",
          "288:         :param session: Database session. If not given, a new session will be",
          "289:             created for this function.",
          "290:         :type session: sqlalchemy.orm.session.Session",
          "291:         \"\"\"",
          "293:     @overload",
          "294:     @classmethod",
          "295:     def get_many(",
          "296:         cls,",
          "297:         execution_date: pendulum.DateTime,",
          "298:         key: Optional[str] = None,",
          "299:         task_ids: Union[str, Iterable[str], None] = None,",
          "300:         dag_ids: Union[str, Iterable[str], None] = None,",
          "301:         include_prior_dates: bool = False,",
          "302:         limit: Optional[int] = None,",
          "303:         session: Optional[Session] = None,",
          "304:     ) -> Query:",
          "305:         \"\"\":sphinx-autoapi-skip:\"\"\"",
          "319:         run_id: Optional[str] = None,",
          "321:         \"\"\":sphinx-autoapi-skip:\"\"\"",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "263:     @classmethod",
          "264:     @provide_session",
          "267:         if isinstance(xcoms, XCom):",
          "268:             xcoms = [xcoms]",
          "269:         for xcom in xcoms:",
          "",
          "[Removed Lines]",
          "265:     def delete(cls, xcoms, session=None):",
          "266:         \"\"\"Delete Xcom\"\"\"",
          "",
          "[Added Lines]",
          "367:     def delete(cls, xcoms: Union[\"XCom\", Iterable[\"XCom\"]], session: Session) -> None:",
          "368:         \"\"\"Delete one or multiple XCom entries.\"\"\"",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "272:             session.delete(xcom)",
          "273:         session.commit()",
          "275:     @classmethod",
          "276:     @provide_session",
          "277:     def clear(",
          "278:         cls,",
          "279:         execution_date: Optional[pendulum.DateTime] = None,",
          "283:         session: Session = None,",
          "284:     ) -> None:",
          "301:         # Given the historic order of this function (execution_date was first argument) to add a new optional",
          "302:         # param we need to add default values for everything :(",
          "304:             raise TypeError(\"clear() missing required argument: dag_id\")",
          "306:             raise TypeError(\"clear() missing required argument: task_id\")",
          "308:         if not (execution_date is None) ^ (run_id is None):",
          "",
          "[Removed Lines]",
          "280:         dag_id: str = None,",
          "281:         task_id: str = None,",
          "282:         run_id: str = None,",
          "285:         \"\"\"",
          "286:         Clears all XCom data from the database for the task instance",
          "288:         ``run_id`` and ``execution_date`` are mutually exclusive.",
          "290:         :param execution_date: Execution date for the task",
          "291:         :type execution_date: pendulum.datetime or None",
          "292:         :param dag_id: ID of DAG to clear the XCom for.",
          "293:         :type dag_id: str",
          "294:         :param task_id: Only XComs from task with matching id will be cleared.",
          "295:         :type task_id: str",
          "296:         :param run_id: Dag run id for the task",
          "297:         :type run_id: str or None",
          "298:         :param session: database session",
          "299:         :type session: sqlalchemy.orm.session.Session",
          "300:         \"\"\"",
          "303:         if not dag_id:",
          "305:         if not task_id:",
          "",
          "[Added Lines]",
          "377:     @overload",
          "378:     @classmethod",
          "379:     def clear(cls, *, dag_id: str, task_id: str, run_id: str, session: Optional[Session] = None) -> None:",
          "380:         \"\"\"Clear all XCom data from the database for the given task instance.",
          "382:         A deprecated form of this function accepts ``execution_date`` instead of",
          "383:         ``run_id``. The two arguments are mutually exclusive.",
          "385:         :param dag_id: ID of DAG to clear the XCom for.",
          "386:         :param task_id: ID of task to clear the XCom for.",
          "387:         :param run_id: ID of DAG run to clear the XCom for.",
          "388:         :param session: Database session. If not given, a new session will be",
          "389:             created for this function.",
          "390:         :type session: sqlalchemy.orm.session.Session",
          "391:         \"\"\"",
          "393:     @overload",
          "394:     @classmethod",
          "395:     def clear(",
          "396:         cls,",
          "397:         execution_date: pendulum.DateTime,",
          "398:         dag_id: str,",
          "399:         task_id: str,",
          "400:         session: Optional[Session] = None,",
          "401:     ) -> None:",
          "402:         \"\"\":sphinx-autoapi-skip:\"\"\"",
          "409:         dag_id: Optional[str] = None,",
          "410:         task_id: Optional[str] = None,",
          "411:         run_id: Optional[str] = None,",
          "414:         \"\"\":sphinx-autoapi-skip:\"\"\"",
          "417:         if dag_id is None:",
          "419:         if task_id is None:",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "364:         return BaseXCom.deserialize_value(self)",
          "368:     \"\"\"Resolves custom XCom class\"\"\"",
          "369:     clazz = conf.getimport(\"core\", \"xcom_backend\", fallback=f\"airflow.models.xcom.{BaseXCom.__name__}\")",
          "370:     if clazz:",
          "",
          "[Removed Lines]",
          "367: def resolve_xcom_backend():",
          "",
          "[Added Lines]",
          "481: def resolve_xcom_backend() -> Type[BaseXCom]:",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "376:     return BaseXCom",
          "",
          "[Removed Lines]",
          "379: XCom = resolve_xcom_backend()",
          "",
          "[Added Lines]",
          "493: if TYPE_CHECKING:",
          "494:     XCom = BaseXCom  # Hack to avoid Mypy \"Variable 'XCom' is not valid as a type\".",
          "495: else:",
          "496:     XCom = resolve_xcom_backend()",
          "",
          "---------------"
        ]
      }
    }
  ]
}