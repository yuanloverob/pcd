{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "3a952933c348ea8a1b52e7ce5e7a4349d9318ec1",
      "candidate_info": {
        "commit_hash": "3a952933c348ea8a1b52e7ce5e7a4349d9318ec1",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/3a952933c348ea8a1b52e7ce5e7a4349d9318ec1",
        "files": [
          "core/src/main/resources/error/error-classes.json",
          "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/core/src/test/resources/sql-tests/results/ansi/array.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/cast.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/date.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/datetime-parsing-invalid.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/map.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/string-functions.sql.out",
          "sql/core/src/test/resources/sql-tests/results/interval.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/boolean.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/float4.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/float8.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/int8.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/select_having.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/text.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part2.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part3.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part4.sql.out",
          "sql/core/src/test/resources/sql-tests/results/timestampNTZ/timestamp-ansi.sql.out",
          "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-select_having.sql.out"
        ],
        "message": "[SPARK-39392][SQL][3.3] Refine ANSI error messages for try_* function hints\n\n### What changes were proposed in this pull request?\n\nRefine ANSI error messages and remove 'To return NULL instead'.\nThis PR is a backport of https://github.com/apache/spark/pull/36780 from `master`\n\n### Why are the changes needed?\n\nImprove error messaging for ANSI mode since the user may not even aware that query was returning NULLs.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nUnit tests\n\nCloses #36792 from vli-databricks/SPARK-39392-3.3.\n\nAuthored-by: Vitalii Li <vitalii.li@databricks.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala||core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala||core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala": [
          "File: core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala -> core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "127:     assert(getMessage(\"DIVIDE_BY_ZERO\", Array(\"foo\", \"bar\", \"baz\")) ==",
          "129:       \"to \\\"false\\\" (except for ANSI interval type) to bypass this error.\")",
          "130:   }",
          "",
          "[Removed Lines]",
          "128:       \"Division by zero. To return NULL instead, use `try_divide`. If necessary set foo \" +",
          "",
          "[Added Lines]",
          "128:       \"Division by zero. Use `try_divide` to tolerate divisor being 0 and return NULL instead. \" +",
          "129:         \"If necessary set foo \" +",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "493:       message: String,",
          "494:       hint: String = \"\",",
          "495:       errorContext: String = \"\"): ArithmeticException = {",
          "497:     new SparkArithmeticException(",
          "498:       errorClass = \"ARITHMETIC_OVERFLOW\",",
          "499:       messageParameters = Array(message, alternative, SQLConf.ANSI_ENABLED.key),",
          "",
          "[Removed Lines]",
          "496:     val alternative = if (hint.nonEmpty) s\" To return NULL instead, use '$hint'.\" else \"\"",
          "",
          "[Added Lines]",
          "496:     val alternative = if (hint.nonEmpty) {",
          "497:       s\" Use '$hint' to tolerate overflow and return NULL instead.\"",
          "498:     } else \"\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1093:       value: Any, from: DataType, to: DataType, errorContext: String): Throwable = {",
          "1094:     val valueString = toSQLValue(value, from)",
          "1095:     new DateTimeException(s\"Invalid input syntax for type ${toSQLType(to)}: $valueString. \" +",
          "1097:       s\"to false to bypass this error.\" + errorContext)",
          "1098:   }",
          "",
          "[Removed Lines]",
          "1096:       s\"To return NULL instead, use 'try_cast'. If necessary set ${SQLConf.ANSI_ENABLED.key} \" +",
          "",
          "[Added Lines]",
          "1098:       s\"Use `try_cast` to tolerate malformed input and return NULL instead. \" +",
          "1099:       s\"If necessary set ${SQLConf.ANSI_ENABLED.key} \" +",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b5ce32f41f9e4aecb02cc383184ac0a6dfabc4dd",
      "candidate_info": {
        "commit_hash": "b5ce32f41f9e4aecb02cc383184ac0a6dfabc4dd",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/b5ce32f41f9e4aecb02cc383184ac0a6dfabc4dd",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala"
        ],
        "message": "[SPARK-39162][SQL][3.3] Jdbc dialect should decide which function could be pushed down\n\n### What changes were proposed in this pull request?\nThis PR used to back port https://github.com/apache/spark/pull/36521 to 3.3\n\n### Why are the changes needed?\nLet function push-down more flexible.\n\n### Does this PR introduce _any_ user-facing change?\n'No'.\nNew feature.\n\n### How was this patch tested?\nExists tests.\n\nCloses #36556 from beliefer/SPARK-39162_3.3.\n\nAuthored-by: Jiaan Geng <beliefer@163.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala||sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala||sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2411:     new AnalysisException(",
          "2412:       \"Sinks cannot request distribution and ordering in continuous execution mode\")",
          "2413:   }",
          "2418: }",
          "",
          "[Removed Lines]",
          "2415:   def noSuchFunctionError(database: String, funcInfo: String): Throwable = {",
          "2416:     new AnalysisException(s\"$database does not support function: $funcInfo\")",
          "2417:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala||sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala -> sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import java.sql.{SQLException, Types}",
          "21: import java.util.Locale",
          "25: import org.apache.spark.sql.AnalysisException",
          "26: import org.apache.spark.sql.catalyst.analysis.{NoSuchNamespaceException, NoSuchTableException, TableAlreadyExistsException}",
          "28: import org.apache.spark.sql.connector.expressions.aggregate.{AggregateFunc, GeneralAggregateFunc}",
          "30: import org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils",
          "31: import org.apache.spark.sql.types.{BooleanType, ByteType, DataType, DecimalType, ShortType, StringType}",
          "",
          "[Removed Lines]",
          "23: import scala.util.control.NonFatal",
          "27: import org.apache.spark.sql.connector.expressions.Expression",
          "29: import org.apache.spark.sql.errors.QueryCompilationErrors",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "34:   override def canHandle(url: String): Boolean =",
          "35:     url.toLowerCase(Locale.ROOT).startsWith(\"jdbc:h2\")",
          "59:   override def compileAggregate(aggFunction: AggregateFunc): Option[String] = {",
          "60:     super.compileAggregate(aggFunction).orElse(",
          "",
          "[Removed Lines]",
          "37:   class H2SQLBuilder extends JDBCSQLBuilder {",
          "38:     override def visitSQLFunction(funcName: String, inputs: Array[String]): String = {",
          "39:       funcName match {",
          "40:         case \"WIDTH_BUCKET\" =>",
          "41:           val functionInfo = super.visitSQLFunction(funcName, inputs)",
          "42:           throw QueryCompilationErrors.noSuchFunctionError(\"H2\", functionInfo)",
          "43:         case _ => super.visitSQLFunction(funcName, inputs)",
          "44:       }",
          "45:     }",
          "46:   }",
          "48:   override def compileExpression(expr: Expression): Option[String] = {",
          "49:     val h2SQLBuilder = new H2SQLBuilder()",
          "50:     try {",
          "51:       Some(h2SQLBuilder.build(expr))",
          "52:     } catch {",
          "53:       case NonFatal(e) =>",
          "54:         logWarning(\"Error occurs while compiling V2 expression\", e)",
          "55:         None",
          "56:     }",
          "57:   }",
          "",
          "[Added Lines]",
          "33:   private val supportedFunctions =",
          "34:     Set(\"ABS\", \"COALESCE\", \"LN\", \"EXP\", \"POWER\", \"SQRT\", \"FLOOR\", \"CEIL\")",
          "36:   override def isSupportedFunction(funcName: String): Boolean =",
          "37:     supportedFunctions.contains(funcName)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala||sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala -> sql/core/src/main/scala/org/apache/spark/sql/jdbc/JdbcDialects.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "240:         getJDBCType(dataType).map(_.databaseTypeDefinition).getOrElse(dataType.typeName)",
          "241:       s\"CAST($l AS $databaseTypeDefinition)\"",
          "242:     }",
          "243:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "244:     override def visitSQLFunction(funcName: String, inputs: Array[String]): String = {",
          "245:       if (isSupportedFunction(funcName)) {",
          "246:         s\"\"\"$funcName(${inputs.mkString(\", \")})\"\"\"",
          "247:       } else {",
          "250:         throw new UnsupportedOperationException(",
          "251:           s\"${this.getClass.getSimpleName} does not support function: $funcName\")",
          "252:       }",
          "253:     }",
          "261:   @Since(\"3.3.0\")",
          "262:   def isSupportedFunction(funcName: String): Boolean = false",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5d3f3365c0b7d4515fd97d0ff7b7b29db69b2faf",
      "candidate_info": {
        "commit_hash": "5d3f3365c0b7d4515fd97d0ff7b7b29db69b2faf",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/5d3f3365c0b7d4515fd97d0ff7b7b29db69b2faf",
        "files": [
          "core/src/main/resources/error/error-classes.json",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala"
        ],
        "message": "[SPARK-39163][SQL][3.3] Throw an exception w/ error class for an invalid bucket file\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to use the INVALID_BUCKET_FILE error classes for an invalid bucket file.\n\nThis is a backport of https://github.com/apache/spark/pull/36603.\n\n### Why are the changes needed?\nPorting the executing errors for multiple rows from a subquery used as an expression to the new error framework should improve user experience with Spark SQL.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nUT\n\nCloses #36913 from panbingkun/branch-3.3-SPARK-39163.\n\nAuthored-by: panbingkun <pbk1982@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2075:     new SparkException(errorClass = \"NULL_COMPARISON_RESULT\",",
          "2076:       messageParameters = Array(), cause = null)",
          "2077:   }",
          "2078: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2079:   def invalidBucketFile(path: String): Throwable = {",
          "2080:     new SparkException(errorClass = \"INVALID_BUCKET_FILE\", messageParameters = Array(path),",
          "2081:       cause = null)",
          "2082:   }",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: import org.apache.spark.sql.catalyst.plans.QueryPlan",
          "32: import org.apache.spark.sql.catalyst.plans.physical.{HashPartitioning, Partitioning, UnknownPartitioning}",
          "33: import org.apache.spark.sql.catalyst.util.truncatedString",
          "34: import org.apache.spark.sql.execution.datasources._",
          "35: import org.apache.spark.sql.execution.datasources.parquet.{ParquetFileFormat => ParquetSource}",
          "36: import org.apache.spark.sql.execution.datasources.v2.PushedDownOperators",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "34: import org.apache.spark.sql.errors.QueryExecutionErrors",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "592:       }.groupBy { f =>",
          "593:         BucketingUtils",
          "594:           .getBucketId(new Path(f.filePath).getName)",
          "597:       }",
          "599:     val prunedFilesGroupedToBuckets = if (optionalBucketSet.isDefined) {",
          "",
          "[Removed Lines]",
          "596:           .getOrElse(throw new IllegalStateException(s\"Invalid bucket file ${f.filePath}\"))",
          "",
          "[Added Lines]",
          "596:           .getOrElse(throw QueryExecutionErrors.invalidBucketFile(f.filePath))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.errors",
          "20: import org.apache.spark.{SparkArithmeticException, SparkException, SparkIllegalArgumentException, SparkRuntimeException, SparkUnsupportedOperationException, SparkUpgradeException}",
          "21: import org.apache.spark.sql.{DataFrame, QueryTest, SaveMode}",
          "22: import org.apache.spark.sql.execution.datasources.orc.OrcTest",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import java.io.File",
          "21: import java.net.URI",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "286:       assert(e2.getMessage === \"The save mode NULL is not supported for: an existent path.\")",
          "287:     }",
          "288:   }",
          "289: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "293:   test(\"INVALID_BUCKET_FILE: error if there exists any malformed bucket files\") {",
          "294:     val df1 = (0 until 50).map(i => (i % 5, i % 13, i.toString)).",
          "295:       toDF(\"i\", \"j\", \"k\").as(\"df1\")",
          "297:     withTable(\"bucketed_table\") {",
          "298:       df1.write.format(\"parquet\").bucketBy(8, \"i\").",
          "299:         saveAsTable(\"bucketed_table\")",
          "300:       val warehouseFilePath = new URI(spark.sessionState.conf.warehousePath).getPath",
          "301:       val tableDir = new File(warehouseFilePath, \"bucketed_table\")",
          "302:       Utils.deleteRecursively(tableDir)",
          "303:       df1.write.parquet(tableDir.getAbsolutePath)",
          "305:       val aggregated = spark.table(\"bucketed_table\").groupBy(\"i\").count()",
          "307:       val e = intercept[SparkException] {",
          "308:         aggregated.count()",
          "309:       }",
          "310:       assert(e.getErrorClass === \"INVALID_BUCKET_FILE\")",
          "311:       assert(e.getMessage.matches(\"Invalid bucket file: .+\"))",
          "312:     }",
          "313:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import org.apache.logging.log4j.Level",
          "24: import org.scalatest.PrivateMethodTester",
          "26: import org.apache.spark.scheduler.{SparkListener, SparkListenerEvent, SparkListenerJobStart}",
          "27: import org.apache.spark.sql.{Dataset, QueryTest, Row, SparkSession, Strategy}",
          "28: import org.apache.spark.sql.catalyst.optimizer.{BuildLeft, BuildRight}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: import org.apache.spark.SparkException",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "856:         df1.write.parquet(tableDir.getAbsolutePath)",
          "858:         val aggregated = spark.table(\"bucketed_table\").groupBy(\"i\").count()",
          "860:           aggregated.count()",
          "861:         }",
          "865:       }",
          "866:     }",
          "867:   }",
          "",
          "[Removed Lines]",
          "859:         val error = intercept[IllegalStateException] {",
          "863:         assert(error.toString contains \"Invalid bucket file\")",
          "864:         assert(error.getSuppressed.size === 0)",
          "",
          "[Added Lines]",
          "860:         val error = intercept[SparkException] {",
          "863:         assert(error.getErrorClass === \"INVALID_BUCKET_FILE\")",
          "864:         assert(error.getMessage contains \"Invalid bucket file\")",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.sources",
          "23: import scala.util.Random",
          "25: import org.apache.spark.sql._",
          "",
          "[Removed Lines]",
          "20: import java.io.File",
          "21: import java.net.URI",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "36: import org.apache.spark.sql.internal.SQLConf",
          "37: import org.apache.spark.sql.internal.StaticSQLConf.CATALOG_IMPLEMENTATION",
          "38: import org.apache.spark.sql.test.{SharedSparkSession, SQLTestUtils}",
          "40: import org.apache.spark.util.collection.BitSet",
          "42: class BucketedReadWithoutHiveSupportSuite",
          "",
          "[Removed Lines]",
          "39: import org.apache.spark.util.Utils",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "832:     }",
          "833:   }",
          "852:   test(\"disable bucketing when the output doesn't contain all bucketing columns\") {",
          "853:     withTable(\"bucketed_table\") {",
          "854:       df1.write.format(\"parquet\").bucketBy(8, \"i\").saveAsTable(\"bucketed_table\")",
          "",
          "[Removed Lines]",
          "835:   test(\"error if there exists any malformed bucket files\") {",
          "836:     withTable(\"bucketed_table\") {",
          "837:       df1.write.format(\"parquet\").bucketBy(8, \"i\").saveAsTable(\"bucketed_table\")",
          "838:       val warehouseFilePath = new URI(spark.sessionState.conf.warehousePath).getPath",
          "839:       val tableDir = new File(warehouseFilePath, \"bucketed_table\")",
          "840:       Utils.deleteRecursively(tableDir)",
          "841:       df1.write.parquet(tableDir.getAbsolutePath)",
          "843:       val aggregated = spark.table(\"bucketed_table\").groupBy(\"i\").count()",
          "844:       val e = intercept[IllegalStateException] {",
          "845:         aggregated.count()",
          "846:       }",
          "848:       assert(e.toString contains \"Invalid bucket file\")",
          "849:     }",
          "850:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ff048f1b69e5520c1fedbfd9869717f0b8919c0f",
      "candidate_info": {
        "commit_hash": "ff048f1b69e5520c1fedbfd9869717f0b8919c0f",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/ff048f1b69e5520c1fedbfd9869717f0b8919c0f",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala"
        ],
        "message": "[SPARK-39422][SQL] Improve error message for 'SHOW CREATE TABLE' with unsupported serdes\n\n### What changes were proposed in this pull request?\n\nThis PR improves the error message that is thrown when trying to run `SHOW CREATE TABLE` on a Hive table with an unsupported serde. Currently this results in an error like\n\n```\norg.apache.spark.sql.AnalysisException: Failed to execute SHOW CREATE TABLE against table rcFileTable, which is created by Hive and uses the following unsupported serde configuration\n SERDE: org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe INPUTFORMAT: org.apache.hadoop.hive.ql.io.RCFileInputFormat OUTPUTFORMAT: org.apache.hadoop.hive.ql.io.RCFileOutputFormat\n```\n\nThis patch improves this error message by adding a suggestion to use `SHOW CREATE TABLE ... AS SERDE`:\n\n```\norg.apache.spark.sql.AnalysisException: Failed to execute SHOW CREATE TABLE against table rcFileTable, which is created by Hive and uses the following unsupported serde configuration\n SERDE: org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe INPUTFORMAT: org.apache.hadoop.hive.ql.io.RCFileInputFormat OUTPUTFORMAT: org.apache.hadoop.hive.ql.io.RCFileOutputFormat\nPlease use `SHOW CREATE TABLE rcFileTable AS SERDE` to show Hive DDL instead.\n```\n\nThe suggestion's wording is consistent with other error messages thrown by SHOW CREATE TABLE.\n\n### Why are the changes needed?\n\nThe existing error message is confusing.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, it improves a user-facing error message.\n\n### How was this patch tested?\n\nManually tested with\n\n```\nCREATE TABLE rcFileTable(i INT)\n    ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe'\n    STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.RCFileInputFormat'\n              OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.RCFileOutputFormat'\n\nSHOW CREATE TABLE rcFileTable\n```\n\nto trigger the error. Confirmed that the `AS SERDE` suggestion actually works.\n\nCloses #36814 from JoshRosen/suggest-show-create-table-as-serde-in-error-message.\n\nAuthored-by: Josh Rosen <joshrosen@databricks.com>\nSigned-off-by: Josh Rosen <joshrosen@databricks.com>\n(cherry picked from commit 8765eea1c08bc58a0cfc22b7cfbc0b5645cc81f9)\nSigned-off-by: Josh Rosen <joshrosen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1989:     new AnalysisException(\"Failed to execute SHOW CREATE TABLE against table \" +",
          "1990:         s\"${table.identifier}, which is created by Hive and uses the \" +",
          "1991:         \"following unsupported serde configuration\\n\" +",
          "1993:     )",
          "1994:   }",
          "",
          "[Removed Lines]",
          "1992:         builder.toString()",
          "",
          "[Added Lines]",
          "1992:         builder.toString() + \"\\n\" +",
          "1993:         s\"Please use `SHOW CREATE TABLE ${table.identifier} AS SERDE` to show Hive DDL instead.\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d63e42d128b8814e885b86533f187724fbb7e9fd",
      "candidate_info": {
        "commit_hash": "d63e42d128b8814e885b86533f187724fbb7e9fd",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/d63e42d128b8814e885b86533f187724fbb7e9fd",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/csvExpressions.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/CsvFunctionsSuite.scala"
        ],
        "message": "[SPARK-38955][SQL] Disable lineSep option in 'from_csv' and 'schema_of_csv'\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to disable `lineSep` option in `from_csv` and `schema_of_csv` expression by setting Noncharacters according to [unicode specification](https://www.unicode.org/charts/PDF/UFFF0.pdf), `\\UFFFF`. This can be used for the internal purpose in a program according to the specification.\n\nThe Univocity parser does not allow to omit the line separator (from my code reading) so this approach was proposed.\n\nThis specific code path is not affected by our `encoding` or `charset` option because Unicovity parser parses them as unicodes as are internally.\n\n### Why are the changes needed?\n\nCurrently, this option is weirdly effective. See the example of `from_csv` as below:\n\n```scala\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\n\nSeq[String](\"1,\\n2,3,4,5\").toDF.select(\n  col(\"value\"),\n  from_csv(\n    col(\"value\"),\n    StructType(Seq(StructField(\"a\", LongType), StructField(\"b\", StringType)\n  )), Map[String,String]())).show()\n```\n\n```\n+-----------+---------------+\n|      value|from_csv(value)|\n+-----------+---------------+\n|1,\\n2,3,4,5|      {1, null}|\n+-----------+---------------+\n```\n\n`{1, null}` has to be `{1, \\n2}`.\n\nThe CSV expressions cannot easily make it supported because this option is plan-wise option that can change the number of returned rows; however, the expressions are designed to emit one row only whereas this option is easily effective in the scan plan with CSV data source. Therefore, we should disable this option.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, now the `lineSep` can be located in the output from `from_csv` and `schema_of_csv`.\n\n### How was this patch tested?\n\nManually tested, and unit test was added.\n\nCloses #36294 from HyukjinKwon/SPARK-38955.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit f3cc2814d4bc585dad92c9eca9a593d1617d27e9)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/csvExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/csvExpressions.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/CsvFunctionsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/CsvFunctionsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/csvExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/csvExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/csvExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/csvExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "98:   val nameOfCorruptRecord = SQLConf.get.getConf(SQLConf.COLUMN_NAME_OF_CORRUPT_RECORD)",
          "100:   @transient lazy val parser = {",
          "101:     val parsedOptions = new CSVOptions(",
          "103:       columnPruning = true,",
          "104:       defaultTimeZoneId = timeZoneId.get,",
          "105:       defaultColumnNameOfCorruptRecord = nameOfCorruptRecord)",
          "",
          "[Removed Lines]",
          "102:       options,",
          "",
          "[Added Lines]",
          "105:     val exprOptions = options ++ Map(\"lineSep\" -> '\\uFFFF'.toString)",
          "108:       exprOptions,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "186:   }",
          "188:   override def eval(v: InternalRow): Any = {",
          "190:     val parser = new CsvParser(parsedOptions.asParserSettings)",
          "191:     val row = parser.parseLine(csv.toString)",
          "192:     assert(row != null, \"Parsed CSV record should not be null.\")",
          "",
          "[Removed Lines]",
          "189:     val parsedOptions = new CSVOptions(options, true, \"UTC\")",
          "",
          "[Added Lines]",
          "199:     val exprOptions = options ++ Map(\"lineSep\" -> '\\uFFFF'.toString)",
          "201:     val parsedOptions = new CSVOptions(exprOptions, true, \"UTC\")",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/CsvFunctionsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/CsvFunctionsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/CsvFunctionsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/CsvFunctionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "364:       .selectExpr(\"value.a\")",
          "365:     checkAnswer(fromCsvDF, Row(null))",
          "366:   }",
          "367: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "368:   test(\"SPARK-38955: disable lineSep option in from_csv and schema_of_csv\") {",
          "369:     val df = Seq[String](\"1,2\\n2\").toDF(\"csv\")",
          "370:     val actual = df.select(from_csv(",
          "371:       $\"csv\", schema_of_csv(\"1,2\\n2\"), Map.empty[String, String].asJava))",
          "372:     checkAnswer(actual, Row(Row(1, \"2\\n2\")))",
          "373:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}