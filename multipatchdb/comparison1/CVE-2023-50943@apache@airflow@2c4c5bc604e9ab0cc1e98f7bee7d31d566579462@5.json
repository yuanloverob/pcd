{
  "cve_id": "CVE-2023-50943",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows a potential attacker to poison the XCom data by bypassing the protection of \"enable_xcom_pickling=False\" configuration setting resulting in poisoned data after XCom deserialization. This vulnerability is considered low since it requires a DAG author to exploit it. Users are recommended to upgrade to version 2.8.1 or later, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
  "patch_info": {
    "commit_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "files": [
      "airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py"
    ],
    "message": "Stop deserializing pickle when enable_xcom_pickling is False (#36255)\n\n* Stop deserializing pickle when enable_xcom_pickling is False\n\n* Fix unit tests\n\n(cherry picked from commit 63e97abec5d56bc62a293c93f5227f364561e51c)",
    "before_after_code_files": [
      "airflow/models/xcom.py||airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py||tests/models/test_xcom.py"
    ]
  },
  "patch_diff": {
    "airflow/models/xcom.py||airflow/models/xcom.py": [
      "File: airflow/models/xcom.py -> airflow/models/xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "685:             except pickle.UnpicklingError:",
      "686:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "687:         else:",
      "693:     @staticmethod",
      "694:     def deserialize_value(result: XCom) -> Any:",
      "",
      "[Removed Lines]",
      "688:             try:",
      "689:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "690:             except (json.JSONDecodeError, UnicodeDecodeError):",
      "691:                 return pickle.loads(result.value)",
      "",
      "[Added Lines]",
      "688:             # Since xcom_pickling is disabled, we should only try to deserialize with JSON",
      "689:             return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "",
      "---------------"
    ],
    "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py": [
      "File: tests/api_connexion/schemas/test_xcom_schema.py -> tests/api_connexion/schemas/test_xcom_schema.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from airflow.models import DagRun, XCom",
      "31: from airflow.utils.dates import parse_execution_date",
      "32: from airflow.utils.session import create_session",
      "34: pytestmark = pytest.mark.db_test",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "188:     default_time = \"2016-04-02T21:00:00+00:00\"",
      "189:     default_time_parsed = parse_execution_date(default_time)",
      "191:     def test_serialize(self, create_xcom, session):",
      "192:         create_xcom(",
      "193:             dag_id=\"test_dag\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "192:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "208:             \"map_index\": -1,",
      "209:         }",
      "211:     def test_deserialize(self):",
      "212:         xcom_dump = {",
      "213:             \"key\": \"test_key\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "213:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------"
    ],
    "tests/models/test_xcom.py||tests/models/test_xcom.py": [
      "File: tests/models/test_xcom.py -> tests/models/test_xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "140:             ret_value = XCom.get_value(key=\"xcom_test3\", ti_key=ti_key, session=session)",
      "141:         assert ret_value == {\"key\": \"value\"}",
      "144:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"}):",
      "145:             XCom.set(",
      "146:                 key=\"xcom_test3\",",
      "",
      "[Removed Lines]",
      "143:     def test_xcom_deserialize_with_pickle_to_json_switch(self, task_instance, session):",
      "",
      "[Added Lines]",
      "143:     def test_xcom_deserialize_pickle_when_xcom_pickling_is_disabled(self, task_instance, session):",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "151:                 session=session,",
      "152:             )",
      "153:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"False\"}):",
      "163:     @conf_vars({(\"core\", \"xcom_enable_pickling\"): \"False\"})",
      "164:     def test_xcom_disable_pickle_type_fail_on_non_json(self, task_instance, session):",
      "",
      "[Removed Lines]",
      "154:             ret_value = XCom.get_one(",
      "155:                 key=\"xcom_test3\",",
      "156:                 dag_id=task_instance.dag_id,",
      "157:                 task_id=task_instance.task_id,",
      "158:                 run_id=task_instance.run_id,",
      "159:                 session=session,",
      "160:             )",
      "161:         assert ret_value == {\"key\": \"value\"}",
      "",
      "[Added Lines]",
      "154:             with pytest.raises(UnicodeDecodeError):",
      "155:                 XCom.get_one(",
      "156:                     key=\"xcom_test3\",",
      "157:                     dag_id=task_instance.dag_id,",
      "158:                     task_id=task_instance.task_id,",
      "159:                     run_id=task_instance.run_id,",
      "160:                     session=session,",
      "161:                 )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "6624adeb01106d73f7c933c4c99edb48d8795e43",
      "candidate_info": {
        "commit_hash": "6624adeb01106d73f7c933c4c99edb48d8795e43",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/6624adeb01106d73f7c933c4c99edb48d8795e43",
        "files": [
          "airflow/api_connexion/endpoints/variable_endpoint.py",
          "tests/api_connexion/endpoints/test_variable_endpoint.py"
        ],
        "message": "Fix the required access for get_variable endpoint (#36396)\n\n(cherry picked from commit 34132e37c691995ff94dd1b8518d1f5c3a2ec998)",
        "before_after_code_files": [
          "airflow/api_connexion/endpoints/variable_endpoint.py||airflow/api_connexion/endpoints/variable_endpoint.py",
          "tests/api_connexion/endpoints/test_variable_endpoint.py||tests/api_connexion/endpoints/test_variable_endpoint.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api_connexion/endpoints/variable_endpoint.py||airflow/api_connexion/endpoints/variable_endpoint.py": [
          "File: airflow/api_connexion/endpoints/variable_endpoint.py -> airflow/api_connexion/endpoints/variable_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "57:     return Response(status=HTTPStatus.NO_CONTENT)",
          "61: @provide_session",
          "62: def get_variable(*, variable_key: str, session: Session = NEW_SESSION) -> Response:",
          "63:     \"\"\"Get a variable by key.\"\"\"",
          "",
          "[Removed Lines]",
          "60: @security.requires_access_variable(\"DELETE\")",
          "",
          "[Added Lines]",
          "60: @security.requires_access_variable(\"GET\")",
          "",
          "---------------"
        ],
        "tests/api_connexion/endpoints/test_variable_endpoint.py||tests/api_connexion/endpoints/test_variable_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_variable_endpoint.py -> tests/api_connexion/endpoints/test_variable_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "46:             (permissions.ACTION_CAN_DELETE, permissions.RESOURCE_VARIABLE),",
          "47:         ],",
          "48:     )",
          "49:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
          "51:     yield app",
          "53:     delete_user(app, username=\"test\")  # type: ignore",
          "54:     delete_user(app, username=\"test_no_permissions\")  # type: ignore",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "49:     create_user(",
          "50:         app,  # type: ignore",
          "51:         username=\"test_read_only\",",
          "52:         role_name=\"TestReadOnly\",",
          "53:         permissions=[",
          "54:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_VARIABLE),",
          "55:         ],",
          "56:     )",
          "57:     create_user(",
          "58:         app,  # type: ignore",
          "59:         username=\"test_delete_only\",",
          "60:         role_name=\"TestDeleteOnly\",",
          "61:         permissions=[",
          "62:             (permissions.ACTION_CAN_DELETE, permissions.RESOURCE_VARIABLE),",
          "63:         ],",
          "64:     )",
          "70:     delete_user(app, username=\"test_read_only\")  # type: ignore",
          "71:     delete_user(app, username=\"test_delete_only\")  # type: ignore",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "111: class TestGetVariable(TestVariableEndpoint):",
          "113:         expected_value = '{\"foo\": 1}'",
          "114:         Variable.set(\"TEST_VARIABLE_KEY\", expected_value)",
          "115:         response = self.client.get(",
          "117:         )",
          "121:     def test_should_respond_404_if_not_found(self):",
          "122:         response = self.client.get(",
          "",
          "[Removed Lines]",
          "112:     def test_should_respond_200(self):",
          "116:             \"/api/v1/variables/TEST_VARIABLE_KEY\", environ_overrides={\"REMOTE_USER\": \"test\"}",
          "118:         assert response.status_code == 200",
          "119:         assert response.json == {\"key\": \"TEST_VARIABLE_KEY\", \"value\": expected_value, \"description\": None}",
          "",
          "[Added Lines]",
          "130:     @pytest.mark.parametrize(",
          "131:         \"user, expected_status_code\",",
          "132:         [",
          "133:             (\"test\", 200),",
          "134:             (\"test_read_only\", 200),",
          "135:             (\"test_delete_only\", 403),",
          "136:             (\"test_no_permissions\", 403),",
          "137:         ],",
          "138:     )",
          "139:     def test_read_variable(self, user, expected_status_code):",
          "143:             \"/api/v1/variables/TEST_VARIABLE_KEY\", environ_overrides={\"REMOTE_USER\": user}",
          "145:         assert response.status_code == expected_status_code",
          "146:         if expected_status_code == 200:",
          "147:             assert response.json == {\"key\": \"TEST_VARIABLE_KEY\", \"value\": expected_value, \"description\": None}",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cde72b6e0e6f7a03e3840a931f55feb0760b5134",
      "candidate_info": {
        "commit_hash": "cde72b6e0e6f7a03e3840a931f55feb0760b5134",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cde72b6e0e6f7a03e3840a931f55feb0760b5134",
        "files": [
          "airflow/providers/hashicorp/_internal_client/vault_client.py",
          "airflow/providers/hashicorp/provider.yaml",
          "generated/provider_dependencies.json",
          "tests/providers/hashicorp/_internal_client/test_vault_client.py",
          "tests/providers/hashicorp/hooks/test_vault.py",
          "tests/providers/hashicorp/secrets/test_vault.py"
        ],
        "message": "Explicitly passing `raise_on_deleted_version=True` to `read_secret_version` in Hashicorp operator (#36532)\n\n* explicitly passing raise_on_deleted_version=True to read_secret_version\n\n* fix tests\n\n* update hvac version\n\n(cherry picked from commit cd5ab08d95aaf4c65e56a91f1843d04c09f27cb1)",
        "before_after_code_files": [
          "airflow/providers/hashicorp/_internal_client/vault_client.py||airflow/providers/hashicorp/_internal_client/vault_client.py",
          "tests/providers/hashicorp/_internal_client/test_vault_client.py||tests/providers/hashicorp/_internal_client/test_vault_client.py",
          "tests/providers/hashicorp/hooks/test_vault.py||tests/providers/hashicorp/hooks/test_vault.py",
          "tests/providers/hashicorp/secrets/test_vault.py||tests/providers/hashicorp/secrets/test_vault.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/hashicorp/_internal_client/vault_client.py||airflow/providers/hashicorp/_internal_client/vault_client.py": [
          "File: airflow/providers/hashicorp/_internal_client/vault_client.py -> airflow/providers/hashicorp/_internal_client/vault_client.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "373:                 response = self.client.secrets.kv.v1.read_secret(path=secret_path, mount_point=mount_point)",
          "374:             else:",
          "375:                 response = self.client.secrets.kv.v2.read_secret_version(",
          "377:                 )",
          "378:         except InvalidPath:",
          "379:             self.log.debug(\"Secret not found %s with mount point %s\", secret_path, mount_point)",
          "",
          "[Removed Lines]",
          "376:                     path=secret_path, mount_point=mount_point, version=secret_version",
          "",
          "[Added Lines]",
          "376:                     path=secret_path,",
          "377:                     mount_point=mount_point,",
          "378:                     version=secret_version,",
          "379:                     raise_on_deleted_version=True,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "422:         try:",
          "423:             mount_point, secret_path = self._parse_secret_path(secret_path)",
          "424:             return self.client.secrets.kv.v2.read_secret_version(",
          "426:             )",
          "427:         except InvalidPath:",
          "428:             self.log.debug(",
          "",
          "[Removed Lines]",
          "425:                 path=secret_path, mount_point=mount_point, version=secret_version",
          "",
          "[Added Lines]",
          "428:                 path=secret_path,",
          "429:                 mount_point=mount_point,",
          "430:                 version=secret_version,",
          "431:                 raise_on_deleted_version=True,",
          "",
          "---------------"
        ],
        "tests/providers/hashicorp/_internal_client/test_vault_client.py||tests/providers/hashicorp/_internal_client/test_vault_client.py": [
          "File: tests/providers/hashicorp/_internal_client/test_vault_client.py -> tests/providers/hashicorp/_internal_client/test_vault_client.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "641:         secret = vault_client.get_secret(secret_path=\"missing\")",
          "642:         assert secret is None",
          "643:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "645:         )",
          "647:     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")",
          "",
          "[Removed Lines]",
          "644:             mount_point=\"secret\", path=\"missing\", version=None",
          "",
          "[Added Lines]",
          "644:             mount_point=\"secret\", path=\"missing\", version=None, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "661:         assert secret is None",
          "662:         assert \"secret\" == vault_client.mount_point",
          "663:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "665:         )",
          "667:     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")",
          "",
          "[Removed Lines]",
          "664:             mount_point=\"secret\", path=\"missing\", version=None",
          "",
          "[Added Lines]",
          "664:             mount_point=\"secret\", path=\"missing\", version=None, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "716:         secret = vault_client.get_secret(secret_path=\"path/to/secret\")",
          "717:         assert {\"secret_key\": \"secret_value\"} == secret",
          "718:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "720:         )",
          "722:     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")",
          "",
          "[Removed Lines]",
          "719:             mount_point=\"secret\", path=\"path/to/secret\", version=None",
          "",
          "[Added Lines]",
          "719:             mount_point=\"secret\", path=\"path/to/secret\", version=None, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "754:         secret = vault_client.get_secret(secret_path=\"mount_point/path/to/secret\")",
          "755:         assert {\"secret_key\": \"secret_value\"} == secret",
          "756:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "758:         )",
          "760:     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")",
          "",
          "[Removed Lines]",
          "757:             mount_point=\"mount_point\", path=\"path/to/secret\", version=None",
          "",
          "[Added Lines]",
          "757:             mount_point=\"mount_point\", path=\"path/to/secret\", version=None, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "791:         secret = vault_client.get_secret(secret_path=\"missing\", secret_version=1)",
          "792:         assert {\"secret_key\": \"secret_value\"} == secret",
          "793:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "795:         )",
          "797:     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")",
          "",
          "[Removed Lines]",
          "794:             mount_point=\"secret\", path=\"missing\", version=1",
          "",
          "[Added Lines]",
          "794:             mount_point=\"secret\", path=\"missing\", version=1, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1015:             \"auth\": None,",
          "1016:         } == metadata",
          "1017:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "1019:         )",
          "1021:     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")",
          "",
          "[Removed Lines]",
          "1018:             mount_point=\"secret\", path=\"missing\", version=None",
          "",
          "[Added Lines]",
          "1018:             mount_point=\"secret\", path=\"missing\", version=None, raise_on_deleted_version=True",
          "",
          "---------------"
        ],
        "tests/providers/hashicorp/hooks/test_vault.py||tests/providers/hashicorp/hooks/test_vault.py": [
          "File: tests/providers/hashicorp/hooks/test_vault.py -> tests/providers/hashicorp/hooks/test_vault.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1005:         secret = test_hook.get_secret(secret_path=\"missing\")",
          "1006:         assert {\"secret_key\": \"secret_value\"} == secret",
          "1007:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "1009:         )",
          "1011:     @mock.patch(\"airflow.providers.hashicorp.hooks.vault.VaultHook.get_connection\")",
          "",
          "[Removed Lines]",
          "1008:             mount_point=\"secret\", path=\"missing\", version=None",
          "",
          "[Added Lines]",
          "1008:             mount_point=\"secret\", path=\"missing\", version=None, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1044:         secret = test_hook.get_secret(secret_path=\"missing\", secret_version=1)",
          "1045:         assert {\"secret_key\": \"secret_value\"} == secret",
          "1046:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "1048:         )",
          "1050:     @mock.patch(\"airflow.providers.hashicorp.hooks.vault.VaultHook.get_connection\")",
          "",
          "[Removed Lines]",
          "1047:             mount_point=\"secret\", path=\"missing\", version=1",
          "",
          "[Added Lines]",
          "1047:             mount_point=\"secret\", path=\"missing\", version=1, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1189:             \"auth\": None,",
          "1190:         } == metadata",
          "1191:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "1193:         )",
          "1195:     @mock.patch(\"airflow.providers.hashicorp.hooks.vault.VaultHook.get_connection\")",
          "",
          "[Removed Lines]",
          "1192:             mount_point=\"secret\", path=\"missing\", version=None",
          "",
          "[Added Lines]",
          "1192:             mount_point=\"secret\", path=\"missing\", version=None, raise_on_deleted_version=True",
          "",
          "---------------"
        ],
        "tests/providers/hashicorp/secrets/test_vault.py||tests/providers/hashicorp/secrets/test_vault.py": [
          "File: tests/providers/hashicorp/secrets/test_vault.py -> tests/providers/hashicorp/secrets/test_vault.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "302:         test_client = VaultBackend(**kwargs)",
          "303:         assert test_client.get_conn_uri(conn_id=\"test_mysql\") is None",
          "304:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "306:         )",
          "307:         assert test_client.get_connection(conn_id=\"test_mysql\") is None",
          "",
          "[Removed Lines]",
          "305:             mount_point=\"airflow\", path=\"connections/test_mysql\", version=None",
          "",
          "[Added Lines]",
          "305:             mount_point=\"airflow\", path=\"connections/test_mysql\", version=None, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "454:         test_client = VaultBackend(**kwargs)",
          "455:         assert test_client.get_variable(\"hello\") is None",
          "456:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "458:         )",
          "459:         assert test_client.get_variable(\"hello\") is None",
          "",
          "[Removed Lines]",
          "457:             mount_point=\"airflow\", path=\"variables/hello\", version=None",
          "",
          "[Added Lines]",
          "457:             mount_point=\"airflow\", path=\"variables/hello\", version=None, raise_on_deleted_version=True",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "6e31c0f2d75e6fc7f5360ca125890e4f97723b11",
      "candidate_info": {
        "commit_hash": "6e31c0f2d75e6fc7f5360ca125890e4f97723b11",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/6e31c0f2d75e6fc7f5360ca125890e4f97723b11",
        "files": [
          "airflow/timetables/_cron.py",
          "kubernetes_tests/test_kubernetes_pod_operator.py",
          "tests/api_connexion/test_parameters.py",
          "tests/providers/cncf/kubernetes/utils/test_pod_manager.py",
          "tests/providers/openlineage/plugins/test_utils.py",
          "tests/serialization/test_serialized_objects.py",
          "tests/timetables/test_events_timetable.py",
          "tests/timetables/test_interval_timetable.py",
          "tests/timetables/test_trigger_timetable.py",
          "tests/timetables/test_workday_timetable.py"
        ],
        "message": "Use UTC explicitly in timetable tests (#36082)\n\n(cherry picked from commit c1d28b36e4ecfad6df2e5c0d412c8b7f8d38c11d)",
        "before_after_code_files": [
          "airflow/timetables/_cron.py||airflow/timetables/_cron.py",
          "kubernetes_tests/test_kubernetes_pod_operator.py||kubernetes_tests/test_kubernetes_pod_operator.py",
          "tests/api_connexion/test_parameters.py||tests/api_connexion/test_parameters.py",
          "tests/providers/cncf/kubernetes/utils/test_pod_manager.py||tests/providers/cncf/kubernetes/utils/test_pod_manager.py",
          "tests/providers/openlineage/plugins/test_utils.py||tests/providers/openlineage/plugins/test_utils.py",
          "tests/serialization/test_serialized_objects.py||tests/serialization/test_serialized_objects.py",
          "tests/timetables/test_events_timetable.py||tests/timetables/test_events_timetable.py",
          "tests/timetables/test_interval_timetable.py||tests/timetables/test_interval_timetable.py",
          "tests/timetables/test_trigger_timetable.py||tests/timetables/test_trigger_timetable.py",
          "tests/timetables/test_workday_timetable.py||tests/timetables/test_workday_timetable.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/timetables/_cron.py||airflow/timetables/_cron.py": [
          "File: airflow/timetables/_cron.py -> airflow/timetables/_cron.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: import datetime",
          "20: from typing import TYPE_CHECKING, Any",
          "22: from cron_descriptor import CasingTypeEnum, ExpressionDescriptor, FormatException, MissingFieldException",
          "23: from croniter import CroniterBadCronError, CroniterBadDateError, croniter",
          "26: from airflow.exceptions import AirflowTimetableInvalid",
          "27: from airflow.utils.dates import cron_presets",
          "",
          "[Removed Lines]",
          "24: from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "22: import pendulum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "30: if TYPE_CHECKING:",
          "31:     from pendulum import DateTime",
          "34: def _covers_every_hour(cron: croniter) -> bool:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32:     from pendulum.tz.timezone import Timezone",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "66:         self._expression = cron_presets.get(cron, cron)",
          "68:         if isinstance(timezone, str):",
          "70:         self._timezone = timezone",
          "72:         try:",
          "",
          "[Removed Lines]",
          "69:             timezone = Timezone(timezone)",
          "",
          "[Added Lines]",
          "70:             timezone = pendulum.tz.timezone(timezone)",
          "",
          "---------------"
        ],
        "kubernetes_tests/test_kubernetes_pod_operator.py||kubernetes_tests/test_kubernetes_pod_operator.py": [
          "File: kubernetes_tests/test_kubernetes_pod_operator.py -> kubernetes_tests/test_kubernetes_pod_operator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: from unittest.mock import ANY, MagicMock",
          "27: from uuid import uuid4",
          "29: import pytest",
          "30: from kubernetes import client",
          "31: from kubernetes.client import V1EnvVar, V1PodSecurityContext, V1SecurityContext, models as k8s",
          "32: from kubernetes.client.api_client import ApiClient",
          "33: from kubernetes.client.rest import ApiException",
          "36: from airflow.exceptions import AirflowException, AirflowSkipException",
          "37: from airflow.models.connection import Connection",
          "",
          "[Removed Lines]",
          "34: from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "29: import pendulum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "54: def create_context(task) -> Context:",
          "55:     dag = DAG(dag_id=\"dag\")",
          "57:     dag_run = DagRun(",
          "58:         dag_id=dag.dag_id,",
          "59:         execution_date=execution_date,",
          "",
          "[Removed Lines]",
          "56:     execution_date = timezone.datetime(2016, 1, 1, 1, 0, 0, tzinfo=Timezone(\"Europe/Amsterdam\"))",
          "",
          "[Added Lines]",
          "56:     execution_date = timezone.datetime(2016, 1, 1, 1, 0, 0, tzinfo=pendulum.tz.timezone(\"Europe/Amsterdam\"))",
          "",
          "---------------"
        ],
        "tests/api_connexion/test_parameters.py||tests/api_connexion/test_parameters.py": [
          "File: tests/api_connexion/test_parameters.py -> tests/api_connexion/test_parameters.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: from unittest import mock",
          "21: import pytest",
          "25: from airflow.api_connexion.exceptions import BadRequest",
          "26: from airflow.api_connexion.parameters import (",
          "",
          "[Removed Lines]",
          "22: from pendulum import DateTime",
          "23: from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "21: import pendulum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "107:         decorated_endpoint(param_a=\"2020-01-01T0:0:00+00:00\")",
          "111:     def test_should_propagate_exceptions(self):",
          "112:         decorator = format_parameters({\"param_a\": format_datetime})",
          "",
          "[Removed Lines]",
          "109:         endpoint.assert_called_once_with(param_a=DateTime(2020, 1, 1, 0, tzinfo=Timezone(\"UTC\")))",
          "",
          "[Added Lines]",
          "108:         endpoint.assert_called_once_with(param_a=pendulum.datetime(2020, 1, 1, 0, tz=\"UTC\"))",
          "",
          "---------------"
        ],
        "tests/providers/cncf/kubernetes/utils/test_pod_manager.py||tests/providers/cncf/kubernetes/utils/test_pod_manager.py": [
          "File: tests/providers/cncf/kubernetes/utils/test_pod_manager.py -> tests/providers/cncf/kubernetes/utils/test_pod_manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: from datetime import datetime",
          "21: from json.decoder import JSONDecodeError",
          "22: from types import SimpleNamespace",
          "24: from unittest import mock",
          "25: from unittest.mock import MagicMock",
          "",
          "[Removed Lines]",
          "23: from typing import cast",
          "",
          "[Added Lines]",
          "23: from typing import TYPE_CHECKING, cast",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "28: import pytest",
          "29: import time_machine",
          "30: from kubernetes.client.rest import ApiException",
          "33: from urllib3.exceptions import HTTPError as BaseHTTPError",
          "35: from airflow.exceptions import AirflowException",
          "",
          "[Removed Lines]",
          "31: from pendulum import DateTime",
          "32: from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "43: )",
          "44: from airflow.utils.timezone import utc",
          "47: class TestPodManager:",
          "48:     def setup_method(self):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "44: if TYPE_CHECKING:",
          "45:     from pendulum import DateTime",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "270:         status = self.pod_manager.fetch_container_logs(mock.MagicMock(), mock.MagicMock(), follow=True)",
          "274:     @mock.patch(\"airflow.providers.cncf.kubernetes.utils.pod_manager.PodManager.container_is_running\")",
          "275:     @mock.patch(\"airflow.providers.cncf.kubernetes.utils.pod_manager.PodManager.read_pod_logs\")",
          "",
          "[Removed Lines]",
          "272:         assert status.last_log_time == cast(DateTime, pendulum.parse(timestamp_string))",
          "",
          "[Added Lines]",
          "273:         assert status.last_log_time == cast(\"DateTime\", pendulum.parse(timestamp_string))",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "306:             mock_consumer_iter.side_effect = consumer_iter",
          "307:             mock_container_is_running.side_effect = [True, True, False]",
          "308:             status = self.pod_manager.fetch_container_logs(mock.MagicMock(), mock.MagicMock(), follow=True)",
          "310:         assert self.mock_progress_callback.call_count == expected_call_count",
          "312:     @mock.patch(\"airflow.providers.cncf.kubernetes.utils.pod_manager.PodManager.container_is_running\")",
          "",
          "[Removed Lines]",
          "309:         assert status.last_log_time == cast(DateTime, pendulum.parse(last_timestamp_string))",
          "",
          "[Added Lines]",
          "310:         assert status.last_log_time == cast(\"DateTime\", pendulum.parse(last_timestamp_string))",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "461:     def test_fetch_container_since_time(self, logs_available, container_running, mock_now):",
          "462:         \"\"\"If given since_time, should be used.\"\"\"",
          "463:         mock_pod = MagicMock()",
          "465:         logs_available.return_value = True",
          "466:         container_running.return_value = False",
          "467:         self.mock_kube_client.read_namespaced_pod_log.return_value = mock.MagicMock(",
          "468:             stream=mock.MagicMock(return_value=[b\"2021-01-01 hi\"])",
          "469:         )",
          "471:         self.pod_manager.fetch_container_logs(pod=mock_pod, container_name=\"base\", since_time=since_time)",
          "472:         args, kwargs = self.mock_kube_client.read_namespaced_pod_log.call_args_list[0]",
          "473:         assert kwargs[\"since_seconds\"] == 5",
          "",
          "[Removed Lines]",
          "464:         mock_now.return_value = DateTime(2020, 1, 1, 0, 0, 5, tzinfo=Timezone(\"UTC\"))",
          "470:         since_time = DateTime(2020, 1, 1, tzinfo=Timezone(\"UTC\"))",
          "",
          "[Added Lines]",
          "465:         mock_now.return_value = pendulum.datetime(2020, 1, 1, 0, 0, 5, tz=\"UTC\")",
          "471:         since_time = pendulum.datetime(2020, 1, 1, tz=\"UTC\")",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "488:         )",
          "489:         ret = self.pod_manager.fetch_container_logs(pod=mock_pod, container_name=\"base\", follow=follow)",
          "490:         assert len(container_running_mock.call_args_list) == is_running_calls",
          "492:         assert ret.running is exp_running",
          "494:     @pytest.mark.parametrize(",
          "",
          "[Removed Lines]",
          "491:         assert ret.last_log_time == DateTime(2021, 1, 1, tzinfo=Timezone(\"UTC\"))",
          "",
          "[Added Lines]",
          "492:         assert ret.last_log_time == pendulum.datetime(2021, 1, 1, tz=\"UTC\")",
          "",
          "---------------"
        ],
        "tests/providers/openlineage/plugins/test_utils.py||tests/providers/openlineage/plugins/test_utils.py": [
          "File: tests/providers/openlineage/plugins/test_utils.py -> tests/providers/openlineage/plugins/test_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: from json import JSONEncoder",
          "24: from typing import Any",
          "26: import pytest",
          "27: from attrs import define",
          "28: from openlineage.client.utils import RedactMixin",
          "30: from pkg_resources import parse_version",
          "32: from airflow.models import DAG as AIRFLOW_DAG, DagModel",
          "",
          "[Removed Lines]",
          "29: from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "26: import pendulum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "86:         state=State.NONE, run_id=run_id, data_interval=dag.get_next_data_interval(dag_model)",
          "87:     )",
          "88:     assert dagrun.data_interval_start is not None",
          "91:     assert dagrun.data_interval_start, dagrun.data_interval_end == (start_date_tz, end_date_tz)",
          "",
          "[Removed Lines]",
          "89:     start_date_tz = datetime.datetime(2022, 1, 1, tzinfo=Timezone(\"UTC\"))",
          "90:     end_date_tz = datetime.datetime(2022, 1, 1, hour=2, tzinfo=Timezone(\"UTC\"))",
          "",
          "[Added Lines]",
          "89:     start_date_tz = datetime.datetime(2022, 1, 1, tzinfo=pendulum.tz.timezone(\"UTC\"))",
          "90:     end_date_tz = datetime.datetime(2022, 1, 1, hour=2, tzinfo=pendulum.tz.timezone(\"UTC\"))",
          "",
          "---------------"
        ],
        "tests/serialization/test_serialized_objects.py||tests/serialization/test_serialized_objects.py": [
          "File: tests/serialization/test_serialized_objects.py -> tests/serialization/test_serialized_objects.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import json",
          "21: from datetime import datetime, timedelta",
          "23: import pytest",
          "24: from dateutil import relativedelta",
          "25: from kubernetes.client import models as k8s",
          "28: from airflow.datasets import Dataset",
          "29: from airflow.exceptions import SerializationError",
          "",
          "[Removed Lines]",
          "26: from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "23: import pendulum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "142:         (1, None, equals),",
          "143:         (datetime.utcnow(), DAT.DATETIME, equal_time),",
          "144:         (timedelta(minutes=2), DAT.TIMEDELTA, equals),",
          "146:         (relativedelta.relativedelta(hours=+1), DAT.RELATIVEDELTA, lambda a, b: a.hours == b.hours),",
          "147:         ({\"test\": \"dict\", \"test-1\": 1}, None, equals),",
          "148:         ([\"array_item\", 2], None, equals),",
          "",
          "[Removed Lines]",
          "145:         (Timezone(\"UTC\"), DAT.TIMEZONE, lambda a, b: a.name == b.name),",
          "",
          "[Added Lines]",
          "145:         (pendulum.tz.timezone(\"UTC\"), DAT.TIMEZONE, lambda a, b: a.name == b.name),",
          "",
          "---------------"
        ],
        "tests/timetables/test_events_timetable.py||tests/timetables/test_events_timetable.py": [
          "File: tests/timetables/test_events_timetable.py -> tests/timetables/test_events_timetable.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import pendulum",
          "21: import pytest",
          "24: from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction, Timetable",
          "25: from airflow.timetables.events import EventsTimetable",
          "29: EVENT_DATES = [",
          "36: ]",
          "38: EVENT_DATES_SORTED = [",
          "44: ]",
          "50: @pytest.fixture()",
          "",
          "[Removed Lines]",
          "23: from airflow.settings import TIMEZONE",
          "27: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=TIMEZONE)  # Precedes all events",
          "30:     pendulum.DateTime(2021, 9, 6, tzinfo=TIMEZONE),",
          "31:     pendulum.DateTime(2021, 9, 7, tzinfo=TIMEZONE),",
          "32:     pendulum.DateTime(2021, 9, 8, tzinfo=TIMEZONE),",
          "33:     pendulum.DateTime(2021, 9, 8, tzinfo=TIMEZONE),  # deliberate duplicate, should be ignored",
          "34:     pendulum.DateTime(2021, 10, 9, tzinfo=TIMEZONE),  # deliberately out of order",
          "35:     pendulum.DateTime(2021, 9, 10, tzinfo=TIMEZONE),",
          "39:     pendulum.DateTime(2021, 9, 6, tzinfo=TIMEZONE),",
          "40:     pendulum.DateTime(2021, 9, 7, tzinfo=TIMEZONE),",
          "41:     pendulum.DateTime(2021, 9, 8, tzinfo=TIMEZONE),",
          "42:     pendulum.DateTime(2021, 9, 10, tzinfo=TIMEZONE),",
          "43:     pendulum.DateTime(2021, 10, 9, tzinfo=TIMEZONE),",
          "46: NON_EVENT_DATE = pendulum.DateTime(2021, 10, 1, tzinfo=TIMEZONE)",
          "47: MOST_RECENT_EVENT = pendulum.DateTime(2021, 9, 10, tzinfo=TIMEZONE)",
          "",
          "[Added Lines]",
          "25: from airflow.utils.timezone import utc",
          "27: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=utc)  # Precedes all events",
          "30:     pendulum.DateTime(2021, 9, 6, tzinfo=utc),",
          "31:     pendulum.DateTime(2021, 9, 7, tzinfo=utc),",
          "32:     pendulum.DateTime(2021, 9, 8, tzinfo=utc),",
          "33:     pendulum.DateTime(2021, 9, 8, tzinfo=utc),  # deliberate duplicate, should be ignored",
          "34:     pendulum.DateTime(2021, 10, 9, tzinfo=utc),  # deliberately out of order",
          "35:     pendulum.DateTime(2021, 9, 10, tzinfo=utc),",
          "39:     pendulum.DateTime(2021, 9, 6, tzinfo=utc),",
          "40:     pendulum.DateTime(2021, 9, 7, tzinfo=utc),",
          "41:     pendulum.DateTime(2021, 9, 8, tzinfo=utc),",
          "42:     pendulum.DateTime(2021, 9, 10, tzinfo=utc),",
          "43:     pendulum.DateTime(2021, 10, 9, tzinfo=utc),",
          "46: NON_EVENT_DATE = pendulum.DateTime(2021, 10, 1, tzinfo=utc)",
          "47: MOST_RECENT_EVENT = pendulum.DateTime(2021, 9, 10, tzinfo=utc)",
          "",
          "---------------"
        ],
        "tests/timetables/test_interval_timetable.py||tests/timetables/test_interval_timetable.py": [
          "File: tests/timetables/test_interval_timetable.py -> tests/timetables/test_interval_timetable.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: import time_machine",
          "27: from airflow.exceptions import AirflowTimetableInvalid",
          "29: from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction, Timetable",
          "30: from airflow.timetables.interval import CronDataIntervalTimetable, DeltaDataIntervalTimetable",
          "34: PREV_DATA_INTERVAL_START = START_DATE",
          "35: PREV_DATA_INTERVAL_END = START_DATE + datetime.timedelta(days=1)",
          "36: PREV_DATA_INTERVAL = DataInterval(start=PREV_DATA_INTERVAL_START, end=PREV_DATA_INTERVAL_END)",
          "37: PREV_DATA_INTERVAL_EXACT = DataInterval.exact(PREV_DATA_INTERVAL_END)",
          "40: YESTERDAY = CURRENT_TIME - datetime.timedelta(days=1)",
          "41: OLD_INTERVAL = DataInterval(start=YESTERDAY, end=CURRENT_TIME)",
          "44: HOURLY_TIMEDELTA_TIMETABLE = DeltaDataIntervalTimetable(datetime.timedelta(hours=1))",
          "45: HOURLY_RELATIVEDELTA_TIMETABLE = DeltaDataIntervalTimetable(dateutil.relativedelta.relativedelta(hours=1))",
          "48: DELTA_FROM_MIDNIGHT = datetime.timedelta(minutes=30, hours=16)",
          "",
          "[Removed Lines]",
          "28: from airflow.settings import TIMEZONE",
          "32: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=TIMEZONE)",
          "39: CURRENT_TIME = pendulum.DateTime(2021, 9, 7, tzinfo=TIMEZONE)",
          "43: HOURLY_CRON_TIMETABLE = CronDataIntervalTimetable(\"@hourly\", TIMEZONE)",
          "47: CRON_TIMETABLE = CronDataIntervalTimetable(\"30 16 * * *\", TIMEZONE)",
          "",
          "[Added Lines]",
          "30: from airflow.utils.timezone import utc",
          "32: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=utc)",
          "39: CURRENT_TIME = pendulum.DateTime(2021, 9, 7, tzinfo=utc)",
          "43: HOURLY_CRON_TIMETABLE = CronDataIntervalTimetable(\"@hourly\", utc)",
          "47: CRON_TIMETABLE = CronDataIntervalTimetable(\"30 16 * * *\", utc)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "148:     \"timetable, error_message\",",
          "149:     [",
          "150:         pytest.param(",
          "152:             \"[0 0 1 13 0] is not acceptable, out of range\",",
          "153:             id=\"invalid-cron\",",
          "154:         ),",
          "",
          "[Removed Lines]",
          "151:             CronDataIntervalTimetable(\"0 0 1 13 0\", TIMEZONE),",
          "",
          "[Added Lines]",
          "151:             CronDataIntervalTimetable(\"0 0 1 13 0\", utc),",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "191:     [",
          "192:         # Arbitrary trigger time.",
          "193:         pytest.param(",
          "195:             DataInterval(",
          "198:             ),",
          "199:             id=\"adhoc\",",
          "200:         ),",
          "201:         # Trigger time falls exactly on interval boundary.",
          "202:         pytest.param(",
          "204:             DataInterval(",
          "207:             ),",
          "208:             id=\"exact\",",
          "209:         ),",
          "",
          "[Removed Lines]",
          "194:             pendulum.DateTime(2022, 8, 8, 1, tzinfo=TIMEZONE),",
          "196:                 pendulum.DateTime(2022, 8, 7, tzinfo=TIMEZONE),",
          "197:                 pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "203:             pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "205:                 pendulum.DateTime(2022, 8, 7, tzinfo=TIMEZONE),",
          "206:                 pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "",
          "[Added Lines]",
          "194:             pendulum.DateTime(2022, 8, 8, 1, tzinfo=utc),",
          "196:                 pendulum.DateTime(2022, 8, 7, tzinfo=utc),",
          "197:                 pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "203:             pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "205:                 pendulum.DateTime(2022, 8, 7, tzinfo=utc),",
          "206:                 pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "213:     trigger_at: pendulum.DateTime,",
          "214:     expected_interval: DataInterval,",
          "215: ) -> None:",
          "217:     assert timetable.infer_manual_data_interval(run_after=trigger_at) == expected_interval",
          "",
          "[Removed Lines]",
          "216:     timetable = CronDataIntervalTimetable(\"@daily\", TIMEZONE)",
          "",
          "[Added Lines]",
          "216:     timetable = CronDataIntervalTimetable(\"@daily\", utc)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "222:     [",
          "223:         pytest.param(",
          "224:             DataInterval(",
          "227:             ),",
          "228:             DagRunInfo.interval(",
          "231:             ),",
          "232:             id=\"exact\",",
          "233:         ),",
          "",
          "[Removed Lines]",
          "225:                 pendulum.DateTime(2022, 8, 7, tzinfo=TIMEZONE),",
          "226:                 pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "229:                 pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "230:                 pendulum.DateTime(2022, 8, 9, tzinfo=TIMEZONE),",
          "",
          "[Added Lines]",
          "225:                 pendulum.DateTime(2022, 8, 7, tzinfo=utc),",
          "226:                 pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "229:                 pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "230:                 pendulum.DateTime(2022, 8, 9, tzinfo=utc),",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "235:             # Previous data interval does not align with the current timetable.",
          "236:             # This is possible if the user edits a DAG with existing runs.",
          "237:             DataInterval(",
          "240:             ),",
          "241:             DagRunInfo.interval(",
          "244:             ),",
          "245:             id=\"changed\",",
          "246:         ),",
          "247:     ],",
          "248: )",
          "249: def test_cron_next_dagrun_info_alignment(last_data_interval: DataInterval, expected_info: DagRunInfo):",
          "251:     info = timetable.next_dagrun_info(",
          "252:         last_automated_data_interval=last_data_interval,",
          "253:         restriction=TimeRestriction(None, None, True),",
          "",
          "[Removed Lines]",
          "238:                 pendulum.DateTime(2022, 8, 7, 1, tzinfo=TIMEZONE),",
          "239:                 pendulum.DateTime(2022, 8, 8, 1, tzinfo=TIMEZONE),",
          "242:                 pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "243:                 pendulum.DateTime(2022, 8, 9, tzinfo=TIMEZONE),",
          "250:     timetable = CronDataIntervalTimetable(\"@daily\", TIMEZONE)",
          "",
          "[Added Lines]",
          "238:                 pendulum.DateTime(2022, 8, 7, 1, tzinfo=utc),",
          "239:                 pendulum.DateTime(2022, 8, 8, 1, tzinfo=utc),",
          "242:                 pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "243:                 pendulum.DateTime(2022, 8, 9, tzinfo=utc),",
          "250:     timetable = CronDataIntervalTimetable(\"@daily\", utc)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "269:     def test_entering_exact(self) -> None:",
          "270:         timetable = CronDataIntervalTimetable(\"0 3 * * *\", timezone=\"Europe/Zurich\")",
          "271:         restriction = TimeRestriction(",
          "273:             latest=None,",
          "274:             catchup=True,",
          "275:         )",
          "",
          "[Removed Lines]",
          "272:             earliest=pendulum.datetime(2023, 3, 24, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "272:             earliest=pendulum.datetime(2023, 3, 24, tz=utc),",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "277:         # Last run before DST. Interval starts and ends on 2am UTC (local time is +1).",
          "278:         next_info = timetable.next_dagrun_info(last_automated_data_interval=None, restriction=restriction)",
          "279:         assert next_info and next_info.data_interval == DataInterval(",
          "282:         )",
          "284:         # Crossing the DST switch. Interval starts on 2am UTC (local time +1)",
          "",
          "[Removed Lines]",
          "280:             pendulum.datetime(2023, 3, 24, 2, tz=TIMEZONE),",
          "281:             pendulum.datetime(2023, 3, 25, 2, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "280:             pendulum.datetime(2023, 3, 24, 2, tz=utc),",
          "281:             pendulum.datetime(2023, 3, 25, 2, tz=utc),",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "288:             restriction=restriction,",
          "289:         )",
          "290:         assert next_info and next_info.data_interval == DataInterval(",
          "293:         )",
          "295:         # In DST. Interval starts and ends on 1am UTC (local time is +2).",
          "",
          "[Removed Lines]",
          "291:             pendulum.datetime(2023, 3, 25, 2, tz=TIMEZONE),",
          "292:             pendulum.datetime(2023, 3, 26, 1, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "291:             pendulum.datetime(2023, 3, 25, 2, tz=utc),",
          "292:             pendulum.datetime(2023, 3, 26, 1, tz=utc),",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "298:             restriction=restriction,",
          "299:         )",
          "300:         assert next_info and next_info.data_interval == DataInterval(",
          "303:         )",
          "305:     def test_entering_skip(self) -> None:",
          "306:         timetable = CronDataIntervalTimetable(\"0 2 * * *\", timezone=\"Europe/Zurich\")",
          "307:         restriction = TimeRestriction(",
          "309:             latest=None,",
          "310:             catchup=True,",
          "311:         )",
          "",
          "[Removed Lines]",
          "301:             pendulum.datetime(2023, 3, 26, 1, tz=TIMEZONE),",
          "302:             pendulum.datetime(2023, 3, 27, 1, tz=TIMEZONE),",
          "308:             earliest=pendulum.datetime(2023, 3, 24, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "301:             pendulum.datetime(2023, 3, 26, 1, tz=utc),",
          "302:             pendulum.datetime(2023, 3, 27, 1, tz=utc),",
          "308:             earliest=pendulum.datetime(2023, 3, 24, tz=utc),",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "313:         # Last run before DST. Interval starts and ends on 1am UTC (local time is +1).",
          "314:         next_info = timetable.next_dagrun_info(last_automated_data_interval=None, restriction=restriction)",
          "315:         assert next_info and next_info.data_interval == DataInterval(",
          "318:         )",
          "320:         # Crossing the DST switch. Interval starts on 1am UTC (local time +1)",
          "",
          "[Removed Lines]",
          "316:             pendulum.datetime(2023, 3, 24, 1, tz=TIMEZONE),",
          "317:             pendulum.datetime(2023, 3, 25, 1, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "316:             pendulum.datetime(2023, 3, 24, 1, tz=utc),",
          "317:             pendulum.datetime(2023, 3, 25, 1, tz=utc),",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "325:             restriction=restriction,",
          "326:         )",
          "327:         assert next_info and next_info.data_interval == DataInterval(",
          "330:         )",
          "332:         # In DST. Interval starts on 1am UTC (local time is +2 but 2am local",
          "",
          "[Removed Lines]",
          "328:             pendulum.datetime(2023, 3, 25, 1, tz=TIMEZONE),",
          "329:             pendulum.datetime(2023, 3, 26, 1, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "328:             pendulum.datetime(2023, 3, 25, 1, tz=utc),",
          "329:             pendulum.datetime(2023, 3, 26, 1, tz=utc),",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "336:             restriction=restriction,",
          "337:         )",
          "338:         assert next_info and next_info.data_interval == DataInterval(",
          "341:         )",
          "343:     def test_exiting_exact(self) -> None:",
          "344:         timetable = CronDataIntervalTimetable(\"0 3 * * *\", timezone=\"Europe/Zurich\")",
          "345:         restriction = TimeRestriction(",
          "347:             latest=None,",
          "348:             catchup=True,",
          "349:         )",
          "",
          "[Removed Lines]",
          "339:             pendulum.datetime(2023, 3, 26, 1, tz=TIMEZONE),",
          "340:             pendulum.datetime(2023, 3, 27, 0, tz=TIMEZONE),",
          "346:             earliest=pendulum.datetime(2023, 10, 27, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "339:             pendulum.datetime(2023, 3, 26, 1, tz=utc),",
          "340:             pendulum.datetime(2023, 3, 27, 0, tz=utc),",
          "346:             earliest=pendulum.datetime(2023, 10, 27, tz=utc),",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "351:         # Last run in DST. Interval starts and ends on 1am UTC (local time is +2).",
          "352:         next_info = timetable.next_dagrun_info(last_automated_data_interval=None, restriction=restriction)",
          "353:         assert next_info and next_info.data_interval == DataInterval(",
          "356:         )",
          "358:         # Crossing the DST switch. Interval starts on 1am UTC (local time +2)",
          "",
          "[Removed Lines]",
          "354:             pendulum.datetime(2023, 10, 27, 1, tz=TIMEZONE),",
          "355:             pendulum.datetime(2023, 10, 28, 1, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "354:             pendulum.datetime(2023, 10, 27, 1, tz=utc),",
          "355:             pendulum.datetime(2023, 10, 28, 1, tz=utc),",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "362:             restriction=restriction,",
          "363:         )",
          "364:         assert next_info and next_info.data_interval == DataInterval(",
          "367:         )",
          "369:         # Out of DST. Interval starts and ends on 2am UTC (local time is +1).",
          "",
          "[Removed Lines]",
          "365:             pendulum.datetime(2023, 10, 28, 1, tz=TIMEZONE),",
          "366:             pendulum.datetime(2023, 10, 29, 2, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "365:             pendulum.datetime(2023, 10, 28, 1, tz=utc),",
          "366:             pendulum.datetime(2023, 10, 29, 2, tz=utc),",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "372:             restriction=restriction,",
          "373:         )",
          "374:         assert next_info and next_info.data_interval == DataInterval(",
          "377:         )",
          "379:     def test_exiting_fold(self) -> None:",
          "380:         timetable = CronDataIntervalTimetable(\"0 2 * * *\", timezone=\"Europe/Zurich\")",
          "381:         restriction = TimeRestriction(",
          "383:             latest=None,",
          "384:             catchup=True,",
          "385:         )",
          "",
          "[Removed Lines]",
          "375:             pendulum.datetime(2023, 10, 29, 2, tz=TIMEZONE),",
          "376:             pendulum.datetime(2023, 10, 30, 2, tz=TIMEZONE),",
          "382:             earliest=pendulum.datetime(2023, 10, 27, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "375:             pendulum.datetime(2023, 10, 29, 2, tz=utc),",
          "376:             pendulum.datetime(2023, 10, 30, 2, tz=utc),",
          "382:             earliest=pendulum.datetime(2023, 10, 27, tz=utc),",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "388:         # time is +2).",
          "389:         next_info = timetable.next_dagrun_info(last_automated_data_interval=None, restriction=restriction)",
          "390:         assert next_info and next_info.data_interval == DataInterval(",
          "393:         )",
          "395:         # Account for folding. Interval starts on 0am UTC (local time +2) and",
          "",
          "[Removed Lines]",
          "391:             pendulum.datetime(2023, 10, 27, 0, tz=TIMEZONE),",
          "392:             pendulum.datetime(2023, 10, 28, 0, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "391:             pendulum.datetime(2023, 10, 27, 0, tz=utc),",
          "392:             pendulum.datetime(2023, 10, 28, 0, tz=utc),",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "402:             restriction=restriction,",
          "403:         )",
          "404:         assert next_info and next_info.data_interval == DataInterval(",
          "407:         )",
          "409:         # Stepping out of DST. Interval starts from the folded 2am local time",
          "",
          "[Removed Lines]",
          "405:             pendulum.datetime(2023, 10, 28, 0, tz=TIMEZONE),",
          "406:             pendulum.datetime(2023, 10, 29, 1, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "405:             pendulum.datetime(2023, 10, 28, 0, tz=utc),",
          "406:             pendulum.datetime(2023, 10, 29, 1, tz=utc),",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "414:             restriction=restriction,",
          "415:         )",
          "416:         assert next_info and next_info.data_interval == DataInterval(",
          "419:         )",
          "",
          "[Removed Lines]",
          "417:             pendulum.datetime(2023, 10, 29, 1, tz=TIMEZONE),",
          "418:             pendulum.datetime(2023, 10, 30, 1, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "417:             pendulum.datetime(2023, 10, 29, 1, tz=utc),",
          "418:             pendulum.datetime(2023, 10, 30, 1, tz=utc),",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "429:     def test_7_to_8_entering(self):",
          "430:         timetable = CronDataIntervalTimetable(\"0 7-8 * * *\", timezone=\"America/Los_Angeles\")",
          "431:         restriction = TimeRestriction(",
          "433:             latest=None,",
          "434:             catchup=True,",
          "435:         )",
          "",
          "[Removed Lines]",
          "432:             earliest=pendulum.datetime(2020, 3, 7, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "432:             earliest=pendulum.datetime(2020, 3, 7, tz=utc),",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "440:             restriction=restriction,",
          "441:         )",
          "442:         assert next_info and next_info.data_interval == DataInterval(",
          "445:         )",
          "447:         # This interval ends an hour early since it includes the DST switch!",
          "",
          "[Removed Lines]",
          "443:             pendulum.datetime(2020, 3, 7, 7 + 8, tz=TIMEZONE),",
          "444:             pendulum.datetime(2020, 3, 7, 8 + 8, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "443:             pendulum.datetime(2020, 3, 7, 7 + 8, tz=utc),",
          "444:             pendulum.datetime(2020, 3, 7, 8 + 8, tz=utc),",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "450:             restriction=restriction,",
          "451:         )",
          "452:         assert next_info and next_info.data_interval == DataInterval(",
          "455:         )",
          "457:         # We're fully into DST so the interval is as expected.",
          "",
          "[Removed Lines]",
          "453:             pendulum.datetime(2020, 3, 7, 8 + 8, tz=TIMEZONE),",
          "454:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "453:             pendulum.datetime(2020, 3, 7, 8 + 8, tz=utc),",
          "454:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=utc),",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "460:             restriction=restriction,",
          "461:         )",
          "462:         assert next_info and next_info.data_interval == DataInterval(",
          "465:         )",
          "467:     def test_7_and_9_entering(self):",
          "468:         timetable = CronDataIntervalTimetable(\"0 7,9 * * *\", timezone=\"America/Los_Angeles\")",
          "469:         restriction = TimeRestriction(",
          "471:             latest=None,",
          "472:             catchup=True,",
          "473:         )",
          "",
          "[Removed Lines]",
          "463:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=TIMEZONE),",
          "464:             pendulum.datetime(2020, 3, 8, 8 + 7, tz=TIMEZONE),",
          "470:             earliest=pendulum.datetime(2020, 3, 7, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "463:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=utc),",
          "464:             pendulum.datetime(2020, 3, 8, 8 + 7, tz=utc),",
          "470:             earliest=pendulum.datetime(2020, 3, 7, tz=utc),",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "478:             restriction=restriction,",
          "479:         )",
          "480:         assert next_info and next_info.data_interval == DataInterval(",
          "483:         )",
          "485:         # This interval ends an hour early since it includes the DST switch!",
          "",
          "[Removed Lines]",
          "481:             pendulum.datetime(2020, 3, 7, 7 + 8, tz=TIMEZONE),",
          "482:             pendulum.datetime(2020, 3, 7, 9 + 8, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "481:             pendulum.datetime(2020, 3, 7, 7 + 8, tz=utc),",
          "482:             pendulum.datetime(2020, 3, 7, 9 + 8, tz=utc),",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "488:             restriction=restriction,",
          "489:         )",
          "490:         assert next_info and next_info.data_interval == DataInterval(",
          "493:         )",
          "495:         # We're fully into DST so the interval is as expected.",
          "",
          "[Removed Lines]",
          "491:             pendulum.datetime(2020, 3, 7, 9 + 8, tz=TIMEZONE),",
          "492:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "491:             pendulum.datetime(2020, 3, 7, 9 + 8, tz=utc),",
          "492:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=utc),",
          "",
          "---------------",
          "--- Hunk 26 ---",
          "[Context before]",
          "498:             restriction=restriction,",
          "499:         )",
          "500:         assert next_info and next_info.data_interval == DataInterval(",
          "503:         )",
          "506: def test_fold_scheduling():",
          "507:     timetable = CronDataIntervalTimetable(\"*/30 * * * *\", timezone=\"Europe/Zurich\")",
          "508:     restriction = TimeRestriction(",
          "510:         latest=None,",
          "511:         catchup=True,",
          "512:     )",
          "",
          "[Removed Lines]",
          "501:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=TIMEZONE),",
          "502:             pendulum.datetime(2020, 3, 8, 9 + 7, tz=TIMEZONE),",
          "509:         earliest=pendulum.datetime(2023, 10, 28, 23, 30, tz=TIMEZONE),  # Locally 1:30 (DST).",
          "",
          "[Added Lines]",
          "501:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=utc),",
          "502:             pendulum.datetime(2020, 3, 8, 9 + 7, tz=utc),",
          "509:         earliest=pendulum.datetime(2023, 10, 28, 23, 30, tz=utc),  # Locally 1:30 (DST).",
          "",
          "---------------",
          "--- Hunk 27 ---",
          "[Context before]",
          "517:         restriction=restriction,",
          "518:     )",
          "519:     assert next_info and next_info.data_interval == DataInterval(",
          "522:     )",
          "523:     next_info = timetable.next_dagrun_info(",
          "524:         last_automated_data_interval=next_info.data_interval,",
          "525:         restriction=restriction,",
          "526:     )",
          "527:     assert next_info and next_info.data_interval == DataInterval(",
          "530:     )",
          "532:     # Crossing into fold.",
          "",
          "[Removed Lines]",
          "520:         pendulum.datetime(2023, 10, 28, 23, 30, tz=TIMEZONE),",
          "521:         pendulum.datetime(2023, 10, 29, 0, 0, tz=TIMEZONE),  # Locally 2am (DST).",
          "528:         pendulum.datetime(2023, 10, 29, 0, 0, tz=TIMEZONE),",
          "529:         pendulum.datetime(2023, 10, 29, 0, 30, tz=TIMEZONE),  # Locally 2:30 (DST).",
          "",
          "[Added Lines]",
          "520:         pendulum.datetime(2023, 10, 28, 23, 30, tz=utc),",
          "521:         pendulum.datetime(2023, 10, 29, 0, 0, tz=utc),  # Locally 2am (DST).",
          "528:         pendulum.datetime(2023, 10, 29, 0, 0, tz=utc),",
          "529:         pendulum.datetime(2023, 10, 29, 0, 30, tz=utc),  # Locally 2:30 (DST).",
          "",
          "---------------",
          "--- Hunk 28 ---",
          "[Context before]",
          "535:         restriction=restriction,",
          "536:     )",
          "537:     assert next_info and next_info.data_interval == DataInterval(",
          "540:     )",
          "542:     # In the \"fold zone\".",
          "",
          "[Removed Lines]",
          "538:         pendulum.datetime(2023, 10, 29, 0, 30, tz=TIMEZONE),",
          "539:         pendulum.datetime(2023, 10, 29, 1, 0, tz=TIMEZONE),  # Locally 2am (fold, not DST).",
          "",
          "[Added Lines]",
          "538:         pendulum.datetime(2023, 10, 29, 0, 30, tz=utc),",
          "539:         pendulum.datetime(2023, 10, 29, 1, 0, tz=utc),  # Locally 2am (fold, not DST).",
          "",
          "---------------",
          "--- Hunk 29 ---",
          "[Context before]",
          "545:         restriction=restriction,",
          "546:     )",
          "547:     assert next_info and next_info.data_interval == DataInterval(",
          "550:     )",
          "552:     # Stepping out of fold.",
          "",
          "[Removed Lines]",
          "548:         pendulum.datetime(2023, 10, 29, 1, 0, tz=TIMEZONE),",
          "549:         pendulum.datetime(2023, 10, 29, 1, 30, tz=TIMEZONE),  # Locally 2am (fold, not DST).",
          "",
          "[Added Lines]",
          "548:         pendulum.datetime(2023, 10, 29, 1, 0, tz=utc),",
          "549:         pendulum.datetime(2023, 10, 29, 1, 30, tz=utc),  # Locally 2am (fold, not DST).",
          "",
          "---------------",
          "--- Hunk 30 ---",
          "[Context before]",
          "555:         restriction=restriction,",
          "556:     )",
          "557:     assert next_info and next_info.data_interval == DataInterval(",
          "560:     )",
          "",
          "[Removed Lines]",
          "558:         pendulum.datetime(2023, 10, 29, 1, 30, tz=TIMEZONE),",
          "559:         pendulum.datetime(2023, 10, 29, 2, 0, tz=TIMEZONE),  # Locally 3am (not DST).",
          "",
          "[Added Lines]",
          "558:         pendulum.datetime(2023, 10, 29, 1, 30, tz=utc),",
          "559:         pendulum.datetime(2023, 10, 29, 2, 0, tz=utc),  # Locally 3am (not DST).",
          "",
          "---------------"
        ],
        "tests/timetables/test_trigger_timetable.py||tests/timetables/test_trigger_timetable.py": [
          "File: tests/timetables/test_trigger_timetable.py -> tests/timetables/test_trigger_timetable.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import dateutil.relativedelta",
          "23: import pendulum",
          "25: import pytest",
          "26: import time_machine",
          "28: from airflow.exceptions import AirflowTimetableInvalid",
          "29: from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction",
          "30: from airflow.timetables.trigger import CronTriggerTimetable",
          "35: PREV_DATA_INTERVAL_END = START_DATE + datetime.timedelta(days=1)",
          "36: PREV_DATA_INTERVAL_EXACT = DataInterval.exact(PREV_DATA_INTERVAL_END)",
          "39: YESTERDAY = CURRENT_TIME - datetime.timedelta(days=1)",
          "43: DELTA_FROM_MIDNIGHT = datetime.timedelta(minutes=30, hours=16)",
          "",
          "[Removed Lines]",
          "24: import pendulum.tz",
          "32: TIMEZONE = pendulum.tz.timezone(\"UTC\")",
          "33: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=TIMEZONE)",
          "38: CURRENT_TIME = pendulum.DateTime(2021, 9, 7, tzinfo=TIMEZONE)",
          "41: HOURLY_CRON_TRIGGER_TIMETABLE = CronTriggerTimetable(\"@hourly\", timezone=TIMEZONE)",
          "",
          "[Added Lines]",
          "30: from airflow.utils.timezone import utc",
          "32: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=utc)",
          "37: CURRENT_TIME = pendulum.DateTime(2021, 9, 7, tzinfo=utc)",
          "40: HOURLY_CRON_TRIGGER_TIMETABLE = CronTriggerTimetable(\"@hourly\", timezone=utc)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "69:     next_start_time: pendulum.DateTime,",
          "70: ) -> None:",
          "71:     \"\"\"If ``catchup=False`` and start_date is a day before\"\"\"",
          "73:     next_info = timetable.next_dagrun_info(",
          "74:         last_automated_data_interval=last_automated_data_interval,",
          "75:         restriction=TimeRestriction(earliest=YESTERDAY, latest=None, catchup=False),",
          "",
          "[Removed Lines]",
          "72:     timetable = CronTriggerTimetable(\"30 16 * * *\", timezone=TIMEZONE)",
          "",
          "[Added Lines]",
          "71:     timetable = CronTriggerTimetable(\"30 16 * * *\", timezone=utc)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "81:     \"current_time, earliest, expected\",",
          "82:     [",
          "83:         pytest.param(",
          "85:             START_DATE,",
          "87:             id=\"current_time_on_boundary\",",
          "88:         ),",
          "89:         pytest.param(",
          "91:             START_DATE,",
          "93:             id=\"current_time_not_on_boundary\",",
          "94:         ),",
          "95:         pytest.param(",
          "97:             START_DATE,",
          "99:             id=\"current_time_miss_one_interval_on_boundary\",",
          "100:         ),",
          "101:         pytest.param(",
          "103:             START_DATE,",
          "105:             id=\"current_time_miss_one_interval_not_on_boundary\",",
          "106:         ),",
          "107:         pytest.param(",
          "111:             id=\"future_start_date\",",
          "112:         ),",
          "113:     ],",
          "",
          "[Removed Lines]",
          "84:             pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=TIMEZONE),",
          "86:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=TIMEZONE)),",
          "90:             pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=TIMEZONE),",
          "92:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=TIMEZONE)),",
          "96:             pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=TIMEZONE),",
          "98:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=TIMEZONE)),",
          "102:             pendulum.DateTime(2022, 7, 27, 1, 30, 0, tzinfo=TIMEZONE),",
          "104:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=TIMEZONE)),",
          "108:             pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=TIMEZONE),",
          "109:             pendulum.DateTime(2199, 12, 31, 22, 30, 0, tzinfo=TIMEZONE),",
          "110:             DagRunInfo.exact(pendulum.DateTime(2199, 12, 31, 23, 0, 0, tzinfo=TIMEZONE)),",
          "",
          "[Added Lines]",
          "83:             pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=utc),",
          "85:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=utc)),",
          "89:             pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=utc),",
          "91:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=utc)),",
          "95:             pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=utc),",
          "97:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=utc)),",
          "101:             pendulum.DateTime(2022, 7, 27, 1, 30, 0, tzinfo=utc),",
          "103:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=utc)),",
          "107:             pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=utc),",
          "108:             pendulum.DateTime(2199, 12, 31, 22, 30, 0, tzinfo=utc),",
          "109:             DagRunInfo.exact(pendulum.DateTime(2199, 12, 31, 23, 0, 0, tzinfo=utc)),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "129:     \"last_automated_data_interval, earliest, expected\",",
          "130:     [",
          "131:         pytest.param(",
          "133:             START_DATE,",
          "135:             id=\"last_automated_on_boundary\",",
          "136:         ),",
          "137:         pytest.param(",
          "139:             START_DATE,",
          "141:             id=\"last_automated_not_on_boundary\",",
          "142:         ),",
          "143:         pytest.param(",
          "144:             None,",
          "147:             id=\"no_last_automated_with_earliest_on_boundary\",",
          "148:         ),",
          "149:         pytest.param(",
          "150:             None,",
          "153:             id=\"no_last_automated_with_earliest_not_on_boundary\",",
          "154:         ),",
          "155:         pytest.param(",
          "",
          "[Removed Lines]",
          "132:             DataInterval.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=TIMEZONE)),",
          "134:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=TIMEZONE)),",
          "138:             DataInterval.exact(pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=TIMEZONE)),",
          "140:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=TIMEZONE)),",
          "145:             pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=TIMEZONE),",
          "146:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=TIMEZONE)),",
          "151:             pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=TIMEZONE),",
          "152:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=TIMEZONE)),",
          "",
          "[Added Lines]",
          "131:             DataInterval.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=utc)),",
          "133:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=utc)),",
          "137:             DataInterval.exact(pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=utc)),",
          "139:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=utc)),",
          "144:             pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=utc),",
          "145:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=utc)),",
          "150:             pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=utc),",
          "151:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=utc)),",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "176:     # Runs every Monday on 16:30, covering the day before the run.",
          "177:     timetable = CronTriggerTimetable(",
          "178:         \"30 16 * * MON\",",
          "180:         interval=datetime.timedelta(hours=16, minutes=30),",
          "181:     )",
          "183:     next_info = timetable.next_dagrun_info(",
          "184:         last_automated_data_interval=DataInterval(",
          "187:         ),",
          "188:         restriction=TimeRestriction(earliest=START_DATE, latest=None, catchup=True),",
          "189:     )",
          "190:     assert next_info == DagRunInfo.interval(",
          "193:     )",
          "",
          "[Removed Lines]",
          "179:         timezone=TIMEZONE,",
          "185:             pendulum.DateTime(2022, 8, 1, tzinfo=TIMEZONE),",
          "186:             pendulum.DateTime(2022, 8, 1, 16, 30, tzinfo=TIMEZONE),",
          "191:         pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "192:         pendulum.DateTime(2022, 8, 8, 16, 30, tzinfo=TIMEZONE),",
          "",
          "[Added Lines]",
          "178:         timezone=utc,",
          "184:             pendulum.DateTime(2022, 8, 1, tzinfo=utc),",
          "185:             pendulum.DateTime(2022, 8, 1, 16, 30, tzinfo=utc),",
          "190:         pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "191:         pendulum.DateTime(2022, 8, 8, 16, 30, tzinfo=utc),",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "200: def test_validate_failure() -> None:",
          "203:     with pytest.raises(AirflowTimetableInvalid) as ctx:",
          "204:         timetable.validate()",
          "",
          "[Removed Lines]",
          "201:     timetable = CronTriggerTimetable(\"0 0 1 13 0\", timezone=TIMEZONE)",
          "",
          "[Added Lines]",
          "200:     timetable = CronTriggerTimetable(\"0 0 1 13 0\", timezone=utc)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "210:     [",
          "211:         (HOURLY_CRON_TRIGGER_TIMETABLE, {\"expression\": \"0 * * * *\", \"timezone\": \"UTC\", \"interval\": 0}),",
          "212:         (",
          "214:             {\"expression\": \"0 0 1 12 *\", \"timezone\": \"UTC\", \"interval\": 7200.0},",
          "215:         ),",
          "216:         (",
          "217:             CronTriggerTimetable(",
          "218:                 \"0 0 1 12 0\",",
          "220:                 interval=dateutil.relativedelta.relativedelta(weekday=dateutil.relativedelta.MO),",
          "221:             ),",
          "222:             {\"expression\": \"0 0 1 12 0\", \"timezone\": \"Asia/Taipei\", \"interval\": {\"weekday\": [0]}},",
          "",
          "[Removed Lines]",
          "213:             CronTriggerTimetable(\"0 0 1 12 *\", timezone=TIMEZONE, interval=datetime.timedelta(hours=2)),",
          "219:                 timezone=pendulum.tz.timezone(\"Asia/Taipei\"),",
          "",
          "[Added Lines]",
          "212:             CronTriggerTimetable(\"0 0 1 12 *\", timezone=utc, interval=datetime.timedelta(hours=2)),",
          "218:                 timezone=\"Asia/Taipei\",",
          "",
          "---------------"
        ],
        "tests/timetables/test_workday_timetable.py||tests/timetables/test_workday_timetable.py": [
          "File: tests/timetables/test_workday_timetable.py -> tests/timetables/test_workday_timetable.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import pytest",
          "25: from airflow.example_dags.plugins.workday import AfterWorkdayTimetable",
          "27: from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction, Timetable",
          "31: WEEK_1_WEEKDAYS = [",
          "37: ]",
          "45: @pytest.fixture()",
          "",
          "[Removed Lines]",
          "26: from airflow.settings import TIMEZONE",
          "29: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=TIMEZONE)  # This is a Saturday.",
          "32:     pendulum.DateTime(2021, 9, 6, tzinfo=TIMEZONE),  # This is a US holiday",
          "33:     pendulum.DateTime(2021, 9, 7, tzinfo=TIMEZONE),",
          "34:     pendulum.DateTime(2021, 9, 8, tzinfo=TIMEZONE),",
          "35:     pendulum.DateTime(2021, 9, 9, tzinfo=TIMEZONE),",
          "36:     pendulum.DateTime(2021, 9, 10, tzinfo=TIMEZONE),",
          "39: WEEK_1_SATURDAY = pendulum.DateTime(2021, 9, 11, tzinfo=TIMEZONE)",
          "41: WEEK_2_MONDAY = pendulum.DateTime(2021, 9, 13, tzinfo=TIMEZONE)",
          "42: WEEK_2_TUESDAY = pendulum.DateTime(2021, 9, 14, tzinfo=TIMEZONE)",
          "",
          "[Added Lines]",
          "27: from airflow.utils.timezone import utc",
          "29: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=utc)  # This is a Saturday.",
          "32:     pendulum.DateTime(2021, 9, 6, tzinfo=utc),  # This is a US holiday",
          "33:     pendulum.DateTime(2021, 9, 7, tzinfo=utc),",
          "34:     pendulum.DateTime(2021, 9, 8, tzinfo=utc),",
          "35:     pendulum.DateTime(2021, 9, 9, tzinfo=utc),",
          "36:     pendulum.DateTime(2021, 9, 10, tzinfo=utc),",
          "39: WEEK_1_SATURDAY = pendulum.DateTime(2021, 9, 11, tzinfo=utc)",
          "41: WEEK_2_MONDAY = pendulum.DateTime(2021, 9, 13, tzinfo=utc)",
          "42: WEEK_2_TUESDAY = pendulum.DateTime(2021, 9, 14, tzinfo=utc)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "525e22907471d08dc23bbcdc82c5c5c9d7687b4e",
      "candidate_info": {
        "commit_hash": "525e22907471d08dc23bbcdc82c5c5c9d7687b4e",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/525e22907471d08dc23bbcdc82c5c5c9d7687b4e",
        "files": [
          "airflow/jobs/backfill_job_runner.py"
        ],
        "message": "Revert \"Refactor _manage_executor_state by refreshing TIs in batch (#36418)\" (#36500)\n\nThis reverts commit 9d45db9e2cca2ad04db72f7e0712c478e5a8e1f1.\n\nt#\n\n(cherry picked from commit 72f43fcc838afc1c95b85dcb27af6519483ef64b)",
        "before_after_code_files": [
          "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py": [
          "File: airflow/jobs/backfill_job_runner.py -> airflow/jobs/backfill_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import attr",
          "24: import pendulum",
          "26: from sqlalchemy.exc import OperationalError",
          "27: from sqlalchemy.orm.session import make_transient",
          "28: from tabulate import tabulate",
          "",
          "[Removed Lines]",
          "25: from sqlalchemy import select, tuple_, update",
          "",
          "[Added Lines]",
          "25: from sqlalchemy import select, update",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "264:         :return: An iterable of expanded TaskInstance per MappedTask",
          "265:         \"\"\"",
          "266:         executor = self.job.executor",
          "291:             state, info = value",
          "294:                 self.log.warning(\"%s state %s not in running=%s\", key, state, running.values())",
          "295:                 continue",
          "299:             self.log.debug(\"Executor state: %s task %s\", state, ti)",
          "",
          "[Removed Lines]",
          "267:         # list of tuples (dag_id, task_id, execution_date, map_index) of running tasks in executor",
          "268:         buffered_events = list(executor.get_event_buffer().items())",
          "269:         running_tis_ids = [",
          "270:             (key.dag_id, key.task_id, key.run_id, key.map_index)",
          "271:             for key, _ in buffered_events",
          "272:             if key in running",
          "273:         ]",
          "274:         # list of TaskInstance of running tasks in executor (refreshed from db in batch)",
          "275:         refreshed_running_tis = session.scalars(",
          "276:             select(TaskInstance).where(",
          "277:                 tuple_(",
          "278:                     TaskInstance.dag_id,",
          "279:                     TaskInstance.task_id,",
          "280:                     TaskInstance.run_id,",
          "281:                     TaskInstance.map_index,",
          "282:                 ).in_(running_tis_ids)",
          "283:             )",
          "284:         ).all()",
          "285:         # dict of refreshed TaskInstance by key to easily find them",
          "286:         refreshed_running_tis_dict = {",
          "287:             (ti.dag_id, ti.task_id, ti.run_id, ti.map_index): ti for ti in refreshed_running_tis",
          "288:         }",
          "290:         for key, value in buffered_events:",
          "292:             ti_key = (key.dag_id, key.task_id, key.run_id, key.map_index)",
          "293:             if ti_key not in refreshed_running_tis_dict:",
          "297:             ti = refreshed_running_tis_dict[ti_key]",
          "",
          "[Added Lines]",
          "268:         # TODO: query all instead of refresh from db",
          "269:         for key, value in list(executor.get_event_buffer().items()):",
          "271:             if key not in running:",
          "275:             ti = running[key]",
          "276:             ti.refresh_from_db()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "81b23317dde8ddf68295725131e4956c10146d5d",
      "candidate_info": {
        "commit_hash": "81b23317dde8ddf68295725131e4956c10146d5d",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/81b23317dde8ddf68295725131e4956c10146d5d",
        "files": [
          "airflow/jobs/backfill_job_runner.py"
        ],
        "message": "Refactor _manage_executor_state by refreshing TIs in batch (#36502)\n\nRefactor _manage_executor_state by refreshing TIs in batch (#36418)\" (#36500)\"\n\nHandle Microsoft SQL Server\n\n(cherry picked from commit 9cf5f6f08483ff141df51c07daa91a0aa34906ec)",
        "before_after_code_files": [
          "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py": [
          "File: airflow/jobs/backfill_job_runner.py -> airflow/jobs/backfill_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import attr",
          "24: import pendulum",
          "26: from sqlalchemy.exc import OperationalError",
          "27: from sqlalchemy.orm.session import make_transient",
          "28: from tabulate import tabulate",
          "",
          "[Removed Lines]",
          "25: from sqlalchemy import select, update",
          "",
          "[Added Lines]",
          "25: from sqlalchemy import select, tuple_, update",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "264:         :return: An iterable of expanded TaskInstance per MappedTask",
          "265:         \"\"\"",
          "266:         executor = self.job.executor",
          "270:             state, info = value",
          "272:                 self.log.warning(\"%s state %s not in running=%s\", key, state, running.values())",
          "273:                 continue",
          "278:             self.log.debug(\"Executor state: %s task %s\", state, ti)",
          "",
          "[Removed Lines]",
          "268:         # TODO: query all instead of refresh from db",
          "269:         for key, value in list(executor.get_event_buffer().items()):",
          "271:             if key not in running:",
          "275:             ti = running[key]",
          "276:             ti.refresh_from_db()",
          "",
          "[Added Lines]",
          "267:         # list of tuples (dag_id, task_id, execution_date, map_index) of running tasks in executor",
          "268:         buffered_events = list(executor.get_event_buffer().items())",
          "269:         if session.get_bind().dialect.name == \"mssql\":",
          "270:             # SQL Server doesn't support multiple column subqueries",
          "271:             # TODO: Remove this once we drop support for SQL Server (#35868)",
          "272:             need_refresh = True",
          "273:             running_dict = {(ti.dag_id, ti.task_id, ti.run_id, ti.map_index): ti for ti in running.values()}",
          "274:         else:",
          "275:             running_tis_ids = [",
          "276:                 (key.dag_id, key.task_id, key.run_id, key.map_index)",
          "277:                 for key, _ in buffered_events",
          "278:                 if key in running",
          "279:             ]",
          "280:             # list of TaskInstance of running tasks in executor (refreshed from db in batch)",
          "281:             refreshed_running_tis = session.scalars(",
          "282:                 select(TaskInstance).where(",
          "283:                     tuple_(",
          "284:                         TaskInstance.dag_id,",
          "285:                         TaskInstance.task_id,",
          "286:                         TaskInstance.run_id,",
          "287:                         TaskInstance.map_index,",
          "288:                     ).in_(running_tis_ids)",
          "289:                 )",
          "290:             ).all()",
          "291:             # dict of refreshed TaskInstance by key to easily find them",
          "292:             running_dict = {",
          "293:                 (ti.dag_id, ti.task_id, ti.run_id, ti.map_index): ti for ti in refreshed_running_tis",
          "294:             }",
          "295:             need_refresh = False",
          "297:         for key, value in buffered_events:",
          "299:             ti_key = (key.dag_id, key.task_id, key.run_id, key.map_index)",
          "300:             if ti_key not in running_dict:",
          "304:             ti = running_dict[ti_key]",
          "305:             if need_refresh:",
          "306:                 ti.refresh_from_db(session=session)",
          "",
          "---------------"
        ]
      }
    }
  ]
}