{
  "cve_id": "CVE-2023-22888",
  "cve_desc": "Apache Airflow, versions before 2.6.3, is affected by a vulnerability that allows an attacker to cause a service disruption by manipulating the run_id parameter. This vulnerability is considered low since it requires an authenticated user to exploit it. It is recommended to upgrade to a version that is not affected",
  "repo": "apache/airflow",
  "patch_hash": "05bd90f563649f2e9c8f0c85cf5838315a665a02",
  "patch_info": {
    "commit_hash": "05bd90f563649f2e9c8f0c85cf5838315a665a02",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/05bd90f563649f2e9c8f0c85cf5838315a665a02",
    "files": [
      "airflow/config_templates/config.yml",
      "airflow/config_templates/default_airflow.cfg",
      "airflow/models/dag.py",
      "airflow/models/dagrun.py",
      "airflow/www/views.py",
      "tests/models/test_dagrun.py",
      "tests/www/views/test_views_trigger_dag.py"
    ],
    "message": "Sanitize `DagRun.run_id` and allow flexibility (#32293)\n\nThis commit sanitizes the DagRun.run_id parameter by introducing a configurable option.\nUsers now have the ability to select a specific run_id pattern for their runs,\nensuring stricter control over the values used. This update does not impact the default run_id\ngeneration performed by the scheduler for scheduled DAG runs or for Dag runs triggered without\nmodifying the run_id parameter in the run configuration page.\nThe configuration flexibility empowers users to align the run_id pattern with their specific requirements.",
    "before_after_code_files": [
      "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg",
      "airflow/models/dag.py||airflow/models/dag.py",
      "airflow/models/dagrun.py||airflow/models/dagrun.py",
      "airflow/www/views.py||airflow/www/views.py",
      "tests/models/test_dagrun.py||tests/models/test_dagrun.py",
      "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
    ]
  },
  "patch_diff": {
    "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg": [
      "File: airflow/config_templates/default_airflow.cfg -> airflow/config_templates/default_airflow.cfg",
      "--- Hunk 1 ---",
      "[Context before]",
      "1325: # longer than `[scheduler] task_queued_timeout`.",
      "1326: task_queued_timeout_check_interval = 120.0",
      "1328: [triggerer]",
      "1329: # How many triggers a single Triggerer will run at once, by default.",
      "1330: default_capacity = 1000",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1328: # The run_id pattern used to verify the validity of user input to the run_id parameter when",
      "1329: # triggering a DAG. This pattern cannot change the pattern used by scheduler to generate run_id",
      "1330: # for scheduled DAG runs or DAG runs triggered without changing the run_id parameter.",
      "1331: allowed_run_id_pattern = ^[A-Za-z0-9_.~:+-]+$",
      "",
      "---------------"
    ],
    "airflow/models/dag.py||airflow/models/dag.py": [
      "File: airflow/models/dag.py -> airflow/models/dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "80: import airflow.templates",
      "81: from airflow import settings, utils",
      "82: from airflow.api_internal.internal_api_call import internal_api_call",
      "84: from airflow.exceptions import (",
      "85:     AirflowDagInconsistent,",
      "86:     AirflowException,",
      "",
      "[Removed Lines]",
      "83: from airflow.configuration import conf, secrets_backend_list",
      "",
      "[Added Lines]",
      "83: from airflow.configuration import conf as airflow_conf, secrets_backend_list",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "96: from airflow.models.baseoperator import BaseOperator",
      "97: from airflow.models.dagcode import DagCode",
      "98: from airflow.models.dagpickle import DagPickle",
      "100: from airflow.models.operator import Operator",
      "101: from airflow.models.param import DagParam, ParamsDict",
      "102: from airflow.models.taskinstance import Context, TaskInstance, TaskInstanceKey, clear_task_instances",
      "",
      "[Removed Lines]",
      "99: from airflow.models.dagrun import DagRun",
      "",
      "[Added Lines]",
      "99: from airflow.models.dagrun import RUN_ID_REGEX, DagRun",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "422:         user_defined_filters: dict | None = None,",
      "423:         default_args: dict | None = None,",
      "424:         concurrency: int | None = None,",
      "427:         dagrun_timeout: timedelta | None = None,",
      "428:         sla_miss_callback: None | SLAMissCallback | list[SLAMissCallback] = None,",
      "432:         on_success_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,",
      "433:         on_failure_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,",
      "434:         doc_md: str | None = None,",
      "",
      "[Removed Lines]",
      "425:         max_active_tasks: int = conf.getint(\"core\", \"max_active_tasks_per_dag\"),",
      "426:         max_active_runs: int = conf.getint(\"core\", \"max_active_runs_per_dag\"),",
      "429:         default_view: str = conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower(),",
      "430:         orientation: str = conf.get_mandatory_value(\"webserver\", \"dag_orientation\"),",
      "431:         catchup: bool = conf.getboolean(\"scheduler\", \"catchup_by_default\"),",
      "",
      "[Added Lines]",
      "425:         max_active_tasks: int = airflow_conf.getint(\"core\", \"max_active_tasks_per_dag\"),",
      "426:         max_active_runs: int = airflow_conf.getint(\"core\", \"max_active_runs_per_dag\"),",
      "429:         default_view: str = airflow_conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower(),",
      "430:         orientation: str = airflow_conf.get_mandatory_value(\"webserver\", \"dag_orientation\"),",
      "431:         catchup: bool = airflow_conf.getboolean(\"scheduler\", \"catchup_by_default\"),",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "2588:         mark_success=False,",
      "2589:         local=False,",
      "2590:         executor=None,",
      "2592:         ignore_task_deps=False,",
      "2593:         ignore_first_depends_on_past=True,",
      "2594:         pool=None,",
      "",
      "[Removed Lines]",
      "2591:         donot_pickle=conf.getboolean(\"core\", \"donot_pickle\"),",
      "",
      "[Added Lines]",
      "2591:         donot_pickle=airflow_conf.getboolean(\"core\", \"donot_pickle\"),",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "2826:                 \"Creating DagRun needs either `run_id` or both `run_type` and `execution_date`\"",
      "2827:             )",
      "2837:         # create a copy of params before validating",
      "2838:         copied_params = copy.deepcopy(self.params)",
      "",
      "[Removed Lines]",
      "2829:         if run_id and \"/\" in run_id:",
      "2830:             warnings.warn(",
      "2831:                 \"Using forward slash ('/') in a DAG run ID is deprecated. Note that this character \"",
      "2832:                 \"also makes the run impossible to retrieve via Airflow's REST API.\",",
      "2833:                 RemovedInAirflow3Warning,",
      "2834:                 stacklevel=3,",
      "2835:             )",
      "",
      "[Added Lines]",
      "2829:         regex = airflow_conf.get(\"scheduler\", \"allowed_run_id_pattern\")",
      "2831:         if run_id and not re.match(RUN_ID_REGEX, run_id):",
      "2832:             if not regex.strip() or not re.match(regex.strip(), run_id):",
      "2833:                 raise AirflowException(",
      "2834:                     f\"The provided run ID '{run_id}' is invalid. It does not match either \"",
      "2835:                     f\"the configured pattern: '{regex}' or the built-in pattern: '{RUN_ID_REGEX}'\"",
      "2836:                 )",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "3125:     def get_default_view(self):",
      "3126:         \"\"\"This is only there for backward compatible jinja2 templates.\"\"\"",
      "3127:         if self.default_view is None:",
      "3129:         else:",
      "3130:             return self.default_view",
      "",
      "[Removed Lines]",
      "3128:             return conf.get(\"webserver\", \"dag_default_view\").lower()",
      "",
      "[Added Lines]",
      "3129:             return airflow_conf.get(\"webserver\", \"dag_default_view\").lower()",
      "",
      "---------------",
      "--- Hunk 7 ---",
      "[Context before]",
      "3342:     root_dag_id = Column(StringID())",
      "3343:     # A DAG can be paused from the UI / DB",
      "3344:     # Set this default value of is_paused based on a configuration value!",
      "3346:     is_paused = Column(Boolean, default=is_paused_at_creation)",
      "3347:     # Whether the DAG is a subdag",
      "3348:     is_subdag = Column(Boolean, default=False)",
      "",
      "[Removed Lines]",
      "3345:     is_paused_at_creation = conf.getboolean(\"core\", \"dags_are_paused_at_creation\")",
      "",
      "[Added Lines]",
      "3346:     is_paused_at_creation = airflow_conf.getboolean(\"core\", \"dags_are_paused_at_creation\")",
      "",
      "---------------",
      "--- Hunk 8 ---",
      "[Context before]",
      "3416:         \"TaskOutletDatasetReference\",",
      "3417:         cascade=\"all, delete, delete-orphan\",",
      "3418:     )",
      "3421:     def __init__(self, concurrency=None, **kwargs):",
      "3422:         super().__init__(**kwargs)",
      "",
      "[Removed Lines]",
      "3419:     NUM_DAGS_PER_DAGRUN_QUERY = conf.getint(\"scheduler\", \"max_dagruns_to_create_per_loop\", fallback=10)",
      "",
      "[Added Lines]",
      "3420:     NUM_DAGS_PER_DAGRUN_QUERY = airflow_conf.getint(",
      "3421:         \"scheduler\", \"max_dagruns_to_create_per_loop\", fallback=10",
      "3422:     )",
      "",
      "---------------",
      "--- Hunk 9 ---",
      "[Context before]",
      "3429:                 )",
      "3430:                 self.max_active_tasks = concurrency",
      "3431:             else:",
      "3434:         if self.max_active_runs is None:",
      "3437:         if self.has_task_concurrency_limits is None:",
      "3438:             # Be safe -- this will be updated later once the DAG is parsed",
      "",
      "[Removed Lines]",
      "3432:                 self.max_active_tasks = conf.getint(\"core\", \"max_active_tasks_per_dag\")",
      "3435:             self.max_active_runs = conf.getint(\"core\", \"max_active_runs_per_dag\")",
      "",
      "[Added Lines]",
      "3435:                 self.max_active_tasks = airflow_conf.getint(\"core\", \"max_active_tasks_per_dag\")",
      "3438:             self.max_active_runs = airflow_conf.getint(\"core\", \"max_active_runs_per_dag\")",
      "",
      "---------------",
      "--- Hunk 10 ---",
      "[Context before]",
      "3510:         have a value.",
      "3511:         \"\"\"",
      "3512:         # This is for backwards-compatibility with old dags that don't have None as default_view",
      "3515:     @property",
      "3516:     def safe_dag_id(self):",
      "",
      "[Removed Lines]",
      "3513:         return self.default_view or conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower()",
      "",
      "[Added Lines]",
      "3516:         return self.default_view or airflow_conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower()",
      "",
      "---------------",
      "--- Hunk 11 ---",
      "[Context before]",
      "3699:     user_defined_filters: dict | None = None,",
      "3700:     default_args: dict | None = None,",
      "3701:     concurrency: int | None = None,",
      "3704:     dagrun_timeout: timedelta | None = None,",
      "3705:     sla_miss_callback: None | SLAMissCallback | list[SLAMissCallback] = None,",
      "3709:     on_success_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,",
      "3710:     on_failure_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,",
      "3711:     doc_md: str | None = None,",
      "",
      "[Removed Lines]",
      "3702:     max_active_tasks: int = conf.getint(\"core\", \"max_active_tasks_per_dag\"),",
      "3703:     max_active_runs: int = conf.getint(\"core\", \"max_active_runs_per_dag\"),",
      "3706:     default_view: str = conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower(),",
      "3707:     orientation: str = conf.get_mandatory_value(\"webserver\", \"dag_orientation\"),",
      "3708:     catchup: bool = conf.getboolean(\"scheduler\", \"catchup_by_default\"),",
      "",
      "[Added Lines]",
      "3705:     max_active_tasks: int = airflow_conf.getint(\"core\", \"max_active_tasks_per_dag\"),",
      "3706:     max_active_runs: int = airflow_conf.getint(\"core\", \"max_active_runs_per_dag\"),",
      "3709:     default_view: str = airflow_conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower(),",
      "3710:     orientation: str = airflow_conf.get_mandatory_value(\"webserver\", \"dag_orientation\"),",
      "3711:     catchup: bool = airflow_conf.getboolean(\"scheduler\", \"catchup_by_default\"),",
      "",
      "---------------"
    ],
    "airflow/models/dagrun.py||airflow/models/dagrun.py": [
      "File: airflow/models/dagrun.py -> airflow/models/dagrun.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "24: from datetime import datetime",
      "25: from typing import TYPE_CHECKING, Any, Callable, Iterable, Iterator, NamedTuple, Sequence, TypeVar, overload",
      "27: from sqlalchemy import (",
      "28:     Boolean,",
      "29:     Column,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "27: import re2 as re",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "44: )",
      "45: from sqlalchemy.exc import IntegrityError",
      "46: from sqlalchemy.ext.associationproxy import association_proxy",
      "48: from sqlalchemy.sql.expression import false, select, true",
      "50: from airflow import settings",
      "",
      "[Removed Lines]",
      "47: from sqlalchemy.orm import Query, Session, declared_attr, joinedload, relationship, synonym",
      "",
      "[Added Lines]",
      "48: from sqlalchemy.orm import Query, Session, declared_attr, joinedload, relationship, synonym, validates",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "76:     CreatedTasks = TypeVar(\"CreatedTasks\", Iterator[\"dict[str, Any]\"], Iterator[TI])",
      "77:     TaskCreator = Callable[[Operator, Iterable[int]], CreatedTasks]",
      "80: class TISchedulingDecision(NamedTuple):",
      "81:     \"\"\"Type of return for DagRun.task_instance_scheduling_decisions.\"\"\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "80: RUN_ID_REGEX = r\"^(?:manual|scheduled|dataset_triggered)__(?:\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\+00:00)$\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "240:             external_trigger=self.external_trigger,",
      "241:         )",
      "243:     @property",
      "244:     def stats_tags(self) -> dict[str, str]:",
      "245:         return prune_dict({\"dag_id\": self.dag_id, \"run_type\": self.run_type})",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "246:     @validates(\"run_id\")",
      "247:     def validate_run_id(self, key: str, run_id: str) -> str | None:",
      "248:         if not run_id:",
      "249:             return None",
      "250:         regex = airflow_conf.get(\"scheduler\", \"allowed_run_id_pattern\")",
      "251:         if not re.match(regex, run_id) and not re.match(RUN_ID_REGEX, run_id):",
      "252:             raise ValueError(",
      "253:                 f\"The run_id provided '{run_id}' does not match the pattern '{regex}' or '{RUN_ID_REGEX}'\"",
      "254:             )",
      "255:         return run_id",
      "",
      "---------------"
    ],
    "airflow/www/views.py||airflow/www/views.py": [
      "File: airflow/www/views.py -> airflow/www/views.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "99: from airflow.models.abstractoperator import AbstractOperator",
      "100: from airflow.models.dag import DAG, get_dataset_triggered_next_run_info",
      "101: from airflow.models.dagcode import DagCode",
      "103: from airflow.models.dataset import DagScheduleDatasetReference, DatasetDagRunQueue, DatasetEvent, DatasetModel",
      "104: from airflow.models.mappedoperator import MappedOperator",
      "105: from airflow.models.operator import Operator",
      "",
      "[Removed Lines]",
      "102: from airflow.models.dagrun import DagRun, DagRunType",
      "",
      "[Added Lines]",
      "102: from airflow.models.dagrun import RUN_ID_REGEX, DagRun, DagRunType",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "1975:     @provide_session",
      "1976:     def trigger(self, dag_id: str, session: Session = NEW_SESSION):",
      "1977:         \"\"\"Triggers DAG Run.\"\"\"",
      "1979:         origin = get_safe_url(request.values.get(\"origin\"))",
      "1980:         unpause = request.values.get(\"unpause\")",
      "1981:         request_conf = request.values.get(\"conf\")",
      "",
      "[Removed Lines]",
      "1978:         run_id = request.values.get(\"run_id\", \"\")",
      "",
      "[Added Lines]",
      "1978:         run_id = request.values.get(\"run_id\", \"\").replace(\" \", \"+\")",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "2096:             flash(message, \"error\")",
      "2097:             return redirect(origin)",
      "2107:         run_conf = {}",
      "2108:         if request_conf:",
      "",
      "[Removed Lines]",
      "2099:         # Flash a warning when slash is used, but still allow it to continue on.",
      "2100:         if run_id and \"/\" in run_id:",
      "2101:             flash(",
      "2102:                 \"Using forward slash ('/') in a DAG run ID is deprecated. Note that this character \"",
      "2103:                 \"also makes the run impossible to retrieve via Airflow's REST API.\",",
      "2104:                 \"warning\",",
      "2105:             )",
      "",
      "[Added Lines]",
      "2099:         regex = conf.get(\"scheduler\", \"allowed_run_id_pattern\")",
      "2100:         if run_id and not re.match(RUN_ID_REGEX, run_id):",
      "2101:             if not regex.strip() or not re.match(regex.strip(), run_id):",
      "2102:                 flash(",
      "2103:                     f\"The provided run ID '{run_id}' is invalid. It does not match either \"",
      "2104:                     f\"the configured pattern: '{regex}' or the built-in pattern: '{RUN_ID_REGEX}'\",",
      "2105:                     \"error\",",
      "2106:                 )",
      "2108:                 form = DateTimeForm(data={\"execution_date\": execution_date})",
      "2109:                 return self.render_template(",
      "2110:                     \"airflow/trigger.html\",",
      "2111:                     form_fields=form_fields,",
      "2112:                     dag=dag,",
      "2113:                     dag_id=dag_id,",
      "2114:                     origin=origin,",
      "2115:                     conf=request_conf,",
      "2116:                     form=form,",
      "2117:                     is_dag_run_conf_overrides_params=is_dag_run_conf_overrides_params,",
      "2118:                     recent_confs=recent_confs,",
      "2119:                 )",
      "",
      "---------------"
    ],
    "tests/models/test_dagrun.py||tests/models/test_dagrun.py": [
      "File: tests/models/test_dagrun.py -> tests/models/test_dagrun.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from airflow import settings",
      "31: from airflow.callbacks.callback_requests import DagCallbackRequest",
      "32: from airflow.decorators import setup, task, task_group, teardown",
      "33: from airflow.models import (",
      "34:     DAG,",
      "35:     DagBag,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from airflow.exceptions import AirflowException",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "54: from airflow.utils.types import DagRunType",
      "55: from tests.models import DEFAULT_DATE as _DEFAULT_DATE",
      "56: from tests.test_utils import db",
      "57: from tests.test_utils.mock_operators import MockOperator",
      "59: DEFAULT_DATE = pendulum.instance(_DEFAULT_DATE)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "58: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "2541:     tis = dr.task_instance_scheduling_decisions(session).tis",
      "2542:     tis_for_state = {x.task_id for x in dr._tis_for_dagrun_state(dag=dag, tis=tis)}",
      "2543:     assert tis_for_state == expected",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "2548: @pytest.mark.parametrize(",
      "2549:     \"pattern, run_id, result\",",
      "2550:     [",
      "2551:         [\"^[A-Z]\", \"ABC\", True],",
      "2552:         [\"^[A-Z]\", \"abc\", False],",
      "2553:         [\"^[0-9]\", \"123\", True],",
      "2554:         # The below params tests that user configuration does not affect internally generated",
      "2555:         # run_ids",
      "2556:         [\"\", \"scheduled__2023-01-01T00:00:00+00:00\", True],",
      "2557:         [\"\", \"manual__2023-01-01T00:00:00+00:00\", True],",
      "2558:         [\"\", \"dataset_triggered__2023-01-01T00:00:00+00:00\", True],",
      "2559:         [\"\", \"scheduled_2023-01-01T00\", False],",
      "2560:         [\"\", \"manual_2023-01-01T00\", False],",
      "2561:         [\"\", \"dataset_triggered_2023-01-01T00\", False],",
      "2562:         [\"^[0-9]\", \"scheduled__2023-01-01T00:00:00+00:00\", True],",
      "2563:         [\"^[0-9]\", \"manual__2023-01-01T00:00:00+00:00\", True],",
      "2564:         [\"^[a-z]\", \"dataset_triggered__2023-01-01T00:00:00+00:00\", True],",
      "2565:     ],",
      "2566: )",
      "2567: def test_dag_run_id_config(session, dag_maker, pattern, run_id, result):",
      "2568:     with conf_vars({(\"scheduler\", \"allowed_run_id_pattern\"): pattern}):",
      "2569:         with dag_maker():",
      "2570:             ...",
      "2571:         if result:",
      "2572:             dag_maker.create_dagrun(run_id=run_id)",
      "2573:         else:",
      "2574:             with pytest.raises(AirflowException):",
      "2575:                 dag_maker.create_dagrun(run_id=run_id)",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
      "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from airflow.utils.session import create_session",
      "31: from airflow.utils.types import DagRunType",
      "32: from tests.test_utils.api_connexion_utils import create_test_client",
      "33: from tests.test_utils.www import check_content_in_response",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "287:         f'<textarea style=\"display: none;\" id=\"json_start\" name=\"json_start\">{expected_dag_conf}</textarea>',",
      "288:         resp,",
      "289:     )",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "293: @pytest.mark.parametrize(",
      "294:     \"pattern, run_id, result\",",
      "295:     [",
      "296:         [\"^[A-Z]\", \"ABC\", True],",
      "297:         [\"^[A-Z]\", \"abc\", False],",
      "298:         [\"^[0-9]\", \"123\", True],",
      "299:         # The below params tests that user configuration does not affect internally generated",
      "300:         # run_ids. We use manual__ as a prefix for manually triggered DAGs due to a restriction",
      "301:         # in manually triggered DAGs that the run_id must not start with scheduled__.",
      "302:         [\"\", \"manual__2023-01-01T00:00:00+00:00\", True],",
      "303:         [\"\", \"scheduled_2023-01-01T00\", False],",
      "304:         [\"\", \"manual_2023-01-01T00\", False],",
      "305:         [\"\", \"dataset_triggered_2023-01-01T00\", False],",
      "306:         [\"^[0-9]\", \"manual__2023-01-01T00:00:00+00:00\", True],",
      "307:         [\"^[a-z]\", \"manual__2023-01-01T00:00:00+00:00\", True],",
      "308:     ],",
      "309: )",
      "310: def test_dag_run_id_pattern(session, admin_client, pattern, run_id, result):",
      "311:     with conf_vars({(\"scheduler\", \"allowed_run_id_pattern\"): pattern}):",
      "312:         test_dag_id = \"example_bash_operator\"",
      "313:         admin_client.post(f\"dags/{test_dag_id}/trigger?&run_id={run_id}\")",
      "314:         run = session.query(DagRun).filter(DagRun.dag_id == test_dag_id).first()",
      "315:         if result:",
      "316:             assert run is not None",
      "317:             assert run.run_type == DagRunType.MANUAL",
      "318:         else:",
      "319:             assert run is None",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "8ff7dfbd9e76aa40b04adeb231df3820606f5ba3",
      "candidate_info": {
        "commit_hash": "8ff7dfbd9e76aa40b04adeb231df3820606f5ba3",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8ff7dfbd9e76aa40b04adeb231df3820606f5ba3",
        "files": [
          "airflow/config_templates/config.yml",
          "airflow/config_templates/default_airflow.cfg",
          "airflow/models/dag.py",
          "airflow/models/dagrun.py",
          "airflow/www/views.py",
          "tests/models/test_dagrun.py",
          "tests/www/views/test_views_trigger_dag.py"
        ],
        "message": "Sanitize `DagRun.run_id` and allow flexibility (#32293)\n\nThis commit sanitizes the DagRun.run_id parameter by introducing a configurable option.\nUsers now have the ability to select a specific run_id pattern for their runs,\nensuring stricter control over the values used. This update does not impact the default run_id\ngeneration performed by the scheduler for scheduled DAG runs or for Dag runs triggered without\nmodifying the run_id parameter in the run configuration page.\nThe configuration flexibility empowers users to align the run_id pattern with their specific requirements.\n\n(cherry picked from commit 05bd90f563649f2e9c8f0c85cf5838315a665a02)",
        "before_after_code_files": [
          "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg",
          "airflow/models/dag.py||airflow/models/dag.py",
          "airflow/models/dagrun.py||airflow/models/dagrun.py",
          "airflow/www/views.py||airflow/www/views.py",
          "tests/models/test_dagrun.py||tests/models/test_dagrun.py",
          "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "diff_branch_cherry_pick": 1,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg",
            "airflow/models/dag.py||airflow/models/dag.py",
            "airflow/models/dagrun.py||airflow/models/dagrun.py",
            "airflow/www/views.py||airflow/www/views.py",
            "tests/models/test_dagrun.py||tests/models/test_dagrun.py",
            "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
          ],
          "candidate": [
            "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg",
            "airflow/models/dag.py||airflow/models/dag.py",
            "airflow/models/dagrun.py||airflow/models/dagrun.py",
            "airflow/www/views.py||airflow/www/views.py",
            "tests/models/test_dagrun.py||tests/models/test_dagrun.py",
            "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg": [
          "File: airflow/config_templates/default_airflow.cfg -> airflow/config_templates/default_airflow.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "1245: # longer than `[scheduler] task_queued_timeout`.",
          "1246: task_queued_timeout_check_interval = 120.0",
          "1248: [triggerer]",
          "1249: # How many triggers a single Triggerer will run at once, by default.",
          "1250: default_capacity = 1000",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1248: # The run_id pattern used to verify the validity of user input to the run_id parameter when",
          "1249: # triggering a DAG. This pattern cannot change the pattern used by scheduler to generate run_id",
          "1250: # for scheduled DAG runs or DAG runs triggered without changing the run_id parameter.",
          "1251: allowed_run_id_pattern = ^[A-Za-z0-9_.~:+-]+$",
          "",
          "---------------"
        ],
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "66: from airflow import settings, utils",
          "67: from airflow.api_internal.internal_api_call import internal_api_call",
          "68: from airflow.compat.functools import cached_property",
          "70: from airflow.exceptions import (",
          "71:     AirflowDagInconsistent,",
          "72:     AirflowException,",
          "",
          "[Removed Lines]",
          "69: from airflow.configuration import conf, secrets_backend_list",
          "",
          "[Added Lines]",
          "69: from airflow.configuration import conf as airflow_conf, secrets_backend_list",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "80: from airflow.models.base import Base, StringID",
          "81: from airflow.models.dagcode import DagCode",
          "82: from airflow.models.dagpickle import DagPickle",
          "84: from airflow.models.operator import Operator",
          "85: from airflow.models.param import DagParam, ParamsDict",
          "86: from airflow.models.taskinstance import Context, TaskInstance, TaskInstanceKey, clear_task_instances",
          "",
          "[Removed Lines]",
          "83: from airflow.models.dagrun import DagRun",
          "",
          "[Added Lines]",
          "83: from airflow.models.dagrun import RUN_ID_REGEX, DagRun",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "402:         user_defined_filters: dict | None = None,",
          "403:         default_args: dict | None = None,",
          "404:         concurrency: int | None = None,",
          "407:         dagrun_timeout: timedelta | None = None,",
          "408:         sla_miss_callback: None | SLAMissCallback | list[SLAMissCallback] = None,",
          "412:         on_success_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,",
          "413:         on_failure_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,",
          "414:         doc_md: str | None = None,",
          "",
          "[Removed Lines]",
          "405:         max_active_tasks: int = conf.getint(\"core\", \"max_active_tasks_per_dag\"),",
          "406:         max_active_runs: int = conf.getint(\"core\", \"max_active_runs_per_dag\"),",
          "409:         default_view: str = conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower(),",
          "410:         orientation: str = conf.get_mandatory_value(\"webserver\", \"dag_orientation\"),",
          "411:         catchup: bool = conf.getboolean(\"scheduler\", \"catchup_by_default\"),",
          "",
          "[Added Lines]",
          "405:         max_active_tasks: int = airflow_conf.getint(\"core\", \"max_active_tasks_per_dag\"),",
          "406:         max_active_runs: int = airflow_conf.getint(\"core\", \"max_active_runs_per_dag\"),",
          "409:         default_view: str = airflow_conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower(),",
          "410:         orientation: str = airflow_conf.get_mandatory_value(\"webserver\", \"dag_orientation\"),",
          "411:         catchup: bool = airflow_conf.getboolean(\"scheduler\", \"catchup_by_default\"),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "2429:         mark_success=False,",
          "2430:         local=False,",
          "2431:         executor=None,",
          "2433:         ignore_task_deps=False,",
          "2434:         ignore_first_depends_on_past=True,",
          "2435:         pool=None,",
          "",
          "[Removed Lines]",
          "2432:         donot_pickle=conf.getboolean(\"core\", \"donot_pickle\"),",
          "",
          "[Added Lines]",
          "2432:         donot_pickle=airflow_conf.getboolean(\"core\", \"donot_pickle\"),",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "2666:                 \"Creating DagRun needs either `run_id` or both `run_type` and `execution_date`\"",
          "2667:             )",
          "2677:         # create a copy of params before validating",
          "2678:         copied_params = copy.deepcopy(self.params)",
          "",
          "[Removed Lines]",
          "2669:         if run_id and \"/\" in run_id:",
          "2670:             warnings.warn(",
          "2671:                 \"Using forward slash ('/') in a DAG run ID is deprecated. Note that this character \"",
          "2672:                 \"also makes the run impossible to retrieve via Airflow's REST API.\",",
          "2673:                 RemovedInAirflow3Warning,",
          "2674:                 stacklevel=3,",
          "2675:             )",
          "",
          "[Added Lines]",
          "2669:         regex = airflow_conf.get(\"scheduler\", \"allowed_run_id_pattern\")",
          "2671:         if run_id and not re.match(RUN_ID_REGEX, run_id):",
          "2672:             if not regex.strip() or not re.match(regex.strip(), run_id):",
          "2673:                 raise AirflowException(",
          "2674:                     f\"The provided run ID '{run_id}' is invalid. It does not match either \"",
          "2675:                     f\"the configured pattern: '{regex}' or the built-in pattern: '{RUN_ID_REGEX}'\"",
          "2676:                 )",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "2960:     def get_default_view(self):",
          "2961:         \"\"\"This is only there for backward compatible jinja2 templates\"\"\"",
          "2962:         if self.default_view is None:",
          "2964:         else:",
          "2965:             return self.default_view",
          "",
          "[Removed Lines]",
          "2963:             return conf.get(\"webserver\", \"dag_default_view\").lower()",
          "",
          "[Added Lines]",
          "2964:             return airflow_conf.get(\"webserver\", \"dag_default_view\").lower()",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "3177:     root_dag_id = Column(StringID())",
          "3178:     # A DAG can be paused from the UI / DB",
          "3179:     # Set this default value of is_paused based on a configuration value!",
          "3181:     is_paused = Column(Boolean, default=is_paused_at_creation)",
          "3182:     # Whether the DAG is a subdag",
          "3183:     is_subdag = Column(Boolean, default=False)",
          "",
          "[Removed Lines]",
          "3180:     is_paused_at_creation = conf.getboolean(\"core\", \"dags_are_paused_at_creation\")",
          "",
          "[Added Lines]",
          "3181:     is_paused_at_creation = airflow_conf.getboolean(\"core\", \"dags_are_paused_at_creation\")",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "3251:         \"TaskOutletDatasetReference\",",
          "3252:         cascade=\"all, delete, delete-orphan\",",
          "3253:     )",
          "3256:     def __init__(self, concurrency=None, **kwargs):",
          "3257:         super().__init__(**kwargs)",
          "",
          "[Removed Lines]",
          "3254:     NUM_DAGS_PER_DAGRUN_QUERY = conf.getint(\"scheduler\", \"max_dagruns_to_create_per_loop\", fallback=10)",
          "",
          "[Added Lines]",
          "3255:     NUM_DAGS_PER_DAGRUN_QUERY = airflow_conf.getint(",
          "3256:         \"scheduler\", \"max_dagruns_to_create_per_loop\", fallback=10",
          "3257:     )",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "3264:                 )",
          "3265:                 self.max_active_tasks = concurrency",
          "3266:             else:",
          "3269:         if self.max_active_runs is None:",
          "3272:         if self.has_task_concurrency_limits is None:",
          "3273:             # Be safe -- this will be updated later once the DAG is parsed",
          "",
          "[Removed Lines]",
          "3267:                 self.max_active_tasks = conf.getint(\"core\", \"max_active_tasks_per_dag\")",
          "3270:             self.max_active_runs = conf.getint(\"core\", \"max_active_runs_per_dag\")",
          "",
          "[Added Lines]",
          "3270:                 self.max_active_tasks = airflow_conf.getint(\"core\", \"max_active_tasks_per_dag\")",
          "3273:             self.max_active_runs = airflow_conf.getint(\"core\", \"max_active_runs_per_dag\")",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "3346:         have a value",
          "3347:         \"\"\"",
          "3348:         # This is for backwards-compatibility with old dags that don't have None as default_view",
          "3351:     @property",
          "3352:     def safe_dag_id(self):",
          "",
          "[Removed Lines]",
          "3349:         return self.default_view or conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower()",
          "",
          "[Added Lines]",
          "3352:         return self.default_view or airflow_conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower()",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "3529:     user_defined_filters: dict | None = None,",
          "3530:     default_args: dict | None = None,",
          "3531:     concurrency: int | None = None,",
          "3534:     dagrun_timeout: timedelta | None = None,",
          "3535:     sla_miss_callback: None | SLAMissCallback | list[SLAMissCallback] = None,",
          "3539:     on_success_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,",
          "3540:     on_failure_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,",
          "3541:     doc_md: str | None = None,",
          "",
          "[Removed Lines]",
          "3532:     max_active_tasks: int = conf.getint(\"core\", \"max_active_tasks_per_dag\"),",
          "3533:     max_active_runs: int = conf.getint(\"core\", \"max_active_runs_per_dag\"),",
          "3536:     default_view: str = conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower(),",
          "3537:     orientation: str = conf.get_mandatory_value(\"webserver\", \"dag_orientation\"),",
          "3538:     catchup: bool = conf.getboolean(\"scheduler\", \"catchup_by_default\"),",
          "",
          "[Added Lines]",
          "3535:     max_active_tasks: int = airflow_conf.getint(\"core\", \"max_active_tasks_per_dag\"),",
          "3536:     max_active_runs: int = airflow_conf.getint(\"core\", \"max_active_runs_per_dag\"),",
          "3539:     default_view: str = airflow_conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower(),",
          "3540:     orientation: str = airflow_conf.get_mandatory_value(\"webserver\", \"dag_orientation\"),",
          "3541:     catchup: bool = airflow_conf.getboolean(\"scheduler\", \"catchup_by_default\"),",
          "",
          "---------------"
        ],
        "airflow/models/dagrun.py||airflow/models/dagrun.py": [
          "File: airflow/models/dagrun.py -> airflow/models/dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: from datetime import datetime",
          "25: from typing import TYPE_CHECKING, Any, Callable, Iterable, Iterator, NamedTuple, Sequence, TypeVar, overload",
          "27: from sqlalchemy import (",
          "28:     Boolean,",
          "29:     Column,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: import re2 as re",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "43: )",
          "44: from sqlalchemy.exc import IntegrityError",
          "45: from sqlalchemy.ext.associationproxy import association_proxy",
          "47: from sqlalchemy.sql.expression import false, select, true",
          "49: from airflow import settings",
          "",
          "[Removed Lines]",
          "46: from sqlalchemy.orm import Session, declared_attr, joinedload, relationship, synonym",
          "",
          "[Added Lines]",
          "47: from sqlalchemy.orm import Session, declared_attr, joinedload, relationship, synonym, validates",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "75:     CreatedTasks = TypeVar(\"CreatedTasks\", Iterator[\"dict[str, Any]\"], Iterator[TI])",
          "76:     TaskCreator = Callable[[Operator, Iterable[int]], CreatedTasks]",
          "79: class TISchedulingDecision(NamedTuple):",
          "80:     \"\"\"Type of return for DagRun.task_instance_scheduling_decisions\"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "79: RUN_ID_REGEX = r\"^(?:manual|scheduled|dataset_triggered)__(?:\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\+00:00)$\"",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "238:             external_trigger=self.external_trigger,",
          "239:         )",
          "241:     @property",
          "242:     def logical_date(self) -> datetime:",
          "243:         return self.execution_date",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "244:     @validates(\"run_id\")",
          "245:     def validate_run_id(self, key: str, run_id: str) -> str | None:",
          "246:         if not run_id:",
          "247:             return None",
          "248:         regex = airflow_conf.get(\"scheduler\", \"allowed_run_id_pattern\")",
          "249:         if not re.match(regex, run_id) and not re.match(RUN_ID_REGEX, run_id):",
          "250:             raise ValueError(",
          "251:                 f\"The run_id provided '{run_id}' does not match the pattern '{regex}' or '{RUN_ID_REGEX}'\"",
          "252:             )",
          "253:         return run_id",
          "",
          "---------------"
        ],
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "96: from airflow.models.abstractoperator import AbstractOperator",
          "97: from airflow.models.dag import DAG, get_dataset_triggered_next_run_info",
          "98: from airflow.models.dagcode import DagCode",
          "100: from airflow.models.dataset import DagScheduleDatasetReference, DatasetDagRunQueue, DatasetEvent, DatasetModel",
          "101: from airflow.models.mappedoperator import MappedOperator",
          "102: from airflow.models.operator import Operator",
          "",
          "[Removed Lines]",
          "99: from airflow.models.dagrun import DagRun, DagRunType",
          "",
          "[Added Lines]",
          "99: from airflow.models.dagrun import RUN_ID_REGEX, DagRun, DagRunType",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1896:     def trigger(self, session: Session = NEW_SESSION):",
          "1897:         \"\"\"Triggers DAG Run.\"\"\"",
          "1898:         dag_id = request.values[\"dag_id\"]",
          "1900:         origin = get_safe_url(request.values.get(\"origin\"))",
          "1901:         unpause = request.values.get(\"unpause\")",
          "1902:         request_conf = request.values.get(\"conf\")",
          "",
          "[Removed Lines]",
          "1899:         run_id = request.values.get(\"run_id\", \"\")",
          "",
          "[Added Lines]",
          "1899:         run_id = request.values.get(\"run_id\", \"\").replace(\" \", \"+\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2016:             flash(message, \"error\")",
          "2017:             return redirect(origin)",
          "2027:         run_conf = {}",
          "2028:         if request_conf:",
          "",
          "[Removed Lines]",
          "2019:         # Flash a warning when slash is used, but still allow it to continue on.",
          "2020:         if run_id and \"/\" in run_id:",
          "2021:             flash(",
          "2022:                 \"Using forward slash ('/') in a DAG run ID is deprecated. Note that this character \"",
          "2023:                 \"also makes the run impossible to retrieve via Airflow's REST API.\",",
          "2024:                 \"warning\",",
          "2025:             )",
          "",
          "[Added Lines]",
          "2019:         regex = conf.get(\"scheduler\", \"allowed_run_id_pattern\")",
          "2020:         if run_id and not re.match(RUN_ID_REGEX, run_id):",
          "2021:             if not regex.strip() or not re.match(regex.strip(), run_id):",
          "2022:                 flash(",
          "2023:                     f\"The provided run ID '{run_id}' is invalid. It does not match either \"",
          "2024:                     f\"the configured pattern: '{regex}' or the built-in pattern: '{RUN_ID_REGEX}'\",",
          "2025:                     \"error\",",
          "2026:                 )",
          "2028:                 form = DateTimeForm(data={\"execution_date\": execution_date})",
          "2029:                 return self.render_template(",
          "2030:                     \"airflow/trigger.html\",",
          "2031:                     form_fields=form_fields,",
          "2032:                     dag=dag,",
          "2033:                     dag_id=dag_id,",
          "2034:                     origin=origin,",
          "2035:                     conf=request_conf,",
          "2036:                     form=form,",
          "2037:                     is_dag_run_conf_overrides_params=is_dag_run_conf_overrides_params,",
          "2038:                     recent_confs=recent_confs,",
          "2039:                 )",
          "",
          "---------------"
        ],
        "tests/models/test_dagrun.py||tests/models/test_dagrun.py": [
          "File: tests/models/test_dagrun.py -> tests/models/test_dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: from airflow import settings",
          "30: from airflow.callbacks.callback_requests import DagCallbackRequest",
          "31: from airflow.decorators import task, task_group",
          "32: from airflow.models import (",
          "33:     DAG,",
          "34:     DagBag,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32: from airflow.exceptions import AirflowException",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "53: from airflow.utils.types import DagRunType",
          "54: from tests.models import DEFAULT_DATE as _DEFAULT_DATE",
          "55: from tests.test_utils import db",
          "56: from tests.test_utils.mock_operators import MockOperator",
          "58: DEFAULT_DATE = pendulum.instance(_DEFAULT_DATE)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "57: from tests.test_utils.config import conf_vars",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2329:     assert session.query(DagRun).filter(DagRun.id == dr.id).one_or_none() is None",
          "2330:     assert session.query(DagRunNote).filter(DagRunNote.dag_run_id == dr.id).one_or_none() is None",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2335: @pytest.mark.parametrize(",
          "2336:     \"pattern, run_id, result\",",
          "2337:     [",
          "2338:         [\"^[A-Z]\", \"ABC\", True],",
          "2339:         [\"^[A-Z]\", \"abc\", False],",
          "2340:         [\"^[0-9]\", \"123\", True],",
          "2341:         # The below params tests that user configuration does not affect internally generated",
          "2342:         # run_ids",
          "2343:         [\"\", \"scheduled__2023-01-01T00:00:00+00:00\", True],",
          "2344:         [\"\", \"manual__2023-01-01T00:00:00+00:00\", True],",
          "2345:         [\"\", \"dataset_triggered__2023-01-01T00:00:00+00:00\", True],",
          "2346:         [\"\", \"scheduled_2023-01-01T00\", False],",
          "2347:         [\"\", \"manual_2023-01-01T00\", False],",
          "2348:         [\"\", \"dataset_triggered_2023-01-01T00\", False],",
          "2349:         [\"^[0-9]\", \"scheduled__2023-01-01T00:00:00+00:00\", True],",
          "2350:         [\"^[0-9]\", \"manual__2023-01-01T00:00:00+00:00\", True],",
          "2351:         [\"^[a-z]\", \"dataset_triggered__2023-01-01T00:00:00+00:00\", True],",
          "2352:     ],",
          "2353: )",
          "2354: def test_dag_run_id_config(session, dag_maker, pattern, run_id, result):",
          "2355:     with conf_vars({(\"scheduler\", \"allowed_run_id_pattern\"): pattern}):",
          "2356:         with dag_maker():",
          "2357:             ...",
          "2358:         if result:",
          "2359:             dag_maker.create_dagrun(run_id=run_id)",
          "2360:         else:",
          "2361:             with pytest.raises(AirflowException):",
          "2362:                 dag_maker.create_dagrun(run_id=run_id)",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
          "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: from airflow.utils.session import create_session",
          "31: from airflow.utils.types import DagRunType",
          "32: from tests.test_utils.api_connexion_utils import create_test_client",
          "33: from tests.test_utils.www import check_content_in_response",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33: from tests.test_utils.config import conf_vars",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "286:         f'<textarea style=\"display: none;\" id=\"json_start\" name=\"json_start\">{expected_dag_conf}</textarea>',",
          "287:         resp,",
          "288:     )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "292: @pytest.mark.parametrize(",
          "293:     \"pattern, run_id, result\",",
          "294:     [",
          "295:         [\"^[A-Z]\", \"ABC\", True],",
          "296:         [\"^[A-Z]\", \"abc\", False],",
          "297:         [\"^[0-9]\", \"123\", True],",
          "298:         # The below params tests that user configuration does not affect internally generated",
          "299:         # run_ids. We use manual__ as a prefix for manually triggered DAGs due to a restriction",
          "300:         # in manually triggered DAGs that the run_id must not start with scheduled__.",
          "301:         [\"\", \"manual__2023-01-01T00:00:00+00:00\", True],",
          "302:         [\"\", \"scheduled_2023-01-01T00\", False],",
          "303:         [\"\", \"manual_2023-01-01T00\", False],",
          "304:         [\"\", \"dataset_triggered_2023-01-01T00\", False],",
          "305:         [\"^[0-9]\", \"manual__2023-01-01T00:00:00+00:00\", True],",
          "306:         [\"^[a-z]\", \"manual__2023-01-01T00:00:00+00:00\", True],",
          "307:     ],",
          "308: )",
          "309: def test_dag_run_id_pattern(session, admin_client, pattern, run_id, result):",
          "310:     with conf_vars({(\"scheduler\", \"allowed_run_id_pattern\"): pattern}):",
          "311:         test_dag_id = \"example_bash_operator\"",
          "312:         admin_client.post(f\"dags/{test_dag_id}/trigger?&run_id={run_id}\")",
          "313:         run = session.query(DagRun).filter(DagRun.dag_id == test_dag_id).first()",
          "314:         if result:",
          "315:             assert run is not None",
          "316:             assert run.run_type == DagRunType.MANUAL",
          "317:         else:",
          "318:             assert run is None",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d6ce328397be19ae2dece4664c2ffd4836c9493f",
      "candidate_info": {
        "commit_hash": "d6ce328397be19ae2dece4664c2ffd4836c9493f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/d6ce328397be19ae2dece4664c2ffd4836c9493f",
        "files": [
          "airflow/www/templates/airflow/dag.html",
          "airflow/www/templates/airflow/dags.html",
          "airflow/www/views.py",
          "tests/www/views/test_views_trigger_dag.py"
        ],
        "message": "Change Trigger UI to use HTTP POST in web ui (#36026)\n\n* Change Trigger UI to use HTTP POST in web ui, GET always shows trigger form\n* Adjust tests to changed behavior of trigger handling, expects data submitted in POST\n\n(cherry picked from commit f5d802791fa5f6b13b635f06a1ea2eccc22a9ba7)",
        "before_after_code_files": [
          "airflow/www/templates/airflow/dag.html||airflow/www/templates/airflow/dag.html",
          "airflow/www/templates/airflow/dags.html||airflow/www/templates/airflow/dags.html",
          "airflow/www/views.py||airflow/www/views.py",
          "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/www/views.py||airflow/www/views.py",
            "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
          ],
          "candidate": [
            "airflow/www/views.py||airflow/www/views.py",
            "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/www/templates/airflow/dag.html||airflow/www/templates/airflow/dag.html": [
          "File: airflow/www/templates/airflow/dag.html -> airflow/www/templates/airflow/dag.html",
          "--- Hunk 1 ---",
          "[Context before]",
          "254:           {% else %}",
          "255:             <a href=\"{{ url_for('Airflow.trigger', dag_id=dag.dag_id, origin=url_for(request.endpoint, dag_id=dag.dag_id, **request.args)) }}\"",
          "256:           {% endif %}",
          "258:               aria-label=\"Trigger DAG\"",
          "259:               class=\"btn btn-default btn-icon-only{{ ' disabled' if not dag.can_trigger }} trigger-dropdown-btn\">",
          "260:               <span class=\"material-icons\" aria-hidden=\"true\">play_arrow</span>",
          "",
          "[Removed Lines]",
          "257:               title=\"Trigger&nbsp;DAG\"",
          "",
          "[Added Lines]",
          "257:               onclick=\"return triggerDag(this, '{{ dag.dag_id }}')\" title=\"Trigger&nbsp;DAG\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "289:       }",
          "290:       return false;",
          "291:     }",
          "292:   </script>",
          "293: {% endblock %}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "293:     function triggerDag(link, dagId) {",
          "294:       postAsForm(link.href, {});",
          "295:       return false;",
          "296:     }",
          "",
          "---------------"
        ],
        "airflow/www/templates/airflow/dags.html||airflow/www/templates/airflow/dags.html": [
          "File: airflow/www/templates/airflow/dags.html -> airflow/www/templates/airflow/dags.html",
          "--- Hunk 1 ---",
          "[Context before]",
          "385:                   </div>",
          "386:                 {% else %}",
          "387:                   <a href=\"{{ url_for('Airflow.trigger', dag_id=dag.dag_id, redirect_url=url_for(request.endpoint)) }}\"",
          "389:                     aria-label=\"Trigger DAG\"",
          "390:                     class=\"btn btn-sm btn-default btn-icon-only{{ ' disabled' if not dag.can_trigger }} trigger-dropdown-btn\">",
          "391:                     <span class=\"material-icons\" aria-hidden=\"true\">play_arrow</span>",
          "",
          "[Removed Lines]",
          "388:                     title=\"Trigger&nbsp;DAG\"",
          "",
          "[Added Lines]",
          "388:                   onclick=\"return triggerDag(this, '{{ dag.dag_id }}')\" title=\"Trigger&nbsp;DAG\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "483:       }",
          "484:       return false;",
          "485:     }",
          "486:   </script>",
          "487: {% endblock %}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "487:     function triggerDag(link, dagId) {",
          "488:       postAsForm(link.href, {});",
          "489:       return false;",
          "490:     }",
          "",
          "---------------"
        ],
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2047:             if isinstance(run_conf, dict) and any(run_conf)",
          "2048:         }",
          "2051:             # Populate conf textarea with conf requests parameter, or dag.params",
          "2052:             default_conf = \"\"",
          "",
          "[Removed Lines]",
          "2050:         if request.method == \"GET\" and (ui_fields_defined or show_trigger_form_if_no_params):",
          "",
          "[Added Lines]",
          "2050:         if request.method == \"GET\" or (",
          "2051:             not request_conf and (ui_fields_defined or show_trigger_form_if_no_params)",
          "2052:         ):",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
          "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "57: )",
          "58: def test_trigger_dag_button(admin_client, req, expected_run_id):",
          "59:     test_dag_id = \"example_bash_operator\"",
          "61:     with create_session() as session:",
          "62:         run = session.query(DagRun).filter(DagRun.dag_id == test_dag_id).first()",
          "63:     assert run is not None",
          "",
          "[Removed Lines]",
          "60:     admin_client.post(f\"dags/{test_dag_id}/trigger?{req}\")",
          "",
          "[Added Lines]",
          "60:     admin_client.post(f\"dags/{test_dag_id}/trigger?{req}\", data={\"conf\": \"{}\"})",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "68: def test_duplicate_run_id(admin_client):",
          "69:     test_dag_id = \"example_bash_operator\"",
          "70:     run_id = \"test_run\"",
          "73:     check_content_in_response(f\"The run ID {run_id} already exists\", response)",
          "",
          "[Removed Lines]",
          "71:     admin_client.post(f\"dags/{test_dag_id}/trigger?run_id={run_id}\", follow_redirects=True)",
          "72:     response = admin_client.post(f\"dags/{test_dag_id}/trigger?run_id={run_id}\", follow_redirects=True)",
          "",
          "[Added Lines]",
          "71:     admin_client.post(",
          "72:         f\"dags/{test_dag_id}/trigger?run_id={run_id}\", data={\"conf\": \"{}\"}, follow_redirects=True",
          "73:     )",
          "74:     response = admin_client.post(",
          "75:         f\"dags/{test_dag_id}/trigger?run_id={run_id}\", data={\"conf\": \"{}\"}, follow_redirects=True",
          "76:     )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "112: def test_trigger_dag_wrong_execution_date(admin_client):",
          "113:     test_dag_id = \"example_bash_operator\"",
          "116:     check_content_in_response(\"Invalid execution date\", response)",
          "118:     with create_session() as session:",
          "",
          "[Removed Lines]",
          "115:     response = admin_client.post(f\"dags/{test_dag_id}/trigger\", data={\"execution_date\": \"not_a_date\"})",
          "",
          "[Added Lines]",
          "119:     response = admin_client.post(",
          "120:         f\"dags/{test_dag_id}/trigger\", data={\"conf\": \"{}\", \"execution_date\": \"not_a_date\"}",
          "121:     )",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "124:     test_dag_id = \"example_bash_operator\"",
          "125:     exec_date = timezone.utcnow()",
          "129:     with create_session() as session:",
          "130:         run = session.query(DagRun).filter(DagRun.dag_id == test_dag_id).first()",
          "",
          "[Removed Lines]",
          "127:     admin_client.post(f\"dags/{test_dag_id}/trigger\", data={\"execution_date\": exec_date.isoformat()})",
          "",
          "[Added Lines]",
          "133:     admin_client.post(",
          "134:         f\"dags/{test_dag_id}/trigger\", data={\"conf\": \"{}\", \"execution_date\": exec_date.isoformat()}",
          "135:     )",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "361: def test_dag_run_id_pattern(session, admin_client, pattern, run_id, result):",
          "362:     with conf_vars({(\"scheduler\", \"allowed_run_id_pattern\"): pattern}):",
          "363:         test_dag_id = \"example_bash_operator\"",
          "365:         run = session.query(DagRun).filter(DagRun.dag_id == test_dag_id).first()",
          "366:         if result:",
          "367:             assert run is not None",
          "",
          "[Removed Lines]",
          "364:         admin_client.post(f\"dags/{test_dag_id}/trigger?&run_id={run_id}\")",
          "",
          "[Added Lines]",
          "372:         admin_client.post(f\"dags/{test_dag_id}/trigger?run_id={run_id}\", data={\"conf\": \"{}\"})",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "adccc2d0dae294e7baddbe249a85e3bb08f61d7d",
      "candidate_info": {
        "commit_hash": "adccc2d0dae294e7baddbe249a85e3bb08f61d7d",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/adccc2d0dae294e7baddbe249a85e3bb08f61d7d",
        "files": [
          "airflow/api/common/trigger_dag.py",
          "airflow/api_connexion/endpoints/dag_run_endpoint.py",
          "airflow/api_connexion/endpoints/task_instance_endpoint.py",
          "airflow/migrations/versions/0121_2_5_0_add_dagrunnote_and_taskinstancenote.py",
          "airflow/migrations/versions/0121_2_5_0_add_user_comment_to_task_instance_and_dag_run.py",
          "airflow/models/dag.py",
          "airflow/models/dagrun.py",
          "airflow/models/taskinstance.py",
          "airflow/operators/trigger_dagrun.py",
          "airflow/www/utils.py",
          "airflow/www/views.py",
          "docs/apache-airflow/img/airflow_erd.sha256",
          "docs/apache-airflow/img/airflow_erd.svg",
          "docs/apache-airflow/migrations-ref.rst",
          "tests/api/client/test_local_client.py",
          "tests/api_connexion/endpoints/test_dag_run_endpoint.py",
          "tests/api_connexion/endpoints/test_task_instance_endpoint.py",
          "tests/api_connexion/schemas/test_dag_run_schema.py",
          "tests/models/test_taskinstance.py",
          "tests/operators/test_trigger_dagrun.py",
          "tests/utils/test_db_cleanup.py",
          "tests/www/views/test_views_dagrun.py",
          "tests/www/views/test_views_tasks.py"
        ],
        "message": "Notes stored in separate table (#27849)\n\n* wip\n\n* try revert rename\n\n* simplify\n\n* working, minimally\n\n* more reverting of notes -> note rename\n\n* more reverting of notes -> note rename\n\n* more reverting of notes -> note rename\n\n* remove scratch code\n\n* remove test speedup\n\n* restore admin view\n\n* add migration\n\n* add migration\n\n* tod\n\n* fix migration\n\n* Add DagRunNote\n\n* add migration file\n\n* disamble notes in search\n\n* fix dagrun tests\n\n* fix some tests and tighten up relationships, i think\n\n* remove notes from create_dagrun method\n\n* more cleanup\n\n* fix collation\n\n* fix db cleanup test\n\n* more test fixup\n\n* more test fixup\n\n* rename to tinote\n\n* rename fixup\n\n* Don't import FAB user models just to define FK rel\n\nWe don't (currently) define any relationships it's just for making the\nFK match the migration, so for now we can have the FK col defined as a\nstring.\n\nWhen we eventually add a relationship to the get the creator of the\nnote, we should move the FAB User model into airflow.models and change\nSecurity manager code to import from there instead.\n\n* Avoid touching test file unnecessarily\n\n* fix import\n\n* Apply suggestions from code review\n\n* Test that a user_id is set when creating note via api\n\n* Fix static checks\n\nCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>\nCo-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>\nCo-authored-by: Jed Cunningham <jedcunningham@apache.org>",
        "before_after_code_files": [
          "airflow/api/common/trigger_dag.py||airflow/api/common/trigger_dag.py",
          "airflow/api_connexion/endpoints/dag_run_endpoint.py||airflow/api_connexion/endpoints/dag_run_endpoint.py",
          "airflow/api_connexion/endpoints/task_instance_endpoint.py||airflow/api_connexion/endpoints/task_instance_endpoint.py",
          "airflow/migrations/versions/0121_2_5_0_add_dagrunnote_and_taskinstancenote.py||airflow/migrations/versions/0121_2_5_0_add_dagrunnote_and_taskinstancenote.py",
          "airflow/migrations/versions/0121_2_5_0_add_user_comment_to_task_instance_and_dag_run.py||airflow/migrations/versions/0121_2_5_0_add_user_comment_to_task_instance_and_dag_run.py",
          "airflow/models/dag.py||airflow/models/dag.py",
          "airflow/models/dagrun.py||airflow/models/dagrun.py",
          "airflow/models/taskinstance.py||airflow/models/taskinstance.py",
          "airflow/operators/trigger_dagrun.py||airflow/operators/trigger_dagrun.py",
          "airflow/www/utils.py||airflow/www/utils.py",
          "airflow/www/views.py||airflow/www/views.py",
          "tests/api/client/test_local_client.py||tests/api/client/test_local_client.py",
          "tests/api_connexion/endpoints/test_dag_run_endpoint.py||tests/api_connexion/endpoints/test_dag_run_endpoint.py",
          "tests/api_connexion/endpoints/test_task_instance_endpoint.py||tests/api_connexion/endpoints/test_task_instance_endpoint.py",
          "tests/api_connexion/schemas/test_dag_run_schema.py||tests/api_connexion/schemas/test_dag_run_schema.py",
          "tests/models/test_taskinstance.py||tests/models/test_taskinstance.py",
          "tests/operators/test_trigger_dagrun.py||tests/operators/test_trigger_dagrun.py",
          "tests/utils/test_db_cleanup.py||tests/utils/test_db_cleanup.py",
          "tests/www/views/test_views_dagrun.py||tests/www/views/test_views_dagrun.py",
          "tests/www/views/test_views_tasks.py||tests/www/views/test_views_tasks.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/models/dag.py||airflow/models/dag.py",
            "airflow/models/dagrun.py||airflow/models/dagrun.py",
            "airflow/www/views.py||airflow/www/views.py"
          ],
          "candidate": [
            "airflow/models/dag.py||airflow/models/dag.py",
            "airflow/models/dagrun.py||airflow/models/dagrun.py",
            "airflow/www/views.py||airflow/www/views.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/api/common/trigger_dag.py||airflow/api/common/trigger_dag.py": [
          "File: airflow/api/common/trigger_dag.py -> airflow/api/common/trigger_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "35:     conf: dict | str | None = None,",
          "36:     execution_date: datetime | None = None,",
          "37:     replace_microseconds: bool = True,",
          "39: ) -> list[DagRun | None]:",
          "40:     \"\"\"Triggers DAG run.",
          "",
          "[Removed Lines]",
          "38:     notes: str | None = None,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "93:             external_trigger=True,",
          "94:             dag_hash=dag_bag.dags_hash.get(dag_id),",
          "95:             data_interval=data_interval,",
          "97:         )",
          "98:         dag_runs.append(dag_run)",
          "",
          "[Removed Lines]",
          "96:             notes=notes,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "106:     conf: dict | str | None = None,",
          "107:     execution_date: datetime | None = None,",
          "108:     replace_microseconds: bool = True,",
          "110: ) -> DagRun | None:",
          "111:     \"\"\"Triggers execution of DAG specified by dag_id.",
          "",
          "[Removed Lines]",
          "109:     notes: str | None = None,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "115:     :param conf: configuration",
          "116:     :param execution_date: date of execution",
          "117:     :param replace_microseconds: whether microseconds should be zeroed",
          "119:     :return: first dag run triggered - even if more than one Dag Runs were triggered or None",
          "120:     \"\"\"",
          "121:     dag_model = DagModel.get_current(dag_id)",
          "",
          "[Removed Lines]",
          "118:     :param notes: set a custom note for the newly created DagRun",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "130:         conf=conf,",
          "131:         execution_date=execution_date,",
          "132:         replace_microseconds=replace_microseconds,",
          "134:     )",
          "136:     return triggers[0] if triggers else None",
          "",
          "[Removed Lines]",
          "133:         notes=notes,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/dag_run_endpoint.py||airflow/api_connexion/endpoints/dag_run_endpoint.py": [
          "File: airflow/api_connexion/endpoints/dag_run_endpoint.py -> airflow/api_connexion/endpoints/dag_run_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "317:                 conf=post_body.get(\"conf\"),",
          "318:                 external_trigger=True,",
          "319:                 dag_hash=get_airflow_app().dag_bag.dags_hash.get(dag_id),",
          "320:             )",
          "321:             return dagrun_schema.dump(dag_run)",
          "322:         except ValueError as ve:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "320:                 session=session,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "412:             include_parentdag=True,",
          "413:             only_failed=False,",
          "414:         )",
          "416:         return dagrun_schema.dump(dag_run)",
          "",
          "[Removed Lines]",
          "415:         dag_run.refresh_from_db()",
          "",
          "[Added Lines]",
          "416:         dag_run = session.query(DagRun).filter(DagRun.id == dag_run.id).one()",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "437:     except ValidationError as err:",
          "438:         raise BadRequest(detail=str(err))",
          "441:     session.commit()",
          "442:     return dagrun_schema.dump(dag_run)",
          "",
          "[Removed Lines]",
          "440:     dag_run.notes = new_value_for_notes or None",
          "",
          "[Added Lines]",
          "441:     from flask_login import current_user",
          "443:     current_user_id = getattr(current_user, \"id\", None)",
          "444:     if dag_run.dag_run_note is None:",
          "445:         dag_run.notes = (new_value_for_notes, current_user_id)",
          "446:     else:",
          "447:         dag_run.dag_run_note.content = new_value_for_notes",
          "448:         dag_run.dag_run_note.user_id = current_user_id",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/task_instance_endpoint.py||airflow/api_connexion/endpoints/task_instance_endpoint.py": [
          "File: airflow/api_connexion/endpoints/task_instance_endpoint.py -> airflow/api_connexion/endpoints/task_instance_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "670:         raise NotFound(error_message)",
          "672:     ti, sla_miss = result",
          "676:     return task_instance_schema.dump((ti, sla_miss))",
          "",
          "[Removed Lines]",
          "673:     ti.notes = new_value_for_notes or None",
          "674:     session.commit()",
          "",
          "[Added Lines]",
          "673:     from flask_login import current_user",
          "675:     current_user_id = getattr(current_user, \"id\", None)",
          "676:     if ti.task_instance_note is None:",
          "677:         ti.notes = (new_value_for_notes, current_user_id)",
          "678:     else:",
          "679:         ti.task_instance_note.content = new_value_for_notes",
          "680:         ti.task_instance_note.user_id = current_user_id",
          "681:     session.commit()",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0121_2_5_0_add_dagrunnote_and_taskinstancenote.py||airflow/migrations/versions/0121_2_5_0_add_dagrunnote_and_taskinstancenote.py": [
          "File: airflow/migrations/versions/0121_2_5_0_add_dagrunnote_and_taskinstancenote.py -> airflow/migrations/versions/0121_2_5_0_add_dagrunnote_and_taskinstancenote.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "19: \"\"\"Add DagRunNote and TaskInstanceNote",
          "21: Revision ID: 1986afd32c1b",
          "22: Revises: ee8d93fcc81e",
          "23: Create Date: 2022-11-22 21:49:05.843439",
          "25: \"\"\"",
          "27: from __future__ import annotations",
          "29: import sqlalchemy as sa",
          "30: from alembic import op",
          "32: from airflow.migrations.db_types import StringID",
          "33: from airflow.utils.sqlalchemy import UtcDateTime",
          "35: # revision identifiers, used by Alembic.",
          "36: revision = \"1986afd32c1b\"",
          "37: down_revision = \"ee8d93fcc81e\"",
          "38: branch_labels = None",
          "39: depends_on = None",
          "40: airflow_version = \"2.5.0\"",
          "43: def upgrade():",
          "44:     \"\"\"Apply Add DagRunNote and TaskInstanceNote\"\"\"",
          "45:     op.create_table(",
          "46:         \"dag_run_note\",",
          "47:         sa.Column(\"user_id\", sa.Integer(), nullable=True),",
          "48:         sa.Column(\"dag_run_id\", sa.Integer(), nullable=False),",
          "49:         sa.Column(",
          "50:             \"content\", sa.String(length=1000).with_variant(sa.Text(length=1000), \"mysql\"), nullable=True",
          "51:         ),",
          "52:         sa.Column(\"created_at\", UtcDateTime(timezone=True), nullable=False),",
          "53:         sa.Column(\"updated_at\", UtcDateTime(timezone=True), nullable=False),",
          "54:         sa.ForeignKeyConstraint(",
          "55:             (\"dag_run_id\",), [\"dag_run.id\"], name=\"dag_run_note_dr_fkey\", ondelete=\"CASCADE\"",
          "56:         ),",
          "57:         sa.ForeignKeyConstraint((\"user_id\",), [\"ab_user.id\"], name=\"dag_run_note_user_fkey\"),",
          "58:         sa.PrimaryKeyConstraint(\"dag_run_id\", name=op.f(\"dag_run_note_pkey\")),",
          "59:     )",
          "61:     op.create_table(",
          "62:         \"task_instance_note\",",
          "63:         sa.Column(\"user_id\", sa.Integer(), nullable=True),",
          "64:         sa.Column(\"task_id\", StringID(), nullable=False),",
          "65:         sa.Column(\"dag_id\", StringID(), nullable=False),",
          "66:         sa.Column(\"run_id\", StringID(), nullable=False),",
          "67:         sa.Column(\"map_index\", sa.Integer(), nullable=False),",
          "68:         sa.Column(",
          "69:             \"content\", sa.String(length=1000).with_variant(sa.Text(length=1000), \"mysql\"), nullable=True",
          "70:         ),",
          "71:         sa.Column(\"created_at\", UtcDateTime(timezone=True), nullable=False),",
          "72:         sa.Column(\"updated_at\", UtcDateTime(timezone=True), nullable=False),",
          "73:         sa.PrimaryKeyConstraint(",
          "74:             \"task_id\", \"dag_id\", \"run_id\", \"map_index\", name=op.f(\"task_instance_note_pkey\")",
          "75:         ),",
          "76:         sa.ForeignKeyConstraint(",
          "77:             (\"dag_id\", \"task_id\", \"run_id\", \"map_index\"),",
          "78:             [",
          "79:                 \"task_instance.dag_id\",",
          "80:                 \"task_instance.task_id\",",
          "81:                 \"task_instance.run_id\",",
          "82:                 \"task_instance.map_index\",",
          "83:             ],",
          "84:             name=\"task_instance_note_ti_fkey\",",
          "85:             ondelete=\"CASCADE\",",
          "86:         ),",
          "87:         sa.ForeignKeyConstraint((\"user_id\",), [\"ab_user.id\"], name=\"task_instance_note_user_fkey\"),",
          "88:     )",
          "91: def downgrade():",
          "92:     \"\"\"Unapply Add DagRunNote and TaskInstanceNote\"\"\"",
          "93:     op.drop_table(\"task_instance_note\")",
          "94:     op.drop_table(\"dag_run_note\")",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0121_2_5_0_add_user_comment_to_task_instance_and_dag_run.py||airflow/migrations/versions/0121_2_5_0_add_user_comment_to_task_instance_and_dag_run.py": [
          "File: airflow/migrations/versions/0121_2_5_0_add_user_comment_to_task_instance_and_dag_run.py -> airflow/migrations/versions/0121_2_5_0_add_user_comment_to_task_instance_and_dag_run.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2552:         dag_hash: str | None = None,",
          "2553:         creating_job_id: int | None = None,",
          "2554:         data_interval: tuple[datetime, datetime] | None = None,",
          "2556:     ):",
          "2557:         \"\"\"",
          "2558:         Creates a dag run from this dag including the tasks associated with this dag.",
          "",
          "[Removed Lines]",
          "2555:         notes: str | None = None,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2569:         :param session: database session",
          "2570:         :param dag_hash: Hash of Serialized DAG",
          "2571:         :param data_interval: Data interval of the DagRun",
          "2573:         \"\"\"",
          "2574:         logical_date = timezone.coerce_datetime(execution_date)",
          "",
          "[Removed Lines]",
          "2572:         :param notes: A custom note for the DAGRun.",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2628:             dag_hash=dag_hash,",
          "2629:             creating_job_id=creating_job_id,",
          "2630:             data_interval=data_interval,",
          "2632:         )",
          "2633:         session.add(run)",
          "2634:         session.flush()",
          "",
          "[Removed Lines]",
          "2631:             notes=notes,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/models/dagrun.py||airflow/models/dagrun.py": [
          "File: airflow/models/dagrun.py -> airflow/models/dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "28:     Boolean,",
          "29:     Column,",
          "30:     ForeignKey,",
          "31:     Index,",
          "32:     Integer,",
          "33:     PickleType,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "31:     ForeignKeyConstraint,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "40:     text,",
          "41: )",
          "42: from sqlalchemy.exc import IntegrityError",
          "43: from sqlalchemy.ext.declarative import declared_attr",
          "44: from sqlalchemy.orm import joinedload, relationship, synonym",
          "45: from sqlalchemy.orm.session import Session",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "44: from sqlalchemy.ext.associationproxy import association_proxy",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "85:     finished_tis: list[TI]",
          "88: class DagRun(Base, LoggingMixin):",
          "89:     \"\"\"",
          "90:     DagRun describes an instance of a Dag. It can be created",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "90: def _creator_note(val):",
          "91:     \"\"\"Custom creator for the ``note`` association proxy.\"\"\"",
          "92:     if isinstance(val, str):",
          "93:         return DagRunNote(content=val)",
          "94:     elif isinstance(val, dict):",
          "95:         return DagRunNote(**val)",
          "96:     else:",
          "97:         return DagRunNote(*val)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "111:     # When a scheduler last attempted to schedule TIs for this DagRun",
          "112:     last_scheduling_decision = Column(UtcDateTime)",
          "113:     dag_hash = Column(String(32))",
          "115:     # Foreign key to LogTemplate. DagRun rows created prior to this column's",
          "116:     # existence have this set to NULL. Later rows automatically populate this on",
          "117:     # insert to point to the latest LogTemplate entry.",
          "",
          "[Removed Lines]",
          "114:     notes = Column(String(1000).with_variant(Text(1000), \"mysql\"))",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "163:         uselist=False,",
          "164:         viewonly=True,",
          "165:     )",
          "167:     DEFAULT_DAGRUNS_TO_EXAMINE = airflow_conf.getint(",
          "168:         \"scheduler\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "177:     dag_run_note = relationship(\"DagRunNote\", back_populates=\"dag_run\", uselist=False)",
          "178:     notes = association_proxy(\"dag_run_note\", \"content\", creator=_creator_note)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "184:         dag_hash: str | None = None,",
          "185:         creating_job_id: int | None = None,",
          "186:         data_interval: tuple[datetime, datetime] | None = None,",
          "188:     ):",
          "189:         if data_interval is None:",
          "190:             # Legacy: Only happen for runs created prior to Airflow 2.2.",
          "",
          "[Removed Lines]",
          "187:         notes: str | None = None,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "207:         self.run_type = run_type",
          "208:         self.dag_hash = dag_hash",
          "209:         self.creating_job_id = creating_job_id",
          "211:         super().__init__()",
          "213:     def __repr__(self):",
          "",
          "[Removed Lines]",
          "210:         self.notes = notes",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "1295:             stacklevel=2,",
          "1296:         )",
          "1297:         return self.get_log_template(session=session).filename",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1311: class DagRunNote(Base):",
          "1312:     \"\"\"For storage of arbitrary notes concerning the dagrun instance.\"\"\"",
          "1314:     __tablename__ = \"dag_run_note\"",
          "1316:     user_id = Column(Integer, nullable=True)",
          "1317:     dag_run_id = Column(Integer, primary_key=True, nullable=False)",
          "1318:     content = Column(String(1000).with_variant(Text(1000), \"mysql\"))",
          "1319:     created_at = Column(UtcDateTime, default=timezone.utcnow, nullable=False)",
          "1320:     updated_at = Column(UtcDateTime, default=timezone.utcnow, onupdate=timezone.utcnow, nullable=False)",
          "1322:     dag_run = relationship(\"DagRun\", back_populates=\"dag_run_note\")",
          "1324:     __table_args__ = (",
          "1325:         ForeignKeyConstraint(",
          "1326:             (dag_run_id,),",
          "1327:             [\"dag_run.id\"],",
          "1328:             name=\"dag_run_note_dr_fkey\",",
          "1329:             ondelete=\"CASCADE\",",
          "1330:         ),",
          "1331:         ForeignKeyConstraint(",
          "1332:             (user_id,),",
          "1333:             [\"ab_user.id\"],",
          "1334:             name=\"dag_run_note_user_fkey\",",
          "1335:         ),",
          "1336:     )",
          "1338:     def __init__(self, content, user_id=None):",
          "1339:         self.content = content",
          "1340:         self.user_id = user_id",
          "1342:     def __repr__(self):",
          "1343:         prefix = f\"<{self.__class__.__name__}: {self.dag_id}.{self.dagrun_id} {self.run_id}\"",
          "1344:         if self.map_index != -1:",
          "1345:             prefix += f\" map_index={self.map_index}\"",
          "1346:         return prefix + \">\"",
          "",
          "---------------"
        ],
        "airflow/models/taskinstance.py||airflow/models/taskinstance.py": [
          "File: airflow/models/taskinstance.py -> airflow/models/taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "313:         return self",
          "316: class TaskInstance(Base, LoggingMixin):",
          "317:     \"\"\"",
          "318:     Task instances store the state of a task instance. This table is the",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "316: def _creator_note(val):",
          "317:     \"\"\"Custom creator for the ``note`` association proxy.\"\"\"",
          "318:     if isinstance(val, str):",
          "319:         return TaskInstanceNote(content=val)",
          "320:     elif isinstance(val, dict):",
          "321:         return TaskInstanceNote(**val)",
          "322:     else:",
          "323:         return TaskInstanceNote(*val)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "355:     queued_by_job_id = Column(Integer)",
          "356:     pid = Column(Integer)",
          "357:     executor_config = Column(ExecutorConfigType(pickler=dill))",
          "359:     updated_at = Column(UtcDateTime, default=timezone.utcnow, onupdate=timezone.utcnow)",
          "361:     external_executor_id = Column(StringID())",
          "",
          "[Removed Lines]",
          "358:     notes = Column(String(1000).with_variant(Text(1000), \"mysql\"))",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "415:     triggerer_job = association_proxy(\"trigger\", \"triggerer_job\")",
          "416:     dag_run = relationship(\"DagRun\", back_populates=\"task_instances\", lazy=\"joined\", innerjoin=True)",
          "417:     rendered_task_instance_fields = relationship(\"RenderedTaskInstanceFields\", lazy=\"noload\", uselist=False)",
          "419:     execution_date = association_proxy(\"dag_run\", \"execution_date\")",
          "421:     task: Operator  # Not always set...",
          "423:     def __init__(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "428:     task_instance_note = relationship(\"TaskInstanceNote\", back_populates=\"task_instance\", uselist=False)",
          "429:     notes = association_proxy(\"task_instance_note\", \"content\", creator=_creator_note)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "794:             self.trigger_id = ti.trigger_id",
          "795:             self.next_method = ti.next_method",
          "796:             self.next_kwargs = ti.next_kwargs",
          "798:         else:",
          "799:             self.state = None",
          "",
          "[Removed Lines]",
          "797:             self.notes = ti.notes",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "2676:         return cls(**obj_dict, start_date=start_date, end_date=end_date, key=ti_key)",
          "2679: STATICA_HACK = True",
          "2680: globals()[\"kcah_acitats\"[::-1].upper()] = False",
          "2681: if STATICA_HACK:  # pragma: no cover",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2687: class TaskInstanceNote(Base):",
          "2688:     \"\"\"For storage of arbitrary notes concerning the task instance.\"\"\"",
          "2690:     __tablename__ = \"task_instance_note\"",
          "2692:     user_id = Column(Integer, nullable=True)",
          "2693:     task_id = Column(StringID(), primary_key=True, nullable=False)",
          "2694:     dag_id = Column(StringID(), primary_key=True, nullable=False)",
          "2695:     run_id = Column(StringID(), primary_key=True, nullable=False)",
          "2696:     map_index = Column(Integer, primary_key=True, nullable=False)",
          "2697:     content = Column(String(1000).with_variant(Text(1000), \"mysql\"))",
          "2698:     created_at = Column(UtcDateTime, default=timezone.utcnow, nullable=False)",
          "2699:     updated_at = Column(UtcDateTime, default=timezone.utcnow, onupdate=timezone.utcnow, nullable=False)",
          "2701:     task_instance = relationship(\"TaskInstance\", back_populates=\"task_instance_note\")",
          "2703:     __table_args__ = (",
          "2704:         ForeignKeyConstraint(",
          "2705:             (dag_id, task_id, run_id, map_index),",
          "2706:             [",
          "2707:                 \"task_instance.dag_id\",",
          "2708:                 \"task_instance.task_id\",",
          "2709:                 \"task_instance.run_id\",",
          "2710:                 \"task_instance.map_index\",",
          "2711:             ],",
          "2712:             name=\"task_instance_note_ti_fkey\",",
          "2713:             ondelete=\"CASCADE\",",
          "2714:         ),",
          "2715:         ForeignKeyConstraint(",
          "2716:             (user_id,),",
          "2717:             [\"ab_user.id\"],",
          "2718:             name=\"task_instance_note_user_fkey\",",
          "2719:         ),",
          "2720:     )",
          "2722:     def __init__(self, content, user_id=None):",
          "2723:         self.content = content",
          "2724:         self.user_id = user_id",
          "2726:     def __repr__(self):",
          "2727:         prefix = f\"<{self.__class__.__name__}: {self.dag_id}.{self.task_id} {self.run_id}\"",
          "2728:         if self.map_index != -1:",
          "2729:             prefix += f\" map_index={self.map_index}\"",
          "2730:         return prefix + \">\"",
          "",
          "---------------"
        ],
        "airflow/operators/trigger_dagrun.py||airflow/operators/trigger_dagrun.py": [
          "File: airflow/operators/trigger_dagrun.py -> airflow/operators/trigger_dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "99:         poke_interval: int = 60,",
          "100:         allowed_states: list | None = None,",
          "101:         failed_states: list | None = None,",
          "104:     ) -> None:",
          "105:         super().__init__(**kwargs)",
          "",
          "[Removed Lines]",
          "102:         dag_run_notes: str | None = None,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "111:         self.poke_interval = poke_interval",
          "112:         self.allowed_states = allowed_states or [State.SUCCESS]",
          "113:         self.failed_states = failed_states or [State.FAILED]",
          "116:         if execution_date is not None and not isinstance(execution_date, (str, datetime.datetime)):",
          "117:             raise TypeError(",
          "",
          "[Removed Lines]",
          "114:         self.dag_run_notes = dag_run_notes",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "144:                 conf=self.conf,",
          "145:                 execution_date=parsed_execution_date,",
          "146:                 replace_microseconds=False,",
          "148:             )",
          "150:         except DagRunAlreadyExists as e:",
          "",
          "[Removed Lines]",
          "147:                 notes=self.dag_run_notes,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/www/utils.py||airflow/www/utils.py": [
          "File: airflow/www/utils.py -> airflow/www/utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "718:         clean_column_names()",
          "719:         # Support for AssociationProxy in search and list columns",
          "721:             if not isinstance(desc, AssociationProxy):",
          "722:                 continue",
          "724:             if hasattr(proxy_instance.remote_attr.prop, \"columns\"):",
          "725:                 self.list_columns[desc.value_attr] = proxy_instance.remote_attr.prop.columns[0]",
          "726:                 self.list_properties[desc.value_attr] = proxy_instance.remote_attr.prop",
          "",
          "[Removed Lines]",
          "720:         for desc in self.obj.__mapper__.all_orm_descriptors:",
          "723:             proxy_instance = getattr(self.obj, desc.value_attr)",
          "",
          "[Added Lines]",
          "720:         for obj_attr, desc in self.obj.__mapper__.all_orm_descriptors.items():",
          "723:             proxy_instance = getattr(self.obj, obj_attr)",
          "",
          "---------------"
        ],
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "101: from airflow.models.dataset import DagScheduleDatasetReference, DatasetDagRunQueue, DatasetEvent, DatasetModel",
          "102: from airflow.models.operator import Operator",
          "103: from airflow.models.serialized_dag import SerializedDagModel",
          "105: from airflow.providers_manager import ProvidersManager",
          "106: from airflow.security import permissions",
          "107: from airflow.ti_deps.dep_context import DepContext",
          "",
          "[Removed Lines]",
          "104: from airflow.models.taskinstance import TaskInstance",
          "",
          "[Added Lines]",
          "104: from airflow.models.taskinstance import TaskInstance, TaskInstanceNote",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "263:             TaskInstance.task_id,",
          "264:             TaskInstance.run_id,",
          "265:             TaskInstance.state,",
          "273:         )",
          "274:         .filter(",
          "275:             TaskInstance.dag_id == dag.dag_id,",
          "276:             TaskInstance.run_id.in_([dag_run.run_id for dag_run in dag_runs]),",
          "277:         )",
          "279:         .order_by(TaskInstance.task_id, TaskInstance.run_id)",
          "280:     )",
          "",
          "[Removed Lines]",
          "266:             TaskInstance.notes,",
          "267:             sqla.func.count(sqla.func.coalesce(TaskInstance.state, sqla.literal(\"no_status\"))).label(",
          "268:                 \"state_count\"",
          "269:             ),",
          "270:             sqla.func.min(TaskInstance.start_date).label(\"start_date\"),",
          "271:             sqla.func.max(TaskInstance.end_date).label(\"end_date\"),",
          "272:             sqla.func.max(TaskInstance._try_number).label(\"_try_number\"),",
          "278:         .group_by(TaskInstance.task_id, TaskInstance.run_id, TaskInstance.state, TaskInstance.notes)",
          "",
          "[Added Lines]",
          "266:             func.min(TaskInstanceNote.content).label(\"notes\"),",
          "267:             func.count(func.coalesce(TaskInstance.state, sqla.literal(\"no_status\"))).label(\"state_count\"),",
          "268:             func.min(TaskInstance.start_date).label(\"start_date\"),",
          "269:             func.max(TaskInstance.end_date).label(\"end_date\"),",
          "270:             func.max(TaskInstance._try_number).label(\"_try_number\"),",
          "272:         .join(TaskInstance.task_instance_note, isouter=True)",
          "277:         .group_by(TaskInstance.task_id, TaskInstance.run_id, TaskInstance.state)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "4899:         \"run_type\",",
          "4900:         \"start_date\",",
          "4901:         \"end_date\",",
          "4903:         \"external_trigger\",",
          "4904:     ]",
          "4905:     label_columns = {",
          "",
          "[Removed Lines]",
          "4902:         \"notes\",",
          "",
          "[Added Lines]",
          "4901:         # \"notes\",  # todo: maybe figure out how to re-enable this",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "5291:         \"operator\",",
          "5292:         \"start_date\",",
          "5293:         \"end_date\",",
          "5295:         \"hostname\",",
          "5296:         \"priority_weight\",",
          "5297:         \"queue\",",
          "",
          "[Removed Lines]",
          "5294:         \"notes\",",
          "",
          "[Added Lines]",
          "5293:         # \"notes\",  # todo: maybe make notes work with TI search?",
          "",
          "---------------"
        ],
        "tests/api/client/test_local_client.py||tests/api/client/test_local_client.py": [
          "File: tests/api/client/test_local_client.py -> tests/api/client/test_local_client.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "84:                 external_trigger=True,",
          "85:                 dag_hash=expected_dag_hash,",
          "86:                 data_interval=expected_data_interval,",
          "88:             )",
          "89:             mock.reset_mock()",
          "",
          "[Removed Lines]",
          "87:                 notes=None,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "98:                 external_trigger=True,",
          "99:                 dag_hash=expected_dag_hash,",
          "100:                 data_interval=expected_data_interval,",
          "102:             )",
          "103:             mock.reset_mock()",
          "",
          "[Removed Lines]",
          "101:                 notes=None,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "113:                 external_trigger=True,",
          "114:                 dag_hash=expected_dag_hash,",
          "115:                 data_interval=expected_data_interval,",
          "117:             )",
          "118:             mock.reset_mock()",
          "",
          "[Removed Lines]",
          "116:                 notes=None,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "128:                 external_trigger=True,",
          "129:                 dag_hash=expected_dag_hash,",
          "130:                 data_interval=expected_data_interval,",
          "132:             )",
          "133:             mock.reset_mock()",
          "",
          "[Removed Lines]",
          "131:                 notes=None,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/api_connexion/endpoints/test_dag_run_endpoint.py||tests/api_connexion/endpoints/test_dag_run_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_dag_run_endpoint.py -> tests/api_connexion/endpoints/test_dag_run_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1421:             \"execution_date\": dr.execution_date.isoformat(),",
          "1422:             \"external_trigger\": False,",
          "1423:             \"logical_date\": dr.logical_date.isoformat(),",
          "1425:             \"state\": \"queued\",",
          "1426:             \"data_interval_start\": dr.data_interval_start.isoformat(),",
          "1427:             \"data_interval_end\": dr.data_interval_end.isoformat(),",
          "",
          "[Removed Lines]",
          "1424:             \"start_date\": dr.logical_date.isoformat(),",
          "",
          "[Added Lines]",
          "1424:             \"start_date\": None,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1631:             \"run_type\": dr.run_type,",
          "1632:             \"notes\": new_notes_value,",
          "1633:         }",
          "1635:     def test_should_raises_401_unauthenticated(self, session):",
          "1636:         response = self.client.patch(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1634:         assert dr.dag_run_note.user_id is not None",
          "",
          "---------------"
        ],
        "tests/api_connexion/endpoints/test_task_instance_endpoint.py||tests/api_connexion/endpoints/test_task_instance_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_task_instance_endpoint.py -> tests/api_connexion/endpoints/test_task_instance_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "380:     def test_should_respond_200_mapped_task_instance_with_rtif(self, session):",
          "381:         \"\"\"Verify we don't duplicate rows through join to RTIF\"\"\"",
          "382:         tis = self.create_task_instances(session)",
          "395:         session.commit()",
          "397:         # in each loop, we should get the right mapped TI back",
          "",
          "[Removed Lines]",
          "383:         session.query()",
          "384:         ti = tis[0]",
          "385:         ti.map_index = 1",
          "386:         rendered_fields = RTIF(ti, render_templates=False)",
          "387:         session.add(rendered_fields)",
          "388:         session.commit()",
          "389:         new_ti = TaskInstance(task=ti.task, run_id=ti.run_id, map_index=2)",
          "390:         for attr in [\"duration\", \"end_date\", \"pid\", \"start_date\", \"state\", \"queue\", \"notes\"]:",
          "391:             setattr(new_ti, attr, getattr(ti, attr))",
          "392:         session.add(new_ti)",
          "393:         rendered_fields = RTIF(new_ti, render_templates=False)",
          "394:         session.add(rendered_fields)",
          "",
          "[Added Lines]",
          "383:         old_ti = tis[0]",
          "384:         for idx in (1, 2):",
          "385:             ti = TaskInstance(task=old_ti.task, run_id=old_ti.run_id, map_index=idx)",
          "386:             ti.rendered_task_instance_fields = RTIF(ti, render_templates=False)",
          "387:             for attr in [\"duration\", \"end_date\", \"pid\", \"start_date\", \"state\", \"queue\", \"notes\"]:",
          "388:                 setattr(ti, attr, getattr(old_ti, attr))",
          "389:             session.add(ti)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1688:         assert response2.json[\"state\"] == NEW_STATE",
          "1690:     def test_should_update_mapped_task_instance_state(self, session):",
          "1692:         NEW_STATE = \"failed\"",
          "1693:         map_index = 1",
          "1695:         tis = self.create_task_instances(session)",
          "1700:         session.commit()",
          "1702:         self.client.patch(",
          "",
          "[Removed Lines]",
          "1696:         ti = tis[0]",
          "1697:         ti.map_index = map_index",
          "1698:         rendered_fields = RTIF(ti, render_templates=False)",
          "1699:         session.add(rendered_fields)",
          "",
          "[Added Lines]",
          "1689:         ti = TaskInstance(task=tis[0].task, run_id=tis[0].run_id, map_index=map_index)",
          "1690:         ti.rendered_task_instance_fields = RTIF(ti, render_templates=False)",
          "1691:         session.add(ti)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1824:     @provide_session",
          "1825:     def test_should_respond_200(self, session):",
          "1827:         new_notes_value = \"My super cool TaskInstance notes.\"",
          "1828:         response = self.client.patch(",
          "1829:             \"api/v1/dags/example_python_operator/dagRuns/TEST_DAG_RUN_ID/taskInstances/\"",
          "",
          "[Removed Lines]",
          "1826:         self.create_task_instances(session)",
          "",
          "[Added Lines]",
          "1818:         tis = self.create_task_instances(session)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1860:             \"trigger\": None,",
          "1861:             \"triggerer_job\": None,",
          "1862:         }",
          "1864:     def test_should_respond_200_mapped_task_instance_with_rtif(self, session):",
          "1865:         \"\"\"Verify we don't duplicate rows through join to RTIF\"\"\"",
          "1866:         tis = self.create_task_instances(session)",
          "1879:         session.commit()",
          "1881:         # in each loop, we should get the right mapped TI back",
          "",
          "[Removed Lines]",
          "1867:         session.query()",
          "1868:         ti = tis[0]",
          "1869:         ti.map_index = 1",
          "1870:         rendered_fields = RTIF(ti, render_templates=False)",
          "1871:         session.add(rendered_fields)",
          "1872:         session.commit()",
          "1873:         new_ti = TaskInstance(task=ti.task, run_id=ti.run_id, map_index=2)",
          "1874:         for attr in [\"duration\", \"end_date\", \"pid\", \"start_date\", \"state\", \"queue\", \"notes\"]:",
          "1875:             setattr(new_ti, attr, getattr(ti, attr))",
          "1876:         session.add(new_ti)",
          "1877:         rendered_fields = RTIF(new_ti, render_templates=False)",
          "1878:         session.add(rendered_fields)",
          "",
          "[Added Lines]",
          "1855:         ti = tis[0]",
          "1856:         assert ti.task_instance_note.user_id is not None",
          "1861:         old_ti = tis[0]",
          "1862:         for idx in (1, 2):",
          "1863:             ti = TaskInstance(task=old_ti.task, run_id=old_ti.run_id, map_index=idx)",
          "1864:             ti.rendered_task_instance_fields = RTIF(ti, render_templates=False)",
          "1865:             for attr in [\"duration\", \"end_date\", \"pid\", \"start_date\", \"state\", \"queue\", \"notes\"]:",
          "1866:                 setattr(ti, attr, getattr(old_ti, attr))",
          "1867:             session.add(ti)",
          "",
          "---------------"
        ],
        "tests/api_connexion/schemas/test_dag_run_schema.py||tests/api_connexion/schemas/test_dag_run_schema.py": [
          "File: tests/api_connexion/schemas/test_dag_run_schema.py -> tests/api_connexion/schemas/test_dag_run_schema.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "57:             execution_date=timezone.parse(self.default_time),",
          "58:             start_date=timezone.parse(self.default_time),",
          "59:             conf='{\"start\": \"stop\"}',",
          "61:         )",
          "62:         session.add(dagrun_model)",
          "63:         session.commit()",
          "",
          "[Removed Lines]",
          "60:             notes=\"my notes\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "78:             \"data_interval_start\": None,",
          "79:             \"last_scheduling_decision\": None,",
          "80:             \"run_type\": \"manual\",",
          "82:         }",
          "84:     @pytest.mark.parametrize(",
          "",
          "[Removed Lines]",
          "81:             \"notes\": \"my notes\",",
          "",
          "[Added Lines]",
          "80:             \"notes\": None,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "142:             run_type=DagRunType.MANUAL.value,",
          "143:             start_date=timezone.parse(self.default_time),",
          "144:             conf='{\"start\": \"stop\"}',",
          "146:         )",
          "147:         dagrun_model_2 = DagRun(",
          "148:             dag_id=\"my-dag-run\",",
          "",
          "[Removed Lines]",
          "145:             notes=\"Notes for first\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "151:             execution_date=timezone.parse(self.second_time),",
          "152:             start_date=timezone.parse(self.default_time),",
          "153:             run_type=DagRunType.MANUAL.value,",
          "155:         )",
          "156:         dagruns = [dagrun_model_1, dagrun_model_2]",
          "157:         session.add_all(dagruns)",
          "",
          "[Removed Lines]",
          "154:             notes=\"Notes for second\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "174:                     \"data_interval_start\": None,",
          "175:                     \"last_scheduling_decision\": None,",
          "176:                     \"run_type\": \"manual\",",
          "178:                 },",
          "179:                 {",
          "180:                     \"dag_id\": \"my-dag-run\",",
          "",
          "[Removed Lines]",
          "177:                     \"notes\": \"Notes for first\",",
          "",
          "[Added Lines]",
          "174:                     \"notes\": None,",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "190:                     \"data_interval_start\": None,",
          "191:                     \"last_scheduling_decision\": None,",
          "192:                     \"run_type\": \"manual\",",
          "194:                 },",
          "195:             ],",
          "196:             \"total_entries\": 2,",
          "",
          "[Removed Lines]",
          "193:                     \"notes\": \"Notes for second\",",
          "",
          "[Added Lines]",
          "190:                     \"notes\": None,",
          "",
          "---------------"
        ],
        "tests/models/test_taskinstance.py||tests/models/test_taskinstance.py": [
          "File: tests/models/test_taskinstance.py -> tests/models/test_taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2778:             \"next_kwargs\": None,",
          "2779:             \"next_method\": None,",
          "2780:             \"updated_at\": None,",
          "2782:         }",
          "2783:         # Make sure we aren't missing any new value in our expected_values list.",
          "2784:         expected_keys = {f\"task_instance.{key.lstrip('_')}\" for key in expected_values}",
          "",
          "[Removed Lines]",
          "2781:             \"notes\": None,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/operators/test_trigger_dagrun.py||tests/operators/test_trigger_dagrun.py": [
          "File: tests/operators/test_trigger_dagrun.py -> tests/operators/test_trigger_dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "148:             assert dagrun.run_id == DagRun.generate_run_id(DagRunType.MANUAL, custom_execution_date)",
          "149:             self.assert_extra_link(dagrun, task, session)",
          "163:     def test_trigger_dagrun_twice(self):",
          "164:         \"\"\"Test TriggerDagRunOperator with custom execution_date.\"\"\"",
          "165:         utc_now = timezone.utcnow()",
          "",
          "[Removed Lines]",
          "151:     def test_trigger_dagrun_with_custom_note(self):",
          "152:         notes_value = \"Custom note for newly created DagRun.\"",
          "153:         task = TriggerDagRunOperator(",
          "154:             task_id=\"test_task\", trigger_dag_id=TRIGGERED_DAG_ID, dag=self.dag, dag_run_notes=notes_value",
          "155:         )",
          "156:         task.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE, ignore_ti_state=True)",
          "158:         with create_session() as session:",
          "159:             dagrun = session.query(DagRun).filter(DagRun.dag_id == TRIGGERED_DAG_ID).one()",
          "160:             assert dagrun.external_trigger",
          "161:             assert dagrun.notes == notes_value",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/utils/test_db_cleanup.py||tests/utils/test_db_cleanup.py": [
          "File: tests/utils/test_db_cleanup.py -> tests/utils/test_db_cleanup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "255:                     with suppress(AttributeError):",
          "256:                         all_models.update({class_.__tablename__: class_})",
          "257:         exclusion_list = {",
          "258:             \"variable\",  # leave alone",
          "259:             \"dataset\",  # not good way to know if \"stale\"",
          "260:             \"trigger\",  # self-maintaining",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "258:             \"ab_user\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "272:             \"task_outlet_dataset_reference\",  # leave alone for now",
          "273:             \"dataset_dag_run_queue\",  # self-managed",
          "274:             \"dataset_event_dag_run\",  # foreign keys",
          "275:         }",
          "277:         from airflow.utils.db_cleanup import config_dict",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "276:             \"task_instance_note\",  # foreign keys",
          "277:             \"dag_run_note\",  # foreign keys",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_dagrun.py||tests/www/views/test_views_dagrun.py": [
          "File: tests/www/views/test_views_dagrun.py -> tests/www/views/test_views_dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "218:         follow_redirects=True,",
          "219:     )",
          "220:     assert resp.status_code == 200",
          "222:     assert session.query(DagRun).filter(DagRun.id == dag_run_id).count() == 0",
          "",
          "[Removed Lines]",
          "221:     assert session.query(TaskInstance).count() == 0  # Deletes associated TIs.",
          "",
          "[Added Lines]",
          "221:     assert session.query(TaskInstance).count() == 0  # associated TIs are deleted",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_tasks.py||tests/www/views/test_views_tasks.py": [
          "File: tests/www/views/test_views_tasks.py -> tests/www/views/test_views_tasks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1017:             \"max_tries\": 0,",
          "1018:             \"next_kwargs\": None,",
          "1019:             \"next_method\": None,",
          "1021:             \"operator\": \"BashOperator\",",
          "1022:             \"pid\": None,",
          "1023:             \"pool\": \"default_pool\",",
          "",
          "[Removed Lines]",
          "1020:             \"notes\": None,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1048:             \"max_tries\": 0,",
          "1049:             \"next_kwargs\": None,",
          "1050:             \"next_method\": None,",
          "1052:             \"operator\": \"BashOperator\",",
          "1053:             \"pid\": None,",
          "1054:             \"pool\": \"default_pool\",",
          "",
          "[Removed Lines]",
          "1051:             \"notes\": None,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1079:             \"max_tries\": 0,",
          "1080:             \"next_kwargs\": None,",
          "1081:             \"next_method\": None,",
          "1083:             \"operator\": \"EmptyOperator\",",
          "1084:             \"pid\": None,",
          "1085:             \"pool\": \"default_pool\",",
          "",
          "[Removed Lines]",
          "1082:             \"notes\": None,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1110:             \"max_tries\": 0,",
          "1111:             \"next_kwargs\": None,",
          "1112:             \"next_method\": None,",
          "1114:             \"operator\": \"BashOperator\",",
          "1115:             \"pid\": None,",
          "1116:             \"pool\": \"default_pool\",",
          "",
          "[Removed Lines]",
          "1113:             \"notes\": None,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1141:             \"max_tries\": 0,",
          "1142:             \"next_kwargs\": None,",
          "1143:             \"next_method\": None,",
          "1145:             \"operator\": \"BashOperator\",",
          "1146:             \"pid\": None,",
          "1147:             \"pool\": \"default_pool\",",
          "",
          "[Removed Lines]",
          "1144:             \"notes\": None,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1172:             \"max_tries\": 0,",
          "1173:             \"next_kwargs\": None,",
          "1174:             \"next_method\": None,",
          "1176:             \"operator\": \"BashOperator\",",
          "1177:             \"pid\": None,",
          "1178:             \"pool\": \"default_pool\",",
          "",
          "[Removed Lines]",
          "1175:             \"notes\": None,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "1203:             \"max_tries\": 0,",
          "1204:             \"next_kwargs\": None,",
          "1205:             \"next_method\": None,",
          "1207:             \"operator\": \"BashOperator\",",
          "1208:             \"pid\": None,",
          "1209:             \"pool\": \"default_pool\",",
          "",
          "[Removed Lines]",
          "1206:             \"notes\": None,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "841d92a03529ac3ce6f5effd36799ba4313b5c72",
      "candidate_info": {
        "commit_hash": "841d92a03529ac3ce6f5effd36799ba4313b5c72",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/841d92a03529ac3ce6f5effd36799ba4313b5c72",
        "files": [
          "airflow/migrations/versions/0114_2_4_0_add_dataset_model.py",
          "airflow/models/baseoperator.py",
          "airflow/models/dag.py",
          "airflow/models/dataset.py",
          "airflow/models/mappedoperator.py",
          "airflow/www/views.py",
          "tests/models/test_dag.py",
          "tests/serialization/test_dag_serialization.py"
        ],
        "message": "Ensure stale dataset references are removed (#25959)\n\nIf a DAG or task previously referenced a dataset but no longer does, that reference should be removed from the DB the next time the dag is processed.",
        "before_after_code_files": [
          "airflow/migrations/versions/0114_2_4_0_add_dataset_model.py||airflow/migrations/versions/0114_2_4_0_add_dataset_model.py",
          "airflow/models/baseoperator.py||airflow/models/baseoperator.py",
          "airflow/models/dag.py||airflow/models/dag.py",
          "airflow/models/dataset.py||airflow/models/dataset.py",
          "airflow/models/mappedoperator.py||airflow/models/mappedoperator.py",
          "airflow/www/views.py||airflow/www/views.py",
          "tests/models/test_dag.py||tests/models/test_dag.py",
          "tests/serialization/test_dag_serialization.py||tests/serialization/test_dag_serialization.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/models/dag.py||airflow/models/dag.py",
            "airflow/www/views.py||airflow/www/views.py"
          ],
          "candidate": [
            "airflow/models/dag.py||airflow/models/dag.py",
            "airflow/www/views.py||airflow/www/views.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/migrations/versions/0114_2_4_0_add_dataset_model.py||airflow/migrations/versions/0114_2_4_0_add_dataset_model.py": [
          "File: airflow/migrations/versions/0114_2_4_0_add_dataset_model.py -> airflow/migrations/versions/0114_2_4_0_add_dataset_model.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "67:     op.create_table(",
          "68:         'dag_schedule_dataset_reference',",
          "69:         sa.Column('dataset_id', Integer, primary_key=True, nullable=False),",
          "71:         sa.Column('created_at', TIMESTAMP, default=func.now, nullable=False),",
          "72:         sa.Column('updated_at', TIMESTAMP, default=func.now, nullable=False),",
          "73:         sa.ForeignKeyConstraint(",
          "",
          "[Removed Lines]",
          "70:         sa.Column('dag_id', String(250), primary_key=True, nullable=False),",
          "",
          "[Added Lines]",
          "70:         sa.Column('dag_id', StringID(), primary_key=True, nullable=False),",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "76:             name=\"dsdr_dataset_fkey\",",
          "77:             ondelete=\"CASCADE\",",
          "78:         ),",
          "80:     )",
          "",
          "[Removed Lines]",
          "79:         sqlite_autoincrement=True,  # ensures PK values not reused",
          "",
          "[Added Lines]",
          "79:         sa.ForeignKeyConstraint(",
          "80:             columns=('dag_id',),",
          "81:             refcolumns=['dag.dag_id'],",
          "82:             name='dsdr_dag_id_fkey',",
          "83:             ondelete='CASCADE',",
          "84:         ),",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "84:     op.create_table(",
          "85:         'task_outlet_dataset_reference',",
          "86:         sa.Column('dataset_id', Integer, primary_key=True, nullable=False),",
          "89:         sa.Column('created_at', TIMESTAMP, default=func.now, nullable=False),",
          "90:         sa.Column('updated_at', TIMESTAMP, default=func.now, nullable=False),",
          "91:         sa.ForeignKeyConstraint(",
          "",
          "[Removed Lines]",
          "87:         sa.Column('dag_id', String(250), primary_key=True, nullable=False),",
          "88:         sa.Column('task_id', String(250), primary_key=True, nullable=False),",
          "",
          "[Added Lines]",
          "92:         sa.Column('dag_id', StringID(), primary_key=True, nullable=False),",
          "93:         sa.Column('task_id', StringID(), primary_key=True, nullable=False),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "94:             name=\"todr_dataset_fkey\",",
          "95:             ondelete=\"CASCADE\",",
          "96:         ),",
          "97:     )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "102:         sa.ForeignKeyConstraint(",
          "103:             columns=('dag_id',),",
          "104:             refcolumns=['dag.dag_id'],",
          "105:             name='todr_dag_id_fkey',",
          "106:             ondelete='CASCADE',",
          "107:         ),",
          "",
          "---------------"
        ],
        "airflow/models/baseoperator.py||airflow/models/baseoperator.py": [
          "File: airflow/models/baseoperator.py -> airflow/models/baseoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "279:     partial_kwargs.setdefault(\"on_success_callback\", on_success_callback)",
          "280:     partial_kwargs.setdefault(\"run_as_user\", run_as_user)",
          "281:     partial_kwargs.setdefault(\"executor_config\", executor_config)",
          "284:     partial_kwargs.setdefault(\"resources\", resources)",
          "285:     partial_kwargs.setdefault(\"doc\", doc)",
          "286:     partial_kwargs.setdefault(\"doc_json\", doc_json)",
          "",
          "[Removed Lines]",
          "282:     partial_kwargs.setdefault(\"inlets\", inlets)",
          "283:     partial_kwargs.setdefault(\"outlets\", outlets)",
          "",
          "[Added Lines]",
          "282:     partial_kwargs.setdefault(\"inlets\", inlets or [])",
          "283:     partial_kwargs.setdefault(\"outlets\", outlets or [])",
          "",
          "---------------"
        ],
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "42:     Iterable,",
          "43:     Iterator,",
          "44:     List,",
          "46:     Optional,",
          "47:     Sequence,",
          "48:     Set,",
          "",
          "[Removed Lines]",
          "45:     NamedTuple,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2582:             session.query(DagModel)",
          "2583:             .options(joinedload(DagModel.tags, innerjoin=False))",
          "2584:             .filter(DagModel.dag_id.in_(dag_ids))",
          "2585:         )",
          "2586:         orm_dags: List[DagModel] = with_row_locks(query, of=DagModel, session=session).all()",
          "2591:         for missing_dag_id in missing_dag_ids:",
          "2592:             orm_dag = DagModel(dag_id=missing_dag_id)",
          "",
          "[Removed Lines]",
          "2588:         existing_dag_ids = {orm_dag.dag_id for orm_dag in orm_dags}",
          "2589:         missing_dag_ids = dag_ids.difference(existing_dag_ids)",
          "",
          "[Added Lines]",
          "2584:             .options(joinedload(DagModel.schedule_dataset_references))",
          "2585:             .options(joinedload(DagModel.task_outlet_dataset_references))",
          "2588:         existing_dags = {orm_dag.dag_id: orm_dag for orm_dag in orm_dags}",
          "2589:         missing_dag_ids = dag_ids.difference(existing_dags)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2602:         most_recent_subq = (",
          "2603:             session.query(DagRun.dag_id, func.max(DagRun.execution_date).label(\"max_execution_date\"))",
          "2604:             .filter(",
          "2606:                 or_(DagRun.run_type == DagRunType.BACKFILL_JOB, DagRun.run_type == DagRunType.SCHEDULED),",
          "2607:             )",
          "2608:             .group_by(DagRun.dag_id)",
          "",
          "[Removed Lines]",
          "2605:                 DagRun.dag_id.in_(existing_dag_ids),",
          "",
          "[Added Lines]",
          "2605:                 DagRun.dag_id.in_(existing_dags),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "2617:         # Get number of active dagruns for all dags we are processing as a single query.",
          "2621:         filelocs = []",
          "",
          "[Removed Lines]",
          "2619:         num_active_runs = DagRun.active_runs_of_dags(dag_ids=existing_dag_ids, session=session)",
          "",
          "[Added Lines]",
          "2619:         num_active_runs = DagRun.active_runs_of_dags(dag_ids=existing_dags, session=session)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "2684:             TaskOutletDatasetReference,",
          "2685:         )",
          "2698:         # We can't use a set here as we want to preserve order",
          "2699:         outlet_datasets: Dict[Dataset, None] = {}",
          "2700:         input_datasets: Dict[Dataset, None] = {}",
          "2701:         for dag in dags:",
          "2702:             for dataset in dag.dataset_triggers:",
          "2704:                 input_datasets[DatasetModel.from_public(dataset)] = None",
          "2705:             for task in dag.tasks:",
          "2710:         all_datasets = outlet_datasets",
          "2711:         all_datasets.update(input_datasets)",
          "",
          "[Removed Lines]",
          "2687:         class OutletRef(NamedTuple):",
          "2688:             dag_id: str",
          "2689:             task_id: str",
          "2690:             uri: str",
          "2692:         class InletRef(NamedTuple):",
          "2693:             dag_id: str",
          "2694:             uri: str",
          "2696:         dag_references = set()",
          "2697:         outlet_references = set()",
          "2703:                 dag_references.add(InletRef(dag.dag_id, dataset.uri))",
          "2706:                 for obj in task.outlets or []:",
          "2707:                     if isinstance(obj, Dataset):",
          "2708:                         outlet_references.add(OutletRef(task.dag_id, task.task_id, obj.uri))",
          "2709:                         outlet_datasets[DatasetModel.from_public(obj)] = None",
          "",
          "[Added Lines]",
          "2687:         dag_references = collections.defaultdict(set)",
          "2688:         outlet_references = collections.defaultdict(set)",
          "2693:         # here we go through dags and tasks to check for dataset references",
          "2694:         # if there are now None and previously there were some, we delete them",
          "2695:         # if there are now *any*, we add them to the above data structures and",
          "2696:         # later we'll persist them to the database.",
          "2698:             curr_orm_dag = existing_dags.get(dag.dag_id)",
          "2699:             if not dag.dataset_triggers:",
          "2700:                 if curr_orm_dag and curr_orm_dag.schedule_dataset_references:",
          "2701:                     curr_orm_dag.schedule_dataset_references = []",
          "2703:                 dag_references[dag.dag_id].add(dataset.uri)",
          "2705:             curr_outlet_references = curr_orm_dag and curr_orm_dag.task_outlet_dataset_references",
          "2707:                 dataset_outlets = [x for x in task.outlets or [] if isinstance(x, Dataset)]",
          "2708:                 if not dataset_outlets:",
          "2709:                     if curr_outlet_references:",
          "2710:                         this_task_outlet_refs = [",
          "2711:                             x",
          "2712:                             for x in curr_outlet_references",
          "2713:                             if x.dag_id == dag.dag_id and x.task_id == task.task_id",
          "2714:                         ]",
          "2715:                         for ref in this_task_outlet_refs:",
          "2716:                             curr_outlet_references.remove(ref)",
          "2717:                 for d in dataset_outlets:",
          "2718:                     outlet_references[(task.dag_id, task.task_id)].add(d.uri)",
          "2719:                     outlet_datasets[DatasetModel.from_public(d)] = None",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "2725:         del all_datasets",
          "2744:             )",
          "2746:         # Issue SQL/finish \"Unit of Work\", but let @provide_session commit (or if passed a session, let caller",
          "2747:         # decide when to commit",
          "",
          "[Removed Lines]",
          "2727:         # store dag-schedule-on-dataset references",
          "2728:         for dag_ref in dag_references:",
          "2729:             session.merge(",
          "2730:                 DagScheduleDatasetReference(",
          "2731:                     dataset_id=stored_datasets[dag_ref.uri].id,",
          "2732:                     dag_id=dag_ref.dag_id,",
          "2733:                 )",
          "2734:             )",
          "2736:         # store task-outlet-dataset references",
          "2737:         for outlet_ref in outlet_references:",
          "2738:             session.merge(",
          "2739:                 TaskOutletDatasetReference(",
          "2740:                     dataset_id=stored_datasets[outlet_ref.uri].id,",
          "2741:                     dag_id=outlet_ref.dag_id,",
          "2742:                     task_id=outlet_ref.task_id,",
          "2743:                 )",
          "",
          "[Added Lines]",
          "2737:         # reconcile dag-schedule-on-dataset references",
          "2738:         for dag_id, uri_list in dag_references.items():",
          "2739:             dag_refs_needed = {",
          "2740:                 DagScheduleDatasetReference(dataset_id=stored_datasets[uri].id, dag_id=dag_id)",
          "2741:                 for uri in uri_list",
          "2742:             }",
          "2743:             dag_refs_stored = set(",
          "2744:                 existing_dags.get(dag_id)",
          "2745:                 and existing_dags.get(dag_id).schedule_dataset_references  # type: ignore",
          "2746:                 or []",
          "2748:             dag_refs_to_add = {x for x in dag_refs_needed if x not in dag_refs_stored}",
          "2749:             session.bulk_save_objects(dag_refs_to_add)",
          "2750:             for obj in dag_refs_stored - dag_refs_needed:",
          "2751:                 session.delete(obj)",
          "2753:         existing_task_outlet_refs_dict = collections.defaultdict(set)",
          "2754:         for dag_id, orm_dag in existing_dags.items():",
          "2755:             for todr in orm_dag.task_outlet_dataset_references:",
          "2756:                 existing_task_outlet_refs_dict[(dag_id, todr.task_id)].add(todr)",
          "2758:         # reconcile task-outlet-dataset references",
          "2759:         for (dag_id, task_id), uri_list in outlet_references.items():",
          "2760:             task_refs_needed = {",
          "2761:                 TaskOutletDatasetReference(dataset_id=stored_datasets[uri].id, dag_id=dag_id, task_id=task_id)",
          "2762:                 for uri in uri_list",
          "2763:             }",
          "2764:             task_refs_stored = existing_task_outlet_refs_dict[(dag_id, task_id)]",
          "2765:             task_refs_to_add = {x for x in task_refs_needed if x not in task_refs_stored}",
          "2766:             session.bulk_save_objects(task_refs_to_add)",
          "2767:             for obj in task_refs_stored - task_refs_needed:",
          "2768:                 session.delete(obj)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "2850:         if not cls.__serialized_fields:",
          "2851:             exclusion_list = {",
          "2852:                 'parent_dag',",
          "2853:                 '_old_context_manager_dags',",
          "2854:                 'safe_dag_id',",
          "2855:                 'last_loaded',",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2877:                 'schedule_dataset_references',",
          "2878:                 'task_outlet_dataset_references',",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "3040:     parent_dag = relationship(",
          "3041:         \"DagModel\", remote_side=[dag_id], primaryjoin=root_dag_id == dag_id, foreign_keys=[root_dag_id]",
          "3042:     )",
          "3044:     NUM_DAGS_PER_DAGRUN_QUERY = conf.getint('scheduler', 'max_dagruns_to_create_per_loop', fallback=10)",
          "3046:     def __init__(self, concurrency=None, **kwargs):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3069:     schedule_dataset_references = relationship(",
          "3070:         \"DagScheduleDatasetReference\",",
          "3071:         cascade='all, delete, delete-orphan',",
          "3072:     )",
          "3073:     task_outlet_dataset_references = relationship(",
          "3074:         \"TaskOutletDatasetReference\",",
          "3075:         cascade='all, delete, delete-orphan',",
          "3076:     )",
          "",
          "---------------"
        ],
        "airflow/models/dataset.py||airflow/models/dataset.py": [
          "File: airflow/models/dataset.py -> airflow/models/dataset.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: from sqlalchemy.orm import relationship",
          "36: from airflow.datasets import Dataset",
          "38: from airflow.settings import json",
          "39: from airflow.utils import timezone",
          "40: from airflow.utils.sqlalchemy import UtcDateTime",
          "",
          "[Removed Lines]",
          "37: from airflow.models.base import ID_LEN, Base, StringID",
          "",
          "[Added Lines]",
          "37: from airflow.models.base import Base, StringID",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "105:     \"\"\"References from a DAG to a dataset of which it is a consumer.\"\"\"",
          "107:     dataset_id = Column(Integer, primary_key=True, nullable=False)",
          "109:     created_at = Column(UtcDateTime, default=timezone.utcnow, nullable=False)",
          "110:     updated_at = Column(UtcDateTime, default=timezone.utcnow, onupdate=timezone.utcnow, nullable=False)",
          "",
          "[Removed Lines]",
          "108:     dag_id = Column(String(ID_LEN), primary_key=True, nullable=False)",
          "",
          "[Added Lines]",
          "108:     dag_id = Column(StringID(), primary_key=True, nullable=False)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "117:             DagScheduleDatasetReference.dag_id == foreign(DatasetDagRunQueue.target_dag_id),",
          "118:         )\"\"\",",
          "119:     )",
          "126:     __tablename__ = \"dag_schedule_dataset_reference\"",
          "127:     __table_args__ = (",
          "",
          "[Removed Lines]",
          "120:     dag = relationship(",
          "121:         \"DagModel\",",
          "122:         primaryjoin=\"foreign(DagModel.dag_id) == DagScheduleDatasetReference.dag_id\",",
          "123:         uselist=False,",
          "124:     )",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "132:             name='dsdr_dataset_fkey',",
          "133:             ondelete=\"CASCADE\",",
          "134:         ),",
          "135:     )",
          "137:     def __eq__(self, other):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "130:         ForeignKeyConstraint(",
          "131:             columns=(dag_id,),",
          "132:             refcolumns=['dag.dag_id'],",
          "133:             name='dsdr_dag_id_fkey',",
          "134:             ondelete='CASCADE',",
          "135:         ),",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "154:     \"\"\"References from a task to a dataset that it updates / produces.\"\"\"",
          "156:     dataset_id = Column(Integer, primary_key=True, nullable=False)",
          "159:     created_at = Column(UtcDateTime, default=timezone.utcnow, nullable=False)",
          "160:     updated_at = Column(UtcDateTime, default=timezone.utcnow, onupdate=timezone.utcnow, nullable=False)",
          "",
          "[Removed Lines]",
          "157:     dag_id = Column(String(ID_LEN), primary_key=True, nullable=False)",
          "158:     task_id = Column(String(ID_LEN), primary_key=True, nullable=False)",
          "",
          "[Added Lines]",
          "158:     dag_id = Column(StringID(), primary_key=True, nullable=False)",
          "159:     task_id = Column(StringID(), primary_key=True, nullable=False)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "170:             ondelete=\"CASCADE\",",
          "171:         ),",
          "172:         PrimaryKeyConstraint(dataset_id, dag_id, task_id, name=\"todr_pkey\", mssql_clustered=True),",
          "173:     )",
          "175:     def __eq__(self, other):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "174:         ForeignKeyConstraint(",
          "175:             columns=(dag_id,),",
          "176:             refcolumns=['dag.dag_id'],",
          "177:             name='todr_dag_id_fkey',",
          "178:             ondelete='CASCADE',",
          "179:         ),",
          "",
          "---------------"
        ],
        "airflow/models/mappedoperator.py||airflow/models/mappedoperator.py": [
          "File: airflow/models/mappedoperator.py -> airflow/models/mappedoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "496:         return self.partial_kwargs.get(\"executor_config\", {})",
          "498:     @property  # type: ignore[override]",
          "502:     @inlets.setter",
          "504:         self.partial_kwargs[\"inlets\"] = value",
          "506:     @property  # type: ignore[override]",
          "510:     @outlets.setter",
          "512:         self.partial_kwargs[\"outlets\"] = value",
          "514:     @property",
          "",
          "[Removed Lines]",
          "499:     def inlets(self) -> Optional[Any]:  # type: ignore[override]",
          "500:         return self.partial_kwargs.get(\"inlets\", None)",
          "503:     def inlets(self, value):  # type: ignore[override]",
          "507:     def outlets(self) -> Optional[Any]:  # type: ignore[override]",
          "508:         return self.partial_kwargs.get(\"outlets\", None)",
          "511:     def outlets(self, value):  # type: ignore[override]",
          "",
          "[Added Lines]",
          "499:     def inlets(self) -> List[Any]:  # type: ignore[override]",
          "500:         return self.partial_kwargs.get(\"inlets\", [])",
          "503:     def inlets(self, value: List[Any]) -> None:  # type: ignore[override]",
          "507:     def outlets(self) -> List[Any]:  # type: ignore[override]",
          "508:         return self.partial_kwargs.get(\"outlets\", [])",
          "511:     def outlets(self, value: List[Any]) -> None:  # type: ignore[override]",
          "",
          "---------------"
        ],
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1144:         owner_links = session.query(DagOwnerAttributes).filter_by(dag_id=dag_id).all()",
          "1146:         attrs_to_avoid = [",
          "1147:             \"NUM_DAGS_PER_DAGRUN_QUERY\",",
          "1148:             \"serialized_dag\",",
          "1149:             \"tags\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1147:             \"schedule_dataset_references\",",
          "1148:             \"task_outlet_dataset_references\",",
          "",
          "---------------"
        ],
        "tests/models/test_dag.py||tests/models/test_dag.py": [
          "File: tests/models/test_dag.py -> tests/models/test_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "857:         \"\"\"",
          "858:         Ensure that datasets referenced in a dag are correctly loaded into the database.",
          "859:         \"\"\"",
          "861:         dag_id1 = 'test_dataset_dag1'",
          "862:         dag_id2 = 'test_dataset_dag2'",
          "863:         task_id = 'test_dataset_task'",
          "",
          "[Removed Lines]",
          "860:         # todo: clear db",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "874:         DAG.bulk_write_to_db([dag1, dag2], session=session)",
          "875:         session.commit()",
          "876:         stored_datasets = {x.uri: x for x in session.query(DatasetModel).all()}",
          "880:         assert stored_datasets[uri1].extra == {\"should\": \"be used\"}",
          "883:         assert set(",
          "884:             session.query(",
          "885:                 TaskOutletDatasetReference.task_id,",
          "",
          "[Removed Lines]",
          "877:         d1 = stored_datasets[d1.uri]",
          "878:         d2 = stored_datasets[d2.uri]",
          "879:         d3 = stored_datasets[d3.uri]",
          "881:         assert [x.dag_id for x in d1.consuming_dags] == [dag_id1]",
          "882:         assert [(x.task_id, x.dag_id) for x in d1.producing_tasks] == [(task_id, dag_id2)]",
          "",
          "[Added Lines]",
          "876:         d1_orm = stored_datasets[d1.uri]",
          "877:         d2_orm = stored_datasets[d2.uri]",
          "878:         d3_orm = stored_datasets[d3.uri]",
          "880:         assert [x.dag_id for x in d1_orm.consuming_dags] == [dag_id1]",
          "881:         assert [(x.task_id, x.dag_id) for x in d1_orm.producing_tasks] == [(task_id, dag_id2)]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "889:             .filter(TaskOutletDatasetReference.dag_id.in_((dag_id1, dag_id2)))",
          "890:             .all()",
          "891:         ) == {",
          "895:         }",
          "897:     def test_sync_to_db(self):",
          "898:         dag = DAG(",
          "899:             'dag',",
          "",
          "[Removed Lines]",
          "892:             (task_id, dag_id1, d2.id),",
          "893:             (task_id, dag_id1, d3.id),",
          "894:             (task_id, dag_id2, d1.id),",
          "",
          "[Added Lines]",
          "891:             (task_id, dag_id1, d2_orm.id),",
          "892:             (task_id, dag_id1, d3_orm.id),",
          "893:             (task_id, dag_id2, d1_orm.id),",
          "896:         # now that we have verified that a new dag has its dataset references recorded properly,",
          "897:         # we need to verify that *changes* are recorded properly.",
          "898:         # so if any references are *removed*, they should also be deleted from the DB",
          "899:         # so let's remove some references and see what happens",
          "900:         dag1 = DAG(dag_id=dag_id1, start_date=DEFAULT_DATE, schedule=None)",
          "901:         EmptyOperator(task_id=task_id, dag=dag1, outlets=[d2])",
          "902:         dag2 = DAG(dag_id=dag_id2, start_date=DEFAULT_DATE)",
          "903:         EmptyOperator(task_id=task_id, dag=dag2)",
          "904:         DAG.bulk_write_to_db([dag1, dag2], session=session)",
          "905:         session.commit()",
          "906:         session.expunge_all()",
          "907:         stored_datasets = {x.uri: x for x in session.query(DatasetModel).all()}",
          "908:         d1_orm = stored_datasets[d1.uri]",
          "909:         d2_orm = stored_datasets[d2.uri]",
          "910:         assert [x.dag_id for x in d1_orm.consuming_dags] == []",
          "911:         assert set(",
          "912:             session.query(",
          "913:                 TaskOutletDatasetReference.task_id,",
          "914:                 TaskOutletDatasetReference.dag_id,",
          "915:                 TaskOutletDatasetReference.dataset_id,",
          "916:             )",
          "917:             .filter(TaskOutletDatasetReference.dag_id.in_((dag_id1, dag_id2)))",
          "918:             .all()",
          "919:         ) == {(task_id, dag_id1, d2_orm.id)}",
          "",
          "---------------"
        ],
        "tests/serialization/test_dag_serialization.py||tests/serialization/test_dag_serialization.py": [
          "File: tests/serialization/test_dag_serialization.py -> tests/serialization/test_dag_serialization.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1339:             '<TIDep(Trigger Rule)>',",
          "1340:         ]",
          "1342:     def test_derived_dag_deps_sensor(self):",
          "1343:         \"\"\"",
          "1344:         Tests DAG dependency detection for sensors, including derived classes",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1342:     def test_serialize_mapped_outlets(self):",
          "1343:         with DAG(dag_id=\"d\", start_date=datetime.now()):",
          "1344:             op = MockOperator.partial(task_id=\"x\").expand(arg1=[1, 2])",
          "1346:         assert op.inlets == []",
          "1347:         assert op.outlets == []",
          "1349:         serialized = SerializedBaseOperator.serialize_mapped_operator(op)",
          "1350:         assert \"inlets\" not in serialized",
          "1351:         assert \"outlets\" not in serialized",
          "1353:         round_tripped = SerializedBaseOperator.deserialize_operator(serialized)",
          "1354:         assert isinstance(round_tripped, MappedOperator)",
          "1355:         assert round_tripped.inlets == []",
          "1356:         assert round_tripped.outlets == []",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "70fc0c5ae5b5714b45b00081faf7a9415c71bf88",
      "candidate_info": {
        "commit_hash": "70fc0c5ae5b5714b45b00081faf7a9415c71bf88",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/70fc0c5ae5b5714b45b00081faf7a9415c71bf88",
        "files": [
          "airflow/example_dags/example_subdag_operator.py",
          "docs/apache-airflow/core-concepts/dags.rst",
          "tests/api_connexion/conftest.py",
          "tests/api_experimental/common/test_mark_tasks.py",
          "tests/conftest.py",
          "tests/dags/test_clear_subdag.py",
          "tests/dags/test_impersonation_subdag.py",
          "tests/dags/test_subdag.py",
          "tests/jobs/test_backfill_job.py",
          "tests/models/test_dagrun.py",
          "tests/www/api/experimental/test_dag_runs_endpoint.py",
          "tests/www/views/conftest.py"
        ],
        "message": "Suppress `SubDagOperator` examples warnings (#39057)",
        "before_after_code_files": [
          "airflow/example_dags/example_subdag_operator.py||airflow/example_dags/example_subdag_operator.py",
          "tests/api_connexion/conftest.py||tests/api_connexion/conftest.py",
          "tests/api_experimental/common/test_mark_tasks.py||tests/api_experimental/common/test_mark_tasks.py",
          "tests/conftest.py||tests/conftest.py",
          "tests/dags/test_clear_subdag.py||tests/dags/test_clear_subdag.py",
          "tests/dags/test_impersonation_subdag.py||tests/dags/test_impersonation_subdag.py",
          "tests/dags/test_subdag.py||tests/dags/test_subdag.py",
          "tests/jobs/test_backfill_job.py||tests/jobs/test_backfill_job.py",
          "tests/models/test_dagrun.py||tests/models/test_dagrun.py",
          "tests/www/api/experimental/test_dag_runs_endpoint.py||tests/www/api/experimental/test_dag_runs_endpoint.py",
          "tests/www/views/conftest.py||tests/www/views/conftest.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "tests/models/test_dagrun.py||tests/models/test_dagrun.py"
          ],
          "candidate": [
            "tests/models/test_dagrun.py||tests/models/test_dagrun.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/example_dags/example_subdag_operator.py||airflow/example_dags/example_subdag_operator.py": [
          "File: airflow/example_dags/example_subdag_operator.py -> airflow/example_dags/example_subdag_operator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: from __future__ import annotations",
          "46:     )",
          "",
          "[Removed Lines]",
          "22: # [START example_subdag_operator]",
          "23: import datetime",
          "25: from airflow.example_dags.subdags.subdag import subdag",
          "26: from airflow.models.dag import DAG",
          "27: from airflow.operators.empty import EmptyOperator",
          "28: from airflow.operators.subdag import SubDagOperator",
          "30: DAG_NAME = \"example_subdag_operator\"",
          "32: with DAG(",
          "33:     dag_id=DAG_NAME,",
          "34:     default_args={\"retries\": 2},",
          "35:     start_date=datetime.datetime(2022, 1, 1),",
          "36:     schedule=\"@once\",",
          "37:     tags=[\"example\"],",
          "38: ) as dag:",
          "39:     start = EmptyOperator(",
          "40:         task_id=\"start\",",
          "41:     )",
          "43:     section_1 = SubDagOperator(",
          "44:         task_id=\"section-1\",",
          "45:         subdag=subdag(DAG_NAME, \"section-1\", dag.default_args),",
          "48:     some_other_task = EmptyOperator(",
          "49:         task_id=\"some-other-task\",",
          "50:     )",
          "52:     section_2 = SubDagOperator(",
          "53:         task_id=\"section-2\",",
          "54:         subdag=subdag(DAG_NAME, \"section-2\", dag.default_args),",
          "55:     )",
          "57:     end = EmptyOperator(",
          "58:         task_id=\"end\",",
          "59:     )",
          "61:     start >> section_1 >> some_other_task >> section_2 >> end",
          "62: # [END example_subdag_operator]",
          "",
          "[Added Lines]",
          "22: import warnings",
          "24: with warnings.catch_warnings():",
          "25:     warnings.filterwarnings(",
          "26:         \"ignore\",",
          "27:         message=r\"This class is deprecated\\. Please use `airflow\\.utils\\.task_group\\.TaskGroup`\\.\",",
          "30:     # [START example_subdag_operator]",
          "31:     import datetime",
          "33:     from airflow.example_dags.subdags.subdag import subdag",
          "34:     from airflow.models.dag import DAG",
          "35:     from airflow.operators.empty import EmptyOperator",
          "36:     from airflow.operators.subdag import SubDagOperator",
          "38:     DAG_NAME = \"example_subdag_operator\"",
          "40:     with DAG(",
          "41:         dag_id=DAG_NAME,",
          "42:         default_args={\"retries\": 2},",
          "43:         start_date=datetime.datetime(2022, 1, 1),",
          "44:         schedule=\"@once\",",
          "45:         tags=[\"example\"],",
          "46:     ) as dag:",
          "47:         start = EmptyOperator(",
          "48:             task_id=\"start\",",
          "49:         )",
          "51:         section_1 = SubDagOperator(",
          "52:             task_id=\"section-1\",",
          "53:             subdag=subdag(DAG_NAME, \"section-1\", dag.default_args),",
          "54:         )",
          "56:         some_other_task = EmptyOperator(",
          "57:             task_id=\"some-other-task\",",
          "58:         )",
          "60:         section_2 = SubDagOperator(",
          "61:             task_id=\"section-2\",",
          "62:             subdag=subdag(DAG_NAME, \"section-2\", dag.default_args),",
          "63:         )",
          "65:         end = EmptyOperator(",
          "66:             task_id=\"end\",",
          "67:         )",
          "69:         start >> section_1 >> some_other_task >> section_2 >> end",
          "70:     # [END example_subdag_operator]",
          "",
          "---------------"
        ],
        "tests/api_connexion/conftest.py||tests/api_connexion/conftest.py": [
          "File: tests/api_connexion/conftest.py -> tests/api_connexion/conftest.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "21: import pytest",
          "24: from airflow.www import app",
          "25: from tests.test_utils.config import conf_vars",
          "26: from tests.test_utils.decorators import dont_initialize_flask_app_submodules",
          "",
          "[Removed Lines]",
          "19: import warnings",
          "23: from airflow.exceptions import RemovedInAirflow3Warning",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "59: def dagbag():",
          "60:     from airflow.models import DagBag",
          "71:     return DagBag(include_examples=True, read_dags_from_db=True)",
          "",
          "[Removed Lines]",
          "62:     with warnings.catch_warnings():",
          "63:         # This explicitly shows off SubDagOperator, no point to warn about that.",
          "64:         warnings.filterwarnings(",
          "65:             \"ignore\",",
          "66:             category=RemovedInAirflow3Warning,",
          "67:             message=r\".+Please use.+TaskGroup.+\",",
          "68:             module=r\".+example_subdag_operator$\",",
          "69:         )",
          "70:         DagBag(include_examples=True, read_dags_from_db=False).sync_to_db()",
          "",
          "[Added Lines]",
          "59:     DagBag(include_examples=True, read_dags_from_db=False).sync_to_db()",
          "",
          "---------------"
        ],
        "tests/api_experimental/common/test_mark_tasks.py||tests/api_experimental/common/test_mark_tasks.py": [
          "File: tests/api_experimental/common/test_mark_tasks.py -> tests/api_experimental/common/test_mark_tasks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import datetime",
          "22: from typing import Callable",
          "24: import pytest",
          "",
          "[Removed Lines]",
          "21: import warnings",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "34:     set_dag_run_state_to_success,",
          "35:     set_state,",
          "36: )",
          "38: from airflow.models import DagRun",
          "39: from airflow.utils import timezone",
          "40: from airflow.utils.session import create_session, provide_session",
          "",
          "[Removed Lines]",
          "37: from airflow.exceptions import RemovedInAirflow3Warning",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "52: def dagbag():",
          "53:     from airflow.models.dagbag import DagBag",
          "62:     return DagBag(read_dags_from_db=True)",
          "",
          "[Removed Lines]",
          "55:     with warnings.catch_warnings():",
          "56:         # Some dags use deprecated operators, e.g SubDagOperator",
          "57:         # if it is not imported, then it might have side effects for the other tests",
          "58:         warnings.simplefilter(\"ignore\", category=RemovedInAirflow3Warning)",
          "59:         # Ensure the DAGs we are looking at from the DB are up-to-date",
          "60:         non_serialized_dagbag = DagBag(read_dags_from_db=False, include_examples=True)",
          "61:         non_serialized_dagbag.sync_to_db()",
          "",
          "[Added Lines]",
          "53:     # Ensure the DAGs we are looking at from the DB are up-to-date",
          "54:     non_serialized_dagbag = DagBag(read_dags_from_db=False, include_examples=True)",
          "55:     non_serialized_dagbag.sync_to_db()",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "485:     @classmethod",
          "486:     def setup_class(cls):",
          "492:         cls.dag1 = dagbag.dags[\"miscellaneous_test_dag\"]",
          "493:         cls.dag1.sync_to_db()",
          "494:         cls.dag2 = dagbag.dags[\"example_subdag_operator\"]",
          "",
          "[Removed Lines]",
          "487:         with warnings.catch_warnings():",
          "488:             # Some dags use deprecated operators, e.g SubDagOperator",
          "489:             # if it is not imported, then it might have side effects for the other tests",
          "490:             warnings.simplefilter(\"ignore\", category=RemovedInAirflow3Warning)",
          "491:             dagbag = models.DagBag(include_examples=True, read_dags_from_db=False)",
          "",
          "[Added Lines]",
          "481:         dagbag = models.DagBag(include_examples=True, read_dags_from_db=False)",
          "",
          "---------------"
        ],
        "tests/conftest.py||tests/conftest.py": [
          "File: tests/conftest.py -> tests/conftest.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import re",
          "24: import subprocess",
          "25: import sys",
          "27: from contextlib import ExitStack, suppress",
          "28: from datetime import datetime, timedelta, timezone",
          "29: from pathlib import Path",
          "",
          "[Removed Lines]",
          "26: import warnings",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "321:     from flask import Flask",
          "323:     from airflow.configuration import conf",
          "325:     from airflow.utils import db",
          "326:     from airflow.www.extensions.init_appbuilder import init_appbuilder",
          "327:     from airflow.www.extensions.init_auth_manager import get_auth_manager",
          "336:     db.resetdb()",
          "342:     # minimal app to add roles",
          "343:     flask_app = Flask(__name__)",
          "344:     flask_app.config[\"SQLALCHEMY_DATABASE_URI\"] = conf.get(\"database\", \"SQL_ALCHEMY_CONN\")",
          "",
          "[Removed Lines]",
          "324:     from airflow.exceptions import RemovedInAirflow3Warning",
          "329:     ignore_warnings = {",
          "330:         RemovedInAirflow3Warning: [",
          "331:             # SubDagOperator warnings",
          "332:             \"This class is deprecated. Please use `airflow.utils.task_group.TaskGroup`.\"",
          "333:         ]",
          "334:     }",
          "337:     with warnings.catch_warnings():",
          "338:         for warning_category, messages in ignore_warnings.items():",
          "339:             for message in messages:",
          "340:                 warnings.filterwarnings(\"ignore\", message=re.escape(message), category=warning_category)",
          "341:         db.bootstrap_dagbag()",
          "",
          "[Added Lines]",
          "328:     db.bootstrap_dagbag()",
          "",
          "---------------"
        ],
        "tests/dags/test_clear_subdag.py||tests/dags/test_clear_subdag.py": [
          "File: tests/dags/test_clear_subdag.py -> tests/dags/test_clear_subdag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "37:         max_active_tasks=2,",
          "38:     )",
          "39:     BashOperator(bash_command=\"echo 1\", task_id=\"daily_job_subdag_task\", dag=subdag)",
          "41:         return SubDagOperator(",
          "42:             task_id=subdag_name,",
          "43:             subdag=subdag,",
          "",
          "[Removed Lines]",
          "40:     with warnings.catch_warnings(record=True):",
          "",
          "[Added Lines]",
          "40:     with warnings.catch_warnings():",
          "41:         warnings.filterwarnings(",
          "42:             \"ignore\",",
          "43:             message=r\"This class is deprecated\\. Please use `airflow\\.utils\\.task_group\\.TaskGroup`\\.\",",
          "44:         )",
          "",
          "---------------"
        ],
        "tests/dags/test_impersonation_subdag.py||tests/dags/test_impersonation_subdag.py": [
          "File: tests/dags/test_impersonation_subdag.py -> tests/dags/test_impersonation_subdag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "45: BashOperator(task_id=\"exec_bash_operator\", bash_command='echo \"Running within SubDag\"', dag=subdag)",
          "49:     subdag_operator = SubDagOperator(",
          "50:         task_id=\"test_subdag_operation\", subdag=subdag, mode=\"reschedule\", poke_interval=1, dag=dag",
          "51:     )",
          "",
          "[Removed Lines]",
          "48: with warnings.catch_warnings(record=True):",
          "",
          "[Added Lines]",
          "48: with warnings.catch_warnings():",
          "49:     warnings.filterwarnings(",
          "50:         \"ignore\",",
          "51:         message=r\"This class is deprecated\\. Please use `airflow\\.utils\\.task_group\\.TaskGroup`\\.\",",
          "52:     )",
          "",
          "---------------"
        ],
        "tests/dags/test_subdag.py||tests/dags/test_subdag.py": [
          "File: tests/dags/test_subdag.py -> tests/dags/test_subdag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "68:         task_id=\"start\",",
          "69:     )",
          "72:         section_1 = SubDagOperator(",
          "73:             task_id=\"section-1\",",
          "74:             subdag=subdag(DAG_NAME, \"section-1\", DEFAULT_TASK_ARGS),",
          "",
          "[Removed Lines]",
          "71:     with warnings.catch_warnings(record=True):",
          "",
          "[Added Lines]",
          "71:     with warnings.catch_warnings():",
          "72:         warnings.filterwarnings(",
          "73:             \"ignore\",",
          "74:             message=r\"This class is deprecated\\. Please use `airflow\\.utils\\.task_group\\.TaskGroup`\\.\",",
          "75:         )",
          "",
          "---------------"
        ],
        "tests/jobs/test_backfill_job.py||tests/jobs/test_backfill_job.py": [
          "File: tests/jobs/test_backfill_job.py -> tests/jobs/test_backfill_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import json",
          "22: import logging",
          "23: import threading",
          "25: from collections import defaultdict",
          "26: from unittest import mock",
          "27: from unittest.mock import patch",
          "",
          "[Removed Lines]",
          "24: import warnings",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "36:     BackfillUnfinished,",
          "37:     DagConcurrencyLimitReached,",
          "38:     NoAvailablePoolSlot,",
          "40:     TaskConcurrencyLimitReached,",
          "41: )",
          "42: from airflow.executors.executor_constants import MOCK_EXECUTOR",
          "",
          "[Removed Lines]",
          "39:     RemovedInAirflow3Warning,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "78: @pytest.fixture(scope=\"module\")",
          "79: def dag_bag():",
          "87: # Patch the MockExecutor into the dict of known executors in the Loader",
          "",
          "[Removed Lines]",
          "80:     with warnings.catch_warnings():",
          "81:         # Some dags use deprecated operators, e.g SubDagOperator",
          "82:         # if it is not imported, then it might have side effects for the other tests",
          "83:         warnings.simplefilter(\"ignore\", category=RemovedInAirflow3Warning)",
          "84:         return DagBag(include_examples=True)",
          "",
          "[Added Lines]",
          "78:     return DagBag(include_examples=True)",
          "",
          "---------------"
        ],
        "tests/models/test_dagrun.py||tests/models/test_dagrun.py": [
          "File: tests/models/test_dagrun.py -> tests/models/test_dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import datetime",
          "22: from functools import reduce",
          "23: from typing import TYPE_CHECKING, Mapping",
          "24: from unittest import mock",
          "",
          "[Removed Lines]",
          "21: import warnings",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "30: from airflow import settings",
          "31: from airflow.callbacks.callback_requests import DagCallbackRequest",
          "32: from airflow.decorators import setup, task, task_group, teardown",
          "34: from airflow.models.baseoperator import BaseOperator",
          "35: from airflow.models.dag import DAG, DagModel",
          "36: from airflow.models.dagrun import DagRun, DagRunNote",
          "",
          "[Removed Lines]",
          "33: from airflow.exceptions import AirflowException, RemovedInAirflow3Warning",
          "",
          "[Added Lines]",
          "32: from airflow.exceptions import AirflowException",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "64: def dagbag():",
          "65:     from airflow.models.dagbag import DagBag",
          "76: class TestDagRun:",
          "",
          "[Removed Lines]",
          "67:     with warnings.catch_warnings():",
          "68:         # Some dags use deprecated operators, e.g SubDagOperator",
          "69:         # if it is not imported, then it might have side effects for the other tests",
          "70:         warnings.simplefilter(\"ignore\", category=RemovedInAirflow3Warning)",
          "71:         # Ensure the DAGs we are looking at from the DB are up-to-date",
          "72:         dag_bag = DagBag(include_examples=True)",
          "73:     return dag_bag",
          "",
          "[Added Lines]",
          "66:     return DagBag(include_examples=True)",
          "",
          "---------------"
        ],
        "tests/www/api/experimental/test_dag_runs_endpoint.py||tests/www/api/experimental/test_dag_runs_endpoint.py": [
          "File: tests/www/api/experimental/test_dag_runs_endpoint.py -> tests/www/api/experimental/test_dag_runs_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import json",
          "23: import pytest",
          "25: from airflow.api.common.trigger_dag import trigger_dag",
          "27: from airflow.models import DagBag, DagRun",
          "28: from airflow.models.serialized_dag import SerializedDagModel",
          "29: from airflow.settings import Session",
          "",
          "[Removed Lines]",
          "21: import warnings",
          "26: from airflow.exceptions import RemovedInAirflow3Warning",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "38:         session.query(DagRun).delete()",
          "39:         session.commit()",
          "40:         session.close()",
          "46:         for dag in dagbag.dags.values():",
          "47:             dag.sync_to_db()",
          "48:             SerializedDagModel.write_dag(dag)",
          "",
          "[Removed Lines]",
          "41:         with warnings.catch_warnings():",
          "42:             # Some dags use deprecated operators, e.g SubDagOperator",
          "43:             # if it is not imported, then it might have side effects for the other tests",
          "44:             warnings.simplefilter(\"ignore\", category=RemovedInAirflow3Warning)",
          "45:             dagbag = DagBag(include_examples=True)",
          "",
          "[Added Lines]",
          "40:         dagbag = DagBag(include_examples=True)",
          "",
          "---------------"
        ],
        "tests/www/views/conftest.py||tests/www/views/conftest.py": [
          "File: tests/www/views/conftest.py -> tests/www/views/conftest.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "21: from contextlib import contextmanager",
          "22: from typing import Any, Generator, NamedTuple",
          "",
          "[Removed Lines]",
          "20: import warnings",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "26: import pytest",
          "28: from airflow import settings",
          "30: from airflow.models import DagBag",
          "31: from airflow.www.app import create_app",
          "32: from tests.test_utils.api_connexion_utils import delete_user",
          "",
          "[Removed Lines]",
          "29: from airflow.exceptions import RemovedInAirflow3Warning",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "44: @pytest.fixture(autouse=True, scope=\"module\")",
          "45: def examples_dag_bag(session):",
          "53:     return dag_bag",
          "",
          "[Removed Lines]",
          "46:     with warnings.catch_warnings():",
          "47:         # Some dags use deprecated operators, e.g SubDagOperator",
          "48:         # if it is not imported, then it might have side effects for the other tests",
          "49:         warnings.simplefilter(\"ignore\", category=RemovedInAirflow3Warning)",
          "50:         DagBag(include_examples=True).sync_to_db()",
          "51:         dag_bag = DagBag(include_examples=True, read_dags_from_db=True)",
          "52:         session.commit()",
          "",
          "[Added Lines]",
          "44:     DagBag(include_examples=True).sync_to_db()",
          "45:     dag_bag = DagBag(include_examples=True, read_dags_from_db=True)",
          "46:     session.commit()",
          "",
          "---------------"
        ]
      }
    }
  ]
}