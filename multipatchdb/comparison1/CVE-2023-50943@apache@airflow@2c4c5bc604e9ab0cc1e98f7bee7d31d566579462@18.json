{
  "cve_id": "CVE-2023-50943",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows a potential attacker to poison the XCom data by bypassing the protection of \"enable_xcom_pickling=False\" configuration setting resulting in poisoned data after XCom deserialization. This vulnerability is considered low since it requires a DAG author to exploit it. Users are recommended to upgrade to version 2.8.1 or later, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
  "patch_info": {
    "commit_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "files": [
      "airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py"
    ],
    "message": "Stop deserializing pickle when enable_xcom_pickling is False (#36255)\n\n* Stop deserializing pickle when enable_xcom_pickling is False\n\n* Fix unit tests\n\n(cherry picked from commit 63e97abec5d56bc62a293c93f5227f364561e51c)",
    "before_after_code_files": [
      "airflow/models/xcom.py||airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py||tests/models/test_xcom.py"
    ]
  },
  "patch_diff": {
    "airflow/models/xcom.py||airflow/models/xcom.py": [
      "File: airflow/models/xcom.py -> airflow/models/xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "685:             except pickle.UnpicklingError:",
      "686:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "687:         else:",
      "693:     @staticmethod",
      "694:     def deserialize_value(result: XCom) -> Any:",
      "",
      "[Removed Lines]",
      "688:             try:",
      "689:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "690:             except (json.JSONDecodeError, UnicodeDecodeError):",
      "691:                 return pickle.loads(result.value)",
      "",
      "[Added Lines]",
      "688:             # Since xcom_pickling is disabled, we should only try to deserialize with JSON",
      "689:             return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "",
      "---------------"
    ],
    "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py": [
      "File: tests/api_connexion/schemas/test_xcom_schema.py -> tests/api_connexion/schemas/test_xcom_schema.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from airflow.models import DagRun, XCom",
      "31: from airflow.utils.dates import parse_execution_date",
      "32: from airflow.utils.session import create_session",
      "34: pytestmark = pytest.mark.db_test",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "188:     default_time = \"2016-04-02T21:00:00+00:00\"",
      "189:     default_time_parsed = parse_execution_date(default_time)",
      "191:     def test_serialize(self, create_xcom, session):",
      "192:         create_xcom(",
      "193:             dag_id=\"test_dag\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "192:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "208:             \"map_index\": -1,",
      "209:         }",
      "211:     def test_deserialize(self):",
      "212:         xcom_dump = {",
      "213:             \"key\": \"test_key\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "213:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------"
    ],
    "tests/models/test_xcom.py||tests/models/test_xcom.py": [
      "File: tests/models/test_xcom.py -> tests/models/test_xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "140:             ret_value = XCom.get_value(key=\"xcom_test3\", ti_key=ti_key, session=session)",
      "141:         assert ret_value == {\"key\": \"value\"}",
      "144:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"}):",
      "145:             XCom.set(",
      "146:                 key=\"xcom_test3\",",
      "",
      "[Removed Lines]",
      "143:     def test_xcom_deserialize_with_pickle_to_json_switch(self, task_instance, session):",
      "",
      "[Added Lines]",
      "143:     def test_xcom_deserialize_pickle_when_xcom_pickling_is_disabled(self, task_instance, session):",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "151:                 session=session,",
      "152:             )",
      "153:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"False\"}):",
      "163:     @conf_vars({(\"core\", \"xcom_enable_pickling\"): \"False\"})",
      "164:     def test_xcom_disable_pickle_type_fail_on_non_json(self, task_instance, session):",
      "",
      "[Removed Lines]",
      "154:             ret_value = XCom.get_one(",
      "155:                 key=\"xcom_test3\",",
      "156:                 dag_id=task_instance.dag_id,",
      "157:                 task_id=task_instance.task_id,",
      "158:                 run_id=task_instance.run_id,",
      "159:                 session=session,",
      "160:             )",
      "161:         assert ret_value == {\"key\": \"value\"}",
      "",
      "[Added Lines]",
      "154:             with pytest.raises(UnicodeDecodeError):",
      "155:                 XCom.get_one(",
      "156:                     key=\"xcom_test3\",",
      "157:                     dag_id=task_instance.dag_id,",
      "158:                     task_id=task_instance.task_id,",
      "159:                     run_id=task_instance.run_id,",
      "160:                     session=session,",
      "161:                 )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "42263f0bbb87c2833398ae70b05e864e22f042eb",
      "candidate_info": {
        "commit_hash": "42263f0bbb87c2833398ae70b05e864e22f042eb",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/42263f0bbb87c2833398ae70b05e864e22f042eb",
        "files": [
          ".github/workflows/ci.yml",
          ".pre-commit-config.yaml",
          ".rat-excludes",
          "BREEZE.rst",
          "STATIC_CODE_CHECKS.rst",
          "clients/README.md",
          "clients/gen/common.sh",
          "clients/gen/go.sh",
          "clients/gen/python.sh",
          "clients/python/.gitignore",
          "clients/python/.openapi-generator-ignore",
          "clients/python/CHANGELOG.md",
          "clients/python/INSTALL",
          "clients/python/LICENSE",
          "clients/python/NOTICE",
          "clients/python/README.md",
          "clients/python/pyproject.toml",
          "clients/python/test_python_client.py",
          "clients/python/version.txt",
          "dev/README_RELEASE_AIRFLOW.md",
          "dev/README_RELEASE_PYTHON_CLIENT.md",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "images/breeze/output_release-management.svg",
          "images/breeze/output_release-management.txt",
          "images/breeze/output_release-management_prepare-airflow-package.svg",
          "images/breeze/output_release-management_prepare-airflow-package.txt",
          "images/breeze/output_release-management_prepare-python-client.svg",
          "images/breeze/output_release-management_prepare-python-client.txt",
          "images/breeze/output_setup_check-all-params-in-groups.svg",
          "images/breeze/output_setup_check-all-params-in-groups.txt",
          "images/breeze/output_setup_regenerate-command-images.svg",
          "images/breeze/output_setup_regenerate-command-images.txt",
          "scripts/ci/docker-compose/local.yml",
          "scripts/ci/openapi/client_codegen_diff.sh"
        ],
        "message": "Generate Python client in reproducible way (#36763)\n\nClient source code and package generation was done using the code\ngenerated and committed to `airflow-client-python` and while the\nrepository with such code is useful to have, it's just a convenience\nrepo, because all sources are (and should be) generated from the\nAPI specification which is present in the Airflow repository.\n\nThis also made the reproducible builds and package generation not really\npossible, because we never knew if the source generated in the\n`airflow-client-python` repository has been generated and not tampered\nwith.\n\nWhile implementing it, it turned out that there were some issues in\nthe past that nade our client generation somewhat broken..\n\n* In 2.7.0 python client, we added the same code twice\n  (See https://github.com/apache/airflow-client-python/pull/93) on\n  top of \"airflow_client.client\" package, we also added copy of the\n  API client generated in \"airflow_client.airflow_client\" - that was\n  likely due to bad bash scripts and tools that were used to generate\n  it and errors during generation the clients.\n\n* We used to generate the code for \"client\" package and then moved\n  the \"client\" package to \"airflow_client.client\" package, while\n  manually modifying imports with `sed` (!?). That was likely due to\n  limitations in some old version of the client generator. However the\n  client generator we use now is capable of generating code directly in\n  the \"airflow_client.client\" package.\n\n* We also manually (via pre-commit) added Apache Licence to the\n  generated files. Whieh was completely unnecessary, because ASF rules\n  do not require licence headers to be added to code automatically\n  generated from a code that already has ASF licence.\n\n* We also generated source tarball packages from such generated code,\n  which was completely unnecessary - because sdist packages are already\n  fulfilling all the reqirements of such source pacakges - the code\n  in the packages is enough to build the package from the sources and\n  it does not contain any binary code, moreover the code is generated\n  out of the API specificiation, which means that anyone can take\n  the code and genearate the pacakged software from just sources in\n  sdist. Similarly as in case of provider packages, we do not need\n  to produce separate -source.tar.gz files.\n\nThis PR fixes all of it.\n\nFirst of all the source that lands in the source repository\n`airflow-client-python` and sdist/wheel packages are generated directly\nfrom the openapi specification.\n\nThey are generated using breeze release_management command from airflow\nsource  tagged with specific tag in the Airflow repo (including the\nsource of reproducible build date that is updated together with airflow\nrelease notes. This means that any PMC member can regenerate packages\n(binary identical) straight from the Airflow repository - without\ngoing through \"airflow-client-python\" repository.\n\nNo source tarball is generated - it is not needed, sdist is enough.\n\nThe `test_python_client.py` has been also moved over to Airflow repo\nand updated with handling case when expose_config is not enabled and\nit is used to automatically test the API client after it has been\ngenerated.\n\n(cherry picked from commit 9787440593196881b481466aa6d3cca4408f99e5)",
        "before_after_code_files": [
          "clients/gen/common.sh||clients/gen/common.sh",
          "clients/gen/go.sh||clients/gen/go.sh",
          "clients/gen/python.sh||clients/gen/python.sh",
          "clients/python/test_python_client.py||clients/python/test_python_client.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "scripts/ci/openapi/client_codegen_diff.sh||scripts/ci/openapi/client_codegen_diff.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "clients/gen/common.sh||clients/gen/common.sh": [
          "File: clients/gen/common.sh -> clients/gen/common.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "clients/gen/go.sh||clients/gen/go.sh": [
          "File: clients/gen/go.sh -> clients/gen/go.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "clients/gen/python.sh||clients/gen/python.sh": [
          "File: clients/gen/python.sh -> clients/gen/python.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "clients/python/test_python_client.py||clients/python/test_python_client.py": [
          "File: clients/python/test_python_client.py -> clients/python/test_python_client.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "17: #",
          "18: # PEP 723 compliant inline script metadata (not yet widely supported)",
          "19: # /// script",
          "20: # requires-python = \">=3.8\"",
          "21: # dependencies = [",
          "22: #   \"apache-airflow-client\",",
          "23: #   \"rich\",",
          "24: # ]",
          "25: # ///",
          "27: from __future__ import annotations",
          "29: import sys",
          "30: import uuid",
          "32: import airflow_client.client",
          "34: try:",
          "35:     # If you have rich installed, you will have nice colored output of the API responses",
          "36:     from rich import print",
          "37: except ImportError:",
          "38:     print(\"Output will not be colored. Please install rich to get colored output: `pip install rich`\")",
          "39:     pass",
          "40: from airflow_client.client.api import config_api, dag_api, dag_run_api",
          "41: from airflow_client.client.model.dag_run import DAGRun",
          "43: # The client must use the authentication and authorization parameters",
          "44: # in accordance with the API server security policy.",
          "45: # Examples for each auth method are provided below, use the example that",
          "46: # satisfies your auth use case.",
          "47: #",
          "48: # In case of the basic authentication below, make sure that Airflow is",
          "49: # configured also with the basic_auth as backend additionally to regular session backend needed",
          "50: # by the UI. In the `[api]` section of your `airflow.cfg` set:",
          "51: #",
          "52: # auth_backend = airflow.api.auth.backend.session,airflow.api.auth.backend.basic_auth",
          "53: #",
          "54: # Make sure that your user/name are configured properly - using the user/password that has admin",
          "55: # privileges in Airflow",
          "57: # Configure HTTP basic authorization: Basic",
          "58: configuration = airflow_client.client.Configuration(",
          "59:     host=\"http://localhost:8080/api/v1\", username=\"admin\", password=\"admin\"",
          "60: )",
          "62: # Make sure in the [core] section, the  `load_examples` config is set to True in your airflow.cfg",
          "63: # or AIRFLOW__CORE__LOAD_EXAMPLES environment variable set to True",
          "64: DAG_ID = \"example_bash_operator\"",
          "66: # Enter a context with an instance of the API client",
          "67: with airflow_client.client.ApiClient(configuration) as api_client:",
          "68:     errors = False",
          "70:     print(\"[blue]Getting DAG list\")",
          "71:     dag_api_instance = dag_api.DAGApi(api_client)",
          "72:     try:",
          "73:         api_response = dag_api_instance.get_dags()",
          "74:         print(api_response)",
          "75:     except airflow_client.client.OpenApiException as e:",
          "76:         print(f\"[red]Exception when calling DagAPI->get_dags: {e}\\n\")",
          "77:         errors = True",
          "78:     else:",
          "79:         print(\"[green]Getting DAG list successful\")",
          "81:     print(\"[blue]Getting Tasks for a DAG\")",
          "82:     try:",
          "83:         api_response = dag_api_instance.get_tasks(DAG_ID)",
          "84:         print(api_response)",
          "85:     except airflow_client.client.exceptions.OpenApiException as e:",
          "86:         print(f\"[red]Exception when calling DagAPI->get_tasks: {e}\\n\")",
          "87:         errors = True",
          "88:     else:",
          "89:         print(\"[green]Getting Tasks successful\")",
          "91:     print(\"[blue]Triggering a DAG run\")",
          "92:     dag_run_api_instance = dag_run_api.DAGRunApi(api_client)",
          "93:     try:",
          "94:         # Create a DAGRun object (no dag_id should be specified because it is read-only property of DAGRun)",
          "95:         # dag_run id is generated randomly to allow multiple executions of the script",
          "96:         dag_run = DAGRun(",
          "97:             dag_run_id=\"some_test_run_\" + uuid.uuid4().hex,",
          "98:         )",
          "99:         api_response = dag_run_api_instance.post_dag_run(DAG_ID, dag_run)",
          "100:         print(api_response)",
          "101:     except airflow_client.client.exceptions.OpenApiException as e:",
          "102:         print(f\"[red]Exception when calling DAGRunAPI->post_dag_run: {e}\\n\")",
          "103:         errors = True",
          "104:     else:",
          "105:         print(\"[green]Posting DAG Run successful\")",
          "107:     # Get current configuration. Note, this is disabled by default with most installation.",
          "108:     # You need to set `expose_config = True` in Airflow configuration in order to retrieve configuration.",
          "109:     conf_api_instance = config_api.ConfigApi(api_client)",
          "110:     try:",
          "111:         api_response = conf_api_instance.get_config()",
          "112:         print(api_response)",
          "113:     except airflow_client.client.OpenApiException as e:",
          "114:         if \"FORBIDDEN\" in str(e):",
          "115:             print(",
          "116:                 \"[yellow]You need to set `expose_config = True` in Airflow configuration\"",
          "117:                 \" in order to retrieve configuration.\"",
          "118:             )",
          "119:             print(\"[bright_blue]This is OK. Exposing config is disabled by default.\")",
          "120:         else:",
          "121:             print(f\"[red]Exception when calling DAGRunAPI->post_dag_run: {e}\\n\")",
          "122:             errors = True",
          "123:     else:",
          "124:         print(\"[green]Config retrieved successfully\")",
          "126:     if errors:",
          "127:         print(\"\\n[red]There were errors while running the script - see above for details\")",
          "128:         sys.exit(1)",
          "129:     else:",
          "130:         print(\"\\n[green]Everything went well\")",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "191:     show_default=True,",
          "192:     envvar=\"PACKAGE_FORMAT\",",
          "193: )",
          "195: if TYPE_CHECKING:",
          "196:     from packaging.version import Version",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "194: option_use_local_hatch = click.option(",
          "195:     \"--use-local-hatch\",",
          "196:     is_flag=True,",
          "197:     help=\"Use local hatch instead of docker to build the package. You need to have hatch installed.\",",
          "198: )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "279: AIRFLOW_BUILD_DOCKERFILE_IGNORE_PATH = AIRFLOW_SOURCES_ROOT / \"airflow-build-dockerfile.dockerignore\"",
          "322:     # This is security feature.",
          "323:     #",
          "324:     # Building the image needed to build airflow package including .git directory",
          "",
          "[Removed Lines]",
          "282: @release_management.command(",
          "283:     name=\"prepare-airflow-package\",",
          "284:     help=\"Prepare sdist/whl package of Airflow.\",",
          "285: )",
          "286: @click.option(",
          "287:     \"--use-local-hatch\",",
          "288:     is_flag=True,",
          "289:     help=\"Use local hatch instead of docker to build the package. You need to have hatch installed.\",",
          "290: )",
          "291: @option_package_format",
          "292: @option_version_suffix_for_pypi",
          "293: @option_verbose",
          "294: @option_dry_run",
          "295: def prepare_airflow_packages(",
          "296:     package_format: str,",
          "297:     version_suffix_for_pypi: str,",
          "298:     use_local_hatch: bool,",
          "299: ):",
          "300:     perform_environment_checks()",
          "301:     fix_ownership_using_docker()",
          "302:     cleanup_python_generated_files()",
          "303:     source_date_epoch = get_source_date_epoch()",
          "304:     if use_local_hatch:",
          "305:         hatch_build_command = [\"hatch\", \"build\", \"-c\", \"-t\", \"custom\"]",
          "306:         if package_format in [\"sdist\", \"both\"]:",
          "307:             hatch_build_command.extend([\"-t\", \"sdist\"])",
          "308:         if package_format in [\"wheel\", \"both\"]:",
          "309:             hatch_build_command.extend([\"-t\", \"wheel\"])",
          "310:         env_copy = os.environ.copy()",
          "311:         env_copy[\"SOURCE_DATE_EPOCH\"] = str(source_date_epoch)",
          "312:         run_command(",
          "313:             hatch_build_command,",
          "314:             check=True,",
          "315:             env=env_copy,",
          "316:         )",
          "317:         get_console().print(\"[success]Successfully prepared Airflow packages:\")",
          "318:         for file in sorted(DIST_DIR.glob(\"apache_airflow*\")):",
          "319:             get_console().print(file.name)",
          "320:         get_console().print()",
          "321:         return",
          "",
          "[Added Lines]",
          "287: def _build_local_build_image():",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "344:         cwd=AIRFLOW_SOURCES_ROOT,",
          "345:         env={\"DOCKER_CLI_HINTS\": \"false\"},",
          "346:     )",
          "347:     container_id = f\"airflow-build-{random.getrandbits(64):08x}\"",
          "348:     result = run_command(",
          "349:         cmd=[",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "315: def _build_airflow_packages_with_docker(",
          "316:     package_format: str, source_date_epoch: int, version_suffix_for_pypi: str",
          "317: ):",
          "318:     _build_local_build_image()",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "355:             \"-e\",",
          "356:             f\"VERSION_SUFFIX_FOR_PYPI={version_suffix_for_pypi}\",",
          "357:             \"-e\",",
          "358:             \"HOME=/opt/airflow/files/home\",",
          "359:             \"-e\",",
          "360:             \"GITHUB_ACTIONS\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "330:             f\"SOURCE_DATE_EPOCH={source_date_epoch}\",",
          "331:             \"-e\",",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "375:     DIST_DIR.mkdir(parents=True, exist_ok=True)",
          "376:     # Copy all files in the dist directory in container to the host dist directory (note '/.' in SRC)",
          "377:     run_command([\"docker\", \"cp\", f\"{container_id}:/opt/airflow/dist/.\", \"./dist\"], check=True)",
          "379:     get_console().print(\"[success]Successfully prepared Airflow packages:\")",
          "380:     for file in sorted(DIST_DIR.glob(\"apache_airflow*\")):",
          "381:         get_console().print(file.name)",
          "",
          "[Removed Lines]",
          "378:     run_command([\"docker\", \"rm\", \"--force\", container_id], check=True)",
          "",
          "[Added Lines]",
          "352:     run_command([\"docker\", \"rm\", \"--force\", container_id], check=False, stderr=DEVNULL, stdout=DEVNULL)",
          "355: def _build_airflow_packages_with_hatch(",
          "356:     package_format: str, source_date_epoch: int, version_suffix_for_pypi: str",
          "357: ):",
          "358:     hatch_build_command = [\"hatch\", \"build\", \"-c\", \"-t\", \"custom\"]",
          "359:     if package_format in [\"sdist\", \"both\"]:",
          "360:         hatch_build_command.extend([\"-t\", \"sdist\"])",
          "361:     if package_format in [\"wheel\", \"both\"]:",
          "362:         hatch_build_command.extend([\"-t\", \"wheel\"])",
          "363:     env_copy = os.environ.copy()",
          "364:     env_copy[\"SOURCE_DATE_EPOCH\"] = str(source_date_epoch)",
          "365:     env_copy[\"VERSION_SUFFIX_FOR_PYPI\"] = version_suffix_for_pypi",
          "366:     run_command(",
          "367:         hatch_build_command,",
          "368:         check=True,",
          "369:         env=env_copy,",
          "370:     )",
          "373: @release_management.command(",
          "374:     name=\"prepare-airflow-package\",",
          "375:     help=\"Prepare sdist/whl package of Airflow.\",",
          "376: )",
          "377: @option_package_format",
          "378: @option_version_suffix_for_pypi",
          "379: @option_use_local_hatch",
          "380: @option_verbose",
          "381: @option_dry_run",
          "382: def prepare_airflow_packages(",
          "383:     package_format: str,",
          "384:     version_suffix_for_pypi: str,",
          "385:     use_local_hatch: bool,",
          "386: ):",
          "387:     perform_environment_checks()",
          "388:     fix_ownership_using_docker()",
          "389:     cleanup_python_generated_files()",
          "390:     source_date_epoch = get_source_date_epoch()",
          "391:     if use_local_hatch:",
          "392:         _build_airflow_packages_with_hatch(",
          "393:             package_format=package_format,",
          "394:             source_date_epoch=source_date_epoch,",
          "395:             version_suffix_for_pypi=version_suffix_for_pypi,",
          "396:         )",
          "397:     else:",
          "398:         _build_airflow_packages_with_docker(",
          "399:             package_format=package_format,",
          "400:             source_date_epoch=source_date_epoch,",
          "401:             version_suffix_for_pypi=version_suffix_for_pypi,",
          "402:         )",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "2078:         type=no_version_file + \"-\" + suffix,",
          "2079:         comparable_version=Version(version),",
          "2080:     )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2107: PYTHON_CLIENT_DIR_PATH = AIRFLOW_SOURCES_ROOT / \"clients\" / \"python\"",
          "2108: PYTHON_CLIENT_DIST_DIR_PATH = PYTHON_CLIENT_DIR_PATH / \"dist\"",
          "2109: PYTHON_CLIENT_TMP_DIR = PYTHON_CLIENT_DIR_PATH / \"tmp\"",
          "2111: REPRODUCIBLE_BUILD_YAML = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"reproducible_build.yaml\"",
          "2113: VERSION_FILE = PYTHON_CLIENT_DIR_PATH / \"version.txt\"",
          "2114: SOURCE_API_YAML_PATH = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"api_connexion\" / \"openapi\" / \"v1.yaml\"",
          "2115: TARGET_API_YAML_PATH = PYTHON_CLIENT_DIR_PATH / \"v1.yaml\"",
          "2116: OPENAPI_GENERATOR_CLI_VER = \"5.4.0\"",
          "2118: GENERATED_CLIENT_DIRECTORIES_TO_COPY = [\"airflow_client\", \"docs\", \"test\"]",
          "2119: FILES_TO_COPY_TO_CLIENT_REPO = [",
          "2120:     \".gitignore\",",
          "2121:     \".openapi-generator-ignore\",",
          "2122:     \"CHANGELOG.md\",",
          "2123:     \"README.md\",",
          "2124:     \"INSTALL\",",
          "2125:     \"LICENSE\",",
          "2126:     \"NOTICE\",",
          "2127:     \"pyproject.toml\",",
          "2128:     \"test_python_client.py\",",
          "2129:     \"version.txt\",",
          "2130: ]",
          "2133: def _get_python_client_version(version_suffix_for_pypi):",
          "2134:     from packaging.version import Version",
          "2136:     python_client_version = VERSION_FILE.read_text().strip()",
          "2137:     version = Version(python_client_version)",
          "2138:     if version_suffix_for_pypi:",
          "2139:         if version.pre:",
          "2140:             currrent_suffix = version.pre[0] + str(version.pre[1])",
          "2141:             if currrent_suffix != version_suffix_for_pypi:",
          "2142:                 get_console().print(",
          "2143:                     f\"[error]The version suffix for PyPI ({version_suffix_for_pypi}) does not match the \"",
          "2144:                     f\"suffix in the version ({version})[/]\"",
          "2145:                 )",
          "2146:                 sys.exit(1)",
          "2147:     return version.base_version + version_suffix_for_pypi",
          "2150: def _generate_python_client_sources(python_client_version: str) -> None:",
          "2151:     get_console().print(f\"\\n[info]Generating client code in {PYTHON_CLIENT_TMP_DIR}[/]\")",
          "2152:     result = run_command(",
          "2153:         [",
          "2154:             \"docker\",",
          "2155:             \"run\",",
          "2156:             \"--rm\",",
          "2157:             \"-u\",",
          "2158:             f\"{os.getuid()}:{os.getgid()}\",",
          "2159:             \"-v\",",
          "2160:             f\"{TARGET_API_YAML_PATH}:/spec.yaml\",",
          "2161:             \"-v\",",
          "2162:             f\"{PYTHON_CLIENT_TMP_DIR}:/output\",",
          "2163:             f\"openapitools/openapi-generator-cli:v{OPENAPI_GENERATOR_CLI_VER}\",",
          "2164:             \"generate\",",
          "2165:             \"--input-spec\",",
          "2166:             \"/spec.yaml\",",
          "2167:             \"--generator-name\",",
          "2168:             \"python\",",
          "2169:             \"--git-user-id\",",
          "2170:             f\"{os.environ.get('GIT_USER')}\",",
          "2171:             \"--output\",",
          "2172:             \"/output\",",
          "2173:             \"--package-name\",",
          "2174:             \"airflow_client.client\",",
          "2175:             \"--git-repo-id\",",
          "2176:             \"airflow-client-python\",",
          "2177:             \"--additional-properties\",",
          "2178:             f'packageVersion=\"{python_client_version}\"',",
          "2179:         ],",
          "2180:         capture_output=True,",
          "2181:         text=True,",
          "2182:     )",
          "2183:     if result.returncode != 0:",
          "2184:         get_console().print(\"[error]Failed to generate client code[/]\")",
          "2185:         get_console().print(result.stdout, markup=False)",
          "2186:         get_console().print(result.stderr, markup=False, style=\"error\")",
          "2187:         sys.exit(result.returncode)",
          "2188:     get_console().print(f\"[success]Generated client code in {PYTHON_CLIENT_TMP_DIR}:[/]\")",
          "2189:     get_console().print(f\"\\n[info]Content of {PYTHON_CLIENT_TMP_DIR}:[/]\")",
          "2190:     for file in sorted(PYTHON_CLIENT_TMP_DIR.glob(\"*\")):",
          "2191:         get_console().print(f\"[info]  {file.name}[/]\")",
          "2192:     get_console().print()",
          "2195: def _copy_selected_sources_from_tmp_directory_to_clients_python():",
          "2196:     get_console().print(",
          "2197:         f\"[info]Copying selected sources: {GENERATED_CLIENT_DIRECTORIES_TO_COPY} from \"",
          "2198:         f\"{PYTHON_CLIENT_TMP_DIR} to {PYTHON_CLIENT_DIR_PATH}[/]\"",
          "2199:     )",
          "2200:     for dir in GENERATED_CLIENT_DIRECTORIES_TO_COPY:",
          "2201:         source_dir = PYTHON_CLIENT_TMP_DIR / dir",
          "2202:         target_dir = PYTHON_CLIENT_DIR_PATH / dir",
          "2203:         get_console().print(f\"[info]  Copying generated sources from {source_dir} to {target_dir}[/]\")",
          "2204:         shutil.rmtree(target_dir, ignore_errors=True)",
          "2205:         shutil.copytree(source_dir, target_dir)",
          "2206:         get_console().print(f\"[success]  Copied generated sources from {source_dir} to {target_dir}[/]\")",
          "2207:     get_console().print(",
          "2208:         f\"[info]Copied selected sources {GENERATED_CLIENT_DIRECTORIES_TO_COPY} from \"",
          "2209:         f\"{PYTHON_CLIENT_TMP_DIR} to {PYTHON_CLIENT_DIR_PATH}[/]\\n\"",
          "2210:     )",
          "2211:     get_console().print(f\"\\n[info]Content of {PYTHON_CLIENT_DIR_PATH}:[/]\")",
          "2212:     for file in sorted(PYTHON_CLIENT_DIR_PATH.glob(\"*\")):",
          "2213:         get_console().print(f\"[info]  {file.name}[/]\")",
          "2214:     get_console().print()",
          "2217: def _build_client_packages_with_hatch(source_date_epoch: int, package_format: str):",
          "2218:     command = [",
          "2219:         \"hatch\",",
          "2220:         \"build\",",
          "2221:         \"-c\",",
          "2222:     ]",
          "2223:     if package_format == \"sdist\" or package_format == \"both\":",
          "2224:         command += [\"-t\", \"sdist\"]",
          "2225:     if package_format == \"wheel\" or package_format == \"both\":",
          "2226:         command += [\"-t\", \"wheel\"]",
          "2227:     env_copy = os.environ.copy()",
          "2228:     env_copy[\"SOURCE_DATE_EPOCH\"] = str(source_date_epoch)",
          "2229:     run_command(",
          "2230:         cmd=command,",
          "2231:         cwd=PYTHON_CLIENT_DIR_PATH,",
          "2232:         env=env_copy,",
          "2233:         check=True,",
          "2234:     )",
          "2235:     shutil.copytree(PYTHON_CLIENT_DIST_DIR_PATH, DIST_DIR, dirs_exist_ok=True)",
          "2238: def _build_client_packages_with_docker(source_date_epoch: int, package_format: str):",
          "2239:     _build_local_build_image()",
          "2240:     command = \"hatch build -c \"",
          "2241:     if package_format == \"sdist\" or package_format == \"both\":",
          "2242:         command += \"-t sdist \"",
          "2243:     if package_format == \"wheel\" or package_format == \"both\":",
          "2244:         command += \"-t wheel \"",
          "2245:     container_id = f\"airflow-build-{random.getrandbits(64):08x}\"",
          "2246:     result = run_command(",
          "2247:         cmd=[",
          "2248:             \"docker\",",
          "2249:             \"run\",",
          "2250:             \"--name\",",
          "2251:             container_id,",
          "2252:             \"-t\",",
          "2253:             \"-e\",",
          "2254:             f\"SOURCE_DATE_EPOCH={source_date_epoch}\",",
          "2255:             \"-e\",",
          "2256:             \"HOME=/opt/airflow/files/home\",",
          "2257:             \"-e\",",
          "2258:             \"GITHUB_ACTIONS\",",
          "2259:             \"-w\",",
          "2260:             \"/opt/airflow/clients/python\",",
          "2261:             AIRFLOW_BUILD_IMAGE_TAG,",
          "2262:             \"bash\",",
          "2263:             \"-c\",",
          "2264:             command,",
          "2265:         ],",
          "2266:         check=False,",
          "2267:     )",
          "2268:     if result.returncode != 0:",
          "2269:         get_console().print(\"[error]Error preparing Python client packages[/]\")",
          "2270:         fix_ownership_using_docker()",
          "2271:         sys.exit(result.returncode)",
          "2272:     DIST_DIR.mkdir(parents=True, exist_ok=True)",
          "2273:     get_console().print()",
          "2274:     # Copy all files in the dist directory in container to the host dist directory (note '/.' in SRC)",
          "2275:     run_command([\"docker\", \"cp\", f\"{container_id}:/opt/airflow/clients/python/dist/.\", \"./dist\"], check=True)",
          "2276:     run_command([\"docker\", \"rm\", \"--force\", container_id], check=False, stdout=DEVNULL, stderr=DEVNULL)",
          "2279: @release_management.command(name=\"prepare-python-client\", help=\"Prepares python client packages.\")",
          "2280: @option_package_format",
          "2281: @option_version_suffix_for_pypi",
          "2282: @option_use_local_hatch",
          "2283: @click.option(",
          "2284:     \"--python-client-repo\",",
          "2285:     envvar=\"PYTHON_CLIENT_REPO\",",
          "2286:     type=click.Path(file_okay=False, dir_okay=True, path_type=Path, exists=True),",
          "2287:     help=\"Directory where the python client repo is checked out\",",
          "2288: )",
          "2289: @click.option(",
          "2290:     \"--only-publish-build-scripts\",",
          "2291:     envvar=\"ONLY_PUBLISH_BUILD_SCRIPTS\",",
          "2292:     is_flag=True,",
          "2293:     help=\"Only publish updated build scripts to puthon client repo, not generated client code.\",",
          "2294: )",
          "2295: @click.option(",
          "2296:     \"--security-schemes\",",
          "2297:     default=\"Basic,GoogleOpenID,Kerberos\",",
          "2298:     envvar=\"SECURITY_SCHEMES\",",
          "2299:     show_default=True,",
          "2300:     help=\"Security schemes to be added to the API documentation (coma separated)\",",
          "2301: )",
          "2302: @option_dry_run",
          "2303: @option_verbose",
          "2304: def prepare_python_client(",
          "2305:     package_format: str,",
          "2306:     version_suffix_for_pypi: str,",
          "2307:     use_local_hatch: bool,",
          "2308:     python_client_repo: Path | None,",
          "2309:     only_publish_build_scripts: bool,",
          "2310:     security_schemes: str,",
          "2311: ):",
          "2312:     shutil.rmtree(PYTHON_CLIENT_TMP_DIR, ignore_errors=True)",
          "2313:     PYTHON_CLIENT_TMP_DIR.mkdir(parents=True, exist_ok=True)",
          "2314:     shutil.copy(src=SOURCE_API_YAML_PATH, dst=TARGET_API_YAML_PATH)",
          "2315:     import yaml",
          "2317:     openapi_yaml = yaml.safe_load(TARGET_API_YAML_PATH.read_text())",
          "2319:     # Add security schemes to documentation",
          "2320:     security: list[dict[str, Any]] = []",
          "2321:     for scheme in security_schemes.split(\",\"):",
          "2322:         security.append({scheme: []})",
          "2323:     openapi_yaml[\"security\"] = security",
          "2324:     python_client_version = _get_python_client_version(version_suffix_for_pypi)",
          "2325:     TARGET_API_YAML_PATH.write_text(yaml.dump(openapi_yaml))",
          "2327:     _generate_python_client_sources(python_client_version=python_client_version)",
          "2328:     _copy_selected_sources_from_tmp_directory_to_clients_python()",
          "2330:     reproducible_build_yaml = yaml.safe_load(REPRODUCIBLE_BUILD_YAML.read_text())",
          "2331:     source_date_epoch = reproducible_build_yaml[\"source-date-epoch\"]",
          "2333:     if python_client_repo:",
          "2334:         if not only_publish_build_scripts:",
          "2335:             get_console().print(",
          "2336:                 f\"[info]Copying generated client from {PYTHON_CLIENT_DIR_PATH} to {python_client_repo}[/]\"",
          "2337:             )",
          "2338:             for dir in GENERATED_CLIENT_DIRECTORIES_TO_COPY:",
          "2339:                 source_dir = PYTHON_CLIENT_DIR_PATH / dir",
          "2340:                 target_dir = python_client_repo / dir",
          "2341:                 get_console().print(f\"[info]  Copying {source_dir} to {target_dir}[/]\")",
          "2342:                 shutil.rmtree(target_dir, ignore_errors=True)",
          "2343:                 shutil.copytree(source_dir, target_dir)",
          "2344:                 get_console().print(f\"[success]  Copied {source_dir} to {target_dir}[/]\")",
          "2345:             get_console().print(",
          "2346:                 f\"[info]Copied generated client from {PYTHON_CLIENT_DIR_PATH} to {python_client_repo}[/]\"",
          "2347:             )",
          "2348:         get_console().print(",
          "2349:             f\"[info]Copying build scripts from {PYTHON_CLIENT_DIR_PATH} to {python_client_repo}[/]\"",
          "2350:         )",
          "2351:         for file in FILES_TO_COPY_TO_CLIENT_REPO:",
          "2352:             source_file = PYTHON_CLIENT_DIR_PATH / file",
          "2353:             target_file = python_client_repo / file",
          "2354:             get_console().print(f\"[info]  Copying {source_file} to {target_file}[/]\")",
          "2355:             shutil.copy(source_file, target_file)",
          "2356:             get_console().print(f\"[success]  Copied {source_file} to {target_file}[/]\")",
          "2357:         get_console().print(",
          "2358:             f\"[success]Copied build scripts from {PYTHON_CLIENT_DIR_PATH} to {python_client_repo}[/]\"",
          "2359:         )",
          "2360:         spec_dir = python_client_repo / \"spec\"",
          "2361:         spec_dir.mkdir(parents=True, exist_ok=True)",
          "2362:         source_spec_file = PYTHON_CLIENT_DIR_PATH / \"v1.yaml\"",
          "2363:         target_spec_file = spec_dir / \"v1.yaml\"",
          "2364:         get_console().print(f\"[info]  Copying {source_spec_file} to {target_spec_file}[/]\")",
          "2365:         shutil.copy(source_spec_file, target_spec_file)",
          "2366:         get_console().print(f\"[success]  Copied {source_spec_file} to {target_spec_file}[/]\")",
          "2367:         get_console().print(",
          "2368:             f\"[success]Copied client code from {PYTHON_CLIENT_DIR_PATH} to {python_client_repo}[/]\\n\"",
          "2369:         )",
          "2370:     else:",
          "2371:         get_console().print(",
          "2372:             \"\\n[warning]No python client repo directory provided - skipping copying the generated client[/]\\n\"",
          "2373:         )",
          "2374:     get_console().print(f\"\\n[info]Building packages in {PYTHON_CLIENT_DIST_DIR_PATH}[/]\\n\")",
          "2375:     shutil.rmtree(PYTHON_CLIENT_DIST_DIR_PATH, ignore_errors=True)",
          "2376:     PYTHON_CLIENT_DIST_DIR_PATH.mkdir(parents=True, exist_ok=True)",
          "2377:     version = _get_python_client_version(version_suffix_for_pypi)",
          "2378:     original_version = VERSION_FILE.read_text().strip()",
          "2379:     if version_suffix_for_pypi:",
          "2380:         VERSION_FILE.write_text(version)",
          "2381:     try:",
          "2382:         if use_local_hatch:",
          "2383:             _build_client_packages_with_hatch(",
          "2384:                 source_date_epoch=source_date_epoch, package_format=package_format",
          "2385:             )",
          "2386:         else:",
          "2387:             _build_client_packages_with_docker(",
          "2388:                 source_date_epoch=source_date_epoch, package_format=package_format",
          "2389:             )",
          "2390:         get_console().print(f\"\\n[success]Built packages in {DIST_DIR}[/]\\n\")",
          "2391:     finally:",
          "2392:         if version_suffix_for_pypi:",
          "2393:             VERSION_FILE.write_text(original_version)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "45:     \"name\": \"Other release commands\",",
          "46:     \"commands\": [",
          "47:         \"add-back-references\",",
          "48:         \"publish-docs\",",
          "49:         \"generate-constraints\",",
          "50:         \"update-constraints\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "48:         \"prepare-python-client\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "57:             \"name\": \"Package flags\",",
          "58:             \"options\": [",
          "59:                 \"--package-format\",",
          "61:                 \"--version-suffix-for-pypi\",",
          "62:             ],",
          "63:         }",
          "64:     ],",
          "",
          "[Removed Lines]",
          "60:                 \"--use-local-hatch\",",
          "",
          "[Added Lines]",
          "62:                 \"--use-local-hatch\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "167:             ],",
          "168:         }",
          "169:     ],",
          "170:     \"breeze release-management generate-constraints\": [",
          "171:         {",
          "172:             \"name\": \"Generate constraints flags\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "171:     \"breeze release-management prepare-python-client\": [",
          "172:         {",
          "173:             \"name\": \"Python client preparation flags\",",
          "174:             \"options\": [",
          "175:                 \"--package-format\",",
          "176:                 \"--version-suffix-for-pypi\",",
          "177:                 \"--use-local-hatch\",",
          "178:                 \"--python-client-repo\",",
          "179:                 \"--only-publish-build-scripts\",",
          "180:                 \"--security-schemes\",",
          "181:             ],",
          "182:         }",
          "183:     ],",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "84:     (\"RELEASE_NOTES.rst\", \"/opt/airflow/RELEASE_NOTES.rst\"),",
          "85:     (\"airflow\", \"/opt/airflow/airflow\"),",
          "86:     (\"constraints\", \"/opt/airflow/constraints\"),",
          "87:     (\"dags\", \"/opt/airflow/dags\"),",
          "88:     (\"dev\", \"/opt/airflow/dev\"),",
          "89:     (\"docs\", \"/opt/airflow/docs\"),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "87:     (\"clients\", \"/opt/airflow/clients\"),",
          "",
          "---------------"
        ],
        "scripts/ci/openapi/client_codegen_diff.sh||scripts/ci/openapi/client_codegen_diff.sh": [
          "File: scripts/ci/openapi/client_codegen_diff.sh -> scripts/ci/openapi/client_codegen_diff.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b9ef5a089837e4d911c7a80d4ab5449946fbb6d8",
      "candidate_info": {
        "commit_hash": "b9ef5a089837e4d911c7a80d4ab5449946fbb6d8",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b9ef5a089837e4d911c7a80d4ab5449946fbb6d8",
        "files": [
          "dev/README_RELEASE_PROVIDER_PACKAGES.md",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2",
          "images/breeze/output_release-management_generate-issue-content-providers.svg",
          "images/breeze/output_release-management_generate-issue-content-providers.txt"
        ],
        "message": "Improve generation of \"Status of testing\" issue (#36470)\n\nThis PR improves/simplifies the process of issue generation when\nprovider package rc candidates are prepared for voting.\n\nIt improves the commmand to generate the issue and makes it simpler\n(less copy&paste) to create such issue, the issue also does not use\nthe \"Meta\" template and gets the right labels assigned automatically.\n\nRecent changes that automatically derive the suffix from PyPI packages\nprepared, removed the need of passing `--suffix` as parameter. In all\ncases the right rc* suffix will be automatically added during issue\ngeneration based on the version of package being prepared. The process\nhas been updated and command simplified by removing the `--suffix` flag.\n\nWhen the issue is prepared, we display the issue in terminal and asked\nthe release manager to create the issue by copy&pasting the issue\ncontent and title to a new issue, but that required a few copy&pastes\nand opening new Issue via \"Meta\" task type. This PR simplifies it a\nbit by not only displaying the content but also generating a URL that\ncan be either copy&pasted to browser URL field or just Cmd+clicked\nif your terminal allows that. Issue created this way does not have\nthe \"Body\" field header and has the labels properly assigned including\na dedicated \"testing status\" label that is used to gether stats for\npast \"status\" issues.\n\nThe advice for release manager has been improved (the comment generated\nhad some missing end of sentence and it should be now clearer on how\nto iterate during issue generation if you want to remove some PRs from\nthe generated issue content.\n\n(cherry picked from commit 5d88f6f9a4c7140c1da9db47aab7caf2d4c5f453)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2||dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: from datetime import datetime",
          "30: from pathlib import Path",
          "31: from subprocess import DEVNULL",
          "34: import click",
          "35: from rich.progress import Progress",
          "",
          "[Removed Lines]",
          "32: from typing import IO, TYPE_CHECKING, Any, Generator, NamedTuple",
          "",
          "[Added Lines]",
          "32: from typing import IO, TYPE_CHECKING, Any, Generator, Iterable, NamedTuple",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1571:     return prs",
          "1574: @release_management.command(",
          "1575:     name=\"generate-issue-content-providers\", help=\"Generates content for issue to test the release.\"",
          "1576: )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1574: def create_github_issue_url(title: str, body: str, labels: Iterable[str]) -> str:",
          "1575:     \"\"\"",
          "1576:     Creates URL to create the issue with title, body and labels.",
          "1577:     :param title: issue title",
          "1578:     :param body: issue body",
          "1579:     :param labels: labels for the issue",
          "1580:     :return: URL to use to create the issue",
          "1581:     \"\"\"",
          "1582:     from urllib.parse import quote",
          "1584:     quoted_labels = quote(\",\".join(labels))",
          "1585:     quoted_title = quote(title)",
          "1586:     quoted_body = quote(body)",
          "1587:     return (",
          "1588:         f\"https://github.com/apache/airflow/issues/new?labels={quoted_labels}&\"",
          "1589:         f\"title={quoted_title}&body={quoted_body}\"",
          "1590:     )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1592:     is_flag=True,",
          "1593:     help=\"Only consider package ids with packages prepared in the dist folder\",",
          "1594: )",
          "1596: @argument_provider_packages",
          "1597: def generate_issue_content_providers(",
          "1598:     disable_progress: bool,",
          "1599:     excluded_pr_list: str,",
          "1600:     github_token: str,",
          "1601:     only_available_in_dist: bool,",
          "1603:     provider_packages: list[str],",
          "1604: ):",
          "1605:     import jinja2",
          "",
          "[Removed Lines]",
          "1595: @click.option(\"--suffix\", default=\"rc1\", help=\"Suffix to add to the version prepared\")",
          "1602:     suffix: str,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1674:                     provider_package_id=provider_id,",
          "1675:                     pypi_package_name=provider_yaml_dict[\"package-name\"],",
          "1676:                     pr_list=pull_request_list,",
          "1678:                 )",
          "1679:         template = jinja2.Template(",
          "1680:             (Path(__file__).parents[1] / \"provider_issue_TEMPLATE.md.jinja2\").read_text()",
          "",
          "[Removed Lines]",
          "1677:                     suffix=package_suffix if package_suffix else suffix,",
          "",
          "[Added Lines]",
          "1694:                     suffix=package_suffix if package_suffix else \"\",",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1688:         get_console().print()",
          "1689:         get_console().print()",
          "1690:         get_console().print(",
          "1692:             f\"prepared on {datetime.now():%B %d, %Y}[/]\"",
          "1693:         )",
          "1694:         get_console().print()",
          "1698:         users: set[str] = set()",
          "1699:         for provider_info in providers.values():",
          "1700:             for pr in provider_info.pr_list:",
          "1701:                 users.add(\"@\" + pr.user.login)",
          "1706: def get_all_constraint_files(refresh_constraints: bool, python_version: str) -> None:",
          "",
          "[Removed Lines]",
          "1691:             \"Issue title: [yellow]Status of testing Providers that were \"",
          "1695:         syntax = Syntax(issue_content, \"markdown\", theme=\"ansi_dark\")",
          "1696:         get_console().print(syntax)",
          "1697:         get_console().print()",
          "1702:         get_console().print(\"All users involved in the PRs:\")",
          "1703:         get_console().print(\" \".join(users))",
          "",
          "[Added Lines]",
          "1708:             \"Issue title: [warning]Status of testing Providers that were \"",
          "1712:         issue_content += \"\\n\"",
          "1717:         issue_content += f\"All users involved in the PRs:\\n{' '.join(users)}\"",
          "1718:         syntax = Syntax(issue_content, \"markdown\", theme=\"ansi_dark\")",
          "1719:         get_console().print(syntax)",
          "1720:         url_to_create_the_issue = create_github_issue_url(",
          "1721:             title=f\"Status of testing Providers that were prepared on {datetime.now():%B %d, %Y}\",",
          "1722:             body=issue_content,",
          "1723:             labels=[\"testing status\", \"kind:meta\"],",
          "1724:         )",
          "1725:         get_console().print()",
          "1726:         get_console().print(",
          "1727:             \"[info]You can prefill the issue by copy&pasting this link to browser \"",
          "1728:             \"(or Cmd+Click if your terminal supports it):\\n\"",
          "1729:         )",
          "1730:         print(url_to_create_the_issue)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "234:                 \"--excluded-pr-list\",",
          "235:                 \"--github-token\",",
          "236:                 \"--only-available-in-dist\",",
          "238:             ],",
          "239:         }",
          "240:     ],",
          "",
          "[Removed Lines]",
          "237:                 \"--suffix\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2||dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2": [
          "File: dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2 -> dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: NOTE TO RELEASE MANAGER:",
          "30: -->",
          "",
          "[Removed Lines]",
          "23: Please move here the providers that have doc-only changes or for which changes are trivial, and",
          "24: you could assess that they are OK. In case",
          "26: The providers are automatically installed on Airflow 2.3 and latest `main` during the CI, so we know they",
          "27: are installable. Also, all classes within the providers are imported during the CI run so we know all",
          "28: providers can be imported.",
          "",
          "[Added Lines]",
          "23: You can move here the providers that have doc-only changes or for which changes are trivial, and",
          "24: you could assess that they are OK.",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e3316c36b4159d5bc6ae73e4b77ba62045064184",
      "candidate_info": {
        "commit_hash": "e3316c36b4159d5bc6ae73e4b77ba62045064184",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e3316c36b4159d5bc6ae73e4b77ba62045064184",
        "files": [
          "dev/README_RELEASE_PROVIDER_PACKAGES.md",
          "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py"
        ],
        "message": "Fix preparing rc candidates for providers (#36465)\n\nThe last auto-upgrade RC implementd in #36441 had a bug - it was bumping\nrc even for providers that have been already released. This change fixes\nit - it skips packages that already have \"final\" tag present in the\nrepo. It also explicitely calls \"Apply template update\" as optional\nstep - only needed in case we modify templates and want to update\nautomatically generated documentation with it.\n\n(cherry picked from commit a3e5a971edf9e4e86c86d595daa04dccd277aabe)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py||dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py||dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py": [
          "File: dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py -> dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "172:         return False, version_suffix",
          "173:     # version_suffix starts with \"rc\"",
          "174:     current_version = int(version_suffix[2:])",
          "175:     while True:",
          "176:         current_tag = get_latest_provider_tag(provider_id, f\"rc{current_version}\")",
          "177:         if tag_exists_for_provider(provider_id, current_tag):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "175:     release_tag = get_latest_provider_tag(provider_id, \"\")",
          "176:     if tag_exists_for_provider(provider_id, release_tag):",
          "177:         get_console().print(f\"[warning]The tag {release_tag} exists. Provider is released. Skipping it.[/]\")",
          "178:         return True, \"\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "010bb695b8d3157f937e489ce6a20935b801b9d9",
      "candidate_info": {
        "commit_hash": "010bb695b8d3157f937e489ce6a20935b801b9d9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/010bb695b8d3157f937e489ce6a20935b801b9d9",
        "files": [
          ".pre-commit-config.yaml",
          "STATIC_CODE_CHECKS.rst",
          "airflow/contrib/operators/__init__.py",
          "airflow/contrib/secrets/__init__.py",
          "airflow/contrib/sensors/__init__.py",
          "airflow/contrib/utils/__init__.py",
          "pyproject.toml",
          "scripts/ci/pre_commit/pre_commit_ruff_format.py",
          "setup.py"
        ],
        "message": "Upgrade to latest ruff and remove ISC001 warning from output (#36649)\n\nThis PR upgrades to latest ruff, and removes the ISC001 warning that\nwarns us against potential conflict between ruff and ruff-formatter\nwhen two strings in one line get concatenated.\n\nThis warning makes sense if you run both ruff and formatting at the\nsame time, but in our case we are doing it in two separate\nsteps - one step is to run ruff linting and the second step is to\nrun formatting and running formatting already runs after linting\nis complete.\n\nThis warnign is pretty misleading as it distracts from real formatting\nissues you might have.\n\nThere is - unfortunately - no standard way to remove the warning\nso we have to do it a little \"around\" - rather than running\nthe pre-commit directly from ruff website, we run our local pre-commit\nwith few lines of Python code that runs ruff through shell and\ngreps out the ISC001 warning. We also force color to make\nsure the output is still coloured.\n\n(cherry picked from commit 11c46fd2ec165da32202b464be7c2df5cca4d6c0)",
        "before_after_code_files": [
          "airflow/contrib/operators/__init__.py||airflow/contrioperators/__init__.py",
          "airflow/contrib/secrets/__init__.py||airflow/contrisecrets/__init__.py",
          "airflow/contrib/sensors/__init__.py||airflow/contrisensors/__init__.py",
          "airflow/contrib/utils/__init__.py||airflow/contriutils/__init__.py",
          "scripts/ci/pre_commit/pre_commit_ruff_format.py||scripts/ci/pre_commit/pre_commit_ruff_format.py",
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/contrib/operators/__init__.py||airflow/contrioperators/__init__.py": [
          "File: airflow/contrib/operators/__init__.py -> airflow/contrioperators/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "180:         \"DataprocDeleteClusterOperator\": (",
          "181:             \"airflow.providers.google.cloud.operators.dataproc.DataprocDeleteClusterOperator\"",
          "182:         ),",
          "186:         \"DataprocInstantiateWorkflowTemplateOperator\": (",
          "187:             \"airflow.providers.google.cloud.operators.dataproc.DataprocInstantiateWorkflowTemplateOperator\"",
          "188:         ),",
          "",
          "[Removed Lines]",
          "183:         \"DataprocInstantiateInlineWorkflowTemplateOperator\":",
          "184:             \"airflow.providers.google.cloud.operators.dataproc.\"",
          "185:             \"DataprocInstantiateInlineWorkflowTemplateOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "240:         \"DataProcSparkSqlOperator\": (",
          "241:             \"airflow.providers.google.cloud.operators.dataproc.DataprocSubmitSparkSqlJobOperator\"",
          "242:         ),",
          "246:         \"DataprocWorkflowTemplateInstantiateOperator\": (",
          "247:             \"airflow.providers.google.cloud.operators.dataproc.DataprocInstantiateWorkflowTemplateOperator\"",
          "248:         ),",
          "",
          "[Removed Lines]",
          "243:         \"DataprocWorkflowTemplateInstantiateInlineOperator\":",
          "244:             \"airflow.providers.google.cloud.operators.dataproc.\"",
          "245:             \"DataprocInstantiateInlineWorkflowTemplateOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "351:         \"ComputeEngineCopyInstanceTemplateOperator\": (",
          "352:             \"airflow.providers.google.cloud.operators.compute.ComputeEngineCopyInstanceTemplateOperator\"",
          "353:         ),",
          "357:         \"ComputeEngineSetMachineTypeOperator\": (",
          "358:             \"airflow.providers.google.cloud.operators.compute.ComputeEngineSetMachineTypeOperator\"",
          "359:         ),",
          "",
          "[Removed Lines]",
          "354:         \"ComputeEngineInstanceGroupUpdateManagerTemplateOperator\":",
          "355:             \"airflow.providers.google.cloud.operators.compute.\"",
          "356:             \"ComputeEngineInstanceGroupUpdateManagerTemplateOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "364:             \"airflow.providers.google.cloud.operators.compute.ComputeEngineStopInstanceOperator\"",
          "365:         ),",
          "366:         \"GceBaseOperator\": \"airflow.providers.google.cloud.operators.compute.ComputeEngineBaseOperator\",",
          "370:         \"GceInstanceStartOperator\": (",
          "371:             \"airflow.providers.google.cloud.operators.compute.ComputeEngineStartInstanceOperator\"",
          "372:         ),",
          "",
          "[Removed Lines]",
          "367:         \"GceInstanceGroupManagerUpdateTemplateOperator\":",
          "368:             \"airflow.providers.google.cloud.operators.compute.\"",
          "369:             \"ComputeEngineInstanceGroupUpdateManagerTemplateOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "513:         ),",
          "514:     },",
          "515:     \"gcp_natural_language_operator\": {",
          "540:     },",
          "541:     \"gcp_spanner_operator\": {",
          "542:         \"SpannerDeleteDatabaseInstanceOperator\": (",
          "",
          "[Removed Lines]",
          "516:         \"CloudNaturalLanguageAnalyzeEntitiesOperator\":",
          "517:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "518:             \"CloudNaturalLanguageAnalyzeEntitiesOperator\",",
          "519:         \"CloudNaturalLanguageAnalyzeEntitySentimentOperator\":",
          "520:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "521:             \"CloudNaturalLanguageAnalyzeEntitySentimentOperator\",",
          "522:         \"CloudNaturalLanguageAnalyzeSentimentOperator\":",
          "523:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "524:             \"CloudNaturalLanguageAnalyzeSentimentOperator\",",
          "525:         \"CloudNaturalLanguageClassifyTextOperator\":",
          "526:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "527:             \"CloudNaturalLanguageClassifyTextOperator\",",
          "528:         \"CloudLanguageAnalyzeEntitiesOperator\":",
          "529:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "530:             \"CloudNaturalLanguageAnalyzeEntitiesOperator\",",
          "531:         \"CloudLanguageAnalyzeEntitySentimentOperator\":",
          "532:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "533:             \"CloudNaturalLanguageAnalyzeEntitySentimentOperator\",",
          "534:         \"CloudLanguageAnalyzeSentimentOperator\":",
          "535:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "536:             \"CloudNaturalLanguageAnalyzeSentimentOperator\",",
          "537:         \"CloudLanguageClassifyTextOperator\":",
          "538:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "539:             \"CloudNaturalLanguageClassifyTextOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "692:         ),",
          "693:     },",
          "694:     \"gcp_transfer_operator\": {",
          "755:     },",
          "756:     \"gcp_translate_operator\": {",
          "757:         \"CloudTranslateTextOperator\": (",
          "",
          "[Removed Lines]",
          "695:         \"CloudDataTransferServiceCancelOperationOperator\":",
          "696:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "697:             \"CloudDataTransferServiceCancelOperationOperator\",",
          "698:         \"CloudDataTransferServiceCreateJobOperator\":",
          "699:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "700:             \"CloudDataTransferServiceCreateJobOperator\",",
          "701:         \"CloudDataTransferServiceDeleteJobOperator\":",
          "702:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "703:             \"CloudDataTransferServiceDeleteJobOperator\",",
          "704:         \"CloudDataTransferServiceGCSToGCSOperator\":",
          "705:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "706:             \"CloudDataTransferServiceGCSToGCSOperator\",",
          "707:         \"CloudDataTransferServiceGetOperationOperator\":",
          "708:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "709:             \"CloudDataTransferServiceGetOperationOperator\",",
          "710:         \"CloudDataTransferServiceListOperationsOperator\":",
          "711:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "712:             \"CloudDataTransferServiceListOperationsOperator\",",
          "713:         \"CloudDataTransferServicePauseOperationOperator\":",
          "714:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "715:             \"CloudDataTransferServicePauseOperationOperator\",",
          "716:         \"CloudDataTransferServiceResumeOperationOperator\":",
          "717:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "718:             \"CloudDataTransferServiceResumeOperationOperator\",",
          "719:         \"CloudDataTransferServiceS3ToGCSOperator\":",
          "720:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "721:             \"CloudDataTransferServiceS3ToGCSOperator\",",
          "722:         \"CloudDataTransferServiceUpdateJobOperator\":",
          "723:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "724:             \"CloudDataTransferServiceUpdateJobOperator\",",
          "725:         \"GcpTransferServiceJobCreateOperator\":",
          "726:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "727:             \"CloudDataTransferServiceCreateJobOperator\",",
          "728:         \"GcpTransferServiceJobDeleteOperator\":",
          "729:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "730:             \"CloudDataTransferServiceDeleteJobOperator\",",
          "731:         \"GcpTransferServiceJobUpdateOperator\":",
          "732:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "733:             \"CloudDataTransferServiceUpdateJobOperator\",",
          "734:         \"GcpTransferServiceOperationCancelOperator\":",
          "735:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "736:             \"CloudDataTransferServiceCancelOperationOperator\",",
          "737:         \"GcpTransferServiceOperationGetOperator\":",
          "738:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "739:             \"CloudDataTransferServiceGetOperationOperator\",",
          "740:         \"GcpTransferServiceOperationPauseOperator\":",
          "741:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "742:             \"CloudDataTransferServicePauseOperationOperator\",",
          "743:         \"GcpTransferServiceOperationResumeOperator\":",
          "744:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "745:             \"CloudDataTransferServiceResumeOperationOperator\",",
          "746:         \"GcpTransferServiceOperationsListOperator\":",
          "747:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "748:             \"CloudDataTransferServiceListOperationsOperator\",",
          "749:         \"GoogleCloudStorageToGoogleCloudStorageTransferOperator\":",
          "750:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "751:             \"CloudDataTransferServiceGCSToGCSOperator\",",
          "752:         \"S3ToGoogleCloudStorageTransferOperator\":",
          "753:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "754:             \"CloudDataTransferServiceS3ToGCSOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "767:         ),",
          "768:     },",
          "769:     \"gcp_video_intelligence_operator\": {",
          "779:     },",
          "780:     \"gcp_vision_operator\": {",
          "781:         \"CloudVisionAddProductToProductSetOperator\": (",
          "",
          "[Removed Lines]",
          "770:         \"CloudVideoIntelligenceDetectVideoExplicitContentOperator\":",
          "771:             \"airflow.providers.google.cloud.operators.video_intelligence.\"",
          "772:             \"CloudVideoIntelligenceDetectVideoExplicitContentOperator\",",
          "773:         \"CloudVideoIntelligenceDetectVideoLabelsOperator\":",
          "774:             \"airflow.providers.google.cloud.operators.video_intelligence.\"",
          "775:             \"CloudVideoIntelligenceDetectVideoLabelsOperator\",",
          "776:         \"CloudVideoIntelligenceDetectVideoShotsOperator\":",
          "777:             \"airflow.providers.google.cloud.operators.video_intelligence.\"",
          "778:             \"CloudVideoIntelligenceDetectVideoShotsOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "945:         \"JiraOperator\": \"airflow.providers.atlassian.jira.operators.jira.JiraOperator\",",
          "946:     },",
          "947:     \"kubernetes_pod_operator\": {",
          "951:     },",
          "952:     \"mlengine_operator\": {",
          "953:         \"MLEngineManageModelOperator\": (",
          "",
          "[Removed Lines]",
          "948:         \"KubernetesPodOperator\": (",
          "949:             \"airflow.providers.cncf.kubernetes.operators.pod.KubernetesPodOperator\"",
          "950:         ),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "997:         \"OpsgenieAlertOperator\": \"airflow.providers.opsgenie.operators.opsgenie.OpsgenieCreateAlertOperator\",",
          "998:     },",
          "999:     \"oracle_to_azure_data_lake_transfer\": {",
          "1003:     },",
          "1004:     \"oracle_to_oracle_transfer\": {",
          "1005:         \"OracleToOracleOperator\": (",
          "",
          "[Removed Lines]",
          "1000:         \"OracleToAzureDataLakeOperator\":",
          "1001:             \"airflow.providers.microsoft.azure.transfers.\"",
          "1002:             \"oracle_to_azure_data_lake.OracleToAzureDataLakeOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1056:         \"S3ToGCSOperator\": \"airflow.providers.google.cloud.transfers.s3_to_gcs.S3ToGCSOperator\",",
          "1057:     },",
          "1058:     \"s3_to_gcs_transfer_operator\": {",
          "1062:     },",
          "1063:     \"s3_to_sftp_operator\": {",
          "1064:         \"S3ToSFTPOperator\": \"airflow.providers.amazon.aws.transfers.s3_to_sftp.S3ToSFTPOperator\",",
          "",
          "[Removed Lines]",
          "1059:         \"CloudDataTransferServiceS3ToGCSOperator\":",
          "1060:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "1061:             \"CloudDataTransferServiceS3ToGCSOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/contrib/secrets/__init__.py||airflow/contrisecrets/__init__.py": [
          "File: airflow/contrib/secrets/__init__.py -> airflow/contrisecrets/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: warnings.warn(",
          "27:     \"This module is deprecated. Please use airflow.providers.*.secrets.\",",
          "28:     RemovedInAirflow3Warning,",
          "30: )",
          "32: __deprecated_classes = {",
          "",
          "[Removed Lines]",
          "29:     stacklevel=2",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/contrib/sensors/__init__.py||airflow/contrisensors/__init__.py": [
          "File: airflow/contrib/sensors/__init__.py -> airflow/contrisensors/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "85:         \"FTPSSensor\": \"airflow.providers.ftp.sensors.ftp.FTPSSensor\",",
          "86:     },",
          "87:     \"gcp_transfer_sensor\": {",
          "94:     },",
          "95:     \"gcs_sensor\": {",
          "96:         \"GCSObjectExistenceSensor\": \"airflow.providers.google.cloud.sensors.gcs.GCSObjectExistenceSensor\",",
          "",
          "[Removed Lines]",
          "88:         \"CloudDataTransferServiceJobStatusSensor\":",
          "89:             \"airflow.providers.google.cloud.sensors.cloud_storage_transfer_service.\"",
          "90:             \"CloudDataTransferServiceJobStatusSensor\",",
          "91:         \"GCPTransferServiceWaitForJobStatusSensor\":",
          "92:             \"airflow.providers.google.cloud.sensors.cloud_storage_transfer_service.\"",
          "93:             \"CloudDataTransferServiceJobStatusSensor\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/contrib/utils/__init__.py||airflow/contriutils/__init__.py": [
          "File: airflow/contrib/utils/__init__.py -> airflow/contriutils/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: from airflow.utils.deprecation_tools import add_deprecated_classes",
          "26: warnings.warn(",
          "30: )",
          "32: __deprecated_classes = {",
          "",
          "[Removed Lines]",
          "27:     \"This module is deprecated. Please use `airflow.utils`.\",",
          "28:     RemovedInAirflow3Warning,",
          "29:     stacklevel=2",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_ruff_format.py||scripts/ci/pre_commit/pre_commit_ruff_format.py": [
          "File: scripts/ci/pre_commit/pre_commit_ruff_format.py -> scripts/ci/pre_commit/pre_commit_ruff_format.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: import os",
          "21: import subprocess",
          "23: ruff_format_cmd = \"ruff format --force-exclude 2>&1 | grep -v '`ISC001`. To avoid unexpected behavior'\"",
          "24: envcopy = os.environ.copy()",
          "25: envcopy[\"CLICOLOR_FORCE\"] = \"1\"",
          "26: subprocess.run(ruff_format_cmd, shell=True, check=True, env=envcopy)",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "466: _devel_only_static_checks = [",
          "467:     \"pre-commit\",",
          "468:     \"black\",",
          "470:     \"yamllint\",",
          "471: ]",
          "",
          "[Removed Lines]",
          "469:     \"ruff>=0.0.219\",",
          "",
          "[Added Lines]",
          "469:     \"ruff==0.1.11\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e98fed3e892d16f09fabd42abc251325d98aaf91",
      "candidate_info": {
        "commit_hash": "e98fed3e892d16f09fabd42abc251325d98aaf91",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e98fed3e892d16f09fabd42abc251325d98aaf91",
        "files": [
          "airflow/www/views.py",
          "tests/www/views/test_views_tasks.py"
        ],
        "message": "Remove option ot set a task instance to running state in UI (#36518)\n\n* Remove option ot set a task instance to running state in UI\n* Uups, fix pytests\n\n(cherry picked from commit 60aa611f04559391621e7d0d9612cfffef6368a1)",
        "before_after_code_files": [
          "airflow/www/views.py||airflow/www/views.py",
          "tests/www/views/test_views_tasks.py||tests/www/views/test_views_tasks.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "5450:         \"action_clear\": \"edit\",",
          "5451:         \"action_clear_downstream\": \"edit\",",
          "5452:         \"action_muldelete\": \"delete\",",
          "5454:         \"action_set_failed\": \"edit\",",
          "5455:         \"action_set_success\": \"edit\",",
          "5456:         \"action_set_retry\": \"edit\",",
          "",
          "[Removed Lines]",
          "5453:         \"action_set_running\": \"edit\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "5727:         except Exception:",
          "5728:             flash(\"Failed to set state\", \"error\")",
          "5739:     @action(\"set_failed\", \"Set state to 'failed'\", \"\", single=False)",
          "5740:     @auth.has_access_dag_entities(\"PUT\", DagAccessEntity.TASK_INSTANCE)",
          "5741:     @action_logging",
          "",
          "[Removed Lines]",
          "5730:     @action(\"set_running\", \"Set state to 'running'\", \"\", single=False)",
          "5731:     @auth.has_access_dag_entities(\"PUT\", DagAccessEntity.TASK_INSTANCE)",
          "5732:     @action_logging",
          "5733:     def action_set_running(self, tis):",
          "5734:         \"\"\"Set state to 'running'.\"\"\"",
          "5735:         self.set_task_instance_state(tis, TaskInstanceState.RUNNING)",
          "5736:         self.update_redirect()",
          "5737:         return redirect(self.get_redirect())",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_tasks.py||tests/www/views/test_views_tasks.py": [
          "File: tests/www/views/test_views_tasks.py -> tests/www/views/test_views_tasks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "943: @pytest.mark.parametrize(",
          "944:     \"action, expected_state\",",
          "945:     [",
          "947:         (\"set_failed\", State.FAILED),",
          "948:         (\"set_success\", State.SUCCESS),",
          "949:         (\"set_retry\", State.UP_FOR_RETRY),",
          "950:         (\"set_skipped\", State.SKIPPED),",
          "951:     ],",
          "953: )",
          "954: def test_task_instance_set_state(session, admin_client, action, expected_state):",
          "955:     task_id = \"runme_0\"",
          "",
          "[Removed Lines]",
          "946:         (\"set_running\", State.RUNNING),",
          "952:     ids=[\"running\", \"failed\", \"success\", \"retry\", \"skipped\"],",
          "",
          "[Added Lines]",
          "951:     ids=[\"failed\", \"success\", \"retry\", \"skipped\"],",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "972: @pytest.mark.parametrize(",
          "973:     \"action\",",
          "974:     [",
          "976:         \"set_failed\",",
          "977:         \"set_success\",",
          "978:         \"set_retry\",",
          "",
          "[Removed Lines]",
          "975:         \"set_running\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    }
  ]
}