{
  "cve_id": "CVE-2022-40604",
  "cve_desc": "In Apache Airflow 2.3.0 through 2.3.4, part of a url was unnecessarily formatted, allowing for possible information extraction.",
  "repo": "apache/airflow",
  "patch_hash": "18386026c28939fa6d91d198c5489c295a05dcd2",
  "patch_info": {
    "commit_hash": "18386026c28939fa6d91d198c5489c295a05dcd2",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/18386026c28939fa6d91d198c5489c295a05dcd2",
    "files": [
      "airflow/utils/log/file_task_handler.py"
    ],
    "message": "Properly build URL to retrieve logs independently from system (#26337)\n\nThe previous way of building the path depended on the OS path\nbut it was really used to build the URL so we should use\nurllib instead of os.path.join.",
    "before_after_code_files": [
      "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
    ]
  },
  "patch_diff": {
    "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
      "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "21: import warnings",
      "22: from pathlib import Path",
      "23: from typing import TYPE_CHECKING, Optional",
      "25: from airflow.configuration import AirflowConfigException, conf",
      "26: from airflow.exceptions import RemovedInAirflow3Warning",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "24: from urllib.parse import urljoin",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "194:         else:",
      "195:             import httpx",
      "199:             )",
      "200:             log += f\"*** Log file does not exist: {location}\\n\"",
      "201:             log += f\"*** Fetching from: {url}\\n\"",
      "",
      "[Removed Lines]",
      "197:             url = os.path.join(\"http://{ti.hostname}:{worker_log_server_port}/log\", log_relative_path).format(",
      "198:                 ti=ti, worker_log_server_port=conf.get('logging', 'WORKER_LOG_SERVER_PORT')",
      "",
      "[Added Lines]",
      "198:             url = urljoin(",
      "199:                 f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log\", log_relative_path",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
      "candidate_info": {
        "commit_hash": "6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
        "files": [
          "airflow/utils/log/file_task_handler.py"
        ],
        "message": "Properly build URL to retrieve logs independently from system (#26337)\n\nThe previous way of building the path depended on the OS path\nbut it was really used to build the URL so we should use\nurllib instead of os.path.join.\n\n(cherry picked from commit 18386026c28939fa6d91d198c5489c295a05dcd2)",
        "before_after_code_files": [
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "diff_branch_cherry_pick": 1,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ],
          "candidate": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import warnings",
          "22: from pathlib import Path",
          "23: from typing import TYPE_CHECKING, Optional",
          "25: from airflow.configuration import AirflowConfigException, conf",
          "26: from airflow.exceptions import RemovedInAirflow3Warning",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24: from urllib.parse import urljoin",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "194:         else:",
          "195:             import httpx",
          "199:             )",
          "200:             log += f\"*** Log file does not exist: {location}\\n\"",
          "201:             log += f\"*** Fetching from: {url}\\n\"",
          "",
          "[Removed Lines]",
          "197:             url = os.path.join(\"http://{ti.hostname}:{worker_log_server_port}/log\", log_relative_path).format(",
          "198:                 ti=ti, worker_log_server_port=conf.get('logging', 'WORKER_LOG_SERVER_PORT')",
          "",
          "[Added Lines]",
          "198:             url = urljoin(",
          "199:                 f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log\", log_relative_path",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "3b25168c413a8434f8f65efb09aaf949cf7adc3b",
      "candidate_info": {
        "commit_hash": "3b25168c413a8434f8f65efb09aaf949cf7adc3b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/3b25168c413a8434f8f65efb09aaf949cf7adc3b",
        "files": [
          "airflow/executors/base_executor.py",
          "airflow/executors/celery_kubernetes_executor.py",
          "airflow/executors/kubernetes_executor.py",
          "airflow/executors/local_kubernetes_executor.py",
          "airflow/utils/log/file_task_handler.py",
          "tests/executors/test_base_executor.py",
          "tests/executors/test_celery_kubernetes_executor.py",
          "tests/executors/test_kubernetes_executor.py",
          "tests/executors/test_local_kubernetes_executor.py",
          "tests/providers/amazon/aws/log/test_s3_task_handler.py",
          "tests/utils/test_log_handlers.py"
        ],
        "message": "AIP-51 - Executor Coupling in Logging (#28161)\n\nExecutors may now implement a method to vend task logs",
        "before_after_code_files": [
          "airflow/executors/base_executor.py||airflow/executors/base_executor.py",
          "airflow/executors/celery_kubernetes_executor.py||airflow/executors/celery_kubernetes_executor.py",
          "airflow/executors/kubernetes_executor.py||airflow/executors/kubernetes_executor.py",
          "airflow/executors/local_kubernetes_executor.py||airflow/executors/local_kubernetes_executor.py",
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py",
          "tests/executors/test_base_executor.py||tests/executors/test_base_executor.py",
          "tests/executors/test_celery_kubernetes_executor.py||tests/executors/test_celery_kubernetes_executor.py",
          "tests/executors/test_kubernetes_executor.py||tests/executors/test_kubernetes_executor.py",
          "tests/executors/test_local_kubernetes_executor.py||tests/executors/test_local_kubernetes_executor.py",
          "tests/providers/amazon/aws/log/test_s3_task_handler.py||tests/providers/amazon/aws/log/test_s3_task_handler.py",
          "tests/utils/test_log_handlers.py||tests/utils/test_log_handlers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ],
          "candidate": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/executors/base_executor.py||airflow/executors/base_executor.py": [
          "File: airflow/executors/base_executor.py -> airflow/executors/base_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "355:         \"\"\"",
          "356:         raise NotImplementedError()",
          "358:     def end(self) -> None:  # pragma: no cover",
          "359:         \"\"\"Wait synchronously for the previously submitted job to complete.\"\"\"",
          "360:         raise NotImplementedError()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "358:     def get_task_log(self, ti: TaskInstance, log: str = \"\") -> None | str | tuple[str, dict[str, bool]]:",
          "359:         \"\"\"",
          "360:         This method can be implemented by any child class to return the task logs.",
          "362:         :param ti: A TaskInstance object",
          "363:         :param log: log str",
          "364:         :return: logs or tuple of logs and meta dict",
          "365:         \"\"\"",
          "",
          "---------------"
        ],
        "airflow/executors/celery_kubernetes_executor.py||airflow/executors/celery_kubernetes_executor.py": [
          "File: airflow/executors/celery_kubernetes_executor.py -> airflow/executors/celery_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "141:             cfg_path=cfg_path,",
          "142:         )",
          "144:     def has_task(self, task_instance: TaskInstance) -> bool:",
          "145:         \"\"\"",
          "146:         Checks if a task is either queued or running in either celery or kubernetes executor.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "144:     def get_task_log(self, ti: TaskInstance, log: str = \"\") -> None | str | tuple[str, dict[str, bool]]:",
          "145:         \"\"\"Fetch task log from Kubernetes executor\"\"\"",
          "146:         if ti.queue == self.kubernetes_executor.kubernetes_queue:",
          "147:             return self.kubernetes_executor.get_task_log(ti=ti, log=log)",
          "148:         return None",
          "",
          "---------------"
        ],
        "airflow/executors/kubernetes_executor.py||airflow/executors/kubernetes_executor.py": [
          "File: airflow/executors/kubernetes_executor.py -> airflow/executors/kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "28: import multiprocessing",
          "29: import time",
          "30: from collections import defaultdict",
          "31: from datetime import timedelta",
          "32: from queue import Empty, Queue",
          "33: from typing import TYPE_CHECKING, Any, Dict, Optional, Sequence, Tuple",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "31: from contextlib import suppress",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "37: from kubernetes.client.rest import ApiException",
          "38: from urllib3.exceptions import ReadTimeoutError",
          "40: from airflow.exceptions import AirflowException, PodMutationHookException, PodReconciliationError",
          "41: from airflow.executors.base_executor import BaseExecutor, CommandType",
          "42: from airflow.kubernetes import pod_generator",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "41: from airflow.configuration import conf",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "771:             # do this once, so only do it when we remove the task from running",
          "772:             self.event_buffer[key] = state, None",
          "774:     def try_adopt_task_instances(self, tis: Sequence[TaskInstance]) -> Sequence[TaskInstance]:",
          "775:         tis_to_flush = [ti for ti in tis if not ti.queued_by_job_id]",
          "776:         scheduler_job_ids = {ti.queued_by_job_id for ti in tis}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "776:     @staticmethod",
          "777:     def _get_pod_namespace(ti: TaskInstance):",
          "778:         pod_override = ti.executor_config.get(\"pod_override\")",
          "779:         namespace = None",
          "780:         with suppress(Exception):",
          "781:             namespace = pod_override.metadata.namespace",
          "782:         return namespace or conf.get(\"kubernetes_executor\", \"namespace\", fallback=\"default\")",
          "784:     def get_task_log(self, ti: TaskInstance, log: str = \"\") -> str | tuple[str, dict[str, bool]]:",
          "786:         try:",
          "787:             from airflow.kubernetes.pod_generator import PodGenerator",
          "789:             client = get_kube_client()",
          "791:             log += f\"*** Trying to get logs (last 100 lines) from worker pod {ti.hostname} ***\\n\\n\"",
          "792:             selector = PodGenerator.build_selector_for_k8s_executor_pod(",
          "793:                 dag_id=ti.dag_id,",
          "794:                 task_id=ti.task_id,",
          "795:                 try_number=ti.try_number,",
          "796:                 map_index=ti.map_index,",
          "797:                 run_id=ti.run_id,",
          "798:                 airflow_worker=ti.queued_by_job_id,",
          "799:             )",
          "800:             namespace = self._get_pod_namespace(ti)",
          "801:             pod_list = client.list_namespaced_pod(",
          "802:                 namespace=namespace,",
          "803:                 label_selector=selector,",
          "804:             ).items",
          "805:             if not pod_list:",
          "806:                 raise RuntimeError(\"Cannot find pod for ti %s\", ti)",
          "807:             elif len(pod_list) > 1:",
          "808:                 raise RuntimeError(\"Found multiple pods for ti %s: %s\", ti, pod_list)",
          "809:             res = client.read_namespaced_pod_log(",
          "810:                 name=pod_list[0].metadata.name,",
          "811:                 namespace=namespace,",
          "812:                 container=\"base\",",
          "813:                 follow=False,",
          "814:                 tail_lines=100,",
          "815:                 _preload_content=False,",
          "816:             )",
          "818:             for line in res:",
          "819:                 log += line.decode()",
          "821:             return log",
          "823:         except Exception as f:",
          "824:             log += f\"*** Unable to fetch logs from worker pod {ti.hostname} ***\\n{str(f)}\\n\\n\"",
          "825:             return log, {\"end_of_log\": True}",
          "",
          "---------------"
        ],
        "airflow/executors/local_kubernetes_executor.py||airflow/executors/local_kubernetes_executor.py": [
          "File: airflow/executors/local_kubernetes_executor.py -> airflow/executors/local_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "142:             cfg_path=cfg_path,",
          "143:         )",
          "145:     def has_task(self, task_instance: TaskInstance) -> bool:",
          "146:         \"\"\"",
          "147:         Checks if a task is either queued or running in either local or kubernetes executor.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "145:     def get_task_log(self, ti: TaskInstance, log: str = \"\") -> None | str | tuple[str, dict[str, bool]]:",
          "146:         \"\"\"Fetch task log from kubernetes executor\"\"\"",
          "147:         if ti.queue == self.kubernetes_executor.kubernetes_queue:",
          "148:             return self.kubernetes_executor.get_task_log(ti=ti, log=log)",
          "150:         return None",
          "",
          "---------------"
        ],
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: from typing import TYPE_CHECKING, Any",
          "27: from urllib.parse import urljoin",
          "31: from airflow.utils.context import Context",
          "32: from airflow.utils.helpers import parse_template_string, render_template_to_string",
          "33: from airflow.utils.log.logging_mixin import SetContextPropagate",
          "",
          "[Removed Lines]",
          "29: from airflow.configuration import AirflowConfigException, conf",
          "30: from airflow.exceptions import RemovedInAirflow3Warning",
          "",
          "[Added Lines]",
          "29: from airflow.configuration import conf",
          "30: from airflow.exceptions import AirflowConfigException, RemovedInAirflow3Warning",
          "31: from airflow.executors.executor_loader import ExecutorLoader",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "146:     def _read_grouped_logs(self):",
          "147:         return False",
          "167:     def _read(self, ti: TaskInstance, try_number: int, metadata: dict[str, Any] | None = None):",
          "168:         \"\"\"",
          "",
          "[Removed Lines]",
          "149:     @staticmethod",
          "150:     def _should_check_k8s(queue):",
          "151:         \"\"\"",
          "152:         If the task is running through kubernetes executor, return True.",
          "154:         When logs aren't available locally, in this case we read from k8s pod logs.",
          "155:         \"\"\"",
          "156:         executor = conf.get(\"core\", \"executor\")",
          "157:         if executor == \"KubernetesExecutor\":",
          "158:             return True",
          "159:         elif executor == \"LocalKubernetesExecutor\":",
          "160:             if queue == conf.get(\"local_kubernetes_executor\", \"kubernetes_queue\"):",
          "161:                 return True",
          "162:         elif executor == \"CeleryKubernetesExecutor\":",
          "163:             if queue == conf.get(\"celery_kubernetes_executor\", \"kubernetes_queue\"):",
          "164:                 return True",
          "165:         return False",
          "",
          "[Added Lines]",
          "150:     def _get_task_log_from_worker(",
          "151:         self, ti: TaskInstance, log: str, log_relative_path: str",
          "152:     ) -> str | tuple[str, dict[str, bool]]:",
          "153:         import httpx",
          "155:         from airflow.utils.jwt_signer import JWTSigner",
          "157:         url = self._get_log_retrieval_url(ti, log_relative_path)",
          "158:         log += f\"*** Fetching from: {url}\\n\"",
          "160:         try:",
          "161:             timeout = None  # No timeout",
          "162:             try:",
          "163:                 timeout = conf.getint(\"webserver\", \"log_fetch_timeout_sec\")",
          "164:             except (AirflowConfigException, ValueError):",
          "165:                 pass",
          "167:             signer = JWTSigner(",
          "168:                 secret_key=conf.get(\"webserver\", \"secret_key\"),",
          "169:                 expiration_time_in_seconds=conf.getint(\"webserver\", \"log_request_clock_grace\", fallback=30),",
          "170:                 audience=\"task-instance-logs\",",
          "171:             )",
          "172:             response = httpx.get(",
          "173:                 url,",
          "174:                 timeout=timeout,",
          "175:                 headers={\"Authorization\": signer.generate_signed_token({\"filename\": log_relative_path})},",
          "176:             )",
          "177:             response.encoding = \"utf-8\"",
          "179:             if response.status_code == 403:",
          "180:                 log += (",
          "181:                     \"*** !!!! Please make sure that all your Airflow components (e.g. \"",
          "182:                     \"schedulers, webservers and workers) have \"",
          "183:                     \"the same 'secret_key' configured in 'webserver' section and \"",
          "184:                     \"time is synchronized on all your machines (for example with ntpd) !!!!!\\n***\"",
          "185:                 )",
          "186:                 log += (",
          "187:                     \"*** See more at https://airflow.apache.org/docs/apache-airflow/\"",
          "188:                     \"stable/configurations-ref.html#secret-key\\n***\"",
          "189:                 )",
          "190:             # Check if the resource was properly fetched",
          "191:             response.raise_for_status()",
          "193:             log += \"\\n\" + response.text",
          "194:             return log",
          "195:         except Exception as e:",
          "196:             log += f\"*** Failed to fetch log file from worker. {str(e)}\\n\"",
          "197:             return log, {\"end_of_log\": True}",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "186:                              This is determined by the status of the TaskInstance",
          "187:                  log_pos: (absolute) Char position to which the log is retrieved",
          "188:         \"\"\"",
          "191:         # Task instance here might be different from task instance when",
          "192:         # initializing the handler. Thus explicitly getting log location",
          "193:         # is needed to get correct log path.",
          "",
          "[Removed Lines]",
          "189:         from airflow.utils.jwt_signer import JWTSigner",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "204:                 log = f\"*** Failed to load local log file: {location}\\n\"",
          "205:                 log += f\"*** {str(e)}\\n\"",
          "206:                 return log, {\"end_of_log\": True}",
          "293:         # Process tailing if log is not at it's end",
          "294:         end_of_log = ti.try_number != try_number or ti.state not in [State.RUNNING, State.DEFERRED]",
          "",
          "[Removed Lines]",
          "207:         elif self._should_check_k8s(ti.queue):",
          "208:             try:",
          "209:                 from airflow.kubernetes.kube_client import get_kube_client",
          "210:                 from airflow.kubernetes.pod_generator import PodGenerator",
          "212:                 client = get_kube_client()",
          "214:                 log += f\"*** Trying to get logs (last 100 lines) from worker pod {ti.hostname} ***\\n\\n\"",
          "215:                 selector = PodGenerator.build_selector_for_k8s_executor_pod(",
          "216:                     dag_id=ti.dag_id,",
          "217:                     task_id=ti.task_id,",
          "218:                     try_number=ti.try_number,",
          "219:                     map_index=ti.map_index,",
          "220:                     run_id=ti.run_id,",
          "221:                     airflow_worker=ti.queued_by_job_id,",
          "222:                 )",
          "223:                 namespace = self._get_pod_namespace(ti)",
          "224:                 pod_list = client.list_namespaced_pod(",
          "225:                     namespace=namespace,",
          "226:                     label_selector=selector,",
          "227:                 ).items",
          "228:                 if not pod_list:",
          "229:                     raise RuntimeError(\"Cannot find pod for ti %s\", ti)",
          "230:                 elif len(pod_list) > 1:",
          "231:                     raise RuntimeError(\"Found multiple pods for ti %s: %s\", ti, pod_list)",
          "232:                 res = client.read_namespaced_pod_log(",
          "233:                     name=pod_list[0].metadata.name,",
          "234:                     namespace=namespace,",
          "235:                     container=\"base\",",
          "236:                     follow=False,",
          "237:                     tail_lines=100,",
          "238:                     _preload_content=False,",
          "239:                 )",
          "241:                 for line in res:",
          "242:                     log += line.decode()",
          "244:             except Exception as f:",
          "245:                 log += f\"*** Unable to fetch logs from worker pod {ti.hostname} ***\\n{str(f)}\\n\\n\"",
          "246:                 return log, {\"end_of_log\": True}",
          "247:         else:",
          "248:             import httpx",
          "250:             url = self._get_log_retrieval_url(ti, log_relative_path)",
          "251:             log += f\"*** Log file does not exist: {location}\\n\"",
          "252:             log += f\"*** Fetching from: {url}\\n\"",
          "253:             try:",
          "254:                 timeout = None  # No timeout",
          "255:                 try:",
          "256:                     timeout = conf.getint(\"webserver\", \"log_fetch_timeout_sec\")",
          "257:                 except (AirflowConfigException, ValueError):",
          "258:                     pass",
          "260:                 signer = JWTSigner(",
          "261:                     secret_key=conf.get(\"webserver\", \"secret_key\"),",
          "262:                     expiration_time_in_seconds=conf.getint(",
          "263:                         \"webserver\", \"log_request_clock_grace\", fallback=30",
          "264:                     ),",
          "265:                     audience=\"task-instance-logs\",",
          "266:                 )",
          "267:                 response = httpx.get(",
          "268:                     url,",
          "269:                     timeout=timeout,",
          "270:                     headers={\"Authorization\": signer.generate_signed_token({\"filename\": log_relative_path})},",
          "271:                 )",
          "272:                 response.encoding = \"utf-8\"",
          "274:                 if response.status_code == 403:",
          "275:                     log += (",
          "276:                         \"*** !!!! Please make sure that all your Airflow components (e.g. \"",
          "277:                         \"schedulers, webservers and workers) have \"",
          "278:                         \"the same 'secret_key' configured in 'webserver' section and \"",
          "279:                         \"time is synchronized on all your machines (for example with ntpd) !!!!!\\n***\"",
          "280:                     )",
          "281:                     log += (",
          "282:                         \"*** See more at https://airflow.apache.org/docs/apache-airflow/\"",
          "283:                         \"stable/configurations-ref.html#secret-key\\n***\"",
          "284:                     )",
          "285:                 # Check if the resource was properly fetched",
          "286:                 response.raise_for_status()",
          "288:                 log += \"\\n\" + response.text",
          "289:             except Exception as e:",
          "290:                 log += f\"*** Failed to fetch log file from worker. {str(e)}\\n\"",
          "291:                 return log, {\"end_of_log\": True}",
          "",
          "[Added Lines]",
          "237:         else:",
          "238:             log += f\"*** Local log file does not exist: {location}\\n\"",
          "239:             executor = ExecutorLoader.get_default_executor()",
          "240:             task_log = None",
          "242:             task_log = executor.get_task_log(ti=ti, log=log)",
          "243:             if isinstance(task_log, tuple):",
          "244:                 return task_log",
          "246:             if task_log is None:",
          "247:                 log += \"*** Failed to fetch log from executor. Falling back to fetching log from worker.\\n\"",
          "248:                 task_log = self._get_task_log_from_worker(ti, log, log_relative_path=log_relative_path)",
          "250:             if isinstance(task_log, tuple):",
          "251:                 return task_log",
          "253:             log = str(task_log)",
          "",
          "---------------"
        ],
        "tests/executors/test_base_executor.py||tests/executors/test_base_executor.py": [
          "File: tests/executors/test_base_executor.py -> tests/executors/test_base_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "28: from airflow.executors.base_executor import BaseExecutor, RunningRetryAttemptType",
          "29: from airflow.models.baseoperator import BaseOperator",
          "31: from airflow.utils import timezone",
          "32: from airflow.utils.state import State",
          "",
          "[Removed Lines]",
          "30: from airflow.models.taskinstance import TaskInstanceKey",
          "",
          "[Added Lines]",
          "30: from airflow.models.taskinstance import TaskInstance, TaskInstanceKey",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "44:     assert not BaseExecutor.is_local",
          "47: def test_serve_logs_default_value():",
          "48:     assert not BaseExecutor.serve_logs",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "47: def test_get_task_log():",
          "48:     executor = BaseExecutor()",
          "49:     ti = TaskInstance(task=BaseOperator(task_id=\"dummy\"))",
          "50:     assert executor.get_task_log(ti=ti) is None",
          "",
          "---------------"
        ],
        "tests/executors/test_celery_kubernetes_executor.py||tests/executors/test_celery_kubernetes_executor.py": [
          "File: tests/executors/test_celery_kubernetes_executor.py -> tests/executors/test_celery_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "173:         celery_executor_mock.try_adopt_task_instances.assert_called_once_with(celery_tis)",
          "174:         k8s_executor_mock.try_adopt_task_instances.assert_called_once_with(k8s_tis)",
          "176:     def test_get_event_buffer(self):",
          "177:         celery_executor_mock = mock.MagicMock()",
          "178:         k8s_executor_mock = mock.MagicMock()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "176:     def test_log_is_fetched_from_k8s_executor_only_for_k8s_queue(self):",
          "177:         celery_executor_mock = mock.MagicMock()",
          "178:         k8s_executor_mock = mock.MagicMock()",
          "179:         cke = CeleryKubernetesExecutor(celery_executor_mock, k8s_executor_mock)",
          "180:         simple_task_instance = mock.MagicMock()",
          "181:         simple_task_instance.queue = KUBERNETES_QUEUE",
          "182:         cke.get_task_log(ti=simple_task_instance, log=\"\")",
          "183:         k8s_executor_mock.get_task_log.assert_called_once_with(ti=simple_task_instance, log=mock.ANY)",
          "185:         k8s_executor_mock.reset_mock()",
          "187:         simple_task_instance.queue = \"test-queue\"",
          "188:         log = cke.get_task_log(ti=simple_task_instance, log=\"\")",
          "189:         k8s_executor_mock.get_task_log.assert_not_called()",
          "190:         assert log is None",
          "",
          "---------------"
        ],
        "tests/executors/test_kubernetes_executor.py||tests/executors/test_kubernetes_executor.py": [
          "File: tests/executors/test_kubernetes_executor.py -> tests/executors/test_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: from airflow.exceptions import PodReconciliationError",
          "35: from airflow.models.taskinstance import TaskInstanceKey",
          "36: from airflow.operators.bash import BashOperator",
          "37: from airflow.utils import timezone",
          "38: from tests.test_utils.config import conf_vars",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "37: from airflow.operators.empty import EmptyOperator",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1215:         assert ti0.state == State.SCHEDULED",
          "1216:         assert ti1.state == State.QUEUED",
          "1218:     def test_supports_pickling(self):",
          "1219:         assert KubernetesExecutor.supports_pickling",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1219:     @mock.patch(\"airflow.executors.kubernetes_executor.get_kube_client\")",
          "1220:     def test_get_task_log(self, mock_get_kube_client, create_task_instance_of_operator):",
          "1221:         \"\"\"fetch task log from pod\"\"\"",
          "1222:         mock_kube_client = mock_get_kube_client.return_value",
          "1224:         mock_kube_client.read_namespaced_pod_log.return_value = [b\"a_\", b\"b_\", b\"c_\"]",
          "1225:         mock_pod = mock.Mock()",
          "1226:         mock_pod.metadata.name = \"x\"",
          "1227:         mock_kube_client.list_namespaced_pod.return_value.items = [mock_pod]",
          "1228:         ti = create_task_instance_of_operator(EmptyOperator, dag_id=\"test_k8s_log_dag\", task_id=\"test_task\")",
          "1230:         executor = KubernetesExecutor()",
          "1231:         log = executor.get_task_log(ti=ti, log=\"test_init_log\")",
          "1233:         mock_kube_client.read_namespaced_pod_log.assert_called_once()",
          "1234:         assert \"test_init_log\" in log",
          "1235:         assert \"Trying to get logs (last 100 lines) from worker pod\" in log",
          "1236:         assert \"a_b_c\" in log",
          "1238:         mock_kube_client.reset_mock()",
          "1239:         mock_kube_client.read_namespaced_pod_log.side_effect = Exception(\"error_fetching_pod_log\")",
          "1241:         log = executor.get_task_log(ti=ti, log=\"test_init_log\")",
          "1242:         assert len(log) == 2",
          "1243:         assert \"error_fetching_pod_log\" in log[0]",
          "1244:         assert log[1][\"end_of_log\"]",
          "",
          "---------------"
        ],
        "tests/executors/test_local_kubernetes_executor.py||tests/executors/test_local_kubernetes_executor.py": [
          "File: tests/executors/test_local_kubernetes_executor.py -> tests/executors/test_local_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "84:         assert k8s_executor_mock.kubernetes_queue == conf.get(\"local_kubernetes_executor\", \"kubernetes_queue\")",
          "86:     def test_send_callback(self):",
          "87:         local_executor_mock = mock.MagicMock()",
          "88:         k8s_executor_mock = mock.MagicMock()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "86:     def test_log_is_fetched_from_k8s_executor_only_for_k8s_queue(self):",
          "87:         local_executor_mock = mock.MagicMock()",
          "88:         k8s_executor_mock = mock.MagicMock()",
          "90:         KUBERNETES_QUEUE = conf.get(\"local_kubernetes_executor\", \"kubernetes_queue\")",
          "91:         LocalKubernetesExecutor(local_executor_mock, k8s_executor_mock)",
          "92:         local_k8s_exec = LocalKubernetesExecutor(local_executor_mock, k8s_executor_mock)",
          "93:         simple_task_instance = mock.MagicMock()",
          "94:         simple_task_instance.queue = KUBERNETES_QUEUE",
          "95:         local_k8s_exec.get_task_log(ti=simple_task_instance, log=\"\")",
          "96:         k8s_executor_mock.get_task_log.assert_called_once_with(ti=simple_task_instance, log=mock.ANY)",
          "98:         k8s_executor_mock.reset_mock()",
          "100:         simple_task_instance.queue = \"test-queue\"",
          "101:         log = local_k8s_exec.get_task_log(ti=simple_task_instance, log=\"\")",
          "102:         k8s_executor_mock.get_task_log.assert_not_called()",
          "103:         assert log is None",
          "",
          "---------------"
        ],
        "tests/providers/amazon/aws/log/test_s3_task_handler.py||tests/providers/amazon/aws/log/test_s3_task_handler.py": [
          "File: tests/providers/amazon/aws/log/test_s3_task_handler.py -> tests/providers/amazon/aws/log/test_s3_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "139:         assert 1 == len(log)",
          "140:         assert len(log) == len(metadata)",
          "142:         assert {\"end_of_log\": True} == metadata[0]",
          "144:     def test_s3_read_when_log_missing(self):",
          "",
          "[Removed Lines]",
          "141:         assert \"*** Log file does not exist:\" in log[0][0][-1]",
          "",
          "[Added Lines]",
          "141:         assert \"*** Local log file does not exist:\" in log[0][0][-1]",
          "",
          "---------------"
        ],
        "tests/utils/test_log_handlers.py||tests/utils/test_log_handlers.py": [
          "File: tests/utils/test_log_handlers.py -> tests/utils/test_log_handlers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import logging.config",
          "22: import os",
          "23: import re",
          "26: import pytest",
          "27: from kubernetes.client import models as k8s",
          "",
          "[Removed Lines]",
          "24: from unittest.mock import patch",
          "",
          "[Added Lines]",
          "24: from unittest import mock",
          "25: from unittest.mock import mock_open, patch",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "35: from airflow.utils.state import State",
          "36: from airflow.utils.timezone import datetime",
          "37: from airflow.utils.types import DagRunType",
          "39: DEFAULT_DATE = datetime(2016, 1, 1)",
          "40: TASK_LOGGER = \"airflow.task\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "39: from tests.test_utils.config import conf_vars",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "220:         # Remove the generated tmp log file.",
          "221:         os.remove(log_filename)",
          "223:     @pytest.mark.parametrize(",
          "224:         \"pod_override, namespace_to_call\",",
          "225:         [",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "225:     def test__read_from_location(self, create_task_instance):",
          "226:         \"\"\"Test if local log file exists, then log is read from it\"\"\"",
          "227:         local_log_file_read = create_task_instance(",
          "228:             dag_id=\"dag_for_testing_local_log_read\",",
          "229:             task_id=\"task_for_testing_local_log_read\",",
          "230:             run_type=DagRunType.SCHEDULED,",
          "231:             execution_date=DEFAULT_DATE,",
          "232:         )",
          "233:         with patch(\"os.path.exists\", return_value=True):",
          "234:             opener = mock_open(read_data=\"dummy test log data\")",
          "235:             with patch(\"airflow.utils.log.file_task_handler.open\", opener):",
          "236:                 fth = FileTaskHandler(\"\")",
          "237:                 log = fth._read(ti=local_log_file_read, try_number=1)",
          "238:                 assert len(log) == 2",
          "239:                 assert \"dummy test log data\" in log[0]",
          "241:     @mock.patch(\"airflow.executors.kubernetes_executor.KubernetesExecutor.get_task_log\")",
          "242:     def test__read_for_k8s_executor(self, mock_k8s_get_task_log, create_task_instance):",
          "243:         \"\"\"Test for k8s executor, the log is read from get_task_log method\"\"\"",
          "244:         executor_name = \"KubernetesExecutor\"",
          "245:         ti = create_task_instance(",
          "246:             dag_id=\"dag_for_testing_k8s_executor_log_read\",",
          "247:             task_id=\"task_for_testing_k8s_executor_log_read\",",
          "248:             run_type=DagRunType.SCHEDULED,",
          "249:             execution_date=DEFAULT_DATE,",
          "250:         )",
          "252:         with conf_vars({(\"core\", \"executor\"): executor_name}):",
          "253:             with patch(\"os.path.exists\", return_value=False):",
          "254:                 fth = FileTaskHandler(\"\")",
          "255:                 fth._read(ti=ti, try_number=1)",
          "256:                 mock_k8s_get_task_log.assert_called_once_with(ti=ti, log=mock.ANY)",
          "258:     def test__read_for_celery_executor_fallbacks_to_worker(self, create_task_instance):",
          "259:         \"\"\"Test for executors which do not have `get_task_log` method, it fallbacks to reading",
          "260:         log from worker\"\"\"",
          "261:         executor_name = \"CeleryExecutor\"",
          "263:         ti = create_task_instance(",
          "264:             dag_id=\"dag_for_testing_celery_executor_log_read\",",
          "265:             task_id=\"task_for_testing_celery_executor_log_read\",",
          "266:             run_type=DagRunType.SCHEDULED,",
          "267:             execution_date=DEFAULT_DATE,",
          "268:         )",
          "270:         with conf_vars({(\"core\", \"executor\"): executor_name}):",
          "271:             with patch(\"os.path.exists\", return_value=False):",
          "272:                 fth = FileTaskHandler(\"\")",
          "274:                 def mock_log_from_worker(ti, log, log_relative_path):",
          "275:                     return (log, {\"end_of_log\": True})",
          "277:                 fth._get_task_log_from_worker = mock.Mock(side_effect=mock_log_from_worker)",
          "278:                 log = fth._read(ti=ti, try_number=1)",
          "279:                 fth._get_task_log_from_worker.assert_called_once()",
          "280:                 assert \"Local log file does not exist\" in log[0]",
          "281:                 assert \"Failed to fetch log from executor. Falling back to fetching log from worker\" in log[0]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "231:         ],",
          "232:     )",
          "233:     @patch.dict(\"os.environ\", AIRFLOW__CORE__EXECUTOR=\"KubernetesExecutor\")",
          "235:     def test_read_from_k8s_under_multi_namespace_mode(",
          "236:         self, mock_kube_client, pod_override, namespace_to_call",
          "237:     ):",
          "",
          "[Removed Lines]",
          "234:     @patch(\"airflow.kubernetes.kube_client.get_kube_client\")",
          "",
          "[Added Lines]",
          "294:     @patch(\"airflow.executors.kubernetes_executor.get_kube_client\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "342:         log_url_ti.hostname = \"hostname\"",
          "343:         url = FileTaskHandler._get_log_retrieval_url(log_url_ti, \"DYNAMIC_PATH\")",
          "344:         assert url == \"http://hostname:8793/log/DYNAMIC_PATH\"",
          "",
          "[Removed Lines]",
          "347: @pytest.mark.parametrize(",
          "348:     \"config, queue, expected\",",
          "349:     [",
          "350:         (dict(AIRFLOW__CORE__EXECUTOR=\"LocalExecutor\"), None, False),",
          "351:         (dict(AIRFLOW__CORE__EXECUTOR=\"LocalExecutor\"), \"kubernetes\", False),",
          "352:         (dict(AIRFLOW__CORE__EXECUTOR=\"KubernetesExecutor\"), None, True),",
          "353:         (dict(AIRFLOW__CORE__EXECUTOR=\"CeleryKubernetesExecutor\"), \"any\", False),",
          "354:         (dict(AIRFLOW__CORE__EXECUTOR=\"CeleryKubernetesExecutor\"), \"kubernetes\", True),",
          "355:         (",
          "356:             dict(",
          "357:                 AIRFLOW__CORE__EXECUTOR=\"CeleryKubernetesExecutor\",",
          "358:                 AIRFLOW__CELERY_KUBERNETES_EXECUTOR__KUBERNETES_QUEUE=\"hithere\",",
          "359:             ),",
          "360:             \"hithere\",",
          "361:             True,",
          "362:         ),",
          "363:         (dict(AIRFLOW__CORE__EXECUTOR=\"LocalKubernetesExecutor\"), \"any\", False),",
          "364:         (dict(AIRFLOW__CORE__EXECUTOR=\"LocalKubernetesExecutor\"), \"kubernetes\", True),",
          "365:         (",
          "366:             dict(",
          "367:                 AIRFLOW__CORE__EXECUTOR=\"LocalKubernetesExecutor\",",
          "368:                 AIRFLOW__LOCAL_KUBERNETES_EXECUTOR__KUBERNETES_QUEUE=\"hithere\",",
          "369:             ),",
          "370:             \"hithere\",",
          "371:             True,",
          "372:         ),",
          "373:     ],",
          "374: )",
          "375: def test__should_check_k8s(config, queue, expected):",
          "376:     with patch.dict(\"os.environ\", **config):",
          "377:         assert FileTaskHandler._should_check_k8s(queue) == expected",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "497c2c243dd168639d34ff35e02e62d5177de338",
      "candidate_info": {
        "commit_hash": "497c2c243dd168639d34ff35e02e62d5177de338",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/497c2c243dd168639d34ff35e02e62d5177de338",
        "files": [
          "airflow/utils/log/file_task_handler.py",
          "tests/utils/test_log_handlers.py"
        ],
        "message": "Log FileTaskHandler to work with KubernetesExecutor's multi_namespace_mode (#28436)",
        "before_after_code_files": [
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py",
          "tests/utils/test_log_handlers.py||tests/utils/test_log_handlers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ],
          "candidate": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "191:                 log += f\"*** {str(e)}\\n\"",
          "192:                 return log, {\"end_of_log\": True}",
          "193:         elif self._should_check_k8s(ti.queue):",
          "194:             try:",
          "195:                 from airflow.kubernetes.kube_client import get_kube_client",
          "197:                 kube_client = get_kube_client()",
          "199:                 log += f\"*** Trying to get logs (last 100 lines) from worker pod {ti.hostname} ***\\n\\n\"",
          "201:                 res = kube_client.read_namespaced_pod_log(",
          "202:                     name=ti.hostname,",
          "204:                     container=\"base\",",
          "205:                     follow=False,",
          "206:                     tail_lines=100,",
          "",
          "[Removed Lines]",
          "203:                     namespace=conf.get(\"kubernetes_executor\", \"namespace\"),",
          "",
          "[Added Lines]",
          "194:             pod_override = ti.executor_config.get(\"pod_override\")",
          "195:             if pod_override and pod_override.metadata and pod_override.metadata.namespace:",
          "196:                 namespace = pod_override.metadata.namespace",
          "197:             else:",
          "198:                 namespace = conf.get(\"kubernetes_executor\", \"namespace\")",
          "207:                     namespace=namespace,",
          "",
          "---------------"
        ],
        "tests/utils/test_log_handlers.py||tests/utils/test_log_handlers.py": [
          "File: tests/utils/test_log_handlers.py -> tests/utils/test_log_handlers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import logging.config",
          "22: import os",
          "23: import re",
          "26: import pytest",
          "28: from airflow.config_templates.airflow_local_settings import DEFAULT_LOGGING_CONFIG",
          "29: from airflow.models import DAG, DagRun, TaskInstance",
          "",
          "[Removed Lines]",
          "24: from unittest.mock import patch",
          "",
          "[Added Lines]",
          "24: from unittest.mock import MagicMock, patch",
          "27: from kubernetes.client import models as k8s",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "219:         # Remove the generated tmp log file.",
          "220:         os.remove(log_filename)",
          "223: class TestFilenameRendering:",
          "224:     def test_python_formatting(self, create_log_template, create_task_instance):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "223:     @pytest.mark.parametrize(",
          "224:         \"pod_override, namespace_to_call\",",
          "225:         [",
          "226:             pytest.param(k8s.V1Pod(metadata=k8s.V1ObjectMeta(namespace=\"namespace-A\")), \"namespace-A\"),",
          "227:             pytest.param(k8s.V1Pod(metadata=k8s.V1ObjectMeta(namespace=\"namespace-B\")), \"namespace-B\"),",
          "228:             pytest.param(k8s.V1Pod(), \"default\"),",
          "229:             pytest.param(None, \"default\"),",
          "230:             pytest.param(k8s.V1Pod(metadata=k8s.V1ObjectMeta(name=\"pod-name-xxx\")), \"default\"),",
          "231:         ],",
          "232:     )",
          "233:     @patch.dict(\"os.environ\", AIRFLOW__CORE__EXECUTOR=\"KubernetesExecutor\")",
          "234:     @patch(\"airflow.kubernetes.kube_client.get_kube_client\")",
          "235:     def test_read_from_k8s_under_multi_namespace_mode(",
          "236:         self, mock_kube_client, pod_override, namespace_to_call",
          "237:     ):",
          "238:         mock_read_namespaced_pod_log = MagicMock()",
          "239:         mock_kube_client.return_value.read_namespaced_pod_log = mock_read_namespaced_pod_log",
          "241:         def task_callable(ti):",
          "242:             ti.log.info(\"test\")",
          "244:         dag = DAG(\"dag_for_testing_file_task_handler\", start_date=DEFAULT_DATE)",
          "245:         dagrun = dag.create_dagrun(",
          "246:             run_type=DagRunType.MANUAL,",
          "247:             state=State.RUNNING,",
          "248:             execution_date=DEFAULT_DATE,",
          "249:         )",
          "250:         executor_config_pod = pod_override",
          "251:         task = PythonOperator(",
          "252:             task_id=\"task_for_testing_file_log_handler\",",
          "253:             dag=dag,",
          "254:             python_callable=task_callable,",
          "255:             executor_config={\"pod_override\": executor_config_pod},",
          "256:         )",
          "257:         ti = TaskInstance(task=task, run_id=dagrun.run_id)",
          "258:         ti.try_number = 3",
          "260:         logger = ti.log",
          "261:         ti.log.disabled = False",
          "263:         file_handler = next(",
          "264:             (handler for handler in logger.handlers if handler.name == FILE_TASK_HANDLER), None",
          "265:         )",
          "266:         set_context(logger, ti)",
          "267:         ti.run(ignore_ti_state=True)",
          "269:         file_handler.read(ti, 3)",
          "271:         # Check if kube_client.read_namespaced_pod_log() is called with the namespace we expect",
          "272:         mock_read_namespaced_pod_log.assert_called_once_with(",
          "273:             name=ti.hostname,",
          "274:             namespace=namespace_to_call,",
          "275:             container=\"base\",",
          "276:             follow=False,",
          "277:             tail_lines=100,",
          "278:             _preload_content=False,",
          "279:         )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1a8a897120762692ca98ac5ce4da881678c073aa",
      "candidate_info": {
        "commit_hash": "1a8a897120762692ca98ac5ce4da881678c073aa",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/1a8a897120762692ca98ac5ce4da881678c073aa",
        "files": [
          "airflow/exceptions.py",
          "airflow/executors/executor_loader.py",
          "airflow/models/param.py",
          "airflow/models/taskinstance.py",
          "airflow/serialization/serialized_objects.py",
          "airflow/utils/cli.py",
          "airflow/utils/helpers.py",
          "airflow/utils/log/file_task_handler.py"
        ],
        "message": "Improve speed to run `airflow` by 6x (#21438)\n\nBy delaying expensive/slow imports to where they are needed, this gets\n`airflow` printing it's usage information in under 0.8s, down from almost\n3s which makes it feel much much snappier.\n\nBy not loading BaseExecutor we can get down to <0.5s",
        "before_after_code_files": [
          "airflow/exceptions.py||airflow/exceptions.py",
          "airflow/executors/executor_loader.py||airflow/executors/executor_loader.py",
          "airflow/models/param.py||airflow/models/param.py",
          "airflow/models/taskinstance.py||airflow/models/taskinstance.py",
          "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py",
          "airflow/utils/cli.py||airflow/utils/cli.py",
          "airflow/utils/helpers.py||airflow/utils/helpers.py",
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ],
          "candidate": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/exceptions.py||airflow/exceptions.py": [
          "File: airflow/exceptions.py -> airflow/exceptions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import warnings",
          "24: from typing import Any, Dict, List, NamedTuple, Optional, Sized",
          "31: class AirflowException(Exception):",
          "32:     \"\"\"",
          "",
          "[Removed Lines]",
          "26: from airflow.api_connexion.exceptions import NotFound as ApiConnexionNotFound",
          "27: from airflow.utils.code_utils import prepare_code_snippet",
          "28: from airflow.utils.platform import is_tty",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "44:     status_code = 400",
          "48:     \"\"\"Raise when the requested object/resource is not available in the system.\"\"\"",
          "50:     status_code = 404",
          "",
          "[Removed Lines]",
          "47: class AirflowNotFoundException(AirflowException, ApiConnexionNotFound):",
          "",
          "[Added Lines]",
          "43: class AirflowNotFoundException(AirflowException):",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "249:         self.parse_errors = parse_errors",
          "251:     def __str__(self):",
          "252:         result = f\"{self.msg}\\nFilename: {self.file_path}\\n\\n\"",
          "254:         for error_no, parse_error in enumerate(self.parse_errors, 1):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "248:         from airflow.utils.code_utils import prepare_code_snippet",
          "249:         from airflow.utils.platform import is_tty",
          "",
          "---------------"
        ],
        "airflow/executors/executor_loader.py||airflow/executors/executor_loader.py": [
          "File: airflow/executors/executor_loader.py -> airflow/executors/executor_loader.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: import logging",
          "19: from contextlib import suppress",
          "20: from enum import Enum, unique",
          "23: from airflow.exceptions import AirflowConfigException",
          "25: from airflow.executors.executor_constants import (",
          "26:     CELERY_EXECUTOR,",
          "27:     CELERY_KUBERNETES_EXECUTOR,",
          "",
          "[Removed Lines]",
          "21: from typing import Optional, Tuple, Type",
          "24: from airflow.executors.base_executor import BaseExecutor",
          "",
          "[Added Lines]",
          "21: from typing import TYPE_CHECKING, Optional, Tuple, Type",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "36: log = logging.getLogger(__name__)",
          "39: @unique",
          "40: class ConnectorSource(Enum):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "37: if TYPE_CHECKING:",
          "38:     from airflow.executors.base_executor import BaseExecutor",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "48: class ExecutorLoader:",
          "49:     \"\"\"Keeps constants for all the currently available executors.\"\"\"",
          "52:     executors = {",
          "53:         LOCAL_EXECUTOR: 'airflow.executors.local_executor.LocalExecutor',",
          "54:         SEQUENTIAL_EXECUTOR: 'airflow.executors.sequential_executor.SequentialExecutor',",
          "",
          "[Removed Lines]",
          "51:     _default_executor: Optional[BaseExecutor] = None",
          "",
          "[Added Lines]",
          "53:     _default_executor: Optional[\"BaseExecutor\"] = None",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "60:     }",
          "62:     @classmethod",
          "64:         \"\"\"Creates a new instance of the configured executor if none exists and returns it\"\"\"",
          "65:         if cls._default_executor is not None:",
          "66:             return cls._default_executor",
          "",
          "[Removed Lines]",
          "63:     def get_default_executor(cls) -> BaseExecutor:",
          "",
          "[Added Lines]",
          "65:     def get_default_executor(cls) -> \"BaseExecutor\":",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "74:         return cls._default_executor",
          "76:     @classmethod",
          "78:         \"\"\"",
          "79:         Loads the executor.",
          "",
          "[Removed Lines]",
          "77:     def load_executor(cls, executor_name: str) -> BaseExecutor:",
          "",
          "[Added Lines]",
          "79:     def load_executor(cls, executor_name: str) -> \"BaseExecutor\":",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "101:         return executor_cls()",
          "103:     @classmethod",
          "105:         \"\"\"",
          "106:         Imports the executor class.",
          "",
          "[Removed Lines]",
          "104:     def import_executor_cls(cls, executor_name: str) -> Tuple[Type[BaseExecutor], ConnectorSource]:",
          "",
          "[Added Lines]",
          "106:     def import_executor_cls(cls, executor_name: str) -> Tuple[Type[\"BaseExecutor\"], ConnectorSource]:",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "127:         return import_string(executor_name), ConnectorSource.CUSTOM_PATH",
          "129:     @classmethod",
          "131:         \"\"\":return: an instance of CeleryKubernetesExecutor\"\"\"",
          "132:         celery_executor = import_string(cls.executors[CELERY_EXECUTOR])()",
          "133:         kubernetes_executor = import_string(cls.executors[KUBERNETES_EXECUTOR])()",
          "",
          "[Removed Lines]",
          "130:     def __load_celery_kubernetes_executor(cls) -> BaseExecutor:",
          "",
          "[Added Lines]",
          "132:     def __load_celery_kubernetes_executor(cls) -> \"BaseExecutor\":",
          "",
          "---------------"
        ],
        "airflow/models/param.py||airflow/models/param.py": [
          "File: airflow/models/param.py -> airflow/models/param.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: import warnings",
          "20: from typing import Any, Dict, ItemsView, MutableMapping, Optional, ValuesView",
          "26: from airflow.exceptions import AirflowException, ParamValidationError",
          "27: from airflow.utils.context import Context",
          "28: from airflow.utils.types import NOTSET, ArgNotSet",
          "",
          "[Removed Lines]",
          "22: import jsonschema",
          "23: from jsonschema import FormatChecker",
          "24: from jsonschema.exceptions import ValidationError",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "61:         :param suppress_exception: To raise an exception or not when the validations fails.",
          "62:             If true and validations fails, the return value would be None.",
          "63:         \"\"\"",
          "64:         try:",
          "65:             json.dumps(value)",
          "66:         except Exception:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "60:         import jsonschema",
          "61:         from jsonschema import FormatChecker",
          "62:         from jsonschema.exceptions import ValidationError",
          "",
          "---------------"
        ],
        "airflow/models/taskinstance.py||airflow/models/taskinstance.py": [
          "File: airflow/models/taskinstance.py -> airflow/models/taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "115: from airflow.utils.state import DagRunState, State, TaskInstanceState",
          "116: from airflow.utils.timeout import timeout",
          "126: TR = TaskReschedule",
          "128: _CURRENT_CONTEXT: List[Context] = []",
          "",
          "[Removed Lines]",
          "118: try:",
          "119:     from kubernetes.client.api_client import ApiClient",
          "121:     from airflow.kubernetes.kube_config import KubeConfig",
          "122:     from airflow.kubernetes.pod_generator import PodGenerator",
          "123: except ImportError:",
          "124:     ApiClient = None",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2033:     def render_k8s_pod_yaml(self) -> Optional[dict]:",
          "2034:         \"\"\"Render k8s pod yaml\"\"\"",
          "2035:         from airflow.kubernetes.kubernetes_helper_functions import create_pod_id  # Circular import",
          "2037:         kube_config = KubeConfig()",
          "2038:         pod = PodGenerator.construct_pod(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2027:         from kubernetes.client.api_client import ApiClient",
          "2029:         from airflow.kubernetes.kube_config import KubeConfig",
          "2031:         from airflow.kubernetes.pod_generator import PodGenerator",
          "",
          "---------------"
        ],
        "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py": [
          "File: airflow/serialization/serialized_objects.py -> airflow/serialization/serialized_objects.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "48: from airflow.utils.module_loading import as_importable_string, import_string",
          "49: from airflow.utils.task_group import MappedTaskGroup, TaskGroup",
          "61: if TYPE_CHECKING:",
          "62:     from airflow.ti_deps.deps.base_ti_dep import BaseTIDep",
          "64: log = logging.getLogger(__name__)",
          "66: _OPERATOR_EXTRA_LINKS: Set[str] = {",
          "",
          "[Removed Lines]",
          "51: try:",
          "52:     # isort: off",
          "53:     from kubernetes.client import models as k8s",
          "54:     from airflow.kubernetes.pod_generator import PodGenerator",
          "56:     # isort: on",
          "57:     HAS_KUBERNETES = True",
          "58: except ImportError:",
          "59:     HAS_KUBERNETES = False",
          "",
          "[Added Lines]",
          "54:     HAS_KUBERNETES: bool",
          "55:     try:",
          "56:         from kubernetes.client import models as k8s",
          "58:         from airflow.kubernetes.pod_generator import PodGenerator",
          "59:     except ImportError:",
          "60:         pass",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "313:             return cls._encode({str(k): cls._serialize(v) for k, v in var.items()}, type_=DAT.DICT)",
          "314:         elif isinstance(var, list):",
          "315:             return [cls._serialize(v) for v in var]",
          "317:             json_pod = PodGenerator.serialize_pod(var)",
          "318:             return cls._encode(json_pod, type_=DAT.POD)",
          "319:         elif isinstance(var, DAG):",
          "",
          "[Removed Lines]",
          "316:         elif HAS_KUBERNETES and isinstance(var, k8s.V1Pod):",
          "",
          "[Added Lines]",
          "314:         elif _has_kubernetes() and isinstance(var, k8s.V1Pod):",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "374:         elif type_ == DAT.DATETIME:",
          "375:             return pendulum.from_timestamp(var)",
          "376:         elif type_ == DAT.POD:",
          "378:                 raise RuntimeError(\"Cannot deserialize POD objects without kubernetes libraries installed!\")",
          "379:             pod = PodGenerator.deserialize_model_dict(var)",
          "380:             return pod",
          "",
          "[Removed Lines]",
          "377:             if not HAS_KUBERNETES:",
          "",
          "[Added Lines]",
          "375:             if not _has_kubernetes():",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1120:     def node_id(self):",
          "1121:         \"\"\"Node ID for graph rendering\"\"\"",
          "1122:         return f\"{self.dependency_type}:{self.source}:{self.target}:{self.dependency_id}\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1123: def _has_kubernetes() -> bool:",
          "1124:     global HAS_KUBERNETES",
          "1125:     if \"HAS_KUBERNETES\" in globals():",
          "1126:         return HAS_KUBERNETES",
          "1128:     # Loading kube modules is expensive, so delay it until the last moment",
          "1130:     try:",
          "1131:         from kubernetes.client import models as k8s",
          "1133:         from airflow.kubernetes.pod_generator import PodGenerator",
          "1135:         globals()['k8s'] = k8s",
          "1136:         globals()['PodGenerator'] = PodGenerator",
          "1138:         # isort: on",
          "1139:         HAS_KUBERNETES = True",
          "1140:     except ImportError:",
          "1141:         HAS_KUBERNETES = False",
          "1142:     return HAS_KUBERNETES",
          "",
          "---------------"
        ],
        "airflow/utils/cli.py||airflow/utils/cli.py": [
          "File: airflow/utils/cli.py -> airflow/utils/cli.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: from airflow import settings",
          "35: from airflow.exceptions import AirflowException",
          "36: from airflow.utils import cli_action_loggers",
          "38: from airflow.utils.log.non_caching_file_handler import NonCachingFileHandler",
          "39: from airflow.utils.platform import getuser, is_terminal_support_colors",
          "40: from airflow.utils.session import provide_session",
          "",
          "[Removed Lines]",
          "37: from airflow.utils.db import check_and_run_migrations, synchronize_log_template",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "93:             try:",
          "94:                 # Check and run migrations if necessary",
          "95:                 if check_db:",
          "96:                     check_and_run_migrations()",
          "97:                     synchronize_log_template()",
          "98:                 return f(*args, **kwargs)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "95:                     from airflow.utils.db import check_and_run_migrations, synchronize_log_template",
          "",
          "---------------"
        ],
        "airflow/utils/helpers.py||airflow/utils/helpers.py": [
          "File: airflow/utils/helpers.py -> airflow/utils/helpers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "37: )",
          "38: from urllib import parse",
          "44: from airflow.configuration import conf",
          "45: from airflow.exceptions import AirflowException",
          "46: from airflow.utils.module_loading import import_string",
          "48: if TYPE_CHECKING:",
          "49:     from airflow.models import TaskInstance",
          "51: KEY_REGEX = re.compile(r'^[\\w.-]+$')",
          "",
          "[Removed Lines]",
          "40: import flask",
          "41: import jinja2",
          "42: import jinja2.nativetypes",
          "",
          "[Added Lines]",
          "45:     import jinja2",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "171:     return [e for i in iterable for e in i]",
          "175:     \"\"\"Parses Jinja template string.\"\"\"",
          "176:     if \"{{\" in template_string:  # jinja mode",
          "177:         return None, jinja2.Template(template_string)",
          "178:     else:",
          "",
          "[Removed Lines]",
          "174: def parse_template_string(template_string: str) -> Tuple[Optional[str], Optional[jinja2.Template]]:",
          "",
          "[Added Lines]",
          "172: def parse_template_string(template_string: str) -> Tuple[Optional[str], Optional[\"jinja2.Template\"]]:",
          "174:     import jinja2",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "255:     For example:",
          "256:     'http://0.0.0.0:8000/base/graph?dag_id=my-task&root=&execution_date=2020-10-27T10%3A59%3A25.615587",
          "257:     \"\"\"",
          "258:     view = conf.get('webserver', 'dag_default_view').lower()",
          "259:     url = flask.url_for(f\"Airflow.{view}\")",
          "260:     return f\"{url}?{parse.urlencode(query)}\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "258:     import flask",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "285:     except Exception:",
          "286:         env.handle_exception()  # Rewrite traceback to point to the template.",
          "287:     if native:",
          "288:         return jinja2.nativetypes.native_concat(nodes)",
          "289:     return \"\".join(nodes)",
          "293:     \"\"\"Shorthand to ``render_template(native=False)`` with better typing support.\"\"\"",
          "294:     return render_template(template, context, native=False)",
          "298:     \"\"\"Shorthand to ``render_template(native=True)`` with better typing support.\"\"\"",
          "299:     return render_template(template, context, native=True)",
          "",
          "[Removed Lines]",
          "292: def render_template_to_string(template: jinja2.Template, context: MutableMapping[str, Any]) -> str:",
          "297: def render_template_as_native(template: jinja2.Template, context: MutableMapping[str, Any]) -> Any:",
          "",
          "[Added Lines]",
          "290:         import jinja2.nativetypes",
          "296: def render_template_to_string(template: \"jinja2.Template\", context: MutableMapping[str, Any]) -> str:",
          "301: def render_template_as_native(template: \"jinja2.Template\", context: MutableMapping[str, Any]) -> Any:",
          "",
          "---------------"
        ],
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: from pathlib import Path",
          "22: from typing import TYPE_CHECKING, Optional",
          "25: from itsdangerous import TimedJSONWebSignatureSerializer",
          "27: from airflow.configuration import AirflowConfigException, conf",
          "",
          "[Removed Lines]",
          "24: import httpx",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "159:             except Exception as f:",
          "160:                 log += f'*** Unable to fetch logs from worker pod {ti.hostname} ***\\n{str(f)}\\n\\n'",
          "161:         else:",
          "162:             url = os.path.join(\"http://{ti.hostname}:{worker_log_server_port}/log\", log_relative_path).format(",
          "163:                 ti=ti, worker_log_server_port=conf.get('logging', 'WORKER_LOG_SERVER_PORT')",
          "164:             )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "161:             import httpx",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f5ed4d599cadd90ca097e304234253e8aa2adad9",
      "candidate_info": {
        "commit_hash": "f5ed4d599cadd90ca097e304234253e8aa2adad9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f5ed4d599cadd90ca097e304234253e8aa2adad9",
        "files": [
          "airflow/executors/base_executor.py",
          "airflow/executors/celery_kubernetes_executor.py",
          "airflow/executors/kubernetes_executor.py",
          "airflow/executors/local_kubernetes_executor.py",
          "airflow/utils/log/file_task_handler.py",
          "tests/executors/test_base_executor.py",
          "tests/executors/test_celery_kubernetes_executor.py",
          "tests/executors/test_kubernetes_executor.py",
          "tests/executors/test_local_kubernetes_executor.py",
          "tests/utils/test_log_handlers.py"
        ],
        "message": "try_number was not being passed to the get_task_log method, instead (#28817)\n\nti.try_number was used for fetching log from k8s pod.\nit was causing incorrect log being returned for k8s pod.\nfixed by passing try_number from _read to get_task_log method\nuse try_number argument instead of ti.try_number for selecting pod in\nk8s executor\n\nCo-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>",
        "before_after_code_files": [
          "airflow/executors/base_executor.py||airflow/executors/base_executor.py",
          "airflow/executors/celery_kubernetes_executor.py||airflow/executors/celery_kubernetes_executor.py",
          "airflow/executors/kubernetes_executor.py||airflow/executors/kubernetes_executor.py",
          "airflow/executors/local_kubernetes_executor.py||airflow/executors/local_kubernetes_executor.py",
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py",
          "tests/executors/test_base_executor.py||tests/executors/test_base_executor.py",
          "tests/executors/test_celery_kubernetes_executor.py||tests/executors/test_celery_kubernetes_executor.py",
          "tests/executors/test_kubernetes_executor.py||tests/executors/test_kubernetes_executor.py",
          "tests/executors/test_local_kubernetes_executor.py||tests/executors/test_local_kubernetes_executor.py",
          "tests/utils/test_log_handlers.py||tests/utils/test_log_handlers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ],
          "candidate": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/executors/base_executor.py||airflow/executors/base_executor.py": [
          "File: airflow/executors/base_executor.py -> airflow/executors/base_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "357:         \"\"\"",
          "358:         raise NotImplementedError()",
          "361:         \"\"\"",
          "362:         This method can be implemented by any child class to return the task logs.",
          "364:         :param ti: A TaskInstance object",
          "367:         \"\"\"",
          "368:         return [], []",
          "",
          "[Removed Lines]",
          "360:     def get_task_log(self, ti: TaskInstance) -> tuple[list[str], list[str]]:",
          "365:         :param log: log str",
          "366:         :return: logs or tuple of logs and meta dict",
          "",
          "[Added Lines]",
          "360:     def get_task_log(self, ti: TaskInstance, try_number: int) -> tuple[list[str], list[str]]:",
          "365:         :param try_number: current try_number to read log from",
          "366:         :return: tuple of logs and messages",
          "",
          "---------------"
        ],
        "airflow/executors/celery_kubernetes_executor.py||airflow/executors/celery_kubernetes_executor.py": [
          "File: airflow/executors/celery_kubernetes_executor.py -> airflow/executors/celery_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "143:             cfg_path=cfg_path,",
          "144:         )",
          "147:         \"\"\"Fetch task log from Kubernetes executor\"\"\"",
          "148:         if ti.queue == self.kubernetes_executor.kubernetes_queue:",
          "150:         return [], []",
          "152:     def has_task(self, task_instance: TaskInstance) -> bool:",
          "",
          "[Removed Lines]",
          "146:     def get_task_log(self, ti: TaskInstance) -> tuple[list[str], list[str]]:",
          "149:             return self.kubernetes_executor.get_task_log(ti=ti)",
          "",
          "[Added Lines]",
          "146:     def get_task_log(self, ti: TaskInstance, try_number: int) -> tuple[list[str], list[str]]:",
          "149:             return self.kubernetes_executor.get_task_log(ti=ti, try_number=try_number)",
          "",
          "---------------"
        ],
        "airflow/executors/kubernetes_executor.py||airflow/executors/kubernetes_executor.py": [
          "File: airflow/executors/kubernetes_executor.py -> airflow/executors/kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "781:             namespace = pod_override.metadata.namespace",
          "782:         return namespace or conf.get(\"kubernetes_executor\", \"namespace\", fallback=\"default\")",
          "785:         messages = []",
          "786:         log = []",
          "787:         try:",
          "",
          "[Removed Lines]",
          "784:     def get_task_log(self, ti: TaskInstance) -> tuple[list[str], list[str]]:",
          "",
          "[Added Lines]",
          "784:     def get_task_log(self, ti: TaskInstance, try_number: int) -> tuple[list[str], list[str]]:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "794:             selector = PodGenerator.build_selector_for_k8s_executor_pod(",
          "795:                 dag_id=ti.dag_id,",
          "796:                 task_id=ti.task_id,",
          "798:                 map_index=ti.map_index,",
          "799:                 run_id=ti.run_id,",
          "800:                 airflow_worker=ti.queued_by_job_id,",
          "",
          "[Removed Lines]",
          "797:                 try_number=ti.try_number,",
          "",
          "[Added Lines]",
          "797:                 try_number=try_number,",
          "",
          "---------------"
        ],
        "airflow/executors/local_kubernetes_executor.py||airflow/executors/local_kubernetes_executor.py": [
          "File: airflow/executors/local_kubernetes_executor.py -> airflow/executors/local_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "144:             cfg_path=cfg_path,",
          "145:         )",
          "148:         \"\"\"Fetch task log from kubernetes executor\"\"\"",
          "149:         if ti.queue == self.kubernetes_executor.kubernetes_queue:",
          "151:         return [], []",
          "153:     def has_task(self, task_instance: TaskInstance) -> bool:",
          "",
          "[Removed Lines]",
          "147:     def get_task_log(self, ti: TaskInstance) -> tuple[list[str], list[str]]:",
          "150:             return self.kubernetes_executor.get_task_log(ti=ti)",
          "",
          "[Added Lines]",
          "147:     def get_task_log(self, ti: TaskInstance, try_number: int) -> tuple[list[str], list[str]]:",
          "150:             return self.kubernetes_executor.get_task_log(ti=ti, try_number=try_number)",
          "",
          "---------------"
        ],
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "266:         return False",
          "268:     @cached_property",
          "270:         \"\"\"This cached property avoids loading executor repeatedly.\"\"\"",
          "271:         executor = ExecutorLoader.get_default_executor()",
          "272:         return executor.get_task_log",
          "",
          "[Removed Lines]",
          "269:     def _executor_get_task_log(self) -> Callable[[TaskInstance], tuple[list[str], list[str]]]:",
          "",
          "[Added Lines]",
          "269:     def _executor_get_task_log(self) -> Callable[[TaskInstance, int], tuple[list[str], list[str]]]:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "312:             remote_messages, remote_logs = self._read_remote_logs(ti, try_number, metadata)",
          "313:             messages_list.extend(remote_messages)",
          "314:         if ti.state == TaskInstanceState.RUNNING:",
          "316:             if response:",
          "317:                 executor_messages, executor_logs = response",
          "318:             if executor_messages:",
          "",
          "[Removed Lines]",
          "315:             response = self._executor_get_task_log(ti)",
          "",
          "[Added Lines]",
          "315:             response = self._executor_get_task_log(ti, try_number)",
          "",
          "---------------"
        ],
        "tests/executors/test_base_executor.py||tests/executors/test_base_executor.py": [
          "File: tests/executors/test_base_executor.py -> tests/executors/test_base_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "55: def test_get_task_log():",
          "56:     executor = BaseExecutor()",
          "57:     ti = TaskInstance(task=BaseOperator(task_id=\"dummy\"))",
          "61: def test_serve_logs_default_value():",
          "",
          "[Removed Lines]",
          "58:     assert executor.get_task_log(ti=ti) == ([], [])",
          "",
          "[Added Lines]",
          "58:     assert executor.get_task_log(ti=ti, try_number=1) == ([], [])",
          "",
          "---------------"
        ],
        "tests/executors/test_celery_kubernetes_executor.py||tests/executors/test_celery_kubernetes_executor.py": [
          "File: tests/executors/test_celery_kubernetes_executor.py -> tests/executors/test_celery_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "185:         cke = CeleryKubernetesExecutor(celery_executor_mock, k8s_executor_mock)",
          "186:         simple_task_instance = mock.MagicMock()",
          "187:         simple_task_instance.queue = KUBERNETES_QUEUE",
          "191:         k8s_executor_mock.reset_mock()",
          "193:         simple_task_instance.queue = \"test-queue\"",
          "195:         k8s_executor_mock.get_task_log.assert_not_called()",
          "196:         assert log == ([], [])",
          "",
          "[Removed Lines]",
          "188:         cke.get_task_log(ti=simple_task_instance)",
          "189:         k8s_executor_mock.get_task_log.assert_called_once_with(ti=simple_task_instance)",
          "194:         log = cke.get_task_log(ti=simple_task_instance)",
          "",
          "[Added Lines]",
          "188:         cke.get_task_log(ti=simple_task_instance, try_number=1)",
          "189:         k8s_executor_mock.get_task_log.assert_called_once_with(ti=simple_task_instance, try_number=1)",
          "194:         log = cke.get_task_log(ti=simple_task_instance, try_number=1)",
          "",
          "---------------"
        ],
        "tests/executors/test_kubernetes_executor.py||tests/executors/test_kubernetes_executor.py": [
          "File: tests/executors/test_kubernetes_executor.py -> tests/executors/test_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1231:         ti = create_task_instance_of_operator(EmptyOperator, dag_id=\"test_k8s_log_dag\", task_id=\"test_task\")",
          "1233:         executor = KubernetesExecutor()",
          "1236:         mock_kube_client.read_namespaced_pod_log.assert_called_once()",
          "1237:         assert \"Trying to get logs (last 100 lines) from worker pod \" in messages",
          "",
          "[Removed Lines]",
          "1234:         messages, logs = executor.get_task_log(ti=ti)",
          "",
          "[Added Lines]",
          "1234:         messages, logs = executor.get_task_log(ti=ti, try_number=1)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1240:         mock_kube_client.reset_mock()",
          "1241:         mock_kube_client.read_namespaced_pod_log.side_effect = Exception(\"error_fetching_pod_log\")",
          "1244:         assert logs == [\"\"]",
          "1245:         assert messages == [",
          "1246:             \"Trying to get logs (last 100 lines) from worker pod \",",
          "",
          "[Removed Lines]",
          "1243:         messages, logs = executor.get_task_log(ti=ti)",
          "",
          "[Added Lines]",
          "1243:         messages, logs = executor.get_task_log(ti=ti, try_number=1)",
          "",
          "---------------"
        ],
        "tests/executors/test_local_kubernetes_executor.py||tests/executors/test_local_kubernetes_executor.py": [
          "File: tests/executors/test_local_kubernetes_executor.py -> tests/executors/test_local_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "96:         local_k8s_exec = LocalKubernetesExecutor(local_executor_mock, k8s_executor_mock)",
          "97:         simple_task_instance = mock.MagicMock()",
          "98:         simple_task_instance.queue = conf.get(\"local_kubernetes_executor\", \"kubernetes_queue\")",
          "101:         k8s_executor_mock.reset_mock()",
          "102:         simple_task_instance.queue = \"test-queue\"",
          "104:         k8s_executor_mock.get_task_log.assert_not_called()",
          "105:         assert logs == []",
          "106:         assert messages == []",
          "",
          "[Removed Lines]",
          "99:         local_k8s_exec.get_task_log(ti=simple_task_instance)",
          "100:         k8s_executor_mock.get_task_log.assert_called_once_with(ti=simple_task_instance)",
          "103:         messages, logs = local_k8s_exec.get_task_log(ti=simple_task_instance)",
          "",
          "[Added Lines]",
          "99:         local_k8s_exec.get_task_log(ti=simple_task_instance, try_number=3)",
          "100:         k8s_executor_mock.get_task_log.assert_called_once_with(ti=simple_task_instance, try_number=3)",
          "103:         messages, logs = local_k8s_exec.get_task_log(ti=simple_task_instance, try_number=3)",
          "",
          "---------------"
        ],
        "tests/utils/test_log_handlers.py||tests/utils/test_log_handlers.py": [
          "File: tests/utils/test_log_handlers.py -> tests/utils/test_log_handlers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "288:         ti.triggerer_job = None",
          "289:         with conf_vars({(\"core\", \"executor\"): executor_name}):",
          "290:             fth = FileTaskHandler(\"\")",
          "292:         if state == TaskInstanceState.RUNNING:",
          "294:         else:",
          "295:             mock_k8s_get_task_log.assert_not_called()",
          "",
          "[Removed Lines]",
          "291:             fth._read(ti=ti, try_number=1)",
          "293:             mock_k8s_get_task_log.assert_called_once_with(ti)",
          "",
          "[Added Lines]",
          "291:             fth._read(ti=ti, try_number=2)",
          "293:             mock_k8s_get_task_log.assert_called_once_with(ti, 2)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "357:         set_context(logger, ti)",
          "358:         ti.run(ignore_ti_state=True)",
          "359:         ti.state = TaskInstanceState.RUNNING",
          "362:         # first we find pod name",
          "363:         mock_list_pod.assert_called_once()",
          "",
          "[Removed Lines]",
          "360:         file_handler.read(ti, 3)",
          "",
          "[Added Lines]",
          "360:         file_handler.read(ti, 2)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "372:                     \"kubernetes_executor=True\",",
          "373:                     \"run_id=manual__2016-01-01T0000000000-2b88d1d57\",",
          "374:                     \"task_id=task_for_testing_file_log_handler\",",
          "376:                     \"airflow-worker\",",
          "377:                 ]",
          "378:             ),",
          "",
          "[Removed Lines]",
          "375:                     \"try_number=.+?\",",
          "",
          "[Added Lines]",
          "375:                     \"try_number=2\",",
          "",
          "---------------"
        ]
      }
    }
  ]
}