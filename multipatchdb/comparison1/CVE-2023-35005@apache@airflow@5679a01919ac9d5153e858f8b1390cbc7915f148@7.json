{
  "cve_id": "CVE-2023-35005",
  "cve_desc": "In Apache Airflow, some potentially sensitive values were being shown to the user in certain situations.\n\nThis vulnerability is mitigated by the fact configuration is not shown in the UI by default (only if `[webserver] expose_config` is set to `non-sensitive-only`), and not all uncensored values are actually sentitive.\n\n\nThis issue affects Apache Airflow: from 2.5.0 before 2.6.2. Users are recommended to update to version 2.6.2 or later.\n\n\n",
  "repo": "apache/airflow",
  "patch_hash": "5679a01919ac9d5153e858f8b1390cbc7915f148",
  "patch_info": {
    "commit_hash": "5679a01919ac9d5153e858f8b1390cbc7915f148",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/5679a01919ac9d5153e858f8b1390cbc7915f148",
    "files": [
      "airflow/config_templates/config.yml",
      "airflow/config_templates/default_airflow.cfg",
      "airflow/configuration.py",
      "airflow/www/views.py",
      "tests/core/test_configuration.py",
      "tests/www/views/test_views_configuration.py"
    ],
    "message": "Use single source of truth for sensitive config items (#31820)\n\nPreviously we had them defined both in constant and in config.yml.\n\nNow just config.yml\n\n(cherry picked from commit cab342ee010bfd048006ca458c760b37470b6ea5)",
    "before_after_code_files": [
      "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg",
      "airflow/configuration.py||airflow/configuration.py",
      "airflow/www/views.py||airflow/www/views.py",
      "tests/core/test_configuration.py||tests/core/test_configuration.py",
      "tests/www/views/test_views_configuration.py||tests/www/views/test_views_configuration.py"
    ]
  },
  "patch_diff": {
    "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg": [
      "File: airflow/config_templates/default_airflow.cfg -> airflow/config_templates/default_airflow.cfg",
      "--- Hunk 1 ---",
      "[Context before]",
      "995: # Example: result_backend = db+postgresql://postgres:airflow@postgres/airflow",
      "996: # result_backend =",
      "998: # Celery Flower is a sweet UI for Celery. Airflow has a shortcut to start",
      "999: # it ``airflow celery flower``. This defines the IP that Celery Flower runs on",
      "1000: flower_host = 0.0.0.0",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "998: # Optional configuration dictionary to pass to the Celery result backend SQLAlchemy engine.",
      "999: # Example: result_backend_sqlalchemy_engine_options = {{\"pool_recycle\": 1800}}",
      "1000: result_backend_sqlalchemy_engine_options =",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "1018: # Import path for celery configuration options",
      "1019: celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG",
      "1020: ssl_active = False",
      "1021: ssl_key =",
      "1022: ssl_cert =",
      "1023: ssl_cacert =",
      "1025: # Celery Pool implementation.",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1026: # Path to the client key.",
      "1029: # Path to the client certificate.",
      "1032: # Path to the CA certificate.",
      "",
      "---------------"
    ],
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "37: from contextlib import contextmanager, suppress",
      "38: from json.decoder import JSONDecodeError",
      "39: from re import Pattern",
      "41: from urllib.parse import urlsplit",
      "43: from typing_extensions import overload",
      "",
      "[Removed Lines]",
      "40: from typing import IO, Any, Dict, Iterable, Tuple, Union",
      "",
      "[Added Lines]",
      "40: from typing import IO, Any, Dict, Iterable, Set, Tuple, Union",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "147:         return yaml.safe_load(config_file)",
      "165: class AirflowConfigParser(ConfigParser):",
      "166:     \"\"\"Custom Airflow Configparser supporting defaults and deprecated options.\"\"\"",
      "",
      "[Removed Lines]",
      "150: SENSITIVE_CONFIG_VALUES = {",
      "151:     (\"database\", \"sql_alchemy_conn\"),",
      "152:     (\"core\", \"fernet_key\"),",
      "153:     (\"celery\", \"broker_url\"),",
      "154:     (\"celery\", \"flower_basic_auth\"),",
      "155:     (\"celery\", \"result_backend\"),",
      "156:     (\"atlas\", \"password\"),",
      "157:     (\"smtp\", \"smtp_password\"),",
      "158:     (\"webserver\", \"secret_key\"),",
      "159:     (\"secrets\", \"backend_kwargs\"),",
      "160:     # The following options are deprecated",
      "161:     (\"core\", \"sql_alchemy_conn\"),",
      "162: }",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "171:     # These configs can also be fetched from Secrets backend",
      "172:     # following the \"{section}__{name}__secret\" pattern",
      "176:     # A mapping of (new section, new option) -> (old section, old option, since_version).",
      "177:     # When reading new option, the old option will be checked to see if it exists. If it does a",
      "",
      "[Removed Lines]",
      "174:     sensitive_config_values: set[tuple[str, str]] = SENSITIVE_CONFIG_VALUES",
      "",
      "[Added Lines]",
      "159:     @cached_property",
      "160:     def sensitive_config_values(self) -> Set[tuple[str, str]]:  # noqa: UP006",
      "161:         default_config = default_config_yaml()",
      "162:         flattened = {",
      "163:             (s, k): item for s, s_c in default_config.items() for k, item in s_c.get(\"options\").items()",
      "164:         }",
      "165:         sensitive = {(section, key) for (section, key), v in flattened.items() if v.get(\"sensitive\") is True}",
      "166:         depr_option = {self.deprecated_options[x][:-1] for x in sensitive if x in self.deprecated_options}",
      "167:         depr_section = {",
      "168:             (self.deprecated_sections[s][0], k) for s, k in sensitive if s in self.deprecated_sections",
      "169:         }",
      "170:         sensitive.update(depr_section, depr_option)",
      "171:         return sensitive",
      "",
      "---------------"
    ],
    "airflow/www/views.py||airflow/www/views.py": [
      "File: airflow/www/views.py -> airflow/www/views.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "3951:         # TODO remove \"if raw\" usage in Airflow 3.0. Configuration can be fetched via the REST API.",
      "3952:         if raw:",
      "3953:             if expose_config == \"non-sensitive-only\":",
      "3956:                 updater = configupdater.ConfigUpdater()",
      "3957:                 updater.read(AIRFLOW_CONFIG)",
      "3959:                     if updater.has_option(sect, key):",
      "3960:                         updater[sect][key].value = \"< hidden >\"",
      "3961:                 config = str(updater)",
      "",
      "[Removed Lines]",
      "3954:                 from airflow.configuration import SENSITIVE_CONFIG_VALUES",
      "3958:                 for sect, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "3956:                 for sect, key in conf.sensitive_config_values:",
      "",
      "---------------"
    ],
    "tests/core/test_configuration.py||tests/core/test_configuration.py": [
      "File: tests/core/test_configuration.py -> tests/core/test_configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "36:     AirflowConfigException,",
      "37:     AirflowConfigParser,",
      "38:     conf,",
      "39:     expand_env_var,",
      "40:     get_airflow_config,",
      "41:     get_airflow_home,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "39:     default_config_yaml,",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "1447:             w = captured.pop()",
      "1448:             assert \"your `conf.get*` call to use the new name\" in str(w.message)",
      "1449:             assert w.category == FutureWarning",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1453: def test_sensitive_values():",
      "1454:     from airflow.settings import conf",
      "1456:     # this list was hardcoded prior to 2.6.2",
      "1457:     # included here to avoid regression in refactor",
      "1458:     # inclusion of keys ending in \"password\" or \"kwargs\" is automated from 2.6.2",
      "1459:     # items not matching this pattern must be added here manually",
      "1460:     sensitive_values = {",
      "1461:         (\"database\", \"sql_alchemy_conn\"),",
      "1462:         (\"core\", \"fernet_key\"),",
      "1463:         (\"celery\", \"broker_url\"),",
      "1464:         (\"celery\", \"flower_basic_auth\"),",
      "1465:         (\"celery\", \"result_backend\"),",
      "1466:         (\"atlas\", \"password\"),",
      "1467:         (\"smtp\", \"smtp_password\"),",
      "1468:         (\"webserver\", \"secret_key\"),",
      "1469:         (\"secrets\", \"backend_kwargs\"),",
      "1470:         (\"sentry\", \"sentry_dsn\"),",
      "1471:         (\"database\", \"sql_alchemy_engine_args\"),",
      "1472:         (\"core\", \"sql_alchemy_conn\"),",
      "1473:     }",
      "1474:     default_config = default_config_yaml()",
      "1475:     all_keys = {(s, k) for s, v in default_config.items() for k in v.get(\"options\")}",
      "1476:     suspected_sensitive = {(s, k) for (s, k) in all_keys if k.endswith((\"password\", \"kwargs\"))}",
      "1477:     exclude_list = {",
      "1478:         (\"kubernetes_executor\", \"delete_option_kwargs\"),",
      "1479:     }",
      "1480:     suspected_sensitive -= exclude_list",
      "1481:     sensitive_values.update(suspected_sensitive)",
      "1482:     assert sensitive_values == conf.sensitive_config_values",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_configuration.py||tests/www/views/test_views_configuration.py": [
      "File: tests/www/views/test_views_configuration.py -> tests/www/views/test_views_configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: import html",
      "22: from tests.test_utils.config import conf_vars",
      "23: from tests.test_utils.www import check_content_in_response, check_content_not_in_response",
      "",
      "[Removed Lines]",
      "21: from airflow.configuration import SENSITIVE_CONFIG_VALUES, conf",
      "",
      "[Added Lines]",
      "21: from airflow.configuration import conf",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "36: @conf_vars({(\"webserver\", \"expose_config\"): \"True\"})",
      "37: def test_user_can_view_configuration(admin_client):",
      "38:     resp = admin_client.get(\"configuration\", follow_redirects=True)",
      "40:         value = conf.get(section, key, fallback=\"\")",
      "41:         if not value:",
      "42:             continue",
      "",
      "[Removed Lines]",
      "39:     for section, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "39:     for section, key in conf.sensitive_config_values:",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "46: @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "47: def test_configuration_redacted(admin_client):",
      "48:     resp = admin_client.get(\"configuration\", follow_redirects=True)",
      "50:         value = conf.get(section, key, fallback=\"\")",
      "51:         if not value or value == \"airflow\":",
      "52:             continue",
      "",
      "[Removed Lines]",
      "49:     for section, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "49:     for section, key in conf.sensitive_config_values:",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "58: @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "59: def test_configuration_redacted_in_running_configuration(admin_client):",
      "60:     resp = admin_client.get(\"configuration\", follow_redirects=True)",
      "62:         value = conf.get(section, key, fallback=\"\")",
      "63:         if not value or value == \"airflow\":",
      "64:             continue",
      "",
      "[Removed Lines]",
      "61:     for section, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "61:     for section, key in conf.sensitive_config_values:",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "2de06561515887c992bb98271938a38267ccd0eb",
      "candidate_info": {
        "commit_hash": "2de06561515887c992bb98271938a38267ccd0eb",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/2de06561515887c992bb98271938a38267ccd0eb",
        "files": [
          "airflow/models/abstractoperator.py"
        ],
        "message": "Remove found_descendents param from get_flat_relative_ids (#31559)\n\nBy the looks of it, this param is unused.  Since the class is designated private, it is permissable to remove it.\n\n(cherry picked from commit 0cbc0dc6d8bbebce5e2f039a70eb48bf3ee7a267)",
        "before_after_code_files": [
          "airflow/models/abstractoperator.py||airflow/models/abstractoperator.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/abstractoperator.py||airflow/models/abstractoperator.py": [
          "File: airflow/models/abstractoperator.py -> airflow/models/abstractoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "154:             return self.upstream_task_ids",
          "155:         return self.downstream_task_ids",
          "163:         dag = self.get_dag()",
          "164:         if not dag:",
          "165:             return set()",
          "170:         task_ids_to_trace = self.get_direct_relative_ids(upstream)",
          "171:         while task_ids_to_trace:",
          "172:             task_ids_to_trace_next: set[str] = set()",
          "173:             for task_id in task_ids_to_trace:",
          "175:                     continue",
          "176:                 task_ids_to_trace_next.update(dag.task_dict[task_id].get_direct_relative_ids(upstream))",
          "178:             task_ids_to_trace = task_ids_to_trace_next",
          "182:     def get_flat_relatives(self, upstream: bool = False) -> Collection[Operator]:",
          "183:         \"\"\"Get a flat list of relatives, either upstream or downstream.\"\"\"",
          "184:         dag = self.get_dag()",
          "185:         if not dag:",
          "186:             return set()",
          "189:     def _iter_all_mapped_downstreams(self) -> Iterator[MappedOperator | MappedTaskGroup]:",
          "190:         \"\"\"Return mapped nodes that are direct dependencies of the current task.",
          "",
          "[Removed Lines]",
          "157:     def get_flat_relative_ids(",
          "158:         self,",
          "159:         upstream: bool = False,",
          "160:         found_descendants: set[str] | None = None,",
          "161:     ) -> set[str]:",
          "162:         \"\"\"Get a flat set of relative IDs, upstream or downstream.\"\"\"",
          "167:         if found_descendants is None:",
          "168:             found_descendants = set()",
          "174:                 if task_id in found_descendants:",
          "177:                 found_descendants.add(task_id)",
          "180:         return found_descendants",
          "187:         return [dag.task_dict[task_id] for task_id in self.get_flat_relative_ids(upstream)]",
          "",
          "[Added Lines]",
          "157:     def get_flat_relative_ids(self, *, upstream: bool = False) -> set[str]:",
          "158:         \"\"\"",
          "159:         Get a flat set of relative IDs, upstream or downstream.",
          "161:         Will recurse each relative found in the direction specified.",
          "163:         :param upstream: Whether to look for upstream or downstream relatives.",
          "164:         \"\"\"",
          "169:         relatives: set[str] = set()",
          "175:                 if task_id in relatives:",
          "178:                 relatives.add(task_id)",
          "181:         return relatives",
          "188:         return [dag.task_dict[task_id] for task_id in self.get_flat_relative_ids(upstream=upstream)]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f2479f48bd7c42d16c9c394c27ce78345cee78c4",
      "candidate_info": {
        "commit_hash": "f2479f48bd7c42d16c9c394c27ce78345cee78c4",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f2479f48bd7c42d16c9c394c27ce78345cee78c4",
        "files": [
          "airflow/executors/kubernetes_executor.py",
          "tests/executors/test_kubernetes_executor.py"
        ],
        "message": "Fix Kubernetes executor set wrong task status (#31274)\n\n* Fix Kubernetes executor set wrong task status\n\nIn the case of multiple schedulers and lots of tasks running\nIf somehow schedulers restart and try to adopt pods\nin some cases, it sets the wrong task status.\nIn this PR, I'm changing some checks so that if the pod status is non-terminal\nthen set the task status Failed only\nif the pod event type is DELETED and POD_EXECUTOR_DONE_KEY is in the pod label\n\n* cleanup\n\n(cherry picked from commit dfbf5299ee2c3e68f6ec39ac502059fedbef3358)",
        "before_after_code_files": [
          "airflow/executors/kubernetes_executor.py||airflow/executors/kubernetes_executor.py",
          "tests/executors/test_kubernetes_executor.py||tests/executors/test_kubernetes_executor.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/executors/kubernetes_executor.py||airflow/executors/kubernetes_executor.py": [
          "File: airflow/executors/kubernetes_executor.py -> airflow/executors/kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "216:         resource_version: str,",
          "217:         event: Any,",
          "218:     ) -> None:",
          "219:         \"\"\"Process status response.\"\"\"",
          "220:         if status == \"Pending\":",
          "223:                 self.watcher_queue.put((pod_name, namespace, State.FAILED, annotations, resource_version))",
          "224:             else:",
          "225:                 self.log.debug(\"Event: %s Pending\", pod_name)",
          "",
          "[Removed Lines]",
          "221:             if event[\"type\"] == \"DELETED\":",
          "222:                 self.log.info(\"Event: Failed to start pod %s\", pod_name)",
          "",
          "[Added Lines]",
          "219:         pod = event[\"object\"]",
          "220:         annotations_string = annotations_for_logging_task_metadata(annotations)",
          "223:             # deletion_timestamp is set by kube server when a graceful deletion is requested.",
          "224:             # since kube server have received request to delete pod set TI state failed",
          "225:             if event[\"type\"] == \"DELETED\" and pod.metadata.deletion_timestamp:",
          "226:                 self.log.info(\"Event: Failed to start pod %s, annotations: %s\", pod_name, annotations_string)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "232:             # If our event type is DELETED, we have the POD_EXECUTOR_DONE_KEY, or the pod has",
          "233:             # a deletion timestamp, we've already seen the initial Succeeded event and sent it",
          "234:             # along to the scheduler.",
          "236:             if (",
          "237:                 event[\"type\"] == \"DELETED\"",
          "238:                 or POD_EXECUTOR_DONE_KEY in pod.metadata.labels",
          "",
          "[Removed Lines]",
          "235:             pod = event[\"object\"]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "246:             self.log.info(\"Event: %s Succeeded\", pod_name)",
          "247:             self.watcher_queue.put((pod_name, namespace, None, annotations, resource_version))",
          "248:         elif status == \"Running\":",
          "251:                 self.watcher_queue.put((pod_name, namespace, State.FAILED, annotations, resource_version))",
          "252:             else:",
          "253:                 self.log.info(\"Event: %s is Running\", pod_name)",
          "",
          "[Removed Lines]",
          "249:             if event[\"type\"] == \"DELETED\":",
          "250:                 self.log.info(\"Event: Pod %s deleted before it could complete\", pod_name)",
          "",
          "[Added Lines]",
          "252:             # deletion_timestamp is set by kube server when a graceful deletion is requested.",
          "253:             # since kube server have received request to delete pod set TI state failed",
          "254:             if event[\"type\"] == \"DELETED\" and pod.metadata.deletion_timestamp:",
          "255:                 self.log.info(",
          "256:                     \"Event: Pod %s deleted before it could complete, annotations: %s\",",
          "257:                     pod_name,",
          "258:                     annotations_string,",
          "259:                 )",
          "",
          "---------------"
        ],
        "tests/executors/test_kubernetes_executor.py||tests/executors/test_kubernetes_executor.py": [
          "File: tests/executors/test_kubernetes_executor.py -> tests/executors/test_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1215:     def test_process_status_pending_deleted(self):",
          "1216:         self.events.append({\"type\": \"DELETED\", \"object\": self.pod})",
          "1218:         self._run()",
          "1219:         self.assert_watcher_queue_called_once_with_state(State.FAILED)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1217:         self.pod.metadata.deletion_timestamp = datetime.utcnow()",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1259:     def test_process_status_running_deleted(self):",
          "1260:         self.pod.status.phase = \"Running\"",
          "1261:         self.events.append({\"type\": \"DELETED\", \"object\": self.pod})",
          "1263:         self._run()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1262:         self.pod.metadata.deletion_timestamp = datetime.utcnow()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "caab2a95faf27727686ae963ea1aed4fc79844af",
      "candidate_info": {
        "commit_hash": "caab2a95faf27727686ae963ea1aed4fc79844af",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/caab2a95faf27727686ae963ea1aed4fc79844af",
        "files": [
          "setup.py"
        ],
        "message": "Convert dask upper-binding into exclusion (#31329)\n\nAs https://github.com/dask/dask/issues/10279 has been solved and\nDask tests should no longer generate a PATH side-effect, we\nshould turn the upper-binding into exclusion, to make sure that\nwe upgrade to newer Dask version when it is released.\n\n(cherry picked from commit 45f1fda2fac8bf5455a1f444b98b58d38ea2db63)",
        "before_after_code_files": [
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "252:     # Supporting it in the future",
          "253:     \"cloudpickle>=1.4.1\",",
          "254:     # Dask and distributed in version 2023.5.0 break our tests for Python > 3.7",
          "259: ]",
          "260: deprecated_api = [",
          "261:     \"requests>=2.26.0\",",
          "",
          "[Removed Lines]",
          "255:     # The upper limit can be removed when https://github.com/dask/dask/issues/10279 is fixed",
          "256:     # Dask in version 2022.10.1 removed `bokeh` support and we should avoid installing it",
          "257:     \"dask>=2.9.0,!=2022.10.1,<2023.5.0\",",
          "258:     \"distributed>=2.11.1,<2023.5.0\",",
          "",
          "[Added Lines]",
          "255:     # See https://github.com/dask/dask/issues/10279",
          "256:     \"dask>=2.9.0,!=2022.10.1,!=2023.5.0\",",
          "257:     \"distributed>=2.11.1,!=2023.5.0\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "9d28825e98219e3b2710bf361792b6228cbad6d0",
      "candidate_info": {
        "commit_hash": "9d28825e98219e3b2710bf361792b6228cbad6d0",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/9d28825e98219e3b2710bf361792b6228cbad6d0",
        "files": [
          "airflow/www/static/js/api/useTaskLog.ts",
          "airflow/www/static/js/dag/details/taskInstance/Logs/index.tsx"
        ],
        "message": "Added spinner activity while the logs load (#31165)\n\n* spinner added\n\n* fixed static checks\n\n* isFetching\n\n* isFetching replaced with isLoading\n\n(cherry picked from commit 584a9f5dae5b29a1968fbdbc9b1edd01ae2be4d2)",
        "before_after_code_files": [
          "airflow/www/static/js/api/useTaskLog.ts||airflow/www/static/js/api/useTaskLog.ts",
          "airflow/www/static/js/dag/details/taskInstance/Logs/index.tsx||airflow/www/static/js/dag/details/taskInstance/Logs/index.tsx"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/static/js/api/useTaskLog.ts||airflow/www/static/js/api/useTaskLog.ts": [
          "File: airflow/www/static/js/api/useTaskLog.ts -> airflow/www/static/js/api/useTaskLog.ts",
          "--- Hunk 1 ---",
          "[Context before]",
          "74:       });",
          "75:     },",
          "76:     {",
          "78:       refetchInterval:",
          "79:         expectingLogs && isRefreshOn && (autoRefreshInterval || 1) * 1000,",
          "80:     }",
          "",
          "[Removed Lines]",
          "77:       placeholderData: \"\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/www/static/js/dag/details/taskInstance/Logs/index.tsx||airflow/www/static/js/dag/details/taskInstance/Logs/index.tsx": [
          "File: airflow/www/static/js/dag/details/taskInstance/Logs/index.tsx -> airflow/www/static/js/dag/details/taskInstance/Logs/index.tsx",
          "--- Hunk 1 ---",
          "[Context before]",
          "26:   Button,",
          "27:   Checkbox,",
          "28:   Icon,",
          "29: } from \"@chakra-ui/react\";",
          "30: import { MdWarning } from \"react-icons/md\";",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "29:   Spinner,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "119:   const { timezone } = useTimezone();",
          "121:   const taskTryNumber = selectedTryNumber || tryNumber || 1;",
          "123:     dagId,",
          "124:     dagRunId,",
          "125:     taskId,",
          "",
          "[Removed Lines]",
          "122:   const { data } = useTaskLog({",
          "",
          "[Added Lines]",
          "123:   const { data, isLoading } = useTaskLog({",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "266:               <Text fontSize=\"sm\">{warning}</Text>",
          "267:             </Flex>",
          "268:           )}",
          "275:           )}",
          "276:         </>",
          "277:       )}",
          "",
          "[Removed Lines]",
          "269:           {!!parsedLogs && (",
          "270:             <LogBlock",
          "271:               parsedLogs={parsedLogs}",
          "272:               wrap={wrap}",
          "273:               tryNumber={taskTryNumber}",
          "274:             />",
          "",
          "[Added Lines]",
          "270:           {isLoading ? (",
          "271:             <Spinner />",
          "272:           ) : (",
          "273:             !!parsedLogs && (",
          "274:               <LogBlock",
          "275:                 parsedLogs={parsedLogs}",
          "276:                 wrap={wrap}",
          "277:                 tryNumber={taskTryNumber}",
          "278:               />",
          "279:             )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1a8d05d7d9e44a090b6dce8548aa68a3119722f9",
      "candidate_info": {
        "commit_hash": "1a8d05d7d9e44a090b6dce8548aa68a3119722f9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/1a8d05d7d9e44a090b6dce8548aa68a3119722f9",
        "files": [
          "airflow/jobs/scheduler_job_runner.py"
        ],
        "message": "Executor events are not always \"exited\" here (#30859)\n\n(cherry picked from commit f39ccad9d8fa239bdbeb7e88afdedea5e8fddf76)",
        "before_after_code_files": [
          "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py": [
          "File: airflow/jobs/scheduler_job_runner.py -> airflow/jobs/scheduler_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "674:             # We create map (dag_id, task_id, execution_date) -> in-memory try_number",
          "675:             ti_primary_key_to_try_number_map[ti_key.primary] = ti_key.try_number",
          "685:             if state in (State.FAILED, State.SUCCESS, State.QUEUED):",
          "686:                 tis_with_right_state.append(ti_key)",
          "",
          "[Removed Lines]",
          "677:             self.log.info(",
          "678:                 \"Executor reports execution of %s.%s run_id=%s exited with status %s for try_number %s\",",
          "679:                 ti_key.dag_id,",
          "680:                 ti_key.task_id,",
          "681:                 ti_key.run_id,",
          "682:                 state,",
          "683:                 ti_key.try_number,",
          "684:             )",
          "",
          "[Added Lines]",
          "677:             self.log.info(\"Received executor event with state %s for task instance %s\", state, ti_key)",
          "",
          "---------------"
        ]
      }
    }
  ]
}