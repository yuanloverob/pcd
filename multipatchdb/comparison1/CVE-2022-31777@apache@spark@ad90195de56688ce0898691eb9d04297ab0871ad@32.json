{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "02780b940b61bc6dad61d91e4773f711de584e4a",
      "candidate_info": {
        "commit_hash": "02780b940b61bc6dad61d91e4773f711de584e4a",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/02780b940b61bc6dad61d91e4773f711de584e4a",
        "files": [
          "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala",
          "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala"
        ],
        "message": "[SPARK-38973][SHUFFLE] Mark stage as merge finalized irrespective of its state\n\n### What changes were proposed in this pull request?\nThis change fixes the scenarios where a stage re-attempt doesn't complete successfully, even though all the tasks complete when push-based shuffle is enabled.  With Adaptive Merge Finalization, a stage may be triggered for finalization when it is the below state:\n- The stage is not running (not in the running set of the DAGScheduler) - had failed or canceled or waiting, and\n- The stage has no pending partitions (all the tasks completed at-least once)\n\nFor such a stage when the finalization completes, the stage will still not be marked as mergeFinalized.\nThe stage of the stage will be:\n- `stage.shuffleDependency.mergeFinalized = false`\n- `stage.shuffleDependency.getFinalizeTask != Nil`\n- Merged statuses of the state are unregistered\n\nWhen the stage is resubmitted, the newer attempt of the stage will never complete even though its tasks may be completed. This is because the newer attempt of the stage will have `shuffleMergeEnabled = true`, since with the previous attempt the stage was never marked as mergedFinalized, and the finalizeTask is present (from finalization attempt for previous stage attempt).\n\n So, when all the tasks of the newer attempt complete, then these conditions will be true:\n- stage will be running\n- There will be no pending partitions since all the tasks completed\n- `stage.shuffleDependency.shuffleMergeEnabled = true`\n- `stage.shuffleDependency.shuffleMergeFinalized = false`\n- `stage.shuffleDependency.getFinalizeTask` is `not empty`\nThis leads the DAGScheduler to try scheduling finalization and not trigger the completion of the Stage. However because of the last condition it never even schedules the finalization and the stage never completes.\n\nIn addition, for determinate stages, which have completed merge finalization, we don't need to unregister merge results - since the stage retry, or any other stage computing the same shuffle id, can use it.\n\n### Why are the changes needed?\nThe change fixes the above issue where the application gets stalled as some stages don't complete successfully.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nI have just modified the existing UT. A stage will be marked finalized irrespective of its state and for deterministic stage we don't want to unregister merge results.\n\nCloses #36293 from otterc/SPARK-38973.\n\nAuthored-by: Chandni Singh <singh.chandni@gmail.com>\nSigned-off-by: Mridul Muralidharan <mridul<at>gmail.com>\n(cherry picked from commit f4a81ae6e631af27fc5eef81097b842d4e0e2e51)\nSigned-off-by: Mridul Muralidharan <mridulatgmail.com>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala||core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala",
          "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala||core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala||core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala": [
          "File: core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala -> core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2339:     if (stage.shuffleDep.shuffleMergeId == shuffleMergeId) {",
          "2341:         if (runningStages.contains(stage)) {",
          "2343:           processShuffleMapStageCompletion(stage)",
          "2347:           mapOutputTracker.unregisterAllMergeResult(stage.shuffleDep.shuffleId)",
          "2348:         }",
          "2354:     }",
          "2355:   }",
          "",
          "[Removed Lines]",
          "2340:       if (stage.pendingPartitions.isEmpty) {",
          "2342:           stage.shuffleDep.markShuffleMergeFinalized()",
          "2344:         } else {",
          "2349:       } else {",
          "2352:         stage.shuffleDep.markShuffleMergeFinalized()",
          "2353:       }",
          "",
          "[Added Lines]",
          "2350:       stage.shuffleDep.markShuffleMergeFinalized()",
          "2351:       if (stage.pendingPartitions.isEmpty)",
          "2354:         } else if (stage.isIndeterminate) {",
          "",
          "---------------"
        ],
        "core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala||core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala": [
          "File: core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala -> core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "3709:     assertDataStructuresEmpty()",
          "3710:   }",
          "3714:     initPushBasedShuffleConfs(conf)",
          "3715:     DAGSchedulerSuite.clearMergerLocs()",
          "3716:     DAGSchedulerSuite.addMergerLocs(Seq(\"host1\", \"host2\", \"host3\", \"host4\", \"host5\"))",
          "",
          "[Removed Lines]",
          "3712:   test(\"SPARK-32920: Merge results should be unregistered if the running stage is cancelled\" +",
          "3713:     \" before shuffle merge is finalized\") {",
          "",
          "[Added Lines]",
          "3712:   test(\"SPARK-32920: Cancelled stage should be marked finalized after the shuffle merge \" +",
          "3713:     \"is finalized\") {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3742:     runEvent(StageCancelled(0, Option(\"Explicit cancel check\")))",
          "3743:     scheduler.handleShuffleMergeFinalized(shuffleMapStageToCancel,",
          "3744:       shuffleMapStageToCancel.shuffleDep.shuffleMergeId)",
          "3746:   }",
          "3748:   test(\"SPARK-32920: SPARK-35549: Merge results should not get registered\" +",
          "",
          "[Removed Lines]",
          "3745:     assert(mapOutputTracker.getNumAvailableMergeResults(shuffleDep.shuffleId) == 0)",
          "",
          "[Added Lines]",
          "3745:     assert(mapOutputTracker.getNumAvailableMergeResults(shuffleDep.shuffleId) == 2)",
          "3746:     assert(shuffleMapStageToCancel.shuffleDep.isShuffleMergeFinalizedMarked)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "4001:   }",
          "4013:     initPushBasedShuffleConfs(conf)",
          "4014:     conf.set(config.PUSH_BASED_SHUFFLE_SIZE_MIN_SHUFFLE_SIZE_TO_WAIT, 10L)",
          "4015:     conf.set(config.SHUFFLE_MERGER_LOCATIONS_MIN_STATIC_THRESHOLD, 5)",
          "",
          "[Removed Lines]",
          "4011:   test(\"SPARK-33701: check adaptive shuffle merge finalization behavior with stage\" +",
          "4012:     \" cancellation during spark.shuffle.push.finalize.timeout wait\") {",
          "",
          "[Added Lines]",
          "4006:   test(\"SPARK-33701: check adaptive shuffle merge finalization behavior with stage \" +",
          "4007:     \"cancellation for determinate and indeterminate stages during \" +",
          "4008:     \"spark.shuffle.push.finalize.timeout wait\") {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "4052:     scheduler.handleShuffleMergeFinalized(shuffleStage1, shuffleStage1.shuffleDep.shuffleMergeId)",
          "4054:     assert(shuffleStage1.shuffleDep.mergerLocs.nonEmpty)",
          "4056:     assert(mapOutputTracker.",
          "4060:     val shuffleMapIndeterminateRdd1 = new MyRDD(sc, parts, Nil, indeterminate = true)",
          "",
          "[Removed Lines]",
          "4055:     assert(!shuffleStage1.shuffleDep.isShuffleMergeFinalizedMarked)",
          "4057:       getNumAvailableMergeResults(shuffleStage1.shuffleDep.shuffleId) == 0)",
          "",
          "[Added Lines]",
          "4051:     assert(shuffleStage1.shuffleDep.isShuffleMergeFinalizedMarked)",
          "4053:       getNumAvailableMergeResults(shuffleStage1.shuffleDep.shuffleId) == 4)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5847014fc3fe08b8a59c107a99c1540fbb2c2208",
      "candidate_info": {
        "commit_hash": "5847014fc3fe08b8a59c107a99c1540fbb2c2208",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/5847014fc3fe08b8a59c107a99c1540fbb2c2208",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala"
        ],
        "message": "[SPARK-39393][SQL] Parquet data source only supports push-down predicate filters for non-repeated primitive types\n\n### What changes were proposed in this pull request?\n\nIn Spark version 3.1.0 and newer, Spark creates extra filter predicate conditions for repeated parquet columns.\nThese fields do not have the ability to have a filter predicate, according to the [PARQUET-34](https://issues.apache.org/jira/browse/PARQUET-34) issue in the parquet library.\n\nThis PR solves this problem until the appropriate functionality is provided by the parquet.\n\nBefore this PR:\n\nAssume follow Protocol buffer schema:\n\n```\nmessage Model {\n    string name = 1;\n    repeated string keywords = 2;\n}\n```\n\nSuppose a parquet file is created from a set of records in the above format with the help of the parquet-protobuf library.\nUsing Spark version 3.1.0 or newer, we get following exception when run the following query using spark-shell:\n\n```\nval data = spark.read.parquet(\"/path/to/parquet\")\ndata.registerTempTable(\"models\")\nspark.sql(\"select * from models where array_contains(keywords, 'X')\").show(false)\n```\n\n```\nCaused by: java.lang.IllegalArgumentException: FilterPredicates do not currently support repeated columns. Column keywords is repeated.\n  at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumn(SchemaCompatibilityValidator.java:176)\n  at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumnFilterPredicate(SchemaCompatibilityValidator.java:149)\n  at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:89)\n  at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:56)\n  at org.apache.parquet.filter2.predicate.Operators$NotEq.accept(Operators.java:192)\n  at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validate(SchemaCompatibilityValidator.java:61)\n  at org.apache.parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:95)\n  at org.apache.parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:45)\n  at org.apache.parquet.filter2.compat.FilterCompat$FilterPredicateCompat.accept(FilterCompat.java:149)\n  at org.apache.parquet.filter2.compat.RowGroupFilter.filterRowGroups(RowGroupFilter.java:72)\n  at org.apache.parquet.hadoop.ParquetFileReader.filterRowGroups(ParquetFileReader.java:870)\n  at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:789)\n  at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)\n  at org.apache.parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:162)\n  at org.apache.parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:140)\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:373)\n  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n...\n```\n\nThe cause of the problem is due to a change in the data filtering conditions:\n\n```\nspark.sql(\"select * from log where array_contains(keywords, 'X')\").explain(true);\n\n// Spark 3.0.2 and older\n== Physical Plan ==\n...\n+- FileScan parquet [link#0,keywords#1]\n  DataFilters: [array_contains(keywords#1, Google)]\n  PushedFilters: []\n  ...\n\n// Spark 3.1.0 and newer\n== Physical Plan == ...\n+- FileScan parquet [link#0,keywords#1]\n  DataFilters: [isnotnull(keywords#1),  array_contains(keywords#1, Google)]\n  PushedFilters: [IsNotNull(keywords)]\n  ...\n```\n\nPushing filters down for repeated columns of parquet is not necessary because it is not supported by parquet library for now. So we can exclude them from pushed predicate filters and solve issue.\n\n### Why are the changes needed?\n\nPredicate filters that are pushed down to parquet should not be created on repeated-type fields.\n\n### Does this PR introduce any user-facing change?\n\nNo, It's only fixed a bug and before this, due to the limitations of the parquet library, no more work was possible.\n\n### How was this patch tested?\n\nAdd an extra test to ensure problem solved.\n\nCloses #36781 from Borjianamin98/master.\n\nAuthored-by: Amin Borjian <borjianamin98@outlook.com>\nSigned-off-by: huaxingao <huaxin_gao@apple.com>\n(cherry picked from commit ac2881a8c3cfb196722a5680a62ebd6bb9fba728)\nSigned-off-by: huaxingao <huaxin_gao@apple.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilters.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "33: import org.apache.parquet.schema.LogicalTypeAnnotation.{DecimalLogicalTypeAnnotation, TimeUnit}",
          "34: import org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName",
          "35: import org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName._",
          "37: import org.apache.spark.sql.catalyst.util.{CaseInsensitiveMap, DateTimeUtils, IntervalUtils}",
          "38: import org.apache.spark.sql.catalyst.util.RebaseDateTime.{rebaseGregorianToJulianDays, rebaseGregorianToJulianMicros, RebaseSpec}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "36: import org.apache.parquet.schema.Type.Repetition",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "64:         fields: Seq[Type],",
          "65:         parentFieldNames: Array[String] = Array.empty): Seq[ParquetPrimitiveField] = {",
          "66:       fields.flatMap {",
          "68:           Some(ParquetPrimitiveField(fieldNames = parentFieldNames :+ p.getName,",
          "69:             fieldType = ParquetSchemaType(p.getLogicalTypeAnnotation,",
          "70:               p.getPrimitiveTypeName, p.getTypeLength)))",
          "",
          "[Removed Lines]",
          "67:         case p: PrimitiveType =>",
          "",
          "[Added Lines]",
          "71:         case p: PrimitiveType if p.getRepetition != Repetition.REPEATED =>",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.execution.datasources.parquet",
          "20: import java.math.{BigDecimal => JBigDecimal}",
          "21: import java.nio.charset.StandardCharsets",
          "22: import java.sql.{Date, Timestamp}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import java.io.File",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1297:     }",
          "1298:   }",
          "1300:   test(\"Filters should be pushed down for vectorized Parquet reader at row group level\") {",
          "1301:     import testImplicits._",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1301:   test(\"SPARK-39393: Do not push down predicate filters for repeated primitive fields\") {",
          "1302:     import ParquetCompatibilityTest._",
          "1303:     withTempDir { dir =>",
          "1304:       val protobufParquetFilePath = new File(dir, \"protobuf-parquet\").getCanonicalPath",
          "1306:       val protobufSchema =",
          "1307:         \"\"\"message protobuf_style {",
          "1308:           |  repeated int32 f;",
          "1309:           |}",
          "1310:         \"\"\".stripMargin",
          "1312:       writeDirect(protobufParquetFilePath, protobufSchema, { rc =>",
          "1313:         rc.message {",
          "1314:           rc.field(\"f\", 0) {",
          "1315:               rc.addInteger(1)",
          "1316:               rc.addInteger(2)",
          "1317:           }",
          "1318:         }",
          "1319:       })",
          "1323:       checkAnswer(",
          "1324:         spark.read.parquet(dir.getCanonicalPath).filter(\"isnotnull(f)\"),",
          "1325:         Seq(Row(Seq(1, 2))))",
          "1326:     }",
          "1327:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0fdb6757946e2a0991256a3b73c0c09d6e764eed",
      "candidate_info": {
        "commit_hash": "0fdb6757946e2a0991256a3b73c0c09d6e764eed",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/0fdb6757946e2a0991256a3b73c0c09d6e764eed",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala"
        ],
        "message": "[SPARK-38813][3.3][SQL][FOLLOWUP] Improve the analysis check for TimestampNTZ output\n\n### What changes were proposed in this pull request?\n\nIn https://github.com/apache/spark/pull/36094, a check for failing TimestampNTZ output is added.\nHowever, if there is an unresolved attribute in the plan, even if it is note related to TimestampNTZ, the error message becomes confusing\n```\nscala> val df = spark.range(2)\ndf: org.apache.spark.sql.Dataset[Long] = [id: bigint]\n\nscala> df.select(\"i\")\norg.apache.spark.sql.AnalysisException: Invalid call to dataType on unresolved object;\n'Project ['i]\n+- Range (0, 2, step=1, splits=Some(16))\n\n  at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.dataType(unresolved.scala:137)\n  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$4(CheckAnalysis.scala:164)\n...\n```\n\nBefore changes it was\n```\norg.apache.spark.sql.AnalysisException: Column 'i' does not exist. Did you mean one of the following? [id];\n```\n\nThis PR is the improve the check for TimestampNTZ and restore the error message for unresolved attributes.\n### Why are the changes needed?\n\nFix a regression in analysis error message.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, it is not released yet.\n\n### How was this patch tested?\n\nManual test\n\nCloses #36316 from gengliangwang/bugFix.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "161:         throw QueryCompilationErrors.commandUnsupportedInV2TableError(\"SHOW TABLE EXTENDED\")",
          "163:       case operator: LogicalPlan",
          "165:         operator.failAnalysis(\"TimestampNTZ type is not supported in Spark 3.3.\")",
          "167:       case operator: LogicalPlan =>",
          "",
          "[Removed Lines]",
          "164:         if !Utils.isTesting && operator.output.exists(_.dataType.isInstanceOf[TimestampNTZType]) =>",
          "",
          "[Added Lines]",
          "164:         if !Utils.isTesting && operator.output.exists(attr =>",
          "165:           attr.resolved && attr.dataType.isInstanceOf[TimestampNTZType]) =>",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4fb7fe2a40623526ed22311eac16c937450031e5",
      "candidate_info": {
        "commit_hash": "4fb7fe2a40623526ed22311eac16c937450031e5",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/4fb7fe2a40623526ed22311eac16c937450031e5",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DecimalExpressionSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ],
        "message": "[SPARK-39208][SQL] Fix query context bugs in decimal overflow under codegen mode\n\n### What changes were proposed in this pull request?\n\n1. Fix logical bugs in adding query contexts as references under codegen mode. https://github.com/apache/spark/pull/36040/files#diff-4a70d2f3a4b99f58796b87192143f9838f4c4cf469f3313eb30af79c4e07153aR145\nThe code\n```\n    val errorContextCode = if (nullOnOverflow) {\n      ctx.addReferenceObj(\"errCtx\", queryContext)\n    } else {\n      \"\\\"\\\"\"\n    }\n```\nshould be\n```\n    val errorContextCode = if (nullOnOverflow) {\n      \"\\\"\\\"\"\n    } else {\n      ctx.addReferenceObj(\"errCtx\", queryContext)\n    }\n```\n\n2. Similar to https://github.com/apache/spark/pull/36557, make `CheckOverflowInSum` support query context when WSCG is not available.\n\n### Why are the changes needed?\n\nBugfix and enhancement in the query context of decimal expressions.\n### Does this PR introduce _any_ user-facing change?\n\nNo, the query context is not released yet.\n\n### How was this patch tested?\n\nNew UT\n\nCloses #36577 from gengliangwang/fixDecimalSumOverflow.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>\n(cherry picked from commit 191e535b975e5813719d3143797c9fcf86321368)\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DecimalExpressionSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DecimalExpressionSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Sum.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "147:     case d: DecimalType =>",
          "150:     case _ if shouldTrackIsEmpty =>",
          "151:       If(isEmpty, Literal.create(null, resultType), sum)",
          "152:     case _ => sum",
          "",
          "[Removed Lines]",
          "146:   protected def getEvaluateExpression: Expression = resultType match {",
          "148:       If(isEmpty, Literal.create(null, resultType),",
          "149:         CheckOverflowInSum(sum, d, !useAnsiAdd))",
          "",
          "[Added Lines]",
          "146:   protected def getEvaluateExpression(queryContext: String): Expression = resultType match {",
          "148:       val checkOverflowInSum =",
          "149:         CheckOverflowInSum(sum, d, !useAnsiAdd, queryContext)",
          "150:       If(isEmpty, Literal.create(null, resultType), checkOverflowInSum)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "172: case class Sum(",
          "173:     child: Expression,",
          "174:     useAnsiAdd: Boolean = SQLConf.get.ansiEnabled)",
          "176:   def this(child: Expression) = this(child, useAnsiAdd = SQLConf.get.ansiEnabled)",
          "178:   override def shouldTrackIsEmpty: Boolean = resultType match {",
          "",
          "[Removed Lines]",
          "175:   extends SumBase(child) {",
          "",
          "[Added Lines]",
          "176:   extends SumBase(child) with SupportQueryContext {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "187:   override lazy val mergeExpressions: Seq[Expression] = getMergeExpressions",
          "190: }",
          "",
          "[Removed Lines]",
          "189:   override lazy val evaluateExpression: Expression = getEvaluateExpression",
          "",
          "[Added Lines]",
          "190:   override lazy val evaluateExpression: Expression = getEvaluateExpression(queryContext)",
          "192:   override def initQueryContext(): String = if (useAnsiAdd) {",
          "193:     origin.context",
          "194:   } else {",
          "195:     \"\"",
          "196:   }",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "244:   override lazy val evaluateExpression: Expression =",
          "245:     if (useAnsiAdd) {",
          "247:     } else {",
          "249:     }",
          "251:   override protected def withNewChildInternal(newChild: Expression): Expression =",
          "",
          "[Removed Lines]",
          "246:       TryEval(getEvaluateExpression)",
          "248:       getEvaluateExpression",
          "",
          "[Added Lines]",
          "253:       TryEval(getEvaluateExpression(\"\"))",
          "255:       getEvaluateExpression(\"\")",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "143:   override protected def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {",
          "144:     val errorContextCode = if (nullOnOverflow) {",
          "147:       \"\\\"\\\"\"",
          "148:     }",
          "149:     nullSafeCodeGen(ctx, ev, eval => {",
          "",
          "[Removed Lines]",
          "145:       ctx.addReferenceObj(\"errCtx\", queryContext)",
          "146:     } else {",
          "",
          "[Added Lines]",
          "146:     } else {",
          "147:       ctx.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "175: case class CheckOverflowInSum(",
          "176:     child: Expression,",
          "177:     dataType: DecimalType,",
          "180:   override def nullable: Boolean = true",
          "",
          "[Removed Lines]",
          "178:     nullOnOverflow: Boolean) extends UnaryExpression {",
          "",
          "[Added Lines]",
          "178:     nullOnOverflow: Boolean,",
          "179:     queryContext: String = \"\") extends UnaryExpression {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "183:     val value = child.eval(input)",
          "184:     if (value == null) {",
          "185:       if (nullOnOverflow) null",
          "187:     } else {",
          "188:       value.asInstanceOf[Decimal].toPrecision(",
          "189:         dataType.precision,",
          "190:         dataType.scale,",
          "191:         Decimal.ROUND_HALF_UP,",
          "192:         nullOnOverflow,",
          "194:     }",
          "195:   }",
          "197:   override protected def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {",
          "198:     val childGen = child.genCode(ctx)",
          "199:     val errorContextCode = if (nullOnOverflow) {",
          "202:       \"\\\"\\\"\"",
          "203:     }",
          "204:     val nullHandling = if (nullOnOverflow) {",
          "205:       \"\"",
          "",
          "[Removed Lines]",
          "186:       else throw QueryExecutionErrors.overflowInSumOfDecimalError(origin.context)",
          "193:         origin.context)",
          "200:       ctx.addReferenceObj(\"errCtx\", origin.context)",
          "201:     } else {",
          "",
          "[Added Lines]",
          "187:       else throw QueryExecutionErrors.overflowInSumOfDecimalError(queryContext)",
          "194:         queryContext)",
          "202:     } else {",
          "203:       ctx.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DecimalExpressionSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DecimalExpressionSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DecimalExpressionSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/DecimalExpressionSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.catalyst.expressions",
          "20: import org.apache.spark.SparkFunSuite",
          "21: import org.apache.spark.sql.internal.SQLConf",
          "22: import org.apache.spark.sql.types.{Decimal, DecimalType, LongType}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import org.apache.spark.sql.catalyst.trees.CurrentOrigin.withOrigin",
          "22: import org.apache.spark.sql.catalyst.trees.Origin",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "83:     checkEvaluation(CheckOverflow(",
          "84:       Literal.create(null, DecimalType(2, 1)), DecimalType(3, 2), false), null)",
          "85:   }",
          "86: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "89:   test(\"SPARK-39208: CheckOverflow & CheckOverflowInSum support query context in runtime errors\") {",
          "90:     val d = Decimal(101, 3, 1)",
          "91:     val query = \"select cast(d as decimal(4, 3)) from t\"",
          "92:     val origin = Origin(",
          "93:       startIndex = Some(7),",
          "94:       stopIndex = Some(30),",
          "95:       sqlText = Some(query))",
          "97:     val expr1 = withOrigin(origin) {",
          "98:       CheckOverflow(Literal(d), DecimalType(4, 3), false)",
          "99:     }",
          "100:     checkExceptionInExpression[ArithmeticException](expr1, query)",
          "102:     val expr2 = CheckOverflowInSum(Literal(d), DecimalType(4, 3), false, queryContext = query)",
          "103:     checkExceptionInExpression[ArithmeticException](expr2, query)",
          "104:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "4423:     }",
          "4424:   }",
          "4428:     withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> \"false\",",
          "4429:       SQLConf.ANSI_ENABLED.key -> \"true\") {",
          "4430:       withTable(\"t\") {",
          "4431:         sql(\"create table t(d decimal(38, 0)) using parquet\")",
          "4438:       }",
          "4439:     }",
          "4440:   }",
          "",
          "[Removed Lines]",
          "4426:   test(\"SPARK-39190: Query context of decimal overflow error should be serialized to executors\" +",
          "4427:     \" when WSCG is off\") {",
          "4432:         sql(\"insert into t values (2e37BD)\")",
          "4433:         val query = \"select d / 0.1 from t\"",
          "4434:         val msg = intercept[SparkException] {",
          "4435:           sql(query).collect()",
          "4436:         }.getMessage",
          "4437:         assert(msg.contains(query))",
          "",
          "[Added Lines]",
          "4426:   test(\"SPARK-39190, SPARK-39208: Query context of decimal overflow error should be serialized \" +",
          "4427:     \"to executors when WSCG is off\") {",
          "4432:         sql(\"insert into t values (6e37BD),(6e37BD)\")",
          "4433:         Seq(",
          "4434:           \"select d / 0.1 from t\",",
          "4435:           \"select sum(d) from t\").foreach { query =>",
          "4436:           val msg = intercept[SparkException] {",
          "4437:             sql(query).collect()",
          "4438:           }.getMessage",
          "4439:           assert(msg.contains(query))",
          "4440:         }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0f13606bb55087da657b87d0c2f5a5583ed75e6c",
      "candidate_info": {
        "commit_hash": "0f13606bb55087da657b87d0c2f5a5583ed75e6c",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/0f13606bb55087da657b87d0c2f5a5583ed75e6c",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/CommandResultExec.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/QueryExecutionSuite.scala"
        ],
        "message": "[SPARK-35378][SQL][FOLLOW-UP] Fix incorrect return type in CommandResultExec.executeCollect()\n\n### What changes were proposed in this pull request?\n\nThis PR is a follow-up for https://github.com/apache/spark/pull/32513 and fixes an issue introduced by that patch.\n\nCommandResultExec is supposed to return `UnsafeRow` records in all of the `executeXYZ` methods but `executeCollect` was left out which causes issues like this one:\n```\nError in SQL statement: ClassCastException: org.apache.spark.sql.catalyst.expressions.GenericInternalRow\ncannot be cast to org.apache.spark.sql.catalyst.expressions.UnsafeRow\n```\n\nWe need to return `unsafeRows` instead of `rows` in `executeCollect` similar to other methods in the class.\n\n### Why are the changes needed?\n\nFixes a bug in CommandResultExec.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nI added a unit test to check the return type of all commands.\n\nCloses #36632 from sadikovi/fix-command-exec.\n\nAuthored-by: Ivan Sadikov <ivan.sadikov@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit a0decfc7db68c464e3ba2c2fb0b79a8b0c464684)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/CommandResultExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/CommandResultExec.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/QueryExecutionSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/QueryExecutionSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/CommandResultExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/CommandResultExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/CommandResultExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/CommandResultExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "76:   }",
          "78:   override def executeCollect(): Array[InternalRow] = {",
          "81:   }",
          "83:   override def executeTake(limit: Int): Array[InternalRow] = {",
          "",
          "[Removed Lines]",
          "79:     longMetric(\"numOutputRows\").add(rows.size)",
          "80:     rows.toArray",
          "",
          "[Added Lines]",
          "79:     longMetric(\"numOutputRows\").add(unsafeRows.size)",
          "80:     unsafeRows",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/QueryExecutionSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/QueryExecutionSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/QueryExecutionSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/QueryExecutionSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import org.apache.spark.sql.{AnalysisException, FastOperator}",
          "22: import org.apache.spark.sql.catalyst.analysis.UnresolvedNamespace",
          "23: import org.apache.spark.sql.catalyst.plans.QueryPlan",
          "24: import org.apache.spark.sql.catalyst.plans.logical.{CommandResult, LogicalPlan, OneRowRelation, Project, ShowTables, SubqueryAlias}",
          "25: import org.apache.spark.sql.catalyst.trees.TreeNodeTag",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "23: import org.apache.spark.sql.catalyst.expressions.UnsafeRow",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "262:     assert(cmdResultExec.commandPhysicalPlan.isInstanceOf[ShowTablesExec])",
          "263:   }",
          "265:   test(\"SPARK-38198: check specify maxFields when call toFile method\") {",
          "266:     withTempDir { dir =>",
          "267:       val path = dir.getCanonicalPath + \"/plans.txt\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "266:   test(\"SPARK-35378: Return UnsafeRow in CommandResultExecCheck execute methods\") {",
          "267:     val plan = spark.sql(\"SHOW FUNCTIONS\").queryExecution.executedPlan",
          "268:     assert(plan.isInstanceOf[CommandResultExec])",
          "269:     plan.executeCollect().foreach { row => assert(row.isInstanceOf[UnsafeRow]) }",
          "270:     plan.executeTake(10).foreach { row => assert(row.isInstanceOf[UnsafeRow]) }",
          "271:     plan.executeTail(10).foreach { row => assert(row.isInstanceOf[UnsafeRow]) }",
          "272:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}