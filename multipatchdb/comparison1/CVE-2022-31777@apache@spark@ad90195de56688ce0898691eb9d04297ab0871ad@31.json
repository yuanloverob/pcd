{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "dd192b7274ae725d018ba26e7a1fdb7f7abe34b8",
      "candidate_info": {
        "commit_hash": "dd192b7274ae725d018ba26e7a1fdb7f7abe34b8",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/dd192b7274ae725d018ba26e7a1fdb7f7abe34b8",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala"
        ],
        "message": "[SPARK-34079][SQL][FOLLOW-UP] Remove debug logging\n\n### What changes were proposed in this pull request?\nTo remove debug logging accidentally left in code after https://github.com/apache/spark/pull/32298.\n\n### Why are the changes needed?\nNo need for that logging.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nExisting tests.\n\nCloses #36354 from peter-toth/SPARK-34079-multi-column-scalar-subquery-follow-up.\n\nAuthored-by: Peter Toth <peter.toth@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit f24c9b0d135ce7ef4f219ab661a6b665663039f0)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "269:   def getNumBloomFilters(plan: LogicalPlan, scalarSubqueryCTEMultiplicator: Int = 1): Integer = {",
          "271:     val numBloomFilterAggs = plan.collectWithSubqueries {",
          "272:       case Aggregate(_, aggregateExpressions, _) =>",
          "273:         aggregateExpressions.collect {",
          "",
          "[Removed Lines]",
          "270:     print(plan)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "66faaa58ea5f69efc0fdbb5e3aa3a2d9703377b5",
      "candidate_info": {
        "commit_hash": "66faaa58ea5f69efc0fdbb5e3aa3a2d9703377b5",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/66faaa58ea5f69efc0fdbb5e3aa3a2d9703377b5",
        "files": [
          "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala"
        ],
        "message": "[SPARK-39965][K8S] Skip PVC cleanup when driver doesn't own PVCs\n\n### What changes were proposed in this pull request?\n\nThis PR aims to skip PVC cleanup logic when `spark.kubernetes.driver.ownPersistentVolumeClaim=false`.\n\n### Why are the changes needed?\n\nTo simplify Spark termination log by removing unnecessary log containing Exception message when Spark jobs have no PVC permission and at the same time `spark.kubernetes.driver.ownPersistentVolumeClaim` is `false`.\n\n### Does this PR introduce _any_ user-facing change?\n\nOnly in the termination logs of Spark jobs that has no PVC permission.\n\n### How was this patch tested?\n\nManually.\n\nCloses #37433 from dongjoon-hyun/SPARK-39965.\n\nLead-authored-by: Dongjoon Hyun <dongjoon@apache.org>\nCo-authored-by: pralabhkumar <pralabhkumar@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 87b312a9c9273535e22168c3da73834c22e1fbbb)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala||resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala||resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala": [
          "File: resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala -> resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "141:       }",
          "142:     }",
          "149:     }",
          "151:     if (shouldDeleteExecutors) {",
          "",
          "[Removed Lines]",
          "144:     Utils.tryLogNonFatalError {",
          "145:       kubernetesClient",
          "146:         .persistentVolumeClaims()",
          "147:         .withLabel(SPARK_APP_ID_LABEL, applicationId())",
          "148:         .delete()",
          "",
          "[Added Lines]",
          "144:     if (conf.get(KUBERNETES_DRIVER_OWN_PVC)) {",
          "145:       Utils.tryLogNonFatalError {",
          "146:         kubernetesClient",
          "147:           .persistentVolumeClaims()",
          "148:           .withLabel(SPARK_APP_ID_LABEL, applicationId())",
          "149:           .delete()",
          "150:       }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "feba33585c61fb06bbe801b86228301f2a143d9c",
      "candidate_info": {
        "commit_hash": "feba33585c61fb06bbe801b86228301f2a143d9c",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/feba33585c61fb06bbe801b86228301f2a143d9c",
        "files": [
          "core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala"
        ],
        "message": "[SPARK-39083][CORE] Fix race condition between update and clean app data\n\n### What changes were proposed in this pull request?\nmake `cleanAppData` atomic to prevent race condition between update and clean app data.\nWhen the race condition happens, it could lead to a scenario when `cleanAppData` delete the entry of\nApplicationInfoWrapper for an application right after it has been updated by `mergeApplicationListing`.\nSo there will be cases when the HS Web UI displays `Application not found` for applications whose logs does exist.\n\n#### Error message\n```\n22/04/29 17:16:21 DEBUG FsHistoryProvider: New/updated attempts found: 1 ArrayBuffer(viewfs://iu/log/spark3/application_1651119726430_138107_1)\n22/04/29 17:16:21 INFO FsHistoryProvider: Parsing viewfs://iu/log/spark3/application_1651119726430_138107_1 for listing data...\n22/04/29 17:16:21 INFO FsHistoryProvider: Looking for end event; skipping 10805037 bytes from viewfs://iu/log/spark3/application_1651119726430_138107_1...\n22/04/29 17:16:21 INFO FsHistoryProvider: Finished parsing viewfs://iu/log/spark3/application_1651119726430_138107_1\n22/04/29 17:16:21 ERROR Utils: Uncaught exception in thread log-replay-executor-7\njava.util.NoSuchElementException\n        at org.apache.spark.util.kvstore.InMemoryStore.read(InMemoryStore.java:85)\n        at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$checkAndCleanLog$3(FsHistoryProvider.scala:927)\n        at scala.Option.foreach(Option.scala:407)\n        at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$checkAndCleanLog$1(FsHistoryProvider.scala:926)\n        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n        at org.apache.spark.util.Utils$.tryLog(Utils.scala:2032)\n        at org.apache.spark.deploy.history.FsHistoryProvider.checkAndCleanLog(FsHistoryProvider.scala:916)\n        at org.apache.spark.deploy.history.FsHistoryProvider.mergeApplicationListing(FsHistoryProvider.scala:712)\n        at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$checkForLogs$15(FsHistoryProvider.scala:576)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n        at java.lang.Thread.run(Thread.java:748)\n```\n#### Background\nCurrently, the HS runs the `checkForLogs` to build the application list based on the current contents of the log directory for every 10 seconds by default.\n- https://github.com/apache/spark/blob/v3.2.1/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#L296-L299\n- https://github.com/apache/spark/blob/v3.2.1/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#L472\n\nIn each turn of execution, this method scans the specified logDir and parse the log files to update its KVStore:\n- detect new updated/added files to process : https://github.com/apache/spark/blob/v3.2.1/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#L574-L578\n- detect stale data to remove: https://github.com/apache/spark/blob/v3.2.1/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#L586-L600\n\nThese 2 operations are executed in different threads as `submitLogProcessTask` uses `replayExecutor` to submit tasks.\nhttps://github.com/apache/spark/blob/v3.2.1/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#L1389-L1401\n\n### When does the bug happen?\n`Application not found` error happens in the following scenario:\nIn the first run of `checkForLogs`, it detected a newly-added log `viewfs://iu/log/spark3/AAA_1.inprogress` (log of an in-progress application named AAA). So it will add 2 entries to the KVStore\n- one entry of key-value as the key is the logPath (`viewfs://iu/log/spark3/AAA_1.inprogress`)  and the value is an instance of LogInfo represented the log\n  -  https://github.com/apache/spark/blob/v3.2.1/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#L495-L505\n  - https://github.com/apache/spark/blob/v3.2.1/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#L545-L552\n- one entry of key-value as the key is the applicationId (`AAA`) and  the value is an instance of ApplicationInfoWrapper holding the information of the application.\n  - https://github.com/apache/spark/blob/v3.2.1/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#L825\n  - https://github.com/apache/spark/blob/v3.2.1/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#L1172\n\nIn the next run of `checkForLogs`, now the AAA application has finished, the log `viewfs://iu/log/spark3/AAA_1.inprogress` has been deleted and a new log `viewfs://iu/log/spark3/AAA_1` is created. So  `checkForLogs` will do the following 2 things in 2 different threads:\n- Thread 1: parsing the new log `viewfs://iu/log/spark3/AAA_1` and update data in its KVStore\n  - add a new entry of key: `viewfs://iu/log/spark3/AAA_1` and value: an instance of LogInfo represented the log\n  - updated the entry with key=applicationId (`AAA`)  with new value of  an instance of ApplicationInfoWrapper (for example: the isCompleted flag now change from false to true)\n- Thread 2:  data related to `viewfs://iu/log/spark3/AAA_1.inprogress` is now considered as stale and it must be deleted.\n  - clean App data for `viewfs://iu/log/spark3/AAA_1.inprogress` https://github.com/apache/spark/blob/v3.2.1/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#L586-L600\n  - Inside `cleanAppData`, first it loads the latest information of `ApplicationInfoWrapper` from the KVStore: https://github.com/apache/spark/blob/v3.2.1/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#L632  For most of the time, when this line is executed, Thread 1 already finished `updating the entry with key=applicationId (AAA)  with new value of  an instance of ApplicationInfoWrapper` so this condition https://github.com/apache/spark/blob/v3.2.1/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#L637 will be evaluated as false, so `isStale` will be false. However, in some rare cases, when Thread1 does not finish the update yet, the old data of ApplicationInfoWrapper will be load, so `isStale` will be true and it leads to deleting the entry of ApplicationInfoWrapper in KVStore: https://github.com/apache/spark/blob/v3.2.1/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#L656-L662 and the worst thing is it delete the entry right after when Thread 1 has finished updating the entry with key=applicationId (`AAA`)  with new value of  an instance of ApplicationInfoWrapper. So the entry for the ApplicationInfoWrapper of applicationId= `AAA` is removed forever then when users access the Web UI for this application, and `Application not found` is shown up while the log for the app does exist.\n\nSo here we make the `cleanAppData` method atomic just like the `addListing` method https://github.com/apache/spark/blob/v3.2.1/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#L1172 so that\n- If Thread 1 gets the lock on the `listing` before Thread 2, it will update the entry for the application, so in Thread2 `isStale` will be false, the entry for the application will not be removed from KVStore\n- If Thread 2 gets the lock on the `listing` before Thread 1, then `isStale` will be true, the entry for the application will be removed from KVStore but after that it will be added again by Thread 1.\nIn both case, the entry for the application will not be deleted unexpectedly from KVStore.\n\n### Why are the changes needed?\nFix the bug causing HS Web UI to display `Application not found` for applications whose logs does exist.\n\n### Does this PR introduce _any_ user-facing change?\nYes, bug fix.\n\n## How was this patch tested?\nManual test.\nDeployed in our Spark HS and the `java.util.NoSuchElementException` exception does not happen anymore.\n`Application not found` error does not happen anymore.\n\nCloses #36424 from tanvn/SPARK-39083.\n\nAuthored-by: tan.vu <tan.vu@linecorp.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n(cherry picked from commit 29643265a9f5e8142d20add5350c614a55161451)\nSigned-off-by: Sean Owen <srowen@gmail.com>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala||core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala||core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala": [
          "File: core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala -> core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "638:   private def cleanAppData(appId: String, attemptId: Option[String], logPath: String): Unit = {",
          "639:     try {",
          "657:           }",
          "661:         }",
          "662:       }",
          "664:       if (isStale) {",
          "670:         }",
          "671:       }",
          "672:     } catch {",
          "673:       case _: NoSuchElementException =>",
          "",
          "[Removed Lines]",
          "640:       val app = load(appId)",
          "641:       val (attempt, others) = app.attempts.partition(_.info.attemptId == attemptId)",
          "643:       assert(attempt.isEmpty || attempt.size == 1)",
          "644:       val isStale = attempt.headOption.exists { a =>",
          "645:         if (a.logPath != new Path(logPath).getName()) {",
          "648:           false",
          "649:         } else {",
          "650:           val maybeUI = synchronized {",
          "651:             activeUIs.remove(appId -> attemptId)",
          "652:           }",
          "654:           maybeUI.foreach { ui =>",
          "655:             ui.invalidate()",
          "656:             ui.ui.store.close()",
          "659:           diskManager.foreach(_.release(appId, attemptId, delete = true))",
          "660:           true",
          "665:         if (others.nonEmpty) {",
          "666:           val newAppInfo = new ApplicationInfoWrapper(app.info, others)",
          "667:           listing.write(newAppInfo)",
          "668:         } else {",
          "669:           listing.delete(classOf[ApplicationInfoWrapper], appId)",
          "",
          "[Added Lines]",
          "640:       var isStale = false",
          "641:       listing.synchronized {",
          "642:         val app = load(appId)",
          "643:         val (attempt, others) = app.attempts.partition(_.info.attemptId == attemptId)",
          "645:         assert(attempt.isEmpty || attempt.size == 1)",
          "646:         isStale = attempt.headOption.exists { a =>",
          "647:           if (a.logPath != new Path(logPath).getName()) {",
          "650:             false",
          "651:           } else {",
          "652:             if (others.nonEmpty) {",
          "653:               val newAppInfo = new ApplicationInfoWrapper(app.info, others)",
          "654:               listing.write(newAppInfo)",
          "655:             } else {",
          "656:               listing.delete(classOf[ApplicationInfoWrapper], appId)",
          "657:             }",
          "658:             true",
          "664:         val maybeUI = synchronized {",
          "665:           activeUIs.remove(appId -> attemptId)",
          "666:         }",
          "667:         maybeUI.foreach { ui =>",
          "668:           ui.invalidate()",
          "669:           ui.ui.store.close()",
          "671:         diskManager.foreach(_.release(appId, attemptId, delete = true))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cc5f074c4c91bc1666b20efa40cfedd65d4137fd",
      "candidate_info": {
        "commit_hash": "cc5f074c4c91bc1666b20efa40cfedd65d4137fd",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/cc5f074c4c91bc1666b20efa40cfedd65d4137fd",
        "files": [
          "dev/deps/spark-deps-hadoop-2-hive-2.3",
          "dev/deps/spark-deps-hadoop-3-hive-2.3",
          "pom.xml"
        ],
        "message": "[SPARK-38817][K8S][BUILD] Upgrade kubernetes-client to 5.12.2\n\n### What changes were proposed in this pull request?\nUpgrade kubernetes-client to 5.12.2:\nChanges list:\n- https://github.com/fabric8io/kubernetes-client/releases/tag/v5.12.2\n\nEspecially, `Supports Queue (cluster) API for Volcano extension` will help us setting queue capacity dynamically in K8s IT.\n\n### Why are the changes needed?\nThe next kubernetes client version will be 6.x with breaking changes: https://github.com/fabric8io/kubernetes-client/blob/master/CHANGELOG.md#note-breaking-changes-in-the-api .\n\nWe'd better to upgrade to latest 5.X first to reduce follow upgrade cost.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\n- CI\n- integration test\n\nCloses #36098 from Yikun/k8scli-5.12.2.\n\nAuthored-by: Yikun Jiang <yikunkero@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 5dae4bebea4e677e098fcf815389b1652dbcba52)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "dev/deps/spark-deps-hadoop-2-hive-2.3||dev/deps/spark-deps-hadoop-2-hive-2.3",
          "dev/deps/spark-deps-hadoop-3-hive-2.3||dev/deps/spark-deps-hadoop-3-hive-2.3"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/deps/spark-deps-hadoop-2-hive-2.3||dev/deps/spark-deps-hadoop-2-hive-2.3": [
          "File: dev/deps/spark-deps-hadoop-2-hive-2.3 -> dev/deps/spark-deps-hadoop-2-hive-2.3",
          "--- Hunk 1 ---",
          "[Context before]",
          "162: jta/1.1//jta-1.1.jar",
          "163: jul-to-slf4j/1.7.32//jul-to-slf4j-1.7.32.jar",
          "164: kryo-shaded/4.0.2//kryo-shaded-4.0.2.jar",
          "186: lapack/2.2.1//lapack-2.2.1.jar",
          "187: leveldbjni-all/1.8//leveldbjni-all-1.8.jar",
          "188: libfb303/0.9.3//libfb303-0.9.3.jar",
          "",
          "[Removed Lines]",
          "165: kubernetes-client/5.12.1//kubernetes-client-5.12.1.jar",
          "166: kubernetes-model-admissionregistration/5.12.1//kubernetes-model-admissionregistration-5.12.1.jar",
          "167: kubernetes-model-apiextensions/5.12.1//kubernetes-model-apiextensions-5.12.1.jar",
          "168: kubernetes-model-apps/5.12.1//kubernetes-model-apps-5.12.1.jar",
          "169: kubernetes-model-autoscaling/5.12.1//kubernetes-model-autoscaling-5.12.1.jar",
          "170: kubernetes-model-batch/5.12.1//kubernetes-model-batch-5.12.1.jar",
          "171: kubernetes-model-certificates/5.12.1//kubernetes-model-certificates-5.12.1.jar",
          "172: kubernetes-model-common/5.12.1//kubernetes-model-common-5.12.1.jar",
          "173: kubernetes-model-coordination/5.12.1//kubernetes-model-coordination-5.12.1.jar",
          "174: kubernetes-model-core/5.12.1//kubernetes-model-core-5.12.1.jar",
          "175: kubernetes-model-discovery/5.12.1//kubernetes-model-discovery-5.12.1.jar",
          "176: kubernetes-model-events/5.12.1//kubernetes-model-events-5.12.1.jar",
          "177: kubernetes-model-extensions/5.12.1//kubernetes-model-extensions-5.12.1.jar",
          "178: kubernetes-model-flowcontrol/5.12.1//kubernetes-model-flowcontrol-5.12.1.jar",
          "179: kubernetes-model-metrics/5.12.1//kubernetes-model-metrics-5.12.1.jar",
          "180: kubernetes-model-networking/5.12.1//kubernetes-model-networking-5.12.1.jar",
          "181: kubernetes-model-node/5.12.1//kubernetes-model-node-5.12.1.jar",
          "182: kubernetes-model-policy/5.12.1//kubernetes-model-policy-5.12.1.jar",
          "183: kubernetes-model-rbac/5.12.1//kubernetes-model-rbac-5.12.1.jar",
          "184: kubernetes-model-scheduling/5.12.1//kubernetes-model-scheduling-5.12.1.jar",
          "185: kubernetes-model-storageclass/5.12.1//kubernetes-model-storageclass-5.12.1.jar",
          "",
          "[Added Lines]",
          "165: kubernetes-client/5.12.2//kubernetes-client-5.12.2.jar",
          "166: kubernetes-model-admissionregistration/5.12.2//kubernetes-model-admissionregistration-5.12.2.jar",
          "167: kubernetes-model-apiextensions/5.12.2//kubernetes-model-apiextensions-5.12.2.jar",
          "168: kubernetes-model-apps/5.12.2//kubernetes-model-apps-5.12.2.jar",
          "169: kubernetes-model-autoscaling/5.12.2//kubernetes-model-autoscaling-5.12.2.jar",
          "170: kubernetes-model-batch/5.12.2//kubernetes-model-batch-5.12.2.jar",
          "171: kubernetes-model-certificates/5.12.2//kubernetes-model-certificates-5.12.2.jar",
          "172: kubernetes-model-common/5.12.2//kubernetes-model-common-5.12.2.jar",
          "173: kubernetes-model-coordination/5.12.2//kubernetes-model-coordination-5.12.2.jar",
          "174: kubernetes-model-core/5.12.2//kubernetes-model-core-5.12.2.jar",
          "175: kubernetes-model-discovery/5.12.2//kubernetes-model-discovery-5.12.2.jar",
          "176: kubernetes-model-events/5.12.2//kubernetes-model-events-5.12.2.jar",
          "177: kubernetes-model-extensions/5.12.2//kubernetes-model-extensions-5.12.2.jar",
          "178: kubernetes-model-flowcontrol/5.12.2//kubernetes-model-flowcontrol-5.12.2.jar",
          "179: kubernetes-model-metrics/5.12.2//kubernetes-model-metrics-5.12.2.jar",
          "180: kubernetes-model-networking/5.12.2//kubernetes-model-networking-5.12.2.jar",
          "181: kubernetes-model-node/5.12.2//kubernetes-model-node-5.12.2.jar",
          "182: kubernetes-model-policy/5.12.2//kubernetes-model-policy-5.12.2.jar",
          "183: kubernetes-model-rbac/5.12.2//kubernetes-model-rbac-5.12.2.jar",
          "184: kubernetes-model-scheduling/5.12.2//kubernetes-model-scheduling-5.12.2.jar",
          "185: kubernetes-model-storageclass/5.12.2//kubernetes-model-storageclass-5.12.2.jar",
          "",
          "---------------"
        ],
        "dev/deps/spark-deps-hadoop-3-hive-2.3||dev/deps/spark-deps-hadoop-3-hive-2.3": [
          "File: dev/deps/spark-deps-hadoop-3-hive-2.3 -> dev/deps/spark-deps-hadoop-3-hive-2.3",
          "--- Hunk 1 ---",
          "[Context before]",
          "148: jta/1.1//jta-1.1.jar",
          "149: jul-to-slf4j/1.7.32//jul-to-slf4j-1.7.32.jar",
          "150: kryo-shaded/4.0.2//kryo-shaded-4.0.2.jar",
          "172: lapack/2.2.1//lapack-2.2.1.jar",
          "173: leveldbjni-all/1.8//leveldbjni-all-1.8.jar",
          "174: libfb303/0.9.3//libfb303-0.9.3.jar",
          "",
          "[Removed Lines]",
          "151: kubernetes-client/5.12.1//kubernetes-client-5.12.1.jar",
          "152: kubernetes-model-admissionregistration/5.12.1//kubernetes-model-admissionregistration-5.12.1.jar",
          "153: kubernetes-model-apiextensions/5.12.1//kubernetes-model-apiextensions-5.12.1.jar",
          "154: kubernetes-model-apps/5.12.1//kubernetes-model-apps-5.12.1.jar",
          "155: kubernetes-model-autoscaling/5.12.1//kubernetes-model-autoscaling-5.12.1.jar",
          "156: kubernetes-model-batch/5.12.1//kubernetes-model-batch-5.12.1.jar",
          "157: kubernetes-model-certificates/5.12.1//kubernetes-model-certificates-5.12.1.jar",
          "158: kubernetes-model-common/5.12.1//kubernetes-model-common-5.12.1.jar",
          "159: kubernetes-model-coordination/5.12.1//kubernetes-model-coordination-5.12.1.jar",
          "160: kubernetes-model-core/5.12.1//kubernetes-model-core-5.12.1.jar",
          "161: kubernetes-model-discovery/5.12.1//kubernetes-model-discovery-5.12.1.jar",
          "162: kubernetes-model-events/5.12.1//kubernetes-model-events-5.12.1.jar",
          "163: kubernetes-model-extensions/5.12.1//kubernetes-model-extensions-5.12.1.jar",
          "164: kubernetes-model-flowcontrol/5.12.1//kubernetes-model-flowcontrol-5.12.1.jar",
          "165: kubernetes-model-metrics/5.12.1//kubernetes-model-metrics-5.12.1.jar",
          "166: kubernetes-model-networking/5.12.1//kubernetes-model-networking-5.12.1.jar",
          "167: kubernetes-model-node/5.12.1//kubernetes-model-node-5.12.1.jar",
          "168: kubernetes-model-policy/5.12.1//kubernetes-model-policy-5.12.1.jar",
          "169: kubernetes-model-rbac/5.12.1//kubernetes-model-rbac-5.12.1.jar",
          "170: kubernetes-model-scheduling/5.12.1//kubernetes-model-scheduling-5.12.1.jar",
          "171: kubernetes-model-storageclass/5.12.1//kubernetes-model-storageclass-5.12.1.jar",
          "",
          "[Added Lines]",
          "151: kubernetes-client/5.12.2//kubernetes-client-5.12.2.jar",
          "152: kubernetes-model-admissionregistration/5.12.2//kubernetes-model-admissionregistration-5.12.2.jar",
          "153: kubernetes-model-apiextensions/5.12.2//kubernetes-model-apiextensions-5.12.2.jar",
          "154: kubernetes-model-apps/5.12.2//kubernetes-model-apps-5.12.2.jar",
          "155: kubernetes-model-autoscaling/5.12.2//kubernetes-model-autoscaling-5.12.2.jar",
          "156: kubernetes-model-batch/5.12.2//kubernetes-model-batch-5.12.2.jar",
          "157: kubernetes-model-certificates/5.12.2//kubernetes-model-certificates-5.12.2.jar",
          "158: kubernetes-model-common/5.12.2//kubernetes-model-common-5.12.2.jar",
          "159: kubernetes-model-coordination/5.12.2//kubernetes-model-coordination-5.12.2.jar",
          "160: kubernetes-model-core/5.12.2//kubernetes-model-core-5.12.2.jar",
          "161: kubernetes-model-discovery/5.12.2//kubernetes-model-discovery-5.12.2.jar",
          "162: kubernetes-model-events/5.12.2//kubernetes-model-events-5.12.2.jar",
          "163: kubernetes-model-extensions/5.12.2//kubernetes-model-extensions-5.12.2.jar",
          "164: kubernetes-model-flowcontrol/5.12.2//kubernetes-model-flowcontrol-5.12.2.jar",
          "165: kubernetes-model-metrics/5.12.2//kubernetes-model-metrics-5.12.2.jar",
          "166: kubernetes-model-networking/5.12.2//kubernetes-model-networking-5.12.2.jar",
          "167: kubernetes-model-node/5.12.2//kubernetes-model-node-5.12.2.jar",
          "168: kubernetes-model-policy/5.12.2//kubernetes-model-policy-5.12.2.jar",
          "169: kubernetes-model-rbac/5.12.2//kubernetes-model-rbac-5.12.2.jar",
          "170: kubernetes-model-scheduling/5.12.2//kubernetes-model-scheduling-5.12.2.jar",
          "171: kubernetes-model-storageclass/5.12.2//kubernetes-model-storageclass-5.12.2.jar",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "433469f284ee24150f6cff4005d39a70e91cc4d9",
      "candidate_info": {
        "commit_hash": "433469f284ee24150f6cff4005d39a70e91cc4d9",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/433469f284ee24150f6cff4005d39a70e91cc4d9",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/unresolved.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/package.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/package.scala",
          "sql/core/src/test/resources/sql-tests/inputs/using-join.sql",
          "sql/core/src/test/resources/sql-tests/results/using-join.sql.out",
          "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/connector/MetadataColumnSuite.scala"
        ],
        "message": "[SPARK-40149][SQL] Propagate metadata columns through Project\n\nThis PR fixes a regression caused by https://github.com/apache/spark/pull/32017 .\n\nIn https://github.com/apache/spark/pull/32017 , we tried to be more conservative and decided to not propagate metadata columns in certain operators, including `Project`. However, the decision was made only considering SQL API, not DataFrame API. In fact, it's very common to chain `Project` operators in DataFrame, e.g. `df.withColumn(...).withColumn(...)...`, and it's very inconvenient if metadata columns are not propagated through `Project`.\n\nThis PR makes 2 changes:\n1. Project should propagate metadata columns\n2. SubqueryAlias should only propagate metadata columns if the child is a leaf node or also a SubqueryAlias\n\nThe second change is needed to still forbid weird queries like `SELECT m from (SELECT a from t)`, which is the main motivation of https://github.com/apache/spark/pull/32017 .\n\nAfter propagating metadata columns, a problem from https://github.com/apache/spark/pull/31666 is exposed: the natural join metadata columns may confuse the analyzer and lead to wrong analyzed plan. For example, `SELECT t1.value FROM t1 LEFT JOIN t2 USING (key) ORDER BY key`, how shall we resolve `ORDER BY key`? It should be resolved to `t1.key` via the rule `ResolveMissingReferences`, which is in the output of the left join. However, if `Project` can propagate metadata columns, `ORDER BY key` will be resolved to `t2.key`.\n\nTo solve this problem, this PR only allows qualified access for metadata columns of natural join. This has no breaking change, as people can only do qualified access for natural join metadata columns before, in the `Project` right after `Join`. This actually enables more use cases, as people can now access natural join metadata columns in ORDER BY. I've added a test for it.\n\nfix a regression\n\nFor SQL API, there is no change, as a `SubqueryAlias` always comes with a `Project` or `Aggregate`, so we still don't propagate metadata columns through a SELECT group.\n\nFor DataFrame API, the behavior becomes more lenient. The only breaking case is an operator that can propagate metadata columns then follows a `SubqueryAlias`, e.g. `df.filter(...).as(\"t\").select(\"t.metadata_col\")`. But this is a weird use case and I don't think we should support it at the first place.\n\nnew tests\n\nCloses #37758 from cloud-fan/metadata.\n\nAuthored-by: Wenchen Fan <wenchen@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 99ae1d9a897909990881f14c5ea70a0d1a0bf456)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/unresolved.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/unresolved.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/package.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/package.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/package.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/package.scala",
          "sql/core/src/test/resources/sql-tests/inputs/using-join.sql||sql/core/src/test/resources/sql-tests/inputs/using-join.sql",
          "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/connector/MetadataColumnSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/connector/MetadataColumnSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "967:     private def addMetadataCol(plan: LogicalPlan): LogicalPlan = plan match {",
          "968:       case s: ExposesMetadataColumns => s.withMetadataColumns()",
          "969:       case p: Project =>",
          "971:           projectList = p.metadataOutput ++ p.projectList,",
          "972:           child = addMetadataCol(p.child))",
          "973:       case _ => plan.withNewChildren(plan.children.map(addMetadataCol))",
          "974:     }",
          "975:   }",
          "",
          "[Removed Lines]",
          "970:         p.copy(",
          "",
          "[Added Lines]",
          "970:         val newProj = p.copy(",
          "973:         newProj.copyTagsFrom(p)",
          "974:         newProj",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3475:     val project = Project(projectList, Join(left, right, joinType, newCondition, hint))",
          "3476:     project.setTagValue(",
          "3477:       Project.hiddenOutputTag,",
          "3480:     project",
          "3481:   }",
          "",
          "[Removed Lines]",
          "3478:       hiddenList.map(_.markAsSupportsQualifiedStar()) ++",
          "3479:         project.child.metadataOutput.filter(_.supportsQualifiedStar))",
          "",
          "[Added Lines]",
          "3480:       hiddenList.map(_.markAsQualifiedAccessOnly()) ++",
          "3481:         project.child.metadataOutput.filter(_.qualifiedAccessOnly))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/unresolved.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/unresolved.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/unresolved.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/unresolved.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "363:     if (target.isEmpty) return input.output",
          "367:     val expandedAttributes = (hiddenOutput ++ input.output).filter(",
          "368:       matchedQualifier(_, target.get, resolver))",
          "",
          "[Removed Lines]",
          "366:     val hiddenOutput = input.metadataOutput.filter(_.supportsQualifiedStar)",
          "",
          "[Added Lines]",
          "366:     val hiddenOutput = input.metadataOutput.filter(_.qualifiedAccessOnly)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/package.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/package.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/package.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/package.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import org.apache.spark.sql.AnalysisException",
          "25: import org.apache.spark.sql.catalyst.analysis.{Resolver, UnresolvedAttribute}",
          "26: import org.apache.spark.sql.types.{StructField, StructType}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: import org.apache.spark.sql.catalyst.util.MetadataColumnHelper",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "265:         case (Seq(), _) =>",
          "266:           val name = nameParts.head",
          "267:           val attributes = collectMatches(name, direct.get(name.toLowerCase(Locale.ROOT)))",
          "269:         case _ => matches",
          "270:       }",
          "271:     }",
          "",
          "[Removed Lines]",
          "268:           (attributes, nameParts.tail)",
          "",
          "[Added Lines]",
          "269:           (attributes.filterNot(_.qualifiedAccessOnly), nameParts.tail)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "314:       var i = nameParts.length - 1",
          "315:       while (i >= 0 && candidates.isEmpty) {",
          "316:         val name = nameParts(i)",
          "321:         if (candidates.nonEmpty) {",
          "322:           nestedFields = nameParts.takeRight(nameParts.length - i - 1)",
          "323:         }",
          "",
          "[Removed Lines]",
          "317:         candidates = collectMatches(",
          "318:           name,",
          "319:           nameParts.take(i),",
          "320:           direct.get(name.toLowerCase(Locale.ROOT)))",
          "",
          "[Added Lines]",
          "318:         val attrsToLookup = if (i == 0) {",
          "319:           direct.get(name.toLowerCase(Locale.ROOT)).map(_.filterNot(_.qualifiedAccessOnly))",
          "320:         } else {",
          "321:           direct.get(name.toLowerCase(Locale.ROOT))",
          "322:         }",
          "323:         candidates = collectMatches(name, nameParts.take(i), attrsToLookup)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "88:     getAllValidConstraints(projectList)",
          "90:   override def metadataOutput: Seq[Attribute] =",
          "93:   override protected def withNewChildInternal(newChild: LogicalPlan): Project =",
          "94:     copy(child = newChild)",
          "",
          "[Removed Lines]",
          "91:     getTagValue(Project.hiddenOutputTag).getOrElse(Nil)",
          "",
          "[Added Lines]",
          "91:     getTagValue(Project.hiddenOutputTag).getOrElse(child.metadataOutput)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1332:   }",
          "1334:   override def metadataOutput: Seq[Attribute] = {",
          "1338:   }",
          "1340:   override def maxRows: Option[Long] = child.maxRows",
          "",
          "[Removed Lines]",
          "1335:     val qualifierList = identifier.qualifier :+ alias",
          "1336:     val nonHiddenMetadataOutput = child.metadataOutput.filter(!_.supportsQualifiedStar)",
          "1337:     nonHiddenMetadataOutput.map(_.withQualifier(qualifierList))",
          "",
          "[Added Lines]",
          "1336:     if (child.isInstanceOf[LeafNode] || child.isInstanceOf[SubqueryAlias]) {",
          "1337:       val qualifierList = identifier.qualifier :+ alias",
          "1338:       val nonHiddenMetadataOutput = child.metadataOutput.filter(!_.qualifiedAccessOnly)",
          "1339:       nonHiddenMetadataOutput.map(_.withQualifier(qualifierList))",
          "1340:     } else {",
          "1341:       Nil",
          "1342:     }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/package.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/package.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/package.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/package.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "190:   implicit class MetadataColumnHelper(attr: Attribute) {",
          "196:     def isMetadataCol: Boolean = attr.metadata.contains(METADATA_COL_ATTR_KEY) &&",
          "197:       attr.metadata.getBoolean(METADATA_COL_ATTR_KEY)",
          "204:       new MetadataBuilder()",
          "205:         .withMetadata(attr.metadata)",
          "206:         .putBoolean(METADATA_COL_ATTR_KEY, true)",
          "208:         .build()",
          "209:     )",
          "210:   }",
          "",
          "[Removed Lines]",
          "194:     val SUPPORTS_QUALIFIED_STAR = \"__supports_qualified_star\"",
          "199:     def supportsQualifiedStar: Boolean = attr.isMetadataCol &&",
          "200:       attr.metadata.contains(SUPPORTS_QUALIFIED_STAR) &&",
          "201:       attr.metadata.getBoolean(SUPPORTS_QUALIFIED_STAR)",
          "203:     def markAsSupportsQualifiedStar(): Attribute = attr.withMetadata(",
          "207:         .putBoolean(SUPPORTS_QUALIFIED_STAR, true)",
          "",
          "[Added Lines]",
          "195:     val QUALIFIED_ACCESS_ONLY = \"__qualified_access_only\"",
          "200:     def qualifiedAccessOnly: Boolean = attr.isMetadataCol &&",
          "201:       attr.metadata.contains(QUALIFIED_ACCESS_ONLY) &&",
          "202:       attr.metadata.getBoolean(QUALIFIED_ACCESS_ONLY)",
          "204:     def markAsQualifiedAccessOnly(): Attribute = attr.withMetadata(",
          "208:         .putBoolean(QUALIFIED_ACCESS_ONLY, true)",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/using-join.sql||sql/core/src/test/resources/sql-tests/inputs/using-join.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/using-join.sql -> sql/core/src/test/resources/sql-tests/inputs/using-join.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: SELECT nt1.k, nt2.k FROM nt1 left outer join nt2 using (k);",
          "22: SELECT k, nt1.k FROM nt1 left outer join nt2 using (k);",
          "24: SELECT k, nt2.k FROM nt1 left outer join nt2 using (k);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: SELECT nt1.k, nt2.k FROM nt1 left outer join nt2 using (k) ORDER BY nt2.k;",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2204:     }",
          "2205:   }",
          "2301:   test(\"SPARK-33505: insert into partitioned table\") {",
          "2302:     val t = \"testpart.ns1.ns2.tbl\"",
          "2303:     withTable(t) {",
          "",
          "[Removed Lines]",
          "2207:   test(\"SPARK-31255: Project a metadata column\") {",
          "2208:     val t1 = s\"${catalogAndNamespace}table\"",
          "2209:     withTable(t1) {",
          "2210:       sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format \" +",
          "2211:           \"PARTITIONED BY (bucket(4, id), id)\")",
          "2212:       sql(s\"INSERT INTO $t1 VALUES (1, 'a'), (2, 'b'), (3, 'c')\")",
          "2214:       val sqlQuery = spark.sql(s\"SELECT id, data, index, _partition FROM $t1\")",
          "2215:       val dfQuery = spark.table(t1).select(\"id\", \"data\", \"index\", \"_partition\")",
          "2217:       Seq(sqlQuery, dfQuery).foreach { query =>",
          "2218:         checkAnswer(query, Seq(Row(1, \"a\", 0, \"3/1\"), Row(2, \"b\", 0, \"0/2\"), Row(3, \"c\", 0, \"1/3\")))",
          "2219:       }",
          "2220:     }",
          "2221:   }",
          "2223:   test(\"SPARK-31255: Projects data column when metadata column has the same name\") {",
          "2224:     val t1 = s\"${catalogAndNamespace}table\"",
          "2225:     withTable(t1) {",
          "2226:       sql(s\"CREATE TABLE $t1 (index bigint, data string) USING $v2Format \" +",
          "2227:           \"PARTITIONED BY (bucket(4, index), index)\")",
          "2228:       sql(s\"INSERT INTO $t1 VALUES (3, 'c'), (2, 'b'), (1, 'a')\")",
          "2230:       val sqlQuery = spark.sql(s\"SELECT index, data, _partition FROM $t1\")",
          "2231:       val dfQuery = spark.table(t1).select(\"index\", \"data\", \"_partition\")",
          "2233:       Seq(sqlQuery, dfQuery).foreach { query =>",
          "2234:         checkAnswer(query, Seq(Row(3, \"c\", \"1/3\"), Row(2, \"b\", \"0/2\"), Row(1, \"a\", \"3/1\")))",
          "2235:       }",
          "2236:     }",
          "2237:   }",
          "2239:   test(\"SPARK-31255: * expansion does not include metadata columns\") {",
          "2240:     val t1 = s\"${catalogAndNamespace}table\"",
          "2241:     withTable(t1) {",
          "2242:       sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format \" +",
          "2243:           \"PARTITIONED BY (bucket(4, id), id)\")",
          "2244:       sql(s\"INSERT INTO $t1 VALUES (3, 'c'), (2, 'b'), (1, 'a')\")",
          "2246:       val sqlQuery = spark.sql(s\"SELECT * FROM $t1\")",
          "2247:       val dfQuery = spark.table(t1)",
          "2249:       Seq(sqlQuery, dfQuery).foreach { query =>",
          "2250:         checkAnswer(query, Seq(Row(3, \"c\"), Row(2, \"b\"), Row(1, \"a\")))",
          "2251:       }",
          "2252:     }",
          "2253:   }",
          "2255:   test(\"SPARK-31255: metadata column should only be produced when necessary\") {",
          "2256:     val t1 = s\"${catalogAndNamespace}table\"",
          "2257:     withTable(t1) {",
          "2258:       sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format \" +",
          "2259:         \"PARTITIONED BY (bucket(4, id), id)\")",
          "2261:       val sqlQuery = spark.sql(s\"SELECT * FROM $t1 WHERE index = 0\")",
          "2262:       val dfQuery = spark.table(t1).filter(\"index = 0\")",
          "2264:       Seq(sqlQuery, dfQuery).foreach { query =>",
          "2265:         assert(query.schema.fieldNames.toSeq == Seq(\"id\", \"data\"))",
          "2266:       }",
          "2267:     }",
          "2268:   }",
          "2270:   test(\"SPARK-34547: metadata columns are resolved last\") {",
          "2271:     val t1 = s\"${catalogAndNamespace}tableOne\"",
          "2272:     val t2 = \"t2\"",
          "2273:     withTable(t1) {",
          "2274:       sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format \" +",
          "2275:         \"PARTITIONED BY (bucket(4, id), id)\")",
          "2276:       sql(s\"INSERT INTO $t1 VALUES (1, 'a'), (2, 'b'), (3, 'c')\")",
          "2277:       withTempView(t2) {",
          "2278:         sql(s\"CREATE TEMPORARY VIEW $t2 AS SELECT * FROM \" +",
          "2279:           s\"VALUES (1, -1), (2, -2), (3, -3) AS $t2(id, index)\")",
          "2281:         val sqlQuery = spark.sql(s\"SELECT $t1.id, $t2.id, data, index, $t1.index, $t2.index FROM \" +",
          "2282:           s\"$t1 JOIN $t2 WHERE $t1.id = $t2.id\")",
          "2283:         val t1Table = spark.table(t1)",
          "2284:         val t2Table = spark.table(t2)",
          "2285:         val dfQuery = t1Table.join(t2Table, t1Table.col(\"id\") === t2Table.col(\"id\"))",
          "2286:           .select(s\"$t1.id\", s\"$t2.id\", \"data\", \"index\", s\"$t1.index\", s\"$t2.index\")",
          "2288:         Seq(sqlQuery, dfQuery).foreach { query =>",
          "2289:           checkAnswer(query,",
          "2290:             Seq(",
          "2291:               Row(1, 1, \"a\", -1, 0, -1),",
          "2292:               Row(2, 2, \"b\", -2, 0, -2),",
          "2293:               Row(3, 3, \"c\", -3, 0, -3)",
          "2294:             )",
          "2295:           )",
          "2296:         }",
          "2297:       }",
          "2298:     }",
          "2299:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2382:     }",
          "2383:   }",
          "2430:   test(\"SPARK-34576: drop/add columns to a dataset of `DESCRIBE COLUMN`\") {",
          "2431:     val tbl = s\"${catalogAndNamespace}tbl\"",
          "2432:     withTable(tbl) {",
          "",
          "[Removed Lines]",
          "2385:   test(\"SPARK-34555: Resolve DataFrame metadata column\") {",
          "2386:     val tbl = s\"${catalogAndNamespace}table\"",
          "2387:     withTable(tbl) {",
          "2388:       sql(s\"CREATE TABLE $tbl (id bigint, data string) USING $v2Format \" +",
          "2389:         \"PARTITIONED BY (bucket(4, id), id)\")",
          "2390:       sql(s\"INSERT INTO $tbl VALUES (1, 'a'), (2, 'b'), (3, 'c')\")",
          "2391:       val table = spark.table(tbl)",
          "2392:       val dfQuery = table.select(",
          "2393:         table.col(\"id\"),",
          "2394:         table.col(\"data\"),",
          "2395:         table.col(\"index\"),",
          "2396:         table.col(\"_partition\")",
          "2397:       )",
          "2399:       checkAnswer(",
          "2400:         dfQuery,",
          "2401:         Seq(Row(1, \"a\", 0, \"3/1\"), Row(2, \"b\", 0, \"0/2\"), Row(3, \"c\", 0, \"1/3\"))",
          "2402:       )",
          "2403:     }",
          "2404:   }",
          "2406:   test(\"SPARK-34561: drop/add columns to a dataset of `DESCRIBE TABLE`\") {",
          "2407:     val tbl = s\"${catalogAndNamespace}tbl\"",
          "2408:     withTable(tbl) {",
          "2409:       sql(s\"CREATE TABLE $tbl (c0 INT) USING $v2Format\")",
          "2410:       val description = sql(s\"DESCRIBE TABLE $tbl\")",
          "2411:       val noCommentDataset = description.drop(\"comment\")",
          "2412:       val expectedSchema = new StructType()",
          "2413:         .add(",
          "2414:           name = \"col_name\",",
          "2415:           dataType = StringType,",
          "2416:           nullable = false,",
          "2417:           metadata = new MetadataBuilder().putString(\"comment\", \"name of the column\").build())",
          "2418:         .add(",
          "2419:           name = \"data_type\",",
          "2420:           dataType = StringType,",
          "2421:           nullable = false,",
          "2422:           metadata = new MetadataBuilder().putString(\"comment\", \"data type of the column\").build())",
          "2423:       assert(noCommentDataset.schema === expectedSchema)",
          "2424:       val isNullDataset = noCommentDataset",
          "2425:         .withColumn(\"is_null\", noCommentDataset(\"col_name\").isNull)",
          "2426:       assert(isNullDataset.schema === expectedSchema.add(\"is_null\", BooleanType, false))",
          "2427:     }",
          "2428:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2446:     }",
          "2447:   }",
          "2552:   test(\"SPARK-36481: Test for SET CATALOG statement\") {",
          "2553:     val catalogManager = spark.sessionState.catalogManager",
          "2554:     assert(catalogManager.currentCatalog.name() == SESSION_CATALOG_NAME)",
          "",
          "[Removed Lines]",
          "2449:   test(\"SPARK-34923: do not propagate metadata columns through Project\") {",
          "2450:     val t1 = s\"${catalogAndNamespace}table\"",
          "2451:     withTable(t1) {",
          "2452:       sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format \" +",
          "2453:         \"PARTITIONED BY (bucket(4, id), id)\")",
          "2454:       sql(s\"INSERT INTO $t1 VALUES (1, 'a'), (2, 'b'), (3, 'c')\")",
          "2456:       assertThrows[AnalysisException] {",
          "2457:         sql(s\"SELECT index, _partition from (SELECT id, data FROM $t1)\")",
          "2458:       }",
          "2459:       assertThrows[AnalysisException] {",
          "2460:         spark.table(t1).select(\"id\", \"data\").select(\"index\", \"_partition\")",
          "2461:       }",
          "2462:     }",
          "2463:   }",
          "2465:   test(\"SPARK-34923: do not propagate metadata columns through View\") {",
          "2466:     val t1 = s\"${catalogAndNamespace}table\"",
          "2467:     val view = \"view\"",
          "2469:     withTable(t1) {",
          "2470:       withTempView(view) {",
          "2471:         sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format \" +",
          "2472:           \"PARTITIONED BY (bucket(4, id), id)\")",
          "2473:         sql(s\"INSERT INTO $t1 VALUES (1, 'a'), (2, 'b'), (3, 'c')\")",
          "2474:         sql(s\"CACHE TABLE $view AS SELECT * FROM $t1\")",
          "2475:         assertThrows[AnalysisException] {",
          "2476:           sql(s\"SELECT index, _partition FROM $view\")",
          "2477:         }",
          "2478:       }",
          "2479:     }",
          "2480:   }",
          "2482:   test(\"SPARK-34923: propagate metadata columns through Filter\") {",
          "2483:     val t1 = s\"${catalogAndNamespace}table\"",
          "2484:     withTable(t1) {",
          "2485:       sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format \" +",
          "2486:         \"PARTITIONED BY (bucket(4, id), id)\")",
          "2487:       sql(s\"INSERT INTO $t1 VALUES (1, 'a'), (2, 'b'), (3, 'c')\")",
          "2489:       val sqlQuery = spark.sql(s\"SELECT id, data, index, _partition FROM $t1 WHERE id > 1\")",
          "2490:       val dfQuery = spark.table(t1).where(\"id > 1\").select(\"id\", \"data\", \"index\", \"_partition\")",
          "2492:       Seq(sqlQuery, dfQuery).foreach { query =>",
          "2493:         checkAnswer(query, Seq(Row(2, \"b\", 0, \"0/2\"), Row(3, \"c\", 0, \"1/3\")))",
          "2494:       }",
          "2495:     }",
          "2496:   }",
          "2498:   test(\"SPARK-34923: propagate metadata columns through Sort\") {",
          "2499:     val t1 = s\"${catalogAndNamespace}table\"",
          "2500:     withTable(t1) {",
          "2501:       sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format \" +",
          "2502:         \"PARTITIONED BY (bucket(4, id), id)\")",
          "2503:       sql(s\"INSERT INTO $t1 VALUES (1, 'a'), (2, 'b'), (3, 'c')\")",
          "2505:       val sqlQuery = spark.sql(s\"SELECT id, data, index, _partition FROM $t1 ORDER BY id\")",
          "2506:       val dfQuery = spark.table(t1).orderBy(\"id\").select(\"id\", \"data\", \"index\", \"_partition\")",
          "2508:       Seq(sqlQuery, dfQuery).foreach { query =>",
          "2509:         checkAnswer(query, Seq(Row(1, \"a\", 0, \"3/1\"), Row(2, \"b\", 0, \"0/2\"), Row(3, \"c\", 0, \"1/3\")))",
          "2510:       }",
          "2511:     }",
          "2512:   }",
          "2514:   test(\"SPARK-34923: propagate metadata columns through RepartitionBy\") {",
          "2515:     val t1 = s\"${catalogAndNamespace}table\"",
          "2516:     withTable(t1) {",
          "2517:       sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format \" +",
          "2518:         \"PARTITIONED BY (bucket(4, id), id)\")",
          "2519:       sql(s\"INSERT INTO $t1 VALUES (1, 'a'), (2, 'b'), (3, 'c')\")",
          "2521:       val sqlQuery = spark.sql(",
          "2522:         s\"SELECT /*+ REPARTITION_BY_RANGE(3, id) */ id, data, index, _partition FROM $t1\")",
          "2523:       val tbl = spark.table(t1)",
          "2524:       val dfQuery = tbl.repartitionByRange(3, tbl.col(\"id\"))",
          "2525:         .select(\"id\", \"data\", \"index\", \"_partition\")",
          "2527:       Seq(sqlQuery, dfQuery).foreach { query =>",
          "2528:         checkAnswer(query, Seq(Row(1, \"a\", 0, \"3/1\"), Row(2, \"b\", 0, \"0/2\"), Row(3, \"c\", 0, \"1/3\")))",
          "2529:       }",
          "2530:     }",
          "2531:   }",
          "2533:   test(\"SPARK-34923: propagate metadata columns through SubqueryAlias\") {",
          "2534:     val t1 = s\"${catalogAndNamespace}table\"",
          "2535:     val sbq = \"sbq\"",
          "2536:     withTable(t1) {",
          "2537:       sql(s\"CREATE TABLE $t1 (id bigint, data string) USING $v2Format \" +",
          "2538:         \"PARTITIONED BY (bucket(4, id), id)\")",
          "2539:       sql(s\"INSERT INTO $t1 VALUES (1, 'a'), (2, 'b'), (3, 'c')\")",
          "2541:       val sqlQuery = spark.sql(",
          "2542:         s\"SELECT $sbq.id, $sbq.data, $sbq.index, $sbq._partition FROM $t1 as $sbq\")",
          "2543:       val dfQuery = spark.table(t1).as(sbq).select(",
          "2544:         s\"$sbq.id\", s\"$sbq.data\", s\"$sbq.index\", s\"$sbq._partition\")",
          "2546:       Seq(sqlQuery, dfQuery).foreach { query =>",
          "2547:         checkAnswer(query, Seq(Row(1, \"a\", 0, \"3/1\"), Row(2, \"b\", 0, \"0/2\"), Row(3, \"c\", 0, \"1/3\")))",
          "2548:       }",
          "2549:     }",
          "2550:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/connector/MetadataColumnSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/connector/MetadataColumnSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/connector/MetadataColumnSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/connector/MetadataColumnSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.sql.connector",
          "20: import org.apache.spark.sql.{AnalysisException, Row}",
          "21: import org.apache.spark.sql.functions.struct",
          "23: class MetadataColumnSuite extends DatasourceV2SQLBase {",
          "24:   import testImplicits._",
          "26:   private val tbl = \"testcat.t\"",
          "28:   private def prepareTable(): Unit = {",
          "29:     sql(s\"CREATE TABLE $tbl (id bigint, data string) PARTITIONED BY (bucket(4, id), id)\")",
          "30:     sql(s\"INSERT INTO $tbl VALUES (1, 'a'), (2, 'b'), (3, 'c')\")",
          "31:   }",
          "33:   test(\"SPARK-31255: Project a metadata column\") {",
          "34:     withTable(tbl) {",
          "35:       prepareTable()",
          "36:       val sqlQuery = sql(s\"SELECT id, data, index, _partition FROM $tbl\")",
          "37:       val dfQuery = spark.table(tbl).select(\"id\", \"data\", \"index\", \"_partition\")",
          "39:       Seq(sqlQuery, dfQuery).foreach { query =>",
          "40:         checkAnswer(query, Seq(Row(1, \"a\", 0, \"3/1\"), Row(2, \"b\", 0, \"0/2\"), Row(3, \"c\", 0, \"1/3\")))",
          "41:       }",
          "42:     }",
          "43:   }",
          "45:   test(\"SPARK-31255: Projects data column when metadata column has the same name\") {",
          "46:     withTable(tbl) {",
          "47:       sql(s\"CREATE TABLE $tbl (index bigint, data string) PARTITIONED BY (bucket(4, index), index)\")",
          "48:       sql(s\"INSERT INTO $tbl VALUES (3, 'c'), (2, 'b'), (1, 'a')\")",
          "50:       val sqlQuery = sql(s\"SELECT index, data, _partition FROM $tbl\")",
          "51:       val dfQuery = spark.table(tbl).select(\"index\", \"data\", \"_partition\")",
          "53:       Seq(sqlQuery, dfQuery).foreach { query =>",
          "54:         checkAnswer(query, Seq(Row(3, \"c\", \"1/3\"), Row(2, \"b\", \"0/2\"), Row(1, \"a\", \"3/1\")))",
          "55:       }",
          "56:     }",
          "57:   }",
          "59:   test(\"SPARK-31255: * expansion does not include metadata columns\") {",
          "60:     withTable(tbl) {",
          "61:       prepareTable()",
          "62:       val sqlQuery = sql(s\"SELECT * FROM $tbl\")",
          "63:       val dfQuery = spark.table(tbl)",
          "65:       Seq(sqlQuery, dfQuery).foreach { query =>",
          "66:         checkAnswer(query, Seq(Row(1, \"a\"), Row(2, \"b\"), Row(3, \"c\")))",
          "67:       }",
          "68:     }",
          "69:   }",
          "71:   test(\"SPARK-31255: metadata column should only be produced when necessary\") {",
          "72:     withTable(tbl) {",
          "73:       prepareTable()",
          "74:       val sqlQuery = sql(s\"SELECT * FROM $tbl WHERE index = 0\")",
          "75:       val dfQuery = spark.table(tbl).filter(\"index = 0\")",
          "77:       Seq(sqlQuery, dfQuery).foreach { query =>",
          "78:         assert(query.schema.fieldNames.toSeq == Seq(\"id\", \"data\"))",
          "79:       }",
          "80:     }",
          "81:   }",
          "83:   test(\"SPARK-34547: metadata columns are resolved last\") {",
          "84:     withTable(tbl) {",
          "85:       prepareTable()",
          "86:       withTempView(\"v\") {",
          "87:         sql(s\"CREATE TEMPORARY VIEW v AS SELECT * FROM \" +",
          "88:           s\"VALUES (1, -1), (2, -2), (3, -3) AS v(id, index)\")",
          "90:         val sqlQuery = sql(s\"SELECT $tbl.id, v.id, data, index, $tbl.index, v.index \" +",
          "91:           s\"FROM $tbl JOIN v WHERE $tbl.id = v.id\")",
          "92:         val tableDf = spark.table(tbl)",
          "93:         val viewDf = spark.table(\"v\")",
          "94:         val dfQuery = tableDf.join(viewDf, tableDf.col(\"id\") === viewDf.col(\"id\"))",
          "95:           .select(s\"$tbl.id\", \"v.id\", \"data\", \"index\", s\"$tbl.index\", \"v.index\")",
          "97:         Seq(sqlQuery, dfQuery).foreach { query =>",
          "98:           checkAnswer(query,",
          "99:             Seq(",
          "100:               Row(1, 1, \"a\", -1, 0, -1),",
          "101:               Row(2, 2, \"b\", -2, 0, -2),",
          "102:               Row(3, 3, \"c\", -3, 0, -3)",
          "103:             )",
          "104:           )",
          "105:         }",
          "106:       }",
          "107:     }",
          "108:   }",
          "110:   test(\"SPARK-34555: Resolve DataFrame metadata column\") {",
          "111:     withTable(tbl) {",
          "112:       prepareTable()",
          "113:       val table = spark.table(tbl)",
          "114:       val dfQuery = table.select(",
          "115:         table.col(\"id\"),",
          "116:         table.col(\"data\"),",
          "117:         table.col(\"index\"),",
          "118:         table.col(\"_partition\")",
          "119:       )",
          "121:       checkAnswer(",
          "122:         dfQuery,",
          "123:         Seq(Row(1, \"a\", 0, \"3/1\"), Row(2, \"b\", 0, \"0/2\"), Row(3, \"c\", 0, \"1/3\"))",
          "124:       )",
          "125:     }",
          "126:   }",
          "128:   test(\"SPARK-34923: propagate metadata columns through Project\") {",
          "129:     withTable(tbl) {",
          "130:       prepareTable()",
          "131:       checkAnswer(",
          "132:         spark.table(tbl).select(\"id\", \"data\").select(\"index\", \"_partition\"),",
          "133:         Seq(Row(0, \"3/1\"), Row(0, \"0/2\"), Row(0, \"1/3\"))",
          "134:       )",
          "135:     }",
          "136:   }",
          "138:   test(\"SPARK-34923: do not propagate metadata columns through View\") {",
          "139:     val view = \"view\"",
          "140:     withTable(tbl) {",
          "141:       withTempView(view) {",
          "142:         prepareTable()",
          "143:         sql(s\"CACHE TABLE $view AS SELECT * FROM $tbl\")",
          "144:         assertThrows[AnalysisException] {",
          "145:           sql(s\"SELECT index, _partition FROM $view\")",
          "146:         }",
          "147:       }",
          "148:     }",
          "149:   }",
          "151:   test(\"SPARK-34923: propagate metadata columns through Filter\") {",
          "152:     withTable(tbl) {",
          "153:       prepareTable()",
          "154:       val sqlQuery = sql(s\"SELECT id, data, index, _partition FROM $tbl WHERE id > 1\")",
          "155:       val dfQuery = spark.table(tbl).where(\"id > 1\").select(\"id\", \"data\", \"index\", \"_partition\")",
          "157:       Seq(sqlQuery, dfQuery).foreach { query =>",
          "158:         checkAnswer(query, Seq(Row(2, \"b\", 0, \"0/2\"), Row(3, \"c\", 0, \"1/3\")))",
          "159:       }",
          "160:     }",
          "161:   }",
          "163:   test(\"SPARK-34923: propagate metadata columns through Sort\") {",
          "164:     withTable(tbl) {",
          "165:       prepareTable()",
          "166:       val sqlQuery = sql(s\"SELECT id, data, index, _partition FROM $tbl ORDER BY id\")",
          "167:       val dfQuery = spark.table(tbl).orderBy(\"id\").select(\"id\", \"data\", \"index\", \"_partition\")",
          "169:       Seq(sqlQuery, dfQuery).foreach { query =>",
          "170:         checkAnswer(query, Seq(Row(1, \"a\", 0, \"3/1\"), Row(2, \"b\", 0, \"0/2\"), Row(3, \"c\", 0, \"1/3\")))",
          "171:       }",
          "172:     }",
          "173:   }",
          "175:   test(\"SPARK-34923: propagate metadata columns through RepartitionBy\") {",
          "176:     withTable(tbl) {",
          "177:       prepareTable()",
          "178:       val sqlQuery = sql(",
          "179:         s\"SELECT /*+ REPARTITION_BY_RANGE(3, id) */ id, data, index, _partition FROM $tbl\")",
          "180:       val dfQuery = spark.table(tbl).repartitionByRange(3, $\"id\")",
          "181:         .select(\"id\", \"data\", \"index\", \"_partition\")",
          "183:       Seq(sqlQuery, dfQuery).foreach { query =>",
          "184:         checkAnswer(query, Seq(Row(1, \"a\", 0, \"3/1\"), Row(2, \"b\", 0, \"0/2\"), Row(3, \"c\", 0, \"1/3\")))",
          "185:       }",
          "186:     }",
          "187:   }",
          "189:   test(\"SPARK-34923: propagate metadata columns through SubqueryAlias if child is leaf node\") {",
          "190:     val sbq = \"sbq\"",
          "191:     withTable(tbl) {",
          "192:       prepareTable()",
          "193:       val sqlQuery = sql(",
          "194:         s\"SELECT $sbq.id, $sbq.data, $sbq.index, $sbq._partition FROM $tbl $sbq\")",
          "195:       val dfQuery = spark.table(tbl).as(sbq).select(",
          "196:         s\"$sbq.id\", s\"$sbq.data\", s\"$sbq.index\", s\"$sbq._partition\")",
          "198:       Seq(sqlQuery, dfQuery).foreach { query =>",
          "199:         checkAnswer(query, Seq(Row(1, \"a\", 0, \"3/1\"), Row(2, \"b\", 0, \"0/2\"), Row(3, \"c\", 0, \"1/3\")))",
          "200:       }",
          "202:       assertThrows[AnalysisException] {",
          "203:         sql(s\"SELECT $sbq.index FROM (SELECT id FROM $tbl) $sbq\")",
          "204:       }",
          "205:       assertThrows[AnalysisException] {",
          "206:         spark.table(tbl).select($\"id\").as(sbq).select(s\"$sbq.index\")",
          "207:       }",
          "208:     }",
          "209:   }",
          "211:   test(\"SPARK-40149: select outer join metadata columns with DataFrame API\") {",
          "212:     val df1 = Seq(1 -> \"a\").toDF(\"k\", \"v\").as(\"left\")",
          "213:     val df2 = Seq(1 -> \"b\").toDF(\"k\", \"v\").as(\"right\")",
          "214:     val dfQuery = df1.join(df2, Seq(\"k\"), \"outer\")",
          "215:       .withColumn(\"left_all\", struct($\"left.*\"))",
          "216:       .withColumn(\"right_all\", struct($\"right.*\"))",
          "217:     checkAnswer(dfQuery, Row(1, \"a\", \"b\", Row(1, \"a\"), Row(1, \"b\")))",
          "218:   }",
          "219: }",
          "",
          "---------------"
        ]
      }
    }
  ]
}