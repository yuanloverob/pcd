{
  "cve_id": "CVE-2023-39441",
  "cve_desc": "Apache Airflow SMTP Provider before 1.3.0, Apache Airflow IMAP Provider before 3.3.0, and\u00a0Apache Airflow before 2.7.0 are affected by the\u00a0Validation of OpenSSL Certificate vulnerability.\n\nThe default SSL context with SSL library did not check a server's X.509\u00a0certificate.\u00a0 Instead, the code accepted any certificate, which could\u00a0result in the disclosure of mail server credentials or mail contents\u00a0when the client connects to an attacker in a MITM position.\n\nUsers are strongly advised to upgrade to Apache Airflow version 2.7.0 or newer, Apache Airflow IMAP Provider version 3.3.0 or newer, and Apache Airflow SMTP Provider version 1.3.0 or newer to mitigate the risk associated with this vulnerability",
  "repo": "apache/airflow",
  "patch_hash": "dbacacbd4d476da757de148a4e747924c34fd7fe",
  "patch_info": {
    "commit_hash": "dbacacbd4d476da757de148a4e747924c34fd7fe",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/dbacacbd4d476da757de148a4e747924c34fd7fe",
    "files": [
      "airflow/providers/smtp/CHANGELOG.rst",
      "airflow/providers/smtp/hooks/smtp.py",
      "airflow/providers/smtp/provider.yaml",
      "docs/apache-airflow-providers-smtp/configurations-ref.rst",
      "docs/apache-airflow-providers-smtp/index.rst",
      "docs/apache-airflow/configurations-ref.rst",
      "tests/providers/smtp/hooks/test_smtp.py"
    ],
    "message": "Allows to choose SSL context for SMTP provider (#33075)\n\n* Allows to choose SSL context for SMTP provider\n\nThis change add two options to choose from when SSL SMTP connection\nis created:\n\n* default - for balance between compatibility and security\n* none - in case compatibility with existing infrastructure is\n\u00a0 preferred\n\nThe fallback is:\n\n* The Airflow \"email\", \"ssl_context\"\n* \"default\"\n\n* Update airflow/providers/smtp/CHANGELOG.rst\n\nCo-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>\n(cherry picked from commit e20325db38fdfdd9db423a345b13d18aab6fe578)",
    "before_after_code_files": [
      "airflow/providers/smtp/hooks/smtp.py||airflow/providers/smtp/hooks/smtp.py",
      "tests/providers/smtp/hooks/test_smtp.py||tests/providers/smtp/hooks/test_smtp.py"
    ]
  },
  "patch_diff": {
    "airflow/providers/smtp/hooks/smtp.py||airflow/providers/smtp/hooks/smtp.py": [
      "File: airflow/providers/smtp/hooks/smtp.py -> airflow/providers/smtp/hooks/smtp.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "26: import os",
      "27: import re",
      "28: import smtplib",
      "29: from email.mime.application import MIMEApplication",
      "30: from email.mime.multipart import MIMEMultipart",
      "31: from email.mime.text import MIMEText",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "29: import ssl",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "109:             smtp_kwargs[\"port\"] = self.port",
      "110:         smtp_kwargs[\"timeout\"] = self.timeout",
      "112:         return SMTP(**smtp_kwargs)",
      "114:     @classmethod",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "112:         if self.use_ssl:",
      "113:             from airflow.configuration import conf",
      "115:             ssl_context_string = conf.get(\"smtp_provider\", \"SSL_CONTEXT\", fallback=None)",
      "116:             if ssl_context_string is None:",
      "117:                 ssl_context_string = conf.get(\"email\", \"SSL_CONTEXT\", fallback=None)",
      "118:             if ssl_context_string is None:",
      "119:                 ssl_context_string = \"default\"",
      "120:             if ssl_context_string == \"default\":",
      "121:                 ssl_context = ssl.create_default_context()",
      "122:             elif ssl_context_string == \"none\":",
      "123:                 ssl_context = None",
      "124:             else:",
      "125:                 raise RuntimeError(",
      "126:                     f\"The email.ssl_context configuration variable must \"",
      "127:                     f\"be set to 'default' or 'none' and is '{ssl_context_string}'.\"",
      "128:                 )",
      "129:             smtp_kwargs[\"context\"] = ssl_context",
      "",
      "---------------"
    ],
    "tests/providers/smtp/hooks/test_smtp.py||tests/providers/smtp/hooks/test_smtp.py": [
      "File: tests/providers/smtp/hooks/test_smtp.py -> tests/providers/smtp/hooks/test_smtp.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from airflow.providers.smtp.hooks.smtp import SmtpHook",
      "31: from airflow.utils import db",
      "32: from airflow.utils.session import create_session",
      "34: smtplib_string = \"airflow.providers.smtp.hooks.smtp.smtplib\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "75:         )",
      "77:     @patch(smtplib_string)",
      "79:         mock_conn = _create_fake_smtp(mock_smtplib)",
      "81:         with SmtpHook():",
      "82:             pass",
      "85:         mock_conn.login.assert_called_once_with(\"smtp_user\", \"smtp_password\")",
      "86:         assert mock_conn.close.call_count == 1",
      "",
      "[Removed Lines]",
      "78:     def test_connect_and_disconnect(self, mock_smtplib):",
      "84:         mock_smtplib.SMTP_SSL.assert_called_once_with(host=\"smtp_server_address\", port=465, timeout=30)",
      "",
      "[Added Lines]",
      "79:     @patch(\"ssl.create_default_context\")",
      "80:     def test_connect_and_disconnect(self, create_default_context, mock_smtplib):",
      "85:         assert create_default_context.called",
      "86:         mock_smtplib.SMTP_SSL.assert_called_once_with(",
      "87:             host=\"smtp_server_address\", port=465, timeout=30, context=create_default_context.return_value",
      "88:         )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "202:     @patch(\"smtplib.SMTP_SSL\")",
      "203:     @patch(\"smtplib.SMTP\")",
      "205:         mock_smtp_ssl.return_value = Mock()",
      "206:         with SmtpHook() as smtp_hook:",
      "207:             smtp_hook.send_email_smtp(to=\"to\", subject=\"subject\", html_content=\"content\", from_email=\"from\")",
      "208:         assert not mock_smtp.called",
      "211:     @patch(\"smtplib.SMTP_SSL\")",
      "212:     @patch(\"smtplib.SMTP\")",
      "",
      "[Removed Lines]",
      "204:     def test_send_mime_ssl(self, mock_smtp, mock_smtp_ssl):",
      "209:         mock_smtp_ssl.assert_called_once_with(host=\"smtp_server_address\", port=465, timeout=30)",
      "",
      "[Added Lines]",
      "208:     @patch(\"ssl.create_default_context\")",
      "209:     def test_send_mime_ssl(self, create_default_context, mock_smtp, mock_smtp_ssl):",
      "214:         assert create_default_context.called",
      "215:         mock_smtp_ssl.assert_called_once_with(",
      "216:             host=\"smtp_server_address\", port=465, timeout=30, context=create_default_context.return_value",
      "217:         )",
      "219:     @patch(\"smtplib.SMTP_SSL\")",
      "220:     @patch(\"smtplib.SMTP\")",
      "221:     @patch(\"ssl.create_default_context\")",
      "222:     def test_send_mime_ssl_none_email_context(self, create_default_context, mock_smtp, mock_smtp_ssl):",
      "223:         mock_smtp_ssl.return_value = Mock()",
      "224:         with conf_vars({(\"smtp\", \"smtp_ssl\"): \"True\", (\"email\", \"ssl_context\"): \"none\"}):",
      "225:             with SmtpHook() as smtp_hook:",
      "226:                 smtp_hook.send_email_smtp(",
      "227:                     to=\"to\", subject=\"subject\", html_content=\"content\", from_email=\"from\"",
      "228:                 )",
      "229:         assert not mock_smtp.called",
      "230:         assert not create_default_context.called",
      "231:         mock_smtp_ssl.assert_called_once_with(host=\"smtp_server_address\", port=465, timeout=30, context=None)",
      "233:     @patch(\"smtplib.SMTP_SSL\")",
      "234:     @patch(\"smtplib.SMTP\")",
      "235:     @patch(\"ssl.create_default_context\")",
      "236:     def test_send_mime_ssl_none_smtp_provider_context(self, create_default_context, mock_smtp, mock_smtp_ssl):",
      "237:         mock_smtp_ssl.return_value = Mock()",
      "238:         with conf_vars({(\"smtp\", \"smtp_ssl\"): \"True\", (\"smtp_provider\", \"ssl_context\"): \"none\"}):",
      "239:             with SmtpHook() as smtp_hook:",
      "240:                 smtp_hook.send_email_smtp(",
      "241:                     to=\"to\", subject=\"subject\", html_content=\"content\", from_email=\"from\"",
      "242:                 )",
      "243:         assert not mock_smtp.called",
      "244:         assert not create_default_context.called",
      "245:         mock_smtp_ssl.assert_called_once_with(host=\"smtp_server_address\", port=465, timeout=30, context=None)",
      "247:     @patch(\"smtplib.SMTP_SSL\")",
      "248:     @patch(\"smtplib.SMTP\")",
      "249:     @patch(\"ssl.create_default_context\")",
      "250:     def test_send_mime_ssl_none_smtp_provider_default_email_context(",
      "251:         self, create_default_context, mock_smtp, mock_smtp_ssl",
      "252:     ):",
      "253:         mock_smtp_ssl.return_value = Mock()",
      "254:         with conf_vars(",
      "255:             {",
      "256:                 (\"smtp\", \"smtp_ssl\"): \"True\",",
      "257:                 (\"email\", \"ssl_context\"): \"default\",",
      "258:                 (\"smtp_provider\", \"ssl_context\"): \"none\",",
      "259:             }",
      "260:         ):",
      "261:             with SmtpHook() as smtp_hook:",
      "262:                 smtp_hook.send_email_smtp(",
      "263:                     to=\"to\", subject=\"subject\", html_content=\"content\", from_email=\"from\"",
      "264:                 )",
      "265:         assert not mock_smtp.called",
      "266:         assert not create_default_context.called",
      "267:         mock_smtp_ssl.assert_called_once_with(host=\"smtp_server_address\", port=465, timeout=30, context=None)",
      "269:     @patch(\"smtplib.SMTP_SSL\")",
      "270:     @patch(\"smtplib.SMTP\")",
      "271:     @patch(\"ssl.create_default_context\")",
      "272:     def test_send_mime_ssl_default_smtp_provider_none_email_context(",
      "273:         self, create_default_context, mock_smtp, mock_smtp_ssl",
      "274:     ):",
      "275:         mock_smtp_ssl.return_value = Mock()",
      "276:         with conf_vars(",
      "277:             {",
      "278:                 (\"smtp\", \"smtp_ssl\"): \"True\",",
      "279:                 (\"email\", \"ssl_context\"): \"none\",",
      "280:                 (\"smtp_provider\", \"ssl_context\"): \"default\",",
      "281:             }",
      "282:         ):",
      "283:             with SmtpHook() as smtp_hook:",
      "284:                 smtp_hook.send_email_smtp(",
      "285:                     to=\"to\", subject=\"subject\", html_content=\"content\", from_email=\"from\"",
      "286:                 )",
      "287:         assert not mock_smtp.called",
      "288:         assert create_default_context.called",
      "289:         mock_smtp_ssl.assert_called_once_with(",
      "290:             host=\"smtp_server_address\", port=465, timeout=30, context=create_default_context.return_value",
      "291:         )",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "270:     @patch(\"airflow.models.connection.Connection\")",
      "271:     @patch(\"smtplib.SMTP_SSL\")",
      "273:         mock_smtp_ssl().sendmail.side_effect = smtplib.SMTPServerDisconnected()",
      "274:         custom_retry_limit = 10",
      "275:         custom_timeout = 60",
      "",
      "[Removed Lines]",
      "272:     def test_send_mime_custom_timeout_retrylimit(self, mock_smtp_ssl, connection_mock):",
      "",
      "[Added Lines]",
      "354:     @patch(\"ssl.create_default_context\")",
      "355:     def test_send_mime_custom_timeout_retrylimit(",
      "356:         self, create_default_context, mock_smtp_ssl, connection_mock",
      "357:     ):",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "287:             with pytest.raises(smtplib.SMTPServerDisconnected):",
      "288:                 smtp_hook.send_email_smtp(to=\"to\", subject=\"subject\", html_content=\"content\")",
      "289:         mock_smtp_ssl.assert_any_call(",
      "291:         )",
      "292:         assert mock_smtp_ssl().sendmail.call_count == 10",
      "",
      "[Removed Lines]",
      "290:             host=fake_conn.host, port=fake_conn.port, timeout=fake_conn.extra_dejson[\"timeout\"]",
      "",
      "[Added Lines]",
      "375:             host=fake_conn.host,",
      "376:             port=fake_conn.port,",
      "377:             timeout=fake_conn.extra_dejson[\"timeout\"],",
      "378:             context=create_default_context.return_value,",
      "380:         assert create_default_context.called",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "4e00fe8ec01c9a2bbfea02ac7841d42eb45ea7f5",
      "candidate_info": {
        "commit_hash": "4e00fe8ec01c9a2bbfea02ac7841d42eb45ea7f5",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4e00fe8ec01c9a2bbfea02ac7841d42eb45ea7f5",
        "files": [
          "airflow/dag_processing/manager.py",
          "airflow/dag_processing/processor.py"
        ],
        "message": "Refactor: Simplify code in dag_processing (#33161)\n\n(cherry picked from commit 2f16a46db51b67e67b8406fbafe9e9add6eee235)",
        "before_after_code_files": [
          "airflow/dag_processing/manager.py||airflow/dag_processing/manager.py",
          "airflow/dag_processing/processor.py||airflow/dag_processing/processor.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/dag_processing/manager.py||airflow/dag_processing/manager.py": [
          "File: airflow/dag_processing/manager.py -> airflow/dag_processing/manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1001:                 processor.terminate()",
          "1002:                 self._file_stats.pop(file_path)",
          "1005:         for key in to_remove:",
          "1006:             # Remove the stats for any dag files that don't exist anymore",
          "1007:             del self._file_stats[key]",
          "",
          "[Removed Lines]",
          "1004:         to_remove = set(self._file_stats.keys()) - set(self._file_paths)",
          "",
          "[Added Lines]",
          "1004:         to_remove = set(self._file_stats).difference(self._file_paths)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1076:         while self._parallelism - len(self._processors) > 0 and self._file_path_queue:",
          "1077:             file_path = self._file_path_queue.popleft()",
          "1078:             # Stop creating duplicate processor i.e. processor with the same filepath",
          "1080:                 continue",
          "1082:             callback_to_execute_for_file = self._callback_to_execute[file_path]",
          "",
          "[Removed Lines]",
          "1079:             if file_path in self._processors.keys():",
          "",
          "[Added Lines]",
          "1079:             if file_path in self._processors:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1115:         self._parsing_start_time = time.perf_counter()",
          "1116:         # If the file path is already being processed, or if a file was",
          "1117:         # processed recently, wait until the next batch",
          "1119:         now = timezone.utcnow()",
          "1121:         # Sort the file paths by the parsing order mode",
          "",
          "[Removed Lines]",
          "1118:         file_paths_in_progress = self._processors.keys()",
          "",
          "[Added Lines]",
          "1118:         file_paths_in_progress = set(self._processors)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1173:             file_path for file_path, stat in self._file_stats.items() if stat.run_count == self._max_runs",
          "1174:         ]",
          "1177:             file_paths_recently_processed,",
          "1178:             files_paths_at_run_limit,",
          "1179:         )",
          "",
          "[Removed Lines]",
          "1176:         file_paths_to_exclude = set(file_paths_in_progress).union(",
          "",
          "[Added Lines]",
          "1176:         file_paths_to_exclude = file_paths_in_progress.union(",
          "",
          "---------------"
        ],
        "airflow/dag_processing/processor.py||airflow/dag_processing/processor.py": [
          "File: airflow/dag_processing/processor.py -> airflow/dag_processing/processor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "573:             for task in tasks_missed_sla:",
          "574:                 if task.email:",
          "575:                     if isinstance(task.email, str):",
          "577:                     elif isinstance(task.email, (list, tuple)):",
          "579:             if emails:",
          "580:                 try:",
          "581:                     send_email(emails, f\"[airflow] SLA miss on DAG={dag.dag_id}\", email_content)",
          "",
          "[Removed Lines]",
          "576:                         emails |= set(get_email_address_list(task.email))",
          "578:                         emails |= set(task.email)",
          "",
          "[Added Lines]",
          "576:                         emails.update(get_email_address_list(task.email))",
          "578:                         emails.update(task.email)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "652:             task_pools = {task.pool for task in dag.tasks}",
          "653:             nonexistent_pools = task_pools - pools",
          "654:             if nonexistent_pools:",
          "659:         pools = {p.pool for p in Pool.get_pools(session)}",
          "660:         for dag in dagbag.dags.values():",
          "",
          "[Removed Lines]",
          "655:                 return (",
          "656:                     f\"Dag '{dag.dag_id}' references non-existent pools: {list(sorted(nonexistent_pools))!r}\"",
          "657:                 )",
          "",
          "[Added Lines]",
          "655:                 return f\"Dag '{dag.dag_id}' references non-existent pools: {sorted(nonexistent_pools)!r}\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "679:         \"\"\"",
          "680:         self._validate_task_pools(dagbag=dagbag)",
          "686:         for warning_to_delete in stored_warnings - self.dag_warnings:",
          "687:             session.delete(warning_to_delete)",
          "",
          "[Removed Lines]",
          "682:         stored_warnings = set(",
          "683:             session.query(DagWarning).filter(DagWarning.dag_id.in_(dagbag.dags.keys())).all()",
          "684:         )",
          "",
          "[Added Lines]",
          "680:         stored_warnings = set(session.query(DagWarning).filter(DagWarning.dag_id.in_(dagbag.dags)).all())",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "11d6b3c8648fc5f6a7bf7ed03475c8a966fb2f74",
      "candidate_info": {
        "commit_hash": "11d6b3c8648fc5f6a7bf7ed03475c8a966fb2f74",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/11d6b3c8648fc5f6a7bf7ed03475c8a966fb2f74",
        "files": [
          "airflow/providers/google/cloud/hooks/bigquery.py",
          "airflow/providers/google/cloud/operators/bigquery.py",
          "airflow/providers/openlineage/extractors/base.py",
          "airflow/providers/openlineage/utils/utils.py",
          "tests/providers/google/cloud/operators/job_details.json",
          "tests/providers/google/cloud/operators/test_bigquery.py"
        ],
        "message": "openlineage, bigquery: add openlineage method support for BigQueryExecuteQueryOperator (#31293)\n\nSigned-off-by: Maciej Obuchowski <obuchowski.maciej@gmail.com>\n(cherry picked from commit e10aa6ae6ad07830cbf5ec59d977654c52012c22)",
        "before_after_code_files": [
          "airflow/providers/google/cloud/hooks/bigquery.py||airflow/providers/google/cloud/hooks/bigquery.py",
          "airflow/providers/google/cloud/operators/bigquery.py||airflow/providers/google/cloud/operators/bigquery.py",
          "airflow/providers/openlineage/extractors/base.py||airflow/providers/openlineage/extractors/base.py",
          "airflow/providers/openlineage/utils/utils.py||airflow/providers/openlineage/utils/utils.py",
          "tests/providers/google/cloud/operators/test_bigquery.py||tests/providers/google/cloud/operators/test_bigquery.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/google/cloud/hooks/bigquery.py||airflow/providers/google/cloud/hooks/bigquery.py": [
          "File: airflow/providers/google/cloud/hooks/bigquery.py -> airflow/providers/google/cloud/hooks/bigquery.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2245:         self.running_job_id = job.job_id",
          "2246:         return job.job_id",
          "2249:         if force_rerun:",
          "2250:             hash_base = str(uuid.uuid4())",
          "2251:         else:",
          "",
          "[Removed Lines]",
          "2248:     def generate_job_id(self, job_id, dag_id, task_id, logical_date, configuration, force_rerun=False):",
          "",
          "[Added Lines]",
          "2248:     def generate_job_id(self, job_id, dag_id, task_id, logical_date, configuration, force_rerun=False) -> str:",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/operators/bigquery.py||airflow/providers/google/cloud/operators/bigquery.py": [
          "File: airflow/providers/google/cloud/operators/bigquery.py -> airflow/providers/google/cloud/operators/bigquery.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "133:         )",
          "136: class BigQueryCheckOperator(_BigQueryDbHookMixin, SQLCheckOperator):",
          "137:     \"\"\"Performs checks against BigQuery.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "136: class _BigQueryOpenLineageMixin:",
          "137:     def get_openlineage_facets_on_complete(self, task_instance):",
          "138:         \"\"\"",
          "139:         Retrieve OpenLineage data for a COMPLETE BigQuery job.",
          "141:         This method retrieves statistics for the specified job_ids using the BigQueryDatasetsProvider.",
          "142:         It calls BigQuery API, retrieving input and output dataset info from it, as well as run-level",
          "143:         usage statistics.",
          "145:         Run facets should contain:",
          "146:             - ExternalQueryRunFacet",
          "147:             - BigQueryJobRunFacet",
          "149:         Job facets should contain:",
          "150:             - SqlJobFacet if operator has self.sql",
          "152:         Input datasets should contain facets:",
          "153:             - DataSourceDatasetFacet",
          "154:             - SchemaDatasetFacet",
          "156:         Output datasets should contain facets:",
          "157:             - DataSourceDatasetFacet",
          "158:             - SchemaDatasetFacet",
          "159:             - OutputStatisticsOutputDatasetFacet",
          "160:         \"\"\"",
          "161:         from openlineage.client.facet import SqlJobFacet",
          "162:         from openlineage.common.provider.bigquery import BigQueryDatasetsProvider",
          "164:         from airflow.providers.openlineage.extractors import OperatorLineage",
          "165:         from airflow.providers.openlineage.utils.utils import normalize_sql",
          "167:         if not self.job_id:",
          "168:             return OperatorLineage()",
          "170:         client = self.hook.get_client(project_id=self.hook.project_id)",
          "171:         job_ids = self.job_id",
          "172:         if isinstance(self.job_id, str):",
          "173:             job_ids = [self.job_id]",
          "174:         inputs, outputs, run_facets = {}, {}, {}",
          "175:         for job_id in job_ids:",
          "176:             stats = BigQueryDatasetsProvider(client=client).get_facets(job_id=job_id)",
          "177:             for input in stats.inputs:",
          "178:                 input = input.to_openlineage_dataset()",
          "179:                 inputs[input.name] = input",
          "180:             if stats.output:",
          "181:                 output = stats.output.to_openlineage_dataset()",
          "182:                 outputs[output.name] = output",
          "183:             for key, value in stats.run_facets.items():",
          "184:                 run_facets[key] = value",
          "186:         job_facets = {}",
          "187:         if hasattr(self, \"sql\"):",
          "188:             job_facets[\"sql\"] = SqlJobFacet(query=normalize_sql(self.sql))",
          "190:         return OperatorLineage(",
          "191:             inputs=list(inputs.values()),",
          "192:             outputs=list(outputs.values()),",
          "193:             run_facets=run_facets,",
          "194:             job_facets=job_facets,",
          "195:         )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1153:         self.encryption_configuration = encryption_configuration",
          "1154:         self.hook: BigQueryHook | None = None",
          "1155:         self.impersonation_chain = impersonation_chain",
          "1157:     def execute(self, context: Context):",
          "1158:         if self.hook is None:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1218:         self.job_id: str | list[str] | None = None",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1164:                 impersonation_chain=self.impersonation_chain,",
          "1165:             )",
          "1166:         if isinstance(self.sql, str):",
          "1168:                 sql=self.sql,",
          "1169:                 destination_dataset_table=self.destination_dataset_table,",
          "1170:                 write_disposition=self.write_disposition,",
          "",
          "[Removed Lines]",
          "1167:             job_id: str | list[str] = self.hook.run_query(",
          "",
          "[Added Lines]",
          "1230:             self.job_id = self.hook.run_query(",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1184:                 encryption_configuration=self.encryption_configuration,",
          "1185:             )",
          "1186:         elif isinstance(self.sql, Iterable):",
          "1188:                 self.hook.run_query(",
          "1189:                     sql=s,",
          "1190:                     destination_dataset_table=self.destination_dataset_table,",
          "",
          "[Removed Lines]",
          "1187:             job_id = [",
          "",
          "[Added Lines]",
          "1250:             self.job_id = [",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1210:             raise AirflowException(f\"argument 'sql' of type {type(str)} is neither a string nor an iterable\")",
          "1211:         project_id = self.hook.project_id",
          "1212:         if project_id:",
          "1214:             context[\"task_instance\"].xcom_push(key=\"job_id_path\", value=job_id_path)",
          "1217:     def on_kill(self) -> None:",
          "1218:         super().on_kill()",
          "",
          "[Removed Lines]",
          "1213:             job_id_path = convert_job_id(job_id=job_id, project_id=project_id, location=self.location)",
          "1215:         return job_id",
          "",
          "[Added Lines]",
          "1276:             job_id_path = convert_job_id(job_id=self.job_id, project_id=project_id, location=self.location)",
          "1278:         return self.job_id",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "2562:         return table",
          "2566:     \"\"\"Execute a BigQuery job.",
          "2568:     Waits for the job to complete and returns job id.",
          "",
          "[Removed Lines]",
          "2565: class BigQueryInsertJobOperator(GoogleCloudBaseOperator):",
          "",
          "[Added Lines]",
          "2628: class BigQueryInsertJobOperator(GoogleCloudBaseOperator, _BigQueryOpenLineageMixin):",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "2663:         self.deferrable = deferrable",
          "2664:         self.poll_interval = poll_interval",
          "2666:     def prepare_template(self) -> None:",
          "2667:         # If .json is passed then we have to read the file",
          "2668:         if isinstance(self.configuration, str) and self.configuration.endswith(\".json\"):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2729:     @property",
          "2730:     def sql(self) -> str | None:",
          "2731:         try:",
          "2732:             return self.configuration[\"query\"][\"query\"]",
          "2733:         except KeyError:",
          "2734:             return None",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "2697:         )",
          "2698:         self.hook = hook",
          "2701:             job_id=self.job_id,",
          "2702:             dag_id=self.dag_id,",
          "2703:             task_id=self.task_id,",
          "",
          "[Removed Lines]",
          "2700:         job_id = hook.generate_job_id(",
          "",
          "[Added Lines]",
          "2770:         self.job_id = hook.generate_job_id(",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "2709:         try:",
          "2710:             self.log.info(\"Executing: %s'\", self.configuration)",
          "2712:         except Conflict:",
          "2713:             # If the job already exists retrieve it",
          "2714:             job = hook.get_job(",
          "2715:                 project_id=self.project_id,",
          "2716:                 location=self.location,",
          "2718:             )",
          "2719:             if job.state in self.reattach_states:",
          "2720:                 # We are reattaching to a job",
          "",
          "[Removed Lines]",
          "2711:             job: BigQueryJob | UnknownJob = self._submit_job(hook, job_id)",
          "2717:                 job_id=job_id,",
          "",
          "[Added Lines]",
          "2781:             job: BigQueryJob | UnknownJob = self._submit_job(hook, self.job_id)",
          "2787:                 job_id=self.job_id,",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "2723:             else:",
          "2724:                 # Same job configuration so we need force_rerun",
          "2725:                 raise AirflowException(",
          "2727:                     f\"want to force rerun it consider setting `force_rerun=True`.\"",
          "2728:                     f\"Or, if you want to reattach in this scenario add {job.state} to `reattach_states`\"",
          "2729:                 )",
          "",
          "[Removed Lines]",
          "2726:                     f\"Job with id: {job_id} already exists and is in {job.state} state. If you \"",
          "",
          "[Added Lines]",
          "2796:                     f\"Job with id: {self.job_id} already exists and is in {job.state} state. If you \"",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "2757:         self.job_id = job.job_id",
          "2758:         project_id = self.project_id or self.hook.project_id",
          "2759:         if project_id:",
          "2761:             context[\"ti\"].xcom_push(key=\"job_id_path\", value=job_id_path)",
          "2762:         # Wait for the job to complete",
          "2763:         if not self.deferrable:",
          "",
          "[Removed Lines]",
          "2760:             job_id_path = convert_job_id(job_id=job_id, project_id=project_id, location=self.location)",
          "",
          "[Added Lines]",
          "2830:             job_id_path = convert_job_id(",
          "2831:                 job_id=self.job_id, project_id=project_id, location=self.location  # type: ignore[arg-type]",
          "2832:             )",
          "",
          "---------------"
        ],
        "airflow/providers/openlineage/extractors/base.py||airflow/providers/openlineage/extractors/base.py": [
          "File: airflow/providers/openlineage/extractors/base.py -> airflow/providers/openlineage/extractors/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "86:         # OpenLineage methods are optional - if there's no method, return None",
          "87:         try:",
          "88:             return self._get_openlineage_facets(self.operator.get_openlineage_facets_on_start)  # type: ignore",
          "89:         except AttributeError:",
          "90:             return None",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "89:         except ImportError:",
          "90:             self.log.error(",
          "91:                 \"OpenLineage provider method failed to import OpenLineage integration. \"",
          "92:                 \"This should not happen. Please report this bug to developers.\"",
          "93:             )",
          "94:             return None",
          "",
          "---------------"
        ],
        "airflow/providers/openlineage/utils/utils.py||airflow/providers/openlineage/utils/utils.py": [
          "File: airflow/providers/openlineage/utils/utils.py -> airflow/providers/openlineage/utils/utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import os",
          "24: from contextlib import suppress",
          "25: from functools import wraps",
          "27: from urllib.parse import parse_qsl, urlencode, urlparse, urlunparse",
          "29: import attrs",
          "",
          "[Removed Lines]",
          "26: from typing import TYPE_CHECKING, Any",
          "",
          "[Added Lines]",
          "26: from typing import TYPE_CHECKING, Any, Iterable",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "414: def get_filtered_unknown_operator_keys(operator: BaseOperator) -> dict:",
          "415:     not_required_keys = {\"dag\", \"task_group\"}",
          "416:     return {attr: value for attr, value in operator.__dict__.items() if attr not in not_required_keys}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "419: def normalize_sql(sql: str | Iterable[str]):",
          "420:     if isinstance(sql, str):",
          "421:         sql = [stmt for stmt in sql.split(\";\") if stmt != \"\"]",
          "422:     sql = [obj for stmt in sql for obj in stmt.split(\";\") if obj != \"\"]",
          "423:     return \";\\n\".join(sql)",
          "",
          "---------------"
        ],
        "tests/providers/google/cloud/operators/test_bigquery.py||tests/providers/google/cloud/operators/test_bigquery.py": [
          "File: tests/providers/google/cloud/operators/test_bigquery.py -> tests/providers/google/cloud/operators/test_bigquery.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: from unittest import mock",
          "21: from unittest.mock import ANY, MagicMock",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import json",
          "21: from contextlib import suppress",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "24: import pytest",
          "25: from google.cloud.bigquery import DEFAULT_RETRY",
          "26: from google.cloud.exceptions import Conflict",
          "28: from airflow.exceptions import AirflowException, AirflowSkipException, AirflowTaskTimeout, TaskDeferred",
          "29: from airflow.providers.google.cloud.operators.bigquery import (",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "29: from openlineage.client.facet import (",
          "30:     DataSourceDatasetFacet,",
          "31:     ExternalQueryRunFacet,",
          "32:     SqlJobFacet,",
          "33: )",
          "34: from openlineage.client.run import Dataset",
          "35: from openlineage.common.provider.bigquery import BigQueryErrorRunFacet",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1520:             force_rerun=True,",
          "1521:         )",
          "1523:     @mock.patch(\"airflow.providers.google.cloud.operators.bigquery.BigQueryHook\")",
          "1524:     def test_execute_force_rerun_async(self, mock_hook, create_task_instance_of_operator):",
          "1525:         job_id = \"123456\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1532:     @mock.patch(\"airflow.providers.google.cloud.operators.bigquery.BigQueryHook\")",
          "1533:     def test_execute_openlineage_events(self, mock_hook):",
          "1534:         job_id = \"123456\"",
          "1535:         hash_ = \"hash\"",
          "1536:         real_job_id = f\"{job_id}_{hash_}\"",
          "1538:         configuration = {",
          "1539:             \"query\": {",
          "1540:                 \"query\": \"SELECT * FROM test_table\",",
          "1541:                 \"useLegacySql\": False,",
          "1542:             }",
          "1543:         }",
          "1544:         mock_hook.return_value.insert_job.return_value = MagicMock(job_id=real_job_id, error_result=False)",
          "1545:         mock_hook.return_value.generate_job_id.return_value = real_job_id",
          "1547:         op = BigQueryInsertJobOperator(",
          "1548:             task_id=\"insert_query_job\",",
          "1549:             configuration=configuration,",
          "1550:             location=TEST_DATASET_LOCATION,",
          "1551:             job_id=job_id,",
          "1552:             project_id=TEST_GCP_PROJECT_ID,",
          "1553:         )",
          "1554:         result = op.execute(context=MagicMock())",
          "1556:         mock_hook.return_value.insert_job.assert_called_once_with(",
          "1557:             configuration=configuration,",
          "1558:             location=TEST_DATASET_LOCATION,",
          "1559:             job_id=real_job_id,",
          "1560:             nowait=True,",
          "1561:             project_id=TEST_GCP_PROJECT_ID,",
          "1562:             retry=DEFAULT_RETRY,",
          "1563:             timeout=None,",
          "1564:         )",
          "1566:         assert result == real_job_id",
          "1568:         with open(file=\"tests/providers/google/cloud/operators/job_details.json\") as f:",
          "1569:             job_details = json.loads(f.read())",
          "1570:         mock_hook.return_value.get_client.return_value.get_job.return_value._properties = job_details",
          "1572:         lineage = op.get_openlineage_facets_on_complete(None)",
          "1573:         assert lineage.inputs == [",
          "1574:             Dataset(",
          "1575:                 namespace=\"bigquery\",",
          "1576:                 name=\"airflow-openlineage.new_dataset.test_table\",",
          "1577:                 facets={\"dataSource\": DataSourceDatasetFacet(name=\"bigquery\", uri=\"bigquery\")},",
          "1578:             )",
          "1579:         ]",
          "1581:         assert lineage.run_facets == {",
          "1582:             \"bigQuery_job\": mock.ANY,",
          "1583:             \"externalQuery\": ExternalQueryRunFacet(externalQueryId=mock.ANY, source=\"bigquery\"),",
          "1584:         }",
          "1585:         assert lineage.job_facets == {\"sql\": SqlJobFacet(query=\"SELECT * FROM test_table\")}",
          "1587:     @mock.patch(\"airflow.providers.google.cloud.operators.bigquery.BigQueryHook\")",
          "1588:     def test_execute_fails_openlineage_events(self, mock_hook):",
          "1589:         job_id = \"1234\"",
          "1591:         configuration = {",
          "1592:             \"query\": {",
          "1593:                 \"query\": \"SELECT * FROM test_table\",",
          "1594:                 \"useLegacySql\": False,",
          "1595:             }",
          "1596:         }",
          "1597:         operator = BigQueryInsertJobOperator(",
          "1598:             task_id=\"insert_query_job_failed\",",
          "1599:             configuration=configuration,",
          "1600:             location=TEST_DATASET_LOCATION,",
          "1601:             job_id=job_id,",
          "1602:             project_id=TEST_GCP_PROJECT_ID,",
          "1603:         )",
          "1604:         mock_hook.return_value.generate_job_id.return_value = \"1234\"",
          "1605:         mock_hook.return_value.get_client.return_value.get_job.side_effect = RuntimeError()",
          "1606:         mock_hook.return_value.insert_job.side_effect = RuntimeError()",
          "1608:         with suppress(RuntimeError):",
          "1609:             operator.execute(MagicMock())",
          "1610:         lineage = operator.get_openlineage_facets_on_complete(None)",
          "1612:         assert lineage.run_facets == {\"bigQuery_error\": BigQueryErrorRunFacet(clientError=mock.ANY)}",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0eb0a7a70bc7d0aa5efa17a081215a9875ddb2e6",
      "candidate_info": {
        "commit_hash": "0eb0a7a70bc7d0aa5efa17a081215a9875ddb2e6",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0eb0a7a70bc7d0aa5efa17a081215a9875ddb2e6",
        "files": [
          "Dockerfile",
          "scripts/docker/entrypoint_prod.sh"
        ],
        "message": "Fixing typo in Dockerfile (#33180)\n\n* Fixing typo in Dockerfile\n\n* Fixing typo in Dockerfile\n\n(cherry picked from commit 624cf7f75a95de63bcf7a22016b13f917669e9f2)",
        "before_after_code_files": [
          "scripts/docker/entrypoint_prod.sh||scripts/docker/entrypoint_prod.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/docker/entrypoint_prod.sh||scripts/docker/entrypoint_prod.sh": [
          "File: scripts/docker/entrypoint_prod.sh -> scripts/docker/entrypoint_prod.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "305:     >&2 echo \"         https://airflow.apache.org/docs/docker-stack/build.html\"",
          "306:     >&2 echo",
          "307:     >&2 echo \"         Adding requirements at container startup is fragile and is done every time\"",
          "309:     >&2 echo \"         of adding dependencies.\"",
          "310:     >&2 echo",
          "311:     pip install --root-user-action ignore --no-cache-dir ${_PIP_ADDITIONAL_REQUIREMENTS}",
          "",
          "[Removed Lines]",
          "308:     >&2 echo \"         the container starts, so it is onlny useful for testing and trying out\"",
          "",
          "[Added Lines]",
          "308:     >&2 echo \"         the container starts, so it is only useful for testing and trying out\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bc1d7b03ab0ba903718b4098196de24977ee1261",
      "candidate_info": {
        "commit_hash": "bc1d7b03ab0ba903718b4098196de24977ee1261",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/bc1d7b03ab0ba903718b4098196de24977ee1261",
        "files": [
          "airflow/jobs/scheduler_job_runner.py"
        ],
        "message": "Fix future DagRun rarely triggered by race conditions when max_active_runs reached its upper limit. (#31414)\n\n* feat: select dag_model with row lock\n\n* fix: logging that scheduling was skipped\n\n* fix: remove unused get_dagmodel\n\n* fix: correct log message to more generic word\n\n---------\n\nCo-authored-by: doiken <doiken@users.noreply.github.com>\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\nCo-authored-by: eladkal <45845474+eladkal@users.noreply.github.com>\n(cherry picked from commit b53e2aeefc1714d306f93e58d211ad9d52356470)",
        "before_after_code_files": [
          "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py": [
          "File: airflow/jobs/scheduler_job_runner.py -> airflow/jobs/scheduler_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "35: from sqlalchemy import and_, delete, func, not_, or_, select, text, update",
          "36: from sqlalchemy.engine import Result",
          "37: from sqlalchemy.exc import OperationalError",
          "39: from sqlalchemy.sql import expression",
          "41: from airflow import settings",
          "",
          "[Removed Lines]",
          "38: from sqlalchemy.orm import Query, Session, load_only, make_transient, selectinload",
          "",
          "[Added Lines]",
          "38: from sqlalchemy.orm import Query, Session, joinedload, load_only, make_transient, selectinload",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1397:         callback: DagCallbackRequest | None = None",
          "1399:         dag = dag_run.dag = self.dagbag.get_dag(dag_run.dag_id, session=session)",
          "1403:             self.log.error(\"Couldn't find DAG %s in DAG bag or database!\", dag_run.dag_id)",
          "1404:             return callback",
          "1406:         if (",
          "1407:             dag_run.start_date",
          "",
          "[Removed Lines]",
          "1400:         dag_model = DM.get_dagmodel(dag_run.dag_id, session)",
          "1402:         if not dag or not dag_model:",
          "",
          "[Added Lines]",
          "1400:         # Adopt row locking to account for inconsistencies when next_dagrun_create_after = None",
          "1401:         query = (",
          "1402:             session.query(DagModel)",
          "1403:             .filter(DagModel.dag_id == dag_run.dag_id)",
          "1404:             .options(joinedload(DagModel.parent_dag))",
          "1405:         )",
          "1406:         dag_model = with_row_locks(",
          "1407:             query, of=DagModel, session=session, **skip_locked(session=session)",
          "1408:         ).one_or_none()",
          "1410:         if not dag:",
          "1413:         if not dag_model:",
          "1414:             self.log.info(",
          "1415:                 \"DAG %s scheduling was skipped, probably because the DAG record was locked\", dag_run.dag_id",
          "1416:             )",
          "1417:             return callback",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "699fba373dbf5ae67377de4ca27ebc0d813becac",
      "candidate_info": {
        "commit_hash": "699fba373dbf5ae67377de4ca27ebc0d813becac",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/699fba373dbf5ae67377de4ca27ebc0d813becac",
        "files": [
          "airflow/api_connexion/endpoints/task_instance_endpoint.py",
          "airflow/api_connexion/openapi/v1.yaml",
          "airflow/api_connexion/schemas/task_instance_schema.py",
          "airflow/www/static/js/types/api-generated.ts",
          "tests/api_connexion/endpoints/test_task_instance_endpoint.py"
        ],
        "message": "add dag_run_ids and task_ids filter for the batch task instance API endpoint (#32705)\n\n* add dag_run_ids and task_ids as filter types for the batch task instance endpoint\n\n* add version notice to the new filters\n\n* Update the released version in OpenAPI spec\n\n---------\n\nCo-authored-by: pierrejeambrun <pierrejbrun@gmail.com>\n(cherry picked from commit 08b1e8d749612af469625add5c7f0ad582969c39)",
        "before_after_code_files": [
          "airflow/api_connexion/endpoints/task_instance_endpoint.py||airflow/api_connexion/endpoints/task_instance_endpoint.py",
          "airflow/api_connexion/schemas/task_instance_schema.py||airflow/api_connexion/schemas/task_instance_schema.py",
          "airflow/www/static/js/types/api-generated.ts||airflow/www/static/js/types/api-generated.ts",
          "tests/api_connexion/endpoints/test_task_instance_endpoint.py||tests/api_connexion/endpoints/test_task_instance_endpoint.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api_connexion/endpoints/task_instance_endpoint.py||airflow/api_connexion/endpoints/task_instance_endpoint.py": [
          "File: airflow/api_connexion/endpoints/task_instance_endpoint.py -> airflow/api_connexion/endpoints/task_instance_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "400:     base_query = select(TI).join(TI.dag_run)",
          "402:     base_query = _apply_array_filter(base_query, key=TI.dag_id, values=data[\"dag_ids\"])",
          "403:     base_query = _apply_range_filter(",
          "404:         base_query,",
          "405:         key=DR.execution_date,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "403:     base_query = _apply_array_filter(base_query, key=TI.run_id, values=data[\"dag_run_ids\"])",
          "404:     base_query = _apply_array_filter(base_query, key=TI.task_id, values=data[\"task_ids\"])",
          "",
          "---------------"
        ],
        "airflow/api_connexion/schemas/task_instance_schema.py||airflow/api_connexion/schemas/task_instance_schema.py": [
          "File: airflow/api_connexion/schemas/task_instance_schema.py -> airflow/api_connexion/schemas/task_instance_schema.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "100:     page_offset = fields.Int(load_default=0, validate=validate.Range(min=0))",
          "101:     page_limit = fields.Int(load_default=100, validate=validate.Range(min=1))",
          "102:     dag_ids = fields.List(fields.Str(), load_default=None)",
          "103:     execution_date_gte = fields.DateTime(load_default=None, validate=validate_istimezone)",
          "104:     execution_date_lte = fields.DateTime(load_default=None, validate=validate_istimezone)",
          "105:     start_date_gte = fields.DateTime(load_default=None, validate=validate_istimezone)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "103:     dag_run_ids = fields.List(fields.Str(), load_default=None)",
          "104:     task_ids = fields.List(fields.Str(), load_default=None)",
          "",
          "---------------"
        ],
        "airflow/www/static/js/types/api-generated.ts||airflow/www/static/js/types/api-generated.ts": [
          "File: airflow/www/static/js/types/api-generated.ts -> airflow/www/static/js/types/api-generated.ts",
          "--- Hunk 1 ---",
          "[Context before]",
          "1985:       dag_ids?: string[];",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1991:       dag_run_ids?: string[];",
          "1997:       task_ids?: string[];",
          "",
          "---------------"
        ],
        "tests/api_connexion/endpoints/test_task_instance_endpoint.py||tests/api_connexion/endpoints/test_task_instance_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_task_instance_endpoint.py -> tests/api_connexion/endpoints/test_task_instance_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "778:                 \"test\",",
          "779:                 id=\"with execution date filter\",",
          "780:             ),",
          "781:         ],",
          "782:     )",
          "783:     def test_should_respond_200(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "781:             pytest.param(",
          "782:                 [",
          "783:                     {\"execution_date\": DEFAULT_DATETIME_1},",
          "784:                     {\"execution_date\": DEFAULT_DATETIME_1 + dt.timedelta(days=1)},",
          "785:                     {\"execution_date\": DEFAULT_DATETIME_1 + dt.timedelta(days=2)},",
          "786:                     {\"execution_date\": DEFAULT_DATETIME_1 + dt.timedelta(days=3)},",
          "787:                 ],",
          "788:                 False,",
          "789:                 {",
          "790:                     \"dag_run_ids\": [\"TEST_DAG_RUN_ID_0\", \"TEST_DAG_RUN_ID_1\"],",
          "791:                 },",
          "792:                 2,",
          "793:                 \"test\",",
          "794:                 id=\"test dag run id filter\",",
          "795:             ),",
          "796:             pytest.param(",
          "797:                 [",
          "798:                     {\"execution_date\": DEFAULT_DATETIME_1},",
          "799:                     {\"execution_date\": DEFAULT_DATETIME_1 + dt.timedelta(days=1)},",
          "800:                     {\"execution_date\": DEFAULT_DATETIME_1 + dt.timedelta(days=2)},",
          "801:                     {\"execution_date\": DEFAULT_DATETIME_1 + dt.timedelta(days=3)},",
          "802:                 ],",
          "803:                 False,",
          "804:                 {",
          "805:                     \"task_ids\": [\"print_the_context\", \"log_sql_query\"],",
          "806:                 },",
          "807:                 2,",
          "808:                 \"test\",",
          "809:                 id=\"test task id filter\",",
          "810:             ),",
          "",
          "---------------"
        ]
      }
    }
  ]
}