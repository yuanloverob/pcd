{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "be9fae2e64ed7e611f2ec70b8cca0e2ae403786d",
      "candidate_info": {
        "commit_hash": "be9fae2e64ed7e611f2ec70b8cca0e2ae403786d",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/be9fae2e64ed7e611f2ec70b8cca0e2ae403786d",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameWriterV2Suite.scala"
        ],
        "message": "[SPARK-39543] The option of DataFrameWriterV2 should be passed to storage properties if fallback to v1\n\n### What changes were proposed in this pull request?\n\nThe option of DataFrameWriterV2 should be passed to storage properties if fallback to v1, to support something such as compressed formats\n\n### Why are the changes needed?\n\nexample:\n\n`spark.range(0, 100).writeTo(\"t1\").option(\"compression\", \"zstd\").using(\"parquet\").create`\n\n**before**\n\ngen: part-00000-644a65ed-0e7a-43d5-8d30-b610a0fb19dc-c000.**snappy**.parquet ...\n\n**after**\n\ngen: part-00000-6eb9d1ae-8fdb-4428-aea3-bd6553954cdd-c000.**zstd**.parquet ...\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nnew test\n\nCloses #36941 from Yikf/writeV2option.\n\nAuthored-by: Yikf <yikaifei1@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit e5b7fb85b2d91f2e84dc60888c94e15b53751078)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala||sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameWriterV2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameWriterV2Suite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala||sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala -> sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "158:         c",
          "159:       }",
          "162:         if isSessionCatalog(catalog) =>",
          "163:       val (storageFormat, provider) = getStorageFormatAndProvider(",
          "165:         ctas = true)",
          "166:       if (!isV2Provider(provider)) {",
          "167:         constructV1TableCmd(Some(c.query), c.tableSpec, name, new StructType, c.partitioning,",
          "168:           c.ignoreIfExists, storageFormat, provider)",
          "",
          "[Removed Lines]",
          "161:     case c @ CreateTableAsSelect(ResolvedDBObjectName(catalog, name), _, _, _, _, _)",
          "164:         c.tableSpec.provider, c.tableSpec.options, c.tableSpec.location, c.tableSpec.serde,",
          "",
          "[Added Lines]",
          "161:     case c @ CreateTableAsSelect(ResolvedDBObjectName(catalog, name), _, _, _, writeOptions, _)",
          "164:         c.tableSpec.provider,",
          "165:         c.tableSpec.options ++ writeOptions,",
          "166:         c.tableSpec.location,",
          "167:         c.tableSpec.serde,",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/DataFrameWriterV2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameWriterV2Suite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/DataFrameWriterV2Suite.scala -> sql/core/src/test/scala/org/apache/spark/sql/DataFrameWriterV2Suite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import org.scalatest.BeforeAndAfter",
          "26: import org.apache.spark.sql.catalyst.analysis.{CannotReplaceMissingTableException, TableAlreadyExistsException}",
          "27: import org.apache.spark.sql.catalyst.plans.logical.{AppendData, LogicalPlan, OverwriteByExpression, OverwritePartitionsDynamic}",
          "28: import org.apache.spark.sql.connector.catalog.{Identifier, InMemoryTable, InMemoryTableCatalog, TableCatalog}",
          "29: import org.apache.spark.sql.connector.expressions.{BucketTransform, DaysTransform, FieldReference, HoursTransform, IdentityTransform, LiteralValue, MonthsTransform, YearsTransform}",
          "30: import org.apache.spark.sql.execution.QueryExecution",
          "31: import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation",
          "32: import org.apache.spark.sql.sources.FakeSourceOne",
          "33: import org.apache.spark.sql.test.SharedSparkSession",
          "34: import org.apache.spark.sql.types.{IntegerType, LongType, StringType, StructType, TimestampType}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: import org.apache.spark.sql.catalyst.TableIdentifier",
          "29: import org.apache.spark.sql.connector.InMemoryV1Provider",
          "34: import org.apache.spark.sql.internal.SQLConf",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "531:     assert(table.properties === (Map(\"provider\" -> \"foo\") ++ defaultOwnership).asJava)",
          "532:   }",
          "534:   test(\"Replace: basic behavior\") {",
          "535:     spark.sql(",
          "536:       \"CREATE TABLE testcat.table_name (id bigint, data string) USING foo PARTITIONED BY (id)\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "537:   test(\"SPARK-39543 writeOption should be passed to storage properties when fallback to v1\") {",
          "538:     val provider = classOf[InMemoryV1Provider].getName",
          "540:     withSQLConf((SQLConf.USE_V1_SOURCE_LIST.key, provider)) {",
          "541:       spark.range(10)",
          "542:         .writeTo(\"table_name\")",
          "543:         .option(\"compression\", \"zstd\").option(\"name\", \"table_name\")",
          "544:         .using(provider)",
          "545:         .create()",
          "546:       val table = spark.sessionState.catalog.getTableMetadata(TableIdentifier(\"table_name\"))",
          "548:       assert(table.identifier === TableIdentifier(\"table_name\", Some(\"default\")))",
          "549:       assert(table.storage.properties.contains(\"compression\"))",
          "550:       assert(table.storage.properties.getOrElse(\"compression\", \"foo\") == \"zstd\")",
          "551:     }",
          "552:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d736bec27b3b0b576db006c5ced82c63a5cb5c6b",
      "candidate_info": {
        "commit_hash": "d736bec27b3b0b576db006c5ced82c63a5cb5c6b",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/d736bec27b3b0b576db006c5ced82c63a5cb5c6b",
        "files": [
          "core/src/main/resources/error/error-classes.json",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/SparkSqlParserSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/command/CreateNamespaceParserSuite.scala"
        ],
        "message": "[SPARK-37939][SQL][3.3] Use error classes in the parsing errors of properties\n\n## What changes were proposed in this pull request?\nMigrate the following errors in QueryParsingErrors onto use error classes:\n\n- cannotCleanReservedNamespacePropertyError => UNSUPPORTED_FEATURE\n- cannotCleanReservedTablePropertyError => UNSUPPORTED_FEATURE\n- invalidPropertyKeyForSetQuotedConfigurationError => INVALID_PROPERTY_KEY\n- invalidPropertyValueForSetQuotedConfigurationError => INVALID_PROPERTY_VALUE\n- propertiesAndDbPropertiesBothSpecifiedError => UNSUPPORTED_FEATURE\n\nThis is a backport of https://github.com/apache/spark/pull/36561.\n\n### Why are the changes needed?\nPorting parsing errors of partitions to new error framework, improve test coverage, and document expected error messages in tests.\n\n### Does this PR introduce any user-facing change?\nNo\n\n### How was this patch tested?\nBy running new test:\n```\n$ build/sbt \"sql/testOnly *QueryParsingErrorsSuite*\"\n```\n\nCloses #36916 from panbingkun/branch-3.3-SPARK-37939.\n\nAuthored-by: panbingkun <pbk1982@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/SparkSqlParserSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/SparkSqlParserSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/command/CreateNamespaceParserSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/command/CreateNamespaceParserSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "260:   }",
          "262:   def cannotCleanReservedNamespacePropertyError(",
          "265:   }",
          "269:   }",
          "271:   def cannotCleanReservedTablePropertyError(",
          "274:   }",
          "276:   def duplicatedTablePathsFoundError(",
          "",
          "[Removed Lines]",
          "263:       property: String, ctx: ParserRuleContext, msg: String): Throwable = {",
          "264:     new ParseException(s\"$property is a reserved namespace property, $msg.\", ctx)",
          "267:   def propertiesAndDbPropertiesBothSpecifiedError(ctx: CreateNamespaceContext): Throwable = {",
          "268:     new ParseException(\"Either PROPERTIES or DBPROPERTIES is allowed.\", ctx)",
          "272:       property: String, ctx: ParserRuleContext, msg: String): Throwable = {",
          "273:     new ParseException(s\"$property is a reserved table property, $msg.\", ctx)",
          "",
          "[Added Lines]",
          "263:       property: String, ctx: ParserRuleContext, msg: String): ParseException = {",
          "264:     new ParseException(\"UNSUPPORTED_FEATURE\",",
          "265:       Array(s\"$property is a reserved namespace property, $msg.\"), ctx)",
          "268:   def propertiesAndDbPropertiesBothSpecifiedError(ctx: CreateNamespaceContext): ParseException = {",
          "269:     new ParseException(\"UNSUPPORTED_FEATURE\",",
          "270:       Array(\"set PROPERTIES and DBPROPERTIES at the same time.\"), ctx)",
          "274:       property: String, ctx: ParserRuleContext, msg: String): ParseException = {",
          "275:     new ParseException(\"UNSUPPORTED_FEATURE\",",
          "276:       Array(s\"$property is a reserved table property, $msg.\"), ctx)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "367:   }",
          "369:   def invalidPropertyKeyForSetQuotedConfigurationError(",
          "373:   }",
          "375:   def invalidPropertyValueForSetQuotedConfigurationError(",
          "379:   }",
          "381:   def unexpectedFormatForResetConfigurationError(ctx: ResetConfigurationContext): Throwable = {",
          "",
          "[Removed Lines]",
          "370:       keyCandidate: String, valueStr: String, ctx: ParserRuleContext): Throwable = {",
          "371:     new ParseException(s\"'$keyCandidate' is an invalid property key, please \" +",
          "372:       s\"use quotes, e.g. SET `$keyCandidate`=`$valueStr`\", ctx)",
          "376:       valueCandidate: String, keyStr: String, ctx: ParserRuleContext): Throwable = {",
          "377:     new ParseException(s\"'$valueCandidate' is an invalid property value, please \" +",
          "378:       s\"use quotes, e.g. SET `$keyStr`=`$valueCandidate`\", ctx)",
          "",
          "[Added Lines]",
          "373:       keyCandidate: String, valueStr: String, ctx: ParserRuleContext): ParseException = {",
          "374:     new ParseException(errorClass = \"INVALID_PROPERTY_KEY\",",
          "375:       messageParameters = Array(toSQLConf(keyCandidate),",
          "376:         toSQLConf(keyCandidate), toSQLConf(valueStr)), ctx)",
          "380:       valueCandidate: String, keyStr: String, ctx: ParserRuleContext): ParseException = {",
          "381:     new ParseException(errorClass = \"INVALID_PROPERTY_VALUE\",",
          "382:       messageParameters = Array(toSQLConf(valueCandidate),",
          "383:         toSQLConf(keyStr), toSQLConf(valueCandidate)), ctx)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "213:           |--------------------------------------------^^^",
          "214:           |\"\"\".stripMargin)",
          "215:   }",
          "216: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "217:   test(\"UNSUPPORTED_FEATURE: cannot set reserved namespace property\") {",
          "218:     val sql = \"CREATE NAMESPACE IF NOT EXISTS a.b.c WITH PROPERTIES ('location'='/home/user/db')\"",
          "219:     val msg = \"\"\"The feature is not supported: location is a reserved namespace property, \"\"\" +",
          "220:       \"\"\"please use the LOCATION clause to specify it.(line 1, pos 0)\"\"\"",
          "221:     validateParsingError(",
          "222:       sqlText = sql,",
          "223:       errorClass = \"UNSUPPORTED_FEATURE\",",
          "224:       sqlState = \"0A000\",",
          "225:       message =",
          "226:         s\"\"\"",
          "227:            |$msg",
          "228:            |",
          "229:            |== SQL ==",
          "230:            |$sql",
          "231:            |^^^",
          "232:            |\"\"\".stripMargin)",
          "233:   }",
          "235:   test(\"UNSUPPORTED_FEATURE: cannot set reserved table property\") {",
          "236:     val sql = \"CREATE TABLE student (id INT, name STRING, age INT) \" +",
          "237:       \"USING PARQUET TBLPROPERTIES ('provider'='parquet')\"",
          "238:     val msg = \"\"\"The feature is not supported: provider is a reserved table property, \"\"\" +",
          "239:       \"\"\"please use the USING clause to specify it.(line 1, pos 66)\"\"\"",
          "240:     validateParsingError(",
          "241:       sqlText = sql,",
          "242:       errorClass = \"UNSUPPORTED_FEATURE\",",
          "243:       sqlState = \"0A000\",",
          "244:       message =",
          "245:         s\"\"\"",
          "246:            |$msg",
          "247:            |",
          "248:            |== SQL ==",
          "249:            |$sql",
          "250:            |------------------------------------------------------------------^^^",
          "251:            |\"\"\".stripMargin)",
          "252:   }",
          "254:   test(\"INVALID_PROPERTY_KEY: invalid property key for set quoted configuration\") {",
          "255:     val sql = \"set =`value`\"",
          "256:     val msg = \"\"\"\"\" is an invalid property key, please use quotes, \"\"\" +",
          "257:       \"\"\"e.g. SET \"\"=\"value\"(line 1, pos 0)\"\"\"",
          "258:     validateParsingError(",
          "259:       sqlText = sql,",
          "260:       errorClass = \"INVALID_PROPERTY_KEY\",",
          "261:       sqlState = null,",
          "262:       message =",
          "263:         s\"\"\"",
          "264:            |$msg",
          "265:            |",
          "266:            |== SQL ==",
          "267:            |$sql",
          "268:            |^^^",
          "269:            |\"\"\".stripMargin)",
          "270:   }",
          "272:   test(\"INVALID_PROPERTY_VALUE: invalid property value for set quoted configuration\") {",
          "273:     val sql = \"set `key`=1;2;;\"",
          "274:     val msg = \"\"\"\"1;2;;\" is an invalid property value, please use quotes, \"\"\" +",
          "275:       \"\"\"e.g. SET \"key\"=\"1;2;;\"(line 1, pos 0)\"\"\"",
          "276:     validateParsingError(",
          "277:       sqlText = sql,",
          "278:       errorClass = \"INVALID_PROPERTY_VALUE\",",
          "279:       sqlState = null,",
          "280:       message =",
          "281:         s\"\"\"",
          "282:            |$msg",
          "283:            |",
          "284:            |== SQL ==",
          "285:            |$sql",
          "286:            |^^^",
          "287:            |\"\"\".stripMargin)",
          "288:   }",
          "290:   test(\"UNSUPPORTED_FEATURE: cannot set Properties and DbProperties at the same time\") {",
          "291:     val sql = \"CREATE NAMESPACE IF NOT EXISTS a.b.c WITH PROPERTIES ('a'='a', 'b'='b', 'c'='c') \" +",
          "292:       \"WITH DBPROPERTIES('a'='a', 'b'='b', 'c'='c')\"",
          "293:     val msg = \"\"\"The feature is not supported: set PROPERTIES and DBPROPERTIES at the same time.\"\"\" +",
          "294:       \"\"\"(line 1, pos 0)\"\"\"",
          "295:     validateParsingError(",
          "296:       sqlText = sql,",
          "297:       errorClass = \"UNSUPPORTED_FEATURE\",",
          "298:       sqlState = \"0A000\",",
          "299:       message =",
          "300:         s\"\"\"",
          "301:            |$msg",
          "302:            |",
          "303:            |== SQL ==",
          "304:            |$sql",
          "305:            |^^^",
          "306:            |\"\"\".stripMargin)",
          "307:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/SparkSqlParserSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/SparkSqlParserSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/SparkSqlParserSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/SparkSqlParserSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "168:     intercept(\"SET a=1;2;;\", expectedErrMsg)",
          "170:     intercept(\"SET a b=`1;;`\",",
          "173:     intercept(\"SET `a`=1;2;;\",",
          "176:   }",
          "178:   test(\"refresh resource\") {",
          "",
          "[Removed Lines]",
          "171:       \"'a b' is an invalid property key, please use quotes, e.g. SET `a b`=`1;;`\")",
          "174:       \"'1;2;;' is an invalid property value, please use quotes, e.g.\" +",
          "175:         \" SET `a`=`1;2;;`\")",
          "",
          "[Added Lines]",
          "171:       \"\\\"a b\\\" is an invalid property key, please use quotes, e.g. SET \\\"a b\\\"=\\\"1;;\\\"\")",
          "174:       \"\\\"1;2;;\\\" is an invalid property value, please use quotes, e.g.\" +",
          "175:         \" SET \\\"a\\\"=\\\"1;2;;\\\"\")",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/command/CreateNamespaceParserSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/command/CreateNamespaceParserSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/command/CreateNamespaceParserSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/command/CreateNamespaceParserSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "84:          |WITH PROPERTIES ('a'='a', 'b'='b', 'c'='c')",
          "85:          |WITH DBPROPERTIES ('a'='a', 'b'='b', 'c'='c')",
          "86:       \"\"\".stripMargin",
          "88:   }",
          "90:   test(\"create namespace - support for other types in PROPERTIES\") {",
          "",
          "[Removed Lines]",
          "87:     intercept(sql, \"Either PROPERTIES or DBPROPERTIES is allowed\")",
          "",
          "[Added Lines]",
          "87:     intercept(sql, \"The feature is not supported: \" +",
          "88:       \"set PROPERTIES and DBPROPERTIES at the same time.\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4d5c85edf78853bf9a21be86bec004a892d1a842",
      "candidate_info": {
        "commit_hash": "4d5c85edf78853bf9a21be86bec004a892d1a842",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/4d5c85edf78853bf9a21be86bec004a892d1a842",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala"
        ],
        "message": "[SPARK-37013][SQL][FOLLOWUP] Add legacy flag for the breaking change of forbidding %0$ usage in format_string\n\n### What changes were proposed in this pull request?\nAdds a legacy flag `spark.sql.legacy.allowZeroIndexInFormatString` for the breaking change introduced in https://github.com/apache/spark/pull/34313 and https://github.com/apache/spark/pull/34454 (followup).\n\nThe flag is disabled by default. But when it is enabled, restore the pre-change behavior that allows the 0 based index in `format_string`.\n\n### Why are the changes needed?\nThe original commit is a breaking change, and breaking changes should be encouraged to add a flag to turn it off for smooth migration between versions.\n\n### Does this PR introduce _any_ user-facing change?\nWith the default value of the conf, there is no user-facing difference.\nIf users turn this conf on, they can restore the pre-change behavior.\n\n### How was this patch tested?\nThrough unit tests.\n\nCloses #36101 from anchovYu/flags-format-string-java.\n\nAuthored-by: Xinyi Yu <xinyi.yu@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit b7af2b3bac1a6d57e98c46831aa37a250d812c70)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1808: case class FormatString(children: Expression*) extends Expression with ImplicitCastInputTypes {",
          "1810:   require(children.nonEmpty, s\"$prettyName() should take at least 1 argument\")",
          "1814:   override def foldable: Boolean = children.forall(_.foldable)",
          "",
          "[Removed Lines]",
          "1811:   checkArgumentIndexNotZero(children(0))",
          "",
          "[Added Lines]",
          "1811:   if (!SQLConf.get.getConf(SQLConf.ALLOW_ZERO_INDEX_IN_FORMAT_STRING)) {",
          "1812:     checkArgumentIndexNotZero(children(0))",
          "1813:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2015:       .booleanConf",
          "2016:       .createWithDefault(false)",
          "2018:   val USE_CURRENT_SQL_CONFIGS_FOR_VIEW =",
          "2019:     buildConf(\"spark.sql.legacy.useCurrentConfigsForView\")",
          "2020:       .internal()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2018:   val ALLOW_ZERO_INDEX_IN_FORMAT_STRING =",
          "2019:     buildConf(\"spark.sql.legacy.allowZeroIndexInFormatString\")",
          "2020:       .internal()",
          "2021:       .doc(\"When false, the `strfmt` in `format_string(strfmt, obj, ...)` and \" +",
          "2022:         \"`printf(strfmt, obj, ...)` will no longer support to use \\\"0$\\\" to specify the first \" +",
          "2023:         \"argument, the first argument should always reference by \\\"1$\\\" when use argument index \" +",
          "2024:         \"to indicating the position of the argument in the argument list.\")",
          "2025:       .version(\"3.3\")",
          "2026:       .booleanConf",
          "2027:       .createWithDefault(false)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import org.apache.spark.sql.{AnalysisException, IntegratedUDFTestUtils, QueryTest}",
          "21: import org.apache.spark.sql.functions.{grouping, grouping_id, sum}",
          "22: import org.apache.spark.sql.test.SharedSparkSession",
          "24: case class StringLongClass(a: String, b: Long)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: import org.apache.spark.sql.internal.SQLConf",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "94:   }",
          "96:   test(\"ILLEGAL_SUBSTRING: the argument_index of string format is invalid\") {",
          "99:     }",
          "103:   }",
          "105:   test(\"CANNOT_USE_MIXTURE: Using aggregate function with grouped aggregate pandas UDF\") {",
          "",
          "[Removed Lines]",
          "97:     val e = intercept[AnalysisException] {",
          "98:       sql(\"select format_string('%0$s', 'Hello')\")",
          "100:     assert(e.errorClass === Some(\"ILLEGAL_SUBSTRING\"))",
          "101:     assert(e.message ===",
          "102:       \"The argument_index of string format cannot contain position 0$.\")",
          "",
          "[Added Lines]",
          "98:     withSQLConf(SQLConf.ALLOW_ZERO_INDEX_IN_FORMAT_STRING.key -> \"false\") {",
          "99:       val e = intercept[AnalysisException] {",
          "100:         sql(\"select format_string('%0$s', 'Hello')\")",
          "101:       }",
          "102:       assert(e.errorClass === Some(\"ILLEGAL_SUBSTRING\"))",
          "103:       assert(e.message ===",
          "104:         \"The argument_index of string format cannot contain position 0$.\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cd9f5642060cea344b6c84dde19de3e96836da19",
      "candidate_info": {
        "commit_hash": "cd9f5642060cea344b6c84dde19de3e96836da19",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/cd9f5642060cea344b6c84dde19de3e96836da19",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/types/DecimalType.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuiteBase.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ],
        "message": "[SPARK-40389][SQL] Decimals can't upcast as integral types if the cast can overflow\n\n### What changes were proposed in this pull request?\n\nIn Spark SQL, the method `canUpCast` returns true iff we can safely up-cast the `from` type to `to` type without truncating or precision loss or possible runtime failures.\n\nMeanwhile, DecimalType(10, 0) is considered as `canUpCast` to Integer type. This is wrong since casting `9000000000BD` as Integer type will overflow.\n\nAs a result:\n\n* The optimizer rule `SimplifyCasts` replies on the method `canUpCast` and it will mistakenly convert `cast(cast(9000000000BD as int) as long)` as `cast(9000000000BD as long)`\n* The STRICT store assignment policy relies on this method too. With the policy enabled, inserting `9000000000BD` into integer columns will pass compiling time check and insert an unexpected value `410065408`.\n* etc...\n\n### Why are the changes needed?\n\nBug fix on the method `Cast.canUpCast`\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, fix a bug on the checking condition of whether a decimal can safely cast as integral types.\n\n### How was this patch tested?\n\nNew UT\n\nCloses #37832 from gengliangwang/SPARK-40389.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>\n(cherry picked from commit 17982519a749bd4ca2aa7eca12fba00ccc1520aa)\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/types/DecimalType.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/types/DecimalType.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuiteBase.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuiteBase.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/types/DecimalType.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/types/DecimalType.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/types/DecimalType.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/types/DecimalType.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "99:     case dt: DecimalType =>",
          "100:       (precision - scale) <= (dt.precision - dt.scale) && scale <= dt.scale",
          "101:     case dt: IntegralType =>",
          "103:     case _ => false",
          "104:   }",
          "",
          "[Removed Lines]",
          "95:   private[sql] def isTighterThan(other: DataType): Boolean = isTighterThanInternal(other)",
          "97:   @tailrec",
          "98:   private def isTighterThanInternal(other: DataType): Boolean = other match {",
          "102:       isTighterThanInternal(DecimalType.forType(dt))",
          "",
          "[Added Lines]",
          "95:   private[sql] def isTighterThan(other: DataType): Boolean = other match {",
          "99:       val integerAsDecimal = DecimalType.forType(dt)",
          "100:       assert(integerAsDecimal.scale == 0)",
          "103:       precision < integerAsDecimal.precision && scale == 0",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuiteBase.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuiteBase.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuiteBase.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuiteBase.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "671:     }",
          "672:   }",
          "674:   test(\"SPARK-27671: cast from nested null type in struct\") {",
          "675:     import DataTypeTestUtils._",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "674:   test(\"SPARK-40389: canUpCast: return false if casting decimal to integral types can cause\" +",
          "675:     \" overflow\") {",
          "676:     Seq(ByteType, ShortType, IntegerType, LongType).foreach { integralType =>",
          "677:       val decimalType = DecimalType.forType(integralType)",
          "678:       assert(!Cast.canUpCast(decimalType, integralType))",
          "679:       assert(Cast.canUpCast(integralType, decimalType))",
          "680:     }",
          "681:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "4551:       sql(\"select * from test_temp_view\"),",
          "4552:       Row(1, 2, 3, 1, 2, 3, 1, 1))",
          "4553:   }",
          "4554: }",
          "4556: case class Foo(bar: Option[String])",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4555:   test(\"SPARK-40389: Don't eliminate a cast which can cause overflow\") {",
          "4556:     withSQLConf(SQLConf.ANSI_ENABLED.key -> \"true\") {",
          "4557:       withTable(\"dt\") {",
          "4558:         sql(\"create table dt using parquet as select 9000000000BD as d\")",
          "4559:         val msg = intercept[SparkException] {",
          "4560:           sql(\"select cast(cast(d as int) as long) from dt\").collect()",
          "4561:         }.getCause.getMessage",
          "4562:         assert(msg.contains(\"[CAST_OVERFLOW]\"))",
          "4563:       }",
          "4564:     }",
          "4565:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "630dc7e34f3da642451d9f7904f75370f0fbc84a",
      "candidate_info": {
        "commit_hash": "630dc7e34f3da642451d9f7904f75370f0fbc84a",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/630dc7e34f3da642451d9f7904f75370f0fbc84a",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/binaryfile/BinaryFileFormat.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/binaryfile/BinaryFileFormatSuite.scala"
        ],
        "message": "[SPARK-39900][SQL] Address partial or negated condition in binary format's predicate pushdown\n\n### What changes were proposed in this pull request?\n\nfix `BinaryFileFormat` filter push down bug.\n\nBefore modification, when Filter tree is:\n````\n-Not\n- - IsNotNull\n````\n\nSince `IsNotNull` cannot be matched, `IsNotNull` will return a result that is always true (that is, `case _ => (_ => true)`), that is, no filter pushdown is performed. But because there is still a `Not`, after negation, it will return a result that is always False, that is, no result can be returned.\n\n### Why are the changes needed?\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\ntest suit in `BinaryFileFormatSuite`\n\n```\n    testCreateFilterFunction(\n      Seq(Not(IsNull(LENGTH))),\n      Seq((t1, true), (t2, true), (t3, true)))\n```\n\nCloses #37350 from zzzzming95/SPARK-39900.\n\nLead-authored-by: zzzzming95 <505306252@qq.com>\nCo-authored-by: Hyukjin Kwon <gurwls223@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit a0dc7d9117b66426aaa2257c8d448a2f96882ecd)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/binaryfile/BinaryFileFormat.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/binaryfile/BinaryFileFormat.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/binaryfile/BinaryFileFormatSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/binaryfile/BinaryFileFormatSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/binaryfile/BinaryFileFormat.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/binaryfile/BinaryFileFormat.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/binaryfile/BinaryFileFormat.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/binaryfile/BinaryFileFormat.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "98:     val broadcastedHadoopConf =",
          "99:       sparkSession.sparkContext.broadcast(new SerializableConfiguration(hadoopConf))",
          "101:     val maxLength = sparkSession.conf.get(SOURCES_BINARY_FILE_MAX_LENGTH)",
          "103:     file: PartitionedFile => {",
          "",
          "[Removed Lines]",
          "100:     val filterFuncs = filters.map(filter => createFilterFunction(filter))",
          "",
          "[Added Lines]",
          "100:     val filterFuncs = filters.flatMap(filter => createFilterFunction(filter))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "158:     StructField(LENGTH, LongType, false) ::",
          "159:     StructField(CONTENT, BinaryType, true) :: Nil)",
          "162:     filter match {",
          "181:       case LessThan(MODIFICATION_TIME, value: Timestamp) =>",
          "183:       case LessThanOrEqual(MODIFICATION_TIME, value: Timestamp) =>",
          "185:       case GreaterThan(MODIFICATION_TIME, value: Timestamp) =>",
          "187:       case GreaterThanOrEqual(MODIFICATION_TIME, value: Timestamp) =>",
          "189:       case EqualTo(MODIFICATION_TIME, value: Timestamp) =>",
          "193:     }",
          "194:   }",
          "195: }",
          "",
          "[Removed Lines]",
          "161:   private[binaryfile] def createFilterFunction(filter: Filter): FileStatus => Boolean = {",
          "163:       case And(left, right) =>",
          "164:         s => createFilterFunction(left)(s) && createFilterFunction(right)(s)",
          "165:       case Or(left, right) =>",
          "166:         s => createFilterFunction(left)(s) || createFilterFunction(right)(s)",
          "167:       case Not(child) =>",
          "168:         s => !createFilterFunction(child)(s)",
          "170:       case LessThan(LENGTH, value: Long) =>",
          "171:         _.getLen < value",
          "172:       case LessThanOrEqual(LENGTH, value: Long) =>",
          "173:         _.getLen <= value",
          "174:       case GreaterThan(LENGTH, value: Long) =>",
          "175:         _.getLen > value",
          "176:       case GreaterThanOrEqual(LENGTH, value: Long) =>",
          "177:         _.getLen >= value",
          "178:       case EqualTo(LENGTH, value: Long) =>",
          "179:         _.getLen == value",
          "182:         _.getModificationTime < value.getTime",
          "184:         _.getModificationTime <= value.getTime",
          "186:         _.getModificationTime > value.getTime",
          "188:         _.getModificationTime >= value.getTime",
          "190:         _.getModificationTime == value.getTime",
          "192:       case _ => (_ => true)",
          "",
          "[Added Lines]",
          "161:   private[binaryfile] def createFilterFunction(filter: Filter): Option[FileStatus => Boolean] = {",
          "163:       case And(left, right) => (createFilterFunction(left), createFilterFunction(right)) match {",
          "164:         case (Some(leftPred), Some(rightPred)) => Some(s => leftPred(s) && rightPred(s))",
          "165:         case (Some(leftPred), None) => Some(leftPred)",
          "166:         case (None, Some(rightPred)) => Some(rightPred)",
          "167:         case (None, None) => Some(_ => true)",
          "168:       }",
          "169:       case Or(left, right) => (createFilterFunction(left), createFilterFunction(right)) match {",
          "170:         case (Some(leftPred), Some(rightPred)) => Some(s => leftPred(s) || rightPred(s))",
          "171:         case _ => Some(_ => true)",
          "172:       }",
          "173:       case Not(child) => createFilterFunction(child) match {",
          "174:         case Some(pred) => Some(s => !pred(s))",
          "175:         case _ => Some(_ => true)",
          "176:       }",
          "177:       case LessThan(LENGTH, value: Long) => Some(_.getLen < value)",
          "178:       case LessThanOrEqual(LENGTH, value: Long) => Some(_.getLen <= value)",
          "179:       case GreaterThan(LENGTH, value: Long) => Some(_.getLen > value)",
          "180:       case GreaterThanOrEqual(LENGTH, value: Long) => Some(_.getLen >= value)",
          "181:       case EqualTo(LENGTH, value: Long) => Some(_.getLen == value)",
          "183:         Some(_.getModificationTime < value.getTime)",
          "185:         Some(_.getModificationTime <= value.getTime)",
          "187:         Some(_.getModificationTime > value.getTime)",
          "189:         Some(_.getModificationTime >= value.getTime)",
          "191:         Some(_.getModificationTime == value.getTime)",
          "192:       case _ => None",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/binaryfile/BinaryFileFormatSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/binaryfile/BinaryFileFormatSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/binaryfile/BinaryFileFormatSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/binaryfile/BinaryFileFormatSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "183:   def testCreateFilterFunction(",
          "184:       filters: Seq[Filter],",
          "185:       testCases: Seq[(FileStatus, Boolean)]): Unit = {",
          "187:     testCases.foreach { case (status, expected) =>",
          "188:       assert(funcs.forall(f => f(status)) === expected,",
          "189:         s\"$filters applied to $status should be $expected.\")",
          "",
          "[Removed Lines]",
          "186:     val funcs = filters.map(BinaryFileFormat.createFilterFunction)",
          "",
          "[Added Lines]",
          "186:     val funcs = filters.flatMap(BinaryFileFormat.createFilterFunction)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "250:       Seq(Or(LessThanOrEqual(MODIFICATION_TIME, new Timestamp(1L)),",
          "251:         GreaterThanOrEqual(MODIFICATION_TIME, new Timestamp(3L)))),",
          "252:       Seq((t1, true), (t2, false), (t3, true)))",
          "255:     testCreateFilterFunction(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "253:     testCreateFilterFunction(",
          "254:       Seq(Not(IsNull(LENGTH))),",
          "255:       Seq((t1, true), (t2, true), (t3, true)))",
          "",
          "---------------"
        ]
      }
    }
  ]
}