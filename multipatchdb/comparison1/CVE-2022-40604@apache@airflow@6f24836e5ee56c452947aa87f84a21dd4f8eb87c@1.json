{
  "cve_id": "CVE-2022-40604",
  "cve_desc": "In Apache Airflow 2.3.0 through 2.3.4, part of a url was unnecessarily formatted, allowing for possible information extraction.",
  "repo": "apache/airflow",
  "patch_hash": "6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
  "patch_info": {
    "commit_hash": "6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
    "files": [
      "airflow/utils/log/file_task_handler.py"
    ],
    "message": "Properly build URL to retrieve logs independently from system (#26337)\n\nThe previous way of building the path depended on the OS path\nbut it was really used to build the URL so we should use\nurllib instead of os.path.join.\n\n(cherry picked from commit 18386026c28939fa6d91d198c5489c295a05dcd2)",
    "before_after_code_files": [
      "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
    ]
  },
  "patch_diff": {
    "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
      "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "21: import warnings",
      "22: from pathlib import Path",
      "23: from typing import TYPE_CHECKING, Optional",
      "25: from airflow.configuration import AirflowConfigException, conf",
      "26: from airflow.exceptions import RemovedInAirflow3Warning",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "24: from urllib.parse import urljoin",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "194:         else:",
      "195:             import httpx",
      "199:             )",
      "200:             log += f\"*** Log file does not exist: {location}\\n\"",
      "201:             log += f\"*** Fetching from: {url}\\n\"",
      "",
      "[Removed Lines]",
      "197:             url = os.path.join(\"http://{ti.hostname}:{worker_log_server_port}/log\", log_relative_path).format(",
      "198:                 ti=ti, worker_log_server_port=conf.get('logging', 'WORKER_LOG_SERVER_PORT')",
      "",
      "[Added Lines]",
      "198:             url = urljoin(",
      "199:                 f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log\", log_relative_path",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "d36dcc1f581ea2d55ccbe7bfc340c2dfa4cc1fdc",
      "candidate_info": {
        "commit_hash": "d36dcc1f581ea2d55ccbe7bfc340c2dfa4cc1fdc",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/d36dcc1f581ea2d55ccbe7bfc340c2dfa4cc1fdc",
        "files": [
          "airflow/executors/executor_loader.py",
          "airflow/migrations/versions/0038_1_10_2_add_sm_dag_index.py",
          "airflow/migrations/versions/0108_2_3_0_default_dag_view_grid.py",
          "airflow/providers/amazon/aws/transfers/redshift_to_s3.py",
          "airflow/providers/databricks/hooks/databricks_base.py",
          "airflow/providers/google/cloud/hooks/cloud_memorystore.py",
          "airflow/providers/mysql/hooks/mysql.py",
          "airflow/providers/qubole/sensors/qubole.py",
          "airflow/utils/process_utils.py",
          "docs/apache-airflow/migrations-ref.rst"
        ],
        "message": "D400 first line should end with period batch02 (#25268)\n\n(cherry picked from commit 50668445137e4037bb4a3b652bec22e53d1eddd7)",
        "before_after_code_files": [
          "airflow/executors/executor_loader.py||airflow/executors/executor_loader.py",
          "airflow/migrations/versions/0038_1_10_2_add_sm_dag_index.py||airflow/migrations/versions/0038_1_10_2_add_sm_dag_index.py",
          "airflow/migrations/versions/0108_2_3_0_default_dag_view_grid.py||airflow/migrations/versions/0108_2_3_0_default_dag_view_grid.py",
          "airflow/providers/amazon/aws/transfers/redshift_to_s3.py||airflow/providers/amazon/aws/transfers/redshift_to_s3.py",
          "airflow/providers/databricks/hooks/databricks_base.py||airflow/providers/databricks/hooks/databricks_base.py",
          "airflow/providers/google/cloud/hooks/cloud_memorystore.py||airflow/providers/google/cloud/hooks/cloud_memorystore.py",
          "airflow/providers/mysql/hooks/mysql.py||airflow/providers/mysql/hooks/mysql.py",
          "airflow/providers/qubole/sensors/qubole.py||airflow/providers/qubole/sensors/qubole.py",
          "airflow/utils/process_utils.py||airflow/utils/process_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/executors/executor_loader.py||airflow/executors/executor_loader.py": [
          "File: airflow/executors/executor_loader.py -> airflow/executors/executor_loader.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "66:     @classmethod",
          "67:     def get_default_executor(cls) -> \"BaseExecutor\":",
          "69:         if cls._default_executor is not None:",
          "70:             return cls._default_executor",
          "",
          "[Removed Lines]",
          "68:         \"\"\"Creates a new instance of the configured executor if none exists and returns it\"\"\"",
          "",
          "[Added Lines]",
          "68:         \"\"\"Creates a new instance of the configured executor if none exists and returns it.\"\"\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "135:     @classmethod",
          "136:     def __load_celery_kubernetes_executor(cls) -> \"BaseExecutor\":",
          "138:         celery_executor = import_string(cls.executors[CELERY_EXECUTOR])()",
          "139:         kubernetes_executor = import_string(cls.executors[KUBERNETES_EXECUTOR])()",
          "",
          "[Removed Lines]",
          "137:         \"\"\":return: an instance of CeleryKubernetesExecutor\"\"\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "144:     @classmethod",
          "145:     def __load_local_kubernetes_executor(cls) -> \"BaseExecutor\":",
          "147:         local_executor = import_string(cls.executors[LOCAL_EXECUTOR])()",
          "148:         kubernetes_executor = import_string(cls.executors[KUBERNETES_EXECUTOR])()",
          "",
          "[Removed Lines]",
          "146:         \"\"\":return: an instance of LocalKubernetesExecutor\"\"\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0038_1_10_2_add_sm_dag_index.py||airflow/migrations/versions/0038_1_10_2_add_sm_dag_index.py": [
          "File: airflow/migrations/versions/0038_1_10_2_add_sm_dag_index.py -> airflow/migrations/versions/0038_1_10_2_add_sm_dag_index.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "20: Revision ID: 03bc53e68815",
          "21: Revises: 0a2a5b66e19d, bf00311e1990",
          "",
          "[Removed Lines]",
          "18: \"\"\"Merge migrations Heads",
          "",
          "[Added Lines]",
          "18: \"\"\"Merge migrations Heads.",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0108_2_3_0_default_dag_view_grid.py||airflow/migrations/versions/0108_2_3_0_default_dag_view_grid.py": [
          "File: airflow/migrations/versions/0108_2_3_0_default_dag_view_grid.py -> airflow/migrations/versions/0108_2_3_0_default_dag_view_grid.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "21: Revision ID: b1b348e02d07",
          "22: Revises: 75d5ed6c2b43",
          "",
          "[Removed Lines]",
          "19: \"\"\"Update dag.default_view to grid",
          "",
          "[Added Lines]",
          "19: \"\"\"Update dag.default_view to grid.",
          "",
          "---------------"
        ],
        "airflow/providers/amazon/aws/transfers/redshift_to_s3.py||airflow/providers/amazon/aws/transfers/redshift_to_s3.py": [
          "File: airflow/providers/amazon/aws/transfers/redshift_to_s3.py -> airflow/providers/amazon/aws/transfers/redshift_to_s3.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: class RedshiftToS3Operator(BaseOperator):",
          "31:     \"\"\"",
          "34:     .. seealso::",
          "35:         For more information on how to use this operator, take a look at the guide:",
          "",
          "[Removed Lines]",
          "32:     Executes an UNLOAD command to s3 as a CSV with headers",
          "",
          "[Added Lines]",
          "32:     Execute an UNLOAD command to s3 as a CSV with headers.",
          "",
          "---------------"
        ],
        "airflow/providers/databricks/hooks/databricks_base.py||airflow/providers/databricks/hooks/databricks_base.py": [
          "File: airflow/providers/databricks/hooks/databricks_base.py -> airflow/providers/databricks/hooks/databricks_base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "169:     @staticmethod",
          "170:     def _parse_host(host: str) -> str:",
          "171:         \"\"\"",
          "175:         For example -- when users supply ``https://xx.cloud.databricks.com`` as the",
          "176:         host, we must strip out the protocol to get the host.::",
          "",
          "[Removed Lines]",
          "172:         The purpose of this function is to be robust to improper connections",
          "173:         settings provided by users, specifically in the host field.",
          "",
          "[Added Lines]",
          "172:         This function is resistant to incorrect connection settings provided by users, in the host field.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "196:     def _get_retry_object(self) -> Retrying:",
          "197:         \"\"\"",
          "199:         :return: instance of Retrying class",
          "200:         \"\"\"",
          "201:         return Retrying(**self.retry_args)",
          "203:     def _a_get_retry_object(self) -> AsyncRetrying:",
          "204:         \"\"\"",
          "206:         :return: instance of AsyncRetrying class",
          "207:         \"\"\"",
          "208:         return AsyncRetrying(**self.retry_args)",
          "210:     def _get_aad_token(self, resource: str) -> str:",
          "211:         \"\"\"",
          "213:         :param resource: resource to issue token to",
          "214:         :return: AAD token, or raise an exception",
          "215:         \"\"\"",
          "",
          "[Removed Lines]",
          "198:         Instantiates a retry object",
          "205:         Instantiates an async retry object",
          "212:         Function to get AAD token for given resource. Supports managed identity or service principal auth",
          "",
          "[Added Lines]",
          "197:         Instantiate a retry object.",
          "204:         Instantiate an async retry object.",
          "211:         Function to get AAD token for given resource.",
          "213:         Supports managed identity or service principal auth.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "341:     def _get_aad_headers(self) -> dict:",
          "342:         \"\"\"",
          "344:         :return: dictionary with filled AAD headers",
          "345:         \"\"\"",
          "346:         headers = {}",
          "",
          "[Removed Lines]",
          "343:         Fills AAD headers if necessary (SPN is outside of the workspace)",
          "",
          "[Added Lines]",
          "344:         Fill AAD headers if necessary (SPN is outside of the workspace).",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "369:     @staticmethod",
          "370:     def _is_aad_token_valid(aad_token: dict) -> bool:",
          "371:         \"\"\"",
          "373:         :param aad_token: dict with properties of AAD token",
          "374:         :return: true if token is valid, false otherwise",
          "375:         :rtype: bool",
          "",
          "[Removed Lines]",
          "372:         Utility function to check AAD token hasn't expired yet",
          "",
          "[Added Lines]",
          "373:         Utility function to check AAD token hasn't expired yet.",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "382:     @staticmethod",
          "383:     def _check_azure_metadata_service() -> None:",
          "384:         \"\"\"",
          "386:         https://docs.microsoft.com/en-us/azure/virtual-machines/linux/instance-metadata-service",
          "387:         \"\"\"",
          "388:         try:",
          "",
          "[Removed Lines]",
          "385:         Check for Azure Metadata Service",
          "",
          "[Added Lines]",
          "387:         Check for Azure Metadata Service.",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "472:         wrap_http_errors: bool = True,",
          "473:     ):",
          "474:         \"\"\"",
          "477:         :param endpoint_info: Tuple of method and endpoint",
          "478:         :param json: Parameters for this API call.",
          "",
          "[Removed Lines]",
          "475:         Utility function to perform an API call with retries",
          "",
          "[Added Lines]",
          "477:         Utility function to perform an API call with retries.",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "618: class _TokenAuth(AuthBase):",
          "619:     \"\"\"",
          "621:     magic function.",
          "622:     \"\"\"",
          "",
          "[Removed Lines]",
          "620:     Helper class for requests Auth field. AuthBase requires you to implement the __call__",
          "",
          "[Added Lines]",
          "622:     Helper class for requests Auth field.",
          "624:     AuthBase requires you to implement the ``__call__``",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/hooks/cloud_memorystore.py||airflow/providers/google/cloud/hooks/cloud_memorystore.py": [
          "File: airflow/providers/google/cloud/hooks/cloud_memorystore.py -> airflow/providers/google/cloud/hooks/cloud_memorystore.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "90:     @staticmethod",
          "91:     def _append_label(instance: Instance, key: str, val: str) -> Instance:",
          "92:         \"\"\"",
          "95:         Labels must fit the regex ``[a-z]([-a-z0-9]*[a-z0-9])?`` (current",
          "96:          airflow version string follows semantic versioning spec: x.y.z).",
          "",
          "[Removed Lines]",
          "93:         Append labels to provided Instance type",
          "",
          "[Added Lines]",
          "93:         Append labels to provided Instance type.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "275:         metadata: Sequence[Tuple[str, str]] = (),",
          "276:     ):",
          "277:         \"\"\"",
          "278:         Initiates a failover of the primary node to current replica node for a specific STANDARD tier Cloud",
          "279:         Memorystore for Redis instance.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "278:         Failover of the primary node to current replica node.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "392:         metadata: Sequence[Tuple[str, str]] = (),",
          "393:     ):",
          "394:         \"\"\"",
          "398:         :param location: The location of the Cloud Memorystore instance (for example europe-west1)",
          "",
          "[Removed Lines]",
          "395:         Lists all Redis instances owned by a project in either the specified location (region) or all",
          "396:         locations.",
          "",
          "[Added Lines]",
          "397:         List Redis instances owned by a project at the specified location (region) or all locations.",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "528:     @staticmethod",
          "529:     def _append_label(instance: cloud_memcache.Instance, key: str, val: str) -> cloud_memcache.Instance:",
          "530:         \"\"\"",
          "533:         Labels must fit the regex ``[a-z]([-a-z0-9]*[a-z0-9])?`` (current",
          "534:          airflow version string follows semantic versioning spec: x.y.z).",
          "",
          "[Removed Lines]",
          "531:         Append labels to provided Instance type",
          "",
          "[Added Lines]",
          "532:         Append labels to provided Instance type.",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "752:         metadata: Sequence[Tuple[str, str]] = (),",
          "753:     ):",
          "754:         \"\"\"",
          "758:         :param location: The location of the Cloud Memorystore instance (for example europe-west1)",
          "",
          "[Removed Lines]",
          "755:         Lists all Memcached instances owned by a project in either the specified location (region) or all",
          "756:         locations.",
          "",
          "[Added Lines]",
          "756:         List Memcached instances owned by a project at the specified location (region) or all locations.",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "851:         metadata: Sequence[Tuple[str, str]] = (),",
          "852:     ):",
          "853:         \"\"\"",
          "858:         :param update_mask: Required. Mask of fields to update.",
          "859:             If a dict is provided, it must be of the same form as the protobuf message",
          "",
          "[Removed Lines]",
          "854:         Updates the defined Memcached Parameters for an existing Instance. This method only stages the",
          "855:             parameters, it must be followed by apply_parameters to apply the parameters to nodes of",
          "856:             the Memcached Instance.",
          "",
          "[Added Lines]",
          "854:         Update the defined Memcached Parameters for an existing Instance.",
          "856:         This method only stages the parameters, it must be followed by apply_parameters",
          "857:         to apply the parameters to nodes of the Memcached Instance.",
          "",
          "---------------"
        ],
        "airflow/providers/mysql/hooks/mysql.py||airflow/providers/mysql/hooks/mysql.py": [
          "File: airflow/providers/mysql/hooks/mysql.py -> airflow/providers/mysql/hooks/mysql.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "62:     def set_autocommit(self, conn: MySQLConnectionTypes, autocommit: bool) -> None:",
          "63:         \"\"\"",
          "67:         :param conn: connection to set autocommit setting",
          "68:         :param autocommit: autocommit setting",
          "",
          "[Removed Lines]",
          "64:         The MySQLdb (mysqlclient) client uses an `autocommit` method rather",
          "65:         than an `autocommit` property to set the autocommit setting",
          "",
          "[Added Lines]",
          "64:         Set *autocommit*.",
          "67:         property, so we need to override this to support it.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "76:     def get_autocommit(self, conn: MySQLConnectionTypes) -> bool:",
          "77:         \"\"\"",
          "81:         :param conn: connection to get autocommit setting from.",
          "82:         :return: connection autocommit setting",
          "",
          "[Removed Lines]",
          "78:         The MySQLdb (mysqlclient) client uses a `get_autocommit` method",
          "79:         rather than an `autocommit` property to get the autocommit setting",
          "",
          "[Added Lines]",
          "80:         Whether *autocommit* is active.",
          "83:         property, so we need to override this to support it.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "147:     def get_conn(self) -> MySQLConnectionTypes:",
          "148:         \"\"\"",
          "149:         Establishes a connection to a mysql database",
          "150:         by extracting the connection configuration from the Airflow connection.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "153:         Connection to a MySQL database.",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "174:         raise ValueError('Unknown MySQL client name provided!')",
          "176:     def bulk_load(self, table: str, tmp_file: str) -> None:",
          "178:         conn = self.get_conn()",
          "179:         cur = conn.cursor()",
          "180:         cur.execute(",
          "",
          "[Removed Lines]",
          "177:         \"\"\"Loads a tab-delimited file into a database table\"\"\"",
          "",
          "[Added Lines]",
          "183:         \"\"\"Load a tab-delimited file into a database table.\"\"\"",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "187:         conn.close()",
          "189:     def bulk_dump(self, table: str, tmp_file: str) -> None:",
          "191:         conn = self.get_conn()",
          "192:         cur = conn.cursor()",
          "193:         cur.execute(",
          "",
          "[Removed Lines]",
          "190:         \"\"\"Dumps a database table into a tab-delimited file\"\"\"",
          "",
          "[Added Lines]",
          "196:         \"\"\"Dump a database table into a tab-delimited file.\"\"\"",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "202:     @staticmethod",
          "203:     def _serialize_cell(cell: object, conn: Optional[Connection] = None) -> object:",
          "204:         \"\"\"",
          "205:         The package MySQLdb converts an argument to a literal",
          "206:         when passing those separately to execute. Hence, this method does nothing.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "211:         Convert argument to a literal.",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "215:     def get_iam_token(self, conn: Connection) -> Tuple[str, int]:",
          "216:         \"\"\"",
          "217:         Uses AWSHook to retrieve a temporary password to connect to MySQL",
          "218:         Port is required. If none is provided, default 3306 is used",
          "219:         \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "225:         Retrieve a temporary password to connect to MySQL.",
          "",
          "---------------"
        ],
        "airflow/providers/qubole/sensors/qubole.py||airflow/providers/qubole/sensors/qubole.py": [
          "File: airflow/providers/qubole/sensors/qubole.py -> airflow/providers/qubole/sensors/qubole.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: class QuboleSensor(BaseSensorOperator):",
          "34:     template_fields: Sequence[str] = ('data', 'qubole_conn_id')",
          "",
          "[Removed Lines]",
          "32:     \"\"\"Base class for all Qubole Sensors\"\"\"",
          "",
          "[Added Lines]",
          "32:     \"\"\"Base class for all Qubole Sensors.\"\"\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "69: class QuboleFileSensor(QuboleSensor):",
          "70:     \"\"\"",
          "74:     .. seealso::",
          "75:         For more information on how to use this sensor, take a look at the guide:",
          "",
          "[Removed Lines]",
          "71:     Wait for a file or folder to be present in cloud storage",
          "72:     and check for its presence via QDS APIs",
          "",
          "[Added Lines]",
          "71:     Wait for a file or folder to be present in cloud storage.",
          "73:     Check for file or folder presence via QDS APIs.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "93: class QubolePartitionSensor(QuboleSensor):",
          "94:     \"\"\"",
          "98:     .. seealso::",
          "99:         For more information on how to use this sensor, take a look at the guide:",
          "",
          "[Removed Lines]",
          "95:     Wait for a Hive partition to show up in QHS (Qubole Hive Service)",
          "96:     and check for its presence via QDS APIs",
          "",
          "[Added Lines]",
          "96:     Wait for a Hive partition to show up in QHS (Qubole Hive Service).",
          "98:     Check for Hive partition presence via QDS APIs.",
          "",
          "---------------"
        ],
        "airflow/utils/process_utils.py||airflow/utils/process_utils.py": [
          "File: airflow/utils/process_utils.py -> airflow/utils/process_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: #",
          "20: import errno",
          "21: import logging",
          "22: import os",
          "",
          "[Removed Lines]",
          "19: \"\"\"Utilities for running or stopping processes\"\"\"",
          "",
          "[Added Lines]",
          "19: \"\"\"Utilities for running or stopping processes.\"\"\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "56:     timeout: int = DEFAULT_TIME_TO_WAIT_AFTER_SIGTERM,",
          "57: ) -> Dict[int, int]:",
          "58:     \"\"\"",
          "59:     Tries really hard to terminate all processes in the group (including grandchildren). Will send",
          "60:     sig (SIGTERM) to the process group of pid. If any process is alive after timeout",
          "61:     a SIGKILL will be send.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "59:     Send sig (SIGTERM) to the process group of pid.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "159: def execute_in_subprocess(cmd: List[str], cwd: Optional[str] = None) -> None:",
          "160:     \"\"\"",
          "162:     :param cmd: command and arguments to run",
          "163:     :param cwd: Current working directory passed to the Popen constructor",
          "164:     \"\"\"",
          "",
          "[Removed Lines]",
          "161:     Execute a process and stream output to logger",
          "",
          "[Added Lines]",
          "163:     Execute a process and stream output to logger.",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "168: def execute_in_subprocess_with_kwargs(cmd: List[str], **kwargs) -> None:",
          "169:     \"\"\"",
          "172:     :param cmd: command and arguments to run",
          "",
          "[Removed Lines]",
          "170:     Execute a process and stream output to logger",
          "",
          "[Added Lines]",
          "172:     Execute a process and stream output to logger.",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "191: def execute_interactive(cmd: List[str], **kwargs) -> None:",
          "192:     \"\"\"",
          "193:     Runs the new command as a subprocess and ensures that the terminal's state is restored to its original",
          "194:     state after the process is completed e.g. if the subprocess hides the cursor, it will be restored after",
          "195:     the process is completed.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "195:     Run the new command as a subprocess.",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "271: @contextmanager",
          "272: def patch_environ(new_env_variables: Dict[str, str]) -> Generator[None, None, None]:",
          "273:     \"\"\"",
          "276:     :param new_env_variables: Environment variables to set",
          "277:     \"\"\"",
          "278:     current_env_state = {key: os.environ.get(key) for key in new_env_variables.keys()}",
          "",
          "[Removed Lines]",
          "274:     Sets environment variables in context. After leaving the context, it restores its original state.",
          "",
          "[Added Lines]",
          "278:     Set environment variables in context.",
          "280:     After leaving the context, it restores its original state.",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "317: def set_new_process_group() -> None:",
          "318:     \"\"\"",
          "320:     That makes it easy to kill all sub-process of this at the OS-level,",
          "321:     rather than having to iterate the child processes.",
          "322:     If current process spawn by system call ``exec()`` than keep current process group",
          "",
          "[Removed Lines]",
          "319:     Tries to set current process to a new process group",
          "",
          "[Added Lines]",
          "324:     Try to set current process to a new process group.",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0bcdba0e4e8d109abf8f98618a3784457364e442",
      "candidate_info": {
        "commit_hash": "0bcdba0e4e8d109abf8f98618a3784457364e442",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0bcdba0e4e8d109abf8f98618a3784457364e442",
        "files": [
          "RELEASE_NOTES.rst",
          "scripts/ci/pre_commit/pre_commit_version_heads_map.py"
        ],
        "message": "Update Release Note for 2.4.0",
        "before_after_code_files": [
          "scripts/ci/pre_commit/pre_commit_version_heads_map.py||scripts/ci/pre_commit/pre_commit_version_heads_map.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/pre_commit/pre_commit_version_heads_map.py||scripts/ci/pre_commit/pre_commit_version_heads_map.py": [
          "File: scripts/ci/pre_commit/pre_commit_version_heads_map.py -> scripts/ci/pre_commit/pre_commit_version_heads_map.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "60:     if airflow_version.is_devrelease or 'b' in (airflow_version.pre or ()):",
          "61:         exit(0)",
          "62:     versions = read_revision_heads_map()",
          "64:         print(\"Current airflow version is not in the REVISION_HEADS_MAP\")",
          "65:         print(\"Current airflow version:\", airflow_version)",
          "66:         print(\"Please add the version to the REVISION_HEADS_MAP at:\", DB_FILE)",
          "",
          "[Removed Lines]",
          "63:     if airflow_version not in versions:",
          "",
          "[Added Lines]",
          "63:     if airflow_version.base_version not in versions:",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "553f7c9d399dc88bb04925b06b2f691840f7ab78",
      "candidate_info": {
        "commit_hash": "553f7c9d399dc88bb04925b06b2f691840f7ab78",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/553f7c9d399dc88bb04925b06b2f691840f7ab78",
        "files": [
          ".github/workflows/ci.yml",
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py"
        ],
        "message": "Make \"quick build\" actually test build time (#26251)\n\n* Make \"quick build\" actually test build time\n\nThere was a \"string\" instead of array passed in case of --max-time\ncommand flag which caused an error - regardless of the time it\ntook to run the build, but it was also ignored because the\njob was run within \"continue-on-error\" cache push job.\n\nThis PR fixes it to use proper \"exit\" command and separates it out\nto a job where quick build failure will be noticed (not in PRs but\nin the merge-run)\n\n(cherry picked from commit 63da96b4331bdc70ec49e360e87610960b31371c)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/developer_commands.py -> dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "184:         force_build=force_build,",
          "185:         db_reset=db_reset,",
          "186:         include_mypy_volume=include_mypy_volume,",
          "188:         answer=answer,",
          "189:         image_tag=image_tag,",
          "190:         platform=platform,",
          "",
          "[Removed Lines]",
          "187:         extra_args=extra_args if not max_time else \"exit\",",
          "",
          "[Added Lines]",
          "187:         extra_args=extra_args if not max_time else [\"exit\"],",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a10fc9aefee0054effb2ccb24717d90bad53c6a9",
      "candidate_info": {
        "commit_hash": "a10fc9aefee0054effb2ccb24717d90bad53c6a9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a10fc9aefee0054effb2ccb24717d90bad53c6a9",
        "files": [
          "airflow/example_dags/example_branch_day_of_week_operator.py",
          "airflow/operators/weekday.py",
          "airflow/sensors/weekday.py"
        ],
        "message": "Add more weekday operator and sensor examples #26071 (#26098)\n\n(cherry picked from commit dd6b2e4e6cb89d9eea2f3db790cb003a2e89aeff)",
        "before_after_code_files": [
          "airflow/example_dags/example_branch_day_of_week_operator.py||airflow/example_dags/example_branch_day_of_week_operator.py",
          "airflow/operators/weekday.py||airflow/operators/weekday.py",
          "airflow/sensors/weekday.py||airflow/sensors/weekday.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/example_dags/example_branch_day_of_week_operator.py||airflow/example_dags/example_branch_day_of_week_operator.py": [
          "File: airflow/example_dags/example_branch_day_of_week_operator.py -> airflow/example_dags/example_branch_day_of_week_operator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: from airflow import DAG",
          "25: from airflow.operators.empty import EmptyOperator",
          "26: from airflow.operators.weekday import BranchDayOfWeekOperator",
          "28: with DAG(",
          "29:     dag_id=\"example_weekday_branch_operator\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: from airflow.utils.weekday import WeekDay",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "35:     # [START howto_operator_day_of_week_branch]",
          "36:     empty_task_1 = EmptyOperator(task_id='branch_true')",
          "37:     empty_task_2 = EmptyOperator(task_id='branch_false')",
          "39:     branch = BranchDayOfWeekOperator(",
          "40:         task_id=\"make_choice\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "39:     empty_task_3 = EmptyOperator(task_id='branch_weekend')",
          "40:     empty_task_4 = EmptyOperator(task_id='branch_mid_week')",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "42:         follow_task_ids_if_false=\"branch_false\",",
          "43:         week_day=\"Monday\",",
          "44:     )",
          "47:     branch >> [empty_task_1, empty_task_2]",
          "48:     # [END howto_operator_day_of_week_branch]",
          "",
          "[Removed Lines]",
          "46:     # Run empty_task_1 if branch executes on Monday",
          "",
          "[Added Lines]",
          "48:     branch_weekend = BranchDayOfWeekOperator(",
          "49:         task_id=\"make_weekend_choice\",",
          "50:         follow_task_ids_if_true=\"branch_weekend\",",
          "51:         follow_task_ids_if_false=\"branch_mid_week\",",
          "52:         week_day={WeekDay.SATURDAY, WeekDay.SUNDAY},",
          "53:     )",
          "55:     # Run empty_task_1 if branch executes on Monday, empty_task_2 otherwise",
          "57:     # Run empty_task_3 if it's a weekend, empty_task_4 otherwise",
          "58:     empty_task_2 >> branch_weekend >> [empty_task_3, empty_task_4]",
          "",
          "---------------"
        ],
        "airflow/operators/weekday.py||airflow/operators/weekday.py": [
          "File: airflow/operators/weekday.py -> airflow/operators/weekday.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31:     For more information on how to use this operator, take a look at the guide:",
          "32:     :ref:`howto/operator:BranchDayOfWeekOperator`",
          "34:     :param follow_task_ids_if_true: task id or task ids to follow if criteria met",
          "35:     :param follow_task_ids_if_false: task id or task ids to follow if criteria does not met",
          "36:     :param week_day: Day of the week to check (full name). Optionally, a set",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "36:         from airflow.operators.empty import EmptyOperator",
          "38:         monday = EmptyOperator(task_id='monday')",
          "39:         other_day = EmptyOperator(task_id='other_day')",
          "41:         monday_check = DayOfWeekSensor(",
          "42:             task_id='monday_check',",
          "43:             week_day='Monday',",
          "44:             use_task_logical_date=True,",
          "45:             follow_task_ids_if_true='monday',",
          "46:             follow_task_ids_if_false='other_day',",
          "47:             dag=dag)",
          "48:         monday_check >> [monday, other_day]",
          "52:         # import WeekDay Enum",
          "53:         from airflow.utils.weekday import WeekDay",
          "54:         from airflow.operators.empty import EmptyOperator",
          "56:         workday = EmptyOperator(task_id='workday')",
          "57:         weekend = EmptyOperator(task_id='weekend')",
          "58:         weekend_check = BranchDayOfWeekOperator(",
          "59:             task_id='weekend_check',",
          "60:             week_day={WeekDay.SATURDAY, WeekDay.SUNDAY},",
          "61:             use_task_logical_date=True,",
          "62:             follow_task_ids_if_true='weekend',",
          "63:             follow_task_ids_if_false='workday',",
          "64:             dag=dag)",
          "65:         # add downstream dependencies as you would do with any branch operator",
          "66:         weekend_check >> [workday, weekend]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "45:     :param use_task_logical_date: If ``True``, uses task's logical date to compare",
          "46:         with is_today. Execution Date is Useful for backfilling.",
          "47:         If ``False``, uses system's day of the week.",
          "48:     \"\"\"",
          "50:     def __init__(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "79:         To use `WeekDay` enum, import it from `airflow.utils.weekday`",
          "84:     :param use_task_execution_day: deprecated parameter, same effect as `use_task_logical_date`",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "53:         follow_task_ids_if_true: Union[str, Iterable[str]],",
          "54:         follow_task_ids_if_false: Union[str, Iterable[str]],",
          "56:         use_task_logical_date: bool = False,",
          "57:         use_task_execution_day: bool = False,",
          "",
          "[Removed Lines]",
          "55:         week_day: Union[str, Iterable[str]],",
          "",
          "[Added Lines]",
          "92:         week_day: Union[str, Iterable[str], WeekDay, Iterable[WeekDay]],",
          "",
          "---------------"
        ],
        "airflow/sensors/weekday.py||airflow/sensors/weekday.py": [
          "File: airflow/sensors/weekday.py -> airflow/sensors/weekday.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: import warnings",
          "20: from airflow.exceptions import RemovedInAirflow3Warning",
          "21: from airflow.sensors.base import BaseSensorOperator",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: from typing import Iterable, Union",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "69:     :param use_task_logical_date: If ``True``, uses task's logical date to compare",
          "70:         with week_day. Execution Date is Useful for backfilling.",
          "71:         If ``False``, uses system's day of the week. Useful when you",
          "72:         don't want to run anything on weekdays on the system.",
          "73:     \"\"\"",
          "76:         super().__init__(**kwargs)",
          "77:         self.week_day = week_day",
          "78:         self.use_task_logical_date = use_task_logical_date",
          "",
          "[Removed Lines]",
          "75:     def __init__(self, *, week_day, use_task_logical_date=False, use_task_execution_day=False, **kwargs):",
          "",
          "[Added Lines]",
          "70:         To use ``WeekDay`` enum, import it from ``airflow.utils.weekday``",
          "76:     :param use_task_execution_day: deprecated parameter, same effect as `use_task_logical_date`",
          "79:     def __init__(",
          "80:         self,",
          "82:         week_day: Union[str, Iterable[str], WeekDay, Iterable[WeekDay]],",
          "83:         use_task_logical_date: bool = False,",
          "84:         use_task_execution_day: bool = False,",
          "86:     ) -> None:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "85:             )",
          "86:         self._week_day_num = WeekDay.validate_week_day(week_day)",
          "89:         self.log.info(",
          "90:             'Poking until weekday is in %s, Today is %s',",
          "91:             self.week_day,",
          "",
          "[Removed Lines]",
          "88:     def poke(self, context: Context):",
          "",
          "[Added Lines]",
          "99:     def poke(self, context: Context) -> bool:",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8c71dd8b13ccba199a77b205425c08087a7f177f",
      "candidate_info": {
        "commit_hash": "8c71dd8b13ccba199a77b205425c08087a7f177f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8c71dd8b13ccba199a77b205425c08087a7f177f",
        "files": [
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py"
        ],
        "message": "Make breeze works with latest docker-compose (#26233)\n\nThe latest docker-compose dropped an alias for `docker-compose` and\nwe need to detect it and use \"docker compose\" instead.\n\n(cherry picked from commit 7f47006effb330429fb510eb52644d22544f4fad)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "dev/breeze/src/airflow_breeze/commands/testing_commands.py||dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/developer_commands.py -> dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "69: from airflow_breeze.utils.console import get_console",
          "70: from airflow_breeze.utils.custom_param_types import BetterChoice, NotVerifiedBetterChoice",
          "71: from airflow_breeze.utils.docker_command_utils import (",
          "72:     check_docker_resources,",
          "73:     get_env_variables_for_docker_commands,",
          "74:     get_extra_docker_flags,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "72:     DOCKER_COMPOSE_COMMAND,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "468:     is_flag=True,",
          "469: )",
          "470: def stop(verbose: bool, dry_run: bool, preserve_volumes: bool):",
          "472:     if not preserve_volumes:",
          "473:         command_to_execute.append(\"--volumes\")",
          "474:     shell_params = ShellParams(verbose=verbose, backend=\"all\", include_mypy_volume=True)",
          "",
          "[Removed Lines]",
          "471:     command_to_execute = ['docker-compose', 'down', \"--remove-orphans\"]",
          "",
          "[Added Lines]",
          "472:     perform_environment_checks(verbose=verbose)",
          "473:     command_to_execute = [*DOCKER_COMPOSE_COMMAND, 'down', \"--remove-orphans\"]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "544:     :param shell_params: parameters of the execution",
          "545:     \"\"\"",
          "546:     shell_params.print_badge_info()",
          "548:     cmd_added = shell_params.command_passed",
          "549:     env_variables = get_env_variables_for_docker_commands(shell_params)",
          "550:     if cmd_added is not None:",
          "",
          "[Removed Lines]",
          "547:     cmd = ['docker-compose', 'run', '--service-ports', \"-e\", \"BREEZE\", '--rm', 'airflow']",
          "",
          "[Added Lines]",
          "549:     cmd = [*DOCKER_COMPOSE_COMMAND, 'run', '--service-ports', \"-e\", \"BREEZE\", '--rm', 'airflow']",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "578:     check_docker_resources(exec_shell_params.airflow_image_name, verbose=verbose, dry_run=dry_run)",
          "579:     exec_shell_params.print_badge_info()",
          "580:     env_variables = get_env_variables_for_docker_commands(exec_shell_params)",
          "582:     docker_compose_ps_command = run_command(",
          "583:         cmd, verbose=verbose, dry_run=dry_run, text=True, capture_output=True, env=env_variables, check=False",
          "584:     )",
          "",
          "[Removed Lines]",
          "581:     cmd = ['docker-compose', 'ps', '--all', '--filter', 'status=running', 'airflow']",
          "",
          "[Added Lines]",
          "583:     cmd = [*DOCKER_COMPOSE_COMMAND, 'ps', '--all', '--filter', 'status=running', 'airflow']",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/testing_commands.py||dev/breeze/src/airflow_breeze/commands/testing_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/testing_commands.py -> dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "50: from airflow_breeze.utils.console import get_console, message_type_from_return_code",
          "51: from airflow_breeze.utils.custom_param_types import NotVerifiedBetterChoice",
          "52: from airflow_breeze.utils.docker_command_utils import (",
          "53:     get_env_variables_for_docker_commands,",
          "54:     perform_environment_checks,",
          "55: )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "53:     DOCKER_COMPOSE_COMMAND,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "269:     if db_reset:",
          "270:         env_variables[\"DB_RESET\"] = \"true\"",
          "271:     perform_environment_checks(verbose=verbose)",
          "273:     cmd.extend(list(extra_pytest_args))",
          "274:     version = (",
          "275:         mssql_version",
          "",
          "[Removed Lines]",
          "272:     cmd = ['docker-compose', 'run', '--service-ports', '--rm', 'airflow']",
          "",
          "[Added Lines]",
          "273:     cmd = [*DOCKER_COMPOSE_COMMAND, 'run', '--service-ports', '--rm', 'airflow']",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "326:     env_variables['RUN_TESTS'] = \"true\"",
          "327:     env_variables['TEST_TYPE'] = 'Helm'",
          "328:     perform_environment_checks(verbose=verbose)",
          "330:     cmd.extend(list(extra_pytest_args))",
          "331:     result = run_command(cmd, verbose=verbose, dry_run=dry_run, env=env_variables, check=False)",
          "332:     sys.exit(result.returncode)",
          "",
          "[Removed Lines]",
          "329:     cmd = ['docker-compose', 'run', '--service-ports', '--rm', 'airflow']",
          "",
          "[Added Lines]",
          "330:     cmd = [*DOCKER_COMPOSE_COMMAND, 'run', '--service-ports', '--rm', 'airflow']",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "266:                 )",
          "269: def check_docker_compose_version(verbose: bool):",
          "270:     \"\"\"",
          "271:     Checks if the docker compose version is as expected, including some specific modifications done by",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "269: DOCKER_COMPOSE_COMMAND = [\"docker-compose\"]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "293:             capture_output=True,",
          "294:             text=True,",
          "295:         )",
          "297:     if docker_compose_version_result.returncode == 0:",
          "298:         docker_compose_version = docker_compose_version_result.stdout",
          "299:         version_extracted = version_pattern.search(docker_compose_version)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "299:         DOCKER_COMPOSE_COMMAND.clear()",
          "300:         DOCKER_COMPOSE_COMMAND.extend(['docker', 'compose'])",
          "",
          "---------------"
        ]
      }
    }
  ]
}