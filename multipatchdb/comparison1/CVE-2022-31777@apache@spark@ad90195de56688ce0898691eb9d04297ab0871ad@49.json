{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "68bec7342433f10dae2fbc0ac9fcb0c267aba536",
      "candidate_info": {
        "commit_hash": "68bec7342433f10dae2fbc0ac9fcb0c267aba536",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/68bec7342433f10dae2fbc0ac9fcb0c267aba536",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/generators.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala"
        ],
        "message": "[SPARK-39061][SQL] Set nullable correctly for `Inline` output attributes\n\n### What changes were proposed in this pull request?\n\nChange `Inline#elementSchema` to make each struct field nullable when the containing array has a null element.\n\n### Why are the changes needed?\n\nThis query returns incorrect results (the last row should be `NULL NULL`):\n```\nspark-sql> select inline(array(named_struct('a', 1, 'b', 2), null));\n1\t2\n-1\t-1\nTime taken: 4.053 seconds, Fetched 2 row(s)\nspark-sql>\n```\nAnd this query gets a NullPointerException:\n```\nspark-sql> select inline(array(named_struct('a', '1', 'b', '2'), null));\n22/04/28 16:51:54 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)\njava.lang.NullPointerException: null\n\tat org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source) ~[?:?]\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) ~[?:?]\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(Buffere\n```\nWhen an array of structs is created by `CreateArray`, and no struct field contains a literal null value, the schema for the struct will have non-nullable fields, even if the array itself has a null entry (as in the example above). As a result, the output attributes for the generator will be non-nullable.\n\nWhen the output attributes for `Inline` are non-nullable, `GenerateUnsafeProjection#writeExpressionsToBuffer` generates incorrect code for null structs.\n\nIn more detail, the issue is this: `GenerateExec#codeGenCollection` generates code that will check if the struct instance (i.e., array element) is null and, if so, set a boolean for each struct field to indicate that the field contains a null. However, unless the generator's output attributes are nullable, `GenerateUnsafeProjection#writeExpressionsToBuffer` will not generate any code to check those booleans. Instead it will generate code to write out whatever is in the variables that normally hold the struct values (which will be garbage if the array element is null).\n\nArrays of structs from file sources do not have this issue. In that case, each `StructField` will have nullable=true due to [this](https://github.com/apache/spark/blob/fe85d7912f86c3e337aa93b23bfa7e7e01c0a32e/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala#L417).\n\n(Note: the eval path for `Inline` has a different bug with null array elements that occurs even when `nullable` is set correctly in the schema, but I will address that in a separate PR).\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nNew unit test.\n\nCloses #36883 from bersprockets/inline_struct_nullability_issue.\n\nAuthored-by: Bruce Robbins <bersprockets@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit fc385dafabe3c609b38b81deaaf36e5eb6ee341b)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/generators.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/generators.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/generators.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/generators.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/generators.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/generators.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "444:   }",
          "446:   override def elementSchema: StructType = child.dataType match {",
          "448:   }",
          "450:   override def collectionType: DataType = child.dataType",
          "",
          "[Removed Lines]",
          "447:     case ArrayType(st: StructType, _) => st",
          "",
          "[Added Lines]",
          "447:     case ArrayType(st: StructType, false) => st",
          "448:     case ArrayType(st: StructType, true) => st.asNullable",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/GeneratorFunctionSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "388:         Row(0, 1) :: Row(3, 4) :: Row(6, 7) :: Row(null, null) :: Row(null, null) :: Nil)",
          "389:     }",
          "390:   }",
          "391: }",
          "393: case class EmptyGenerator() extends Generator with LeafLike[Expression] {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "392:   test(\"SPARK-39061: inline should handle null struct\") {",
          "393:     val df = sql(",
          "394:       \"\"\"select * from values",
          "395:         |(",
          "396:         |  1,",
          "397:         |  array(",
          "398:         |    named_struct('c1', 0, 'c2', 1),",
          "399:         |    null,",
          "400:         |    named_struct('c1', 2, 'c2', 3),",
          "401:         |    null",
          "402:         |  )",
          "403:         |)",
          "404:         |as tbl(a, b)",
          "405:          \"\"\".stripMargin)",
          "406:     df.createOrReplaceTempView(\"t1\")",
          "408:     checkAnswer(",
          "409:       sql(\"select inline(b) from t1\"),",
          "410:       Row(0, 1) :: Row(null, null) :: Row(2, 3) :: Row(null, null) :: Nil)",
          "412:     checkAnswer(",
          "413:       sql(\"select a, inline(b) from t1\"),",
          "414:       Row(1, 0, 1) :: Row(1, null, null) :: Row(1, 2, 3) :: Row(1, null, null) :: Nil)",
          "415:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2f1abc1cf121958d8646c26becd57e121b4ed6ec",
      "candidate_info": {
        "commit_hash": "2f1abc1cf121958d8646c26becd57e121b4ed6ec",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/2f1abc1cf121958d8646c26becd57e121b4ed6ec",
        "files": [
          "python/pyspark/tests/test_rdd.py"
        ],
        "message": "[SPARK-38927][TESTS] Skip NumPy/Pandas tests in `test_rdd.py` if not available\n\n### What changes were proposed in this pull request?\nThis PR aims to skip NumPy/Pandas tests in `test_rdd.py` if they are not available.\n\n### Why are the changes needed?\nCurrently, the tests that involve NumPy or Pandas are failing because NumPy and Pandas are unavailable in underlying Python. The tests should be skipped instead instead of showing failure.\n\n**BEFORE**\n```\n======================================================================\nERROR: test_take_on_jrdd_with_large_rows_should_not_cause_deadlock (pyspark.tests.test_rdd.RDDTests)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \".../test_rdd.py\", line 723, in test_take_on_jrdd_with_large_rows_should_not_cause_deadlock\n    import numpy as np\nModuleNotFoundError: No module named 'numpy'\n\n----------------------------------------------------------------------\nRan 1 test in 1.990s\n\nFAILED (errors=1)\n```\n\n**AFTER**\n```\nFinished test(python3.9): pyspark.tests.test_rdd RDDTests.test_take_on_jrdd_with_large_rows_should_not_cause_deadlock (1s) ... 1 tests were skipped\nTests passed in 1 seconds\n\nSkipped tests in pyspark.tests.test_rdd RDDTests.test_take_on_jrdd_with_large_rows_should_not_cause_deadlock with python3.9:\n    test_take_on_jrdd_with_large_rows_should_not_cause_deadlock (pyspark.tests.test_rdd.RDDTests) ... skipped 'NumPy or Pandas not installed'\n```\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nPass the CIs.\n\nCloses #36235 from williamhyun/skipnumpy.\n\nAuthored-by: William Hyun <william@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit c34140d8d744dc75d130af60080a2a8e25d501b1)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "python/pyspark/tests/test_rdd.py||python/pyspark/tests/test_rdd.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/tests/test_rdd.py||python/pyspark/tests/test_rdd.py": [
          "File: python/pyspark/tests/test_rdd.py -> python/pyspark/tests/test_rdd.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import random",
          "21: import tempfile",
          "22: import time",
          "23: from glob import glob",
          "25: from py4j.protocol import Py4JJavaError",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "23: import unittest",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "35:     NoOpSerializer,",
          "36: )",
          "37: from pyspark.sql import SparkSession",
          "41: global_func = lambda: \"Hi\"  # noqa: E731",
          "",
          "[Removed Lines]",
          "38: from pyspark.testing.utils import ReusedPySparkTestCase, SPARK_HOME, QuietTest",
          "",
          "[Added Lines]",
          "39: from pyspark.testing.utils import ReusedPySparkTestCase, SPARK_HOME, QuietTest, have_numpy",
          "40: from pyspark.testing.sqlutils import have_pandas",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "698:         rdd = self.sc.parallelize(range(1 << 20)).map(lambda x: str(x))",
          "699:         rdd._jrdd.first()",
          "701:     def test_take_on_jrdd_with_large_rows_should_not_cause_deadlock(self):",
          "702:         # Regression test for SPARK-38677.",
          "703:         #",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "703:     @unittest.skipIf(not have_numpy or not have_pandas, \"NumPy or Pandas not installed\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "be63826d423c3f5b15bb0ad39a58a564cf1d2b96",
      "candidate_info": {
        "commit_hash": "be63826d423c3f5b15bb0ad39a58a564cf1d2b96",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/be63826d423c3f5b15bb0ad39a58a564cf1d2b96",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala"
        ],
        "message": "[SPARK-39286][DOC] Update documentation for the decode function\n\n### What changes were proposed in this pull request?\nThe documentation for the decode function introduced in [SPARK-33527](https://issues.apache.org/jira/browse/SPARK-33527) refers erroneously to Oracle. It appears that the documentation string has been in large parts copied from https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/DECODE.html#GUID-39341D91-3442-4730-BD34-D3CF5D4701CE\n\nThis proposes to update the documentation of the decode function to fix the issue.\n\n### Why are the changes needed?\nDocumentation fix.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nNA\n\nCloses #36662 from LucaCanali/fixDecodeDoc.\n\nAuthored-by: Luca Canali <luca.canali@cern.ch>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit f4c34aa642320defb81c71f5755672603f866b49)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2504:   usage = \"\"\"",
          "2505:     _FUNC_(bin, charset) - Decodes the first argument using the second argument character set.",
          "2510:   \"\"\",",
          "2511:   examples = \"\"\"",
          "2512:     Examples:",
          "",
          "[Removed Lines]",
          "2507:     _FUNC_(expr, search, result [, search, result ] ... [, default]) - Decode compares expr",
          "2508:       to each search value one by one. If expr is equal to a search, returns the corresponding result.",
          "2509:       If no match is found, then Oracle returns default. If default is omitted, returns null.",
          "",
          "[Added Lines]",
          "2507:     _FUNC_(expr, search, result [, search, result ] ... [, default]) - Compares expr",
          "2508:       to each search value in order. If expr is equal to a search value, _FUNC_ returns",
          "2509:       the corresponding result. If no match is found, then it returns default. If default",
          "2510:       is omitted, it returns null.",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0d1005e8ccdd379f93ae3ce9a61bb0469ec0d695",
      "candidate_info": {
        "commit_hash": "0d1005e8ccdd379f93ae3ce9a61bb0469ec0d695",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/0d1005e8ccdd379f93ae3ce9a61bb0469ec0d695",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala"
        ],
        "message": "[SPARK-38928][TESTS][SQL] Skip Pandas UDF test in `QueryCompilationErrorsSuite` if not available\n\n### What changes were proposed in this pull request?\nThis PR aims to skip Pandas UDF tests in `QueryCompilationErrorsSuite` if not available.\n\n### Why are the changes needed?\nThe tests should be skipped instead of showing failure.\n\n**BEFORE**\n```\n$ build/sbt \"sql/testOnly org.apache.spark.sql.errors.QueryCompilationErrorsSuite\"\n...\n[info] *** 2 TESTS FAILED ***\n[error] Failed tests:\n[error] \torg.apache.spark.sql.errors.QueryCompilationErrorsSuite\n[error] (sql / Test / testOnly) sbt.TestsFailedException: Tests unsuccessful\n```\n\n**AFTER**\n```\n$ build/sbt \"sql/testOnly org.apache.spark.sql.errors.QueryCompilationErrorsSuite\"\n...\n[info] Tests: succeeded 13, failed 0, canceled 2, ignored 0, pending 0\n[info] All tests passed.\n```\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nPass the CIs.\n\nCloses #36236 from williamhyun/skippandas.\n\nAuthored-by: William Hyun <william@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 1f55a2af225b9c6226004180d9b83d2424bbe154)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "108:   test(\"CANNOT_USE_MIXTURE: Using aggregate function with grouped aggregate pandas UDF\") {",
          "109:     import IntegratedUDFTestUtils._",
          "111:     val df = Seq(",
          "112:       (536361, \"85123A\", 2, 17850),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "110:     assume(shouldTestGroupedAggPandasUDFs)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "154:   test(\"UNSUPPORTED_FEATURE: Using pandas UDF aggregate expression with pivot\") {",
          "155:     import IntegratedUDFTestUtils._",
          "157:     val df = Seq(",
          "158:       (536361, \"85123A\", 2, 17850),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "157:     assume(shouldTestGroupedAggPandasUDFs)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "df786adb6a1ba97f232ea0fec14a0db493b9ca3d",
      "candidate_info": {
        "commit_hash": "df786adb6a1ba97f232ea0fec14a0db493b9ca3d",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/df786adb6a1ba97f232ea0fec14a0db493b9ca3d",
        "files": [
          "python/pyspark/pandas/groupby.py",
          "python/pyspark/pandas/tests/test_groupby.py"
        ],
        "message": "[SPARK-38837][PYTHON] Implement `dropna` parameter of `SeriesGroupBy.value_counts`\n\nImplement `dropna` parameter of `SeriesGroupBy.value_counts` to exclude counts of NaN.\n\nIt also fixes the behavior of `self._dropna` in the context of `SeriesGroupBy.value_counts`.\n\nTo reach parity with pandas.\n\nYes. `dropna` parameter of `SeriesGroupBy.value_counts` is supported.\n\n```py\n>>> psdf = ps.DataFrame(\n...             {\"A\": [np.nan, 2, 2, 3, 3, 3], \"B\": [1, 1, 2, 3, 3, np.nan]}, columns=[\"A\", \"B\"]\n...         )\n\n>>> psdf.groupby(\"A\")[\"B\"].value_counts(dropna=False).sort_index()\nA    B\n2.0  1.0    1\n     2.0    1\n3.0  3.0    2\n     NaN    1\nName: B, dtype: int64\n\n>>> psdf.groupby(\"A\", dropna=False)[\"B\"].value_counts(dropna=False).sort_index()   # self.dropna=False\nA    B\n2.0  1.0    1\n     2.0    1\n3.0  3.0    2\n     NaN    1\nNaN  1.0    1\nName: B, dtype: int64\n\n>>> psdf.groupby(\"A\")[\"B\"].value_counts(dropna=True).sort_index()\nA    B\n2.0  1.0    1\n     2.0    1\n3.0  3.0    2\nName: B, dtype: int64\n```\n\nUnit tests.\n\nCloses #36093 from xinrong-databricks/SeriesGroupBy.value_counts.\n\nAuthored-by: Xinrong Meng <xinrong.meng@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 2f122ba6d13ea26411fa4bf3e636ced449a8a205)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/pandas/groupby.py||python/pyspark/pandas/groupby.py",
          "python/pyspark/pandas/tests/test_groupby.py||python/pyspark/pandas/tests/test_groupby.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/pandas/groupby.py||python/pyspark/pandas/groupby.py": [
          "File: python/pyspark/pandas/groupby.py -> python/pyspark/pandas/groupby.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "3205:         Examples",
          "3206:         --------",
          "3207:         >>> df = ps.DataFrame({'A': [1, 2, 2, 3, 3, 3],",
          "3209:         ...                   columns=['A', 'B'])",
          "3210:         >>> df",
          "3219:         >>> df.groupby('A')['B'].value_counts().sort_index()  # doctest: +NORMALIZE_WHITESPACE",
          "3220:         A  B",
          "3225:         Name: B, dtype: int64",
          "3226:         \"\"\"",
          "3227:         groupkeys = self._groupkeys + self._agg_columns",
          "",
          "[Removed Lines]",
          "3208:         ...                    'B': [1, 1, 2, 3, 3, 3]},",
          "3211:            A  B",
          "3212:         0  1  1",
          "3213:         1  2  1",
          "3214:         2  2  2",
          "3215:         3  3  3",
          "3216:         4  3  3",
          "3217:         5  3  3",
          "3221:         1  1    1",
          "3222:         2  1    1",
          "3223:            2    1",
          "3224:         3  3    3",
          "",
          "[Added Lines]",
          "3208:         ...                    'B': [1, 1, 2, 3, 3, np.nan]},",
          "3211:            A    B",
          "3212:         0  1  1.0",
          "3213:         1  2  1.0",
          "3214:         2  2  2.0",
          "3215:         3  3  3.0",
          "3216:         4  3  3.0",
          "3217:         5  3  NaN",
          "3221:         1  1.0    1",
          "3222:         2  1.0    1",
          "3223:            2.0    1",
          "3224:         3  3.0    2",
          "3225:         Name: B, dtype: int64",
          "3227:         Don't include counts of NaN when dropna is False.",
          "3229:         >>> df.groupby('A')['B'].value_counts(",
          "3230:         ...   dropna=False).sort_index()  # doctest: +NORMALIZE_WHITESPACE",
          "3231:         A  B",
          "3232:         1  1.0    1",
          "3233:         2  1.0    1",
          "3234:            2.0    1",
          "3235:         3  3.0    2",
          "3236:            NaN    1",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3229:         groupkey_cols = [s.spark.column.alias(name) for s, name in zip(groupkeys, groupkey_names)]",
          "3231:         sdf = self._psdf._internal.spark_frame",
          "3232:         agg_column = self._agg_columns[0]._internal.data_spark_column_names[0]",
          "3233:         sdf = sdf.groupby(*groupkey_cols).count().withColumnRenamed(\"count\", agg_column)",
          "3235:         if sort:",
          "3236:             if ascending:",
          "3237:                 sdf = sdf.orderBy(scol_for(sdf, agg_column).asc())",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3248:         if self._dropna:",
          "3249:             _groupkey_column_names = groupkey_names[: len(self._groupkeys)]",
          "3250:             sdf = sdf.dropna(subset=_groupkey_column_names)",
          "3252:         if dropna:",
          "3253:             _agg_columns_names = groupkey_names[len(self._groupkeys) :]",
          "3254:             sdf = sdf.dropna(subset=_agg_columns_names)",
          "",
          "---------------"
        ],
        "python/pyspark/pandas/tests/test_groupby.py||python/pyspark/pandas/tests/test_groupby.py": [
          "File: python/pyspark/pandas/tests/test_groupby.py -> python/pyspark/pandas/tests/test_groupby.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1054:                     self.assertTrue(sorted(act) == sorted(exp))",
          "1056:     def test_value_counts(self):",
          "1058:         psdf = ps.from_pandas(pdf)",
          "1059:         self.assert_eq(",
          "1060:             psdf.groupby(\"A\")[\"B\"].value_counts().sort_index(),",
          "1061:             pdf.groupby(\"A\")[\"B\"].value_counts().sort_index(),",
          "1062:         )",
          "1063:         self.assert_eq(",
          "1064:             psdf.groupby(\"A\")[\"B\"].value_counts(sort=True, ascending=False).sort_index(),",
          "1065:             pdf.groupby(\"A\")[\"B\"].value_counts(sort=True, ascending=False).sort_index(),",
          "1066:         )",
          "1067:         self.assert_eq(",
          "1070:         )",
          "1071:         self.assert_eq(",
          "1072:             psdf.B.rename().groupby(psdf.A).value_counts().sort_index(),",
          "1073:             pdf.B.rename().groupby(pdf.A).value_counts().sort_index(),",
          "1074:         )",
          "1075:         self.assert_eq(",
          "1076:             psdf.B.groupby(psdf.A.rename()).value_counts().sort_index(),",
          "1077:             pdf.B.groupby(pdf.A.rename()).value_counts().sort_index(),",
          "",
          "[Removed Lines]",
          "1057:         pdf = pd.DataFrame({\"A\": [1, 2, 2, 3, 3, 3], \"B\": [1, 1, 2, 3, 3, 3]}, columns=[\"A\", \"B\"])",
          "1068:             psdf.groupby(\"A\")[\"B\"].value_counts(sort=True, ascending=True).sort_index(),",
          "1069:             pdf.groupby(\"A\")[\"B\"].value_counts(sort=True, ascending=True).sort_index(),",
          "",
          "[Added Lines]",
          "1057:         pdf = pd.DataFrame(",
          "1058:             {\"A\": [np.nan, 2, 2, 3, 3, 3], \"B\": [1, 1, 2, 3, 3, np.nan]}, columns=[\"A\", \"B\"]",
          "1059:         )",
          "1065:         self.assert_eq(",
          "1066:             psdf.groupby(\"A\")[\"B\"].value_counts(dropna=False).sort_index(),",
          "1067:             pdf.groupby(\"A\")[\"B\"].value_counts(dropna=False).sort_index(),",
          "1068:         )",
          "1069:         self.assert_eq(",
          "1070:             psdf.groupby(\"A\", dropna=False)[\"B\"].value_counts(dropna=False).sort_index(),",
          "1071:             pdf.groupby(\"A\", dropna=False)[\"B\"].value_counts(dropna=False).sort_index(),",
          "1072:             # Returns are the same considering values and types,",
          "1073:             # disable check_exact to pass the assert_eq",
          "1074:             check_exact=False,",
          "1075:         )",
          "1081:             psdf.groupby(\"A\")[\"B\"]",
          "1082:             .value_counts(sort=True, ascending=False, dropna=False)",
          "1083:             .sort_index(),",
          "1084:             pdf.groupby(\"A\")[\"B\"]",
          "1085:             .value_counts(sort=True, ascending=False, dropna=False)",
          "1086:             .sort_index(),",
          "1087:         )",
          "1088:         self.assert_eq(",
          "1089:             psdf.groupby(\"A\")[\"B\"]",
          "1090:             .value_counts(sort=True, ascending=True, dropna=False)",
          "1091:             .sort_index(),",
          "1092:             pdf.groupby(\"A\")[\"B\"]",
          "1093:             .value_counts(sort=True, ascending=True, dropna=False)",
          "1094:             .sort_index(),",
          "1100:         self.assert_eq(",
          "1101:             psdf.B.rename().groupby(psdf.A, dropna=False).value_counts().sort_index(),",
          "1102:             pdf.B.rename().groupby(pdf.A, dropna=False).value_counts().sort_index(),",
          "1103:             # Returns are the same considering values and types,",
          "1104:             # disable check_exact to pass the assert_eq",
          "1105:             check_exact=False,",
          "1106:         )",
          "",
          "---------------"
        ]
      }
    }
  ]
}