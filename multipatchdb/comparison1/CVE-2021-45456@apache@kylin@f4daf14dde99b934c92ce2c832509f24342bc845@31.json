{
  "cve_id": "CVE-2021-45456",
  "cve_desc": "Apache kylin checks the legitimacy of the project before executing some commands with the project name passed in by the user. There is a mismatch between what is being checked and what is being used as the shell command argument in DiagnosisService. This may cause an illegal project name to pass the check and perform the following steps, resulting in a command injection vulnerability. This issue affects Apache Kylin 4.0.0.",
  "repo": "apache/kylin",
  "patch_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
  "patch_info": {
    "commit_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
    "repo": "apache/kylin",
    "commit_url": "https://github.com/apache/kylin/commit/f4daf14dde99b934c92ce2c832509f24342bc845",
    "files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "server/src/main/webapp/WEB-INF/web.xml"
    ],
    "message": "test fix",
    "before_after_code_files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java"
    ]
  },
  "patch_diff": {
    "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "3403:     public String getKerberosPrincipal() {",
      "3404:         return getOptional(\"kylin.kerberos.principal\");",
      "3405:     }",
      "3406: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "3407:     public String getEncryptCipherIvSpec() {",
      "3408:         return getOptional(\"kylin.security.encrypt.cipher.ivSpec\", \"AAAAAAAAAAAAAAAA\");",
      "3409:     }",
      "",
      "---------------"
    ],
    "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java -> core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "25: import java.security.NoSuchAlgorithmException;",
      "27: import org.apache.commons.codec.binary.Base64;",
      "29: import javax.crypto.Cipher;",
      "30: import javax.crypto.NoSuchPaddingException;",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "28: import org.apache.kylin.common.KylinConfig;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "42:             InvalidKeyException, NoSuchPaddingException, NoSuchAlgorithmException, UnsupportedEncodingException {",
      "43:         Cipher cipher = Cipher.getInstance(\"AES/CFB/PKCS5Padding\");",
      "44:         final SecretKeySpec secretKey = new SecretKeySpec(key, \"AES\");",
      "46:         cipher.init(cipherMode, secretKey, ivSpec);",
      "47:         return cipher;",
      "48:     }",
      "",
      "[Removed Lines]",
      "45:         IvParameterSpec ivSpec = new IvParameterSpec(\"AAAAAAAAAAAAAAAA\".getBytes(\"UTF-8\"));",
      "",
      "[Added Lines]",
      "46:         IvParameterSpec ivSpec = new IvParameterSpec(KylinConfig.getInstanceFromEnv().getEncryptCipherIvSpec().getBytes(\"UTF-8\"));",
      "",
      "---------------"
    ],
    "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java": [
      "File: core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java -> core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: package org.apache.kylin.common.util;",
      "21: import org.junit.Assert;",
      "22: import org.junit.Test;",
      "26:     @Test",
      "27:     public void testAESEncrypt(){",
      "",
      "[Removed Lines]",
      "24: public class EncryptUtilTest {",
      "",
      "[Added Lines]",
      "21: import org.junit.After;",
      "23: import org.junit.Before;",
      "26: public class EncryptUtilTest extends LocalFileMetadataTestCase {",
      "27:     @Before",
      "28:     public void setUp() throws Exception {",
      "29:         this.createTestMetadata();",
      "30:     }",
      "32:     @After",
      "33:     public void after() throws Exception {",
      "34:         this.cleanupTestMetadata();",
      "35:     }",
      "",
      "---------------"
    ],
    "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java": [
      "File: server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java -> server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "87:     public String dumpProjectDiagnosisInfo(String project, File exportPath) throws IOException {",
      "88:         Message msg = MsgPicker.getMsg();",
      "89:         ProjectInstance projectInstance =",
      "90:                 ProjectManager.getInstance(KylinConfig.getInstanceFromEnv())",
      "92:         if (null == projectInstance) {",
      "93:             throw new BadRequestException(",
      "95:         }",
      "96:         aclEvaluate.checkProjectOperationPermission(projectInstance);",
      "98:         runDiagnosisCLI(args);",
      "99:         return getDiagnosisPackageName(exportPath);",
      "100:     }",
      "",
      "[Removed Lines]",
      "91:                         .getProject(ValidateUtil.convertStringToBeAlphanumericUnderscore(project));",
      "94:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), project));",
      "97:         String[] args = { project, exportPath.getAbsolutePath() };",
      "",
      "[Added Lines]",
      "89:         String projectName = ValidateUtil.convertStringToBeAlphanumericUnderscore(project);",
      "92:                         .getProject(projectName);",
      "95:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), projectName));",
      "98:         String[] args = { projectName, exportPath.getAbsolutePath() };",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "dfd012f45b9740ae0598041d7f1326e3a58c0da7",
      "candidate_info": {
        "commit_hash": "dfd012f45b9740ae0598041d7f1326e3a58c0da7",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/dfd012f45b9740ae0598041d7f1326e3a58c0da7",
        "files": [
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeTableEncoder.scala",
          "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala",
          "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala"
        ],
        "message": "KYLIN-4945 repartition encoded dataset to avoid data skew caused by single column",
        "before_after_code_files": [
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeTableEncoder.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeTableEncoder.scala",
          "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala||kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala",
          "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala||kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ],
          "candidate": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ]
        }
      },
      "candidate_diff": {
        "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "3104:     public int getMaxParentDatasetPersistCount() {",
          "3105:         return Integer.parseInt(getOptional(\"kylin.engine.spark.parent-dataset.max.persist.count\", \"1\"));",
          "3106:     }",
          "3107: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3108:     public int getRepartitionNumAfterEncode() {",
          "3109:         return Integer.valueOf(getOptional(\"kylin.engine.spark.dataset.repartition.num.after.encoding\", \"0\"));",
          "3110:     }",
          "3125:     public boolean rePartitionEncodedDatasetWithRowKey() {",
          "3126:         return Boolean.valueOf(getOptional(\"kylin.engine.spark.repartition.encoded.dataset\", \"false\"));",
          "3127:     }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeTableEncoder.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeTableEncoder.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeTableEncoder.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeTableEncoder.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "43:     val bucketThreshold = seg.kylinconf.getGlobalDictV2ThresholdBucketSize",
          "44:     val minBucketSize: Long = sourceCnt / bucketThreshold",
          "46:     cols.asScala.foreach(",
          "47:       ref => {",
          "48:         val globalDict = new NGlobalDictionary(seg.project, ref.tableAliasName, ref.columnName, seg.kylinconf.getHdfsWorkingDirectory)",
          "49:         val bucketSize = globalDict.getBucketSizeOrDefault(seg.kylinconf.getGlobalDictV2MinHashPartitions)",
          "50:         val enlargedBucketSize = (((minBucketSize / bucketSize) + 1) * bucketSize).toInt",
          "52:         val encodeColRef = convertFromDot(ref.identity)",
          "53:         val columnIndex = structType.fieldIndex(encodeColRef)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "46:     var repartitionSizeAfterEncode = 0;",
          "52:         if (enlargedBucketSize > repartitionSizeAfterEncode) {",
          "53:           repartitionSizeAfterEncode = enlargedBucketSize;",
          "54:         }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "63:           .select(columns: _*)",
          "64:       }",
          "65:     )",
          "66:     ds.sparkSession.sparkContext.setJobDescription(null)",
          "67:     partitionedDs",
          "68:   }",
          "69: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "74:     if (!cols.isEmpty && seg.kylinconf.rePartitionEncodedDatasetWithRowKey) {",
          "75:       val colsInDS = partitionedDs.schema.map(_.name)",
          "76:       val rowKeyColRefs = seg.allRowKeyCols.map(colDesc => convertFromDot(colDesc.identity)).filter(colsInDS.contains).map(col)",
          "78:       if (seg.kylinconf.getRepartitionNumAfterEncode > 0) {",
          "79:         repartitionSizeAfterEncode = seg.kylinconf.getRepartitionNumAfterEncode;",
          "80:       }",
          "81:       logInfo(s\"repartition encoded dataset to $repartitionSizeAfterEncode partitions to avoid data skew\")",
          "82:       partitionedDs = partitionedDs.repartition(repartitionSizeAfterEncode, rowKeyColRefs.toArray: _*)",
          "83:     }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala||kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala": [
          "File: kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala -> kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetaData.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "111:                        allDictColumns: Set[ColumnDesc],",
          "112:                        partitionExp: String,",
          "113:                        filterCondition: String,",
          "114:                        var snapshotInfo: Map[String, String] = Map.empty[String, String]) {",
          "116:   def updateLayout(layoutEntity: LayoutEntity): Unit = {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "114:                        allRowKeyCols: List[ColumnDesc],",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala||kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala": [
          "File: kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala -> kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "38: object MetadataConverter {",
          "39:   def getSegmentInfo(cubeInstance: CubeInstance, segmentId: String, segmentName: String, identifier: String): SegmentInfo = {",
          "41:     val (layoutEntities, measure) = extractEntityAndMeasures(cubeInstance)",
          "42:     val dictColumn = measure.values.filter(_.returnType.dataType.equals(\"bitmap\"))",
          "43:       .map(_.pra.head).toSet",
          "",
          "[Removed Lines]",
          "40:     val allColumnDesc = extractAllColumnDesc(cubeInstance)",
          "",
          "[Added Lines]",
          "40:     val (allColumnDesc, allRowKeyCols) = extractAllColumnDesc(cubeInstance)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "47:       dictColumn,",
          "48:       dictColumn,",
          "49:       extractPartitionExp(cubeInstance.getSegmentById(segmentId)),",
          "51:   }",
          "53:   def getCubeUpdate(segmentInfo: SegmentInfo): CubeUpdate = {",
          "",
          "[Removed Lines]",
          "50:       extractFilterCondition(cubeInstance.getSegmentById(segmentId)))",
          "",
          "[Added Lines]",
          "50:       extractFilterCondition(cubeInstance.getSegmentById(segmentId)),",
          "51:       allRowKeyCols.asScala.values.toList)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "94:       tb.getColumns.asScala.map(ref => toColumnDesc(ref = ref)).toList, tb.getAlias, tb.getTableDesc.getSourceType, addInfo)",
          "95:   }",
          "99:     val columns = cubeInstance.getDescriptor",
          "100:       .getRowkey",
          "101:       .getRowKeyColumns",
          "102:     val dimensionMapping = columns",
          "103:       .map(co => (co.getColRef, co.getBitIndex))",
          "104:     val set = dimensionMapping.map(_._1).toSet",
          "106:       .zipWithIndex",
          "107:       .map(tp => (tp._1, tp._2 + dimensionMapping.length))",
          "116:   }",
          "118:   def toLayoutEntity(cubeInstance: CubeInstance, cuboid: Cuboid): LayoutEntity = {",
          "",
          "[Removed Lines]",
          "97:   def extractAllColumnDesc(cubeInstance: CubeInstance): java.util.LinkedHashMap[Integer, ColumnDesc] = {",
          "98:     val dimensionIndex = new util.LinkedHashMap[Integer, ColumnDesc]()",
          "105:     val refs = cubeInstance.getAllColumns.asScala.diff(set)",
          "109:     val columnIDTuples = dimensionMapping ++ refs",
          "110:     val colToIndex = columnIDTuples.toMap",
          "111:     columnIDTuples",
          "112:       .foreach { co =>",
          "113:         dimensionIndex.put(co._2, toColumnDesc(co._1, co._2, set.contains(co._1)))",
          "114:       }",
          "115:     dimensionIndex",
          "",
          "[Added Lines]",
          "98:   def extractAllColumnDesc(cubeInstance: CubeInstance): (java.util.LinkedHashMap[Integer, ColumnDesc],",
          "99:     java.util.LinkedHashMap[Integer, ColumnDesc]) = {",
          "101:     val dimensions = new util.LinkedHashMap[Integer, ColumnDesc]()",
          "108:     val measureCols = cubeInstance.getAllColumns.asScala.diff(set)",
          "111:     dimensionMapping.foreach(co => dimensions.put(co._2, toColumnDesc(co._1, co._2, true)))",
          "112:     val allColumns = new util.LinkedHashMap[Integer, ColumnDesc]()",
          "114:     allColumns.putAll(dimensions)",
          "115:     measureCols.foreach(co =>  allColumns.put(co._2, toColumnDesc(co._1, co._2, false)))",
          "116:     (allColumns, dimensions)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "9a880c47b4c6dc6db47a6ea8c8cb0635f6ff3a82",
      "candidate_info": {
        "commit_hash": "9a880c47b4c6dc6db47a6ea8c8cb0635f6ff3a82",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/9a880c47b4c6dc6db47a6ea8c8cb0635f6ff3a82",
        "files": [
          "core-job/src/main/java/org/apache/kylin/job/impl/threadpool/DefaultFetcherRunner.java"
        ],
        "message": "KYLIN-4671 update log level for ignore too many logs\n\n(cherry picked from commit 80e924ea79c408bf983b741a88770fb1dda2be43)",
        "before_after_code_files": [
          "core-job/src/main/java/org/apache/kylin/job/impl/threadpool/DefaultFetcherRunner.java||core-job/src/main/java/org/apache/kylin/job/impl/threadpool/DefaultFetcherRunner.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core-job/src/main/java/org/apache/kylin/job/impl/threadpool/DefaultFetcherRunner.java||core-job/src/main/java/org/apache/kylin/job/impl/threadpool/DefaultFetcherRunner.java": [
          "File: core-job/src/main/java/org/apache/kylin/job/impl/threadpool/DefaultFetcherRunner.java -> core-job/src/main/java/org/apache/kylin/job/impl/threadpool/DefaultFetcherRunner.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "74:                 try {",
          "75:                     outputDigest = getExecutableManager().getOutputDigest(id);",
          "76:                 } catch (IllegalArgumentException e) {",
          "78:                     nOthers++;",
          "79:                     continue;",
          "80:                 }",
          "",
          "[Removed Lines]",
          "77:                     logger.warn(\"job \" + id + \" output digest is null, skip.\", e);",
          "",
          "[Added Lines]",
          "77:                     if (logger.isDebugEnabled()) {",
          "78:                         logger.debug(\"job \" + id + \" output digest is null.\", e);",
          "79:                     }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "94b109f363629a8829e9a90ac6f8853c6fd961b0",
      "candidate_info": {
        "commit_hash": "94b109f363629a8829e9a90ac6f8853c6fd961b0",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/94b109f363629a8829e9a90ac6f8853c6fd961b0",
        "files": [
          "core-cube/src/main/java/org/apache/kylin/cube/CubeCapabilityChecker.java",
          "core-cube/src/main/java/org/apache/kylin/cube/model/CubeDesc.java",
          "core-job/src/main/java/org/apache/kylin/job/execution/ExecutableManager.java"
        ],
        "message": "KYLIN-5027 Fix query for the cube dose not build basecuboid",
        "before_after_code_files": [
          "core-cube/src/main/java/org/apache/kylin/cube/CubeCapabilityChecker.java||core-cube/src/main/java/org/apache/kylin/cube/CubeCapabilityChecker.java",
          "core-cube/src/main/java/org/apache/kylin/cube/model/CubeDesc.java||core-cube/src/main/java/org/apache/kylin/cube/model/CubeDesc.java",
          "core-job/src/main/java/org/apache/kylin/job/execution/ExecutableManager.java||core-job/src/main/java/org/apache/kylin/job/execution/ExecutableManager.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core-cube/src/main/java/org/apache/kylin/cube/CubeCapabilityChecker.java||core-cube/src/main/java/org/apache/kylin/cube/CubeCapabilityChecker.java": [
          "File: core-cube/src/main/java/org/apache/kylin/cube/CubeCapabilityChecker.java -> core-cube/src/main/java/org/apache/kylin/cube/CubeCapabilityChecker.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import java.util.List;",
          "25: import java.util.Set;",
          "26: import java.util.Arrays;",
          "29: import org.apache.commons.collections.CollectionUtils;",
          "30: import org.apache.commons.lang.StringUtils;",
          "",
          "[Removed Lines]",
          "27: import java.util.stream.Collectors;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "183:         } else {",
          "184:             HashSet<TblColRef> aggResult = result;",
          "185:             for (AggregationGroup aggGroup : cubeDesc.getAggregationGroups()) {",
          "187:                 if (tmpAggResult.size() < aggResult.size()) {",
          "188:                     aggResult = tmpAggResult;",
          "189:                 }",
          "190:             }",
          "191:             result = aggResult;",
          "192:         }",
          "193:         return result;",
          "",
          "[Removed Lines]",
          "186:                 HashSet<TblColRef> tmpAggResult = (HashSet<TblColRef>) result.stream().filter(col -> !Arrays.asList(aggGroup.getIncludes()).contains(col.getCanonicalName())).collect(Collectors.toSet());",
          "",
          "[Added Lines]",
          "185:                 List<String> aggGroupColumn = Arrays.asList(aggGroup.getIncludes());",
          "186:                 HashSet<TblColRef> tmpAggResult = new HashSet<>(dimensionColumns);",
          "187:                 Iterator<TblColRef> iterator = result.iterator();",
          "188:                 while (iterator.hasNext()) {",
          "189:                     TblColRef col = iterator.next();",
          "190:                     String colName = col.getCanonicalName();",
          "191:                     String[] colInfo = colName.split(\"\\\\.\");",
          "193:                     if (colInfo.length == 3) {",
          "194:                         colName = colInfo[1] + \".\" + colInfo[2];",
          "195:                     }",
          "197:                     if (col.getTableAlias() != null) {",
          "198:                         colName = col.getTableAlias() + \".\" + colInfo[2];",
          "199:                     }",
          "201:                     if (aggGroupColumn.contains(colName)) {",
          "202:                         tmpAggResult.remove(col);",
          "203:                     }",
          "204:                 }",
          "209:             if (aggResult.size() > 0) {",
          "210:                 aggResult.removeAll(cubeDesc.listDerivedDimensionColumns());",
          "211:             }",
          "",
          "---------------"
        ],
        "core-cube/src/main/java/org/apache/kylin/cube/model/CubeDesc.java||core-cube/src/main/java/org/apache/kylin/cube/model/CubeDesc.java": [
          "File: core-cube/src/main/java/org/apache/kylin/cube/model/CubeDesc.java -> core-cube/src/main/java/org/apache/kylin/cube/model/CubeDesc.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "254:         return dimensionColumns == null ? null : Collections.unmodifiableSet(dimensionColumns);",
          "255:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "260:     public List<TblColRef> listDerivedDimensionColumns() {",
          "261:         List<TblColRef> result = new ArrayList<TblColRef>();",
          "262:         for (TblColRef col : dimensionColumns) {",
          "263:             if (isDerived(col)) {",
          "264:                 result.add(col);",
          "265:             }",
          "266:         }",
          "267:         return result;",
          "268:     }",
          "",
          "---------------"
        ],
        "core-job/src/main/java/org/apache/kylin/job/execution/ExecutableManager.java||core-job/src/main/java/org/apache/kylin/job/execution/ExecutableManager.java": [
          "File: core-job/src/main/java/org/apache/kylin/job/execution/ExecutableManager.java -> core-job/src/main/java/org/apache/kylin/job/execution/ExecutableManager.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "570:             if (endTime != 0) {",
          "571:                 long interruptTime = System.currentTimeMillis() - endTime + job.getInterruptTime();",
          "572:                 info = Maps.newHashMap(getJobOutput(jobId).getInfo());",
          "574:                 info.put(AbstractExecutable.INTERRUPT_TIME, Long.toString(interruptTime));",
          "575:                 info.remove(AbstractExecutable.END_TIME);",
          "576:             }",
          "",
          "[Removed Lines]",
          "573:                 getJobOutput(jobId).getInfo().remove(AbstractExecutable.END_TIME);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "683:                     jobOutput.setInfo(info);",
          "684:                 }",
          "685:             }",
          "686:             if (output != null) {",
          "687:                 if (output.length() > config.getJobOutputMaxSize()) {",
          "688:                     output = output.substring(0, config.getJobOutputMaxSize());",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "685:             if ((ExecutableState.ERROR.equals(oldStatus) || ExecutableState.STOPPED.equals(oldStatus))",
          "686:                     && ExecutableState.READY.equals(newStatus)) {",
          "687:                 jobOutput.getInfo().remove(AbstractExecutable.END_TIME);",
          "688:             }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bb54a2ab56ac3ab47b3cc77ed0f4fedd5ba857af",
      "candidate_info": {
        "commit_hash": "bb54a2ab56ac3ab47b3cc77ed0f4fedd5ba857af",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/bb54a2ab56ac3ab47b3cc77ed0f4fedd5ba857af",
        "files": [
          "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictBuilderAssist.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeDictionaryBuilder.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/DictHelper.scala",
          "kylin-spark-project/kylin-spark-engine/src/test/java/org/apache/kylin/engine/spark/dict/NGlobalDictionaryTest.java",
          "query/src/main/codegen/javacc/org/apache/kylin/query/util/CommentParser.jj"
        ],
        "message": "KYLIN-4926 Optimize Global Dict building: replace operation 'mapPartitions.count()' with 'foreachPartitions'",
        "before_after_code_files": [
          "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictBuilderAssist.scala||kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictBuilderAssist.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeDictionaryBuilder.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeDictionaryBuilder.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/DictHelper.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/DictHelper.scala",
          "kylin-spark-project/kylin-spark-engine/src/test/java/org/apache/kylin/engine/spark/dict/NGlobalDictionaryTest.java||kylin-spark-project/kylin-spark-engine/src/test/java/org/apache/kylin/engine/spark/dict/NGlobalDictionaryTest.java",
          "query/src/main/codegen/javacc/org/apache/kylin/query/util/CommentParser.jj||query/src/main/codegen/javacc/org/apache/kylin/query/util/CommentParser.jj"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictBuilderAssist.scala||kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictBuilderAssist.scala": [
          "File: kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictBuilderAssist.scala -> kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/dict/NGlobalDictBuilderAssist.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: import org.apache.spark.TaskContext",
          "26: import org.apache.spark.internal.Logging",
          "27: import org.apache.spark.sql.SparkSession",
          "29: import org.apache.spark.sql.functions.col",
          "30: import org.apache.spark.sql.types.StringType",
          "",
          "[Removed Lines]",
          "28: import org.apache.spark.sql.catalyst.encoders.RowEncoder",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "55:     ss.sparkContext.setJobDescription(\"Resize dict \" + ref.identity)",
          "56:     existsDictDs",
          "57:       .repartition(bucketPartitionSize, col(existsDictDs.schema.head.name).cast(StringType))",
          "59:         iter =>",
          "60:           val partitionID = TaskContext.get().partitionId()",
          "61:           logInfo(s\"Rebuild partition dict col: ${ref.identity}, partitionId: $partitionID\")",
          "",
          "[Removed Lines]",
          "58:       .mapPartitions {",
          "",
          "[Added Lines]",
          "57:       .foreachPartition {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "66:             bucketDict.addAbsoluteValue(dictTuple._1, dictTuple._2)",
          "67:           }",
          "68:           bucketDict.saveBucketDict(partitionID)",
          "73:     globalDict.writeMetaDict(bucketPartitionSize,",
          "74:       desc.kylinconf.getGlobalDictV2MaxVersions, desc.kylinconf.getGlobalDictV2VersionTTL)",
          "",
          "[Removed Lines]",
          "69:           Iterator.empty",
          "70:       }(RowEncoder.apply(existsDictDs.schema))",
          "71:       .count()",
          "",
          "[Added Lines]",
          "68:       }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeDictionaryBuilder.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeDictionaryBuilder.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeDictionaryBuilder.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CubeDictionaryBuilder.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "28: import org.apache.kylin.engine.spark.metadata.{ColumnDesc, SegmentInfo}",
          "29: import org.apache.spark.dict.NGlobalDictionary",
          "30: import org.apache.spark.internal.Logging",
          "32: import org.apache.spark.sql.functions.{col, expr}",
          "33: import org.apache.spark.sql.types.StringType",
          "34: import org.apache.spark.sql.{Column, Dataset, Row, SparkSession}",
          "",
          "[Removed Lines]",
          "31: import org.apache.spark.sql.catalyst.encoders.RowEncoder",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "79:     afterDistinct",
          "80:       .filter(dictCol.isNotNull)",
          "81:       .repartition(bucketPartitionSize, dictCol)",
          "83:         iter =>",
          "84:           DictHelper.genDict(columnName, broadcastDict, iter)",
          "88:     globalDict.writeMetaDict(bucketPartitionSize, seg.kylinconf.getGlobalDictV2MaxVersions, seg.kylinconf.getGlobalDictV2VersionTTL)",
          "89:   }",
          "",
          "[Removed Lines]",
          "82:       .mapPartitions {",
          "85:       }(RowEncoder.apply(schema = afterDistinct.schema))",
          "86:       .count()",
          "",
          "[Added Lines]",
          "81:       .foreachPartition {",
          "84:       }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/DictHelper.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/DictHelper.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/DictHelper.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/DictHelper.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import org.apache.spark.dict.NGlobalDictionary",
          "25: import org.apache.spark.internal.Logging",
          "29: object DictHelper extends Logging{",
          "32:     val partitionID = TaskContext.get().partitionId()",
          "33:     logInfo(s\"Build partition dict col: ${columnName}, partitionId: $partitionID\")",
          "34:     val broadcastGlobalDict = broadcastDict.value",
          "35:     val bucketDict = broadcastGlobalDict.loadBucketDictionary(partitionID)",
          "36:     iter.foreach(dic => bucketDict.addRelativeValue(dic.getString(0)))",
          "37:     bucketDict.saveBucketDict(partitionID)",
          "39:   }",
          "40: }",
          "",
          "[Removed Lines]",
          "27: import scala.collection.mutable.ListBuffer",
          "31:   def genDict(columnName: String, broadcastDict: Broadcast[NGlobalDictionary], iter: Iterator[Row]) = {",
          "38:     ListBuffer.empty.iterator",
          "",
          "[Added Lines]",
          "29:   def genDict(columnName: String, broadcastDict: Broadcast[NGlobalDictionary],",
          "30:               iter: Iterator[Row]): Unit = {",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/test/java/org/apache/kylin/engine/spark/dict/NGlobalDictionaryTest.java||kylin-spark-project/kylin-spark-engine/src/test/java/org/apache/kylin/engine/spark/dict/NGlobalDictionaryTest.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/test/java/org/apache/kylin/engine/spark/dict/NGlobalDictionaryTest.java -> kylin-spark-project/kylin-spark-engine/src/test/java/org/apache/kylin/engine/spark/dict/NGlobalDictionaryTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "28: import org.apache.kylin.job.exception.SchedulerException;",
          "29: import org.apache.spark.DebugFilesystem;",
          "30: import org.apache.spark.HashPartitioner;",
          "32: import org.apache.spark.api.java.function.PairFunction;",
          "33: import org.apache.spark.dict.NBucketDictionary;",
          "34: import org.apache.spark.dict.NGlobalDictHDFSStore;",
          "35: import org.apache.spark.dict.NGlobalDictMetaInfo;",
          "",
          "[Removed Lines]",
          "31: import org.apache.spark.api.java.function.Function2;",
          "",
          "[Added Lines]",
          "32: import org.apache.spark.api.java.function.VoidFunction;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "41: import org.apache.spark.sql.types.DataTypes;",
          "42: import org.apache.spark.sql.types.StructField;",
          "43: import org.apache.spark.sql.types.StructType;",
          "44: import org.junit.Assert;",
          "45: import org.junit.Test;",
          "46: import scala.Tuple2;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "44: import org.apache.spark.TaskContext;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "115:             if (row.get(0) == null)",
          "116:                 return new Tuple2<>(null, null);",
          "117:             return new Tuple2<>(row.get(0).toString(), null);",
          "120:                     NBucketDictionary bucketDict = dict.loadBucketDictionary(bucketId);",
          "121:                     while (tuple2Iterator.hasNext()) {",
          "122:                         Tuple2<String, String> tuple2 = tuple2Iterator.next();",
          "123:                         bucketDict.addRelativeValue(tuple2._1);",
          "124:                     }",
          "125:                     bucketDict.saveBucketDict(bucketId);",
          "129:         dict.writeMetaDict(BUCKET_SIZE, config.getGlobalDictV2MaxVersions(), config.getGlobalDictV2VersionTTL());",
          "130:     }",
          "",
          "[Removed Lines]",
          "118:         }).sortByKey().partitionBy(new HashPartitioner(BUCKET_SIZE)).mapPartitionsWithIndex(",
          "119:                 (Function2<Integer, Iterator<Tuple2<String, String>>, Iterator<Object>>) (bucketId, tuple2Iterator) -> {",
          "126:                     return Lists.newArrayList().iterator();",
          "127:                 }, true).count();",
          "",
          "[Added Lines]",
          "119:         }).sortByKey().partitionBy(new HashPartitioner(BUCKET_SIZE)).foreachPartition(",
          "120:                 (VoidFunction<Iterator<Tuple2<String, String>>>) (tuple2Iterator) -> {",
          "121:                     int bucketId = TaskContext.get().partitionId();",
          "128:                 });",
          "",
          "---------------"
        ],
        "query/src/main/codegen/javacc/org/apache/kylin/query/util/CommentParser.jj||query/src/main/codegen/javacc/org/apache/kylin/query/util/CommentParser.jj": [
          "File: query/src/main/codegen/javacc/org/apache/kylin/query/util/CommentParser.jj -> query/src/main/codegen/javacc/org/apache/kylin/query/util/CommentParser.jj"
        ]
      }
    },
    {
      "candidate_hash": "6847d860056e9479b32d1cea8395cb689539203e",
      "candidate_info": {
        "commit_hash": "6847d860056e9479b32d1cea8395cb689539203e",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/6847d860056e9479b32d1cea8395cb689539203e",
        "files": [
          "build/bin/prepare-hadoop-dependency.sh"
        ],
        "message": "KYLIN-5069 Fix",
        "before_after_code_files": [
          "build/bin/prepare_hadoop_dependency.sh||build/bin/prepare-hadoop-dependency.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "build/bin/prepare_hadoop_dependency.sh||build/bin/prepare-hadoop-dependency.sh": [
          "File: build/bin/prepare_hadoop_dependency.sh -> build/bin/prepare-hadoop-dependency.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "27:     return",
          "28: fi",
          "32:   return",
          "33: fi",
          "",
          "[Removed Lines]",
          "30: if [ ! -d \"$KYLIN_HOME/spark\" ]; then",
          "31:   echo \"Skip spark which not owned by kylin. SPARK_HOME is $SPARK_HOME and KYLIN_HOME is $KYLIN_HOME .\"",
          "",
          "[Added Lines]",
          "30: if [[ $SPARK_HOME != $KYLIN_HOME* ]]; then",
          "31:   echo \"Skip spark which not owned by kylin. SPARK_HOME is $SPARK_HOME and KYLIN_HOME is $KYLIN_HOME.",
          "32:   Please download the correct version of Apache Spark, unzip it, rename it to 'spark' and put it in $KYLIN_HOME directory.",
          "33:   Do not use the spark that comes with your hadoop environment.",
          "34:   If your hadoop environment is cdh6.x, you need to do some additional operations in advance.",
          "35:   Please refer to the link: https://cwiki.apache.org/confluence/display/KYLIN/Deploy+Kylin+4+on+CDH+6.\"",
          "",
          "---------------"
        ]
      }
    }
  ]
}