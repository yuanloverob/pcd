{
  "cve_id": "CVE-2021-45229",
  "cve_desc": "It was discovered that the \"Trigger DAG with config\" screen was susceptible to XSS attacks via the `origin` query argument. This issue affects Apache Airflow versions 2.2.3 and below.",
  "repo": "apache/airflow",
  "patch_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
  "patch_info": {
    "commit_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "files": [
      "airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py"
    ],
    "message": "Simplify trigger cancel button (#21591)\n\nCo-authored-by: Jed Cunningham <jedcunningham@apache.org>\n(cherry picked from commit 65297673a318660fba76797e50d0c06804dfcafc)",
    "before_after_code_files": [
      "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
    ]
  },
  "patch_diff": {
    "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html": [
      "File: airflow/www/templates/airflow/trigger.html -> airflow/www/templates/airflow/trigger.html",
      "--- Hunk 1 ---",
      "[Context before]",
      "63:       </label>",
      "64:     </div>",
      "65:     <button type=\"submit\" class=\"btn btn-primary\">Trigger</button>",
      "67:   </form>",
      "68: {% endblock %}",
      "",
      "[Removed Lines]",
      "66:     <button type=\"button\" class=\"btn\" onclick=\"location.href = '{{ origin }}'; return false\">Cancel</button>",
      "",
      "[Added Lines]",
      "66:     <a class=\"btn\" href=\"{{ origin }}\">Cancel</a>",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
      "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "133:         (\"javascript:alert(1)\", \"/home\"),",
      "134:         (\"http://google.com\", \"/home\"),",
      "135:         (\"36539'%3balert(1)%2f%2f166\", \"/home\"),",
      "136:         (",
      "137:             \"%2Ftree%3Fdag_id%3Dexample_bash_operator';alert(33)//\",",
      "138:             \"/home\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "136:         (",
      "137:             '\"><script>alert(99)</script><a href=\"',",
      "138:             \"&#34;&gt;&lt;script&gt;alert(99)&lt;/script&gt;&lt;a href=&#34;\",",
      "139:         ),",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "145:     test_dag_id = \"example_bash_operator\"",
      "147:     resp = admin_client.get(f'trigger?dag_id={test_dag_id}&origin={test_origin}')",
      "156: @pytest.mark.parametrize(",
      "",
      "[Removed Lines]",
      "148:     check_content_in_response(",
      "149:         '<button type=\"button\" class=\"btn\" onclick=\"location.href = \\'{}\\'; return false\">'.format(",
      "150:             expected_origin",
      "151:         ),",
      "152:         resp,",
      "153:     )",
      "",
      "[Added Lines]",
      "152:     check_content_in_response(f'<a class=\"btn\" href=\"{expected_origin}\">Cancel</a>', resp)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "6f4d29cee2fea277eaaa11490cb5871a788dd018",
      "candidate_info": {
        "commit_hash": "6f4d29cee2fea277eaaa11490cb5871a788dd018",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/6f4d29cee2fea277eaaa11490cb5871a788dd018",
        "files": [
          "tests/www/views/test_views_base.py"
        ],
        "message": "Add roles to create_user test (#20773)\n\nFlask App Builder 3.4.3 made role and conf_password obligatory\nwhen creating user:\nhttps://github.com/dpgaspar/Flask-AppBuilder/pull/1758\n\nOur test for user creation did not have the role set (though the\nUI used it and the cient also allows to set it).\n\nThis change adds the `roles` in our tests to enable upgrade\nto FAB 3.4.3 for the CI (currently tests in main fail because of\nthe test failure)\n\n(cherry picked from commit a1a32f7c7c2df41e6150f2594a3a41bfecb76fb0)",
        "before_after_code_files": [
          "tests/www/views/test_views_base.py||tests/www/views/test_views_base.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/www/views/test_views_base.py||tests/www/views/test_views_base.py": [
          "File: tests/www/views/test_views_base.py -> tests/www/views/test_views_base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "325:             'last_name': 'fake_last_name',",
          "326:             'username': non_exist_username,",
          "327:             'email': 'fake_email@email.com',",
          "328:             'password': 'test',",
          "329:             'conf_password': 'test',",
          "330:         },",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "328:             'roles': [1],",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0ba033daead44624847cbf26a5e19962575c94d0",
      "candidate_info": {
        "commit_hash": "0ba033daead44624847cbf26a5e19962575c94d0",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0ba033daead44624847cbf26a5e19962575c94d0",
        "files": [
          "dev/provider_packages/prepare_provider_packages.py"
        ],
        "message": "Actually fix tuple and bool checks for black 22.1.0 (#21221)\n\nPrevious two fixes in #21215 and #21216 did not really fix the\nproblem introduced by Black 22.1.0 (they could not as they were\nwrong). This change was actually tested with the new black and\nshould fix it finally.\n\n(cherry picked from commit f9e20067e0ac593fd18ad068fcc56501c6a99f2b)",
        "before_after_code_files": [
          "dev/provider_packages/prepare_provider_packages.py||dev/provider_packages/prepare_provider_packages.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/provider_packages/prepare_provider_packages.py||dev/provider_packages/prepare_provider_packages.py": [
          "File: dev/provider_packages/prepare_provider_packages.py -> dev/provider_packages/prepare_provider_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1334:                         )",
          "1335:                         # Returns 66 in case of doc-only changes",
          "1336:                         sys.exit(66)",
          "1337:                 except subprocess.CalledProcessError:",
          "1338:                     # ignore when the commit mentioned as last doc-only change is obsolete",
          "1339:                     pass",
          "1340:             console.print(f\"[yellow]The provider {provider_package_id} has changes since last release[/]\")",
          "1341:             console.print()",
          "1342:             console.print(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1337:                     if len(changes) > len(changes_since_last_doc_only_check):",
          "1338:                         # if doc-only was released after previous release - use it as starting point",
          "1339:                         # but if before - stay with the releases from last tag.",
          "1340:                         changes = changes_since_last_doc_only_check",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1697:     config = parse_pyproject_toml(os.path.join(SOURCE_DIR_PATH, \"pyproject.toml\"))",
          "1699:     target_versions = set(",
          "1701:     )",
          "1703:     return Mode(",
          "1704:         target_versions=target_versions,",
          "1705:         line_length=config.get('line_length', Mode.line_length),",
          "1710:         ),",
          "1711:     )",
          "",
          "[Removed Lines]",
          "1700:         target_version_option_callback(None, None, config.get('target_version', [])),",
          "1706:         is_pyi=config.get('is_pyi', Mode.is_pyi),",
          "1707:         string_normalization=not config.get('skip_string_normalization', not Mode.string_normalization),",
          "1708:         experimental_string_processing=config.get(",
          "1709:             'experimental_string_processing', Mode.experimental_string_processing",
          "",
          "[Added Lines]",
          "1705:         target_version_option_callback(None, None, tuple(config.get('target_version', ()))),",
          "1711:         is_pyi=bool(config.get('is_pyi', Mode.is_pyi)),",
          "1712:         string_normalization=not bool(config.get('skip_string_normalization', not Mode.string_normalization)),",
          "1713:         experimental_string_processing=bool(",
          "1714:             config.get('experimental_string_processing', Mode.experimental_string_processing)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2180:     'This module is deprecated. Please use `airflow.providers.amazon.aws.operators.sagemaker`.',",
          "2181:     'This module is deprecated. Please use `airflow.providers.amazon.aws.sensors.sagemaker`.',",
          "2182:     'This module is deprecated. Please use `airflow.providers.amazon.aws.hooks.emr`.',",
          "2183: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2188:     'This module is deprecated. Please use `airflow.providers.opsgenie.hooks.opsgenie`.',",
          "2189:     'This module is deprecated. Please use `airflow.providers.opsgenie.operators.opsgenie`.',",
          "2190:     'This module is deprecated. Please use `airflow.hooks.redshift_sql` '",
          "2191:     'or `airflow.hooks.redshift_cluster` as appropriate.',",
          "2192:     'This module is deprecated. Please use `airflow.providers.amazon.aws.operators.redshift_sql` or '",
          "2193:     '`airflow.providers.amazon.aws.operators.redshift_cluster` as appropriate.',",
          "2194:     'This module is deprecated. Please use `airflow.providers.amazon.aws.sensors.redshift_cluster`.',",
          "2195:     \"This module is deprecated. Please use airflow.providers.amazon.aws.transfers.sql_to_s3`.\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f25a58eebeb9ad3283fca2daa6666811f1a036c6",
      "candidate_info": {
        "commit_hash": "f25a58eebeb9ad3283fca2daa6666811f1a036c6",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f25a58eebeb9ad3283fca2daa6666811f1a036c6",
        "files": [
          "airflow/www/views.py",
          "tests/www/views/test_views.py"
        ],
        "message": "Show task status only for running dags or only for the last finished dag (#21352)\n\n* Show task status only for running dags or only for the last finished dag\n\n* Brought the logic of getting task statistics into a separate function\n\n(cherry picked from commit 28d7bde2750c38300e5cf70ba32be153b1a11f2c)",
        "before_after_code_files": [
          "airflow/www/views.py||airflow/www/views.py",
          "tests/www/views/test_views.py||tests/www/views/test_views.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "408:     return result",
          "411: ######################################################################################",
          "412: #                                    Error handlers",
          "413: ######################################################################################",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "411: def get_task_stats_from_query(qry):",
          "412:     \"\"\"",
          "413:     Return a dict of the task quantity, grouped by dag id and task status.",
          "415:     :param qry: The data in the format (<dag id>, <task state>, <is dag running>, <task count>),",
          "416:         ordered by <dag id> and <is dag running>",
          "417:     \"\"\"",
          "418:     data = {}",
          "419:     last_dag_id = None",
          "420:     has_running_dags = False",
          "421:     for dag_id, state, is_dag_running, count in qry:",
          "422:         if last_dag_id != dag_id:",
          "423:             last_dag_id = dag_id",
          "424:             has_running_dags = False",
          "425:         elif not is_dag_running and has_running_dags:",
          "426:             continue",
          "428:         if is_dag_running:",
          "429:             has_running_dags = True",
          "430:         if dag_id not in data:",
          "431:             data[dag_id] = {}",
          "432:         data[dag_id][state] = count",
          "433:     return data",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "815:         # Select all task_instances from active dag_runs.",
          "816:         running_task_instance_query_result = session.query(",
          "818:         ).join(",
          "819:             running_dag_run_query_result,",
          "820:             and_(",
          "",
          "[Removed Lines]",
          "817:             TaskInstance.dag_id.label('dag_id'), TaskInstance.state.label('state')",
          "",
          "[Added Lines]",
          "842:             TaskInstance.dag_id.label('dag_id'),",
          "843:             TaskInstance.state.label('state'),",
          "844:             sqla.literal(True).label('is_dag_running'),",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "838:             # Select all task_instances from active dag_runs.",
          "839:             # If no dag_run is active, return task instances from most recent dag_run.",
          "840:             last_task_instance_query_result = (",
          "842:                 .join(TaskInstance.dag_run)",
          "843:                 .join(",
          "844:                     last_dag_run,",
          "",
          "[Removed Lines]",
          "841:                 session.query(TaskInstance.dag_id.label('dag_id'), TaskInstance.state.label('state'))",
          "",
          "[Added Lines]",
          "868:                 session.query(",
          "869:                     TaskInstance.dag_id.label('dag_id'),",
          "870:                     TaskInstance.state.label('state'),",
          "871:                     sqla.literal(False).label('is_dag_running'),",
          "872:                 )",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "855:         else:",
          "856:             final_task_instance_query_result = running_task_instance_query_result.subquery('final_ti')",
          "870:         payload = {}",
          "871:         for dag_id in filter_dag_ids:",
          "872:             payload[dag_id] = []",
          "",
          "[Removed Lines]",
          "858:         qry = session.query(",
          "859:             final_task_instance_query_result.c.dag_id,",
          "860:             final_task_instance_query_result.c.state,",
          "861:             sqla.func.count(),",
          "862:         ).group_by(final_task_instance_query_result.c.dag_id, final_task_instance_query_result.c.state)",
          "864:         data = {}",
          "865:         for dag_id, state, count in qry:",
          "866:             if dag_id not in data:",
          "867:                 data[dag_id] = {}",
          "868:             data[dag_id][state] = count",
          "",
          "[Added Lines]",
          "889:         qry = (",
          "890:             session.query(",
          "891:                 final_task_instance_query_result.c.dag_id,",
          "892:                 final_task_instance_query_result.c.state,",
          "893:                 final_task_instance_query_result.c.is_dag_running,",
          "894:                 sqla.func.count(),",
          "895:             )",
          "896:             .group_by(",
          "897:                 final_task_instance_query_result.c.dag_id,",
          "898:                 final_task_instance_query_result.c.state,",
          "899:                 final_task_instance_query_result.c.is_dag_running,",
          "900:             )",
          "901:             .order_by(",
          "902:                 final_task_instance_query_result.c.dag_id,",
          "903:                 final_task_instance_query_result.c.is_dag_running.desc(),",
          "904:             )",
          "905:         )",
          "907:         data = get_task_stats_from_query(qry)",
          "",
          "---------------"
        ],
        "tests/www/views/test_views.py||tests/www/views/test_views.py": [
          "File: tests/www/views/test_views.py -> tests/www/views/test_views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: from airflow.configuration import initialize_config",
          "25: from airflow.plugins_manager import AirflowPlugin, EntryPointSource",
          "26: from airflow.www import views",
          "28: from tests.test_utils.config import conf_vars",
          "29: from tests.test_utils.mock_plugins import mock_plugin_manager",
          "30: from tests.test_utils.www import check_content_in_response, check_content_not_in_response",
          "",
          "[Removed Lines]",
          "27: from airflow.www.views import get_key_paths, get_safe_url, get_value_from_path, truncate_task_duration",
          "",
          "[Added Lines]",
          "27: from airflow.www.views import (",
          "28:     get_key_paths,",
          "29:     get_safe_url,",
          "30:     get_task_stats_from_query,",
          "31:     get_value_from_path,",
          "32:     truncate_task_duration,",
          "33: )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "333:     action_funcs = action_funcs - {\"action_post\"}",
          "334:     for action_function in action_funcs:",
          "335:         assert_decorator_used(cls, action_function, views.action_has_dag_edit_access)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "344: def test_get_task_stats_from_query():",
          "345:     query_data = [",
          "346:         ['dag1', 'queued', True, 1],",
          "347:         ['dag1', 'running', True, 2],",
          "348:         ['dag1', 'success', False, 3],",
          "349:         ['dag2', 'running', True, 4],",
          "350:         ['dag2', 'success', True, 5],",
          "351:         ['dag3', 'success', False, 6],",
          "352:     ]",
          "353:     expected_data = {",
          "354:         'dag1': {",
          "355:             'queued': 1,",
          "356:             'running': 2,",
          "357:         },",
          "358:         'dag2': {",
          "359:             'running': 4,",
          "360:             'success': 5,",
          "361:         },",
          "362:         'dag3': {",
          "363:             'success': 6,",
          "364:         },",
          "365:     }",
          "367:     data = get_task_stats_from_query(query_data)",
          "368:     assert data == expected_data",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "027e1d1e114ab92410bb3e1a8c2edfa9a9b5134b",
      "candidate_info": {
        "commit_hash": "027e1d1e114ab92410bb3e1a8c2edfa9a9b5134b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/027e1d1e114ab92410bb3e1a8c2edfa9a9b5134b",
        "files": [
          "README.md",
          "docs/docker-stack/docker-examples/extending/add-apt-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-build-essential-extend/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-providers/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-pypi-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/embedding-dags/Dockerfile",
          "docs/docker-stack/docker-examples/extending/writable-directory/Dockerfile",
          "docs/docker-stack/docker-examples/restricted/restricted_environments.sh",
          "docs/docker-stack/entrypoint.rst",
          "setup.py"
        ],
        "message": "Bump version to 2.2.4",
        "before_after_code_files": [
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "42: logger = logging.getLogger(__name__)",
          "46: my_dir = dirname(__file__)",
          "",
          "[Removed Lines]",
          "44: version = '2.2.3'",
          "",
          "[Added Lines]",
          "44: version = '2.2.4'",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "601d00c61dab6b12b925d76602347fd7ae2d51b5",
      "candidate_info": {
        "commit_hash": "601d00c61dab6b12b925d76602347fd7ae2d51b5",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/601d00c61dab6b12b925d76602347fd7ae2d51b5",
        "files": [
          ".github/workflows/ci.yml",
          "docker_tests/test_docker_compose_quick_start.py",
          "docker_tests/test_prod_image.py",
          "scripts/ci/images/ci_run_docker_compose_quick_start_test.sh",
          "scripts/ci/images/ci_run_docker_tests.py"
        ],
        "message": "Add tests for docker-compose quick start (#19874)\n\n(cherry picked from commit 0df50f42dde4bd9b4c99cb6646416dde6fd4961e)",
        "before_after_code_files": [
          "docker_tests/test_docker_compose_quick_start.py||docker_tests/test_docker_compose_quick_start.py",
          "docker_tests/test_prod_image.py||docker_tests/test_prod_image.py",
          "scripts/ci/images/ci_run_docker_compose_quick_start_test.sh||scripts/ci/images/ci_run_docker_compose_quick_start_test.sh",
          "scripts/ci/images/ci_run_docker_tests.py||scripts/ci/images/ci_run_docker_tests.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "docker_tests/test_docker_compose_quick_start.py||docker_tests/test_docker_compose_quick_start.py": [
          "File: docker_tests/test_docker_compose_quick_start.py -> docker_tests/test_docker_compose_quick_start.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "18: import contextlib",
          "19: import os",
          "20: import subprocess",
          "21: import tempfile",
          "22: from pathlib import Path",
          "23: from pprint import pprint",
          "24: from shutil import copyfile",
          "25: from time import monotonic, sleep",
          "26: from typing import Dict",
          "27: from unittest import mock",
          "29: import requests",
          "31: from docker_tests.command_utils import run_command",
          "32: from docker_tests.constants import SOURCE_ROOT",
          "33: from docker_tests.docker_tests_utils import docker_image",
          "35: AIRFLOW_WWW_USER_USERNAME = os.environ.get(\"_AIRFLOW_WWW_USER_USERNAME\", \"airflow\")",
          "36: AIRFLOW_WWW_USER_PASSWORD = os.environ.get(\"_AIRFLOW_WWW_USER_PASSWORD\", \"airflow\")",
          "37: DAG_ID = \"example_bash_operator\"",
          "38: DAG_RUN_ID = \"test_dag_run_id\"",
          "41: def api_request(method: str, path: str, base_url: str = \"http://localhost:8080/api/v1\", **kwargs) -> Dict:",
          "42:     response = requests.request(",
          "43:         method=method,",
          "44:         url=f\"{base_url}/{path}\",",
          "45:         auth=(AIRFLOW_WWW_USER_USERNAME, AIRFLOW_WWW_USER_PASSWORD),",
          "46:         headers={\"Content-Type\": \"application/json\"},",
          "48:     )",
          "49:     response.raise_for_status()",
          "50:     return response.json()",
          "53: @contextlib.contextmanager",
          "54: def tmp_chdir(path):",
          "55:     current_cwd = os.getcwd()",
          "56:     try:",
          "57:         os.chdir(path)",
          "58:         yield current_cwd",
          "59:     finally:",
          "60:         os.chdir(current_cwd)",
          "63: def wait_for_container(container_id: str, timeout: int = 300):",
          "64:     container_name = (",
          "65:         subprocess.check_output([\"docker\", \"inspect\", container_id, \"--format\", '{{ .Name }}'])",
          "66:         .decode()",
          "67:         .strip()",
          "68:     )",
          "69:     print(f\"Waiting for container: {container_name} [{container_id}]\")",
          "70:     waiting_done = False",
          "71:     start_time = monotonic()",
          "72:     while not waiting_done:",
          "73:         container_state = (",
          "74:             subprocess.check_output([\"docker\", \"inspect\", container_id, \"--format\", '{{ .State.Status }}'])",
          "75:             .decode()",
          "76:             .strip()",
          "77:         )",
          "78:         if container_state in (\"running\", 'restarting'):",
          "79:             health_status = (",
          "80:                 subprocess.check_output(",
          "81:                     [",
          "82:                         \"docker\",",
          "83:                         \"inspect\",",
          "84:                         container_id,",
          "85:                         \"--format\",",
          "86:                         \"{{ if .State.Health }}{{ .State.Health.Status }}{{ else }}no-check{{ end }}\",",
          "87:                     ]",
          "88:                 )",
          "89:                 .decode()",
          "90:                 .strip()",
          "91:             )",
          "92:             print(f\"{container_name}: container_state={container_state}, health_status={health_status}\")",
          "94:             if health_status == \"healthy\" or health_status == \"no-check\":",
          "95:                 waiting_done = True",
          "96:         else:",
          "97:             print(f\"{container_name}: container_state={container_state}\")",
          "98:             waiting_done = True",
          "99:         if timeout != 0 and monotonic() - start_time > timeout:",
          "100:             raise Exception(f\"Timeout. The operation takes longer than the maximum waiting time ({timeout}s)\")",
          "101:         sleep(1)",
          "104: def wait_for_terminal_dag_state(dag_id, dag_run_id):",
          "105:     # Wait 30 seconds",
          "106:     for _ in range(30):",
          "107:         dag_state = api_request(\"GET\", f\"dags/{dag_id}/dagRuns/{dag_run_id}\").get(\"state\")",
          "108:         print(f\"Waiting for DAG Run: dag_state={dag_state}\")",
          "109:         sleep(1)",
          "110:         if dag_state in (\"success\", \"failed\"):",
          "111:             break",
          "114: def test_trigger_dag_and_wait_for_result():",
          "115:     compose_file_path = SOURCE_ROOT / \"docs\" / \"apache-airflow\" / \"start\" / \"docker-compose.yaml\"",
          "117:     with tempfile.TemporaryDirectory() as tmp_dir, tmp_chdir(tmp_dir), mock.patch.dict(",
          "118:         'os.environ', AIRFLOW_IMAGE_NAME=docker_image",
          "119:     ):",
          "120:         copyfile(str(compose_file_path), f\"{tmp_dir}/docker-compose.yaml\")",
          "121:         os.mkdir(f\"{tmp_dir}/dags\")",
          "122:         os.mkdir(f\"{tmp_dir}/logs\")",
          "123:         os.mkdir(f\"{tmp_dir}/plugins\")",
          "124:         (Path(tmp_dir) / \".env\").write_text(f\"AIRFLOW_UID={subprocess.check_output(['id', '-u']).decode()}\\n\")",
          "125:         print(\".emv=\", (Path(tmp_dir) / \".env\").read_text())",
          "126:         copyfile(",
          "127:             str(SOURCE_ROOT / \"airflow\" / \"example_dags\" / \"example_bash_operator.py\"),",
          "128:             f\"{tmp_dir}/dags/example_bash_operator.py\",",
          "129:         )",
          "131:         run_command([\"docker-compose\", \"config\"])",
          "132:         run_command([\"docker-compose\", \"down\", \"--volumes\", \"--remove-orphans\"])",
          "133:         try:",
          "134:             run_command([\"docker-compose\", \"up\", \"-d\"])",
          "135:             # The --wait condition was released in docker-compose v2.1.1, but we want to support",
          "136:             # docker-compose v1 yet.",
          "137:             # See:",
          "138:             # https://github.com/docker/compose/releases/tag/v2.1.1",
          "139:             # https://github.com/docker/compose/pull/8777",
          "140:             for container_id in (",
          "141:                 subprocess.check_output([\"docker-compose\", 'ps', '-q']).decode().strip().splitlines()",
          "142:             ):",
          "143:                 wait_for_container(container_id)",
          "144:             api_request(\"PATCH\", path=f\"dags/{DAG_ID}\", json={\"is_paused\": False})",
          "145:             api_request(\"POST\", path=f\"dags/{DAG_ID}/dagRuns\", json={\"dag_run_id\": DAG_RUN_ID})",
          "146:             try:",
          "147:                 wait_for_terminal_dag_state(dag_id=DAG_ID, dag_run_id=DAG_RUN_ID)",
          "148:                 dag_state = api_request(\"GET\", f\"dags/{DAG_ID}/dagRuns/{DAG_RUN_ID}\").get(\"state\")",
          "149:                 assert dag_state == \"success\"",
          "150:             except Exception:",
          "151:                 print(f\"HTTP: GET dags/{DAG_ID}/dagRuns/{DAG_RUN_ID}\")",
          "152:                 pprint(api_request(\"GET\", f\"dags/{DAG_ID}/dagRuns/{DAG_RUN_ID}\"))",
          "153:                 print(f\"HTTP: GET dags/{DAG_ID}/dagRuns/{DAG_RUN_ID}/taskInstances\")",
          "154:                 pprint(api_request(\"GET\", f\"dags/{DAG_ID}/dagRuns/{DAG_RUN_ID}/taskInstances\"))",
          "155:                 raise",
          "156:         except Exception:",
          "157:             run_command([\"docker\", \"ps\"])",
          "158:             run_command([\"docker-compose\", \"logs\"])",
          "159:             raise",
          "160:         finally:",
          "161:             run_command([\"docker-compose\", \"down\", \"--volumes\"])",
          "",
          "---------------"
        ],
        "docker_tests/test_prod_image.py||docker_tests/test_prod_image.py": [
          "File: docker_tests/test_prod_image.py -> docker_tests/test_prod_image.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "79:         packages_to_install = {f\"apache-airflow-providers-{d.replace('.', '-')}\" for d in lines}",
          "80:         assert len(packages_to_install) != 0",
          "83:         providers = json.loads(output)",
          "84:         packages_installed = {d['package_name'] for d in providers}",
          "85:         assert len(packages_installed) != 0",
          "",
          "[Removed Lines]",
          "82:         output = run_bash_in_docker(\"airflow providers list --output json\", stderr=subprocess.DEVNULL)",
          "",
          "[Added Lines]",
          "82:         output = run_bash_in_docker(",
          "83:             \"airflow providers list --output json\", stderr=subprocess.DEVNULL, return_output=True",
          "84:         )",
          "",
          "---------------"
        ],
        "scripts/ci/images/ci_run_docker_compose_quick_start_test.sh||scripts/ci/images/ci_run_docker_compose_quick_start_test.sh": [
          "File: scripts/ci/images/ci_run_docker_compose_quick_start_test.sh -> scripts/ci/images/ci_run_docker_compose_quick_start_test.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env bash",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: # shellcheck source=scripts/ci/libraries/_script_init.sh",
          "19: . \"$(dirname \"${BASH_SOURCE[0]}\")/../libraries/_script_init.sh\"",
          "22: DOCKER_IMAGE=\"${AIRFLOW_PROD_IMAGE}:${GITHUB_REGISTRY_PULL_IMAGE_TAG}\"",
          "23: export DOCKER_IMAGE",
          "25: build_images::prepare_prod_build",
          "26: push_pull_remove_images::wait_for_image \"${DOCKER_IMAGE}\"",
          "28: python3 \"${SCRIPTS_CI_DIR}/images/ci_run_docker_tests.py\" \"${AIRFLOW_SOURCES}/docker_tests/test_docker_compose_quick_start.py\"",
          "",
          "---------------"
        ],
        "scripts/ci/images/ci_run_docker_tests.py||scripts/ci/images/ci_run_docker_tests.py": [
          "File: scripts/ci/images/ci_run_docker_tests.py -> scripts/ci/images/ci_run_docker_tests.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "47:     return parser",
          "51:     print(f\"{CBLUE}$ {' '.join(shlex.quote(c) for c in cmd)}{CEND}\")",
          "55: def create_virtualenv():",
          "",
          "[Removed Lines]",
          "50: def run_verbose(cmd: List[str], **kwargs):",
          "52:     subprocess.run(cmd, **kwargs)",
          "",
          "[Added Lines]",
          "50: def run_verbose(cmd: List[str], *, check=True, **kwargs):",
          "52:     subprocess.run(cmd, check=check, **kwargs)",
          "",
          "---------------"
        ]
      }
    }
  ]
}