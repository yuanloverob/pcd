{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "86e3514ceeac0b494ec58cbd413ae957f4d4bc8d",
      "candidate_info": {
        "commit_hash": "86e3514ceeac0b494ec58cbd413ae957f4d4bc8d",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/86e3514ceeac0b494ec58cbd413ae957f4d4bc8d",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalog.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/DescribeNamespaceSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalogSuite.scala",
          "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/command/DescribeNamespaceSuite.scala"
        ],
        "message": "[SPARK-39547][SQL] V2SessionCatalog should not throw NoSuchDatabaseException in loadNamspaceMetadata\n\n### What changes were proposed in this pull request?\n\nThis change attempts to make V2SessionCatalog return NoSuchNameSpaceException rather than NoSuchDataseException\n\n### Why are the changes needed?\n\nif a catalog doesn't overrides `namespaceExists` it by default uses `loadNamespaceMetadata` and in case a `db` not exists loadNamespaceMetadata throws a `NoSuchDatabaseException` which is not catched and we see failures even with `if exists` clause. One such use case we observed was in iceberg table a post test clean up was failing with `NoSuchDatabaseException` now. Also queries such as `DROP TABLE IF EXISTS {}` fails with no such db exception.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nModified the UT to match the proposed behviour\n\nCloses #36948 from singhpk234/fix/loadNamespaceMetadata.\n\nAuthored-by: Prashant Singh <psinghvk@amazon.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 95133932a661742bf0dd1343bc7eda08f2cf752f)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalog.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalog.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/DescribeNamespaceSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/DescribeNamespaceSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalogSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalogSuite.scala",
          "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/command/DescribeNamespaceSuite.scala||sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/command/DescribeNamespaceSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalog.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalog.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalog.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalog.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import scala.collection.mutable",
          "26: import org.apache.spark.sql.catalyst.{FunctionIdentifier, SQLConfHelper, TableIdentifier}",
          "28: import org.apache.spark.sql.catalyst.catalog.{CatalogDatabase, CatalogTable, CatalogTableType, CatalogUtils, SessionCatalog}",
          "29: import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogV2Util, FunctionCatalog, Identifier, NamespaceChange, SupportsNamespaces, Table, TableCatalog, TableChange, V1Table}",
          "30: import org.apache.spark.sql.connector.catalog.NamespaceChange.RemoveProperty",
          "",
          "[Removed Lines]",
          "27: import org.apache.spark.sql.catalyst.analysis.{NoSuchTableException, TableAlreadyExistsException}",
          "",
          "[Added Lines]",
          "27: import org.apache.spark.sql.catalyst.analysis.{NoSuchDatabaseException, NoSuchTableException, TableAlreadyExistsException}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "244:   override def loadNamespaceMetadata(namespace: Array[String]): util.Map[String, String] = {",
          "245:     namespace match {",
          "246:       case Array(db) =>",
          "249:       case _ =>",
          "250:         throw QueryCompilationErrors.noSuchNamespaceError(namespace)",
          "",
          "[Removed Lines]",
          "247:         catalog.getDatabaseMetadata(db).toMetadata",
          "",
          "[Added Lines]",
          "247:         try {",
          "248:           catalog.getDatabaseMetadata(db).toMetadata",
          "249:         } catch {",
          "250:           case _: NoSuchDatabaseException =>",
          "251:             throw QueryCompilationErrors.noSuchNamespaceError(namespace)",
          "252:         }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/DescribeNamespaceSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/DescribeNamespaceSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/DescribeNamespaceSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/DescribeNamespaceSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "61: class DescribeNamespaceSuite extends DescribeNamespaceSuiteBase with CommandSuiteBase {",
          "62:   override def commandVersion: String = super[DescribeNamespaceSuiteBase].commandVersion",
          "63: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "62:   override def notFoundMsgPrefix: String = if (conf.useV1Command) \"Database\" else \"Namespace\"",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalogSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalogSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalogSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/V2SessionCatalogSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "850:   test(\"loadNamespaceMetadata: fail missing namespace\") {",
          "851:     val catalog = newCatalog()",
          "854:       catalog.loadNamespaceMetadata(testNs)",
          "855:     }",
          "",
          "[Removed Lines]",
          "853:     val exc = intercept[NoSuchDatabaseException] {",
          "",
          "[Added Lines]",
          "853:     val exc = intercept[NoSuchNamespaceException] {",
          "",
          "---------------"
        ],
        "sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/command/DescribeNamespaceSuite.scala||sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/command/DescribeNamespaceSuite.scala": [
          "File: sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/command/DescribeNamespaceSuite.scala -> sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/command/DescribeNamespaceSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: class DescribeNamespaceSuite extends v1.DescribeNamespaceSuiteBase with CommandSuiteBase {",
          "27:   override def commandVersion: String = super[DescribeNamespaceSuiteBase].commandVersion",
          "28: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27:   override def notFoundMsgPrefix: String = if (conf.useV1Command) \"Database\" else \"Namespace\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d616da74a4ed7202b3480ebc234eb109dcc86fb9",
      "candidate_info": {
        "commit_hash": "d616da74a4ed7202b3480ebc234eb109dcc86fb9",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/d616da74a4ed7202b3480ebc234eb109dcc86fb9",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/csv/CSVScan.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala"
        ],
        "message": "[SPARK-40468][SQL] Fix column pruning in CSV when _corrupt_record is selected\n\n### What changes were proposed in this pull request?\n\nThe PR fixes an issue when depending on the name of the `_corrupt_record` field, column pruning would behave differently for a record that has no parsing errors.\n\nFor example, with a CSV file like this (c1 and c2 columns):\n```\n1,a\n```\n\nBefore the patch, the following query would return:\n```scala\nval df = spark.read\n  .schema(\"c1 int, c2 string, x string, _corrupt_record string\")\n  .csv(\"file:/tmp/file.csv\")\n  .withColumn(\"x\", lit(\"A\"))\n\nResult:\n\n+---+---+---+---------------+\n|c1 |c2 |x  |_corrupt_record|\n+---+---+---+---------------+\n|1  |a  |A  |1,a            |\n+---+---+---+---------------+\n```\n\nHowever, if you rename the corrupt record column, the result is different (the original, arguably correct, behaviour before https://github.com/apache/spark/commit/959694271e30879c944d7fd5de2740571012460a):\n\n```scala\nval df = spark.read\n  .option(\"columnNameCorruptRecord\", \"corrupt_record\")\n  .schema(\"c1 int, c2 string, x string, corrupt_record string\")\n  .csv(\"file:/tmp/file.csv\") .withColumn(\"x\", lit(\"A\"))\n\n+---+---+---+--------------+\n|c1 |c2 |x  |corrupt_record|\n+---+---+---+--------------+\n|1  |a  |A  |null          |\n+---+---+---+--------------+\n```\n\nThis patch fixes the former so both results would return `null` for corrupt record as there are no parsing issues. Note that https://issues.apache.org/jira/browse/SPARK-38523 is still fixed and works correctly.\n\n### Why are the changes needed?\n\nFixes a bug where corrupt record would be non-null even though the record has no parsing errors.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, fixes the output of corrupt record with additional columns provided by user. Everything should be unchanged outside of that scenario.\n\n### How was this patch tested?\n\nI added a unit test that reproduces the issue.\n\nCloses #37909 from sadikovi/SPARK-40468.\n\nAuthored-by: Ivan Sadikov <ivan.sadikov@databricks.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>\n(cherry picked from commit 0776f9e7bcb10612eb977ed4884e9848aea86c33)\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/csv/CSVScan.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/csv/CSVScan.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv/CSVFileFormat.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "100:       hadoopConf: Configuration): (PartitionedFile) => Iterator[InternalRow] = {",
          "101:     val broadcastedHadoopConf =",
          "102:       sparkSession.sparkContext.broadcast(new SerializableConfiguration(hadoopConf))",
          "105:     val parsedOptions = new CSVOptions(",
          "106:       options,",
          "107:       columnPruning,",
          "",
          "[Removed Lines]",
          "103:     val columnPruning = sparkSession.sessionState.conf.csvColumnPruning &&",
          "104:       !requiredSchema.exists(_.name == sparkSession.sessionState.conf.columnNameOfCorruptRecord)",
          "",
          "[Added Lines]",
          "103:     val columnPruning = sparkSession.sessionState.conf.csvColumnPruning",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/csv/CSVScan.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/csv/CSVScan.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/csv/CSVScan.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/csv/CSVScan.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:     dataFilters: Seq[Expression] = Seq.empty)",
          "45:   extends TextBasedFileScan(sparkSession, options) {",
          "49:   private lazy val parsedOptions: CSVOptions = new CSVOptions(",
          "50:     options.asScala.toMap,",
          "51:     columnPruning = columnPruning,",
          "",
          "[Removed Lines]",
          "47:   val columnPruning = sparkSession.sessionState.conf.csvColumnPruning &&",
          "48:     !readDataSchema.exists(_.name == sparkSession.sessionState.conf.columnNameOfCorruptRecord)",
          "",
          "[Added Lines]",
          "47:   val columnPruning = sparkSession.sessionState.conf.csvColumnPruning",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1655:       Row(1, Date.valueOf(\"1983-08-04\"), null) :: Nil)",
          "1656:   }",
          "1658:   test(\"SPARK-23846: schema inferring touches less data if samplingRatio < 1.0\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1658:   test(\"SPARK-40468: column pruning with the corrupt record column\") {",
          "1659:     withTempPath { path =>",
          "1660:       Seq(\"1,a\").toDF()",
          "1661:         .repartition(1)",
          "1662:         .write.text(path.getAbsolutePath)",
          "1665:       val corruptRecordCol = spark.sessionState.conf.columnNameOfCorruptRecord",
          "1666:       var df = spark.read",
          "1667:         .schema(s\"c1 int, c2 string, x string, ${corruptRecordCol} string\")",
          "1668:         .csv(path.getAbsolutePath)",
          "1669:         .selectExpr(\"c1\", \"c2\", \"'A' as x\", corruptRecordCol)",
          "1671:       checkAnswer(df, Seq(Row(1, \"a\", \"A\", null)))",
          "1674:       df = spark.read",
          "1675:         .schema(s\"c1 int, c2 string, x string, _invalid string\")",
          "1676:         .option(\"columnNameCorruptRecord\", \"_invalid\")",
          "1677:         .csv(path.getAbsolutePath)",
          "1678:         .selectExpr(\"c1\", \"c2\", \"'A' as x\", \"_invalid\")",
          "1680:       checkAnswer(df, Seq(Row(1, \"a\", \"A\", null)))",
          "1681:     }",
          "1682:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0a6ed8acd7423fba3cb499c7c89a662e3818d66a",
      "candidate_info": {
        "commit_hash": "0a6ed8acd7423fba3cb499c7c89a662e3818d66a",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/0a6ed8acd7423fba3cb499c7c89a662e3818d66a",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBLoader.scala"
        ],
        "message": "[SPARK-39847][SS] Fix race condition in RocksDBLoader.loadLibrary() if caller thread is interrupted\n\n### What changes were proposed in this pull request?\n\nThis PR fixes a race condition in `RocksDBLoader.loadLibrary()`, which can occur if the thread which calls that method is interrupted.\n\nOne of our jobs experienced a failure in `RocksDBLoader`:\n\n```\nCaused by: java.lang.IllegalThreadStateException\n\tat java.lang.Thread.start(Thread.java:708)\n\tat org.apache.spark.sql.execution.streaming.state.RocksDBLoader$.loadLibrary(RocksDBLoader.scala:51)\n```\n\nAfter investigation, we determined that this was due to task cancellation/interruption: if the task which starts the RocksDB library loading is interrupted, another thread may begin a load and crash with the thread state exception:\n\n- Although the `loadLibraryThread` child thread is is uninterruptible, the task thread which calls loadLibrary is still interruptible.\n- Let's say we have two tasks, A and B, both of which will call `RocksDBLoader.loadLibrary()`\n- Say that Task A wins the race to perform the load and enters the `synchronized` block in `loadLibrary()`, starts the `loadLibraryThread`, then blocks in the `loadLibraryThread.join()` call.\n- If Task A is interrupted, an `InterruptedException` will be thrown and it will exit the loadLibrary synchronized block.\n- At this point, Task B enters the synchronized block of its `loadLibrary() call and sees that `exception == null` because the `loadLibraryThread` started by the other task is still running, so Task B calls `loadLibraryThread.start()` and hits the thread state error because it tries to start an already-started thread.\n\nThis PR fixes this issue by adding code to check `loadLibraryThread`'s state before calling `start()`: if the thread has already been started then we will skip the `start()` call and proceed directly to the `join()`. I also modified the logging so that we can detect when this case occurs.\n\n### Why are the changes needed?\n\nFix a bug that can lead to task or job failures.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nI reproduced the original race condition by adding a `Thread.sleep(10000)` to `loadLibraryThread.run()` (so it wouldn't complete instantly), then ran\n\n```scala\n  test(\"multi-threaded RocksDBLoader calls with interruption\") {\n\n    val taskThread = new Thread(\"interruptible Task Thread 1\") {\n      override def run(): Unit = {\n        RocksDBLoader.loadLibrary()\n      }\n    }\n\n    taskThread.start()\n    // Give the thread time to enter the `loadLibrary()` call:\n    Thread.sleep(1000)\n    taskThread.interrupt()\n    // Check that the load hasn't finished:\n    assert(RocksDBLoader.exception == null)\n    assert(RocksDBLoader.loadLibraryThread.getState != Thread.State.NEW)\n    // Simulate the second task thread starting the load:\n    RocksDBLoader.loadLibrary()\n    // The load should finish successfully:\n    RocksDBLoader.exception.isEmpty\n  }\n```\n\nThis test failed prior to my changes and succeeds afterwards.\n\nI don't want to actually commit this test because I'm concerned about flakiness and false-negatives: in order to ensure that the test would have failed before my change, we need to carefully control the thread interleaving. This code rarely changes and is relatively simple, so I think the ROI on spending time to write and commit a reliable test is low.\n\nCloses #37260 from JoshRosen/rocksdbloader-fix.\n\nAuthored-by: Josh Rosen <joshrosen@databricks.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 9cee1bb2527a496943ffedbd935dc737246a2d89)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBLoader.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBLoader.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBLoader.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBLoader.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBLoader.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBLoader.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "49:   def loadLibrary(): Unit = synchronized {",
          "50:     if (exception == null) {",
          "52:       logInfo(\"RocksDB library loading thread started\")",
          "53:       loadLibraryThread.join()",
          "54:       exception.foreach(throw _)",
          "",
          "[Removed Lines]",
          "51:       loadLibraryThread.start()",
          "",
          "[Added Lines]",
          "58:       if (loadLibraryThread.getState == Thread.State.NEW) {",
          "59:         loadLibraryThread.start()",
          "60:       }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "052ae962b5104e36af013117e13baa46202d428a",
      "candidate_info": {
        "commit_hash": "052ae962b5104e36af013117e13baa46202d428a",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/052ae962b5104e36af013117e13baa46202d428a",
        "files": [
          "python/pyspark/pandas/base.py"
        ],
        "message": "[SPARK-39030][PYTHON] Rename sum to avoid shading the builtin Python function\n\n### What changes were proposed in this pull request?\nRename sum to something else.\n\n### Why are the changes needed?\nSum is a build in function in python. [SUM() at python docs](https://docs.python.org/3/library/functions.html#sum)\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nUse existing tests.\n\nCloses #36364 from bjornjorgensen/rename-sum.\n\nAuthored-by: bjornjorgensen <bjornjorgensen@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 3821d807a599a2d243465b4e443f1eb68251d432)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/pandas/base.py||python/pyspark/pandas/base.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/pandas/base.py||python/pyspark/pandas/base.py": [
          "File: python/pyspark/pandas/base.py -> python/pyspark/pandas/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1362:                 sdf = sdf.orderBy(F.col(\"count\").desc())",
          "1364:         if normalize:",
          "1368:         internal = InternalFrame(",
          "1369:             spark_frame=sdf,",
          "",
          "[Removed Lines]",
          "1365:             sum = sdf_dropna.count()",
          "1366:             sdf = sdf.withColumn(\"count\", F.col(\"count\") / SF.lit(sum))",
          "",
          "[Added Lines]",
          "1365:             drop_sum = sdf_dropna.count()",
          "1366:             sdf = sdf.withColumn(\"count\", F.col(\"count\") / SF.lit(drop_sum))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7d57577037fe082b6b1ded093943669dd1f8dd05",
      "candidate_info": {
        "commit_hash": "7d57577037fe082b6b1ded093943669dd1f8dd05",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/7d57577037fe082b6b1ded093943669dd1f8dd05",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/v2ResolutionPlans.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/ExplainSuite.scala"
        ],
        "message": "[SPARK-39112][SQL] UnsupportedOperationException if spark.sql.ui.explainMode is set to cost\n\n### What changes were proposed in this pull request?\n\nAdd a new leaf like node `LeafNodeWithoutStats` and apply to the list:\n- ResolvedDBObjectName\n- ResolvedNamespace\n- ResolvedTable\n- ResolvedView\n- ResolvedNonPersistentFunc\n- ResolvedPersistentFunc\n\n### Why are the changes needed?\n\nWe enable v2 command at 3.3.0 branch by default `spark.sql.legacy.useV1Command`. However this is a behavior change between v1 and c2 command.\n\n- v1 command:\n  We resolve logical plan to command at analyzer phase by `ResolveSessionCatalog`\n\n- v2 commnd:\n  We resolve logical plan to v2 command at physical phase by `DataSourceV2Strategy`\n\nFoe cost explain mode, we will call `LogicalPlanStats.stats` using optimized plan so there is a gap between v1 and v2 command.\nUnfortunately, the logical plan of v2 command contains the `LeafNode` which does not override the `computeStats`. As a result, there is a error running such sql:\n```sql\nset spark.sql.ui.explainMode=cost;\nshow tables;\n```\n\n```\njava.lang.UnsupportedOperationException:\n\tat org.apache.spark.sql.catalyst.plans.logical.LeafNode.computeStats(LogicalPlan.scala:171)\n\tat org.apache.spark.sql.catalyst.plans.logical.LeafNode.computeStats$(LogicalPlan.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.ResolvedNamespace.computeStats(v2ResolutionPlans.scala:155)\n\tat org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.default(SizeInBytesOnlyStatsPlanVisitor.scala:55)\n\tat org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.default(SizeInBytesOnlyStatsPlanVisitor.scala:27)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlanVisitor.visit(LogicalPlanVisitor.scala:49)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlanVisitor.visit$(LogicalPlanVisitor.scala:25)\n\tat org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.visit(SizeInBytesOnlyStatsPlanVisitor.scala:27)\n\tat org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats.$anonfun$stats$1(LogicalPlanStats.scala:37)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats.stats(LogicalPlanStats.scala:33)\n\tat org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats.stats$(LogicalPlanStats.scala:33)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.stats(LogicalPlan.scala:30)\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nyes, bug fix\n\n### How was this patch tested?\n\nadd test\n\nCloses #36488 from ulysses-you/SPARK-39112.\n\nAuthored-by: ulysses-you <ulyssesyou18@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 06fd340daefd67a3e96393539401c9bf4b3cbde9)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/v2ResolutionPlans.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/v2ResolutionPlans.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/ExplainSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/ExplainSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/v2ResolutionPlans.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/v2ResolutionPlans.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/v2ResolutionPlans.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/v2ResolutionPlans.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import org.apache.spark.sql.catalyst.InternalRow",
          "21: import org.apache.spark.sql.catalyst.catalog.CatalogTypes.TablePartitionSpec",
          "22: import org.apache.spark.sql.catalyst.expressions.{Attribute, LeafExpression, Unevaluable}",
          "24: import org.apache.spark.sql.catalyst.trees.TreePattern.{TreePattern, UNRESOLVED_FUNC}",
          "25: import org.apache.spark.sql.catalyst.util.CharVarcharUtils",
          "26: import org.apache.spark.sql.connector.catalog.{CatalogPlugin, FunctionCatalog, Identifier, Table, TableCatalog}",
          "",
          "[Removed Lines]",
          "23: import org.apache.spark.sql.catalyst.plans.logical.LeafNode",
          "",
          "[Added Lines]",
          "23: import org.apache.spark.sql.catalyst.plans.logical.{LeafNode, Statistics}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "140:   override def output: Seq[Attribute] = Nil",
          "141: }",
          "146: case class ResolvedNamespace(catalog: CatalogPlugin, namespace: Seq[String])",
          "148:   override def output: Seq[Attribute] = Nil",
          "149: }",
          "",
          "[Removed Lines]",
          "147:   extends LeafNode {",
          "",
          "[Added Lines]",
          "146: trait LeafNodeWithoutStats extends LeafNode {",
          "148:   override def stats: Statistics = Statistics.DUMMY",
          "149: }",
          "155:   extends LeafNodeWithoutStats {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "156:     identifier: Identifier,",
          "157:     table: Table,",
          "158:     outputAttributes: Seq[Attribute])",
          "160:   override def output: Seq[Attribute] = {",
          "161:     val qualifier = catalog.name +: identifier.namespace :+ identifier.name",
          "162:     outputAttributes.map(_.withQualifier(qualifier))",
          "",
          "[Removed Lines]",
          "159:   extends LeafNode {",
          "",
          "[Added Lines]",
          "167:   extends LeafNodeWithoutStats {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "195:   override def output: Seq[Attribute] = Nil",
          "196: }",
          "",
          "[Removed Lines]",
          "194: case class ResolvedView(identifier: Identifier, isTemp: Boolean) extends LeafNode {",
          "",
          "[Added Lines]",
          "202: case class ResolvedView(identifier: Identifier, isTemp: Boolean) extends LeafNodeWithoutStats {",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "202:     catalog: FunctionCatalog,",
          "203:     identifier: Identifier,",
          "204:     func: UnboundFunction)",
          "206:   override def output: Seq[Attribute] = Nil",
          "207: }",
          "213:   override def output: Seq[Attribute] = Nil",
          "214: }",
          "220:   override def output: Seq[Attribute] = Nil",
          "221: }",
          "",
          "[Removed Lines]",
          "205:   extends LeafNode {",
          "212: case class ResolvedNonPersistentFunc(name: String, func: UnboundFunction) extends LeafNode {",
          "219: case class ResolvedDBObjectName(catalog: CatalogPlugin, nameParts: Seq[String]) extends LeafNode {",
          "",
          "[Added Lines]",
          "213:   extends LeafNodeWithoutStats {",
          "220: case class ResolvedNonPersistentFunc(",
          "221:     name: String,",
          "222:     func: UnboundFunction)",
          "223:   extends LeafNodeWithoutStats {",
          "230: case class ResolvedDBObjectName(",
          "231:     catalog: CatalogPlugin,",
          "232:     nameParts: Seq[String])",
          "233:   extends LeafNodeWithoutStats {",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/ExplainSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/ExplainSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/ExplainSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/ExplainSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "528:         \"== Analyzed Logical Plan ==\\nCreateViewCommand\")",
          "529:     }",
          "530:   }",
          "531: }",
          "533: class ExplainSuiteAE extends ExplainSuiteHelper with EnableAdaptiveExecutionSuite {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "532:   test(\"SPARK-39112: UnsupportedOperationException if explain cost command using v2 command\") {",
          "533:     withTempDir { dir =>",
          "534:       sql(\"EXPLAIN COST CREATE DATABASE tmp\")",
          "535:       sql(\"EXPLAIN COST DESC DATABASE tmp\")",
          "536:       sql(s\"EXPLAIN COST ALTER DATABASE tmp SET LOCATION '${dir.toURI.toString}'\")",
          "537:       sql(\"EXPLAIN COST USE tmp\")",
          "538:       sql(\"EXPLAIN COST CREATE TABLE t(c1 int) USING PARQUET\")",
          "539:       sql(\"EXPLAIN COST SHOW TABLES\")",
          "540:       sql(\"EXPLAIN COST SHOW CREATE TABLE t\")",
          "541:       sql(\"EXPLAIN COST SHOW TBLPROPERTIES t\")",
          "542:       sql(\"EXPLAIN COST DROP TABLE t\")",
          "543:       sql(\"EXPLAIN COST DROP DATABASE tmp\")",
          "544:     }",
          "545:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}