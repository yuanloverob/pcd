{
  "cve_id": "CVE-2021-45229",
  "cve_desc": "It was discovered that the \"Trigger DAG with config\" screen was susceptible to XSS attacks via the `origin` query argument. This issue affects Apache Airflow versions 2.2.3 and below.",
  "repo": "apache/airflow",
  "patch_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
  "patch_info": {
    "commit_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "files": [
      "airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py"
    ],
    "message": "Simplify trigger cancel button (#21591)\n\nCo-authored-by: Jed Cunningham <jedcunningham@apache.org>\n(cherry picked from commit 65297673a318660fba76797e50d0c06804dfcafc)",
    "before_after_code_files": [
      "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
    ]
  },
  "patch_diff": {
    "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html": [
      "File: airflow/www/templates/airflow/trigger.html -> airflow/www/templates/airflow/trigger.html",
      "--- Hunk 1 ---",
      "[Context before]",
      "63:       </label>",
      "64:     </div>",
      "65:     <button type=\"submit\" class=\"btn btn-primary\">Trigger</button>",
      "67:   </form>",
      "68: {% endblock %}",
      "",
      "[Removed Lines]",
      "66:     <button type=\"button\" class=\"btn\" onclick=\"location.href = '{{ origin }}'; return false\">Cancel</button>",
      "",
      "[Added Lines]",
      "66:     <a class=\"btn\" href=\"{{ origin }}\">Cancel</a>",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
      "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "133:         (\"javascript:alert(1)\", \"/home\"),",
      "134:         (\"http://google.com\", \"/home\"),",
      "135:         (\"36539'%3balert(1)%2f%2f166\", \"/home\"),",
      "136:         (",
      "137:             \"%2Ftree%3Fdag_id%3Dexample_bash_operator';alert(33)//\",",
      "138:             \"/home\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "136:         (",
      "137:             '\"><script>alert(99)</script><a href=\"',",
      "138:             \"&#34;&gt;&lt;script&gt;alert(99)&lt;/script&gt;&lt;a href=&#34;\",",
      "139:         ),",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "145:     test_dag_id = \"example_bash_operator\"",
      "147:     resp = admin_client.get(f'trigger?dag_id={test_dag_id}&origin={test_origin}')",
      "156: @pytest.mark.parametrize(",
      "",
      "[Removed Lines]",
      "148:     check_content_in_response(",
      "149:         '<button type=\"button\" class=\"btn\" onclick=\"location.href = \\'{}\\'; return false\">'.format(",
      "150:             expected_origin",
      "151:         ),",
      "152:         resp,",
      "153:     )",
      "",
      "[Added Lines]",
      "152:     check_content_in_response(f'<a class=\"btn\" href=\"{expected_origin}\">Cancel</a>', resp)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "40283e3b3b66cbc73a7fbb6818852b8289a0800d",
      "candidate_info": {
        "commit_hash": "40283e3b3b66cbc73a7fbb6818852b8289a0800d",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/40283e3b3b66cbc73a7fbb6818852b8289a0800d",
        "files": [
          "scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh"
        ],
        "message": "Fix CI tests so they correctly fail in case of error! (#19678)\n\n(cherry picked from commit 889f1571259ae5ce83fb8723ac2d10cd21dc9d50)",
        "before_after_code_files": [
          "scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh||scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh||scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh": [
          "File: scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh -> scripts/ci/testing/ci_run_single_airflow_test_in_docker.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "125:       \"${DOCKER_COMPOSE_LOCAL[@]}\" \\",
          "126:       --project-name \"airflow-${TEST_TYPE}-${BACKEND}\" \\",
          "127:          run airflow \"${@}\"",
          "129:     exit_code=$?",
          "130:     if [[ ${exit_code} != \"0\" && ${CI} == \"true\" ]]; then",
          "131:         docker ps --all",
          "132:         local container",
          "",
          "[Removed Lines]",
          "128:     docker ps",
          "",
          "[Added Lines]",
          "129:     docker ps",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "907d107525b8fb40c7b9175f7c27d38904392796",
      "candidate_info": {
        "commit_hash": "907d107525b8fb40c7b9175f7c27d38904392796",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/907d107525b8fb40c7b9175f7c27d38904392796",
        "files": [
          "scripts/ci/images/ci_run_docker_tests.py"
        ],
        "message": "Add color to pytest tests on CI (#20723)\n\nWhile GitHub CI supports coloured output, the programs cannot\ndetect it properly because the output is redirected. Similarly\nas in all other cases we force the color output.\n\n(cherry picked from commit 6ed3f5d97fe8f8967df5624f62e69ce2a58a9413)",
        "before_after_code_files": [
          "scripts/ci/images/ci_run_docker_tests.py||scripts/ci/images/ci_run_docker_tests.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/images/ci_run_docker_tests.py||scripts/ci/images/ci_run_docker_tests.py": [
          "File: scripts/ci/images/ci_run_docker_tests.py -> scripts/ci/images/ci_run_docker_tests.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "87:     if not extra_pytest_args:",
          "88:         raise SystemExit(\"You must select the tests to run.\")",
          "97:     run_verbose([str(python_bin), \"-m\", \"pytest\", *pytest_args, *extra_pytest_args])",
          "",
          "[Removed Lines]",
          "90:     pytest_args = (",
          "91:         \"--pythonwarnings=ignore::DeprecationWarning\",",
          "92:         \"--pythonwarnings=ignore::PendingDeprecationWarning\",",
          "93:         \"-n\",",
          "94:         \"auto\",",
          "95:     )",
          "",
          "[Added Lines]",
          "90:     pytest_args = (\"-n\", \"auto\", \"--color=yes\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a9c178e23dc918a184a99029f883fecfe2d0cd84",
      "candidate_info": {
        "commit_hash": "a9c178e23dc918a184a99029f883fecfe2d0cd84",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a9c178e23dc918a184a99029f883fecfe2d0cd84",
        "files": [
          "airflow/www/fab_security/manager.py"
        ],
        "message": "Add Roles from Azure OAUTH Response in security manager as it is currently not able map any AD roles to airflow ones (#20707)\n\n(cherry picked from commit 088cbf2835cb6e29deb555a931bafc6b73deadef)",
        "before_after_code_files": [
          "airflow/www/fab_security/manager.py||airflow/www/fab_security/manager.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/fab_security/manager.py||airflow/www/fab_security/manager.py": [
          "File: airflow/www/fab_security/manager.py -> airflow/www/fab_security/manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "591:                 \"last_name\": me.get(\"family_name\", \"\"),",
          "592:                 \"id\": me[\"oid\"],",
          "593:                 \"username\": me[\"oid\"],",
          "594:             }",
          "595:         # for OpenShift",
          "596:         if provider == \"openshift\":",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "594:                 \"role_keys\": me.get(\"roles\", []),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7f41a049737c864fb807593f330578df62094e43",
      "candidate_info": {
        "commit_hash": "7f41a049737c864fb807593f330578df62094e43",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7f41a049737c864fb807593f330578df62094e43",
        "files": [
          "scripts/ci/libraries/_testing.sh",
          "scripts/ci/testing/ci_run_airflow_testing.sh"
        ],
        "message": "Optimizes running tests for public GitHub Runners. (#19512)\n\nWe started to get more (and almost consistent) OOM failures when\nwe tried to run all tests in parallel for the public GitHub\nrunners. This could previously happen for Providers and Integration\ntests but it started to happen for Core tests.\n\nThis PR optimizes this to also make Core tests sequentially run\nand refactors the code to make it much more readable and easy to\nunderstand what's going on there.\n\n(cherry picked from commit a1b7f98ff371bea42188a189f848675b348b977c)",
        "before_after_code_files": [
          "scripts/ci/libraries/_testing.sh||scripts/ci/libraries/_testing.sh",
          "scripts/ci/testing/ci_run_airflow_testing.sh||scripts/ci/testing/ci_run_airflow_testing.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/libraries/_testing.sh||scripts/ci/libraries/_testing.sh": [
          "File: scripts/ci/libraries/_testing.sh -> scripts/ci/libraries/_testing.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "21: function testing::skip_tests_if_requested(){",
          "22:     if [[ -f ${BUILD_CACHE_DIR}/.skip_tests ]]; then",
          "",
          "[Removed Lines]",
          "19: export MEMORY_REQUIRED_FOR_INTEGRATION_TEST_PARALLEL_RUN=33000",
          "",
          "[Added Lines]",
          "19: export MEMORY_REQUIRED_FOR_HEAVY_TEST_PARALLEL_RUN=33000",
          "",
          "---------------"
        ],
        "scripts/ci/testing/ci_run_airflow_testing.sh||scripts/ci/testing/ci_run_airflow_testing.sh": [
          "File: scripts/ci/testing/ci_run_airflow_testing.sh -> scripts/ci/testing/ci_run_airflow_testing.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "59: # Runs all test types in parallel depending on the number of CPUs available",
          "60: # We monitors their progress, display the progress  and summarize the result when finished.",
          "61: #",
          "63: # the docker engine, the integration tests (which take a lot of memory for all the integrations)",
          "64: # are run sequentially after all other tests were run in parallel.",
          "65: #",
          "66: # Input:",
          "67: #   * TEST_TYPES  - contains all test types that should be executed",
          "69: #             in parallel to other tests",
          "70: #",
          "71: function run_all_test_types_in_parallel() {",
          "",
          "[Removed Lines]",
          "62: # In case there is not enough memory (MEMORY_REQUIRED_FOR_INTEGRATION_TEST_PARALLEL_RUN) available for",
          "68: #   * MEMORY_REQUIRED_FOR_INTEGRATION_TEST_PARALLEL_RUN - memory in bytes required to run integration tests",
          "",
          "[Added Lines]",
          "60: # In case there is not enough memory (MEMORY_REQUIRED_FOR_HEAVY_TEST_PARALLEL_RUN) available for",
          "66: #   * MEMORY_REQUIRED_FOR_HEAVY_TEST_PARALLEL_RUN - memory in bytes required to run integration tests",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "75:     echo",
          "76:     echo \"${COLOR_YELLOW}Running maximum ${MAX_PARALLEL_TEST_JOBS} test types in parallel${COLOR_RESET}\"",
          "77:     echo",
          "81:     # shellcheck disable=SC2153",
          "82:     local test_types_to_run=${TEST_TYPES}",
          "111:                 test_types_to_run=\"${test_types_to_run//Providers/}\"",
          "118:             fi",
          "119:         fi",
          "120:     fi",
          "",
          "[Removed Lines]",
          "79:     local run_integration_tests_separately=\"false\"",
          "80:     local run_providers_tests_separately=\"false\"",
          "84:     if [[ ${test_types_to_run} == *\"Integration\"* ]]; then",
          "85:         if (( MEMORY_AVAILABLE_FOR_DOCKER < MEMORY_REQUIRED_FOR_INTEGRATION_TEST_PARALLEL_RUN )) ; then",
          "86:             # In case of Integration tests - they need more resources (Memory) thus we only run them in",
          "87:             # parallel if we have more than 32 GB memory available. Otherwise we run them sequentially",
          "88:             # after cleaning up the memory and stopping all docker instances",
          "89:             echo \"\"",
          "90:             echo \"${COLOR_YELLOW}There is not enough memory to run Integration test in parallel${COLOR_RESET}\"",
          "91:             echo \"${COLOR_YELLOW}   Available memory: ${MEMORY_AVAILABLE_FOR_DOCKER}${COLOR_RESET}\"",
          "92:             echo \"${COLOR_YELLOW}   Required memory: ${MEMORY_REQUIRED_FOR_INTEGRATION_TEST_PARALLEL_RUN}${COLOR_RESET}\"",
          "93:             echo \"\"",
          "94:             echo \"${COLOR_YELLOW}Integration tests will be run separately at the end after cleaning up docker${COLOR_RESET}\"",
          "95:             echo \"\"",
          "96:             if [[ ${BACKEND} == \"mssql\" ]]; then",
          "97:                 # Skip running \"Integration\" and \"Providers\" tests for low memory condition for mssql",
          "98:                 # Both might lead to memory issues even in run on their own. We have no need to",
          "99:                 # Test those specifically for MSSQL (and they will be tested in `main` as there",
          "100:                 # We have no memory limits",
          "101:                 test_types_to_run=\"${test_types_to_run//Integration/}\"",
          "102:                 run_integration_tests_separately=\"false\"",
          "103:                 test_types_to_run=\"${test_types_to_run//Providers/}\"",
          "104:                 run_providers_tests_separately=\"false\"",
          "105:             elif [[ ${BACKEND} == \"mysql\" ]]; then",
          "106:                 # Separate \"Integration\" and \"Providers\" tests for low memory condition for mysql",
          "107:                 # To not run them in parallel with other tests as this often leads to memory issue",
          "108:                 # (Error 137 or 143).",
          "109:                 test_types_to_run=\"${test_types_to_run//Integration/}\"",
          "110:                 run_integration_tests_separately=\"true\"",
          "112:                 run_providers_tests_separately=\"true\"",
          "113:             else",
          "114:                 # Remove Integration from list of tests to run in parallel",
          "115:                 # and run them separately for all other backends",
          "116:                 test_types_to_run=\"${test_types_to_run//Integration/}\"",
          "117:                 run_integration_tests_separately=\"true\"",
          "",
          "[Added Lines]",
          "76:     local sequential_tests=()",
          "80:     if (( MEMORY_AVAILABLE_FOR_DOCKER < MEMORY_REQUIRED_FOR_HEAVY_TEST_PARALLEL_RUN )) ; then",
          "81:         # In case of Heavy tests - they need more resources (Memory) thus we only run them in",
          "82:         # parallel if we have more than 32 GB memory available. Otherwise we run them sequentially",
          "83:         # after cleaning up the memory and stopping all docker instances",
          "84:         echo \"\"",
          "85:         echo \"${COLOR_YELLOW}There is not enough memory to run heavy test in parallel${COLOR_RESET}\"",
          "86:         echo \"${COLOR_YELLOW}   Available memory: ${MEMORY_AVAILABLE_FOR_DOCKER}${COLOR_RESET}\"",
          "87:         echo \"${COLOR_YELLOW}   Required memory: ${MEMORY_REQUIRED_FOR_HEAVY_TEST_PARALLEL_RUN}${COLOR_RESET}\"",
          "88:         echo \"\"",
          "89:         echo \"${COLOR_YELLOW}Heavy tests will be run sequentially after parallel tests including cleaning up docker between tests${COLOR_RESET}\"",
          "90:         echo \"\"",
          "91:         if [[ ${test_types_to_run} == *\"Integration\"* ]]; then",
          "92:             echo \"${COLOR_YELLOW}Remove Integration from tests_types_to_run and add them to sequential tests due to low memory.${COLOR_RESET}\"",
          "93:             test_types_to_run=\"${test_types_to_run//Integration/}\"",
          "94:             sequential_tests+=(\"Integration\")",
          "95:         fi",
          "96:         if [[ ${test_types_to_run} == *\"Core\"* ]]; then",
          "97:             echo \"${COLOR_YELLOW}Remove Core from tests_types_to_run and add them to sequential tests due to low memory.${COLOR_RESET}\"",
          "98:             test_types_to_run=\"${test_types_to_run//Core/}\"",
          "99:             sequential_tests+=(\"Core\")",
          "100:         fi",
          "101:         if [[ ${BACKEND} == \"mssql\" || ${BACKEND} == \"mysql\" ]]; then",
          "102:             # For mssql/mysql - they take far more memory than postgres (or sqlite) - we skip the Provider",
          "103:             # tests altogether as they take too much memory even if run sequentially.",
          "104:             # Those tests will run in `main` anyway.",
          "105:             if [[ ${test_types_to_run} == *\"Providers\"* ]]; then",
          "106:                 echo \"${COLOR_YELLOW}Remove Providers from tests_types_to_run and skip running them altogether (mysql/mssql case).${COLOR_RESET}\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "124:     parallel::initialize_monitoring",
          "126:     run_test_types_in_parallel \"${@}\"",
          "133:         parallel::cleanup_runner",
          "135:         run_test_types_in_parallel \"${@}\"",
          "137:     set -e",
          "138:     # this will exit with error code in case some of the non-Quarantined tests failed",
          "139:     parallel::print_job_summary_and_return_status_code",
          "",
          "[Removed Lines]",
          "127:     if [[ ${run_providers_tests_separately} == \"true\" ]]; then",
          "128:         parallel::cleanup_runner",
          "129:         test_types_to_run=\"Providers\"",
          "130:         run_test_types_in_parallel \"${@}\"",
          "131:     fi",
          "132:     if [[ ${run_integration_tests_separately} == \"true\" ]]; then",
          "134:         test_types_to_run=\"Integration\"",
          "136:     fi",
          "",
          "[Added Lines]",
          "116:     # Run all tests that should run in parallel (from test_types_to_run variable)",
          "119:     # if needed run remaining tests sequentially",
          "120:     for sequential_test in \"${sequential_tests[@]}\"; do",
          "122:         test_types_to_run=\"${sequential_test}\"",
          "124:     done",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "3c3f246fdf71c52c6a9ec9ce51aec27439821a07",
      "candidate_info": {
        "commit_hash": "3c3f246fdf71c52c6a9ec9ce51aec27439821a07",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/3c3f246fdf71c52c6a9ec9ce51aec27439821a07",
        "files": [
          "dev/README_RELEASE_PROVIDER_PACKAGES.md",
          "dev/provider_packages/PROVIDER_ISSUE_TEMPLATE.md.jinja2",
          "dev/provider_packages/prepare_provider_packages.py"
        ],
        "message": "Fix generation of \"Status provider\" issue (#20621)\n\nThe script for generating issue for \"Provider status\" and release\nprocess did not work well when only subset of providers were released.\nThe issue was generated including some already released packages\neven if they were not released in recent batch of providers (if there\nwas not even a doc change since last release, the package was considered\nas being released again).\n\nThis PR fixes it by adding a flag that only considers packages that\nare present in dist folder (which matches the process of release\nmanager)\n\nThe process has also been updated with more accurate description of\nthe steps to take - including manual execution of the script rather\nthan using Breeze (Breeze is not neede for this script).\n\n(cherry picked from commit d823cf7406092cf5b9b7b6df46738cd95a08c485)",
        "before_after_code_files": [
          "dev/provider_packages/PROVIDER_ISSUE_TEMPLATE.md.jinja2||dev/provider_packages/PROVIDER_ISSUE_TEMPLATE.md.jinja2",
          "dev/provider_packages/prepare_provider_packages.py||dev/provider_packages/prepare_provider_packages.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/provider_packages/PROVIDER_ISSUE_TEMPLATE.md.jinja2||dev/provider_packages/PROVIDER_ISSUE_TEMPLATE.md.jinja2": [
          "File: dev/provider_packages/PROVIDER_ISSUE_TEMPLATE.md.jinja2 -> dev/provider_packages/PROVIDER_ISSUE_TEMPLATE.md.jinja2",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: I have a kind request for all the contributors to the latest provider packages release.",
          "7: Those are providers that require testing as there were some substantial changes introduced:",
          "9: {% for provider_id, provider_pr_info in interesting_providers.items()  %}",
          "11: {%- for pr in provider_pr_info.pr_list %}",
          "12:    - [ ] [{{ pr.title }} (#{{ pr.number }})]({{ pr.html_url }}): @{{ pr.user.login }}",
          "13: {%- endfor %}",
          "14: {%- endfor %}",
          "24: <!--",
          "26: NOTE TO RELEASE MANAGER:",
          "",
          "[Removed Lines]",
          "2: Could you help us to test the RC versions of the providers and let us know in the comment,",
          "3: if the issue is addressed there.",
          "5: ## Providers that need testing",
          "10: ### Provider [{{ provider_id }}: {{ provider_pr_info.provider_details.versions[0] }}{{ suffix }}](https://pypi.org/project/{{ provider_pr_info.provider_details.pypi_package_name }}/{{ provider_pr_info.provider_details.versions[0] }}{{ suffix }})",
          "16: ## Providers that do not need testing",
          "18: Those are providers that were either doc-only or had changes that do not require testing.",
          "20: {% for provider_id, provider_pr_info in non_interesting_providers.items()  %}",
          "22: {%- endfor %}",
          "",
          "[Added Lines]",
          "2: Could you please help us to test the RC versions of the providers?",
          "4: Let us know in the comment, whether the issue is addressed.",
          "9: ## Provider [{{ provider_id }}: {{ provider_pr_info.provider_details.versions[0] }}{{ suffix }}](https://pypi.org/project/{{ provider_pr_info.provider_details.pypi_package_name }}/{{ provider_pr_info.provider_details.versions[0] }}{{ suffix }})",
          "",
          "---------------"
        ],
        "dev/provider_packages/prepare_provider_packages.py||dev/provider_packages/prepare_provider_packages.py": [
          "File: dev/provider_packages/prepare_provider_packages.py -> dev/provider_packages/prepare_provider_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "87: HTTPS_REMOTE = \"apache-https-for-providers\"",
          "88: HEAD_OF_HTTPS_REMOTE = f\"{HTTPS_REMOTE}/main\"",
          "103: # those imports need to come after the above sys.path.insert to make sure that Airflow",
          "104: # sources are importable without having to add the airflow sources to the PYTHONPATH before",
          "",
          "[Removed Lines]",
          "90: MY_DIR_PATH = os.path.dirname(__file__)",
          "91: SOURCE_DIR_PATH = os.path.abspath(os.path.join(MY_DIR_PATH, os.pardir, os.pardir))",
          "92: AIRFLOW_PATH = os.path.join(SOURCE_DIR_PATH, \"airflow\")",
          "93: PROVIDERS_PATH = os.path.join(AIRFLOW_PATH, \"providers\")",
          "94: DOCUMENTATION_PATH = os.path.join(SOURCE_DIR_PATH, \"docs\")",
          "95: TARGET_PROVIDER_PACKAGES_PATH = os.path.join(SOURCE_DIR_PATH, \"provider_packages\")",
          "96: GENERATED_AIRFLOW_PATH = os.path.join(TARGET_PROVIDER_PACKAGES_PATH, \"airflow\")",
          "97: GENERATED_PROVIDERS_PATH = os.path.join(GENERATED_AIRFLOW_PATH, \"providers\")",
          "99: PROVIDER_RUNTIME_DATA_SCHEMA_PATH = os.path.join(SOURCE_DIR_PATH, \"airflow\", \"provider_info.schema.json\")",
          "101: sys.path.insert(0, SOURCE_DIR_PATH)",
          "",
          "[Added Lines]",
          "90: MY_DIR_PATH = Path(__file__).parent",
          "91: SOURCE_DIR_PATH = MY_DIR_PATH.parent.parent",
          "92: AIRFLOW_PATH = SOURCE_DIR_PATH / \"airflow\"",
          "93: DIST_PATH = SOURCE_DIR_PATH / \"dist\"",
          "94: PROVIDERS_PATH = AIRFLOW_PATH / \"providers\"",
          "95: DOCUMENTATION_PATH = SOURCE_DIR_PATH / \"docs\"",
          "96: TARGET_PROVIDER_PACKAGES_PATH = SOURCE_DIR_PATH / \"provider_packages\"",
          "97: GENERATED_AIRFLOW_PATH = TARGET_PROVIDER_PACKAGES_PATH / \"airflow\"",
          "98: GENERATED_PROVIDERS_PATH = GENERATED_AIRFLOW_PATH / \"providers\"",
          "100: PROVIDER_RUNTIME_DATA_SCHEMA_PATH = SOURCE_DIR_PATH / \"airflow\" / \"provider_info.schema.json\"",
          "102: sys.path.insert(0, str(SOURCE_DIR_PATH))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "508:     :param ancestor_match: type of the object the method looks for",
          "509:     :param expected_class_name_pattern: regexp of class name pattern to expect",
          "510:     :param unexpected_class_name_patterns: set of regexp of class name pattern that are not expected",
          "512:            they should be excluded from the list)",
          "513:     :param false_positive_class_names: set of class names that are wrongly recognised as badly named",
          "514:     \"\"\"",
          "",
          "[Removed Lines]",
          "511:     :param exclude_class_type: exclude class of this type (Sensor are also Operators so",
          "",
          "[Added Lines]",
          "512:     :param exclude_class_type: exclude class of this type (Sensor are also Operators, so",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "555: def convert_classes_to_table(entity_type: EntityType, entities: List[str], full_package_name: str) -> str:",
          "556:     \"\"\"",
          "559:     :param entity_type: entity type to convert to markup",
          "560:     :param entities: list of  entities",
          "",
          "[Removed Lines]",
          "557:     Converts new entities tp a markdown table.",
          "",
          "[Added Lines]",
          "558:     Converts new entities to a Markdown table.",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "575:     full_package_name: str,",
          "576: ) -> EntityTypeSummary:",
          "577:     \"\"\"",
          "580:     :param entity_type: type of entity (Operators, Hooks etc.)",
          "581:     :param entities: set of entities found",
          "",
          "[Removed Lines]",
          "578:     Get details about entities..",
          "",
          "[Added Lines]",
          "579:     Get details about entities.",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "621: def get_class_code_link(base_package: str, class_name: str, git_tag: str) -> str:",
          "622:     \"\"\"",
          "625:     :param base_package: base package to strip from most names",
          "626:     :param class_name: name of the class",
          "",
          "[Removed Lines]",
          "623:     Provides markdown link for the class passed as parameter.",
          "",
          "[Added Lines]",
          "624:     Provides a Markdown link for the class passed as parameter.",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "735:     keep_trailing_newline: bool = False,",
          "736: ) -> str:",
          "737:     \"\"\"",
          "739:     :param template_name: name of the template to use",
          "740:     :param context: Jinja2 context",
          "741:     :param extension: Target file extension",
          "",
          "[Removed Lines]",
          "738:     Renders template based on it's name. Reads the template from <name>_TEMPLATE.md.jinja2 in current dir.",
          "",
          "[Added Lines]",
          "739:     Renders template based on its name. Reads the template from <name>_TEMPLATE.md.jinja2 in current dir.",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "794:     version: str, changes: str, base_url: str, markdown: bool = True",
          "795: ) -> Tuple[str, List[Change]]:",
          "796:     \"\"\"",
          "799:     The changes are in the form of multiple lines where each line consists of:",
          "800:     FULL_COMMIT_HASH SHORT_COMMIT_HASH COMMIT_DATE COMMIT_SUBJECT",
          "",
          "[Removed Lines]",
          "797:     Converts list of changes from it's string form to markdown/RST table and array of change information",
          "",
          "[Added Lines]",
          "798:     Converts list of changes from its string form to markdown/RST table and array of change information",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "804:     :param version: Version from which the changes are",
          "805:     :param changes: list of changes in a form of multiple-line string",
          "806:     :param base_url: base url for the commit URL",
          "808:     :return: formatted table + list of changes (starting from the latest)",
          "809:     \"\"\"",
          "810:     from tabulate import tabulate",
          "",
          "[Removed Lines]",
          "807:     :param markdown: if True, markdown format is used else rst",
          "",
          "[Added Lines]",
          "808:     :param markdown: if True, Markdown format is used else rst",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "843: def convert_pip_requirements_to_table(requirements: Iterable[str], markdown: bool = True) -> str:",
          "844:     \"\"\"",
          "846:     :param requirements: requirements list",
          "848:     :return: formatted table",
          "849:     \"\"\"",
          "850:     from tabulate import tabulate",
          "",
          "[Removed Lines]",
          "845:     Converts PIP requirement list to a markdown table.",
          "847:     :param markdown: if True, markdown format is used else rst",
          "",
          "[Added Lines]",
          "846:     Converts PIP requirement list to a Markdown table.",
          "848:     :param markdown: if True, Markdown format is used else rst",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "869:     markdown: bool = True,",
          "870: ) -> str:",
          "871:     \"\"\"",
          "873:     :param cross_package_dependencies: list of cross-package dependencies",
          "875:     :return: formatted table",
          "876:     \"\"\"",
          "877:     from tabulate import tabulate",
          "",
          "[Removed Lines]",
          "872:     Converts cross-package dependencies to a markdown table",
          "874:     :param markdown: if True, markdown format is used else rst",
          "",
          "[Added Lines]",
          "873:     Converts cross-package dependencies to a Markdown table",
          "875:     :param markdown: if True, Markdown format is used else rst",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "1014:     Make sure that apache remote exist in git. We need to take a log from the apache",
          "1015:     repository - not locally.",
          "1019:     This will:",
          "1024:     :param git_update: If the git remote already exists, should we try to update it",
          "",
          "[Removed Lines]",
          "1017:     Also the local repo might be shallow so we need to unshallow it.",
          "",
          "[Added Lines]",
          "1018:     Also, the local repo might be shallow, so we need to un-shallow it.",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "1068:     if is_shallow_repo:",
          "1069:         if verbose:",
          "1070:             console.print(",
          "1072:                 \"making all history available and increasing storage!\"",
          "1073:             )",
          "1074:         fetch_command.append(\"--unshallow\")",
          "",
          "[Removed Lines]",
          "1071:                 \"This will also unshallow the repository, \"",
          "",
          "[Added Lines]",
          "1072:                 \"This will also un-shallow the repository, \"",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "1599:     version_suffix: str,",
          "1600: ):",
          "1601:     \"\"\"",
          "1604:     :param provider_package_id: id of the package",
          "1605:     :param version_suffix: version suffix corresponding to the version in the code",
          "",
          "[Removed Lines]",
          "1602:     Updates generated setup.cfg/setup.py/manifest.in/provider_info) for packages",
          "",
          "[Added Lines]",
          "1603:     Updates generated setup.cfg/setup.py/manifest.in/provider_info for packages",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "2215:         imported_classes, warns = import_all_classes(",
          "2216:             provider_ids=provider_ids,",
          "2217:             print_imports=True,",
          "2219:             prefix=\"airflow.providers.\",",
          "2220:         )",
          "2221:         total = 0",
          "",
          "[Removed Lines]",
          "2218:             paths=[PROVIDERS_PATH],",
          "",
          "[Added Lines]",
          "2219:             paths=[str(PROVIDERS_PATH)],",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "2455:     pr_list: List[PullRequestOrIssue]",
          "2458: @cli.command()",
          "2460: @click.option('--suffix', default='rc1')",
          "2461: @click.option('--excluded-pr-list', type=str, help=\"Coma-separated list of PRs to exclude from the issue.\")",
          "2462: @argument_package_ids",
          "2464:     if not package_ids:",
          "2465:         package_ids = get_all_providers()",
          "2466:     \"\"\"Generates content for issue to test the release.\"\"\"",
          "",
          "[Removed Lines]",
          "2459: @click.option('--github-token', envvar='GITHUB_TOKEN')",
          "2463: def generate_issue_content(package_ids: List[str], github_token: str, suffix: str, excluded_pr_list: str):",
          "",
          "[Added Lines]",
          "2459: def is_package_in_dist(dist_files: List[str], package: str) -> bool:",
          "2460:     \"\"\"Check if package has been prepared in dist folder.\"\"\"",
          "2461:     for file in dist_files:",
          "2462:         if file.startswith(f'apache_airflow_providers_{package.replace(\".\",\"_\")}') or file.startswith(",
          "2463:             f'apache-airflow-providers-{package.replace(\".\",\"-\")}'",
          "2464:         ):",
          "2465:             return True",
          "2466:     return False",
          "2470: @click.option(",
          "2471:     '--github-token',",
          "2472:     envvar='GITHUB_TOKEN',",
          "2473:     help=textwrap.dedent(",
          "2474:         \"\"\"",
          "2475:       Github token used to authenticate.",
          "2476:       You can set omit it if you have GITHUB_TOKEN env variable set.",
          "2477:       Can be generated with:",
          "2478:       https://github.com/settings/tokens/new?description=Read%20sssues&scopes=repo:status\"\"\"",
          "2479:     ),",
          "2480: )",
          "2482: @click.option(",
          "2483:     '--only-available-in-dist',",
          "2484:     is_flag=True,",
          "2485:     help='Only consider package ids with packages prepared in the dist folder',",
          "2486: )",
          "2489: def generate_issue_content(",
          "2490:     package_ids: List[str],",
          "2491:     github_token: str,",
          "2492:     suffix: str,",
          "2493:     only_available_in_dist: bool,",
          "2494:     excluded_pr_list: str,",
          "2495: ):",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "2471:             excluded_prs = []",
          "2472:         all_prs: Set[int] = set()",
          "2473:         provider_prs: Dict[str, List[int]] = {}",
          "2474:         for package_id in package_ids:",
          "2476:             prs = get_prs_for_package(package_id)",
          "2477:             provider_prs[package_id] = list(filter(lambda pr: pr not in excluded_prs, prs))",
          "2478:             all_prs.update(provider_prs[package_id])",
          "",
          "[Removed Lines]",
          "2475:             console.print(f\"Extracting PRs for provider {package_id}\")",
          "",
          "[Added Lines]",
          "2506:         if only_available_in_dist:",
          "2507:             files_in_dist = os.listdir(str(DIST_PATH))",
          "2508:         prepared_package_ids = []",
          "2510:             if not only_available_in_dist or is_package_in_dist(files_in_dist, package_id):",
          "2511:                 console.print(f\"Extracting PRs for provider {package_id}\")",
          "2512:                 prepared_package_ids.append(package_id)",
          "2513:             else:",
          "2514:                 console.print(f\"Skipping extracting PRs for provider {package_id} as it is missing in dist\")",
          "2515:                 continue",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "2498:                 progress.advance(task)",
          "2499:         interesting_providers: Dict[str, ProviderPRInfo] = {}",
          "2500:         non_interesting_providers: Dict[str, ProviderPRInfo] = {}",
          "2502:             pull_request_list = [pull_requests[pr] for pr in provider_prs[package_id] if pr in pull_requests]",
          "2503:             provider_details = get_provider_details(package_id)",
          "2504:             if pull_request_list:",
          "",
          "[Removed Lines]",
          "2501:         for package_id in package_ids:",
          "",
          "[Added Lines]",
          "2541:         for package_id in prepared_package_ids:",
          "",
          "---------------"
        ]
      }
    }
  ]
}