{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "4ad0b2c25eba29694811445f72513629bf90e7e9",
      "candidate_info": {
        "commit_hash": "4ad0b2c25eba29694811445f72513629bf90e7e9",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/4ad0b2c25eba29694811445f72513629bf90e7e9",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala"
        ],
        "message": "[SPARK-38957][SQL] Use multipartIdentifier for parsing table-valued functions\n\nThis PR uses multipart identifiers when parsing table-valued functions.\n\nTo make table-valued functions error messages consistent for 2-part names and n-part names. For example, before this PR:\n```\nselect * from a.b.c\norg.apache.spark.sql.catalyst.parser.ParseException:\nInvalid SQL syntax: Unsupported function name `a`.`b`.`c`(line 1, pos 14)\n\n== SQL ==\nselect * from a.b.c(1)\n--------------^^^\n```\nAfter this PR:\n```\nInvalid SQL syntax: table valued function cannot specify database name (line 1, pos 14)\n\n== SQL ==\nSELECT * FROM a.b.c(1)\n--------------^^^\n```\n\nNo\n\nUnit test.\n\nCloses #36272 from allisonwang-db/spark-38957-parse-table-func.\n\nAuthored-by: allisonwang-db <allison.wang@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 8fe5bca1773521d967b82a920c6881f081155bc3)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1296:     } else {",
          "1297:       Seq.empty",
          "1298:     }",
          "1302:     }",
          "1304:     val tvf = UnresolvedTableValuedFunction(",
          "1306:     tvf.optionalMap(func.tableAlias.strictIdentifier)(aliasPlan)",
          "1307:   }",
          "",
          "[Removed Lines]",
          "1299:     val name = getFunctionIdentifier(func.functionName)",
          "1300:     if (name.database.nonEmpty) {",
          "1301:       operationNotAllowed(s\"table valued function cannot specify database name: $name\", ctx)",
          "1305:       name, func.expression.asScala.map(expression).toSeq, aliases)",
          "",
          "[Added Lines]",
          "1299:     val name = getFunctionMultiparts(func.functionName)",
          "1300:     if (name.length > 1) {",
          "1301:       throw QueryParsingErrors.invalidTableValuedFunctionNameError(name, ctx)",
          "1305:       name.asFunctionIdentifier, func.expression.asScala.map(expression).toSeq, aliases)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1952:     }",
          "1953:   }",
          "1966:   protected def getFunctionMultiparts(ctx: FunctionNameContext): Seq[String] = {",
          "1967:     if (ctx.qualifiedName != null) {",
          "1968:       ctx.qualifiedName().identifier().asScala.map(_.getText).toSeq",
          "",
          "[Removed Lines]",
          "1958:   protected def getFunctionIdentifier(ctx: FunctionNameContext): FunctionIdentifier = {",
          "1959:     if (ctx.qualifiedName != null) {",
          "1960:       visitFunctionName(ctx.qualifiedName)",
          "1961:     } else {",
          "1962:       FunctionIdentifier(ctx.getText, None)",
          "1963:     }",
          "1964:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryParsingErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "452:       s\"Specifying a database in CREATE TEMPORARY FUNCTION is not allowed: '$databaseName'\", ctx)",
          "453:   }",
          "455:   def unclosedBracketedCommentError(command: String, position: Origin): Throwable = {",
          "456:     new ParseException(Some(command), \"Unclosed bracketed comment\", position, position)",
          "457:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "455:   def invalidTableValuedFunctionNameError(",
          "456:       name: Seq[String],",
          "457:       ctx: TableValuedFunctionContext): Throwable = {",
          "458:     new ParseException(",
          "459:       \"INVALID_SQL_SYNTAX\",",
          "460:       Array(\"table valued function cannot specify database name \", toSQLId(name)), ctx)",
          "461:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "692:       UnresolvedTableValuedFunction(\"range\", Literal(2) :: Nil, Seq.empty).select(star()))",
          "694:     intercept(\"select * from default.range(2)\",",
          "696:   }",
          "698:   test(\"SPARK-20311 range(N) as alias\") {",
          "",
          "[Removed Lines]",
          "695:       \"table valued function cannot specify database name: default.range\")",
          "",
          "[Added Lines]",
          "695:       \"table valued function cannot specify database name\")",
          "697:     intercept(\"select * from spark_catalog.default.range(2)\",",
          "698:       \"table valued function cannot specify database name\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0baa5c7d2b71f379fad6a8a0b72f427acf70f4e4",
      "candidate_info": {
        "commit_hash": "0baa5c7d2b71f379fad6a8a0b72f427acf70f4e4",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/0baa5c7d2b71f379fad6a8a0b72f427acf70f4e4",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CollapseProjectSuite.scala"
        ],
        "message": "[SPARK-36718][SQL][FOLLOWUP] Fix the `isExtractOnly` check in CollapseProject\n\nThis PR fixes a perf regression in Spark 3.3 caused by https://github.com/apache/spark/pull/33958\n\nIn `CollapseProject`, we want to treat `CreateStruct` and its friends as cheap expressions if they are only referenced by `ExtractValue`, but the check is too conservative, which causes a perf regression. This PR fixes this check. Now \"extract-only\" means: the attribute only appears as a child of `ExtractValue`, but the consumer expression can be in any shape.\n\nFixes perf regression\n\nNo\n\nnew tests\n\nCloses #36510 from cloud-fan/bug.\n\nLead-authored-by: Wenchen Fan <cloud0fan@gmail.com>\nCo-authored-by: Wenchen Fan <wenchen@databricks.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 547f032d04bd2cf06c54b5a4a2f984f5166beb7d)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CollapseProjectSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CollapseProjectSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "997:       }",
          "998:   }",
          "1006:   }",
          "",
          "[Removed Lines]",
          "1000:   @scala.annotation.tailrec",
          "1001:   private def isExtractOnly(expr: Expression, ref: Attribute): Boolean = expr match {",
          "1002:     case a: Alias => isExtractOnly(a.child, ref)",
          "1003:     case e: ExtractValue => isExtractOnly(e.children.head, ref)",
          "1004:     case a: Attribute => a.semanticEquals(ref)",
          "1005:     case _ => false",
          "",
          "[Added Lines]",
          "1000:   private def isExtractOnly(expr: Expression, ref: Attribute): Boolean = {",
          "1001:     def hasRefInNonExtractValue(e: Expression): Boolean = e match {",
          "1002:       case a: Attribute => a.semanticEquals(ref)",
          "1004:       case e: ExtractValue if e.children.head.semanticEquals(ref) => false",
          "1005:       case _ => e.children.exists(hasRefInNonExtractValue)",
          "1006:     }",
          "1007:     !hasRefInNonExtractValue(expr)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CollapseProjectSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CollapseProjectSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CollapseProjectSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CollapseProjectSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "30:   object Optimize extends RuleExecutor[LogicalPlan] {",
          "31:     val batches =",
          "32:       Batch(\"Subqueries\", FixedPoint(10), EliminateSubqueryAliases) ::",
          "34:   }",
          "36:   val testRelation = LocalRelation('a.int, 'b.int)",
          "",
          "[Removed Lines]",
          "33:       Batch(\"CollapseProject\", Once, CollapseProject) :: Nil",
          "",
          "[Added Lines]",
          "33:       Batch(\"CollapseProject\", Once, CollapseProject) ::",
          "34:       Batch(\"SimplifyExtractValueOps\", Once, SimplifyExtractValueOps) :: Nil",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "124:   test(\"SPARK-36718: do not collapse project if non-cheap expressions will be repeated\") {",
          "125:     val query = testRelation",
          "128:       .analyze",
          "130:     val optimized = Optimize.execute(query)",
          "131:     comparePlans(optimized, query)",
          "132:   }",
          "134:   test(\"preserve top-level alias metadata while collapsing projects\") {",
          "",
          "[Removed Lines]",
          "126:       .select(('a + 1).as('a_plus_1))",
          "127:       .select(('a_plus_1 + 'a_plus_1).as('a_2_plus_2))",
          "",
          "[Added Lines]",
          "127:       .select(($\"a\" + 1).as(\"a_plus_1\"))",
          "128:       .select(($\"a_plus_1\" + $\"a_plus_1\").as(\"a_2_plus_2\"))",
          "135:     val query2 = testRelation",
          "136:       .select(namedStruct(\"a\", $\"a\", \"a_plus_1\", $\"a\" + 1).as(\"struct\"))",
          "137:       .select(($\"struct\".getField(\"a\") + $\"struct\".getField(\"a_plus_1\")).as(\"add\"))",
          "138:       .analyze",
          "139:     val optimized2 = Optimize.execute(query2)",
          "140:     val expected2 = testRelation",
          "141:       .select(($\"a\" + ($\"a\" + 1)).as(\"add\"))",
          "142:       .analyze",
          "143:     comparePlans(optimized2, expected2)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "42b30ee9a65ed2a50c13364309eb0608d75b7999",
      "candidate_info": {
        "commit_hash": "42b30ee9a65ed2a50c13364309eb0608d75b7999",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/42b30ee9a65ed2a50c13364309eb0608d75b7999",
        "files": [
          "resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnClusterSuite.scala"
        ],
        "message": "[SPARK-40022][YARN][TESTS] Ignore pyspark suites in `YarnClusterSuite` when python3 is unavailable\n\n### What changes were proposed in this pull request?\nThis pr adds `assume(isPythonAvailable)`  to `testPySpark` method in `YarnClusterSuite` to make `YarnClusterSuite` test succeeded in an environment without Python 3 configured.\n\n### Why are the changes needed?\n`YarnClusterSuite` should not `ABORTED` when `python3` is not configured.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\n\n- Pass GitHub Actions\n- Manual test\n\nRun\n\n```\nmvn clean test -pl resource-managers/yarn -am -Pyarn -DwildcardSuites=org.apache.spark.deploy.yarn.YarnClusterSuite  -Dtest=none\n```\nin an environment without Python 3 configured:\n\n**Before**\n\n```\nYarnClusterSuite:\norg.apache.spark.deploy.yarn.YarnClusterSuite *** ABORTED ***\n  java.lang.RuntimeException: Unable to load a Suite class that was discovered in the runpath: org.apache.spark.deploy.yarn.YarnClusterSuite\n  at org.scalatest.tools.DiscoverySuite$.getSuiteInstance(DiscoverySuite.scala:81)\n  at org.scalatest.tools.DiscoverySuite.$anonfun$nestedSuites$1(DiscoverySuite.scala:38)\n  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n  at scala.collection.Iterator.foreach(Iterator.scala:943)\n  at scala.collection.Iterator.foreach$(Iterator.scala:943)\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n  at scala.collection.IterableLike.foreach(IterableLike.scala:74)\n  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\n  ...\nRun completed in 833 milliseconds.\nTotal number of tests run: 0\nSuites: completed 1, aborted 1\nTests: succeeded 0, failed 0, canceled 0, ignored 0, pending 0\n*** 1 SUITE ABORTED ***\n```\n\n**After**\n\n```\nYarnClusterSuite:\n- run Spark in yarn-client mode\n- run Spark in yarn-cluster mode\n- run Spark in yarn-client mode with unmanaged am\n- run Spark in yarn-client mode with different configurations, ensuring redaction\n- run Spark in yarn-cluster mode with different configurations, ensuring redaction\n- yarn-cluster should respect conf overrides in SparkHadoopUtil (SPARK-16414, SPARK-23630)\n- SPARK-35672: run Spark in yarn-client mode with additional jar using URI scheme 'local'\n- SPARK-35672: run Spark in yarn-cluster mode with additional jar using URI scheme 'local'\n- SPARK-35672: run Spark in yarn-client mode with additional jar using URI scheme 'local' and gateway-replacement path\n- SPARK-35672: run Spark in yarn-cluster mode with additional jar using URI scheme 'local' and gateway-replacement path\n- SPARK-35672: run Spark in yarn-cluster mode with additional jar using URI scheme 'local' and gateway-replacement path containing an environment variable\n- SPARK-35672: run Spark in yarn-client mode with additional jar using URI scheme 'file'\n- SPARK-35672: run Spark in yarn-cluster mode with additional jar using URI scheme 'file'\n- run Spark in yarn-cluster mode unsuccessfully\n- run Spark in yarn-cluster mode failure after sc initialized\n- run Python application in yarn-client mode !!! CANCELED !!!\n  YarnClusterSuite.this.isPythonAvailable was false (YarnClusterSuite.scala:376)\n- run Python application in yarn-cluster mode !!! CANCELED !!!\n  YarnClusterSuite.this.isPythonAvailable was false (YarnClusterSuite.scala:376)\n- run Python application in yarn-cluster mode using spark.yarn.appMasterEnv to override local envvar !!! CANCELED !!!\n  YarnClusterSuite.this.isPythonAvailable was false (YarnClusterSuite.scala:376)\n- user class path first in client mode\n- user class path first in cluster mode\n- monitor app using launcher library\n- running Spark in yarn-cluster mode displays driver log links\n- timeout to get SparkContext in cluster mode triggers failure\n- executor env overwrite AM env in client mode\n- executor env overwrite AM env in cluster mode\n- SPARK-34472: ivySettings file with no scheme or file:// scheme should be localized on driver in cluster mode\n- SPARK-34472: ivySettings file with no scheme or file:// scheme should retain user provided path in client mode\n- SPARK-34472: ivySettings file with non-file:// schemes should throw an error\nRun completed in 7 minutes, 2 seconds.\nTotal number of tests run: 25\nSuites: completed 2, aborted 0\nTests: succeeded 25, failed 0, canceled 3, ignored 0, pending 0\nAll tests passed.\n```\n\nCloses #37454 from LuciferYang/yarnclustersuite.\n\nAuthored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 8e472443081342a0e0dc37aa154e30a0a6df39b7)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnClusterSuite.scala||resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnClusterSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnClusterSuite.scala||resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnClusterSuite.scala": [
          "File: resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnClusterSuite.scala -> resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnClusterSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "55: @ExtendedYarnTest",
          "56: class YarnClusterSuite extends BaseYarnClusterSuite {",
          "63:   }",
          "65:   override def newYarnConfig(): YarnConfiguration = new YarnConfiguration()",
          "",
          "[Removed Lines]",
          "58:   private val pythonExecutablePath = {",
          "60:     val maybePath = TestUtils.getAbsolutePathFromExecutable(\"python3\")",
          "61:     assert(maybePath.isDefined)",
          "62:     maybePath.get",
          "",
          "[Added Lines]",
          "58:   private val (isPythonAvailable, pythonExecutablePath) = {",
          "60:     TestUtils.getAbsolutePathFromExecutable(\"python3\") match {",
          "61:       case Some(path) => (true, path)",
          "62:       case _ => (false, \"\")",
          "63:     }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "371:       clientMode: Boolean,",
          "372:       extraConf: Map[String, String] = Map(),",
          "373:       extraEnv: Map[String, String] = Map()): Unit = {",
          "374:     val primaryPyFile = new File(tempDir, \"test.py\")",
          "375:     Files.write(TEST_PYFILE, primaryPyFile, StandardCharsets.UTF_8)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "375:     assume(isPythonAvailable)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e2ce0885b6666babe536707008ed0afaa09dca99",
      "candidate_info": {
        "commit_hash": "e2ce0885b6666babe536707008ed0afaa09dca99",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/e2ce0885b6666babe536707008ed0afaa09dca99",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ],
        "message": "[SPARK-39190][SQL] Provide query context for decimal precision overflow error when WSCG is off\n\n### What changes were proposed in this pull request?\n\nSimilar to https://github.com/apache/spark/pull/36525, this PR provides query context for decimal precision overflow error when WSCG is off\n\n### Why are the changes needed?\n\nEnhance the runtime error query context of checking decimal overflow. After changes, it works when the whole stage codegen is not available.\n\n### Does this PR introduce _any_ user-facing change?\n\nNO\n\n### How was this patch tested?\n\nUT\n\nCloses #36557 from gengliangwang/decimalContextWSCG.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>\n(cherry picked from commit 17b85ff97569a43d7fd33863d17bfdaf62d539e0)\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/decimalExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "128: case class CheckOverflow(",
          "129:     child: Expression,",
          "130:     dataType: DecimalType,",
          "133:   override def nullable: Boolean = true",
          "",
          "[Removed Lines]",
          "131:     nullOnOverflow: Boolean) extends UnaryExpression {",
          "",
          "[Added Lines]",
          "131:     nullOnOverflow: Boolean) extends UnaryExpression with SupportQueryContext {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "138:       dataType.scale,",
          "139:       Decimal.ROUND_HALF_UP,",
          "140:       nullOnOverflow,",
          "143:   override protected def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {",
          "144:     val errorContextCode = if (nullOnOverflow) {",
          "146:     } else {",
          "147:       \"\\\"\\\"\"",
          "148:     }",
          "",
          "[Removed Lines]",
          "141:       origin.context)",
          "145:       ctx.addReferenceObj(\"errCtx\", origin.context)",
          "",
          "[Added Lines]",
          "141:       queryContext)",
          "145:       ctx.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "164:   override protected def withNewChildInternal(newChild: Expression): CheckOverflow =",
          "165:     copy(child = newChild)",
          "166: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "167:   override def initQueryContext(): String = if (nullOnOverflow) {",
          "168:     \"\"",
          "169:   } else {",
          "170:     origin.context",
          "171:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "4423:     }",
          "4424:   }",
          "4426:   test(\"SPARK-38589: try_avg should return null if overflow happens before merging\") {",
          "4427:     val yearMonthDf = Seq(Int.MaxValue, Int.MaxValue, 2)",
          "4428:       .map(Period.ofMonths)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4426:   test(\"SPARK-39190: Query context of decimal overflow error should be serialized to executors\" +",
          "4427:     \" when WSCG is off\") {",
          "4428:     withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> \"false\",",
          "4429:       SQLConf.ANSI_ENABLED.key -> \"true\") {",
          "4430:       withTable(\"t\") {",
          "4431:         sql(\"create table t(d decimal(38, 0)) using parquet\")",
          "4432:         sql(\"insert into t values (2e37BD)\")",
          "4433:         val query = \"select d / 0.1 from t\"",
          "4434:         val msg = intercept[SparkException] {",
          "4435:           sql(query).collect()",
          "4436:         }.getMessage",
          "4437:         assert(msg.contains(query))",
          "4438:       }",
          "4439:     }",
          "4440:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1a01a492c051bb861c480f224a3c310e133e4d01",
      "candidate_info": {
        "commit_hash": "1a01a492c051bb861c480f224a3c310e133e4d01",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/1a01a492c051bb861c480f224a3c310e133e4d01",
        "files": [
          "python/pyspark/sql/tests/test_udf.py",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala"
        ],
        "message": "[SPARK-40121][PYTHON][SQL] Initialize projection used for Python UDF\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to initialize the projection so non-deterministic expressions can be evaluated with Python UDFs.\n\n### Why are the changes needed?\n\nTo make the Python UDF working with non-deterministic expressions.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes.\n\n```python\nfrom pyspark.sql.functions import udf, rand\nspark.range(10).select(udf(lambda x: x, \"double\")(rand())).show()\n```\n\n**Before**\n\n```\njava.lang.NullPointerException\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$10(EvalPythonExec.scala:126)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1161)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\n```\n\n**After**\n\n```\n+----------------------------------+\n|<lambda>rand(-2507211707257730645)|\n+----------------------------------+\n|                0.7691724424045242|\n|               0.09602244075319044|\n|                0.3006471278112862|\n|                0.4182649571961977|\n|               0.29349096650900974|\n|                0.7987097908937618|\n|                0.5324802583101007|\n|                  0.72460930912789|\n|                0.1367749768412846|\n|               0.17277322931919348|\n+----------------------------------+\n```\n\n### How was this patch tested?\n\nManually tested, and unittest was added.\n\nCloses #37552 from HyukjinKwon/SPARK-40121.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 336c9bc535895530cc3983b24e7507229fa9570d)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/sql/tests/test_udf.py||python/pyspark/sql/tests/test_udf.py",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/sql/tests/test_udf.py||python/pyspark/sql/tests/test_udf.py": [
          "File: python/pyspark/sql/tests/test_udf.py -> python/pyspark/sql/tests/test_udf.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: from pyspark import SparkContext, SQLContext",
          "26: from pyspark.sql import SparkSession, Column, Row",
          "28: from pyspark.sql.udf import UserDefinedFunction",
          "29: from pyspark.sql.types import (",
          "30:     StringType,",
          "",
          "[Removed Lines]",
          "27: from pyspark.sql.functions import udf, assert_true, lit",
          "",
          "[Added Lines]",
          "27: from pyspark.sql.functions import udf, assert_true, lit, rand",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "798:         finally:",
          "799:             shutil.rmtree(path)",
          "802: class UDFInitializationTests(unittest.TestCase):",
          "803:     def tearDown(self):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "801:     def test_udf_with_rand(self):",
          "802:         # SPARK-40121: rand() with Python UDF.",
          "803:         self.assertEqual(",
          "804:             len(self.spark.range(10).select(udf(lambda x: x, DoubleType())(rand())).collect()), 10",
          "805:         )",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvalPythonExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "116:         }.toArray",
          "117:       }.toArray",
          "118:       val projection = MutableProjection.create(allInputs.toSeq, child.output)",
          "119:       val schema = StructType(dataTypes.zipWithIndex.map { case (dt, i) =>",
          "120:         StructField(s\"_$i\", dt)",
          "121:       }.toSeq)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "119:       projection.initialize(context.partitionId())",
          "",
          "---------------"
        ]
      }
    }
  ]
}