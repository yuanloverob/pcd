{
  "cve_id": "CVE-2022-40604",
  "cve_desc": "In Apache Airflow 2.3.0 through 2.3.4, part of a url was unnecessarily formatted, allowing for possible information extraction.",
  "repo": "apache/airflow",
  "patch_hash": "18386026c28939fa6d91d198c5489c295a05dcd2",
  "patch_info": {
    "commit_hash": "18386026c28939fa6d91d198c5489c295a05dcd2",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/18386026c28939fa6d91d198c5489c295a05dcd2",
    "files": [
      "airflow/utils/log/file_task_handler.py"
    ],
    "message": "Properly build URL to retrieve logs independently from system (#26337)\n\nThe previous way of building the path depended on the OS path\nbut it was really used to build the URL so we should use\nurllib instead of os.path.join.",
    "before_after_code_files": [
      "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
    ]
  },
  "patch_diff": {
    "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
      "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "21: import warnings",
      "22: from pathlib import Path",
      "23: from typing import TYPE_CHECKING, Optional",
      "25: from airflow.configuration import AirflowConfigException, conf",
      "26: from airflow.exceptions import RemovedInAirflow3Warning",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "24: from urllib.parse import urljoin",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "194:         else:",
      "195:             import httpx",
      "199:             )",
      "200:             log += f\"*** Log file does not exist: {location}\\n\"",
      "201:             log += f\"*** Fetching from: {url}\\n\"",
      "",
      "[Removed Lines]",
      "197:             url = os.path.join(\"http://{ti.hostname}:{worker_log_server_port}/log\", log_relative_path).format(",
      "198:                 ti=ti, worker_log_server_port=conf.get('logging', 'WORKER_LOG_SERVER_PORT')",
      "",
      "[Added Lines]",
      "198:             url = urljoin(",
      "199:                 f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log\", log_relative_path",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "21e550ea71509f8d23bc2269c58345561674f9c2",
      "candidate_info": {
        "commit_hash": "21e550ea71509f8d23bc2269c58345561674f9c2",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/21e550ea71509f8d23bc2269c58345561674f9c2",
        "files": [
          "airflow/cli/cli_parser.py",
          "airflow/config_templates/config.yml",
          "airflow/config_templates/default_airflow.cfg",
          "airflow/configuration.py",
          "airflow/example_dags/example_kubernetes_executor.py",
          "airflow/example_dags/example_local_kubernetes_executor.py",
          "airflow/kubernetes/kube_client.py",
          "airflow/kubernetes/kube_config.py",
          "airflow/utils/log/file_task_handler.py",
          "docs/apache-airflow/executor/kubernetes.rst",
          "docs/apache-airflow/upgrading-from-1-10/index.rst",
          "newsfragments/26873.significant.rst",
          "tests/core/test_config_templates.py",
          "tests/kubernetes/test_client.py"
        ],
        "message": "Rename kubernetes config section to kubernetes_executor (#26873)\n\nNow that KPO does not consider any core k8s config params, this section truly is just about kubernetes executor. Renaming it reduces potential for confusion.",
        "before_after_code_files": [
          "airflow/cli/cli_parser.py||airflow/cli/cli_parser.py",
          "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg",
          "airflow/configuration.py||airflow/configuration.py",
          "airflow/example_dags/example_kubernetes_executor.py||airflow/example_dags/example_kubernetes_executor.py",
          "airflow/example_dags/example_local_kubernetes_executor.py||airflow/example_dags/example_local_kubernetes_executor.py",
          "airflow/kubernetes/kube_client.py||airflow/kubernetes/kube_client.py",
          "airflow/kubernetes/kube_config.py||airflow/kubernetes/kube_config.py",
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py",
          "tests/core/test_config_templates.py||tests/core/test_config_templates.py",
          "tests/kubernetes/test_client.py||tests/kubernetes/test_client.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ],
          "candidate": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/cli/cli_parser.py||airflow/cli/cli_parser.py": [
          "File: airflow/cli/cli_parser.py -> airflow/cli/cli_parser.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "889: # kubernetes cleanup-pods",
          "890: ARG_NAMESPACE = Arg(",
          "891:     (\"--namespace\",),",
          "893:     help=\"Kubernetes Namespace. Default value is `[kubernetes] namespace` in configuration.\",",
          "894: )",
          "",
          "[Removed Lines]",
          "892:     default=conf.get('kubernetes', 'namespace'),",
          "",
          "[Added Lines]",
          "892:     default=conf.get('kubernetes_executor', 'namespace'),",
          "",
          "---------------"
        ],
        "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg": [
          "File: airflow/config_templates/default_airflow.cfg -> airflow/config_templates/default_airflow.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "1153: use_ssl = False",
          "1154: verify_certs = True",
          "1157: # Path to the YAML pod file that forms the basis for KubernetesExecutor workers.",
          "1158: pod_template_file =",
          "",
          "[Removed Lines]",
          "1156: [kubernetes]",
          "",
          "[Added Lines]",
          "1156: [kubernetes_executor]",
          "",
          "---------------"
        ],
        "airflow/configuration.py||airflow/configuration.py": [
          "File: airflow/configuration.py -> airflow/configuration.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "227:         ('database', 'sql_alchemy_connect_args'): ('core', 'sql_alchemy_connect_args', '2.3.0'),",
          "228:         ('database', 'load_default_connections'): ('core', 'load_default_connections', '2.3.0'),",
          "229:         ('database', 'max_db_retries'): ('core', 'max_db_retries', '2.3.0'),",
          "230:     }",
          "232:     # A mapping of old default values that we want to change and warn the user",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "231:             ('kubernetes_executor', x): ('kubernetes', x, '2.4.2')",
          "232:             for x in (",
          "233:                 'pod_template_file',",
          "234:                 'worker_container_repository',",
          "235:                 'worker_container_tag',",
          "236:                 'namespace',",
          "237:                 'delete_worker_pods',",
          "238:                 'delete_worker_pods_on_failure',",
          "239:                 'worker_pods_creation_batch_size',",
          "240:                 'multi_namespace_mode',",
          "241:                 'in_cluster',",
          "242:                 'cluster_context',",
          "243:                 'config_file',",
          "244:                 'kube_client_request_args',",
          "245:                 'delete_option_kwargs',",
          "246:                 'enable_tcp_keepalive',",
          "247:                 'tcp_keep_idle',",
          "248:                 'tcp_keep_intvl',",
          "249:                 'tcp_keep_cnt',",
          "250:                 'verify_ssl',",
          "251:                 'worker_pods_pending_timeout',",
          "252:                 'worker_pods_pending_timeout_check_interval',",
          "253:                 'worker_pods_queued_check_interval',",
          "254:                 'worker_pods_pending_timeout_batch_size',",
          "255:             )",
          "256:         },",
          "",
          "---------------"
        ],
        "airflow/example_dags/example_kubernetes_executor.py||airflow/example_dags/example_kubernetes_executor.py": [
          "File: airflow/example_dags/example_kubernetes_executor.py -> airflow/example_dags/example_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "33: log = logging.getLogger(__name__)",
          "38: try:",
          "39:     from kubernetes.client import models as k8s",
          "",
          "[Removed Lines]",
          "35: worker_container_repository = conf.get('kubernetes', 'worker_container_repository')",
          "36: worker_container_tag = conf.get('kubernetes', 'worker_container_tag')",
          "",
          "[Added Lines]",
          "35: worker_container_repository = conf.get('kubernetes_executor', 'worker_container_repository')",
          "36: worker_container_tag = conf.get('kubernetes_executor', 'worker_container_tag')",
          "",
          "---------------"
        ],
        "airflow/example_dags/example_local_kubernetes_executor.py||airflow/example_dags/example_local_kubernetes_executor.py": [
          "File: airflow/example_dags/example_local_kubernetes_executor.py -> airflow/example_dags/example_local_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: log = logging.getLogger(__name__)",
          "36: try:",
          "37:     from kubernetes.client import models as k8s",
          "",
          "[Removed Lines]",
          "33: worker_container_repository = conf.get('kubernetes', 'worker_container_repository')",
          "34: worker_container_tag = conf.get('kubernetes', 'worker_container_tag')",
          "",
          "[Added Lines]",
          "33: worker_container_repository = conf.get('kubernetes_executor', 'worker_container_repository')",
          "34: worker_container_tag = conf.get('kubernetes_executor', 'worker_container_tag')",
          "",
          "---------------"
        ],
        "airflow/kubernetes/kube_client.py||airflow/kubernetes/kube_client.py": [
          "File: airflow/kubernetes/kube_client.py -> airflow/kubernetes/kube_client.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "60:     from urllib3.connection import HTTPConnection, HTTPSConnection",
          "66:     socket_options = [(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)]",
          "",
          "[Removed Lines]",
          "62:     tcp_keep_idle = conf.getint('kubernetes', 'tcp_keep_idle')",
          "63:     tcp_keep_intvl = conf.getint('kubernetes', 'tcp_keep_intvl')",
          "64:     tcp_keep_cnt = conf.getint('kubernetes', 'tcp_keep_cnt')",
          "",
          "[Added Lines]",
          "62:     tcp_keep_idle = conf.getint('kubernetes_executor', 'tcp_keep_idle')",
          "63:     tcp_keep_intvl = conf.getint('kubernetes_executor', 'tcp_keep_intvl')",
          "64:     tcp_keep_cnt = conf.getint('kubernetes_executor', 'tcp_keep_cnt')",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "87: def get_kube_client(",
          "89:     cluster_context: str | None = None,",
          "90:     config_file: str | None = None,",
          "91: ) -> client.CoreV1Api:",
          "",
          "[Removed Lines]",
          "88:     in_cluster: bool = conf.getboolean('kubernetes', 'in_cluster'),",
          "",
          "[Added Lines]",
          "88:     in_cluster: bool = conf.getboolean('kubernetes_executor', 'in_cluster'),",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "101:     if not has_kubernetes:",
          "102:         raise _import_err",
          "105:         _enable_tcp_keepalive()",
          "107:     if in_cluster:",
          "108:         config.load_incluster_config()",
          "109:     else:",
          "110:         if cluster_context is None:",
          "112:         if config_file is None:",
          "114:         config.load_kube_config(config_file=config_file, context=cluster_context)",
          "117:         _disable_verify_ssl()",
          "119:     return client.CoreV1Api()",
          "",
          "[Removed Lines]",
          "104:     if conf.getboolean('kubernetes', 'enable_tcp_keepalive'):",
          "111:             cluster_context = conf.get('kubernetes', 'cluster_context', fallback=None)",
          "113:             config_file = conf.get('kubernetes', 'config_file', fallback=None)",
          "116:     if not conf.getboolean('kubernetes', 'verify_ssl'):",
          "",
          "[Added Lines]",
          "104:     if conf.getboolean('kubernetes_executor', 'enable_tcp_keepalive'):",
          "111:             cluster_context = conf.get('kubernetes_executor', 'cluster_context', fallback=None)",
          "113:             config_file = conf.get('kubernetes_executor', 'config_file', fallback=None)",
          "116:     if not conf.getboolean('kubernetes_executor', 'verify_ssl'):",
          "",
          "---------------"
        ],
        "airflow/kubernetes/kube_config.py||airflow/kubernetes/kube_config.py": [
          "File: airflow/kubernetes/kube_config.py -> airflow/kubernetes/kube_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "25:     \"\"\"Configuration for Kubernetes\"\"\"",
          "27:     core_section = 'core'",
          "29:     logging_section = 'logging'",
          "31:     def __init__(self):",
          "",
          "[Removed Lines]",
          "28:     kubernetes_section = 'kubernetes'",
          "",
          "[Added Lines]",
          "28:     kubernetes_section = 'kubernetes_executor'",
          "",
          "---------------"
        ],
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "184:                     # Kubernetes takes the pod name and truncates it for the hostname. This truncated hostname",
          "185:                     # is returned for the fqdn to comply with the 63 character limit imposed by DNS standards",
          "186:                     # on any label of a FQDN.",
          "188:                     matches = [",
          "189:                         pod.metadata.name",
          "190:                         for pod in pod_list.items",
          "",
          "[Removed Lines]",
          "187:                     pod_list = kube_client.list_namespaced_pod(conf.get('kubernetes', 'namespace'))",
          "",
          "[Added Lines]",
          "187:                     pod_list = kube_client.list_namespaced_pod(conf.get('kubernetes_executor', 'namespace'))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "199:                 res = kube_client.read_namespaced_pod_log(",
          "200:                     name=ti.hostname,",
          "202:                     container='base',",
          "203:                     follow=False,",
          "204:                     tail_lines=100,",
          "",
          "[Removed Lines]",
          "201:                     namespace=conf.get('kubernetes', 'namespace'),",
          "",
          "[Added Lines]",
          "201:                     namespace=conf.get('kubernetes_executor', 'namespace'),",
          "",
          "---------------"
        ],
        "tests/core/test_config_templates.py||tests/core/test_config_templates.py": [
          "File: tests/core/test_config_templates.py -> tests/core/test_config_templates.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "52:     'kerberos',",
          "53:     'elasticsearch',",
          "54:     'elasticsearch_configs',",
          "56:     'sensors',",
          "57: ]",
          "",
          "[Removed Lines]",
          "55:     'kubernetes',",
          "",
          "[Added Lines]",
          "55:     'kubernetes_executor',",
          "",
          "---------------"
        ],
        "tests/kubernetes/test_client.py||tests/kubernetes/test_client.py": [
          "File: tests/kubernetes/test_client.py -> tests/kubernetes/test_client.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "43:     def test_load_config_disable_ssl(self, conf, config):",
          "44:         conf.getboolean.return_value = False",
          "45:         get_kube_client(in_cluster=False)",
          "47:         # Support wide range of kube client libraries",
          "48:         if hasattr(Configuration, 'get_default_copy'):",
          "49:             configuration = Configuration.get_default_copy()",
          "",
          "[Removed Lines]",
          "46:         conf.getboolean.assert_called_with('kubernetes', 'verify_ssl')",
          "",
          "[Added Lines]",
          "46:         conf.getboolean.assert_called_with('kubernetes_executor', 'verify_ssl')",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1f7b296227fee772de9ba15af6ce107937ef9b9b",
      "candidate_info": {
        "commit_hash": "1f7b296227fee772de9ba15af6ce107937ef9b9b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/1f7b296227fee772de9ba15af6ce107937ef9b9b",
        "files": [
          "airflow/providers/alibaba/cloud/log/oss_task_handler.py",
          "airflow/providers/amazon/aws/log/s3_task_handler.py",
          "airflow/providers/google/cloud/log/gcs_task_handler.py",
          "airflow/providers/microsoft/azure/log/wasb_task_handler.py",
          "airflow/utils/log/file_task_handler.py",
          "airflow/utils/log/log_reader.py",
          "airflow/www/static/js/ti_log.js",
          "tests/api_connexion/endpoints/test_log_endpoint.py",
          "tests/providers/google/cloud/log/test_gcs_task_handler.py",
          "tests/utils/log/test_log_reader.py"
        ],
        "message": "Auto tail file logs in Web UI (#26169)",
        "before_after_code_files": [
          "airflow/providers/alibaba/cloud/log/oss_task_handler.py||airflow/providers/alibaba/cloud/log/oss_task_handler.py",
          "airflow/providers/amazon/aws/log/s3_task_handler.py||airflow/providers/amazon/aws/log/s3_task_handler.py",
          "airflow/providers/google/cloud/log/gcs_task_handler.py||airflow/providers/google/cloud/log/gcs_task_handler.py",
          "airflow/providers/microsoft/azure/log/wasb_task_handler.py||airflow/providers/microsoft/azure/log/wasb_task_handler.py",
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py",
          "airflow/utils/log/log_reader.py||airflow/utils/log/log_reader.py",
          "airflow/www/static/js/ti_log.js||airflow/www/static/js/ti_log.js",
          "tests/api_connexion/endpoints/test_log_endpoint.py||tests/api_connexion/endpoints/test_log_endpoint.py",
          "tests/providers/google/cloud/log/test_gcs_task_handler.py||tests/providers/google/cloud/log/test_gcs_task_handler.py",
          "tests/utils/log/test_log_reader.py||tests/utils/log/test_log_reader.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ],
          "candidate": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/providers/alibaba/cloud/log/oss_task_handler.py||airflow/providers/alibaba/cloud/log/oss_task_handler.py": [
          "File: airflow/providers/alibaba/cloud/log/oss_task_handler.py -> airflow/providers/alibaba/cloud/log/oss_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "114:         remote_loc = log_relative_path",
          "116:         if not self.oss_log_exists(remote_loc):",
          "118:         # If OSS remote file exists, we do not fetch logs from task instance",
          "119:         # local machine even if there are errors reading remote logs, as",
          "120:         # returned remote_log will contain error messages.",
          "",
          "[Removed Lines]",
          "117:             return super()._read(ti, try_number)",
          "",
          "[Added Lines]",
          "117:             return super()._read(ti, try_number, metadata)",
          "",
          "---------------"
        ],
        "airflow/providers/amazon/aws/log/s3_task_handler.py||airflow/providers/amazon/aws/log/s3_task_handler.py": [
          "File: airflow/providers/amazon/aws/log/s3_task_handler.py -> airflow/providers/amazon/aws/log/s3_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "130:             return log, {'end_of_log': True}",
          "131:         else:",
          "132:             log += '*** Falling back to local log\\n'",
          "134:             return log + local_log, metadata",
          "136:     def s3_log_exists(self, remote_log_location: str) -> bool:",
          "",
          "[Removed Lines]",
          "133:             local_log, metadata = super()._read(ti, try_number)",
          "",
          "[Added Lines]",
          "133:             local_log, metadata = super()._read(ti, try_number, metadata)",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/log/gcs_task_handler.py||airflow/providers/google/cloud/log/gcs_task_handler.py": [
          "File: airflow/providers/google/cloud/log/gcs_task_handler.py -> airflow/providers/google/cloud/log/gcs_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "152:         except Exception as e:",
          "153:             log = f'*** Unable to read remote log from {remote_loc}\\n*** {str(e)}\\n\\n'",
          "154:             self.log.error(log)",
          "156:             log += local_log",
          "157:             return log, metadata",
          "",
          "[Removed Lines]",
          "155:             local_log, metadata = super()._read(ti, try_number)",
          "",
          "[Added Lines]",
          "155:             local_log, metadata = super()._read(ti, try_number, metadata)",
          "",
          "---------------"
        ],
        "airflow/providers/microsoft/azure/log/wasb_task_handler.py||airflow/providers/microsoft/azure/log/wasb_task_handler.py": [
          "File: airflow/providers/microsoft/azure/log/wasb_task_handler.py -> airflow/providers/microsoft/azure/log/wasb_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import os",
          "21: import shutil",
          "23: from azure.common import AzureHttpError",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: from typing import Any",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "104:         # Mark closed so we don't double write if close is called twice",
          "105:         self.closed = True",
          "108:         \"\"\"",
          "109:         Read logs of given task instance and try_number from Wasb remote storage.",
          "110:         If failed, read the log from task instance host machine.",
          "",
          "[Removed Lines]",
          "107:     def _read(self, ti, try_number: int, metadata: str | None = None) -> tuple[str, dict[str, bool]]:",
          "",
          "[Added Lines]",
          "108:     def _read(",
          "109:         self, ti, try_number: int, metadata: dict[str, Any] | None = None",
          "110:     ) -> tuple[str, dict[str, bool]]:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "128:             log = f'*** Reading remote log from {remote_loc}.\\n{remote_log}\\n'",
          "129:             return log, {'end_of_log': True}",
          "130:         else:",
          "133:     def wasb_log_exists(self, remote_log_location: str) -> bool:",
          "134:         \"\"\"",
          "",
          "[Removed Lines]",
          "131:             return super()._read(ti, try_number)",
          "",
          "[Added Lines]",
          "134:             return super()._read(ti, try_number, metadata)",
          "",
          "---------------"
        ],
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import os",
          "23: import warnings",
          "24: from pathlib import Path",
          "26: from urllib.parse import urljoin",
          "28: from airflow.configuration import AirflowConfigException, conf",
          "",
          "[Removed Lines]",
          "25: from typing import TYPE_CHECKING",
          "",
          "[Added Lines]",
          "25: from typing import TYPE_CHECKING, Any",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "31: from airflow.utils.helpers import parse_template_string, render_template_to_string",
          "32: from airflow.utils.log.non_caching_file_handler import NonCachingFileHandler",
          "33: from airflow.utils.session import create_session",
          "35: if TYPE_CHECKING:",
          "36:     from airflow.models import TaskInstance",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "34: from airflow.utils.state import State",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "129:     def _read_grouped_logs(self):",
          "130:         return False",
          "133:         \"\"\"",
          "134:         Template method that contains custom logic of reading",
          "135:         logs given the try_number.",
          "",
          "[Removed Lines]",
          "132:     def _read(self, ti, try_number, metadata=None):",
          "",
          "[Added Lines]",
          "133:     def _read(self, ti: TaskInstance, try_number: int, metadata: dict[str, Any] | None = None):",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "138:         :param try_number: current try_number to read log from",
          "139:         :param metadata: log metadata,",
          "140:                          can be used for steaming log reading and auto-tailing.",
          "141:         :return: log message as a string and metadata.",
          "142:         \"\"\"",
          "143:         from airflow.utils.jwt_signer import JWTSigner",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "142:                          Following attributes are used:",
          "143:                          log_pos: (absolute) Char position to which the log",
          "144:                                   which was retrieved in previous calls, this",
          "145:                                   part will be skipped and only following test",
          "146:                                   returned to be added to tail.",
          "149:                  Following attributes are used in metadata:",
          "150:                  end_of_log: Boolean, True if end of log is reached or False",
          "151:                              if further calls might get more log text.",
          "152:                              This is determined by the status of the TaskInstance",
          "153:                  log_pos: (absolute) Char position to which the log is retrieved",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "158:             except Exception as e:",
          "159:                 log = f\"*** Failed to load local log file: {location}\\n\"",
          "160:                 log += f\"*** {str(e)}\\n\"",
          "161:         elif conf.get('core', 'executor') == 'KubernetesExecutor':",
          "162:             try:",
          "163:                 from airflow.kubernetes.kube_client import get_kube_client",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "173:                 return log, {'end_of_log': True}",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "195:             except Exception as f:",
          "196:                 log += f'*** Unable to fetch logs from worker pod {ti.hostname} ***\\n{str(f)}\\n\\n'",
          "197:         else:",
          "198:             import httpx",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "210:                 return log, {'end_of_log': True}",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "219:                 response = httpx.get(",
          "220:                     url,",
          "221:                     timeout=timeout,",
          "223:                 )",
          "224:                 response.encoding = \"utf-8\"",
          "",
          "[Removed Lines]",
          "222:                     headers={b'Authorization': signer.generate_signed_token({\"filename\": log_relative_path})},",
          "",
          "[Added Lines]",
          "236:                     headers={'Authorization': signer.generate_signed_token({\"filename\": log_relative_path})},",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "240:                 log += '\\n' + response.text",
          "241:             except Exception as e:",
          "242:                 log += f\"*** Failed to fetch log file from worker. {str(e)}\\n\"",
          "246:     def read(self, task_instance, try_number=None, metadata=None):",
          "247:         \"\"\"",
          "",
          "[Removed Lines]",
          "244:         return log, {'end_of_log': True}",
          "",
          "[Added Lines]",
          "257:                 return log, {'end_of_log': True}",
          "259:         # Process tailing if log is not at it's end",
          "260:         end_of_log = ti.try_number != try_number or ti.state not in State.running",
          "261:         log_pos = len(log)",
          "262:         if metadata and 'log_pos' in metadata:",
          "263:             previous_chars = metadata['log_pos']",
          "264:             log = log[previous_chars:]  # Cut off previously passed log test as new tail",
          "266:         return log, {'end_of_log': end_of_log, 'log_pos': log_pos}",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "273:         logs = [''] * len(try_numbers)",
          "274:         metadata_array = [{}] * len(try_numbers)",
          "275:         for i, try_number_element in enumerate(try_numbers):",
          "277:             # es_task_handler return logs grouped by host. wrap other handler returning log string",
          "278:             # with default/ empty host so that UI can render the response in the same way",
          "279:             logs[i] = log if self._read_grouped_logs() else [(task_instance.hostname, log)]",
          "282:         return logs, metadata_array",
          "",
          "[Removed Lines]",
          "276:             log, metadata = self._read(task_instance, try_number_element, metadata)",
          "280:             metadata_array[i] = metadata",
          "",
          "[Added Lines]",
          "298:             log, out_metadata = self._read(task_instance, try_number_element, metadata)",
          "302:             metadata_array[i] = out_metadata",
          "",
          "---------------"
        ],
        "airflow/utils/log/log_reader.py||airflow/utils/log/log_reader.py": [
          "File: airflow/utils/log/log_reader.py -> airflow/utils/log/log_reader.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: from airflow.utils.helpers import render_log_filename",
          "28: from airflow.utils.log.logging_mixin import ExternalLoggingMixin",
          "29: from airflow.utils.session import NEW_SESSION, provide_session",
          "32: class TaskLogReader:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "30: from airflow.utils.state import State",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "77:             metadata.pop('end_of_log', None)",
          "78:             metadata.pop('max_offset', None)",
          "79:             metadata.pop('offset', None)",
          "81:                 logs, metadata = self.read_log_chunks(ti, current_try_number, metadata)",
          "82:                 for host, log in logs[0]:",
          "83:                     yield \"\\n\".join([host or '', log]) + \"\\n\"",
          "",
          "[Removed Lines]",
          "80:             while 'end_of_log' not in metadata or not metadata['end_of_log']:",
          "",
          "[Added Lines]",
          "81:             metadata.pop('log_pos', None)",
          "82:             while 'end_of_log' not in metadata or (",
          "83:                 not metadata['end_of_log'] and ti.state not in State.running",
          "84:             ):",
          "",
          "---------------"
        ],
        "airflow/www/static/js/ti_log.js||airflow/www/static/js/ti_log.js": [
          "File: airflow/www/static/js/ti_log.js -> airflow/www/static/js/ti_log.js",
          "--- Hunk 1 ---",
          "[Context before]",
          "123:           .replace(urlRegex, (url) => `<a href=\"${url}\" target=\"_blank\">${url}</a>`)",
          "124:           .replaceAll(dateRegex, (date) => `<time datetime=\"${date}+00:00\" data-with-tz=\"true\">${formatDateTime(`${date}+00:00`)}</time>`)",
          "125:           .replaceAll(iso8601Regex, (date) => `<time datetime=\"${date}\" data-with-tz=\"true\">${formatDateTime(`${date}`)}</time>`);",
          "127:       });",
          "",
          "[Removed Lines]",
          "126:         logBlock.innerHTML += `${linkifiedMessage}\\n`;",
          "",
          "[Added Lines]",
          "126:         logBlock.innerHTML += `${linkifiedMessage}`;",
          "",
          "---------------"
        ],
        "tests/api_connexion/endpoints/test_log_endpoint.py||tests/api_connexion/endpoints/test_log_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_log_endpoint.py -> tests/api_connexion/endpoints/test_log_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "169:             == f\"[('localhost', '*** Reading local file: {expected_filename}\\\\nLog for testing.')]\"",
          "170:         )",
          "171:         info = serializer.loads(response.json['continuation_token'])",
          "173:         assert 200 == response.status_code",
          "175:     @pytest.mark.parametrize(",
          "",
          "[Removed Lines]",
          "172:         assert info == {'end_of_log': True}",
          "",
          "[Added Lines]",
          "172:         assert info == {'end_of_log': True, 'log_pos': 41 + len(expected_filename)}",
          "",
          "---------------"
        ],
        "tests/providers/google/cloud/log/test_gcs_task_handler.py||tests/providers/google/cloud/log/test_gcs_task_handler.py": [
          "File: tests/providers/google/cloud/log/test_gcs_task_handler.py -> tests/providers/google/cloud/log/test_gcs_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "105:             log == \"*** Unable to read remote log from gs://bucket/remote/log/location/1.log\\n*** \"",
          "106:             f\"Failed to connect\\n\\n*** Reading local file: {self.local_log_location}/1.log\\n\"",
          "107:         )",
          "109:         mock_blob.from_string.assert_called_once_with(",
          "110:             \"gs://bucket/remote/log/location/1.log\", mock_client.return_value",
          "111:         )",
          "",
          "[Removed Lines]",
          "108:         assert metadata == {\"end_of_log\": True}",
          "",
          "[Added Lines]",
          "108:         assert metadata == {'end_of_log': False, 'log_pos': 31 + len(self.local_log_location)}",
          "",
          "---------------"
        ],
        "tests/utils/log/test_log_reader.py||tests/utils/log/test_log_reader.py": [
          "File: tests/utils/log/test_log_reader.py -> tests/utils/log/test_log_reader.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "127:                 f\"try_number=1.\\n\",",
          "128:             )",
          "129:         ] == logs[0]",
          "132:     def test_test_read_log_chunks_should_read_all_files(self):",
          "133:         task_log_reader = TaskLogReader()",
          "",
          "[Removed Lines]",
          "130:         assert {\"end_of_log\": True} == metadatas",
          "",
          "[Added Lines]",
          "130:         assert metadatas == {'end_of_log': True, 'log_pos': 102 + len(self.log_dir)}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "159:                 )",
          "160:             ],",
          "161:         ] == logs",
          "164:     def test_test_test_read_log_stream_should_read_one_try(self):",
          "165:         task_log_reader = TaskLogReader()",
          "",
          "[Removed Lines]",
          "162:         assert {\"end_of_log\": True} == metadatas",
          "",
          "[Added Lines]",
          "162:         assert {'end_of_log': True, 'log_pos': 102 + len(self.log_dir)} == metadatas",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "175:     def test_test_test_read_log_stream_should_read_all_logs(self):",
          "176:         task_log_reader = TaskLogReader()",
          "177:         stream = task_log_reader.read_log_stream(ti=self.ti, try_number=None, metadata={})",
          "178:         assert [",
          "179:             \"localhost\\n*** Reading local file: \"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "177:         self.ti.state = TaskInstanceState.SUCCESS  # Ensure mocked instance is completed to return stream",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "199:         mock_read.side_effect = [first_return, second_return, third_return, fourth_return]",
          "201:         task_log_reader = TaskLogReader()",
          "202:         log_stream = task_log_reader.read_log_stream(ti=self.ti, try_number=1, metadata={})",
          "203:         assert [\"\\n1st line\\n\", \"\\n2nd line\\n\", \"\\n3rd line\\n\"] == list(log_stream)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "203:         self.ti.state = TaskInstanceState.SUCCESS",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5609641e6ff25cd282fc6471a5fa3a24780f9a00",
      "candidate_info": {
        "commit_hash": "5609641e6ff25cd282fc6471a5fa3a24780f9a00",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/5609641e6ff25cd282fc6471a5fa3a24780f9a00",
        "files": [
          "airflow/executors/kubernetes_executor.py",
          "airflow/utils/log/file_task_handler.py",
          "tests/executors/test_kubernetes_executor.py"
        ],
        "message": "Fix timestamp parse failure for k8s executor pod tailing (#31175)\n\n(cherry picked from commit 86d62d3c0398d0d842c9368c5832de92bc6ec4c5)",
        "before_after_code_files": [
          "airflow/executors/kubernetes_executor.py||airflow/executors/kubernetes_executor.py",
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py",
          "tests/executors/test_kubernetes_executor.py||tests/executors/test_kubernetes_executor.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ],
          "candidate": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/executors/kubernetes_executor.py||airflow/executors/kubernetes_executor.py": [
          "File: airflow/executors/kubernetes_executor.py -> airflow/executors/kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "48: from airflow.kubernetes.pod_generator import PodGenerator",
          "49: from airflow.models.taskinstance import TaskInstance",
          "50: from airflow.utils.event_scheduler import EventScheduler",
          "52: from airflow.utils.session import NEW_SESSION, provide_session",
          "53: from airflow.utils.state import State, TaskInstanceState",
          "",
          "[Removed Lines]",
          "51: from airflow.utils.log.logging_mixin import LoggingMixin",
          "",
          "[Added Lines]",
          "51: from airflow.utils.log.logging_mixin import LoggingMixin, remove_escape_codes",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "795:             client = get_kube_client()",
          "798:             selector = PodGenerator.build_selector_for_k8s_executor_pod(",
          "799:                 dag_id=ti.dag_id,",
          "800:                 task_id=ti.task_id,",
          "",
          "[Removed Lines]",
          "797:             messages.append(f\"Trying to get logs (last 100 lines) from worker pod {ti.hostname}\")",
          "",
          "[Added Lines]",
          "797:             messages.append(f\"Attempting to fetch logs from pod {ti.hostname} through kube API\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "820:                 tail_lines=100,",
          "821:                 _preload_content=False,",
          "822:             )",
          "824:             for line in res:",
          "826:         except Exception as e:",
          "827:             messages.append(f\"Reading from k8s pod logs failed: {str(e)}\")",
          "828:         return messages, [\"\\n\".join(log)]",
          "",
          "[Removed Lines]",
          "825:                 log.append(line.decode())",
          "",
          "[Added Lines]",
          "824:                 log.append(remove_escape_codes(line.decode()))",
          "825:             if log:",
          "826:                 messages.append(\"Found logs through kube API\")",
          "",
          "---------------"
        ],
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "315:             if response:",
          "316:                 executor_messages, executor_logs = response",
          "317:             if executor_messages:",
          "319:         if not (remote_logs and ti.state not in State.unfinished):",
          "320:             # when finished, if we have remote logs, no need to check local",
          "321:             worker_log_full_path = Path(self.local_base, worker_log_rel_path)",
          "",
          "[Removed Lines]",
          "318:                 messages_list.extend(messages_list)",
          "",
          "[Added Lines]",
          "318:                 messages_list.extend(executor_messages)",
          "",
          "---------------"
        ],
        "tests/executors/test_kubernetes_executor.py||tests/executors/test_kubernetes_executor.py": [
          "File: tests/executors/test_kubernetes_executor.py -> tests/executors/test_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1130:         messages, logs = executor.get_task_log(ti=ti, try_number=1)",
          "1132:         mock_kube_client.read_namespaced_pod_log.assert_called_once()",
          "1134:         assert logs[0] == \"a_\\nb_\\nc_\"",
          "1136:         mock_kube_client.reset_mock()",
          "",
          "[Removed Lines]",
          "1133:         assert \"Trying to get logs (last 100 lines) from worker pod \" in messages",
          "",
          "[Added Lines]",
          "1133:         assert messages == [",
          "1134:             \"Attempting to fetch logs from pod  through kube API\",",
          "1135:             \"Found logs through kube API\",",
          "1136:         ]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1139:         messages, logs = executor.get_task_log(ti=ti, try_number=1)",
          "1140:         assert logs == [\"\"]",
          "1141:         assert messages == [",
          "1143:             \"Reading from k8s pod logs failed: error_fetching_pod_log\",",
          "1144:         ]",
          "",
          "[Removed Lines]",
          "1142:             \"Trying to get logs (last 100 lines) from worker pod \",",
          "",
          "[Added Lines]",
          "1145:             \"Attempting to fetch logs from pod  through kube API\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bdc3d2e6474f7f23f75683fd072b4a07ef5aaeaa",
      "candidate_info": {
        "commit_hash": "bdc3d2e6474f7f23f75683fd072b4a07ef5aaeaa",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/bdc3d2e6474f7f23f75683fd072b4a07ef5aaeaa",
        "files": [
          "airflow/kubernetes/kubernetes_helper_functions.py",
          "airflow/kubernetes/pod_generator.py",
          "airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py",
          "airflow/utils/log/file_task_handler.py",
          "tests/kubernetes/test_kubernetes_helper_functions.py"
        ],
        "message": "Keep pod name for k8s executor under 63 characters (#28237)\n\nBecause of the way that the task log handler reads from running k8s executor pods, we must keep pod name <= 63 characters.  The handler gets pod name from ti.hostname. TI hostname is derived from the container hostname, which is truncated to 63 characters. We could lift this limit by using label selectors instead of pod name to find the pod. But for now, easy enough to keep limited to 63.\n\nSince we limit to 63 in the code, we can remove the logic to find the matching pod when length is >= 63.",
        "before_after_code_files": [
          "airflow/kubernetes/kubernetes_helper_functions.py||airflow/kubernetes/kubernetes_helper_functions.py",
          "airflow/kubernetes/pod_generator.py||airflow/kubernetes/pod_generator.py",
          "airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py||airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py",
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py",
          "tests/kubernetes/test_kubernetes_helper_functions.py||tests/kubernetes/test_kubernetes_helper_functions.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ],
          "candidate": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/kubernetes/kubernetes_helper_functions.py||airflow/kubernetes/kubernetes_helper_functions.py": [
          "File: airflow/kubernetes/kubernetes_helper_functions.py -> airflow/kubernetes/kubernetes_helper_functions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "48:     dag_id: str | None = None,",
          "49:     task_id: str | None = None,",
          "52:     unique: bool = True,",
          "53: ) -> str:",
          "54:     \"\"\"",
          "55:     Generates unique pod ID given a dag_id and / or task_id.",
          "57:     :param dag_id: DAG ID",
          "58:     :param task_id: Task ID",
          "59:     :param max_length: max number of characters",
          "",
          "[Removed Lines]",
          "51:     max_length: int = 80,",
          "",
          "[Added Lines]",
          "51:     max_length: int = 63,  # must be 63 for now, see below",
          "57:     Because of the way that the task log handler reads from running k8s executor pods,",
          "58:     we must keep pod name <= 63 characters.  The handler gets pod name from ti.hostname.",
          "59:     TI hostname is derived from the container hostname, which is truncated to 63 characters.",
          "60:     We could lift this limit by using label selectors instead of pod name to find the pod.",
          "",
          "---------------"
        ],
        "airflow/kubernetes/pod_generator.py||airflow/kubernetes/pod_generator.py": [
          "File: airflow/kubernetes/pod_generator.py -> airflow/kubernetes/pod_generator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "346:                 \"pod_id supplied is longer than 253 characters; truncating and adding unique suffix.\"",
          "347:             )",
          "348:             pod_id = add_pod_suffix(pod_name=pod_id, max_len=253)",
          "349:         try:",
          "350:             image = pod_override_object.spec.containers[0].image  # type: ignore",
          "351:             if not image:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "349:         if len(pod_id) > 63:",
          "350:             # because in task handler we get pod name from ti hostname (which truncates",
          "351:             # pod_id to 63 characters) we won't be able to find the pod unless it is <= 63 characters.",
          "352:             # our code creates pod names shorter than this so this warning should not normally be triggered.",
          "353:             warnings.warn(",
          "354:                 \"Supplied pod_id is longer than 63 characters. Due to implementation details, the webserver \"",
          "355:                 \"may not be able to stream logs while task is running. Please choose a shorter pod name.\"",
          "356:             )",
          "",
          "---------------"
        ],
        "airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py||airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py": [
          "File: airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py -> airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "92:     dag_id: str | None = None,",
          "93:     task_id: str | None = None,",
          "96:     unique: bool = True,",
          "97: ) -> str:",
          "98:     \"\"\"",
          "",
          "[Removed Lines]",
          "95:     max_length: int = 80,",
          "",
          "[Added Lines]",
          "95:     max_length: int = 63,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "648:         pod = PodGenerator.reconcile_pods(pod_template, pod)",
          "650:         if not pod.metadata.name:",
          "652:         elif self.random_name_suffix:",
          "653:             # user has supplied pod name, we're just adding suffix",
          "654:             pod.metadata.name = _add_pod_suffix(pod_name=pod.metadata.name)",
          "",
          "[Removed Lines]",
          "651:             pod.metadata.name = _create_pod_id(task_id=self.task_id, unique=self.random_name_suffix)",
          "",
          "[Added Lines]",
          "651:             pod.metadata.name = _create_pod_id(",
          "652:                 task_id=self.task_id, unique=self.random_name_suffix, max_length=80",
          "653:             )",
          "",
          "---------------"
        ],
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "197:                 kube_client = get_kube_client()",
          "213:                 log += f\"*** Trying to get logs (last 100 lines) from worker pod {ti.hostname} ***\\n\\n\"",
          "215:                 res = kube_client.read_namespaced_pod_log(",
          "",
          "[Removed Lines]",
          "199:                 if len(ti.hostname) >= 63:",
          "200:                     # Kubernetes takes the pod name and truncates it for the hostname. This truncated hostname",
          "201:                     # is returned for the fqdn to comply with the 63 character limit imposed by DNS standards",
          "202:                     # on any label of a FQDN.",
          "203:                     pod_list = kube_client.list_namespaced_pod(conf.get(\"kubernetes_executor\", \"namespace\"))",
          "204:                     matches = [",
          "205:                         pod.metadata.name",
          "206:                         for pod in pod_list.items",
          "207:                         if pod.metadata.name.startswith(ti.hostname)",
          "208:                     ]",
          "209:                     if len(matches) == 1:",
          "210:                         if len(matches[0]) > len(ti.hostname):",
          "211:                             ti.hostname = matches[0]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/kubernetes/test_kubernetes_helper_functions.py||tests/kubernetes/test_kubernetes_helper_functions.py": [
          "File: tests/kubernetes/test_kubernetes_helper_functions.py -> tests/kubernetes/test_kubernetes_helper_functions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "89:     def test_create_pod_id_dag_too_long_with_suffix(self, create_pod_id):",
          "90:         actual = create_pod_id(\"0\" * 254)",
          "92:         assert re.match(pod_name_regex, actual)",
          "94:     def test_create_pod_id_dag_too_long_non_unique(self, create_pod_id):",
          "95:         actual = create_pod_id(\"0\" * 254, unique=False)",
          "97:         assert re.match(pod_name_regex, actual)",
          "99:     @pytest.mark.parametrize(\"unique\", [True, False])",
          "",
          "[Removed Lines]",
          "91:         assert re.match(r\"0{71}-[a-z0-9]{8}\", actual)",
          "96:         assert re.match(r\"0{80}\", actual)",
          "",
          "[Added Lines]",
          "91:         assert len(actual) == 63",
          "92:         assert re.match(r\"0{54}-[a-z0-9]{8}\", actual)",
          "97:         assert len(actual) == 63",
          "98:         assert re.match(r\"0{63}\", actual)",
          "",
          "---------------"
        ]
      }
    }
  ]
}