{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "b8904c39c0426ac55ceff8b2716c24e3ef7cdfbb",
      "candidate_info": {
        "commit_hash": "b8904c39c0426ac55ceff8b2716c24e3ef7cdfbb",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/b8904c39c0426ac55ceff8b2716c24e3ef7cdfbb",
        "files": [
          "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBackend.scala"
        ],
        "message": "[SPARK-39341][K8S] KubernetesExecutorBackend should allow IPv6 pod IP\n\n### What changes were proposed in this pull request?\nThis PR aims to make KubernetesExecutorBackend allow IPv6 pod IP.\n\n### Why are the changes needed?\nThe `hostname` comes from `SPARK_EXECUTOR_POD_IP`.\n```\nresource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh:      --hostname $SPARK_EXECUTOR_POD_IP\n```\n\n`SPARK_EXECUTOR_POD_IP` comes from `status.podIP` where it does not have `[]` in case of IPv6.\nhttps://github.com/apache/spark/blob/1a54a2bd69e35ab5f0cbd83df673c6f1452df418/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala#L140-L145\n\n- https://kubernetes.io/docs/concepts/services-networking/dual-stack/\n- https://en.wikipedia.org/wiki/IPv6_address\n\n### Does this PR introduce _any_ user-facing change?\nNo, this PR removes only the `[]` constraint from `checkHost`.\n\n### How was this patch tested?\n\nPass the CIs.\n\nCloses #36728 from williamhyun/IPv6.\n\nAuthored-by: William Hyun <william@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 7bb009888eec416eef36587546d4c0ab0077bcf5)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBackend.scala||resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBackend.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBackend.scala||resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBackend.scala": [
          "File: resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBackend.scala -> resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesExecutorBackend.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "67:     SparkHadoopUtil.get.runAsSparkUser { () =>",
          "72:       val executorConf = new SparkConf",
          "",
          "[Removed Lines]",
          "69:       Utils.checkHost(arguments.hostname)",
          "",
          "[Added Lines]",
          "69:       assert(arguments.hostname != null &&",
          "70:           (arguments.hostname.indexOf(':') == -1 || arguments.hostname.split(\":\").length > 2))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ec771f19a33bf8ed6cbf03721d8da7d159083ed7",
      "candidate_info": {
        "commit_hash": "ec771f19a33bf8ed6cbf03721d8da7d159083ed7",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/ec771f19a33bf8ed6cbf03721d8da7d159083ed7",
        "files": [
          "python/docs/source/user_guide/pandas_on_spark/index.rst",
          "python/docs/source/user_guide/pandas_on_spark/supported_pandas_api.rst",
          "python/pyspark/pandas/missing/frame.py",
          "python/pyspark/pandas/missing/groupby.py",
          "python/pyspark/pandas/missing/indexes.py",
          "python/pyspark/pandas/missing/series.py",
          "python/pyspark/pandas/missing/window.py"
        ],
        "message": "[SPARK-38581][PYTHON][DOCS][3.3] List of supported pandas APIs for pandas-on-Spark docs\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to add new page named \"Supported pandas APIs\" for pandas-on-Spark documents.\n\nThis is cherry-pick from https://github.com/apache/spark/commit/f43c68cb38cb0556f2058be6d3a016083ef5152d to `branch-3.3`.\n\n### Why are the changes needed?\n\nTo let users can more easily find out whether a specific pandas API and its parameters are supported or not from the single document page.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, the \"Supported pandas APIs\" page is added to the user guide for pandas API on Spark documents.\n\n### How was this patch tested?\n\nManually check the links in the documents & the existing doc build should be passed.\n\nCloses #36308 from itholic/SPARK-38581-3.3.\n\nAuthored-by: itholic <haejoon.lee@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/pandas/missing/frame.py||python/pyspark/pandas/missing/frame.py",
          "python/pyspark/pandas/missing/groupby.py||python/pyspark/pandas/missing/groupby.py",
          "python/pyspark/pandas/missing/indexes.py||python/pyspark/pandas/missing/indexes.py",
          "python/pyspark/pandas/missing/series.py||python/pyspark/pandas/missing/series.py",
          "python/pyspark/pandas/missing/window.py||python/pyspark/pandas/missing/window.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/pandas/missing/frame.py||python/pyspark/pandas/missing/frame.py": [
          "File: python/pyspark/pandas/missing/frame.py -> python/pyspark/pandas/missing/frame.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "32: class _MissingPandasLikeDataFrame:",
          "34:     # Functions",
          "35:     asfreq = _unsupported_function(\"asfreq\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33:     # NOTE: Please update the document \"Supported pandas APIs\" when implementing the new API.",
          "34:     # Documentation path: `python/docs/source/user_guide/pandas_on_spark/supported_pandas_api.rst`.",
          "",
          "---------------"
        ],
        "python/pyspark/pandas/missing/groupby.py||python/pyspark/pandas/missing/groupby.py": [
          "File: python/pyspark/pandas/missing/groupby.py -> python/pyspark/pandas/missing/groupby.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "39: class MissingPandasLikeDataFrameGroupBy:",
          "41:     # Properties",
          "42:     corr = _unsupported_property(\"corr\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "40:     # NOTE: Please update the document \"Supported pandas APIs\" when implementing the new API.",
          "41:     # Documentation path: `python/docs/source/user_guide/pandas_on_spark/supported_pandas_api.rst`.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "71: class MissingPandasLikeSeriesGroupBy:",
          "73:     # Properties",
          "74:     corr = _unsupported_property(\"corr\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "74:     # NOTE: Please update the document \"Supported pandas APIs\" when implementing the new API.",
          "75:     # Documentation path: `python/docs/source/user_guide/pandas_on_spark/supported_pandas_api.rst`.",
          "",
          "---------------"
        ],
        "python/pyspark/pandas/missing/indexes.py||python/pyspark/pandas/missing/indexes.py": [
          "File: python/pyspark/pandas/missing/indexes.py -> python/pyspark/pandas/missing/indexes.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "38: class MissingPandasLikeIndex:",
          "40:     # Properties",
          "41:     nbytes = _unsupported_property(\"nbytes\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "39:     # NOTE: Please update the document \"Supported pandas APIs\" when implementing the new API.",
          "40:     # Documentation path: `python/docs/source/user_guide/pandas_on_spark/supported_pandas_api.rst`.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "80: class MissingPandasLikeDatetimeIndex(MissingPandasLikeIndex):",
          "82:     # Properties",
          "83:     nanosecond = _unsupported_property(\"nanosecond\", cls=\"DatetimeIndex\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "83:     # NOTE: Please update the document \"Supported pandas APIs\" when implementing the new API.",
          "84:     # Documentation path: `python/docs/source/user_guide/pandas_on_spark/supported_pandas_api.rst`.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "103: class MissingPandasLikeTimedeltaIndex(MissingPandasLikeIndex):",
          "105:     # Properties",
          "106:     nanoseconds = _unsupported_property(\"nanoseconds\", cls=\"TimedeltaIndex\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "108:     # NOTE: Please update the document \"Supported pandas APIs\" when implementing the new API.",
          "109:     # Documentation path: `python/docs/source/user_guide/pandas_on_spark/supported_pandas_api.rst`.",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "118: class MissingPandasLikeMultiIndex:",
          "120:     # Functions",
          "121:     argsort = _unsupported_function(\"argsort\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "125:     # NOTE: Please update the document \"Supported pandas APIs\" when implementing the new API.",
          "126:     # Documentation path: `python/docs/source/user_guide/pandas_on_spark/supported_pandas_api.rst`.",
          "",
          "---------------"
        ],
        "python/pyspark/pandas/missing/series.py||python/pyspark/pandas/missing/series.py": [
          "File: python/pyspark/pandas/missing/series.py -> python/pyspark/pandas/missing/series.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "32: class MissingPandasLikeSeries:",
          "34:     # Functions",
          "35:     asfreq = _unsupported_function(\"asfreq\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33:     # NOTE: Please update the document \"Supported pandas APIs\" when implementing the new API.",
          "34:     # Documentation path: `python/docs/source/user_guide/pandas_on_spark/supported_pandas_api.rst`.",
          "",
          "---------------"
        ],
        "python/pyspark/pandas/missing/window.py||python/pyspark/pandas/missing/window.py": [
          "File: python/pyspark/pandas/missing/window.py -> python/pyspark/pandas/missing/window.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "57: class MissingPandasLikeExpanding:",
          "58:     agg = _unsupported_function_expanding(\"agg\")",
          "59:     aggregate = _unsupported_function_expanding(\"aggregate\")",
          "60:     apply = _unsupported_function_expanding(\"apply\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "58:     # NOTE: Please update the document \"Supported pandas APIs\" when implementing the new API.",
          "59:     # Documentation path: `python/docs/source/user_guide/pandas_on_spark/supported_pandas_api.rst`.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "75: class MissingPandasLikeRolling:",
          "76:     agg = _unsupported_function_rolling(\"agg\")",
          "77:     aggregate = _unsupported_function_rolling(\"aggregate\")",
          "78:     apply = _unsupported_function_rolling(\"apply\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "79:     # NOTE: Please update the document \"Supported pandas APIs\" when implementing the new API.",
          "80:     # Documentation path: `python/docs/source/user_guide/pandas_on_spark/supported_pandas_api.rst`.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "93: class MissingPandasLikeExpandingGroupby:",
          "94:     agg = _unsupported_function_expanding(\"agg\")",
          "95:     aggregate = _unsupported_function_expanding(\"aggregate\")",
          "96:     apply = _unsupported_function_expanding(\"apply\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "100:     # NOTE: Please update the document \"Supported pandas APIs\" when implementing the new API.",
          "101:     # Documentation path: `python/docs/source/user_guide/pandas_on_spark/supported_pandas_api.rst`.",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "111: class MissingPandasLikeRollingGroupby:",
          "112:     agg = _unsupported_function_rolling(\"agg\")",
          "113:     aggregate = _unsupported_function_rolling(\"aggregate\")",
          "114:     apply = _unsupported_function_rolling(\"apply\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "121:     # NOTE: Please update the document \"Supported pandas APIs\" when implementing the new API.",
          "122:     # Documentation path: `python/docs/source/user_guide/pandas_on_spark/supported_pandas_api.rst`.",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ab1d986b631f531ae0f756d4f3a536d30c858604",
      "candidate_info": {
        "commit_hash": "ab1d986b631f531ae0f756d4f3a536d30c858604",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/ab1d986b631f531ae0f756d4f3a536d30c858604",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala"
        ],
        "message": "[SPARK-37544][SQL] Correct date arithmetic in sequences\n\n### What changes were proposed in this pull request?\n\nChange `InternalSequenceBase` to pass a time-zone aware value to `DateTimeUtils#timestampAddInterval`, rather than a time-zone agnostic value, when performing `Date` arithmetic.\n\n### Why are the changes needed?\n\nThe following query gets the wrong answer if run in the America/Los_Angeles time zone:\n```\nspark-sql> select sequence(date '2021-01-01', date '2022-01-01', interval '3' month) x;\n[2021-01-01,2021-03-31,2021-06-30,2021-09-30,2022-01-01]\nTime taken: 0.664 seconds, Fetched 1 row(s)\nspark-sql>\n```\nThe answer should be\n```\n[2021-01-01,2021-04-01,2021-07-01,2021-10-01,2022-01-01]\n```\n`InternalSequenceBase` converts the date to micros by multiplying days by micros per day. This converts the date into a time-zone agnostic timestamp. However, `InternalSequenceBase` uses `DateTimeUtils#timestampAddInterval` to perform the arithmetic, and that function assumes a _time-zone aware_ timestamp.\n\nOne simple fix would be to call `DateTimeUtils#timestampNTZAddInterval` instead for date arithmetic. However, Spark date arithmetic is typically time-zone aware (see the comment in the test added by this PR), so this PR converts the date to a time-zone aware value before calling `DateTimeUtils#timestampAddInterval`.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nNew unit test.\n\nCloses #36546 from bersprockets/date_sequence_issue.\n\nAuthored-by: Bruce Robbins <bersprockets@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 14ee0d8f04f218ad61688196a0b984f024151468)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "3012:       case TimestampNTZType => timestampNTZAddInterval",
          "3013:     }",
          "3015:     override def eval(input1: Any, input2: Any, input3: Any): Array[T] = {",
          "3016:       val start = input1.asInstanceOf[T]",
          "3017:       val stop = input2.asInstanceOf[T]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3015:     private def toMicros(value: Long, scale: Long): Long = {",
          "3016:       if (scale == MICROS_PER_DAY) {",
          "3017:         daysToMicros(value.toInt, zoneId)",
          "3018:       } else {",
          "3019:         value * scale",
          "3020:       }",
          "3021:     }",
          "3023:     private def fromMicros(value: Long, scale: Long): Long = {",
          "3024:       if (scale == MICROS_PER_DAY) {",
          "3025:         microsToDays(value, zoneId).toLong",
          "3026:       } else {",
          "3027:         value / scale",
          "3028:       }",
          "3029:     }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3036:         val intervalStepInMicros =",
          "3037:           stepMicros + stepMonths * microsPerMonth + stepDays * MICROS_PER_DAY",
          "3041:         val maxEstimatedArrayLength =",
          "3042:           getSequenceLength(startMicros, stopMicros, input3, intervalStepInMicros)",
          "",
          "[Removed Lines]",
          "3038:         val startMicros: Long = num.toLong(start) * scale",
          "3039:         val stopMicros: Long = num.toLong(stop) * scale",
          "",
          "[Added Lines]",
          "3055:         val startMicros: Long = toMicros(num.toLong(start), scale)",
          "3056:         val stopMicros: Long = toMicros(num.toLong(stop), scale)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "3048:         var i = 0",
          "3050:         while (t < exclusiveItem ^ stepSign < 0) {",
          "3052:           i += 1",
          "3053:           t = addInterval(startMicros, i * stepMonths, i * stepDays, i * stepMicros, zoneId)",
          "3054:         }",
          "",
          "[Removed Lines]",
          "3051:           arr(i) = fromLong(t / scale)",
          "",
          "[Added Lines]",
          "3068:           val result = fromMicros(t, scale)",
          "3069:           arr(i) = fromLong(result)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "3061:     protected def stepSplitCode(",
          "3062:          stepMonths: String, stepDays: String, stepMicros: String, step: String): String",
          "3064:     private val addIntervalCode = outerDataType match {",
          "3069:     }",
          "3071:     override def genCode(",
          "3072:         ctx: CodegenContext,",
          "3073:         start: String,",
          "",
          "[Removed Lines]",
          "3065:       case TimestampType | DateType =>",
          "3066:         \"org.apache.spark.sql.catalyst.util.DateTimeUtils.timestampAddInterval\"",
          "3067:       case TimestampNTZType =>",
          "3068:         \"org.apache.spark.sql.catalyst.util.DateTimeUtils.timestampNTZAddInterval\"",
          "",
          "[Added Lines]",
          "3082:     private val dtu = DateTimeUtils.getClass.getName.stripSuffix(\"$\")",
          "3085:       case TimestampType | DateType => s\"$dtu.timestampAddInterval\"",
          "3086:       case TimestampNTZType => s\"$dtu.timestampNTZAddInterval\"",
          "3089:     private val daysToMicrosCode = s\"$dtu.daysToMicros\"",
          "3090:     private val microsToDaysCode = s\"$dtu.microsToDays\"",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "3112:       val stepSplits = stepSplitCode(stepMonths, stepDays, stepMicros, step)",
          "3114:       s\"\"\"",
          "3115:          |$stepSplits",
          "3116:          |",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3135:       val toMicrosCode = if (scale == MICROS_PER_DAY) {",
          "3136:         s\"\"\"",
          "3137:           |  final long $startMicros = $daysToMicrosCode((int) $start, $zid);",
          "3138:           |  final long $stopMicros = $daysToMicrosCode((int) $stop, $zid);",
          "3139:           |\"\"\".stripMargin",
          "3140:       } else {",
          "3141:         s\"\"\"",
          "3142:           |  final long $startMicros = $start * ${scale}L;",
          "3143:           |  final long $stopMicros = $stop * ${scale}L;",
          "3144:           |\"\"\".stripMargin",
          "3145:       }",
          "3147:       val fromMicrosCode = if (scale == MICROS_PER_DAY) {",
          "3148:         s\"($elemType) $microsToDaysCode($t, $zid)\"",
          "3149:       } else {",
          "3150:         s\"($elemType) ($t / ${scale}L)\"",
          "3151:       }",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "3122:          |} else if ($stepMonths == 0 && $stepDays == 0 && ${scale}L == 1) {",
          "3123:          |  ${backedSequenceImpl.genCode(ctx, start, stop, stepMicros, arr, elemType)};",
          "3124:          |} else {",
          "3127:          |",
          "3128:          |  $sequenceLengthCode",
          "3129:          |",
          "",
          "[Removed Lines]",
          "3125:          |  final long $startMicros = $start * ${scale}L;",
          "3126:          |  final long $stopMicros = $stop * ${scale}L;",
          "",
          "[Added Lines]",
          "3164:          |  $toMicrosCode",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "3135:          |  int $i = 0;",
          "3136:          |",
          "3137:          |  while ($t < $exclusiveItem ^ $stepSign < 0) {",
          "3139:          |    $i += 1;",
          "3140:          |    $t = $addIntervalCode(",
          "3141:          |       $startMicros, $i * $stepMonths, $i * $stepDays, $i * $stepMicros, $zid);",
          "",
          "[Removed Lines]",
          "3138:          |    $arr[$i] = ($elemType) ($t / ${scale}L);",
          "",
          "[Added Lines]",
          "3176:          |    $arr[$i] = $fromMicrosCode;",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: import org.apache.spark.sql.catalyst.InternalRow",
          "30: import org.apache.spark.sql.catalyst.analysis.TypeCheckResult",
          "31: import org.apache.spark.sql.catalyst.util.{DateTimeTestUtils, DateTimeUtils}",
          "33: import org.apache.spark.sql.catalyst.util.IntervalUtils._",
          "34: import org.apache.spark.sql.internal.SQLConf",
          "35: import org.apache.spark.sql.types._",
          "",
          "[Removed Lines]",
          "32: import org.apache.spark.sql.catalyst.util.DateTimeTestUtils.UTC",
          "",
          "[Added Lines]",
          "32: import org.apache.spark.sql.catalyst.util.DateTimeTestUtils.{outstandingZoneIds, LA, UTC}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "964:     }",
          "965:   }",
          "967:   test(\"SPARK-35088: Accept ANSI intervals by the Sequence expression\") {",
          "968:     checkEvaluation(new Sequence(",
          "969:       Literal(Timestamp.valueOf(\"2018-01-01 00:00:00\")),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "964:     }",
          "965:   }",
          "967:   test(\"SPARK-37544: Time zone should not affect date sequence with month interval\") {",
          "968:     outstandingZoneIds.foreach { zid =>",
          "969:       DateTimeTestUtils.withDefaultTimeZone(zid) {",
          "970:         checkEvaluation(new Sequence(",
          "971:           Literal(Date.valueOf(\"2021-01-01\")),",
          "972:           Literal(Date.valueOf(\"2022-01-01\")),",
          "973:           Literal(stringToInterval(\"interval 3 month\"))),",
          "974:           Seq(",
          "975:             Date.valueOf(\"2021-01-01\"),",
          "976:             Date.valueOf(\"2021-04-01\"),",
          "977:             Date.valueOf(\"2021-07-01\"),",
          "978:             Date.valueOf(\"2021-10-01\"),",
          "979:             Date.valueOf(\"2022-01-01\")))",
          "980:       }",
          "981:     }",
          "990:     DateTimeTestUtils.withDefaultTimeZone(LA) {",
          "991:       checkEvaluation(new Sequence(",
          "992:         Literal(Date.valueOf(\"2022-03-09\")),",
          "993:         Literal(Date.valueOf(\"2022-03-15\")),",
          "994:         Literal(stringToInterval(\"interval 4 days 23 hours\"))),",
          "995:         Seq(",
          "996:           Date.valueOf(\"2022-03-09\"),",
          "997:           Date.valueOf(\"2022-03-14\")))",
          "998:     }",
          "1000:     DateTimeTestUtils.withDefaultTimeZone(UTC) {",
          "1001:       checkEvaluation(new Sequence(",
          "1002:         Literal(Date.valueOf(\"2022-03-09\")),",
          "1003:         Literal(Date.valueOf(\"2022-03-15\")),",
          "1004:         Literal(stringToInterval(\"interval 4 days 23 hours\"))),",
          "1005:         Seq(",
          "1006:           Date.valueOf(\"2022-03-09\"),",
          "1007:           Date.valueOf(\"2022-03-13\"))) // this is different from LA time zone above",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b066561cba01ef1ddc2dc8c5e21ef54cc22bfe08",
      "candidate_info": {
        "commit_hash": "b066561cba01ef1ddc2dc8c5e21ef54cc22bfe08",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/b066561cba01ef1ddc2dc8c5e21ef54cc22bfe08",
        "files": [
          "dev/deps/spark-deps-hadoop-2-hive-2.3",
          "dev/deps/spark-deps-hadoop-3-hive-2.3",
          "pom.xml"
        ],
        "message": "[SPARK-40326][BUILD] Upgrade `fasterxml.jackson.version` to 2.13.4\n\nupgrade `com.fasterxml.jackson.dataformat:jackson-dataformat-yaml` and `fasterxml.jackson.databind.version` from 2.13.3 to 2.13.4\n\n[CVE-2022-25857](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-25857)\n\n[SNYK-JAVA-ORGYAML](https://security.snyk.io/vuln/SNYK-JAVA-ORGYAML-2806360)\n\nNo.\n\nPass GA\n\nCloses #37796 from bjornjorgensen/upgrade-fasterxml.jackson-to-2.13.4.\n\nAuthored-by: Bj\u00f8rn <bjornjorgensen@gmail.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n(cherry picked from commit a82a006df80ac3aa6900d8688eb5bf77b804785d)\nSigned-off-by: Sean Owen <srowen@gmail.com>",
        "before_after_code_files": [
          "dev/deps/spark-deps-hadoop-2-hive-2.3||dev/deps/spark-deps-hadoop-2-hive-2.3",
          "dev/deps/spark-deps-hadoop-3-hive-2.3||dev/deps/spark-deps-hadoop-3-hive-2.3"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/deps/spark-deps-hadoop-2-hive-2.3||dev/deps/spark-deps-hadoop-2-hive-2.3": [
          "File: dev/deps/spark-deps-hadoop-2-hive-2.3 -> dev/deps/spark-deps-hadoop-2-hive-2.3",
          "--- Hunk 1 ---",
          "[Context before]",
          "112: httpcore/4.4.14//httpcore-4.4.14.jar",
          "113: istack-commons-runtime/3.0.8//istack-commons-runtime-3.0.8.jar",
          "114: ivy/2.5.0//ivy-2.5.0.jar",
          "116: jackson-core-asl/1.9.13//jackson-core-asl-1.9.13.jar",
          "122: jackson-jaxrs/1.9.13//jackson-jaxrs-1.9.13.jar",
          "123: jackson-mapper-asl/1.9.13//jackson-mapper-asl-1.9.13.jar",
          "125: jackson-xc/1.9.13//jackson-xc-1.9.13.jar",
          "126: jakarta.annotation-api/1.3.5//jakarta.annotation-api-1.3.5.jar",
          "127: jakarta.inject/2.6.1//jakarta.inject-2.6.1.jar",
          "",
          "[Removed Lines]",
          "115: jackson-annotations/2.13.3//jackson-annotations-2.13.3.jar",
          "117: jackson-core/2.13.3//jackson-core-2.13.3.jar",
          "118: jackson-databind/2.13.3//jackson-databind-2.13.3.jar",
          "119: jackson-dataformat-cbor/2.13.3//jackson-dataformat-cbor-2.13.3.jar",
          "120: jackson-dataformat-yaml/2.13.3//jackson-dataformat-yaml-2.13.3.jar",
          "121: jackson-datatype-jsr310/2.13.3//jackson-datatype-jsr310-2.13.3.jar",
          "124: jackson-module-scala_2.12/2.13.3//jackson-module-scala_2.12-2.13.3.jar",
          "",
          "[Added Lines]",
          "115: jackson-annotations/2.13.4//jackson-annotations-2.13.4.jar",
          "117: jackson-core/2.13.4//jackson-core-2.13.4.jar",
          "118: jackson-databind/2.13.4//jackson-databind-2.13.4.jar",
          "119: jackson-dataformat-cbor/2.13.4//jackson-dataformat-cbor-2.13.4.jar",
          "120: jackson-dataformat-yaml/2.13.4//jackson-dataformat-yaml-2.13.4.jar",
          "121: jackson-datatype-jsr310/2.13.4//jackson-datatype-jsr310-2.13.4.jar",
          "124: jackson-module-scala_2.12/2.13.4//jackson-module-scala_2.12-2.13.4.jar",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "245: shapeless_2.12/2.3.7//shapeless_2.12-2.3.7.jar",
          "246: shims/0.9.25//shims-0.9.25.jar",
          "247: slf4j-api/1.7.32//slf4j-api-1.7.32.jar",
          "249: snappy-java/1.1.8.4//snappy-java-1.1.8.4.jar",
          "250: spire-macros_2.12/0.17.0//spire-macros_2.12-0.17.0.jar",
          "251: spire-platform_2.12/0.17.0//spire-platform_2.12-0.17.0.jar",
          "",
          "[Removed Lines]",
          "248: snakeyaml/1.30//snakeyaml-1.30.jar",
          "",
          "[Added Lines]",
          "248: snakeyaml/1.31//snakeyaml-1.31.jar",
          "",
          "---------------"
        ],
        "dev/deps/spark-deps-hadoop-3-hive-2.3||dev/deps/spark-deps-hadoop-3-hive-2.3": [
          "File: dev/deps/spark-deps-hadoop-3-hive-2.3 -> dev/deps/spark-deps-hadoop-3-hive-2.3",
          "--- Hunk 1 ---",
          "[Context before]",
          "102: ini4j/0.5.4//ini4j-0.5.4.jar",
          "103: istack-commons-runtime/3.0.8//istack-commons-runtime-3.0.8.jar",
          "104: ivy/2.5.0//ivy-2.5.0.jar",
          "106: jackson-core-asl/1.9.13//jackson-core-asl-1.9.13.jar",
          "112: jackson-mapper-asl/1.9.13//jackson-mapper-asl-1.9.13.jar",
          "114: jakarta.annotation-api/1.3.5//jakarta.annotation-api-1.3.5.jar",
          "115: jakarta.inject/2.6.1//jakarta.inject-2.6.1.jar",
          "116: jakarta.servlet-api/4.0.3//jakarta.servlet-api-4.0.3.jar",
          "",
          "[Removed Lines]",
          "105: jackson-annotations/2.13.3//jackson-annotations-2.13.3.jar",
          "107: jackson-core/2.13.3//jackson-core-2.13.3.jar",
          "108: jackson-databind/2.13.3//jackson-databind-2.13.3.jar",
          "109: jackson-dataformat-cbor/2.13.3//jackson-dataformat-cbor-2.13.3.jar",
          "110: jackson-dataformat-yaml/2.13.3//jackson-dataformat-yaml-2.13.3.jar",
          "111: jackson-datatype-jsr310/2.13.3//jackson-datatype-jsr310-2.13.3.jar",
          "113: jackson-module-scala_2.12/2.13.3//jackson-module-scala_2.12-2.13.3.jar",
          "",
          "[Added Lines]",
          "105: jackson-annotations/2.13.4//jackson-annotations-2.13.4.jar",
          "107: jackson-core/2.13.4//jackson-core-2.13.4.jar",
          "108: jackson-databind/2.13.4//jackson-databind-2.13.4.jar",
          "109: jackson-dataformat-cbor/2.13.4//jackson-dataformat-cbor-2.13.4.jar",
          "110: jackson-dataformat-yaml/2.13.4//jackson-dataformat-yaml-2.13.4.jar",
          "111: jackson-datatype-jsr310/2.13.4//jackson-datatype-jsr310-2.13.4.jar",
          "113: jackson-module-scala_2.12/2.13.4//jackson-module-scala_2.12-2.13.4.jar",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "234: shapeless_2.12/2.3.7//shapeless_2.12-2.3.7.jar",
          "235: shims/0.9.25//shims-0.9.25.jar",
          "236: slf4j-api/1.7.32//slf4j-api-1.7.32.jar",
          "238: snappy-java/1.1.8.4//snappy-java-1.1.8.4.jar",
          "239: spire-macros_2.12/0.17.0//spire-macros_2.12-0.17.0.jar",
          "240: spire-platform_2.12/0.17.0//spire-platform_2.12-0.17.0.jar",
          "",
          "[Removed Lines]",
          "237: snakeyaml/1.30//snakeyaml-1.30.jar",
          "",
          "[Added Lines]",
          "237: snakeyaml/1.31//snakeyaml-1.31.jar",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4512e0943036d30587ab19a95efb0e66b47dd746",
      "candidate_info": {
        "commit_hash": "4512e0943036d30587ab19a95efb0e66b47dd746",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/4512e0943036d30587ab19a95efb0e66b47dd746",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ColumnPruningSuite.scala"
        ],
        "message": "Revert \"[SPARK-38531][SQL] Fix the condition of \"Prune unrequired child index\" branch of ColumnPruning\"\n\nThis reverts commit 17c56fc03b8e7269b293d6957c542eab9d723d52.",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ColumnPruningSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ColumnPruningSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "314:   }",
          "315: }",
          "",
          "[Removed Lines]",
          "317: object GeneratorUnrequiredChildrenPruning {",
          "318:   def unapply(plan: LogicalPlan): Option[LogicalPlan] = plan match {",
          "319:     case p @ Project(_, g: Generate) =>",
          "320:       val requiredAttrs = p.references ++ g.generator.references",
          "321:       val newChild = ColumnPruning.prunedChild(g.child, requiredAttrs)",
          "322:       val unrequired = g.generator.references -- p.references",
          "323:       val unrequiredIndices = newChild.output.zipWithIndex.filter(t => unrequired.contains(t._1))",
          "324:         .map(_._2)",
          "325:       if (!newChild.fastEquals(g.child) ||",
          "326:         unrequiredIndices.toSet != g.unrequiredChildIndex.toSet) {",
          "327:         Some(p.copy(child = g.copy(child = newChild, unrequiredChildIndex = unrequiredIndices)))",
          "328:       } else {",
          "329:         None",
          "330:       }",
          "331:     case _ => None",
          "332:   }",
          "333: }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "842:       e.copy(child = prunedChild(child, e.references))",
          "853:     case GeneratorNestedColumnAliasing(rewrittenPlan) => rewrittenPlan",
          "",
          "[Removed Lines]",
          "850:     case GeneratorUnrequiredChildrenPruning(rewrittenPlan) => rewrittenPlan",
          "",
          "[Added Lines]",
          "845:     case p @ Project(_, g: Generate) if p.references != g.outputSet =>",
          "846:       val requiredAttrs = p.references -- g.producedAttributes ++ g.generator.references",
          "847:       val newChild = prunedChild(g.child, requiredAttrs)",
          "848:       val unrequired = g.generator.references -- p.references",
          "849:       val unrequiredIndices = newChild.output.zipWithIndex.filter(t => unrequired.contains(t._1))",
          "850:         .map(_._2)",
          "851:       p.copy(child = g.copy(child = newChild, unrequiredChildIndex = unrequiredIndices))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "907:   })",
          "911:     if (!c.outputSet.subsetOf(allReferences)) {",
          "912:       Project(c.output.filter(allReferences.contains), c)",
          "913:     } else {",
          "",
          "[Removed Lines]",
          "910:   def prunedChild(c: LogicalPlan, allReferences: AttributeSet): LogicalPlan =",
          "",
          "[Added Lines]",
          "911:   private def prunedChild(c: LogicalPlan, allReferences: AttributeSet) =",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ColumnPruningSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ColumnPruningSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ColumnPruningSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ColumnPruningSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import org.apache.spark.sql.catalyst.dsl.plans._",
          "25: import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder",
          "26: import org.apache.spark.sql.catalyst.expressions._",
          "28: import org.apache.spark.sql.catalyst.plans.{Inner, PlanTest}",
          "29: import org.apache.spark.sql.catalyst.plans.logical._",
          "30: import org.apache.spark.sql.catalyst.rules.RuleExecutor",
          "",
          "[Removed Lines]",
          "27: import org.apache.spark.sql.catalyst.optimizer.NestedColumnAliasingSuite.collectGeneratedAliases",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "460:     val correctAnswer1 = Project(Seq('a), input).analyze",
          "461:     comparePlans(Optimize.execute(plan1.analyze), correctAnswer1)",
          "462:   }",
          "494: }",
          "",
          "[Removed Lines]",
          "464:   test(\"SPARK-38531: Nested field pruning for Project and PosExplode\") {",
          "465:     val name = StructType.fromDDL(\"first string, middle string, last string\")",
          "466:     val employer = StructType.fromDDL(\"id int, company struct<name:string, address:string>\")",
          "467:     val contact = LocalRelation(",
          "468:       'id.int,",
          "469:       'name.struct(name),",
          "470:       'address.string,",
          "471:       'friends.array(name),",
          "472:       'relatives.map(StringType, name),",
          "473:       'employer.struct(employer))",
          "475:     val query = contact",
          "476:       .select('id, 'friends)",
          "477:       .generate(PosExplode('friends))",
          "478:       .select('col.getField(\"middle\"))",
          "479:       .analyze",
          "480:     val optimized = Optimize.execute(query)",
          "482:     val aliases = collectGeneratedAliases(optimized)",
          "484:     val expected = contact",
          "486:       .select(",
          "487:         'friends.getField(\"middle\").as(aliases(0)))",
          "488:       .generate(PosExplode($\"${aliases(0)}\"),",
          "489:         unrequiredChildIndex = Seq(0)) // unrequiredChildIndex is added.",
          "490:       .select('col.as(\"col.middle\"))",
          "491:       .analyze",
          "492:     comparePlans(optimized, expected)",
          "493:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    }
  ]
}