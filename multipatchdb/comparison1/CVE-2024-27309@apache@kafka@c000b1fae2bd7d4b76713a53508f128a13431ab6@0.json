{
  "cve_id": "CVE-2024-27309",
  "cve_desc": "While an Apache Kafka cluster is being migrated from ZooKeeper mode to KRaft mode, in some cases ACLs will not be correctly enforced.\n\nTwo preconditions are needed to trigger the bug:\n1. The administrator decides to remove an ACL\n2. The resource associated with the removed ACL continues to have two or more other ACLs associated with it after the removal.\n\nWhen those two preconditions are met, Kafka will treat the resource as if it had only one ACL associated with it after the removal, rather than the two or more that would be correct.\n\nThe incorrect condition is cleared by removing all brokers in ZK mode, or by adding a new ACL to the affected resource. Once the migration is completed, there is no metadata loss (the ACLs all remain).\n\nThe full impact depends on the ACLs in use. If only ALLOW ACLs were configured during the migration, the impact would be limited to availability impact. if DENY ACLs were configured, the impact could include confidentiality and integrity impact depending on the ACLs configured, as the DENY ACLs might be ignored due to this vulnerability during the migration period.",
  "repo": "apache/kafka",
  "patch_hash": "c000b1fae2bd7d4b76713a53508f128a13431ab6",
  "patch_info": {
    "commit_hash": "c000b1fae2bd7d4b76713a53508f128a13431ab6",
    "repo": "apache/kafka",
    "commit_url": "https://github.com/apache/kafka/commit/c000b1fae2bd7d4b76713a53508f128a13431ab6",
    "files": [
      "core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala",
      "core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala",
      "core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala",
      "metadata/src/main/java/org/apache/kafka/image/AclsDelta.java",
      "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
      "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java",
      "metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java"
    ],
    "message": "MINOR: Fix some MetadataDelta handling issues during ZK migration (#15327)\n\nReviewers: Colin P. McCabe <cmccabe@apache.org>",
    "before_after_code_files": [
      "core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala",
      "core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala",
      "core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala",
      "metadata/src/main/java/org/apache/kafka/image/AclsDelta.java||metadata/src/main/java/org/apache/kafka/image/AclsDelta.java",
      "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
      "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java",
      "metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java||metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java"
    ]
  },
  "patch_diff": {
    "core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala": [
      "File: core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala -> core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala",
      "--- Hunk 1 ---",
      "[Context before]",
      "21: import kafka.utils.TestUtils",
      "22: import org.apache.kafka.common.Uuid",
      "23: import org.apache.kafka.common.acl._",
      "25: import org.apache.kafka.common.resource.{PatternType, ResourcePattern, ResourcePatternFilter, ResourceType}",
      "26: import org.apache.kafka.common.security.auth.KafkaPrincipal",
      "27: import org.apache.kafka.common.utils.SecurityUtils",
      "28: import org.apache.kafka.image.{MetadataDelta, MetadataImage, MetadataProvenance}",
      "29: import org.apache.kafka.metadata.migration.KRaftMigrationZkWriter",
      "30: import org.apache.kafka.server.common.ApiMessageAndVersion",
      "32: import org.junit.jupiter.api.Test",
      "34: import scala.collection.mutable",
      "",
      "[Removed Lines]",
      "24: import org.apache.kafka.common.metadata.AccessControlEntryRecord",
      "31: import org.junit.jupiter.api.Assertions.{assertEquals, assertTrue}",
      "",
      "[Added Lines]",
      "24: import org.apache.kafka.common.metadata.{AccessControlEntryRecord, RemoveAccessControlEntryRecord}",
      "31: import org.junit.jupiter.api.Assertions.{assertEquals, assertTrue, fail}",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "169:     val image = delta.apply(MetadataProvenance.EMPTY)",
      "173:     kraftMigrationZkWriter.handleSnapshot(image, (_, _, operation) => { migrationState = operation.apply(migrationState) })",
      "",
      "[Removed Lines]",
      "172:     val kraftMigrationZkWriter = new KRaftMigrationZkWriter(migrationClient)",
      "",
      "[Added Lines]",
      "172:     val kraftMigrationZkWriter = new KRaftMigrationZkWriter(migrationClient, fail(_))",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "189:         AclPermissionType.fromCode(acl1Resource3.permissionType())),",
      "190:       resource3AclsInZk.head.ace)",
      "191:   }",
      "192: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "193:   def user(user: String): String = {",
      "194:     new KafkaPrincipal(KafkaPrincipal.USER_TYPE, user).toString",
      "195:   }",
      "197:   def acl(resourceName: String,",
      "198:           resourceType: ResourceType,",
      "199:           resourcePattern: PatternType,",
      "200:           principal: String,",
      "201:           host: String = \"*\",",
      "202:           operation: AclOperation = AclOperation.READ,",
      "203:           permissionType: AclPermissionType = AclPermissionType.ALLOW",
      "204:   ): AccessControlEntryRecord = {",
      "205:     new AccessControlEntryRecord()",
      "206:       .setId(Uuid.randomUuid())",
      "207:       .setHost(host)",
      "208:       .setOperation(operation.code())",
      "209:       .setPrincipal(principal)",
      "210:       .setPermissionType(permissionType.code())",
      "211:       .setPatternType(resourcePattern.code())",
      "212:       .setResourceName(resourceName)",
      "213:       .setResourceType(resourceType.code())",
      "214:   }",
      "216:   @Test",
      "217:   def testDeleteOneAclOfMany(): Unit = {",
      "218:     zkClient.createAclPaths()",
      "219:     val topicName = \"topic-\" + Uuid.randomUuid()",
      "220:     val resource = new ResourcePattern(ResourceType.TOPIC, topicName, PatternType.LITERAL)",
      "223:     val delta = new MetadataDelta(MetadataImage.EMPTY)",
      "224:     val acl1 = acl(topicName, ResourceType.TOPIC, PatternType.LITERAL, user(\"alice\"))",
      "225:     val acl2 = acl(topicName, ResourceType.TOPIC, PatternType.LITERAL, user(\"bob\"))",
      "226:     val acl3 = acl(topicName, ResourceType.TOPIC, PatternType.LITERAL, user(\"carol\"))",
      "227:     delta.replay(acl1)",
      "228:     delta.replay(acl2)",
      "229:     delta.replay(acl3)",
      "230:     val image = delta.apply(MetadataProvenance.EMPTY)",
      "233:     val errorLogs = mutable.Buffer[String]()",
      "234:     val kraftMigrationZkWriter = new KRaftMigrationZkWriter(migrationClient, errorLogs.append)",
      "235:     kraftMigrationZkWriter.handleSnapshot(image, (_, _, operation) => {",
      "236:       migrationState = operation.apply(migrationState)",
      "237:     })",
      "240:     val aclsInZk = zkClient.getVersionedAclsForResource(resource).acls",
      "241:     assertEquals(3, aclsInZk.size)",
      "244:     val delta2 = new MetadataDelta.Builder()",
      "245:       .setImage(image)",
      "246:       .build()",
      "247:     delta2.replay(new RemoveAccessControlEntryRecord().setId(acl3.id()))",
      "248:     val image2 = delta2.apply(MetadataProvenance.EMPTY)",
      "249:     kraftMigrationZkWriter.handleDelta(image, image2, delta2, (_, _, operation) => {",
      "250:       migrationState = operation.apply(migrationState)",
      "251:     })",
      "254:     val aclsInZk2 = zkClient.getVersionedAclsForResource(resource).acls",
      "255:     assertEquals(2, aclsInZk2.size)",
      "256:     assertEquals(0, errorLogs.size)",
      "259:     val acl4 = acl(topicName, ResourceType.TOPIC, PatternType.LITERAL, user(\"carol\"))",
      "260:     delta2.replay(acl4)",
      "261:     val image3 = delta2.apply(MetadataProvenance.EMPTY)",
      "265:     kraftMigrationZkWriter.handleDelta(image3, image3, delta2, (_, _, operation) => {",
      "266:       migrationState = operation.apply(migrationState)",
      "267:     })",
      "269:     val aclsInZk3 = zkClient.getVersionedAclsForResource(resource).acls",
      "270:     assertEquals(3, aclsInZk3.size)",
      "271:     assertEquals(1, errorLogs.size)",
      "272:     assertEquals(s\"Cannot delete ACL ${acl3.id()} from ZK since it is missing from previous AclImage\", errorLogs.head)",
      "273:   }",
      "275:   @Test",
      "276:   def testAclUpdateAndDelete(): Unit = {",
      "277:     zkClient.createAclPaths()",
      "278:     val errorLogs = mutable.Buffer[String]()",
      "279:     val kraftMigrationZkWriter = new KRaftMigrationZkWriter(migrationClient, errorLogs.append)",
      "281:     val topicName = \"topic-\" + Uuid.randomUuid()",
      "282:     val otherName = \"other-\" + Uuid.randomUuid()",
      "283:     val literalResource = new ResourcePattern(ResourceType.TOPIC, topicName, PatternType.LITERAL)",
      "284:     val prefixedResource = new ResourcePattern(ResourceType.TOPIC, topicName, PatternType.PREFIXED)",
      "285:     val otherResource = new ResourcePattern(ResourceType.TOPIC, otherName, PatternType.LITERAL)",
      "288:     val acl1 = acl(topicName, ResourceType.TOPIC, PatternType.LITERAL, user(\"alice\"))",
      "289:     val acl2 = acl(topicName, ResourceType.TOPIC, PatternType.LITERAL, user(\"bob\"))",
      "290:     val acl3 = acl(topicName, ResourceType.TOPIC, PatternType.LITERAL, user(\"carol\"))",
      "291:     val acl4 = acl(topicName, ResourceType.TOPIC, PatternType.LITERAL, user(\"dave\"))",
      "293:     val delta1 = new MetadataDelta(MetadataImage.EMPTY)",
      "294:     delta1.replay(acl1)",
      "295:     delta1.replay(acl2)",
      "296:     delta1.replay(acl3)",
      "297:     delta1.replay(acl4)",
      "299:     val image1 = delta1.apply(MetadataProvenance.EMPTY)",
      "300:     kraftMigrationZkWriter.handleDelta(MetadataImage.EMPTY, image1, delta1, (_, _, operation) => {",
      "301:       migrationState = operation.apply(migrationState)",
      "302:     })",
      "303:     assertEquals(4, zkClient.getVersionedAclsForResource(literalResource).acls.size)",
      "304:     assertEquals(0, zkClient.getVersionedAclsForResource(prefixedResource).acls.size)",
      "305:     assertEquals(0, zkClient.getVersionedAclsForResource(otherResource).acls.size)",
      "306:     assertEquals(0, errorLogs.size)",
      "308:     val acl5 = acl(topicName, ResourceType.TOPIC, PatternType.PREFIXED, user(\"alice\"))",
      "309:     val acl6 = acl(topicName, ResourceType.TOPIC, PatternType.PREFIXED, user(\"bob\"))",
      "310:     val acl7 = acl(otherName, ResourceType.TOPIC, PatternType.LITERAL, user(\"carol\"))",
      "311:     val acl8 = acl(otherName, ResourceType.TOPIC, PatternType.LITERAL, user(\"dave\"))",
      "314:     val delta2 = new MetadataDelta.Builder().setImage(image1).build()",
      "315:     delta2.replay(acl5)",
      "316:     delta2.replay(acl6)",
      "317:     delta2.replay(acl7)",
      "318:     delta2.replay(acl8)",
      "319:     delta2.replay(new RemoveAccessControlEntryRecord().setId(acl1.id()))",
      "321:     val image2 = delta2.apply(MetadataProvenance.EMPTY)",
      "322:     kraftMigrationZkWriter.handleDelta(image1, image2, delta2, (_, _, operation) => {",
      "323:       migrationState = operation.apply(migrationState)",
      "324:     })",
      "325:     assertEquals(3, zkClient.getVersionedAclsForResource(literalResource).acls.size)",
      "326:     assertEquals(2, zkClient.getVersionedAclsForResource(prefixedResource).acls.size)",
      "327:     assertEquals(2, zkClient.getVersionedAclsForResource(otherResource).acls.size)",
      "328:     assertEquals(0, errorLogs.size)",
      "331:     val acl9 = acl(otherName, ResourceType.TOPIC, PatternType.LITERAL, user(\"eve\"))",
      "332:     val delta3 = new MetadataDelta.Builder().setImage(image2).build()",
      "333:     delta3.replay(acl1)",
      "334:     delta3.replay(new RemoveAccessControlEntryRecord().setId(acl2.id()))",
      "335:     delta3.replay(new RemoveAccessControlEntryRecord().setId(acl5.id()))",
      "336:     delta3.replay(new RemoveAccessControlEntryRecord().setId(acl6.id()))",
      "337:     delta3.replay(acl9)",
      "339:     val image3 = delta3.apply(MetadataProvenance.EMPTY)",
      "340:     kraftMigrationZkWriter.handleDelta(image2, image3, delta3, (_, _, operation) => {",
      "341:       migrationState = operation.apply(migrationState)",
      "342:     })",
      "343:     assertEquals(3, zkClient.getVersionedAclsForResource(literalResource).acls.size)",
      "344:     assertEquals(0, zkClient.getVersionedAclsForResource(prefixedResource).acls.size)",
      "345:     assertEquals(3, zkClient.getVersionedAclsForResource(otherResource).acls.size)",
      "346:     assertEquals(0, errorLogs.size)",
      "347:   }",
      "",
      "---------------"
    ],
    "core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala": [
      "File: core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala -> core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala",
      "--- Hunk 1 ---",
      "[Context before]",
      "40: import org.apache.kafka.server.common.ApiMessageAndVersion",
      "41: import org.apache.kafka.server.config.ConfigType",
      "42: import org.apache.kafka.server.util.MockRandom",
      "44: import org.junit.jupiter.api.Test",
      "46: import java.util",
      "",
      "[Removed Lines]",
      "43: import org.junit.jupiter.api.Assertions.{assertEquals, assertTrue}",
      "",
      "[Added Lines]",
      "43: import org.junit.jupiter.api.Assertions.{assertEquals, assertTrue, fail}",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "326:     val image = delta.apply(MetadataProvenance.EMPTY)",
      "330:     kraftMigrationZkWriter.handleSnapshot(image, (_, _, operation) => {",
      "331:       migrationState = operation.apply(migrationState)",
      "332:     })",
      "",
      "[Removed Lines]",
      "329:     val kraftMigrationZkWriter = new KRaftMigrationZkWriter(migrationClient)",
      "",
      "[Added Lines]",
      "329:     val kraftMigrationZkWriter = new KRaftMigrationZkWriter(migrationClient, fail(_))",
      "",
      "---------------"
    ],
    "core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala": [
      "File: core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala -> core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala",
      "--- Hunk 1 ---",
      "[Context before]",
      "318:   @Test",
      "319:   def testTopicAndBrokerConfigsMigrationWithSnapshots(): Unit = {",
      "323:     val topicName = \"testTopic\"",
      "",
      "[Removed Lines]",
      "320:     val kraftWriter = new KRaftMigrationZkWriter(migrationClient)",
      "",
      "[Added Lines]",
      "320:     val kraftWriter = new KRaftMigrationZkWriter(migrationClient, fail(_))",
      "",
      "---------------"
    ],
    "metadata/src/main/java/org/apache/kafka/image/AclsDelta.java||metadata/src/main/java/org/apache/kafka/image/AclsDelta.java": [
      "File: metadata/src/main/java/org/apache/kafka/image/AclsDelta.java -> metadata/src/main/java/org/apache/kafka/image/AclsDelta.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "25: import org.apache.kafka.server.common.MetadataVersion;",
      "27: import java.util.HashMap;",
      "29: import java.util.LinkedHashMap;",
      "30: import java.util.Map;",
      "31: import java.util.Map.Entry;",
      "32: import java.util.Optional;",
      "34: import java.util.stream.Collectors;",
      "",
      "[Removed Lines]",
      "28: import java.util.HashSet;",
      "33: import java.util.Set;",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "40: public final class AclsDelta {",
      "41:     private final AclsImage image;",
      "42:     private final Map<Uuid, Optional<StandardAcl>> changes = new LinkedHashMap<>();",
      "45:     public AclsDelta(AclsImage image) {",
      "46:         this.image = image;",
      "",
      "[Removed Lines]",
      "43:     private final Set<StandardAcl> deleted = new HashSet<>();",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "56:         return changes;",
      "57:     }",
      "68:     void finishSnapshot() {",
      "69:         for (Entry<Uuid, StandardAcl> entry : image.acls().entrySet()) {",
      "70:             if (!changes.containsKey(entry.getKey())) {",
      "",
      "[Removed Lines]",
      "64:     public Set<StandardAcl> deleted() {",
      "65:         return deleted;",
      "66:     }",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "93:     public void replay(RemoveAccessControlEntryRecord record) {",
      "94:         if (image.acls().containsKey(record.id())) {",
      "95:             changes.put(record.id(), Optional.empty());",
      "97:         } else if (changes.containsKey(record.id())) {",
      "98:             changes.remove(record.id());",
      "",
      "[Removed Lines]",
      "96:             deleted.add(image.acls().get(record.id()));",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------"
    ],
    "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java": [
      "File: metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "134:         this.time = time;",
      "135:         LogContext logContext = new LogContext(\"[KRaftMigrationDriver id=\" + nodeId + \"] \");",
      "136:         this.controllerMetrics = controllerMetrics;",
      "138:         this.migrationState = MigrationDriverState.UNINITIALIZED;",
      "139:         this.migrationLeadershipState = ZkMigrationLeadershipState.EMPTY;",
      "140:         this.eventQueue = new KafkaEventQueue(Time.SYSTEM, logContext, \"controller-\" + nodeId + \"-migration-driver-\");",
      "",
      "[Removed Lines]",
      "137:         this.log = logContext.logger(KRaftMigrationDriver.class);",
      "",
      "[Added Lines]",
      "137:         Logger log = logContext.logger(KRaftMigrationDriver.class);",
      "138:         this.log = log;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "144:         this.initialZkLoadHandler = initialZkLoadHandler;",
      "145:         this.faultHandler = faultHandler;",
      "146:         this.quorumFeatures = quorumFeatures;",
      "148:         this.recordRedactor = new RecordRedactor(configSchema);",
      "149:         this.minBatchSize = minBatchSize;",
      "150:     }",
      "",
      "[Removed Lines]",
      "147:         this.zkMetadataWriter = new KRaftMigrationZkWriter(zkMigrationClient);",
      "",
      "[Added Lines]",
      "148:         this.zkMetadataWriter = new KRaftMigrationZkWriter(zkMigrationClient, log::error);",
      "",
      "---------------"
    ],
    "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java": [
      "File: metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "60: import java.util.Optional;",
      "61: import java.util.Set;",
      "62: import java.util.function.BiConsumer;",
      "63: import java.util.function.Function;",
      "66: public class KRaftMigrationZkWriter {",
      "",
      "[Removed Lines]",
      "64: import java.util.stream.Collectors;",
      "",
      "[Added Lines]",
      "63: import java.util.function.Consumer;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "84:     private final MigrationClient migrationClient;",
      "86:     public KRaftMigrationZkWriter(",
      "88:     ) {",
      "89:         this.migrationClient = migrationClient;",
      "90:     }",
      "92:     public void handleSnapshot(MetadataImage image, KRaftMigrationOperationConsumer operationConsumer) {",
      "",
      "[Removed Lines]",
      "87:         MigrationClient migrationClient",
      "",
      "[Added Lines]",
      "85:     private final Consumer<String> errorLogger;",
      "88:         MigrationClient migrationClient,",
      "89:         Consumer<String> errorLogger",
      "92:         this.errorLogger = errorLogger;",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "122:             updated = true;",
      "123:         }",
      "124:         if (delta.aclsDelta() != null) {",
      "126:             updated = true;",
      "127:         }",
      "128:         if (delta.delegationTokenDelta() != null) {",
      "",
      "[Removed Lines]",
      "125:             handleAclsDelta(image.acls(), delta.aclsDelta(), operationConsumer);",
      "",
      "[Added Lines]",
      "128:             handleAclsDelta(previousImage.acls(), image.acls(), delta.aclsDelta(), operationConsumer);",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "612:         });",
      "613:     }",
      "630:         Map<ResourcePattern, List<AccessControlEntry>> aclsToWrite = new HashMap<>();",
      "639:             }",
      "640:         });",
      "646:         });",
      "648:         aclsToWrite.forEach((resourcePattern, accessControlEntries) -> {",
      "652:         });",
      "653:     }",
      "",
      "[Removed Lines]",
      "615:     void handleAclsDelta(AclsImage image, AclsDelta delta, KRaftMigrationOperationConsumer operationConsumer) {",
      "617:         Set<ResourcePattern> resourcesWithChangedAcls = delta.changes().values()",
      "618:             .stream()",
      "619:             .filter(Optional::isPresent)",
      "620:             .map(Optional::get)",
      "621:             .map(this::resourcePatternFromAcl)",
      "622:             .collect(Collectors.toSet());",
      "624:         Set<ResourcePattern> resourcesWithDeletedAcls = delta.deleted()",
      "625:             .stream()",
      "626:             .map(this::resourcePatternFromAcl)",
      "627:             .collect(Collectors.toSet());",
      "631:         image.acls().forEach((uuid, standardAcl) -> {",
      "632:             ResourcePattern resourcePattern = resourcePatternFromAcl(standardAcl);",
      "633:             boolean removed = resourcesWithDeletedAcls.remove(resourcePattern);",
      "635:             if (resourcesWithChangedAcls.contains(resourcePattern) || removed) {",
      "636:                 aclsToWrite.computeIfAbsent(resourcePattern, __ -> new ArrayList<>()).add(",
      "637:                     new AccessControlEntry(standardAcl.principal(), standardAcl.host(), standardAcl.operation(), standardAcl.permissionType())",
      "638:                 );",
      "642:         resourcesWithDeletedAcls.forEach(deletedResource -> {",
      "643:             String name = \"Deleting resource \" + deletedResource + \" which has no more ACLs\";",
      "644:             operationConsumer.accept(DELETE_ACL, name, migrationState ->",
      "645:                 migrationClient.aclClient().deleteResource(deletedResource, migrationState));",
      "649:             String name = \"Writing \" + accessControlEntries.size() + \" for resource \" + resourcePattern;",
      "650:             operationConsumer.accept(UPDATE_ACL, name, migrationState ->",
      "651:                 migrationClient.aclClient().writeResourceAcls(resourcePattern, accessControlEntries, migrationState));",
      "",
      "[Added Lines]",
      "619:     void handleAclsDelta(AclsImage prevImage, AclsImage image, AclsDelta delta, KRaftMigrationOperationConsumer operationConsumer) {",
      "622:         delta.changes().forEach((aclId, aclChange) -> {",
      "623:             if (aclChange.isPresent()) {",
      "624:                 ResourcePattern resourcePattern = resourcePatternFromAcl(aclChange.get());",
      "625:                 aclsToWrite.put(resourcePattern, new ArrayList<>());",
      "626:             } else {",
      "628:                 StandardAcl deletedAcl = prevImage.acls().get(aclId);",
      "629:                 if (deletedAcl == null) {",
      "630:                     errorLogger.accept(\"Cannot delete ACL \" + aclId + \" from ZK since it is missing from previous AclImage\");",
      "631:                 } else {",
      "632:                     ResourcePattern resourcePattern = resourcePatternFromAcl(deletedAcl);",
      "633:                     aclsToWrite.put(resourcePattern, new ArrayList<>());",
      "634:                 }",
      "639:         image.acls().forEach((uuid, standardAcl) -> {",
      "640:             ResourcePattern resourcePattern = resourcePatternFromAcl(standardAcl);",
      "641:             List<AccessControlEntry> entries = aclsToWrite.get(resourcePattern);",
      "642:             if (entries != null) {",
      "643:                 entries.add(new AccessControlEntry(standardAcl.principal(), standardAcl.host(), standardAcl.operation(), standardAcl.permissionType()));",
      "644:             }",
      "649:             if (accessControlEntries.isEmpty()) {",
      "650:                 String name = \"Deleting resource \" + resourcePattern + \" which has no more ACLs\";",
      "651:                 operationConsumer.accept(DELETE_ACL, name, migrationState ->",
      "652:                     migrationClient.aclClient().deleteResource(resourcePattern, migrationState));",
      "653:             } else {",
      "654:                 String name = \"Writing \" + accessControlEntries.size() + \" for resource \" + resourcePattern;",
      "655:                 operationConsumer.accept(UPDATE_ACL, name, migrationState ->",
      "656:                     migrationClient.aclClient().writeResourceAcls(resourcePattern, accessControlEntries, migrationState));",
      "657:             }",
      "",
      "---------------"
    ],
    "metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java||metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java": [
      "File: metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java -> metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "80:             .setConfigMigrationClient(configClient)",
      "81:             .build();",
      "85:         MetadataImage image = new MetadataImage(",
      "86:             MetadataProvenance.EMPTY,",
      "",
      "[Removed Lines]",
      "83:         KRaftMigrationZkWriter writer = new KRaftMigrationZkWriter(migrationClient);",
      "",
      "[Added Lines]",
      "83:         KRaftMigrationZkWriter writer = new KRaftMigrationZkWriter(migrationClient, __ -> { });",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "120:             .setAclMigrationClient(aclClient)",
      "121:             .build();",
      "125:         MetadataImage image = new MetadataImage(",
      "126:             MetadataProvenance.EMPTY,",
      "",
      "[Removed Lines]",
      "123:         KRaftMigrationZkWriter writer = new KRaftMigrationZkWriter(migrationClient);",
      "",
      "[Added Lines]",
      "123:         KRaftMigrationZkWriter writer = new KRaftMigrationZkWriter(migrationClient, __ -> { });",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "179:             .setAclMigrationClient(aclClient)",
      "180:             .build();",
      "184:         MetadataImage image = new MetadataImage(",
      "185:             MetadataProvenance.EMPTY,",
      "",
      "[Removed Lines]",
      "182:         KRaftMigrationZkWriter writer = new KRaftMigrationZkWriter(migrationClient);",
      "",
      "[Added Lines]",
      "182:         KRaftMigrationZkWriter writer = new KRaftMigrationZkWriter(migrationClient, __ -> { });",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "ea6ce3bf82e72565b53db6df7cd1e583ea132eb9",
      "candidate_info": {
        "commit_hash": "ea6ce3bf82e72565b53db6df7cd1e583ea132eb9",
        "repo": "apache/kafka",
        "commit_url": "https://github.com/apache/kafka/commit/ea6ce3bf82e72565b53db6df7cd1e583ea132eb9",
        "files": [
          "core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java"
        ],
        "message": "KAFKA-15009: Handle new ACLs in KRaft snapshot during migration (#13741)\n\nWhen loading a snapshot during dual-write mode, we were missing the logic to detect new ACLs that \nhad been added on the KRaft side. This patch adds support for finding those new ACLs as well as tests\nto verify the correct behavior.\n\nReviewers: David Arthur <mumrah@gmail.com>",
        "before_after_code_files": [
          "core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala",
            "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java"
          ],
          "candidate": [
            "core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala",
            "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java"
          ]
        }
      },
      "candidate_diff": {
        "core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala": [
          "File: core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala -> core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: package kafka.zk.migration",
          "20: import kafka.security.authorizer.AclEntry.{WildcardHost, WildcardPrincipalString}",
          "21: import kafka.utils.TestUtils",
          "22: import org.apache.kafka.common.acl._",
          "23: import org.apache.kafka.common.metadata.AccessControlEntryRecord",
          "24: import org.apache.kafka.common.resource.{PatternType, ResourcePattern, ResourcePatternFilter, ResourceType}",
          "25: import org.apache.kafka.common.security.auth.KafkaPrincipal",
          "26: import org.apache.kafka.common.utils.SecurityUtils",
          "27: import org.apache.kafka.server.common.ApiMessageAndVersion",
          "28: import org.junit.jupiter.api.Assertions.{assertEquals, assertTrue}",
          "29: import org.junit.jupiter.api.Test",
          "32: import scala.collection.mutable",
          "33: import scala.jdk.CollectionConverters._",
          "",
          "[Removed Lines]",
          "19: import kafka.security.authorizer.AclAuthorizer",
          "31: import java.util.UUID",
          "",
          "[Added Lines]",
          "19: import kafka.security.authorizer.{AclAuthorizer, AclEntry}",
          "22: import org.apache.kafka.common.Uuid",
          "28: import org.apache.kafka.image.{MetadataDelta, MetadataImage, MetadataProvenance}",
          "29: import org.apache.kafka.metadata.migration.KRaftMigrationZkWriter",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "74:   @Test",
          "75:   def testAclsMigrateAndDualWrite(): Unit = {",
          "79:     val username = \"alice\"",
          "80:     val principal = new KafkaPrincipal(KafkaPrincipal.USER_TYPE, username)",
          "81:     val wildcardPrincipal = SecurityUtils.parseKafkaPrincipal(WildcardPrincipalString)",
          "",
          "[Removed Lines]",
          "76:     val resource1 = new ResourcePattern(ResourceType.TOPIC, \"foo-\" + UUID.randomUUID(), PatternType.LITERAL)",
          "77:     val resource2 = new ResourcePattern(ResourceType.TOPIC, \"bar-\" + UUID.randomUUID(), PatternType.LITERAL)",
          "78:     val prefixedResource = new ResourcePattern(ResourceType.TOPIC, \"bar-\", PatternType.PREFIXED)",
          "",
          "[Added Lines]",
          "78:     val resource1 = new ResourcePattern(ResourceType.TOPIC, \"foo-\" + Uuid.randomUuid(), PatternType.LITERAL)",
          "79:     val resource2 = new ResourcePattern(ResourceType.TOPIC, \"bar-\" + Uuid.randomUuid(), PatternType.LITERAL)",
          "80:     val prefixedResource = new ResourcePattern(ResourceType.TOPIC, \"bar-\" + Uuid.randomUuid(), PatternType.PREFIXED)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "115:       authorizer.close()",
          "116:     }",
          "117:   }",
          "118: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "122:   @Test",
          "123:   def testAclsChangesInSnapshot(): Unit = {",
          "125:     val resource1 = new ResourcePattern(ResourceType.TOPIC, \"foo-\" + Uuid.randomUuid(), PatternType.LITERAL)",
          "126:     val resource2 = new ResourcePattern(ResourceType.TOPIC, \"bar-\" + Uuid.randomUuid(), PatternType.LITERAL)",
          "127:     val resource3 = new ResourcePattern(ResourceType.TOPIC, \"baz-\" + Uuid.randomUuid(), PatternType.LITERAL)",
          "128:     val username1 = \"alice\"",
          "129:     val username2 = \"blah\"",
          "130:     val principal1 = new KafkaPrincipal(KafkaPrincipal.USER_TYPE, username1)",
          "131:     val principal2 = new KafkaPrincipal(KafkaPrincipal.USER_TYPE, username2)",
          "132:     val acl1Resource1 = new AclEntry(new AccessControlEntry(principal1.toString, WildcardHost, AclOperation.WRITE, AclPermissionType.ALLOW))",
          "133:     val acl1Resource2 = new AclEntry(new AccessControlEntry(principal2.toString, WildcardHost, AclOperation.READ, AclPermissionType.ALLOW))",
          "135:     zkClient.createAclPaths()",
          "136:     zkClient.createAclsForResourceIfNotExists(resource1, Set(acl1Resource1))",
          "137:     zkClient.createAclsForResourceIfNotExists(resource2, Set(acl1Resource2))",
          "143:     val delta = new MetadataDelta(MetadataImage.EMPTY)",
          "144:     val acl1Resource3 = new AccessControlEntryRecord()",
          "145:       .setId(Uuid.randomUuid())",
          "146:       .setHost(\"192.168.10.1\")",
          "147:       .setOperation(AclOperation.READ.code())",
          "148:       .setPrincipal(WildcardPrincipalString)",
          "149:       .setPermissionType(AclPermissionType.ALLOW.code())",
          "150:       .setPatternType(resource3.patternType().code())",
          "151:       .setResourceName(resource3.name())",
          "152:       .setResourceType(resource3.resourceType().code()",
          "153:       )",
          "154:     delta.replay(acl1Resource3)",
          "157:     val acl2Resource1 = new AccessControlEntryRecord()",
          "158:       .setId(Uuid.randomUuid())",
          "159:       .setHost(\"192.168.15.1\")",
          "160:       .setOperation(AclOperation.WRITE.code())",
          "161:       .setPrincipal(principal1.toString)",
          "162:       .setPermissionType(AclPermissionType.ALLOW.code())",
          "163:       .setPatternType(resource1.patternType().code())",
          "164:       .setResourceName(resource1.name())",
          "165:       .setResourceType(resource1.resourceType().code()",
          "166:       )",
          "167:     delta.replay(acl2Resource1)",
          "170:     val image = delta.apply(MetadataProvenance.EMPTY)",
          "173:     val kraftMigrationZkWriter = new KRaftMigrationZkWriter(migrationClient,",
          "174:       (_, operation) => { migrationState = operation.apply(migrationState) })",
          "175:     kraftMigrationZkWriter.handleLoadSnapshot(image)",
          "178:     val resource1AclsInZk = zkClient.getVersionedAclsForResource(resource1).acls",
          "179:     assertEquals(1, resource1AclsInZk.size)",
          "180:     assertEquals(",
          "181:       new AccessControlEntry(acl2Resource1.principal(), acl2Resource1.host(),",
          "182:         AclOperation.fromCode(acl2Resource1.operation()),",
          "183:         AclPermissionType.fromCode(acl2Resource1.permissionType())),",
          "184:       resource1AclsInZk.head.ace)",
          "185:     val resource2AclsInZk = zkClient.getVersionedAclsForResource(resource2).acls",
          "186:     assertTrue(resource2AclsInZk.isEmpty)",
          "187:     val resource3AclsInZk = zkClient.getVersionedAclsForResource(resource3).acls",
          "188:     assertEquals(",
          "189:       new AccessControlEntry(acl1Resource3.principal(), acl1Resource3.host(),",
          "190:         AclOperation.fromCode(acl1Resource3.operation()),",
          "191:         AclPermissionType.fromCode(acl1Resource3.permissionType())),",
          "192:       resource3AclsInZk.head.ace)",
          "193:   }",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java": [
          "File: metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "350:             );",
          "351:         });",
          "353:         Set<ResourcePattern> resourcesToDelete = new HashSet<>();",
          "354:         Map<ResourcePattern, Set<AccessControlEntry>> changedResources = new HashMap<>();",
          "355:         migrationClient.aclClient().iterateAcls((resourcePattern, accessControlEntries) -> {",
          "356:             if (!allAclsInSnapshot.containsKey(resourcePattern)) {",
          "357:                 resourcesToDelete.add(resourcePattern);",
          "358:             } else {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "353:         Set<ResourcePattern> newResources = new HashSet<>(allAclsInSnapshot.keySet());",
          "357:             newResources.remove(resourcePattern);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "363:             }",
          "364:         });",
          "366:         resourcesToDelete.forEach(deletedResource -> {",
          "367:             String name = \"Deleting resource \" + deletedResource + \" which has no ACLs in snapshot\";",
          "368:             operationConsumer.accept(name, migrationState ->",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "368:         newResources.forEach(resourcePattern -> {",
          "369:             Set<AccessControlEntry> accessControlEntries = allAclsInSnapshot.get(resourcePattern);",
          "370:             String name = \"Writing \" + accessControlEntries.size() + \" for resource \" + resourcePattern;",
          "371:             operationConsumer.accept(name, migrationState ->",
          "372:                 migrationClient.aclClient().writeResourceAcls(resourcePattern, accessControlEntries, migrationState));",
          "373:         });",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c1b5c75d9271638776392822a094e9e7ef37f490",
      "candidate_info": {
        "commit_hash": "c1b5c75d9271638776392822a094e9e7ef37f490",
        "repo": "apache/kafka",
        "commit_url": "https://github.com/apache/kafka/commit/c1b5c75d9271638776392822a094e9e7ef37f490",
        "files": [
          "checkstyle/suppressions.xml",
          "core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java",
          "core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala",
          "core/src/test/scala/unit/kafka/server/BrokerRegistrationRequestTest.scala",
          "metadata/src/main/java/org/apache/kafka/controller/ClusterControlManager.java",
          "metadata/src/main/java/org/apache/kafka/controller/FeatureControlManager.java",
          "metadata/src/main/java/org/apache/kafka/controller/MigrationControlManager.java",
          "metadata/src/main/java/org/apache/kafka/controller/ProducerIdControlManager.java",
          "metadata/src/main/java/org/apache/kafka/controller/QuorumController.java",
          "metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java",
          "metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetrics.java",
          "metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsPublisher.java",
          "metadata/src/main/java/org/apache/kafka/image/FeaturesDelta.java",
          "metadata/src/main/java/org/apache/kafka/image/FeaturesImage.java",
          "metadata/src/main/java/org/apache/kafka/image/MetadataDelta.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/ZkMigrationState.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/ZkRecordConsumer.java",
          "metadata/src/main/resources/common/metadata/ZkMigrationRecord.json",
          "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
          "metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java",
          "metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsTest.java",
          "metadata/src/test/java/org/apache/kafka/image/FeaturesImageTest.java",
          "metadata/src/test/java/org/apache/kafka/image/publisher/SnapshotGeneratorTest.java",
          "metadata/src/test/java/org/apache/kafka/metadata/RecordTestUtils.java",
          "metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationDriverTest.java",
          "tests/docker/Dockerfile",
          "tests/kafkatest/tests/core/zookeeper_migration_test.py"
        ],
        "message": "KAFKA-14805 KRaft controller supports pre-migration mode (#13407)\n\nThis patch adds the concept of pre-migration mode to the KRaft controller. While in this mode, \nthe controller will only allow certain write operations. The purpose of this is to disallow metadata \nchanges when the controller is waiting for the ZK migration records to be committed.\n\nThe following ControllerWriteEvent operations are permitted in pre-migration mode\n\n* completeActivation\n* maybeFenceReplicas\n* writeNoOpRecord\n* processBrokerHeartbeat\n* registerBroker (only for migrating ZK brokers)\n* unregisterBroker\n\nRaft events and other controller events do not follow the same code path as ControllerWriteEvent, \nso they are not affected by this new behavior.\n\nThis patch also add a new metric as defined in KIP-868: kafka.controller:type=KafkaController,name=ZkMigrationState\n\nIn order to support upgrades from 3.4.0, this patch also redefines the enum value of value 1 to mean \nMIGRATION rather than PRE_MIGRATION.\n\nReviewers: Chia-Ping Tsai <chia7712@gmail.com>, Colin P. McCabe <cmccabe@apache.org>",
        "before_after_code_files": [
          "core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java||core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java",
          "core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala||core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala",
          "core/src/test/scala/unit/kafka/server/BrokerRegistrationRequestTest.scala||core/src/test/scala/unit/kafka/server/BrokerRegistrationRequestTest.scala",
          "metadata/src/main/java/org/apache/kafka/controller/ClusterControlManager.java||metadata/src/main/java/org/apache/kafka/controller/ClusterControlManager.java",
          "metadata/src/main/java/org/apache/kafka/controller/FeatureControlManager.java||metadata/src/main/java/org/apache/kafka/controller/FeatureControlManager.java",
          "metadata/src/main/java/org/apache/kafka/controller/MigrationControlManager.java||metadata/src/main/java/org/apache/kafka/controller/MigrationControlManager.java",
          "metadata/src/main/java/org/apache/kafka/controller/ProducerIdControlManager.java||metadata/src/main/java/org/apache/kafka/controller/ProducerIdControlManager.java",
          "metadata/src/main/java/org/apache/kafka/controller/QuorumController.java||metadata/src/main/java/org/apache/kafka/controller/QuorumController.java",
          "metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java||metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java",
          "metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetrics.java||metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetrics.java",
          "metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsPublisher.java||metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsPublisher.java",
          "metadata/src/main/java/org/apache/kafka/image/FeaturesDelta.java||metadata/src/main/java/org/apache/kafka/image/FeaturesDelta.java",
          "metadata/src/main/java/org/apache/kafka/image/FeaturesImage.java||metadata/src/main/java/org/apache/kafka/image/FeaturesImage.java",
          "metadata/src/main/java/org/apache/kafka/image/MetadataDelta.java||metadata/src/main/java/org/apache/kafka/image/MetadataDelta.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/ZkMigrationState.java||metadata/src/main/java/org/apache/kafka/metadata/migration/ZkMigrationState.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/ZkRecordConsumer.java||metadata/src/main/java/org/apache/kafka/metadata/migration/ZkRecordConsumer.java",
          "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java||metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
          "metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java||metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java",
          "metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsTest.java||metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsTest.java",
          "metadata/src/test/java/org/apache/kafka/image/FeaturesImageTest.java||metadata/src/test/java/org/apache/kafka/image/FeaturesImageTest.java",
          "metadata/src/test/java/org/apache/kafka/image/publisher/SnapshotGeneratorTest.java||metadata/src/test/java/org/apache/kafka/image/publisher/SnapshotGeneratorTest.java",
          "metadata/src/test/java/org/apache/kafka/metadata/RecordTestUtils.java||metadata/src/test/java/org/apache/kafka/metadata/RecordTestUtils.java",
          "metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationDriverTest.java||metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationDriverTest.java",
          "tests/kafkatest/tests/core/zookeeper_migration_test.py||tests/kafkatest/tests/core/zookeeper_migration_test.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java"
          ],
          "candidate": [
            "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java"
          ]
        }
      },
      "candidate_diff": {
        "core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java||core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java": [
          "File: core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java -> core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "105:                     zkReference.set(new EmbeddedZookeeper());",
          "106:                     builder.setConfigProp(\"zookeeper.connect\", String.format(\"localhost:%d\", zkReference.get().port()));",
          "107:                 }",
          "110:                 clusterConfig.serverProperties().forEach((key, value) -> builder.setConfigProp(key.toString(), value.toString()));",
          "112:                 KafkaClusterTestKit cluster = builder.build();",
          "113:                 clusterReference.set(cluster);",
          "114:                 cluster.format();",
          "121:             },",
          "122:             (AfterTestExecutionCallback) context -> clusterInstance.stop(),",
          "123:             new ClusterInstanceParameterResolver(clusterInstance),",
          "",
          "[Removed Lines]",
          "115:                 cluster.startup();",
          "116:                 kafka.utils.TestUtils.waitUntilTrue(",
          "117:                     () -> cluster.brokers().get(0).brokerState() == BrokerState.RUNNING,",
          "118:                     () -> \"Broker never made it to RUNNING state.\",",
          "119:                     org.apache.kafka.test.TestUtils.DEFAULT_MAX_WAIT_MS,",
          "120:                     100L);",
          "",
          "[Added Lines]",
          "114:                 if (clusterConfig.isAutoStart()) {",
          "115:                     cluster.startup();",
          "116:                     kafka.utils.TestUtils.waitUntilTrue(",
          "117:                         () -> cluster.brokers().get(0).brokerState() == BrokerState.RUNNING,",
          "118:                         () -> \"Broker never made it to RUNNING state.\",",
          "119:                         org.apache.kafka.test.TestUtils.DEFAULT_MAX_WAIT_MS,",
          "120:                         100L);",
          "121:                 }",
          "",
          "---------------"
        ],
        "core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala||core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala": [
          "File: core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala -> core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "44: import org.apache.kafka.raft.RaftConfig",
          "45: import org.apache.kafka.server.common.{ApiMessageAndVersion, MetadataVersion, ProducerIdsBlock}",
          "46: import org.junit.jupiter.api.Assertions.{assertEquals, assertFalse, assertNotNull, assertTrue}",
          "48: import org.junit.jupiter.api.extension.ExtendWith",
          "49: import org.slf4j.LoggerFactory",
          "",
          "[Removed Lines]",
          "47: import org.junit.jupiter.api.{Disabled, Timeout}",
          "",
          "[Added Lines]",
          "47: import org.junit.jupiter.api.Timeout",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "182:     migrationState = migrationClient.releaseControllerLeadership(migrationState)",
          "183:   }",
          "186:   @ClusterTest(clusterType = Type.ZK, brokers = 3, metadataVersion = MetadataVersion.IBP_3_4_IV0, serverProperties = Array(",
          "187:     new ClusterConfigProperty(key = \"inter.broker.listener.name\", value = \"EXTERNAL\"),",
          "188:     new ClusterConfigProperty(key = \"listeners\", value = \"PLAINTEXT://localhost:0,EXTERNAL://localhost:0\"),",
          "",
          "[Removed Lines]",
          "185:   @Disabled(\"Will be fixed by KAFKA-14840\")",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "core/src/test/scala/unit/kafka/server/BrokerRegistrationRequestTest.scala||core/src/test/scala/unit/kafka/server/BrokerRegistrationRequestTest.scala": [
          "File: core/src/test/scala/unit/kafka/server/BrokerRegistrationRequestTest.scala -> core/src/test/scala/unit/kafka/server/BrokerRegistrationRequestTest.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import kafka.test.ClusterInstance",
          "23: import kafka.test.junit.ClusterTestExtensions",
          "24: import org.apache.kafka.clients.ClientResponse",
          "26: import org.apache.kafka.common.metrics.Metrics",
          "27: import org.apache.kafka.common.network.ListenerName",
          "28: import org.apache.kafka.common.protocol.Errors",
          "30: import org.apache.kafka.common.security.auth.SecurityProtocol",
          "31: import org.apache.kafka.common.utils.Time",
          "32: import org.apache.kafka.common.{Node, Uuid}",
          "33: import org.apache.kafka.server.common.MetadataVersion",
          "35: import org.junit.jupiter.api.extension.ExtendWith",
          "36: import org.junit.jupiter.api.{Tag, Timeout}",
          "",
          "[Removed Lines]",
          "18: package unit.kafka.server",
          "20: import kafka.server.{BrokerToControllerChannelManager, ControllerInformation, ControllerNodeProvider, ControllerRequestCompletionHandler}",
          "22: import kafka.test.annotation.{ClusterConfigProperty, ClusterTest, Type}",
          "25: import org.apache.kafka.common.message.{BrokerRegistrationRequestData, BrokerRegistrationResponseData}",
          "29: import org.apache.kafka.common.requests.{BrokerRegistrationRequest, BrokerRegistrationResponse}",
          "34: import org.junit.jupiter.api.Assertions.assertEquals",
          "",
          "[Added Lines]",
          "18: package kafka.server",
          "21: import kafka.test.annotation._",
          "23: import kafka.test.junit.RaftClusterInvocationContext.RaftClusterInstance",
          "25: import org.apache.kafka.common.message.CreateTopicsRequestData.CreatableTopic",
          "26: import org.apache.kafka.common.message.{BrokerRegistrationRequestData, CreateTopicsRequestData}",
          "30: import org.apache.kafka.common.requests._",
          "35: import org.junit.jupiter.api.Assertions.{assertEquals, assertThrows}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "74:     )",
          "75:   }",
          "78:     channelManager: BrokerToControllerChannelManager,",
          "83:       override def onTimeout(): Unit = responseFuture.completeExceptionally(new TimeoutException())",
          "85:       override def onComplete(response: ClientResponse): Unit =",
          "87:     })",
          "89:   }",
          "91:   def registerBroker(",
          "",
          "[Removed Lines]",
          "77:   def sendAndRecieve(",
          "79:     req: BrokerRegistrationRequestData",
          "80:   ): BrokerRegistrationResponseData = {",
          "81:     val responseFuture = new CompletableFuture[BrokerRegistrationResponseData]()",
          "82:     channelManager.sendRequest(new BrokerRegistrationRequest.Builder(req), new ControllerRequestCompletionHandler() {",
          "86:         responseFuture.complete(response.responseBody().asInstanceOf[BrokerRegistrationResponse].data())",
          "88:     responseFuture.get(30, TimeUnit.SECONDS)",
          "",
          "[Added Lines]",
          "78:   def sendAndReceive[T <: AbstractRequest, R <: AbstractResponse](",
          "80:     reqBuilder: AbstractRequest.Builder[T],",
          "81:     timeoutMs: Int",
          "82:   ): R = {",
          "83:     val responseFuture = new CompletableFuture[R]()",
          "84:     channelManager.sendRequest(reqBuilder, new ControllerRequestCompletionHandler() {",
          "88:         responseFuture.complete(response.responseBody().asInstanceOf[R])",
          "90:     responseFuture.get(timeoutMs, TimeUnit.MILLISECONDS)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "113:       .setIsMigratingZkBroker(zkEpoch.isDefined)",
          "114:       .setFeatures(features)",
          "117:   }",
          "119:   @ClusterTest(clusterType = Type.KRAFT, brokers = 0, controllers = 1, metadataVersion = MetadataVersion.IBP_3_4_IV0,",
          "",
          "[Removed Lines]",
          "116:     Errors.forCode(sendAndRecieve(channelManager, req).errorCode())",
          "",
          "[Added Lines]",
          "118:     val resp = sendAndReceive[BrokerRegistrationRequest, BrokerRegistrationResponse](",
          "119:       channelManager, new BrokerRegistrationRequest.Builder(req), 30000)",
          "120:     Errors.forCode(resp.data().errorCode())",
          "121:   }",
          "124:   def createTopics(channelManager: BrokerToControllerChannelManager,",
          "125:                    topicName: String): Errors = {",
          "126:     val createTopics = new CreateTopicsRequestData()",
          "127:     createTopics.setTopics(new CreateTopicsRequestData.CreatableTopicCollection())",
          "128:     createTopics.topics().add(new CreatableTopic().setName(topicName).setNumPartitions(10).setReplicationFactor(1))",
          "129:     createTopics.setTimeoutMs(500)",
          "131:     val req = new CreateTopicsRequest.Builder(createTopics)",
          "132:     val resp = sendAndReceive[CreateTopicsRequest, CreateTopicsResponse](channelManager, req, 3000).data()",
          "133:     Errors.forCode(resp.topics().find(topicName).errorCode())",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "145:   }",
          "147:   @ClusterTest(clusterType = Type.KRAFT, brokers = 0, controllers = 1, metadataVersion = MetadataVersion.IBP_3_3_IV3,",
          "150:     val clusterId = clusterInstance.clusterId()",
          "151:     val channelManager = brokerToControllerChannelManager(clusterInstance)",
          "152:     try {",
          "153:       channelManager.start()",
          "155:       assertEquals(",
          "156:         Errors.BROKER_ID_NOT_REGISTERED,",
          "159:       assertEquals(",
          "160:         Errors.BROKER_ID_NOT_REGISTERED,",
          "161:         registerBroker(channelManager, clusterId, 100, Some(1), None))",
          "163:       assertEquals(",
          "164:         Errors.BROKER_ID_NOT_REGISTERED,",
          "165:         registerBroker(channelManager, clusterId, 100, Some(1), Some((MetadataVersion.IBP_3_4_IV0, MetadataVersion.IBP_3_4_IV0))))",
          "167:       assertEquals(",
          "168:         Errors.NONE,",
          "169:         registerBroker(channelManager, clusterId, 100, None, Some((MetadataVersion.IBP_3_3_IV3, MetadataVersion.IBP_3_4_IV0))))",
          "",
          "[Removed Lines]",
          "148:     serverProperties = Array(new ClusterConfigProperty(key = \"zookeeper.metadata.migration.enable\", value = \"true\")))",
          "149:   def testRegisterZkWithKRaftOldMetadataVersion(clusterInstance: ClusterInstance): Unit = {",
          "157:         registerBroker(channelManager, clusterId, 100, Some(1), Some((MetadataVersion.IBP_3_3_IV0, MetadataVersion.IBP_3_3_IV0))))",
          "",
          "[Added Lines]",
          "165:     serverProperties = Array(new ClusterConfigProperty(key = \"zookeeper.metadata.migration.enable\", value = \"false\")))",
          "166:   def testRegisterZkWith33Controller(clusterInstance: ClusterInstance): Unit = {",
          "175:         registerBroker(channelManager, clusterId, 100, Some(1), Some((MetadataVersion.IBP_3_3_IV0, MetadataVersion.IBP_3_3_IV3))))",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "172:     }",
          "173:   }",
          "176:     serverProperties = Array(new ClusterConfigProperty(key = \"zookeeper.metadata.migration.enable\", value = \"true\")))",
          "177:   def testRegisterZkWithKRaftMigrationEnabled(clusterInstance: ClusterInstance): Unit = {",
          "178:     val clusterId = clusterInstance.clusterId()",
          "179:     val channelManager = brokerToControllerChannelManager(clusterInstance)",
          "180:     try {",
          "",
          "[Removed Lines]",
          "175:   @ClusterTest(clusterType = Type.KRAFT, brokers = 0, controllers = 1, metadataVersion = MetadataVersion.IBP_3_4_IV0,",
          "",
          "[Added Lines]",
          "196:   @ClusterTest(",
          "197:     clusterType = Type.KRAFT,",
          "198:     brokers = 1,",
          "199:     controllers = 1,",
          "200:     metadataVersion = MetadataVersion.IBP_3_4_IV0,",
          "201:     autoStart = AutoStart.NO,",
          "204:     clusterInstance.asInstanceOf[RaftClusterInstance].controllers().forEach(_.startup())",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "192:         Errors.UNSUPPORTED_VERSION,",
          "193:         registerBroker(channelManager, clusterId, 100, Some(1), Some((MetadataVersion.IBP_3_3_IV3, MetadataVersion.IBP_3_3_IV3))))",
          "195:       assertEquals(",
          "197:         registerBroker(channelManager, clusterId, 100, None, Some((MetadataVersion.IBP_3_4_IV0, MetadataVersion.IBP_3_4_IV0))))",
          "198:     } finally {",
          "199:       channelManager.shutdown()",
          "200:     }",
          "201:   }",
          "202: }",
          "",
          "[Removed Lines]",
          "196:         Errors.NONE,",
          "",
          "[Added Lines]",
          "225:         Errors.BROKER_ID_NOT_REGISTERED,",
          "236:   @ClusterTests(Array(",
          "237:     new ClusterTest(clusterType = Type.KRAFT, autoStart = AutoStart.NO, controllers = 1, metadataVersion = MetadataVersion.IBP_3_4_IV0,",
          "238:       serverProperties = Array(new ClusterConfigProperty(key = \"zookeeper.metadata.migration.enable\", value = \"true\")))",
          "239:   ))",
          "240:   def testNoMetadataChangesInPreMigrationMode(clusterInstance: ClusterInstance): Unit = {",
          "241:     clusterInstance.asInstanceOf[RaftClusterInstance].controllers().forEach(_.startup())",
          "243:     val channelManager = brokerToControllerChannelManager(clusterInstance)",
          "244:     try {",
          "245:       channelManager.start()",
          "246:       assertThrows(classOf[TimeoutException], () => createTopics(channelManager, \"test-pre-migration\"))",
          "247:     } finally {",
          "248:       channelManager.shutdown()",
          "249:     }",
          "250:   }",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/controller/ClusterControlManager.java||metadata/src/main/java/org/apache/kafka/controller/ClusterControlManager.java": [
          "File: metadata/src/main/java/org/apache/kafka/controller/ClusterControlManager.java -> metadata/src/main/java/org/apache/kafka/controller/ClusterControlManager.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "340:             throw new BrokerIdNotRegisteredException(\"Controller does not support registering ZK brokers.\");",
          "341:         }",
          "343:         RegisterBrokerRecord record = new RegisterBrokerRecord().",
          "344:             setBrokerId(brokerId).",
          "345:             setIsMigratingZkBroker(request.isMigratingZkBroker()).",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "343:         if (!request.isMigratingZkBroker() && featureControl.inPreMigrationMode()) {",
          "344:             throw new BrokerIdNotRegisteredException(\"Controller is in pre-migration mode and cannot register KRaft \" +",
          "345:                 \"brokers until the metadata migration is complete.\");",
          "346:         }",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/controller/FeatureControlManager.java||metadata/src/main/java/org/apache/kafka/controller/FeatureControlManager.java": [
          "File: metadata/src/main/java/org/apache/kafka/controller/FeatureControlManager.java -> metadata/src/main/java/org/apache/kafka/controller/FeatureControlManager.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: import org.apache.kafka.clients.ApiVersions;",
          "31: import org.apache.kafka.clients.admin.FeatureUpdate;",
          "32: import org.apache.kafka.common.metadata.FeatureLevelRecord;",
          "33: import org.apache.kafka.common.protocol.Errors;",
          "34: import org.apache.kafka.common.requests.ApiError;",
          "35: import org.apache.kafka.common.utils.LogContext;",
          "36: import org.apache.kafka.metadata.FinalizedControllerFeatures;",
          "37: import org.apache.kafka.metadata.VersionRange;",
          "38: import org.apache.kafka.server.common.ApiMessageAndVersion;",
          "39: import org.apache.kafka.server.common.MetadataVersion;",
          "40: import org.apache.kafka.timeline.SnapshotRegistry;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33: import org.apache.kafka.common.metadata.ZkMigrationStateRecord;",
          "39: import org.apache.kafka.metadata.migration.ZkMigrationState;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "85:                 quorumFeatures = new QuorumFeatures(0, new ApiVersions(), QuorumFeatures.defaultFeatureMap(),",
          "86:                         Collections.emptyList());",
          "87:             }",
          "89:                 quorumFeatures,",
          "90:                 snapshotRegistry,",
          "91:                 metadataVersion,",
          "93:         }",
          "94:     }",
          "",
          "[Removed Lines]",
          "88:             return new FeatureControlManager(logContext,",
          "92:                 minimumBootstrapVersion);",
          "",
          "[Added Lines]",
          "91:             return new FeatureControlManager(",
          "92:                 logContext,",
          "96:                 minimumBootstrapVersion",
          "97:             );",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "111:     private final TimelineObject<MetadataVersion> metadataVersion;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "121:     private final TimelineObject<ZkMigrationState> migrationControlState;",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "127:         this.finalizedVersions = new TimelineHashMap<>(snapshotRegistry, 0);",
          "128:         this.metadataVersion = new TimelineObject<>(snapshotRegistry, metadataVersion);",
          "129:         this.minimumBootstrapVersion = minimumBootstrapVersion;",
          "130:     }",
          "132:     ControllerResult<Map<String, ApiError>> updateFeatures(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "140:         this.migrationControlState = new TimelineObject<>(snapshotRegistry, ZkMigrationState.NONE);",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "153:         return metadataVersion.get();",
          "154:     }",
          "156:     private ApiError updateFeature(",
          "157:         String featureName,",
          "158:         short newVersion,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "167:     ZkMigrationState zkMigrationState() {",
          "168:         return migrationControlState.get();",
          "169:     }",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "232:         Consumer<ApiMessageAndVersion> recordConsumer",
          "233:     ) {",
          "234:         MetadataVersion currentVersion = metadataVersion();",
          "235:         final MetadataVersion newVersion;",
          "236:         try {",
          "237:             newVersion = MetadataVersion.fromFeatureLevel(newVersionLevel);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "250:         ZkMigrationState zkMigrationState = zkMigrationState();",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "239:             return invalidMetadataVersion(newVersionLevel, \"Unknown metadata.version.\");",
          "240:         }",
          "244:         if (newVersion.isLessThan(minimumBootstrapVersion)) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "259:         if (zkMigrationState.inProgress()) {",
          "260:             return invalidMetadataVersion(newVersionLevel, \"Unable to modify metadata.version while a \" +",
          "261:                 \"ZK migration is in progress.\");",
          "262:         }",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "284:         return new FinalizedControllerFeatures(features, epoch);",
          "285:     }",
          "287:     public void replay(FeatureLevelRecord record) {",
          "288:         VersionRange range = quorumFeatures.localSupportedFeature(record.name());",
          "289:         if (!range.contains(record.featureLevel())) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "316:     boolean inPreMigrationMode() {",
          "317:         return migrationControlState.get().equals(ZkMigrationState.PRE_MIGRATION);",
          "318:     }",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "304:         }",
          "305:     }",
          "307:     boolean isControllerId(int nodeId) {",
          "308:         return quorumFeatures.isControllerId(nodeId);",
          "309:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "340:     public void replay(ZkMigrationStateRecord record) {",
          "341:         ZkMigrationState recordState = ZkMigrationState.of(record.zkMigrationState());",
          "342:         ZkMigrationState currentState = migrationControlState.get();",
          "343:         log.info(\"Transitioning ZK migration state from {} to {}\", currentState, recordState);",
          "344:         migrationControlState.set(recordState);",
          "345:     }",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/controller/MigrationControlManager.java||metadata/src/main/java/org/apache/kafka/controller/MigrationControlManager.java": [
          "File: metadata/src/main/java/org/apache/kafka/controller/MigrationControlManager.java -> metadata/src/main/java/org/apache/kafka/controller/MigrationControlManager.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/controller/ProducerIdControlManager.java||metadata/src/main/java/org/apache/kafka/controller/ProducerIdControlManager.java": [
          "File: metadata/src/main/java/org/apache/kafka/controller/ProducerIdControlManager.java -> metadata/src/main/java/org/apache/kafka/controller/ProducerIdControlManager.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "69:         ProducerIdsBlock nextBlock = nextProducerBlock.get();",
          "70:         if (nextBlock != ProducerIdsBlock.EMPTY && record.nextProducerId() <= nextBlock.firstProducerId()) {",
          "71:             throw new RuntimeException(\"Next Producer ID from replayed record (\" + record.nextProducerId() + \")\" +",
          "73:         } else {",
          "74:             nextProducerBlock.set(new ProducerIdsBlock(record.brokerId(), record.nextProducerId(), ProducerIdsBlock.PRODUCER_ID_BLOCK_SIZE));",
          "75:             brokerEpoch.set(record.brokerEpoch());",
          "",
          "[Removed Lines]",
          "72:                 \" is not greater than current next Producer ID (\" + nextBlock.firstProducerId() + \")\");",
          "",
          "[Added Lines]",
          "72:                 \" is not greater than current next Producer ID in block (\" + nextBlock + \")\");",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/controller/QuorumController.java||metadata/src/main/java/org/apache/kafka/controller/QuorumController.java": [
          "File: metadata/src/main/java/org/apache/kafka/controller/QuorumController.java -> metadata/src/main/java/org/apache/kafka/controller/QuorumController.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "113: import java.util.Arrays;",
          "114: import java.util.Collection;",
          "115: import java.util.Collections;",
          "116: import java.util.HashMap;",
          "117: import java.util.List;",
          "118: import java.util.Map.Entry;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "116: import java.util.EnumSet;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "134: import static java.util.concurrent.TimeUnit.MICROSECONDS;",
          "135: import static java.util.concurrent.TimeUnit.NANOSECONDS;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "137: import static org.apache.kafka.controller.QuorumController.ControllerOperationFlag.DOES_NOT_UPDATE_QUEUE_TIME;",
          "138: import static org.apache.kafka.controller.QuorumController.ControllerOperationFlag.RUNS_IN_PREMIGRATION;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "414:         OptionalInt latestController = raftClient.leaderAndEpoch().leaderId();",
          "415:         if (latestController.isPresent()) {",
          "416:             return new NotControllerException(ACTIVE_CONTROLLER_EXCEPTION_TEXT_PREFIX +",
          "418:         } else {",
          "419:             return new NotControllerException(\"No controller appears to be active.\");",
          "420:         }",
          "",
          "[Removed Lines]",
          "417:                 latestController.getAsInt());",
          "",
          "[Added Lines]",
          "420:                 latestController.getAsInt() + \".\");",
          "421:         } else {",
          "422:             return new NotControllerException(\"No controller appears to be active.\");",
          "423:         }",
          "424:     }",
          "426:     private NotControllerException newPreMigrationException() {",
          "427:         OptionalInt latestController = raftClient.leaderAndEpoch().leaderId();",
          "428:         if (latestController.isPresent()) {",
          "429:             return new NotControllerException(\"The controller is in pre-migration mode.\");",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "593:         return event.future();",
          "594:     }",
          "596:     interface ControllerWriteOperation<T> {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "608:     enum ControllerOperationFlag {",
          "613:         DOES_NOT_UPDATE_QUEUE_TIME,",
          "625:         RUNS_IN_PREMIGRATION",
          "626:     }",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "630:         private final CompletableFuture<T> future;",
          "631:         private final ControllerWriteOperation<T> op;",
          "632:         private final long eventCreatedTimeNs = time.nanoseconds();",
          "634:         private OptionalLong startProcessingTimeNs = OptionalLong.empty();",
          "635:         private ControllerResultAndOffset<T> resultAndOffset;",
          "642:             this.name = name;",
          "643:             this.future = new CompletableFuture<T>();",
          "644:             this.op = op;",
          "646:             this.resultAndOffset = null;",
          "647:         }",
          "",
          "[Removed Lines]",
          "633:         private final boolean deferred;",
          "637:         ControllerWriteEvent(String name, ControllerWriteOperation<T> op) {",
          "638:             this(name, op, false);",
          "639:         }",
          "641:         ControllerWriteEvent(String name, ControllerWriteOperation<T> op, boolean deferred) {",
          "645:             this.deferred = deferred;",
          "",
          "[Added Lines]",
          "665:         private final EnumSet<ControllerOperationFlag> flags;",
          "669:         ControllerWriteEvent(",
          "670:             String name,",
          "671:             ControllerWriteOperation<T> op,",
          "672:             EnumSet<ControllerOperationFlag> flags",
          "673:         ) {",
          "677:             this.flags = flags;",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "653:         @Override",
          "654:         public void run() throws Exception {",
          "655:             long now = time.nanoseconds();",
          "659:                 controllerMetrics.updateEventQueueTime(NANOSECONDS.toMillis(now - eventCreatedTimeNs));",
          "660:             }",
          "661:             int controllerEpoch = curClaimEpoch;",
          "663:                 throw newNotControllerException();",
          "664:             }",
          "665:             startProcessingTimeNs = OptionalLong.of(now);",
          "666:             ControllerResult<T> result = op.generateRecordsAndResult();",
          "667:             if (result.records().isEmpty()) {",
          "",
          "[Removed Lines]",
          "656:             if (!deferred) {",
          "662:             if (!isActiveController()) {",
          "",
          "[Added Lines]",
          "688:             if (!flags.contains(DOES_NOT_UPDATE_QUEUE_TIME)) {",
          "694:             if (!isActiveController(controllerEpoch)) {",
          "697:             if (featureControl.inPreMigrationMode() && !flags.contains(RUNS_IN_PREMIGRATION)) {",
          "698:                 log.info(\"Cannot run write operation {} in pre-migration mode. Returning NOT_CONTROLLER.\", name);",
          "699:                 throw newPreMigrationException();",
          "700:             }",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "820:         }",
          "821:     }",
          "827:         if (deadlineNs.isPresent()) {",
          "828:             queue.appendWithDeadline(deadlineNs.getAsLong(), event);",
          "829:         } else {",
          "",
          "[Removed Lines]",
          "823:     <T> CompletableFuture<T> appendWriteEvent(String name,",
          "824:                                               OptionalLong deadlineNs,",
          "825:                                               ControllerWriteOperation<T> op) {",
          "826:         ControllerWriteEvent<T> event = new ControllerWriteEvent<>(name, op);",
          "",
          "[Added Lines]",
          "859:     <T> CompletableFuture<T> appendWriteEvent(",
          "860:         String name,",
          "861:         OptionalLong deadlineNs,",
          "862:         ControllerWriteOperation<T> op",
          "863:     ) {",
          "864:         return appendWriteEvent(name, deadlineNs, op, EnumSet.noneOf(ControllerOperationFlag.class));",
          "865:     }",
          "867:     <T> CompletableFuture<T> appendWriteEvent(",
          "868:         String name,",
          "869:         OptionalLong deadlineNs,",
          "870:         ControllerWriteOperation<T> op,",
          "871:         EnumSet<ControllerOperationFlag> flags",
          "872:     ) {",
          "873:         ControllerWriteEvent<T> event = new ControllerWriteEvent<>(name, op, flags);",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "852:         }",
          "853:         @Override",
          "854:         public void beginMigration() {",
          "864:         }",
          "866:         @Override",
          "",
          "[Removed Lines]",
          "856:             ControllerWriteEvent<Void> event = new ControllerWriteEvent<>(\"Begin ZK Migration\",",
          "857:                 new MigrationWriteOperation(",
          "858:                     Collections.singletonList(",
          "859:                         new ApiMessageAndVersion(",
          "860:                             new ZkMigrationStateRecord().setZkMigrationState(ZkMigrationState.PRE_MIGRATION.value()),",
          "861:                             ZkMigrationStateRecord.LOWEST_SUPPORTED_VERSION)",
          "862:                     )));",
          "863:             queue.append(event);",
          "",
          "[Added Lines]",
          "902:             log.info(\"Starting ZK Migration\");",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "871:                 return future;",
          "872:             }",
          "873:             ControllerWriteEvent<Void> batchEvent = new ControllerWriteEvent<>(\"ZK Migration Batch\",",
          "875:             queue.append(batchEvent);",
          "876:             return batchEvent.future;",
          "877:         }",
          "879:         @Override",
          "883:         }",
          "885:         @Override",
          "886:         public void abortMigration() {",
          "888:         }",
          "889:     }",
          "",
          "[Removed Lines]",
          "874:                 new MigrationWriteOperation(recordBatch));",
          "880:         public OffsetAndEpoch completeMigration() {",
          "882:             return highestMigrationRecordOffset;",
          "",
          "[Added Lines]",
          "914:                 new MigrationWriteOperation(recordBatch), EnumSet.of(RUNS_IN_PREMIGRATION));",
          "920:         public CompletableFuture<OffsetAndEpoch> completeMigration() {",
          "921:             log.info(\"Completing ZK Migration\");",
          "923:             ControllerWriteEvent<Void> event = new ControllerWriteEvent<>(\"Complete ZK Migration\",",
          "924:                 new MigrationWriteOperation(",
          "925:                     Collections.singletonList(ZkMigrationState.MIGRATION.toRecord())),",
          "926:                 EnumSet.of(RUNS_IN_PREMIGRATION));",
          "927:             queue.append(event);",
          "928:             return event.future.thenApply(__ -> highestMigrationRecordOffset);",
          "933:             fatalFaultHandler.handleFault(\"Aborting the ZK migration\");",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1087:     }",
          "1089:     private boolean isActiveController() {",
          "1091:     }",
          "1093:     private void updateWriteOffset(long offset) {",
          "",
          "[Removed Lines]",
          "1090:         return curClaimEpoch != -1;",
          "",
          "[Added Lines]",
          "1137:         return isActiveController(curClaimEpoch);",
          "1138:     }",
          "1140:     private static boolean isActiveController(int claimEpoch) {",
          "1141:         return claimEpoch != -1;",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "1130:         } catch (Throwable e) {",
          "1131:             fatalFaultHandler.handleFault(\"exception while claiming leadership\", e);",
          "1132:         }",
          "1133:     }",
          "1148:                 log.info(\"No metadata.version feature level record was found in the log. \" +",
          "1150:             }",
          "1151:             return ControllerResult.atomicOf(records, null);",
          "1152:         }",
          "",
          "[Removed Lines]",
          "1128:             queue.prepend(new ControllerWriteEvent<>(\"completeActivation[\" + epoch + \"]\",",
          "1129:                     new CompleteActivationEvent()));",
          "1135:     class CompleteActivationEvent implements ControllerWriteOperation<Void> {",
          "1136:         @Override",
          "1137:         public ControllerResult<Void> generateRecordsAndResult() throws Exception {",
          "1138:             List<ApiMessageAndVersion> records = new ArrayList<>();",
          "1139:             if (logReplayTracker.empty()) {",
          "1143:                 log.info(\"The metadata log appears to be empty. Appending {} bootstrap record(s) \" +",
          "1144:                         \"at metadata.version {} from {}.\", bootstrapMetadata.records().size(),",
          "1145:                         bootstrapMetadata.metadataVersion(), bootstrapMetadata.source());",
          "1146:                 records.addAll(bootstrapMetadata.records());",
          "1147:             } else if (featureControl.metadataVersion().equals(MetadataVersion.MINIMUM_KRAFT_VERSION)) {",
          "1149:                         \"Treating the log as version {}.\", MetadataVersion.MINIMUM_KRAFT_VERSION);",
          "",
          "[Added Lines]",
          "1179:             ControllerWriteEvent<Void> activationEvent = new ControllerWriteEvent<>(",
          "1180:                 \"completeActivation[\" + epoch + \"]\",",
          "1181:                 new CompleteActivationEvent(),",
          "1182:                 EnumSet.of(DOES_NOT_UPDATE_QUEUE_TIME, RUNS_IN_PREMIGRATION)",
          "1183:             );",
          "1184:             activationEvent.future.exceptionally(t -> {",
          "1185:                 fatalFaultHandler.handleFault(\"exception while activating controller\", t);",
          "1186:                 return null;",
          "1187:             });",
          "1188:             queue.prepend(activationEvent);",
          "1199:     public static List<ApiMessageAndVersion> generateActivationRecords(",
          "1200:         Logger log,",
          "1201:         boolean isLogEmpty,",
          "1202:         boolean zkMigrationEnabled,",
          "1203:         BootstrapMetadata bootstrapMetadata,",
          "1204:         FeatureControlManager featureControl",
          "1205:     ) {",
          "1206:         List<ApiMessageAndVersion> records = new ArrayList<>();",
          "1207:         if (isLogEmpty) {",
          "1211:             log.info(\"The metadata log appears to be empty. Appending {} bootstrap record(s) \" +",
          "1212:                 \"at metadata.version {} from {}.\", bootstrapMetadata.records().size(),",
          "1213:                 bootstrapMetadata.metadataVersion(), bootstrapMetadata.source());",
          "1214:             records.addAll(bootstrapMetadata.records());",
          "1216:             if (bootstrapMetadata.metadataVersion().isMigrationSupported()) {",
          "1217:                 if (zkMigrationEnabled) {",
          "1218:                     log.info(\"Putting the controller into pre-migration mode. No metadata updates will be allowed until \" +",
          "1219:                         \"the ZK metadata has been migrated\");",
          "1220:                     records.add(ZkMigrationState.PRE_MIGRATION.toRecord());",
          "1221:                 } else {",
          "1222:                     log.debug(\"Setting the ZK migration state to NONE since this is a de-novo KRaft cluster.\");",
          "1223:                     records.add(ZkMigrationState.NONE.toRecord());",
          "1224:                 }",
          "1225:             } else {",
          "1226:                 if (zkMigrationEnabled) {",
          "1227:                     throw new RuntimeException(\"The bootstrap metadata.version \" + bootstrapMetadata.metadataVersion() +",
          "1228:                         \" does not support ZK migrations. Cannot continue with ZK migrations enabled.\");",
          "1229:                 }",
          "1230:             }",
          "1231:         } else {",
          "1233:             if (featureControl.metadataVersion().equals(MetadataVersion.MINIMUM_KRAFT_VERSION)) {",
          "1235:                     \"Treating the log as version {}.\", MetadataVersion.MINIMUM_KRAFT_VERSION);",
          "1236:             }",
          "1238:             if (featureControl.metadataVersion().isMigrationSupported()) {",
          "1239:                 log.info(\"Loaded ZK migration state of {}\", featureControl.zkMigrationState());",
          "1240:                 switch (featureControl.zkMigrationState()) {",
          "1241:                     case NONE:",
          "1244:                         if (zkMigrationEnabled) {",
          "1245:                             throw new RuntimeException(\"Should not have ZK migrations enabled on a cluster that was created in KRaft mode.\");",
          "1246:                         }",
          "1247:                         break;",
          "1248:                     case PRE_MIGRATION:",
          "1249:                         log.warn(\"Activating pre-migration controller without empty log. There may be a partial migration\");",
          "1250:                         break;",
          "1251:                     case MIGRATION:",
          "1252:                         if (!zkMigrationEnabled) {",
          "1256:                             log.warn(\"Completing the ZK migration since this controller was configured with \" +",
          "1257:                                 \"'zookeeper.metadata.migration.enable' set to 'false'.\");",
          "1258:                             records.add(ZkMigrationState.POST_MIGRATION.toRecord());",
          "1259:                         } else {",
          "1260:                             log.info(\"Staying in the ZK migration since 'zookeeper.metadata.migration.enable' is still 'true'.\");",
          "1261:                         }",
          "1262:                         break;",
          "1263:                     case POST_MIGRATION:",
          "1264:                         if (zkMigrationEnabled) {",
          "1265:                             log.info(\"Ignoring 'zookeeper.metadata.migration.enable' value of 'true' since the ZK migration\" +",
          "1266:                                 \"has been completed.\");",
          "1267:                         }",
          "1268:                         break;",
          "1269:                 }",
          "1270:             } else {",
          "1271:                 if (zkMigrationEnabled) {",
          "1272:                     throw new RuntimeException(\"Should not have ZK migrations enabled on a cluster running metadata.version \" + featureControl.metadataVersion());",
          "1273:                 }",
          "1275:         }",
          "1276:         return records;",
          "1277:     }",
          "1278:     class CompleteActivationEvent implements ControllerWriteOperation<Void> {",
          "1279:         @Override",
          "1280:         public ControllerResult<Void> generateRecordsAndResult() {",
          "1281:             List<ApiMessageAndVersion> records = generateActivationRecords(log, logReplayTracker.empty(),",
          "1282:                 zkMigrationEnabled, bootstrapMetadata, featureControl);",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "1205:         }",
          "1206:     }",
          "1211:         queue.scheduleDeferred(name, new EarliestDeadlineFunction(deadlineNs), event);",
          "1212:         event.future.exceptionally(e -> {",
          "1213:             if (e instanceof UnknownServerException && e.getCause() != null &&",
          "",
          "[Removed Lines]",
          "1208:     private <T> void scheduleDeferredWriteEvent(String name, long deadlineNs,",
          "1209:                                                 ControllerWriteOperation<T> op) {",
          "1210:         ControllerWriteEvent<T> event = new ControllerWriteEvent<>(name, op, true);",
          "",
          "[Added Lines]",
          "1340:     private <T> void scheduleDeferredWriteEvent(",
          "1341:         String name,",
          "1342:         long deadlineNs,",
          "1343:         ControllerWriteOperation<T> op,",
          "1344:         EnumSet<ControllerOperationFlag> flags",
          "1345:     ) {",
          "1346:         if (!flags.contains(DOES_NOT_UPDATE_QUEUE_TIME)) {",
          "1347:             throw new RuntimeException(\"deferred events should not update the queue time.\");",
          "1348:         }",
          "1349:         ControllerWriteEvent<T> event = new ControllerWriteEvent<>(name, op, flags);",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "1223:             log.error(\"Unexpected exception while executing deferred write event {}. \" +",
          "1224:                 \"Rescheduling for a minute from now.\", name, e);",
          "1225:             scheduleDeferredWriteEvent(name,",
          "1227:             return null;",
          "1228:         });",
          "1229:     }",
          "",
          "[Removed Lines]",
          "1226:                 deadlineNs + NANOSECONDS.convert(1, TimeUnit.MINUTES), op);",
          "",
          "[Added Lines]",
          "1365:                 deadlineNs + NANOSECONDS.convert(1, TimeUnit.MINUTES), op, flags);",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "1236:             cancelMaybeFenceReplicas();",
          "1237:             return;",
          "1238:         }",
          "1246:     }",
          "1248:     private void cancelMaybeFenceReplicas() {",
          "",
          "[Removed Lines]",
          "1239:         scheduleDeferredWriteEvent(MAYBE_FENCE_REPLICAS, nextCheckTimeNs, () -> {",
          "1240:             ControllerResult<Void> result = replicationControl.maybeFenceOneStaleBroker();",
          "1243:             rescheduleMaybeFenceStaleBrokers();",
          "1244:             return result;",
          "1245:         });",
          "",
          "[Added Lines]",
          "1378:         scheduleDeferredWriteEvent(MAYBE_FENCE_REPLICAS, nextCheckTimeNs,",
          "1379:             () -> {",
          "1380:                 ControllerResult<Void> result = replicationControl.maybeFenceOneStaleBroker();",
          "1383:                 rescheduleMaybeFenceStaleBrokers();",
          "1384:                 return result;",
          "1385:             },",
          "1386:             EnumSet.of(DOES_NOT_UPDATE_QUEUE_TIME, RUNS_IN_PREMIGRATION));",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "1282:                 return result;",
          "1285:             long delayNs = time.nanoseconds();",
          "1286:             if (imbalancedScheduled == ImbalanceSchedule.DEFERRED) {",
          "",
          "[Removed Lines]",
          "1283:             }, true);",
          "",
          "[Added Lines]",
          "1424:             }, EnumSet.of(DOES_NOT_UPDATE_QUEUE_TIME));",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "1328:                         null",
          "1329:                     );",
          "1330:                 },",
          "1332:             );",
          "1334:             long delayNs = time.nanoseconds() + maxIdleIntervalNs.getAsLong();",
          "",
          "[Removed Lines]",
          "1331:                 true",
          "",
          "[Added Lines]",
          "1472:                 EnumSet.of(DOES_NOT_UPDATE_QUEUE_TIME, RUNS_IN_PREMIGRATION)",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "1423:                 break;",
          "1424:             case ZK_MIGRATION_STATE_RECORD:",
          "1426:                 break;",
          "1427:             default:",
          "1428:                 throw new RuntimeException(\"Unhandled record type \" + type);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1566:                 featureControl.replay((ZkMigrationStateRecord) message);",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "1647:     private final ZkRecordConsumer zkRecordConsumer;",
          "1652:     private final int maxRecordsPerBatch;",
          "1654:     private QuorumController(",
          "1655:         FaultHandler fatalFaultHandler,",
          "1656:         LogContext logContext,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1790:     private final boolean zkMigrationEnabled;",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "1751:         this.curClaimEpoch = -1;",
          "1752:         this.needToCompleteAuthorizerLoad = authorizer.isPresent();",
          "1753:         this.zkRecordConsumer = new MigrationRecordConsumer();",
          "1754:         updateWriteOffset(-1);",
          "1756:         resetToEmptyState();",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1898:         this.zkMigrationEnabled = zkMigrationEnabled;",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "1802:         int brokerId",
          "1803:     ) {",
          "1804:         return appendWriteEvent(\"unregisterBroker\", context.deadlineNs(),",
          "1806:     }",
          "1808:     @Override",
          "",
          "[Removed Lines]",
          "1805:             () -> replicationControl.unregisterBroker(brokerId));",
          "",
          "[Added Lines]",
          "1950:             () -> replicationControl.unregisterBroker(brokerId), EnumSet.of(RUNS_IN_PREMIGRATION));",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "1977:                             maybeUpdateControlledShutdownOffset(brokerId, offset);",
          "1978:                     }",
          "1979:                 }",
          "1981:     }",
          "1983:     @Override",
          "",
          "[Removed Lines]",
          "1980:             });",
          "",
          "[Added Lines]",
          "2125:             },",
          "2126:             EnumSet.of(RUNS_IN_PREMIGRATION));",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "1985:         ControllerRequestContext context,",
          "1986:         BrokerRegistrationRequestData request",
          "1987:     ) {",
          "1995:     }",
          "1997:     @Override",
          "",
          "[Removed Lines]",
          "1988:         return appendWriteEvent(\"registerBroker\", context.deadlineNs(), () -> {",
          "1989:             ControllerResult<BrokerRegistrationReply> result = clusterControl.",
          "1990:                 registerBroker(request, writeOffset + 1, featureControl.",
          "1991:                     finalizedFeatures(Long.MAX_VALUE));",
          "1992:             rescheduleMaybeFenceStaleBrokers();",
          "1993:             return result;",
          "1994:         });",
          "",
          "[Added Lines]",
          "2134:         return appendWriteEvent(\"registerBroker\", context.deadlineNs(),",
          "2135:             () -> {",
          "2136:                 ControllerResult<BrokerRegistrationReply> result = clusterControl.",
          "2137:                     registerBroker(request, writeOffset + 1, featureControl.",
          "2138:                         finalizedFeatures(Long.MAX_VALUE));",
          "2139:                 rescheduleMaybeFenceStaleBrokers();",
          "2140:                 return result;",
          "2141:             },",
          "2142:             EnumSet.of(RUNS_IN_PREMIGRATION));",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java||metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java": [
          "File: metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java -> metadata/src/main/java/org/apache/kafka/controller/ReplicationControlManager.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.kafka.controller;",
          "21: import org.apache.kafka.clients.admin.AlterConfigOp.OpType;",
          "22: import org.apache.kafka.clients.admin.ConfigEntry;",
          "23: import org.apache.kafka.common.ElectionType;",
          "",
          "[Removed Lines]",
          "20: import org.apache.kafka.clients.ApiVersions;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "92: import org.apache.kafka.metadata.placement.TopicAssignment;",
          "93: import org.apache.kafka.metadata.placement.UsableBroker;",
          "94: import org.apache.kafka.server.common.ApiMessageAndVersion;",
          "96: import org.apache.kafka.server.policy.CreateTopicPolicy;",
          "97: import org.apache.kafka.timeline.SnapshotRegistry;",
          "98: import org.apache.kafka.timeline.TimelineHashMap;",
          "",
          "[Removed Lines]",
          "95: import org.apache.kafka.server.common.MetadataVersion;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "204:             if (configurationControl == null) {",
          "205:                 throw new IllegalStateException(\"Configuration control must be set before building\");",
          "206:             } else if (clusterControl == null) {",
          "208:             }",
          "209:             if (logContext == null) logContext = new LogContext();",
          "210:             if (snapshotRegistry == null) snapshotRegistry = configurationControl.snapshotRegistry();",
          "211:             if (featureControl == null) {",
          "220:             }",
          "221:             return new ReplicationControlManager(snapshotRegistry,",
          "222:                 logContext,",
          "",
          "[Removed Lines]",
          "207:                 throw new IllegalStateException(\"Cluster controller must be set before building\");",
          "212:                 featureControl = new FeatureControlManager.Builder().",
          "213:                     setLogContext(logContext).",
          "214:                     setSnapshotRegistry(snapshotRegistry).",
          "215:                     setQuorumFeatures(new QuorumFeatures(0, new ApiVersions(),",
          "216:                         QuorumFeatures.defaultFeatureMap(),",
          "217:                         Collections.singletonList(0))).",
          "218:                     setMetadataVersion(MetadataVersion.latest()).",
          "219:                     build();",
          "",
          "[Added Lines]",
          "205:                 throw new IllegalStateException(\"Cluster control must be set before building\");",
          "210:                 throw new IllegalStateException(\"FeatureControlManager must not be null\");",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetrics.java||metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetrics.java": [
          "File: metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetrics.java -> metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetrics.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "49:         \"KafkaController\", \"PreferredReplicaImbalanceCount\");",
          "50:     private final static MetricName METADATA_ERROR_COUNT = getMetricName(",
          "51:         \"KafkaController\", \"MetadataErrorCount\");",
          "53:     private final Optional<MetricsRegistry> registry;",
          "54:     private final AtomicInteger fencedBrokerCount = new AtomicInteger(0);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "52:     private final static MetricName ZK_MIGRATION_STATE = getMetricName(",
          "53:         \"KafkaController\", \"ZkMigrationState\");",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "58:     private final AtomicInteger offlinePartitionCount = new AtomicInteger(0);",
          "59:     private final AtomicInteger preferredReplicaImbalanceCount = new AtomicInteger(0);",
          "60:     private final AtomicInteger metadataErrorCount = new AtomicInteger(0);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "63:     private final AtomicInteger zkMigrationState = new AtomicInteger(-1);",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "108:                 return metadataErrorCount();",
          "109:             }",
          "110:         }));",
          "111:     }",
          "113:     public void setFencedBrokerCount(int brokerCount) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "114:         registry.ifPresent(r -> r.newGauge(ZK_MIGRATION_STATE, new Gauge<Integer>() {",
          "115:             @Override",
          "116:             public Integer value() {",
          "117:                 return (int) zkMigrationState();",
          "118:             }",
          "119:         }));",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "190:         return this.metadataErrorCount.get();",
          "191:     }",
          "193:     @Override",
          "194:     public void close() {",
          "195:         registry.ifPresent(r -> Arrays.asList(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "202:     public void setZkMigrationState(byte migrationStateValue) {",
          "203:         this.zkMigrationState.set(migrationStateValue);",
          "204:     }",
          "206:     public byte zkMigrationState() {",
          "207:         return zkMigrationState.byteValue();",
          "208:     }",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "199:             GLOBAL_PARTITION_COUNT,",
          "200:             OFFLINE_PARTITION_COUNT,",
          "201:             PREFERRED_REPLICA_IMBALANCE_COUNT,",
          "203:         ).forEach(r::removeMetric));",
          "204:     }",
          "",
          "[Removed Lines]",
          "202:             METADATA_ERROR_COUNT",
          "",
          "[Added Lines]",
          "219:             METADATA_ERROR_COUNT,",
          "220:             ZK_MIGRATION_STATE",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsPublisher.java||metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsPublisher.java": [
          "File: metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsPublisher.java -> metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsPublisher.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "115:             }",
          "116:         }",
          "117:         changes.apply(metrics);",
          "118:     }",
          "120:     private void publishSnapshot(MetadataImage newImage) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "118:         if (delta.featuresDelta() != null) {",
          "119:             delta.featuresDelta().getZkMigrationStateChange().ifPresent(state -> metrics.setZkMigrationState(state.value()));",
          "120:         }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "147:         metrics.setGlobalPartitionCount(totalPartitions);",
          "148:         metrics.setOfflinePartitionCount(offlinePartitions);",
          "149:         metrics.setPreferredReplicaImbalanceCount(partitionsWithoutPreferredLeader);",
          "150:     }",
          "152:     @Override",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "153:         metrics.setZkMigrationState(newImage.features().zkMigrationState().value());",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/image/FeaturesDelta.java||metadata/src/main/java/org/apache/kafka/image/FeaturesDelta.java": [
          "File: metadata/src/main/java/org/apache/kafka/image/FeaturesDelta.java -> metadata/src/main/java/org/apache/kafka/image/FeaturesDelta.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.kafka.image;",
          "20: import org.apache.kafka.common.metadata.FeatureLevelRecord;",
          "21: import org.apache.kafka.server.common.MetadataVersion;",
          "23: import java.util.HashMap;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import org.apache.kafka.common.metadata.ZkMigrationStateRecord;",
          "22: import org.apache.kafka.metadata.migration.ZkMigrationState;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "37:     private MetadataVersion metadataVersionChange = null;",
          "39:     public FeaturesDelta(FeaturesImage image) {",
          "40:         this.image = image;",
          "41:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "41:     private ZkMigrationState zkMigrationStateChange = null;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "44:         return changes;",
          "45:     }",
          "47:     public Optional<MetadataVersion> metadataVersionChange() {",
          "48:         return Optional.ofNullable(metadataVersionChange);",
          "49:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "51:     public Optional<ZkMigrationState> getZkMigrationStateChange() {",
          "52:         return Optional.ofNullable(zkMigrationStateChange);",
          "53:     }",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "68:         }",
          "69:     }",
          "71:     public FeaturesImage apply() {",
          "72:         Map<String, Short> newFinalizedVersions =",
          "73:             new HashMap<>(image.finalizedVersions().size());",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "79:     public void replay(ZkMigrationStateRecord record) {",
          "80:         this.zkMigrationStateChange = ZkMigrationState.of(record.zkMigrationState());",
          "81:     }",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "96:         } else {",
          "97:             metadataVersion = metadataVersionChange;",
          "98:         }",
          "100:     }",
          "102:     @Override",
          "",
          "[Removed Lines]",
          "99:         return new FeaturesImage(newFinalizedVersions, metadataVersion);",
          "",
          "[Added Lines]",
          "112:         final ZkMigrationState zkMigrationState;",
          "113:         if (zkMigrationStateChange == null) {",
          "114:             zkMigrationState = image.zkMigrationState();",
          "115:         } else {",
          "116:             zkMigrationState = zkMigrationStateChange;",
          "117:         }",
          "118:         return new FeaturesImage(newFinalizedVersions, metadataVersion, zkMigrationState);",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "104:         return \"FeaturesDelta(\" +",
          "105:             \"changes=\" + changes +",
          "106:             \", metadataVersionChange=\" + metadataVersionChange +",
          "107:             ')';",
          "108:     }",
          "109: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "126:             \", zkMigrationStateChange=\" + zkMigrationStateChange +",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/image/FeaturesImage.java||metadata/src/main/java/org/apache/kafka/image/FeaturesImage.java": [
          "File: metadata/src/main/java/org/apache/kafka/image/FeaturesImage.java -> metadata/src/main/java/org/apache/kafka/image/FeaturesImage.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import org.apache.kafka.common.metadata.FeatureLevelRecord;",
          "21: import org.apache.kafka.image.writer.ImageWriter;",
          "22: import org.apache.kafka.image.writer.ImageWriterOptions;",
          "23: import org.apache.kafka.server.common.MetadataVersion;",
          "25: import java.util.ArrayList;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "23: import org.apache.kafka.metadata.migration.ZkMigrationState;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "27: import java.util.List;",
          "28: import java.util.Map;",
          "29: import java.util.Map.Entry;",
          "30: import java.util.Optional;",
          "31: import java.util.stream.Collectors;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "31: import java.util.Objects;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "39: public final class FeaturesImage {",
          "42:     private final Map<String, Short> finalizedVersions;",
          "44:     private final MetadataVersion metadataVersion;",
          "47:         this.finalizedVersions = Collections.unmodifiableMap(finalizedVersions);",
          "48:         this.metadataVersion = metadataVersion;",
          "49:     }",
          "51:     public boolean isEmpty() {",
          "53:     }",
          "55:     public MetadataVersion metadataVersion() {",
          "",
          "[Removed Lines]",
          "40:     public static final FeaturesImage EMPTY = new FeaturesImage(Collections.emptyMap(), MetadataVersion.MINIMUM_KRAFT_VERSION);",
          "46:     public FeaturesImage(Map<String, Short> finalizedVersions, MetadataVersion metadataVersion) {",
          "52:         return finalizedVersions.isEmpty();",
          "",
          "[Added Lines]",
          "42:     public static final FeaturesImage EMPTY = new FeaturesImage(",
          "43:         Collections.emptyMap(),",
          "44:         MetadataVersion.MINIMUM_KRAFT_VERSION,",
          "45:         ZkMigrationState.NONE",
          "46:     );",
          "52:     private final ZkMigrationState zkMigrationState;",
          "54:     public FeaturesImage(",
          "55:         Map<String, Short> finalizedVersions,",
          "56:         MetadataVersion metadataVersion,",
          "57:         ZkMigrationState zkMigrationState",
          "58:     ) {",
          "61:         this.zkMigrationState = zkMigrationState;",
          "65:         return finalizedVersions.isEmpty() &&",
          "66:             metadataVersion.equals(MetadataVersion.MINIMUM_KRAFT_VERSION) &&",
          "67:             zkMigrationState.equals(ZkMigrationState.NONE);",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "60:         return finalizedVersions;",
          "61:     }",
          "63:     private Optional<Short> finalizedVersion(String feature) {",
          "64:         return Optional.ofNullable(finalizedVersions.get(feature));",
          "65:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "78:     public ZkMigrationState zkMigrationState() {",
          "79:         return zkMigrationState;",
          "80:     }",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "70:         } else {",
          "71:             writeFeatureLevels(writer, options);",
          "72:         }",
          "73:     }",
          "75:     private void handleFeatureLevelNotSupported(ImageWriterOptions options) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "93:         if (options.metadataVersion().isMigrationSupported()) {",
          "94:             writer.write(0, zkMigrationState.toRecord().message());",
          "95:         } else {",
          "96:             if (!zkMigrationState.equals(ZkMigrationState.NONE)) {",
          "97:                 options.handleLoss(\"the ZK Migration state which was \" + zkMigrationState);",
          "98:             }",
          "99:         }",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "106:     @Override",
          "107:     public int hashCode() {",
          "109:     }",
          "111:     @Override",
          "112:     public boolean equals(Object o) {",
          "113:         if (!(o instanceof FeaturesImage)) return false;",
          "114:         FeaturesImage other = (FeaturesImage) o;",
          "116:     }",
          "",
          "[Removed Lines]",
          "108:         return finalizedVersions.hashCode();",
          "115:         return finalizedVersions.equals(other.finalizedVersions);",
          "",
          "[Added Lines]",
          "135:         return Objects.hash(finalizedVersions, metadataVersion, zkMigrationState);",
          "142:         return finalizedVersions.equals(other.finalizedVersions) &&",
          "143:             metadataVersion.equals(other.metadataVersion) &&",
          "144:             zkMigrationState.equals(other.zkMigrationState);",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "121:         return \"FeaturesImage{\" +",
          "122:                 \"finalizedVersions=\" + finalizedVersions +",
          "123:                 \", metadataVersion=\" + metadataVersion +",
          "124:                 '}';",
          "125:     }",
          "126: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "153:                 \", zkMigrationState=\" + zkMigrationState +",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/image/MetadataDelta.java||metadata/src/main/java/org/apache/kafka/image/MetadataDelta.java": [
          "File: metadata/src/main/java/org/apache/kafka/image/MetadataDelta.java -> metadata/src/main/java/org/apache/kafka/image/MetadataDelta.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "35: import org.apache.kafka.common.metadata.UnfenceBrokerRecord;",
          "36: import org.apache.kafka.common.metadata.UnregisterBrokerRecord;",
          "37: import org.apache.kafka.common.metadata.UserScramCredentialRecord;",
          "38: import org.apache.kafka.common.protocol.ApiMessage;",
          "39: import org.apache.kafka.server.common.MetadataVersion;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "38: import org.apache.kafka.common.metadata.ZkMigrationStateRecord;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "227:                 break;",
          "228:             case ZK_MIGRATION_STATE_RECORD:",
          "230:                 break;",
          "231:             default:",
          "232:                 throw new RuntimeException(\"Unknown metadata record type \" + type);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "230:                 replay((ZkMigrationStateRecord) record);",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "312:         getOrCreateScramDelta().replay(record);",
          "313:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "316:     public void replay(ZkMigrationStateRecord record) {",
          "317:         getOrCreateFeaturesDelta().replay(record);",
          "318:     }",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java": [
          "File: metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "36: import org.apache.kafka.raft.OffsetAndEpoch;",
          "37: import org.apache.kafka.server.common.ApiMessageAndVersion;",
          "38: import org.apache.kafka.server.fault.FaultHandler;",
          "39: import org.slf4j.Logger;",
          "41: import java.util.ArrayList;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "39: import org.apache.kafka.server.util.Deadline;",
          "40: import org.apache.kafka.server.util.FutureUtils;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "47: import java.util.Map;",
          "48: import java.util.Set;",
          "49: import java.util.concurrent.CompletableFuture;",
          "51: import java.util.concurrent.RejectedExecutionException;",
          "52: import java.util.concurrent.atomic.AtomicInteger;",
          "53: import java.util.function.Consumer;",
          "54: import java.util.function.Function;",
          "",
          "[Removed Lines]",
          "50: import java.util.concurrent.ExecutionException;",
          "",
          "[Added Lines]",
          "53: import java.util.concurrent.TimeUnit;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "58: import static java.util.concurrent.TimeUnit.SECONDS;",
          "64: public class KRaftMigrationDriver implements MetadataPublisher {",
          "65:     private final static Consumer<Throwable> NO_OP_HANDLER = ex -> { };",
          "67:     private final Time time;",
          "68:     private final Logger log;",
          "69:     private final int nodeId;",
          "70:     private final MigrationClient zkMigrationClient;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "74:     private final static int METADATA_COMMIT_MAX_WAIT_MS = 300_000;",
          "77:     private final LogContext logContext;",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "81:     private volatile MigrationDriverState migrationState;",
          "82:     private volatile ZkMigrationLeadershipState migrationLeadershipState;",
          "83:     private volatile MetadataImage image;",
          "85:     public KRaftMigrationDriver(",
          "86:         int nodeId,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "94:     private volatile boolean firstPublish;",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "95:         this.zkMigrationClient = zkMigrationClient;",
          "96:         this.propagator = propagator;",
          "97:         this.time = Time.SYSTEM;",
          "100:         this.migrationState = MigrationDriverState.UNINITIALIZED;",
          "101:         this.migrationLeadershipState = ZkMigrationLeadershipState.EMPTY;",
          "102:         this.eventQueue = new KafkaEventQueue(Time.SYSTEM, logContext, \"controller-\" + nodeId + \"-migration-driver-\");",
          "103:         this.image = MetadataImage.EMPTY;",
          "104:         this.leaderAndEpoch = LeaderAndEpoch.UNKNOWN;",
          "105:         this.initialZkLoadHandler = initialZkLoadHandler;",
          "106:         this.faultHandler = faultHandler;",
          "",
          "[Removed Lines]",
          "98:         LogContext logContext = new LogContext(\"[KRaftMigrationDriver id=\" + nodeId + \"] \");",
          "99:         this.log = logContext.logger(KRaftMigrationDriver.class);",
          "",
          "[Added Lines]",
          "109:         this.logContext = new LogContext(\"[KRaftMigrationDriver id=\" + nodeId + \"] \");",
          "110:         this.log = this.logContext.logger(KRaftMigrationDriver.class);",
          "115:         this.firstPublish = false;",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "123:         return stateFuture;",
          "124:     }",
          "128:         apply(\"Recovery\", zkMigrationClient::getOrCreateMigrationRecoveryState);",
          "129:         String maybeDone = migrationLeadershipState.zkMigrationComplete() ? \"done\" : \"not done\";",
          "130:         log.info(\"Recovered migration state {}. ZK migration is {}.\", migrationLeadershipState, maybeDone);",
          "131:         initialZkLoadHandler.accept(this);",
          "139:     }",
          "141:     private boolean imageDoesNotContainAllBrokers(MetadataImage image, Set<Integer> brokerIds) {",
          "",
          "[Removed Lines]",
          "126:     private void initializeMigrationState() {",
          "127:         log.info(\"Recovering migration state\");",
          "133:         transitionTo(MigrationDriverState.INACTIVE);",
          "134:     }",
          "136:     private boolean isControllerQuorumReadyForMigration() {",
          "138:         return true;",
          "",
          "[Added Lines]",
          "138:     private void recoverMigrationStateFromZK() {",
          "139:         log.info(\"Recovering migration state from ZK\");",
          "149:         transitionTo(MigrationDriverState.INACTIVE);",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "148:     }",
          "150:     private boolean areZkBrokersReadyForMigration() {",
          "153:             log.info(\"Waiting for initial metadata publish before checking if Zk brokers are registered.\");",
          "154:             return false;",
          "155:         }",
          "158:         Set<Integer> zkBrokerRegistrations = zkMigrationClient.readBrokerIds();",
          "159:         if (imageDoesNotContainAllBrokers(image, zkBrokerRegistrations)) {",
          "160:             log.info(\"Still waiting for ZK brokers {} to register with KRaft.\", zkBrokerRegistrations);",
          "161:             return false;",
          "",
          "[Removed Lines]",
          "151:         if (image == MetadataImage.EMPTY) {",
          "",
          "[Added Lines]",
          "162:         if (!firstPublish) {",
          "167:         if (image.cluster().isEmpty()) {",
          "170:             log.info(\"No brokers are known to KRaft, waiting for brokers to register.\");",
          "171:             return false;",
          "172:         }",
          "175:         if (zkBrokerRegistrations.isEmpty()) {",
          "177:             log.info(\"No brokers are registered in ZK, waiting for brokers to register.\");",
          "178:             return false;",
          "179:         }",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "187:     private boolean isValidStateChange(MigrationDriverState newState) {",
          "188:         if (migrationState == newState)",
          "189:             return true;",
          "190:         switch (migrationState) {",
          "191:             case UNINITIALIZED:",
          "192:             case DUAL_WRITE:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "213:         if (newState == MigrationDriverState.UNINITIALIZED) {",
          "214:             return false;",
          "215:         }",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "196:             case WAIT_FOR_CONTROLLER_QUORUM:",
          "197:                 return",
          "198:                     newState == MigrationDriverState.INACTIVE ||",
          "199:                     newState == MigrationDriverState.WAIT_FOR_BROKERS;",
          "200:             case WAIT_FOR_BROKERS:",
          "201:                 return",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "226:                     newState == MigrationDriverState.BECOME_CONTROLLER ||",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "223:     private void transitionTo(MigrationDriverState newState) {",
          "224:         if (!isValidStateChange(newState)) {",
          "227:         }",
          "228:         if (newState != migrationState) {",
          "229:             log.debug(\"{} transitioning from {} to {} state\", nodeId, migrationState, newState);",
          "230:         } else {",
          "231:             log.trace(\"{} transitioning from {} to {} state\", nodeId, migrationState, newState);",
          "232:         }",
          "242:         migrationState = newState;",
          "243:     }",
          "",
          "[Removed Lines]",
          "225:             log.error(\"Error transition in migration driver from {} to {}\", migrationState, newState);",
          "226:             return;",
          "233:         switch (newState) {",
          "234:             case UNINITIALIZED:",
          "236:                 throw new IllegalStateException(\"Illegal transition from \" + migrationState + \" to \" + newState + \" \" +",
          "237:                 \"state in Zk to KRaft migration\");",
          "238:             case INACTIVE:",
          "240:                 break;",
          "241:         }",
          "",
          "[Added Lines]",
          "253:             throw new IllegalStateException(",
          "254:                 String.format(\"Invalid transition in migration driver from %s to %s\", migrationState, newState));",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "319:         public void run() throws Exception {",
          "320:             switch (migrationState) {",
          "321:                 case UNINITIALIZED:",
          "323:                     break;",
          "324:                 case INACTIVE:",
          "327:                     break;",
          "328:                 case WAIT_FOR_CONTROLLER_QUORUM:",
          "329:                     eventQueue.append(new WaitForControllerQuorumEvent());",
          "",
          "[Removed Lines]",
          "322:                     initializeMigrationState();",
          "",
          "[Added Lines]",
          "343:                     recoverMigrationStateFromZK();",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "368:             KRaftMigrationDriver.this.leaderAndEpoch = leaderAndEpoch;",
          "369:             boolean isActive = leaderAndEpoch.isLeader(KRaftMigrationDriver.this.nodeId);",
          "391:             }",
          "392:         }",
          "393:     }",
          "",
          "[Removed Lines]",
          "370:             switch (migrationState) {",
          "371:                 case UNINITIALIZED:",
          "373:                     long deadline = time.nanoseconds() + NANOSECONDS.convert(10, SECONDS);",
          "374:                     eventQueue.scheduleDeferred(",
          "375:                         \"poll\",",
          "376:                         new EventQueue.DeadlineFunction(deadline),",
          "377:                         this);",
          "378:                     break;",
          "379:                 default:",
          "380:                     if (!isActive) {",
          "381:                         apply(\"KRaftLeaderEvent is not active\", state -> ZkMigrationLeadershipState.EMPTY);",
          "382:                         transitionTo(MigrationDriverState.INACTIVE);",
          "383:                     } else {",
          "385:                         apply(\"KRaftLeaderEvent is active\", state -> state.withNewKRaftController(nodeId, leaderAndEpoch.epoch()));",
          "388:                         transitionTo(MigrationDriverState.WAIT_FOR_CONTROLLER_QUORUM);",
          "389:                     }",
          "390:                     break;",
          "",
          "[Added Lines]",
          "397:             if (!isActive) {",
          "398:                 apply(\"KRaftLeaderEvent is not active\", state ->",
          "399:                     state.withNewKRaftController(",
          "400:                         leaderAndEpoch.leaderId().orElse(ZkMigrationLeadershipState.EMPTY.kraftControllerId()),",
          "401:                         leaderAndEpoch.epoch())",
          "402:                 );",
          "403:                 transitionTo(MigrationDriverState.INACTIVE);",
          "404:             } else {",
          "406:                 apply(\"KRaftLeaderEvent is active\", state -> state.withNewKRaftController(nodeId, leaderAndEpoch.epoch()));",
          "410:                 transitionTo(MigrationDriverState.WAIT_FOR_CONTROLLER_QUORUM);",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "397:         @Override",
          "398:         public void run() throws Exception {",
          "405:                         transitionTo(MigrationDriverState.WAIT_FOR_BROKERS);",
          "411:             }",
          "412:         }",
          "413:     }",
          "",
          "[Removed Lines]",
          "399:             switch (migrationState) {",
          "400:                 case WAIT_FOR_CONTROLLER_QUORUM:",
          "401:                     if (isControllerQuorumReadyForMigration()) {",
          "402:                         log.debug(\"Controller Quorum is ready for Zk to KRaft migration\");",
          "406:                     }",
          "407:                     break;",
          "408:                 default:",
          "410:                     break;",
          "",
          "[Added Lines]",
          "419:             if (migrationState.equals(MigrationDriverState.WAIT_FOR_CONTROLLER_QUORUM)) {",
          "420:                 if (!firstPublish) {",
          "421:                     log.trace(\"Waiting until we have received metadata before proceeding with migration\");",
          "422:                     return;",
          "423:                 }",
          "425:                 ZkMigrationState zkMigrationState = image.features().zkMigrationState();",
          "426:                 switch (zkMigrationState) {",
          "427:                     case NONE:",
          "429:                         log.error(\"The controller's ZkMigrationState is NONE which means this cluster should not be migrated from ZooKeeper. \" +",
          "430:                             \"This controller should not be configured with 'zookeeper.metadata.migration.enable' set to true. \" +",
          "431:                             \"Will not proceed with a migration.\");",
          "432:                         transitionTo(MigrationDriverState.INACTIVE);",
          "433:                         break;",
          "434:                     case PRE_MIGRATION:",
          "436:                         log.debug(\"Controller Quorum is ready for Zk to KRaft migration. Now waiting for ZK brokers.\");",
          "438:                         break;",
          "439:                     case MIGRATION:",
          "440:                         if (!migrationLeadershipState.zkMigrationComplete()) {",
          "441:                             log.error(\"KRaft controller indicates an active migration, but the ZK state does not.\");",
          "442:                             transitionTo(MigrationDriverState.INACTIVE);",
          "443:                         } else {",
          "445:                             log.debug(\"Migration is in already progress, not waiting on ZK brokers.\");",
          "446:                             transitionTo(MigrationDriverState.BECOME_CONTROLLER);",
          "447:                         }",
          "448:                         break;",
          "449:                     case POST_MIGRATION:",
          "450:                         log.error(\"KRaft controller indicates a completed migration, but the migration driver is somehow active.\");",
          "451:                         transitionTo(MigrationDriverState.INACTIVE);",
          "452:                         break;",
          "453:                 }",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "463:                             log.info(\"Migrating {} records from ZK\", batch.size());",
          "464:                         }",
          "465:                         CompletableFuture<?> future = zkRecordConsumer.acceptBatch(batch);",
          "466:                         count.addAndGet(batch.size());",
          "469:                         throw new RuntimeException(e);",
          "472:                     }",
          "473:                 }, brokersInMetadata::add);",
          "475:                 log.info(\"Completed migration of metadata from Zookeeper to KRaft. A total of {} metadata records were \" +",
          "476:                          \"generated. The current metadata offset is now {} with an epoch of {}. Saw {} brokers in the \" +",
          "477:                          \"migrated metadata {}.\",",
          "",
          "[Removed Lines]",
          "467:                         future.get();",
          "468:                     } catch (InterruptedException e) {",
          "470:                     } catch (ExecutionException e) {",
          "471:                         throw new RuntimeException(e.getCause());",
          "474:                 OffsetAndEpoch offsetAndEpochAfterMigration = zkRecordConsumer.completeMigration();",
          "",
          "[Added Lines]",
          "509:                         FutureUtils.waitWithLogging(KRaftMigrationDriver.this.log, KRaftMigrationDriver.this.logContext.logPrefix(),",
          "510:                             \"the metadata layer to commit migration record batch\",",
          "511:                             future, Deadline.fromDelay(time, METADATA_COMMIT_MAX_WAIT_MS, TimeUnit.MILLISECONDS), time);",
          "513:                     } catch (Throwable e) {",
          "517:                 CompletableFuture<OffsetAndEpoch> completeMigrationFuture = zkRecordConsumer.completeMigration();",
          "518:                 OffsetAndEpoch offsetAndEpochAfterMigration = FutureUtils.waitWithLogging(",
          "519:                     KRaftMigrationDriver.this.log, KRaftMigrationDriver.this.logContext.logPrefix(),",
          "520:                     \"the metadata layer to complete the migration\",",
          "521:                     completeMigrationFuture, Deadline.fromDelay(time, METADATA_COMMIT_MAX_WAIT_MS, TimeUnit.MILLISECONDS), time);",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "483:                 ZkMigrationLeadershipState newState = migrationLeadershipState.withKRaftMetadataOffsetAndEpoch(",
          "484:                     offsetAndEpochAfterMigration.offset(),",
          "485:                     offsetAndEpochAfterMigration.epoch());",
          "487:                 transitionTo(MigrationDriverState.KRAFT_CONTROLLER_TO_BROKER_COMM);",
          "488:             } catch (Throwable t) {",
          "489:                 zkRecordConsumer.abortMigration();",
          "",
          "[Removed Lines]",
          "486:                 apply(\"Migrate metadata from Zk\", state -> zkMigrationClient.setMigrationRecoveryState(newState));",
          "",
          "[Added Lines]",
          "533:                 apply(\"Finished migrating ZK data\", state -> zkMigrationClient.setMigrationRecoveryState(newState));",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "536:         @Override",
          "537:         public void run() throws Exception {",
          "538:             MetadataImage prevImage = KRaftMigrationDriver.this.image;",
          "539:             KRaftMigrationDriver.this.image = image;",
          "540:             String metadataType = isSnapshot ? \"snapshot\" : \"delta\";",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "585:             KRaftMigrationDriver.this.firstPublish = true;",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/metadata/migration/ZkMigrationState.java||metadata/src/main/java/org/apache/kafka/metadata/migration/ZkMigrationState.java": [
          "File: metadata/src/main/java/org/apache/kafka/metadata/migration/ZkMigrationState.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/ZkMigrationState.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: package org.apache.kafka.metadata.migration;",
          "19: import java.util.Optional;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: import org.apache.kafka.common.metadata.ZkMigrationStateRecord;",
          "20: import org.apache.kafka.server.common.ApiMessageAndVersion;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "42:     PRE_MIGRATION((byte) 1),",
          "49:     MIGRATION((byte) 2),",
          "",
          "[Added Lines]",
          "51:     PRE_MIGRATION((byte) 2),",
          "58:     MIGRATION((byte) 1),",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "65:         return value;",
          "66:     }",
          "68:     public static ZkMigrationState of(byte value) {",
          "69:         return optionalOf(value)",
          "70:             .orElseThrow(() -> new IllegalArgumentException(String.format(\"Value %s is not a valid Zk migration state\", value)));",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "77:     public ApiMessageAndVersion toRecord() {",
          "78:         return new ApiMessageAndVersion(",
          "79:             new ZkMigrationStateRecord().setZkMigrationState(value()),",
          "80:             (short) 0",
          "81:         );",
          "82:     }",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "78:         }",
          "79:         return Optional.empty();",
          "80:     }",
          "81: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "98:     public boolean inProgress() {",
          "99:         return this == PRE_MIGRATION || this == MIGRATION;",
          "100:     }",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/metadata/migration/ZkRecordConsumer.java||metadata/src/main/java/org/apache/kafka/metadata/migration/ZkRecordConsumer.java": [
          "File: metadata/src/main/java/org/apache/kafka/metadata/migration/ZkRecordConsumer.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/ZkRecordConsumer.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: public interface ZkRecordConsumer {",
          "26:     void beginMigration();",
          "27:     CompletableFuture<?> acceptBatch(List<ApiMessageAndVersion> recordBatch);",
          "29:     void abortMigration();",
          "30: }",
          "",
          "[Removed Lines]",
          "28:     OffsetAndEpoch completeMigration();",
          "",
          "[Added Lines]",
          "28:     CompletableFuture<OffsetAndEpoch> completeMigration();",
          "",
          "---------------"
        ],
        "metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java||metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java": [
          "File: metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java -> metadata/src/test/java/org/apache/kafka/controller/FeatureControlManagerTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "35: import org.apache.kafka.metadata.FinalizedControllerFeatures;",
          "36: import org.apache.kafka.metadata.RecordTestUtils;",
          "37: import org.apache.kafka.metadata.VersionRange;",
          "38: import org.apache.kafka.server.common.ApiMessageAndVersion;",
          "39: import org.apache.kafka.server.common.MetadataVersion;",
          "40: import org.apache.kafka.timeline.SnapshotRegistry;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "38: import org.apache.kafka.metadata.bootstrap.BootstrapMetadata;",
          "39: import org.apache.kafka.metadata.migration.ZkMigrationState;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "225:     }",
          "227:     private static final FeatureControlManager.Builder TEST_MANAGER_BUILDER1 =",
          "233:     @Test",
          "234:     public void testApplyMetadataVersionChangeRecord() {",
          "",
          "[Removed Lines]",
          "228:             new FeatureControlManager.Builder().",
          "229:                     setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,",
          "230:                             MetadataVersion.IBP_3_3_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).",
          "231:                     setMetadataVersion(MetadataVersion.IBP_3_3_IV2);",
          "",
          "[Added Lines]",
          "230:         new FeatureControlManager.Builder().",
          "231:             setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,",
          "232:                 MetadataVersion.IBP_3_3_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).",
          "233:             setMetadataVersion(MetadataVersion.IBP_3_3_IV2);",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "355:                 setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,",
          "356:                         MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV1.featureLevel())).",
          "357:                 setMetadataVersion(MetadataVersion.IBP_3_1_IV0).",
          "359:         assertEquals(ControllerResult.of(Collections.emptyList(),",
          "360:                         singletonMap(MetadataVersion.FEATURE_NAME, ApiError.NONE)),",
          "361:                 manager.updateFeatures(",
          "",
          "[Removed Lines]",
          "358:                 setMinimumBootstrapVersion(MetadataVersion.IBP_3_0_IV0).build();",
          "",
          "[Added Lines]",
          "360:                 setMinimumBootstrapVersion(MetadataVersion.IBP_3_0_IV0).",
          "361:                 build();",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "369:     public void testCanotDowngradeBefore3_3_IV0() {",
          "370:         FeatureControlManager manager = new FeatureControlManager.Builder().",
          "371:             setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,",
          "374:         assertEquals(ControllerResult.of(Collections.emptyList(),",
          "375:                         singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,",
          "376:                         \"Invalid metadata.version 3. Unable to set a metadata.version less than 3.3-IV0\"))),",
          "",
          "[Removed Lines]",
          "372:                     MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).",
          "373:                     setMetadataVersion(MetadataVersion.IBP_3_3_IV0).build();",
          "",
          "[Added Lines]",
          "375:                 MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_3_IV3.featureLevel())).",
          "376:             setMetadataVersion(MetadataVersion.IBP_3_3_IV0).",
          "377:             build();",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "412:         RecordTestUtils.replayAll(manager, result2.records());",
          "413:         assertEquals(Optional.empty(), manager.finalizedFeatures(Long.MAX_VALUE).get(\"foo\"));",
          "414:     }",
          "415: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "420:     @Test",
          "421:     public void testNoMetadataVersionChangeDuringMigration() {",
          "422:         FeatureControlManager manager = new FeatureControlManager.Builder().",
          "423:             setQuorumFeatures(features(MetadataVersion.FEATURE_NAME,",
          "424:                     MetadataVersion.IBP_3_0_IV0.featureLevel(), MetadataVersion.IBP_3_5_IV1.featureLevel())).",
          "425:             setMetadataVersion(MetadataVersion.IBP_3_4_IV0).",
          "426:             build();",
          "427:         BootstrapMetadata bootstrapMetadata = BootstrapMetadata.fromVersion(MetadataVersion.IBP_3_4_IV0, \"FeatureControlManagerTest\");",
          "428:         RecordTestUtils.replayAll(manager, bootstrapMetadata.records());",
          "429:         RecordTestUtils.replayOne(manager, ZkMigrationState.PRE_MIGRATION.toRecord());",
          "431:         assertEquals(ControllerResult.of(Collections.emptyList(),",
          "432:             singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,",
          "433:                 \"Invalid metadata.version 10. Unable to modify metadata.version while a ZK migration is in progress.\"))),",
          "434:             manager.updateFeatures(",
          "435:                 singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_5_IV1.featureLevel()),",
          "436:                 singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),",
          "437:                 emptyMap(),",
          "438:                 true));",
          "440:         assertEquals(ControllerResult.of(Collections.emptyList(),",
          "441:                 singletonMap(MetadataVersion.FEATURE_NAME, new ApiError(Errors.INVALID_UPDATE_VERSION,",
          "442:                 \"Invalid metadata.version 4. Unable to modify metadata.version while a ZK migration is in progress.\"))),",
          "443:             manager.updateFeatures(",
          "444:                 singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_3_IV0.featureLevel()),",
          "445:                 singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.SAFE_DOWNGRADE),",
          "446:                 emptyMap(),",
          "447:                 true));",
          "450:         RecordTestUtils.replayOne(manager, ZkMigrationState.POST_MIGRATION.toRecord());",
          "451:         ControllerResult<Map<String, ApiError>> result = manager.updateFeatures(",
          "452:             singletonMap(MetadataVersion.FEATURE_NAME, MetadataVersion.IBP_3_5_IV1.featureLevel()),",
          "453:             singletonMap(MetadataVersion.FEATURE_NAME, FeatureUpdate.UpgradeType.UPGRADE),",
          "454:             emptyMap(),",
          "455:             false);",
          "456:         assertEquals(Errors.NONE, result.response().get(MetadataVersion.FEATURE_NAME).error());",
          "457:         RecordTestUtils.replayAll(manager, result.records());",
          "458:         assertEquals(MetadataVersion.IBP_3_5_IV1, manager.metadataVersion());",
          "459:     }",
          "",
          "---------------"
        ],
        "metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java||metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java": [
          "File: metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java -> metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "49: import org.apache.kafka.common.metadata.BrokerRegistrationChangeRecord;",
          "50: import org.apache.kafka.common.metadata.ConfigRecord;",
          "51: import org.apache.kafka.common.metadata.UnfenceBrokerRecord;",
          "52: import org.apache.kafka.common.requests.AlterPartitionRequest;",
          "53: import org.apache.kafka.common.security.auth.KafkaPrincipal;",
          "54: import org.apache.kafka.common.utils.Time;",
          "55: import org.apache.kafka.common.utils.Utils;",
          "56: import org.apache.kafka.common.config.ConfigResource;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "52: import org.apache.kafka.common.metadata.ZkMigrationStateRecord;",
          "55: import org.apache.kafka.common.utils.LogContext;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "92: import org.apache.kafka.metadata.BrokerRegistrationReply;",
          "93: import org.apache.kafka.metadata.FinalizedControllerFeatures;",
          "94: import org.apache.kafka.metadata.PartitionRegistration;",
          "95: import org.apache.kafka.metadata.authorizer.StandardAuthorizer;",
          "96: import org.apache.kafka.metadata.bootstrap.BootstrapMetadata;",
          "97: import org.apache.kafka.metadata.util.BatchFileWriter;",
          "98: import org.apache.kafka.metalog.LocalLogManager;",
          "99: import org.apache.kafka.metalog.LocalLogManagerTestEnv;",
          "100: import org.apache.kafka.raft.OffsetAndEpoch;",
          "101: import org.apache.kafka.server.common.ApiMessageAndVersion;",
          "102: import org.apache.kafka.server.common.MetadataVersion;",
          "103: import org.apache.kafka.snapshot.FileRawSnapshotReader;",
          "104: import org.apache.kafka.snapshot.RawSnapshotReader;",
          "105: import org.apache.kafka.snapshot.Snapshots;",
          "106: import org.apache.kafka.test.TestUtils;",
          "107: import org.junit.jupiter.api.Disabled;",
          "108: import org.junit.jupiter.api.Test;",
          "109: import org.junit.jupiter.api.Timeout;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "97: import org.apache.kafka.metadata.RecordTestUtils;",
          "100: import org.apache.kafka.metadata.migration.ZkMigrationState;",
          "107: import org.apache.kafka.server.fault.FaultHandlerException;",
          "112: import org.apache.kafka.timeline.SnapshotRegistry;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "244:             new ResultOrError<>(Collections.emptyMap())),",
          "245:             controller.describeConfigs(ANONYMOUS_CONTEXT, Collections.singletonMap(",
          "246:                 BROKER0, Collections.emptyList())).get());",
          "248:         assertEquals(Collections.singletonMap(BROKER0, ApiError.NONE), future1.get());",
          "249:     }",
          "",
          "[Removed Lines]",
          "247:         logEnv.logManagers().forEach(m -> m.setMaxReadOffset(3L));",
          "",
          "[Added Lines]",
          "253:         logEnv.logManagers().forEach(m -> m.setMaxReadOffset(4L));",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "556:                     setIncarnationId(Uuid.fromString(\"kxAT73dKQsitIedpiPtwBA\")).",
          "557:                     setFeatures(brokerFeatures(MetadataVersion.IBP_3_0_IV1, MetadataVersion.IBP_3_5_IV2)).",
          "558:                     setListeners(listeners));",
          "560:             CreateTopicsRequestData createTopicsRequestData =",
          "561:                 new CreateTopicsRequestData().setTopics(",
          "562:                     new CreatableTopicCollection(Collections.singleton(",
          "",
          "[Removed Lines]",
          "559:             assertEquals(2L, reply.get().epoch());",
          "",
          "[Added Lines]",
          "565:             assertEquals(3L, reply.get().epoch());",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "572:                         get().topics().find(\"foo\").errorMessage());",
          "573:             assertEquals(new BrokerHeartbeatReply(true, false, false, false),",
          "574:                 active.processBrokerHeartbeat(ANONYMOUS_CONTEXT, new BrokerHeartbeatRequestData().",
          "576:                         setCurrentMetadataOffset(100000L)).get());",
          "577:             assertEquals(Errors.NONE.code(), active.createTopics(ANONYMOUS_CONTEXT,",
          "578:                 createTopicsRequestData, Collections.singleton(\"foo\")).",
          "",
          "[Removed Lines]",
          "575:                         setWantFence(false).setBrokerEpoch(2L).setBrokerId(0).",
          "",
          "[Added Lines]",
          "581:                         setWantFence(false).setBrokerEpoch(3L).setBrokerId(0).",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1338:                         2,",
          "1339:                         appender)).getMessage());",
          "1340:     }",
          "1341: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1348:     @Test",
          "1349:     public void testBootstrapZkMigrationRecord() throws Exception {",
          "1350:         assertEquals(ZkMigrationState.PRE_MIGRATION,",
          "1351:             checkBootstrapZkMigrationRecord(MetadataVersion.IBP_3_4_IV0, true));",
          "1353:         assertEquals(ZkMigrationState.NONE,",
          "1354:             checkBootstrapZkMigrationRecord(MetadataVersion.IBP_3_4_IV0, false));",
          "1356:         assertEquals(ZkMigrationState.NONE,",
          "1357:             checkBootstrapZkMigrationRecord(MetadataVersion.IBP_3_3_IV0, false));",
          "1359:         assertEquals(",
          "1360:             \"The bootstrap metadata.version 3.3-IV0 does not support ZK migrations. Cannot continue with ZK migrations enabled.\",",
          "1361:             assertThrows(FaultHandlerException.class, () ->",
          "1362:                 checkBootstrapZkMigrationRecord(MetadataVersion.IBP_3_3_IV0, true)).getCause().getCause().getMessage()",
          "1363:         );",
          "1364:     }",
          "1366:     public ZkMigrationState checkBootstrapZkMigrationRecord(",
          "1367:         MetadataVersion metadataVersion,",
          "1368:         boolean migrationEnabled",
          "1369:     ) throws Exception {",
          "1370:         try (",
          "1371:             LocalLogManagerTestEnv logEnv = new LocalLogManagerTestEnv.Builder(1).build();",
          "1372:             QuorumControllerTestEnv controlEnv = new QuorumControllerTestEnv.Builder(logEnv).",
          "1373:                 setControllerBuilderInitializer(controllerBuilder -> {",
          "1374:                     controllerBuilder.setZkMigrationEnabled(migrationEnabled);",
          "1375:                 }).",
          "1376:                 setBootstrapMetadata(BootstrapMetadata.fromVersion(metadataVersion, \"test\")).",
          "1377:                 build();",
          "1378:         ) {",
          "1379:             QuorumController active = controlEnv.activeController();",
          "1380:             return active.appendReadEvent(\"read migration state\", OptionalLong.empty(),",
          "1381:                 () -> active.featureControl().zkMigrationState()).get(30, TimeUnit.SECONDS);",
          "1382:         }",
          "1383:     }",
          "1385:     @Test",
          "1386:     public void testUpgradeMigrationStateFrom34() throws Exception {",
          "1387:         try (LocalLogManagerTestEnv logEnv = new LocalLogManagerTestEnv.Builder(1).build()) {",
          "1390:             BootstrapMetadata bootstrapMetadata = BootstrapMetadata.fromVersion(MetadataVersion.IBP_3_4_IV0, \"test\");",
          "1391:             List<ApiMessageAndVersion> initialRecords = new ArrayList<>(bootstrapMetadata.records());",
          "1392:             initialRecords.add(ZkMigrationState.of((byte) 1).toRecord());",
          "1393:             logEnv.appendInitialRecords(initialRecords);",
          "1394:             try (",
          "1395:                 QuorumControllerTestEnv controlEnv = new QuorumControllerTestEnv.Builder(logEnv).",
          "1396:                     setControllerBuilderInitializer(controllerBuilder -> {",
          "1397:                         controllerBuilder.setZkMigrationEnabled(true);",
          "1398:                     }).",
          "1399:                     setBootstrapMetadata(bootstrapMetadata).",
          "1400:                     build();",
          "1401:             ) {",
          "1402:                 QuorumController active = controlEnv.activeController();",
          "1403:                 assertEquals(active.featureControl().zkMigrationState(), ZkMigrationState.MIGRATION);",
          "1404:                 assertFalse(active.featureControl().inPreMigrationMode());",
          "1405:             }",
          "1406:         }",
          "1407:     }",
          "1409:     FeatureControlManager getActivationRecords(",
          "1410:             MetadataVersion metadataVersion,",
          "1411:             Optional<ZkMigrationState> stateInLog,",
          "1412:             boolean zkMigrationEnabled",
          "1413:     ) {",
          "1414:         SnapshotRegistry snapshotRegistry = new SnapshotRegistry(new LogContext());",
          "1415:         FeatureControlManager featureControlManager = new FeatureControlManager.Builder()",
          "1416:                 .setSnapshotRegistry(snapshotRegistry)",
          "1417:                 .setMetadataVersion(metadataVersion)",
          "1418:                 .build();",
          "1420:         stateInLog.ifPresent(zkMigrationState ->",
          "1421:             featureControlManager.replay((ZkMigrationStateRecord) zkMigrationState.toRecord().message()));",
          "1423:         List<ApiMessageAndVersion> records = QuorumController.generateActivationRecords(",
          "1424:             log,",
          "1425:             !stateInLog.isPresent(),",
          "1426:             zkMigrationEnabled,",
          "1427:             BootstrapMetadata.fromVersion(metadataVersion, \"test\"),",
          "1428:             featureControlManager);",
          "1429:         RecordTestUtils.replayAll(featureControlManager, records);",
          "1430:         return featureControlManager;",
          "1431:     }",
          "1433:     @Test",
          "1434:     public void testActivationRecords33() {",
          "1435:         FeatureControlManager featureControl;",
          "1437:         assertEquals(",
          "1438:             \"The bootstrap metadata.version 3.3-IV0 does not support ZK migrations. Cannot continue with ZK migrations enabled.\",",
          "1439:             assertThrows(RuntimeException.class, () -> getActivationRecords(MetadataVersion.IBP_3_3_IV0, Optional.empty(), true)).getMessage()",
          "1440:         );",
          "1442:         featureControl = getActivationRecords(MetadataVersion.IBP_3_3_IV0, Optional.empty(), false);",
          "1443:         assertEquals(MetadataVersion.IBP_3_3_IV0, featureControl.metadataVersion());",
          "1444:         assertEquals(ZkMigrationState.NONE, featureControl.zkMigrationState());",
          "1446:         assertEquals(",
          "1447:             \"Should not have ZK migrations enabled on a cluster running metadata.version 3.3-IV0\",",
          "1448:             assertThrows(RuntimeException.class, () -> getActivationRecords(MetadataVersion.IBP_3_3_IV0, Optional.of(ZkMigrationState.NONE), true)).getMessage()",
          "1449:         );",
          "1451:         featureControl = getActivationRecords(MetadataVersion.IBP_3_3_IV0, Optional.of(ZkMigrationState.NONE), false);",
          "1452:         assertEquals(MetadataVersion.IBP_3_3_IV0, featureControl.metadataVersion());",
          "1453:         assertEquals(ZkMigrationState.NONE, featureControl.zkMigrationState());",
          "1454:     }",
          "1456:     @Test",
          "1457:     public void testActivationRecords34() {",
          "1458:         FeatureControlManager featureControl;",
          "1460:         featureControl = getActivationRecords(MetadataVersion.IBP_3_4_IV0, Optional.empty(), true);",
          "1461:         assertEquals(MetadataVersion.IBP_3_4_IV0, featureControl.metadataVersion());",
          "1462:         assertEquals(ZkMigrationState.PRE_MIGRATION, featureControl.zkMigrationState());",
          "1464:         featureControl = getActivationRecords(MetadataVersion.IBP_3_4_IV0, Optional.empty(), false);",
          "1465:         assertEquals(MetadataVersion.IBP_3_4_IV0, featureControl.metadataVersion());",
          "1466:         assertEquals(ZkMigrationState.NONE, featureControl.zkMigrationState());",
          "1468:         assertEquals(",
          "1469:             \"Should not have ZK migrations enabled on a cluster that was created in KRaft mode.\",",
          "1470:             assertThrows(RuntimeException.class, () -> getActivationRecords(MetadataVersion.IBP_3_4_IV0, Optional.of(ZkMigrationState.NONE), true)).getMessage()",
          "1471:         );",
          "1473:         featureControl = getActivationRecords(MetadataVersion.IBP_3_4_IV0, Optional.of(ZkMigrationState.NONE), false);",
          "1474:         assertEquals(MetadataVersion.IBP_3_4_IV0, featureControl.metadataVersion());",
          "1475:         assertEquals(ZkMigrationState.NONE, featureControl.zkMigrationState());",
          "1477:         featureControl = getActivationRecords(MetadataVersion.IBP_3_4_IV0, Optional.of(ZkMigrationState.PRE_MIGRATION), true);",
          "1478:         assertEquals(MetadataVersion.IBP_3_4_IV0, featureControl.metadataVersion());",
          "1479:         assertEquals(ZkMigrationState.PRE_MIGRATION, featureControl.zkMigrationState());",
          "1481:         featureControl = getActivationRecords(MetadataVersion.IBP_3_4_IV0, Optional.of(ZkMigrationState.MIGRATION), true);",
          "1482:         assertEquals(MetadataVersion.IBP_3_4_IV0, featureControl.metadataVersion());",
          "1483:         assertEquals(ZkMigrationState.MIGRATION, featureControl.zkMigrationState());",
          "1485:         featureControl = getActivationRecords(MetadataVersion.IBP_3_4_IV0, Optional.of(ZkMigrationState.MIGRATION), false);",
          "1486:         assertEquals(MetadataVersion.IBP_3_4_IV0, featureControl.metadataVersion());",
          "1487:         assertEquals(ZkMigrationState.POST_MIGRATION, featureControl.zkMigrationState());",
          "1489:         featureControl = getActivationRecords(MetadataVersion.IBP_3_4_IV0, Optional.of(ZkMigrationState.POST_MIGRATION), true);",
          "1490:         assertEquals(MetadataVersion.IBP_3_4_IV0, featureControl.metadataVersion());",
          "1491:         assertEquals(ZkMigrationState.POST_MIGRATION, featureControl.zkMigrationState());",
          "1493:         featureControl = getActivationRecords(MetadataVersion.IBP_3_4_IV0, Optional.of(ZkMigrationState.POST_MIGRATION), false);",
          "1494:         assertEquals(MetadataVersion.IBP_3_4_IV0, featureControl.metadataVersion());",
          "1495:         assertEquals(ZkMigrationState.POST_MIGRATION, featureControl.zkMigrationState());",
          "1496:     }",
          "1498:     @Test",
          "1499:     public void testActivationRecordsNonEmptyLog() {",
          "1500:         FeatureControlManager featureControl;",
          "1502:         featureControl = getActivationRecords(MetadataVersion.IBP_3_4_IV0, Optional.empty(), true);",
          "1503:         assertEquals(MetadataVersion.IBP_3_4_IV0, featureControl.metadataVersion());",
          "1504:         assertEquals(ZkMigrationState.PRE_MIGRATION, featureControl.zkMigrationState());    }",
          "1506:     @Test",
          "1507:     public void testMigrationsEnabledForOldBootstrapMetadataVersion() throws Exception {",
          "1508:         try (",
          "1509:             LocalLogManagerTestEnv logEnv = new LocalLogManagerTestEnv.Builder(1).build();",
          "1510:         ) {",
          "1511:             QuorumControllerTestEnv.Builder controlEnvBuilder = new QuorumControllerTestEnv.Builder(logEnv).",
          "1512:                     setControllerBuilderInitializer(controllerBuilder -> {",
          "1513:                         controllerBuilder.setZkMigrationEnabled(true);",
          "1514:                     }).",
          "1515:                     setBootstrapMetadata(BootstrapMetadata.fromVersion(MetadataVersion.IBP_3_3_IV0, \"test\"));",
          "1517:             QuorumControllerTestEnv controlEnv = controlEnvBuilder.build();",
          "1518:             QuorumController active = controlEnv.activeController();",
          "1519:             assertEquals(ZkMigrationState.NONE, active.appendReadEvent(\"read migration state\", OptionalLong.empty(),",
          "1520:                 () -> active.featureControl().zkMigrationState()).get(30, TimeUnit.SECONDS));",
          "1521:             assertThrows(FaultHandlerException.class, controlEnv::close);",
          "1522:         }",
          "1523:     }",
          "",
          "---------------"
        ],
        "metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsTest.java||metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsTest.java": [
          "File: metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsTest.java -> metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "45:                         \"kafka.controller:type=KafkaController,name=GlobalTopicCount\",",
          "46:                         \"kafka.controller:type=KafkaController,name=MetadataErrorCount\",",
          "47:                         \"kafka.controller:type=KafkaController,name=OfflinePartitionsCount\",",
          "49:                     )));",
          "50:             }",
          "51:             ControllerMetricsTestUtils.assertMetricsForTypeEqual(registry, \"KafkaController\",",
          "",
          "[Removed Lines]",
          "48:                         \"kafka.controller:type=KafkaController,name=PreferredReplicaImbalanceCount\"",
          "",
          "[Added Lines]",
          "48:                         \"kafka.controller:type=KafkaController,name=PreferredReplicaImbalanceCount\",",
          "49:                         \"kafka.controller:type=KafkaController,name=ZkMigrationState\"",
          "",
          "---------------"
        ],
        "metadata/src/test/java/org/apache/kafka/image/FeaturesImageTest.java||metadata/src/test/java/org/apache/kafka/image/FeaturesImageTest.java": [
          "File: metadata/src/test/java/org/apache/kafka/image/FeaturesImageTest.java -> metadata/src/test/java/org/apache/kafka/image/FeaturesImageTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import org.apache.kafka.image.writer.ImageWriterOptions;",
          "22: import org.apache.kafka.image.writer.RecordListWriter;",
          "23: import org.apache.kafka.metadata.RecordTestUtils;",
          "24: import org.apache.kafka.server.common.ApiMessageAndVersion;",
          "25: import org.apache.kafka.server.common.MetadataVersion;",
          "26: import org.junit.jupiter.api.Test;",
          "27: import org.junit.jupiter.api.Timeout;",
          "29: import java.util.ArrayList;",
          "30: import java.util.HashMap;",
          "31: import java.util.List;",
          "32: import java.util.Map;",
          "34: import static org.junit.jupiter.api.Assertions.assertEquals;",
          "37: @Timeout(value = 40)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24: import org.apache.kafka.metadata.migration.ZkMigrationState;",
          "31: import java.util.Collections;",
          "37: import static org.junit.jupiter.api.Assertions.assertFalse;",
          "38: import static org.junit.jupiter.api.Assertions.assertTrue;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "46:         map1.put(\"foo\", (short) 2);",
          "47:         map1.put(\"bar\", (short) 1);",
          "48:         map1.put(\"baz\", (short) 8);",
          "51:         DELTA1_RECORDS = new ArrayList<>();",
          "52:         DELTA1_RECORDS.add(new ApiMessageAndVersion(new FeatureLevelRecord().",
          "",
          "[Removed Lines]",
          "49:         IMAGE1 = new FeaturesImage(map1, MetadataVersion.latest());",
          "",
          "[Added Lines]",
          "53:         IMAGE1 = new FeaturesImage(map1, MetadataVersion.latest(), ZkMigrationState.NONE);",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "65:         Map<String, Short> map2 = new HashMap<>();",
          "66:         map2.put(\"foo\", (short) 3);",
          "68:     }",
          "70:     @Test",
          "",
          "[Removed Lines]",
          "67:         IMAGE2 = new FeaturesImage(map2, MetadataVersion.latest());",
          "",
          "[Added Lines]",
          "71:         IMAGE2 = new FeaturesImage(map2, MetadataVersion.latest(), ZkMigrationState.NONE);",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "90:     private void testToImageAndBack(FeaturesImage image) throws Throwable {",
          "91:         RecordListWriter writer = new RecordListWriter();",
          "93:         FeaturesDelta delta = new FeaturesDelta(FeaturesImage.EMPTY);",
          "94:         RecordTestUtils.replayAll(delta, writer.records());",
          "95:         FeaturesImage nextImage = delta.apply();",
          "96:         assertEquals(image, nextImage);",
          "97:     }",
          "98: }",
          "",
          "[Removed Lines]",
          "92:         image.write(writer, new ImageWriterOptions.Builder().build());",
          "",
          "[Added Lines]",
          "96:         image.write(writer, new ImageWriterOptions.Builder().setMetadataVersion(image.metadataVersion()).build());",
          "103:     @Test",
          "104:     public void testEmpty() {",
          "105:         assertTrue(FeaturesImage.EMPTY.isEmpty());",
          "106:         assertFalse(new FeaturesImage(Collections.singletonMap(\"foo\", (short) 1),",
          "107:             FeaturesImage.EMPTY.metadataVersion(), FeaturesImage.EMPTY.zkMigrationState()).isEmpty());",
          "108:         assertFalse(new FeaturesImage(FeaturesImage.EMPTY.finalizedVersions(),",
          "109:             MetadataVersion.IBP_3_3_IV0, FeaturesImage.EMPTY.zkMigrationState()).isEmpty());",
          "110:         assertFalse(new FeaturesImage(FeaturesImage.EMPTY.finalizedVersions(),",
          "111:             FeaturesImage.EMPTY.metadataVersion(), ZkMigrationState.MIGRATION).isEmpty());",
          "112:     }",
          "",
          "---------------"
        ],
        "metadata/src/test/java/org/apache/kafka/image/publisher/SnapshotGeneratorTest.java||metadata/src/test/java/org/apache/kafka/image/publisher/SnapshotGeneratorTest.java": [
          "File: metadata/src/test/java/org/apache/kafka/image/publisher/SnapshotGeneratorTest.java -> metadata/src/test/java/org/apache/kafka/image/publisher/SnapshotGeneratorTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "48:     static class MockEmitter implements SnapshotGenerator.Emitter {",
          "49:         private final CountDownLatch latch = new CountDownLatch(1);",
          "50:         private final List<MetadataImage> images = new CopyOnWriteArrayList<>();",
          "53:         MockEmitter setReady() {",
          "54:             latch.countDown();",
          "",
          "[Removed Lines]",
          "51:         private RuntimeException problem = null;",
          "",
          "[Added Lines]",
          "51:         private volatile RuntimeException problem = null;",
          "",
          "---------------"
        ],
        "metadata/src/test/java/org/apache/kafka/metadata/RecordTestUtils.java||metadata/src/test/java/org/apache/kafka/metadata/RecordTestUtils.java": [
          "File: metadata/src/test/java/org/apache/kafka/metadata/RecordTestUtils.java -> metadata/src/test/java/org/apache/kafka/metadata/RecordTestUtils.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "33: import java.lang.reflect.InvocationTargetException;",
          "34: import java.lang.reflect.Method;",
          "35: import java.util.ArrayList;",
          "36: import java.util.Comparator;",
          "37: import java.util.HashSet;",
          "38: import java.util.Iterator;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "36: import java.util.Collections;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "87:         }",
          "88:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "91:     public static void replayOne(",
          "92:         Object target,",
          "93:         ApiMessageAndVersion recordAndVersion",
          "94:     ) {",
          "95:         replayAll(target, Collections.singletonList(recordAndVersion));",
          "96:     }",
          "",
          "---------------"
        ],
        "metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationDriverTest.java||metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationDriverTest.java": [
          "File: metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationDriverTest.java -> metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationDriverTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "42: import org.junit.jupiter.params.provider.ValueSource;",
          "44: import java.util.Arrays;",
          "45: import java.util.HashMap;",
          "46: import java.util.HashSet;",
          "47: import java.util.List;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "45: import java.util.Collections;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "64:         @Override",
          "65:         public CompletableFuture<?> acceptBatch(List<ApiMessageAndVersion> recordBatch) {",
          "67:         }",
          "69:         @Override",
          "72:         }",
          "74:         @Override",
          "",
          "[Removed Lines]",
          "66:             return null;",
          "70:         public OffsetAndEpoch completeMigration() {",
          "71:             return new OffsetAndEpoch(100, 1);",
          "",
          "[Added Lines]",
          "67:             return CompletableFuture.completedFuture(null);",
          "71:         public CompletableFuture<OffsetAndEpoch> completeMigration() {",
          "72:             return CompletableFuture.completedFuture(new OffsetAndEpoch(100, 1));",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "82:         private final Set<Integer> brokerIds;",
          "83:         public final Map<ConfigResource, Map<String, String>> capturedConfigs = new HashMap<>();",
          "85:         public CapturingMigrationClient(Set<Integer> brokerIdsInZk) {",
          "86:             this.brokerIds = brokerIdsInZk;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "85:         private ZkMigrationLeadershipState state = null;",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "89:         @Override",
          "90:         public ZkMigrationLeadershipState getOrCreateMigrationRecoveryState(ZkMigrationLeadershipState initialState) {",
          "92:         }",
          "94:         @Override",
          "95:         public ZkMigrationLeadershipState setMigrationRecoveryState(ZkMigrationLeadershipState state) {",
          "96:             return state;",
          "97:         }",
          "99:         @Override",
          "100:         public ZkMigrationLeadershipState claimControllerLeadership(ZkMigrationLeadershipState state) {",
          "101:             return state;",
          "102:         }",
          "104:         @Override",
          "105:         public ZkMigrationLeadershipState releaseControllerLeadership(ZkMigrationLeadershipState state) {",
          "106:             return state;",
          "107:         }",
          "",
          "[Removed Lines]",
          "91:             return initialState;",
          "",
          "[Added Lines]",
          "93:             if (state == null) {",
          "94:                 state = initialState;",
          "95:             }",
          "96:             return state;",
          "101:             this.state = state;",
          "107:             this.state = state;",
          "113:             this.state = state;",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "113:             Map<Integer, PartitionRegistration> topicPartitions,",
          "114:             ZkMigrationLeadershipState state",
          "115:         ) {",
          "116:             return state;",
          "117:         }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "124:             this.state = state;",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "121:             Map<String, Map<Integer, PartitionRegistration>> topicPartitions,",
          "122:             ZkMigrationLeadershipState state",
          "123:         ) {",
          "124:             return state;",
          "125:         }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "133:             this.state = state;",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "131:             ZkMigrationLeadershipState state",
          "132:         ) {",
          "133:             capturedConfigs.computeIfAbsent(configResource, __ -> new HashMap<>()).putAll(configMap);",
          "134:             return state;",
          "135:         }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "144:             this.state = state;",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "140:             Map<String, Double> quotas,",
          "141:             ZkMigrationLeadershipState state",
          "142:         ) {",
          "143:             return state;",
          "144:         }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "154:             this.state = state;",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "148:             long nextProducerId,",
          "149:             ZkMigrationLeadershipState state",
          "150:         ) {",
          "151:             return state;",
          "152:         }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "163:             this.state = state;",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "157:             List<AccessControlEntry> deletedAcls,",
          "158:             ZkMigrationLeadershipState state",
          "159:         ) {",
          "160:             return state;",
          "161:         }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "173:             this.state = state;",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "166:             List<AccessControlEntry> addedAcls,",
          "167:             ZkMigrationLeadershipState state",
          "168:         ) {",
          "169:             return state;",
          "170:         }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "183:             this.state = state;",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "290:         MetadataDelta delta = new MetadataDelta(image);",
          "292:         driver.start();",
          "293:         delta.replay(zkBrokerRecord(1));",
          "294:         delta.replay(zkBrokerRecord(2));",
          "295:         delta.replay(zkBrokerRecord(3));",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "308:         delta.replay(ZkMigrationState.PRE_MIGRATION.toRecord().message());",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "371:             MetadataDelta delta = new MetadataDelta(image);",
          "373:             driver.start();",
          "374:             delta.replay(zkBrokerRecord(1));",
          "375:             delta.replay(zkBrokerRecord(2));",
          "376:             delta.replay(zkBrokerRecord(3));",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "390:             delta.replay(ZkMigrationState.PRE_MIGRATION.toRecord().message());",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "393:             }",
          "394:         }",
          "395:     }",
          "396: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "414:     @Test",
          "415:     public void testSkipWaitForBrokersInDualWrite() throws Exception {",
          "416:         CountingMetadataPropagator metadataPropagator = new CountingMetadataPropagator();",
          "417:         CapturingMigrationClient migrationClient = new CapturingMigrationClient(Collections.emptySet());",
          "418:         MockFaultHandler faultHandler = new MockFaultHandler(\"testMigrationClientExpiration\");",
          "419:         try (KRaftMigrationDriver driver = new KRaftMigrationDriver(",
          "420:                 3000,",
          "421:                 new NoOpRecordConsumer(),",
          "422:                 migrationClient,",
          "423:                 metadataPropagator,",
          "424:                 metadataPublisher -> { },",
          "425:                 faultHandler",
          "426:         )) {",
          "427:             MetadataImage image = MetadataImage.EMPTY;",
          "428:             MetadataDelta delta = new MetadataDelta(image);",
          "431:             migrationClient.setMigrationRecoveryState(",
          "432:                 ZkMigrationLeadershipState.EMPTY.withKRaftMetadataOffsetAndEpoch(100, 1));",
          "434:             driver.start();",
          "435:             delta.replay(ZkMigrationState.PRE_MIGRATION.toRecord().message());",
          "436:             delta.replay(zkBrokerRecord(1));",
          "437:             delta.replay(zkBrokerRecord(2));",
          "438:             delta.replay(zkBrokerRecord(3));",
          "439:             delta.replay(ZkMigrationState.MIGRATION.toRecord().message());",
          "440:             MetadataProvenance provenance = new MetadataProvenance(100, 1, 1);",
          "441:             image = delta.apply(provenance);",
          "443:             driver.onControllerChange(new LeaderAndEpoch(OptionalInt.of(3000), 1));",
          "444:             driver.onMetadataUpdate(delta, image, new LogDeltaManifest(provenance,",
          "445:                 new LeaderAndEpoch(OptionalInt.of(3000), 1), 1, 100, 42));",
          "447:             TestUtils.waitForCondition(() -> driver.migrationState().get(1, TimeUnit.MINUTES).equals(MigrationDriverState.DUAL_WRITE),",
          "448:                 \"Waiting for KRaftMigrationDriver to enter ZK_MIGRATION state\");",
          "449:         }",
          "450:     }",
          "",
          "---------------"
        ],
        "tests/kafkatest/tests/core/zookeeper_migration_test.py||tests/kafkatest/tests/core/zookeeper_migration_test.py": [
          "File: tests/kafkatest/tests/core/zookeeper_migration_test.py -> tests/kafkatest/tests/core/zookeeper_migration_test.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: import time",
          "19: from ducktape.utils.util import wait_until",
          "21: from kafkatest.services.console_consumer import ConsoleConsumer",
          "22: from kafkatest.services.kafka import KafkaService",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: from ducktape.mark import parametrize",
          "21: from ducktape.errors import TimeoutError",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "26: from kafkatest.services.zookeeper import ZookeeperService",
          "27: from kafkatest.tests.produce_consume_validate import ProduceConsumeValidateTest",
          "28: from kafkatest.utils import is_int",
          "32: class TestMigration(ProduceConsumeValidateTest):",
          "",
          "[Removed Lines]",
          "29: from kafkatest.version import DEV_BRANCH",
          "",
          "[Added Lines]",
          "31: from kafkatest.version import DEV_BRANCH, V_3_4_0",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "115:                                         message_validator=is_int, version=DEV_BRANCH)",
          "117:         self.run_produce_consume_validate(core_test_action=self.do_migration)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "121:     @parametrize(metadata_quorum=isolated_kraft)",
          "122:     def test_pre_migration_mode_3_4(self, metadata_quorum):",
          "123:         \"\"\"",
          "124:         Start a KRaft quorum in 3.4 without migrations enabled. Since we were not correctly writing",
          "125:         ZkMigrationStateRecord in 3.4, there will be no ZK migration state in the log.",
          "127:         When upgrading to 3.5+, the controller should see that there are records in the log and",
          "128:         automatically bootstrap a ZkMigrationStateRecord(NONE) into the log (indicating that this",
          "129:         cluster was created in KRaft mode).",
          "131:         This test ensures that even if we enable migrations after the upgrade to 3.5, that no migration",
          "132:         is able to take place.",
          "133:         \"\"\"",
          "134:         self.zk = ZookeeperService(self.test_context, num_nodes=1, version=V_3_4_0)",
          "135:         self.zk.start()",
          "137:         self.kafka = KafkaService(self.test_context,",
          "138:                                   num_nodes=3,",
          "139:                                   zk=self.zk,",
          "140:                                   allow_zk_with_kraft=True,",
          "141:                                   version=V_3_4_0,",
          "142:                                   server_prop_overrides=[[\"zookeeper.metadata.migration.enable\", \"false\"]],",
          "143:                                   topics={self.topic: {\"partitions\": self.partitions,",
          "144:                                                        \"replication-factor\": self.replication_factor,",
          "145:                                                        'configs': {\"min.insync.replicas\": 2}}})",
          "146:         self.kafka.start()",
          "148:         # Now reconfigure the cluster as if we're trying to do a migration",
          "149:         self.kafka.server_prop_overrides.clear()",
          "150:         self.kafka.server_prop_overrides.extend([",
          "151:             [\"zookeeper.metadata.migration.enable\", \"true\"]",
          "152:         ])",
          "154:         self.logger.info(\"Performing rolling upgrade.\")",
          "155:         for node in self.kafka.controller_quorum.nodes:",
          "156:             self.logger.info(\"Stopping controller node %s\" % node.account.hostname)",
          "157:             self.kafka.controller_quorum.stop_node(node)",
          "158:             node.version = DEV_BRANCH",
          "159:             self.logger.info(\"Restarting controller node %s\" % node.account.hostname)",
          "160:             self.kafka.controller_quorum.start_node(node)",
          "161:             # Controller should crash",
          "163:         # Check the controller's logs for the error message about the migration state",
          "164:         saw_expected_error = False",
          "165:         for node in self.kafka.controller_quorum.nodes:",
          "166:             wait_until(lambda: not self.kafka.controller_quorum.alive(node), timeout_sec=60,",
          "167:                        backoff_sec=1, err_msg=\"Controller did not halt in the expected amount of time\")",
          "168:             with node.account.monitor_log(KafkaService.STDOUT_STDERR_CAPTURE) as monitor:",
          "169:                 monitor.offset = 0",
          "170:                 try:",
          "171:                     # Shouldn't have to wait too long to see this log message after startup",
          "172:                     monitor.wait_until(",
          "173:                         \"Should not have ZK migrations enabled on a cluster that was created in KRaft mode.\",",
          "174:                         timeout_sec=10.0, backoff_sec=.25,",
          "175:                         err_msg=\"\"",
          "176:                     )",
          "177:                     saw_expected_error = True",
          "178:                     break",
          "179:                 except TimeoutError:",
          "180:                     continue",
          "182:         assert saw_expected_error, \"Did not see expected ERROR log in the controller logs\"",
          "184:     def test_upgrade_after_3_4_migration(self):",
          "185:         \"\"\"",
          "186:         Perform a migration on version 3.4.0. Then do a rolling upgrade to 3.5+ and ensure we see",
          "187:         the correct migration state in the log.",
          "188:         \"\"\"",
          "189:         zk_quorum = partial(ServiceQuorumInfo, zk)",
          "190:         self.zk = ZookeeperService(self.test_context, num_nodes=1, version=V_3_4_0)",
          "191:         self.kafka = KafkaService(self.test_context,",
          "192:                                   num_nodes=3,",
          "193:                                   zk=self.zk,",
          "194:                                   version=V_3_4_0,",
          "195:                                   quorum_info_provider=zk_quorum,",
          "196:                                   allow_zk_with_kraft=True,",
          "197:                                   server_prop_overrides=[[\"zookeeper.metadata.migration.enable\", \"true\"]])",
          "199:         remote_quorum = partial(ServiceQuorumInfo, isolated_kraft)",
          "200:         controller = KafkaService(self.test_context, num_nodes=1, zk=self.zk, version=V_3_4_0,",
          "201:                                   allow_zk_with_kraft=True,",
          "202:                                   isolated_kafka=self.kafka,",
          "203:                                   server_prop_overrides=[[\"zookeeper.connect\", self.zk.connect_setting()],",
          "204:                                                          [\"zookeeper.metadata.migration.enable\", \"true\"]],",
          "205:                                   quorum_info_provider=remote_quorum)",
          "207:         self.kafka.security_protocol = \"PLAINTEXT\"",
          "208:         self.kafka.interbroker_security_protocol = \"PLAINTEXT\"",
          "209:         self.zk.start()",
          "211:         controller.start()",
          "213:         self.logger.info(\"Pre-generating clusterId for ZK.\")",
          "214:         cluster_id_json = \"\"\"{\"version\": \"1\", \"id\": \"%s\"}\"\"\" % CLUSTER_ID",
          "215:         self.zk.create(path=\"/cluster\")",
          "216:         self.zk.create(path=\"/cluster/id\", value=cluster_id_json)",
          "217:         self.kafka.reconfigure_zk_for_migration(controller)",
          "218:         self.kafka.start()",
          "220:         topic_cfg = {",
          "221:             \"topic\": self.topic,",
          "222:             \"partitions\": self.partitions,",
          "223:             \"replication-factor\": self.replication_factor,",
          "224:             \"configs\": {\"min.insync.replicas\": 2}",
          "225:         }",
          "226:         self.kafka.create_topic(topic_cfg)",
          "228:         # Now we're in dual-write mode. The 3.4 controller will have written a PRE_MIGRATION record (1) into the log.",
          "229:         # We now upgrade the controller to 3.5+ where 1 is redefined as MIGRATION.",
          "230:         for node in controller.nodes:",
          "231:             self.logger.info(\"Stopping controller node %s\" % node.account.hostname)",
          "232:             self.kafka.controller_quorum.stop_node(node)",
          "233:             node.version = DEV_BRANCH",
          "234:             self.logger.info(\"Restarting controller node %s\" % node.account.hostname)",
          "235:             self.kafka.controller_quorum.start_node(node)",
          "236:             self.wait_until_rejoin()",
          "237:             self.logger.info(\"Successfully restarted controller node %s\" % node.account.hostname)",
          "239:         # Check the controller's logs for the INFO message that we're still in the migration state",
          "240:         saw_expected_log = False",
          "241:         for node in self.kafka.controller_quorum.nodes:",
          "242:             with node.account.monitor_log(KafkaService.STDOUT_STDERR_CAPTURE) as monitor:",
          "243:                 monitor.offset = 0",
          "244:                 try:",
          "245:                     # Shouldn't have to wait too long to see this log message after startup",
          "246:                     monitor.wait_until(",
          "247:                         \"Staying in the ZK migration\",",
          "248:                         timeout_sec=10.0, backoff_sec=.25,",
          "249:                         err_msg=\"\"",
          "250:                     )",
          "251:                     saw_expected_log = True",
          "252:                     break",
          "253:                 except TimeoutError:",
          "254:                     continue",
          "256:         assert saw_expected_log, \"Did not see expected INFO log after upgrading from a 3.4 migration\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "9318b591d7a57b9db1e7519986d78f0402cd5b5e",
      "candidate_info": {
        "commit_hash": "9318b591d7a57b9db1e7519986d78f0402cd5b5e",
        "repo": "apache/kafka",
        "commit_url": "https://github.com/apache/kafka/commit/9318b591d7a57b9db1e7519986d78f0402cd5b5e",
        "files": [
          "core/src/main/scala/kafka/server/BrokerServer.scala",
          "core/src/main/scala/kafka/server/ControllerServer.scala",
          "core/src/main/scala/kafka/server/metadata/AclPublisher.scala",
          "core/src/main/scala/kafka/server/metadata/BrokerMetadataPublisher.scala",
          "core/src/test/scala/unit/kafka/security/authorizer/AuthorizerTest.scala",
          "core/src/test/scala/unit/kafka/server/metadata/BrokerMetadataPublisherTest.scala",
          "metadata/src/main/java/org/apache/kafka/controller/AclControlManager.java",
          "metadata/src/main/java/org/apache/kafka/controller/QuorumController.java",
          "metadata/src/main/java/org/apache/kafka/image/AclsDelta.java",
          "metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java",
          "metadata/src/test/java/org/apache/kafka/controller/MockAclControlManager.java",
          "metadata/src/test/java/org/apache/kafka/controller/MockAclMutator.java",
          "metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java",
          "metadata/src/test/java/org/apache/kafka/metadata/RecordTestUtils.java",
          "metadata/src/test/java/org/apache/kafka/metadata/authorizer/MockAclMutator.java"
        ],
        "message": "KAFKA-15318: Update the Authorizer via AclPublisher (#14169)\n\nOn the controller, move publishing acls to the Authorizer into a dedicated MetadataPublisher,\nAclPublisher. This publisher listens for notifications from MetadataLoader, and receives only\ncommitted data. This brings the controller side in line with how the broker has always worked. It\nalso avoids some ugly code related to publishing directly from the QuorumController. Most important\nof all, it clears the way to implement metadata transactions without worrying about Authorizer\nstate (since it will be handled by the MetadataLoader, along with other metadata image state).\n\nIn AclsDelta, we can remove isSnapshotDelta. We always know when the MetadataLoader is giving us a\nsnapshot. Also bring AclsDelta in line with the other delta classes, where completeSnapshot\ncalculates the diff between the previous image and the next one. We don't use this delta (since we\njust apply the image directly to the authorizer) but we should have it, for consistency.\n\nFinally, change MockAclMutator to avoid the need to subclass AclControlManager.\n\nReviewers: David Arthur <mumrah@gmail.com>",
        "before_after_code_files": [
          "core/src/main/scala/kafka/server/BrokerServer.scala||core/src/main/scala/kafka/server/BrokerServer.scala",
          "core/src/main/scala/kafka/server/ControllerServer.scala||core/src/main/scala/kafka/server/ControllerServer.scala",
          "core/src/main/scala/kafka/server/metadata/AclPublisher.scala||core/src/main/scala/kafka/server/metadata/AclPublisher.scala",
          "core/src/main/scala/kafka/server/metadata/BrokerMetadataPublisher.scala||core/src/main/scala/kafka/server/metadata/BrokerMetadataPublisher.scala",
          "core/src/test/scala/unit/kafka/security/authorizer/AuthorizerTest.scala||core/src/test/scala/unit/kafka/security/authorizer/AuthorizerTest.scala",
          "core/src/test/scala/unit/kafka/server/metadata/BrokerMetadataPublisherTest.scala||core/src/test/scala/unit/kafka/server/metadata/BrokerMetadataPublisherTest.scala",
          "metadata/src/main/java/org/apache/kafka/controller/AclControlManager.java||metadata/src/main/java/org/apache/kafka/controller/AclControlManager.java",
          "metadata/src/main/java/org/apache/kafka/controller/QuorumController.java||metadata/src/main/java/org/apache/kafka/controller/QuorumController.java",
          "metadata/src/main/java/org/apache/kafka/image/AclsDelta.java||metadata/src/main/java/org/apache/kafka/image/AclsDelta.java",
          "metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java||metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java",
          "metadata/src/test/java/org/apache/kafka/controller/MockAclControlManager.java||metadata/src/test/java/org/apache/kafka/controller/MockAclControlManager.java",
          "metadata/src/test/java/org/apache/kafka/controller/MockAclMutator.java||metadata/src/test/java/org/apache/kafka/controller/MockAclMutator.java",
          "metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java||metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java",
          "metadata/src/test/java/org/apache/kafka/metadata/RecordTestUtils.java||metadata/src/test/java/org/apache/kafka/metadata/RecordTestUtils.java",
          "metadata/src/test/java/org/apache/kafka/metadata/authorizer/MockAclMutator.java||metadata/src/test/java/org/apache/kafka/metadata/authorizer/MockAclMutator.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "metadata/src/main/java/org/apache/kafka/image/AclsDelta.java||metadata/src/main/java/org/apache/kafka/image/AclsDelta.java"
          ],
          "candidate": [
            "metadata/src/main/java/org/apache/kafka/image/AclsDelta.java||metadata/src/main/java/org/apache/kafka/image/AclsDelta.java"
          ]
        }
      },
      "candidate_diff": {
        "core/src/main/scala/kafka/server/BrokerServer.scala||core/src/main/scala/kafka/server/BrokerServer.scala": [
          "File: core/src/main/scala/kafka/server/BrokerServer.scala -> core/src/main/scala/kafka/server/BrokerServer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: import kafka.network.{DataPlaneAcceptor, SocketServer}",
          "26: import kafka.raft.KafkaRaftManager",
          "27: import kafka.security.CredentialProvider",
          "29: import kafka.utils.CoreUtils",
          "30: import org.apache.kafka.clients.NetworkClient",
          "31: import org.apache.kafka.common.config.ConfigException",
          "",
          "[Removed Lines]",
          "28: import kafka.server.metadata.{BrokerMetadataPublisher, ClientQuotaMetadataManager, DynamicClientQuotaPublisher, DynamicConfigPublisher, KRaftMetadataCache, ScramPublisher}",
          "",
          "[Added Lines]",
          "28: import kafka.server.metadata.{AclPublisher, BrokerMetadataPublisher, ClientQuotaMetadataManager, DynamicClientQuotaPublisher, DynamicConfigPublisher, KRaftMetadataCache, ScramPublisher}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "42: import org.apache.kafka.coordinator.group.util.SystemTimerReaper",
          "43: import org.apache.kafka.coordinator.group.{GroupCoordinator, GroupCoordinatorConfig, GroupCoordinatorService, RecordSerde}",
          "44: import org.apache.kafka.image.publisher.MetadataPublisher",
          "46: import org.apache.kafka.metadata.{BrokerState, VersionRange}",
          "47: import org.apache.kafka.raft.RaftConfig",
          "48: import org.apache.kafka.server.authorizer.Authorizer",
          "",
          "[Removed Lines]",
          "45: import org.apache.kafka.metadata.authorizer.ClusterMetadataAuthorizer",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "419:           sharedServer.metadataPublishingFaultHandler,",
          "420:           \"broker\",",
          "421:           credentialProvider),",
          "423:         sharedServer.initialBrokerMetadataLoadFaultHandler,",
          "424:         sharedServer.metadataPublishingFaultHandler)",
          "425:       metadataPublishers.add(brokerMetadataPublisher)",
          "",
          "[Removed Lines]",
          "422:         authorizer,",
          "",
          "[Added Lines]",
          "421:         new AclPublisher(",
          "422:           config.nodeId,",
          "423:           sharedServer.metadataPublishingFaultHandler,",
          "424:           \"broker\",",
          "425:           authorizer",
          "426:         ),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "468:         rlm.startup()",
          "469:       })",
          "481:       FutureUtils.waitWithLogging(logger.underlying, logIdent,",
          "",
          "[Removed Lines]",
          "473:       authorizer match {",
          "474:         case Some(clusterMetadataAuthorizer: ClusterMetadataAuthorizer) =>",
          "475:           clusterMetadataAuthorizer.completeInitialLoad()",
          "476:         case _ => // nothing to do",
          "477:       }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "core/src/main/scala/kafka/server/ControllerServer.scala||core/src/main/scala/kafka/server/ControllerServer.scala": [
          "File: core/src/main/scala/kafka/server/ControllerServer.scala -> core/src/main/scala/kafka/server/ControllerServer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import kafka.server.QuotaFactory.QuotaManagers",
          "28: import scala.collection.immutable",
          "30: import kafka.utils.{CoreUtils, Logging, PasswordEncoder}",
          "31: import kafka.zk.{KafkaZkClient, ZkMigrationClient}",
          "32: import org.apache.kafka.common.config.ConfigException",
          "",
          "[Removed Lines]",
          "29: import kafka.server.metadata.{ClientQuotaMetadataManager, DynamicClientQuotaPublisher, DynamicConfigPublisher, ScramPublisher}",
          "",
          "[Added Lines]",
          "29: import kafka.server.metadata.{AclPublisher, ClientQuotaMetadataManager, DynamicClientQuotaPublisher, DynamicConfigPublisher, ScramPublisher}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "238:           setNonFatalFaultHandler(sharedServer.nonFatalQuorumControllerFaultHandler).",
          "239:           setZkMigrationEnabled(config.migrationEnabled)",
          "240:       }",
          "241:       authorizer match {",
          "244:       }",
          "247:       if (config.migrationEnabled) {",
          "248:         val zkClient = KafkaZkClient.createZkClient(\"KRaft Migration\", time, config, KafkaServer.zkClientConfigFromKafkaConfig(config))",
          "",
          "[Removed Lines]",
          "242:         case Some(a: ClusterMetadataAuthorizer) => controllerBuilder.setAuthorizer(a)",
          "243:         case _ => // nothing to do",
          "245:       controller = controllerBuilder.build()",
          "",
          "[Added Lines]",
          "241:       controller = controllerBuilder.build()",
          "246:         case Some(a: ClusterMetadataAuthorizer) => a.setAclMutator(controller)",
          "247:         case _ =>",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "361:         sharedServer.metadataPublishingFaultHandler",
          "362:       ))",
          "365:       FutureUtils.waitWithLogging(logger.underlying, logIdent,",
          "366:         \"the controller metadata publishers to be installed\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "368:       metadataPublishers.add(new AclPublisher(",
          "369:         config.nodeId,",
          "370:         sharedServer.metadataPublishingFaultHandler,",
          "371:         \"controller\",",
          "372:         authorizer",
          "373:       ))",
          "",
          "---------------"
        ],
        "core/src/main/scala/kafka/server/metadata/AclPublisher.scala||core/src/main/scala/kafka/server/metadata/AclPublisher.scala": [
          "File: core/src/main/scala/kafka/server/metadata/AclPublisher.scala -> core/src/main/scala/kafka/server/metadata/AclPublisher.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package kafka.server.metadata",
          "20: import kafka.utils.Logging",
          "21: import org.apache.kafka.image.loader.{LoaderManifest, LoaderManifestType}",
          "22: import org.apache.kafka.image.{MetadataDelta, MetadataImage}",
          "23: import org.apache.kafka.metadata.authorizer.ClusterMetadataAuthorizer",
          "24: import org.apache.kafka.server.authorizer.Authorizer",
          "25: import org.apache.kafka.server.fault.FaultHandler",
          "27: import scala.concurrent.TimeoutException",
          "30: class AclPublisher(",
          "31:   nodeId: Int,",
          "32:   faultHandler: FaultHandler,",
          "33:   nodeType: String,",
          "34:   authorizer: Option[Authorizer],",
          "35: ) extends Logging with org.apache.kafka.image.publisher.MetadataPublisher {",
          "36:   logIdent = s\"[${name()}] \"",
          "38:   override def name(): String = s\"AclPublisher ${nodeType} id=${nodeId}\"",
          "40:   var completedInitialLoad = false",
          "42:   override def onMetadataUpdate(",
          "43:     delta: MetadataDelta,",
          "44:     newImage: MetadataImage,",
          "45:     manifest: LoaderManifest",
          "46:   ): Unit = {",
          "47:     val deltaName = s\"MetadataDelta up to ${newImage.offset()}\"",
          "55:     Option(delta.aclsDelta()).foreach { aclsDelta =>",
          "56:       authorizer match {",
          "57:         case Some(authorizer: ClusterMetadataAuthorizer) => if (manifest.`type`().equals(LoaderManifestType.SNAPSHOT)) {",
          "58:           try {",
          "63:             info(s\"Loading authorizer snapshot at offset ${newImage.offset()}\")",
          "64:             authorizer.loadSnapshot(newImage.acls().acls())",
          "65:           } catch {",
          "66:             case t: Throwable => faultHandler.handleFault(\"Error loading \" +",
          "67:               s\"authorizer snapshot in $deltaName\", t)",
          "68:           }",
          "69:         } else {",
          "70:           try {",
          "73:             aclsDelta.changes().entrySet().forEach(e =>",
          "74:               if (e.getValue.isPresent) {",
          "75:                 authorizer.addAcl(e.getKey, e.getValue.get())",
          "76:               } else {",
          "77:                 authorizer.removeAcl(e.getKey)",
          "78:               })",
          "79:           } catch {",
          "80:             case t: Throwable => faultHandler.handleFault(\"Error loading \" +",
          "81:               s\"authorizer changes in $deltaName\", t)",
          "82:           }",
          "83:         }",
          "84:         if (!completedInitialLoad) {",
          "88:           completedInitialLoad = true",
          "89:           authorizer.completeInitialLoad()",
          "90:         }",
          "91:         case _ => // No ClusterMetadataAuthorizer is configured. There is nothing to do.",
          "92:       }",
          "93:     }",
          "94:   }",
          "96:   override def close(): Unit = {",
          "97:     authorizer match {",
          "98:       case Some(authorizer: ClusterMetadataAuthorizer) => authorizer.completeInitialLoad(new TimeoutException)",
          "99:       case _ =>",
          "100:     }",
          "101:   }",
          "102: }",
          "",
          "---------------"
        ],
        "core/src/main/scala/kafka/server/metadata/BrokerMetadataPublisher.scala||core/src/main/scala/kafka/server/metadata/BrokerMetadataPublisher.scala": [
          "File: core/src/main/scala/kafka/server/metadata/BrokerMetadataPublisher.scala -> core/src/main/scala/kafka/server/metadata/BrokerMetadataPublisher.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: import org.apache.kafka.image.loader.LoaderManifest",
          "30: import org.apache.kafka.image.publisher.MetadataPublisher",
          "31: import org.apache.kafka.image.{MetadataDelta, MetadataImage, TopicDelta, TopicsImage}",
          "34: import org.apache.kafka.server.fault.FaultHandler",
          "36: import java.util.concurrent.CompletableFuture",
          "",
          "[Removed Lines]",
          "32: import org.apache.kafka.metadata.authorizer.ClusterMetadataAuthorizer",
          "33: import org.apache.kafka.server.authorizer.Authorizer",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "107:   var dynamicConfigPublisher: DynamicConfigPublisher,",
          "108:   dynamicClientQuotaPublisher: DynamicClientQuotaPublisher,",
          "109:   scramPublisher: ScramPublisher,",
          "111:   fatalFaultHandler: FaultHandler,",
          "112:   metadataPublishingFaultHandler: FaultHandler",
          "113: ) extends MetadataPublisher with Logging {",
          "",
          "[Removed Lines]",
          "110:   private val _authorizer: Option[Authorizer],",
          "",
          "[Added Lines]",
          "108:   aclPublisher: AclPublisher,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "230:       scramPublisher.onMetadataUpdate(delta, newImage)",
          "270:       try {",
          "",
          "[Removed Lines]",
          "238:       Option(delta.aclsDelta()).foreach { aclsDelta =>",
          "239:         _authorizer match {",
          "240:           case Some(authorizer: ClusterMetadataAuthorizer) => if (aclsDelta.isSnapshotDelta) {",
          "241:             try {",
          "246:               authorizer.loadSnapshot(newImage.acls().acls())",
          "247:             } catch {",
          "248:               case t: Throwable => metadataPublishingFaultHandler.handleFault(\"Error loading \" +",
          "249:                 s\"authorizer snapshot in $deltaName\", t)",
          "250:             }",
          "251:           } else {",
          "252:             try {",
          "255:               aclsDelta.changes().entrySet().forEach(e =>",
          "256:                 if (e.getValue.isPresent) {",
          "257:                   authorizer.addAcl(e.getKey, e.getValue.get())",
          "258:                 } else {",
          "259:                   authorizer.removeAcl(e.getKey)",
          "260:                 })",
          "261:             } catch {",
          "262:               case t: Throwable => metadataPublishingFaultHandler.handleFault(\"Error loading \" +",
          "263:                 s\"authorizer changes in $deltaName\", t)",
          "264:             }",
          "265:           }",
          "266:           case _ => // No ClusterMetadataAuthorizer is configured. There is nothing to do.",
          "267:         }",
          "268:       }",
          "",
          "[Added Lines]",
          "231:       aclPublisher.onMetadataUpdate(delta, newImage, manifest)",
          "",
          "---------------"
        ],
        "core/src/test/scala/unit/kafka/security/authorizer/AuthorizerTest.scala||core/src/test/scala/unit/kafka/security/authorizer/AuthorizerTest.scala": [
          "File: core/src/test/scala/unit/kafka/security/authorizer/AuthorizerTest.scala -> core/src/test/scala/unit/kafka/security/authorizer/AuthorizerTest.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "35: import org.apache.kafka.common.resource.{PatternType, ResourcePattern, ResourcePatternFilter, ResourceType}",
          "36: import org.apache.kafka.common.security.auth.{KafkaPrincipal, SecurityProtocol}",
          "37: import org.apache.kafka.common.utils.{Time, SecurityUtils => JSecurityUtils}",
          "38: import org.apache.kafka.metadata.authorizer.StandardAuthorizerTest.AuthorizerTestServerInfo",
          "40: import org.apache.kafka.server.authorizer._",
          "41: import org.apache.kafka.server.common.MetadataVersion",
          "42: import org.apache.kafka.server.common.MetadataVersion.{IBP_2_0_IV0, IBP_2_0_IV1}",
          "",
          "[Removed Lines]",
          "39: import org.apache.kafka.metadata.authorizer.{MockAclMutator, StandardAuthorizer}",
          "",
          "[Added Lines]",
          "38: import org.apache.kafka.controller.MockAclMutator",
          "40: import org.apache.kafka.metadata.authorizer.StandardAuthorizer",
          "",
          "---------------"
        ],
        "core/src/test/scala/unit/kafka/server/metadata/BrokerMetadataPublisherTest.scala||core/src/test/scala/unit/kafka/server/metadata/BrokerMetadataPublisherTest.scala": [
          "File: core/src/test/scala/unit/kafka/server/metadata/BrokerMetadataPublisherTest.scala -> core/src/test/scala/unit/kafka/server/metadata/BrokerMetadataPublisherTest.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "289:       mock(classOf[DynamicConfigPublisher]),",
          "290:       mock(classOf[DynamicClientQuotaPublisher]),",
          "291:       mock(classOf[ScramPublisher]),",
          "293:       faultHandler,",
          "294:       faultHandler",
          "295:     )",
          "",
          "[Removed Lines]",
          "292:       None,",
          "",
          "[Added Lines]",
          "292:       mock(classOf[AclPublisher]),",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/controller/AclControlManager.java||metadata/src/main/java/org/apache/kafka/controller/AclControlManager.java": [
          "File: metadata/src/main/java/org/apache/kafka/controller/AclControlManager.java -> metadata/src/main/java/org/apache/kafka/controller/AclControlManager.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: import org.apache.kafka.common.metadata.RemoveAccessControlEntryRecord;",
          "28: import org.apache.kafka.common.requests.ApiError;",
          "29: import org.apache.kafka.common.utils.LogContext;",
          "31: import org.apache.kafka.metadata.authorizer.StandardAcl;",
          "32: import org.apache.kafka.metadata.authorizer.StandardAclWithId;",
          "34: import org.apache.kafka.server.authorizer.AclCreateResult;",
          "35: import org.apache.kafka.server.authorizer.AclDeleteResult;",
          "36: import org.apache.kafka.server.authorizer.AclDeleteResult.AclBindingDeleteResult;",
          "",
          "[Removed Lines]",
          "30: import org.apache.kafka.metadata.authorizer.ClusterMetadataAuthorizer;",
          "33: import org.apache.kafka.raft.OffsetAndEpoch;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "48: import java.util.List;",
          "49: import java.util.Map;",
          "50: import java.util.Map.Entry;",
          "52: import java.util.Set;",
          "53: import java.util.stream.Collectors;",
          "",
          "[Removed Lines]",
          "51: import java.util.Optional;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "72: public class AclControlManager {",
          "73:     static class Builder {",
          "74:         private LogContext logContext = null;",
          "75:         private SnapshotRegistry snapshotRegistry = null;",
          "78:         Builder setLogContext(LogContext logContext) {",
          "79:             this.logContext = logContext;",
          "",
          "[Removed Lines]",
          "76:         private Optional<ClusterMetadataAuthorizer> authorizer = Optional.empty();",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "85:             return this;",
          "86:         }",
          "93:         AclControlManager build() {",
          "94:             if (logContext == null) logContext = new LogContext();",
          "95:             if (snapshotRegistry == null) snapshotRegistry = new SnapshotRegistry(logContext);",
          "97:         }",
          "98:     }",
          "100:     private final Logger log;",
          "101:     private final TimelineHashMap<Uuid, StandardAcl> idToAcl;",
          "102:     private final TimelineHashSet<StandardAcl> existingAcls;",
          "106:         LogContext logContext,",
          "109:     ) {",
          "110:         this.log = logContext.logger(AclControlManager.class);",
          "111:         this.idToAcl = new TimelineHashMap<>(snapshotRegistry, 0);",
          "112:         this.existingAcls = new TimelineHashSet<>(snapshotRegistry, 0);",
          "114:     }",
          "116:     ControllerResult<List<AclCreateResult>> createAcls(List<AclBinding> acls) {",
          "",
          "[Removed Lines]",
          "88:         Builder setClusterMetadataAuthorizer(Optional<ClusterMetadataAuthorizer> authorizer) {",
          "89:             this.authorizer = authorizer;",
          "90:             return this;",
          "91:         }",
          "96:             return new AclControlManager(logContext, snapshotRegistry, authorizer);",
          "103:     private final Optional<ClusterMetadataAuthorizer> authorizer;",
          "105:     AclControlManager(",
          "107:         SnapshotRegistry snapshotRegistry,",
          "108:         Optional<ClusterMetadataAuthorizer> authorizer",
          "113:         this.authorizer = authorizer;",
          "",
          "[Added Lines]",
          "78:             return new AclControlManager(logContext, snapshotRegistry);",
          "86:     private AclControlManager(",
          "88:         SnapshotRegistry snapshotRegistry",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "227:         }",
          "228:     }",
          "234:         StandardAclWithId aclWithId = StandardAclWithId.fromRecord(record);",
          "235:         idToAcl.put(aclWithId.id(), aclWithId.acl());",
          "236:         existingAcls.add(aclWithId.acl());",
          "242:         log.info(\"Replayed AccessControlEntryRecord for {}, setting {}\", record.id(),",
          "243:                 aclWithId.acl());",
          "244:     }",
          "250:         StandardAcl acl = idToAcl.remove(record.id());",
          "251:         if (acl == null) {",
          "252:             throw new RuntimeException(\"Unable to replay \" + record + \": no acl with \" +",
          "",
          "[Removed Lines]",
          "230:     public void replay(",
          "231:         AccessControlEntryRecord record,",
          "232:         Optional<OffsetAndEpoch> snapshotId",
          "233:     ) {",
          "237:         if (!snapshotId.isPresent()) {",
          "238:             authorizer.ifPresent(a -> {",
          "239:                 a.addAcl(aclWithId.id(), aclWithId.acl());",
          "240:             });",
          "241:         }",
          "246:     public void replay(",
          "247:         RemoveAccessControlEntryRecord record,",
          "248:         Optional<OffsetAndEpoch> snapshotId",
          "249:     ) {",
          "",
          "[Added Lines]",
          "209:     public void replay(AccessControlEntryRecord record) {",
          "217:     public void replay(RemoveAccessControlEntryRecord record) {",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "256:             throw new RuntimeException(\"Unable to replay \" + record + \" for \" + acl +",
          "257:                 \": acl not found \" + \"in existingAcls.\");",
          "258:         }",
          "264:         log.info(\"Replayed RemoveAccessControlEntryRecord for {}, removing {}\", record.id(), acl);",
          "265:     }",
          "",
          "[Removed Lines]",
          "259:         if (!snapshotId.isPresent()) {",
          "260:             authorizer.ifPresent(a -> {",
          "261:                 a.removeAcl(record.id());",
          "262:             });",
          "263:         }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/controller/QuorumController.java||metadata/src/main/java/org/apache/kafka/controller/QuorumController.java": [
          "File: metadata/src/main/java/org/apache/kafka/controller/QuorumController.java -> metadata/src/main/java/org/apache/kafka/controller/QuorumController.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "83: import org.apache.kafka.metadata.BrokerRegistrationReply;",
          "84: import org.apache.kafka.metadata.FinalizedControllerFeatures;",
          "85: import org.apache.kafka.metadata.KafkaConfigSchema;",
          "87: import org.apache.kafka.metadata.bootstrap.BootstrapMetadata;",
          "88: import org.apache.kafka.metadata.migration.ZkMigrationState;",
          "89: import org.apache.kafka.metadata.migration.ZkRecordConsumer;",
          "",
          "[Removed Lines]",
          "86: import org.apache.kafka.metadata.authorizer.ClusterMetadataAuthorizer;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "195:         private Optional<CreateTopicPolicy> createTopicPolicy = Optional.empty();",
          "196:         private Optional<AlterConfigPolicy> alterConfigPolicy = Optional.empty();",
          "197:         private ConfigurationValidator configurationValidator = ConfigurationValidator.NO_OP;",
          "199:         private Map<String, Object> staticConfig = Collections.emptyMap();",
          "200:         private BootstrapMetadata bootstrapMetadata = null;",
          "201:         private int maxRecordsPerBatch = MAX_RECORDS_PER_BATCH;",
          "",
          "[Removed Lines]",
          "198:         private Optional<ClusterMetadataAuthorizer> authorizer = Optional.empty();",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "310:             return this;",
          "311:         }",
          "318:         public Builder setStaticConfig(Map<String, Object> staticConfig) {",
          "319:             this.staticConfig = staticConfig;",
          "320:             return this;",
          "",
          "[Removed Lines]",
          "313:         public Builder setAuthorizer(ClusterMetadataAuthorizer authorizer) {",
          "314:             this.authorizer = Optional.of(authorizer);",
          "315:             return this;",
          "316:         }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "373:                     createTopicPolicy,",
          "374:                     alterConfigPolicy,",
          "375:                     configurationValidator,",
          "377:                     staticConfig,",
          "378:                     bootstrapMetadata,",
          "379:                     maxRecordsPerBatch,",
          "",
          "[Removed Lines]",
          "376:                     authorizer,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "965:         public void handleCommit(BatchReader<ApiMessageAndVersion> reader) {",
          "966:             appendRaftEvent(\"handleCommit[baseOffset=\" + reader.baseOffset() + \"]\", () -> {",
          "967:                 try {",
          "969:                     boolean isActive = isActiveController();",
          "970:                     while (reader.hasNext()) {",
          "971:                         Batch<ApiMessageAndVersion> batch = reader.next();",
          "",
          "[Removed Lines]",
          "968:                     maybeCompleteAuthorizerInitialLoad();",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1063:                         reader.lastContainedLogEpoch(),",
          "1064:                         reader.lastContainedLogTimestamp());",
          "1065:                     snapshotRegistry.getOrCreateSnapshot(lastCommittedOffset);",
          "1067:                 } finally {",
          "1068:                     reader.close();",
          "1069:                 }",
          "",
          "[Removed Lines]",
          "1066:                     authorizer.ifPresent(a -> a.loadSnapshot(aclControlManager.idToAcl()));",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "1112:                 if (this != metaLogListener) {",
          "1113:                     log.debug(\"Ignoring {} raft event from an old registration\", name);",
          "1114:                 } else {",
          "1120:                 }",
          "1121:             });",
          "1122:         }",
          "1123:     }",
          "1144:     private boolean isActiveController() {",
          "1145:         return isActiveController(curClaimEpoch);",
          "1146:     }",
          "",
          "[Removed Lines]",
          "1115:                     try {",
          "1116:                         runnable.run();",
          "1117:                     } finally {",
          "1118:                         maybeCompleteAuthorizerInitialLoad();",
          "1119:                     }",
          "1125:     private void maybeCompleteAuthorizerInitialLoad() {",
          "1126:         if (!needToCompleteAuthorizerLoad) return;",
          "1127:         OptionalLong highWatermark = raftClient.highWatermark();",
          "1128:         if (highWatermark.isPresent()) {",
          "1129:             if (lastCommittedOffset + 1 >= highWatermark.getAsLong()) {",
          "1130:                 log.info(\"maybeCompleteAuthorizerInitialLoad: completing authorizer \" +",
          "1131:                     \"initial load at last committed offset {}.\", lastCommittedOffset);",
          "1132:                 authorizer.get().completeInitialLoad();",
          "1133:                 needToCompleteAuthorizerLoad = false;",
          "1134:             } else {",
          "1135:                 log.trace(\"maybeCompleteAuthorizerInitialLoad: can't proceed because \" +",
          "1136:                     \"lastCommittedOffset  = {}, but highWatermark = {}.\",",
          "1137:                     lastCommittedOffset, highWatermark.getAsLong());",
          "1138:             }",
          "1139:         } else {",
          "1140:             log.trace(\"maybeCompleteAuthorizerInitialLoad: highWatermark not set.\");",
          "1141:         }",
          "1142:     }",
          "",
          "[Added Lines]",
          "1105:                     runnable.run();",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "1342:                         lastCommittedEpoch + \" in snapshot registry.\");",
          "1343:             }",
          "1344:             snapshotRegistry.revertToSnapshot(lastCommittedOffset);",
          "1346:             updateWriteOffset(-1);",
          "1347:             clusterControl.deactivate();",
          "1348:             cancelMaybeFenceReplicas();",
          "",
          "[Removed Lines]",
          "1345:             authorizer.ifPresent(a -> a.loadSnapshot(aclControlManager.idToAcl()));",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "1572:                 clusterControl.replay((BrokerRegistrationChangeRecord) message);",
          "1573:                 break;",
          "1574:             case ACCESS_CONTROL_ENTRY_RECORD:",
          "1576:                 break;",
          "1577:             case REMOVE_ACCESS_CONTROL_ENTRY_RECORD:",
          "1579:                 break;",
          "1580:             case USER_SCRAM_CREDENTIAL_RECORD:",
          "1581:                 scramControlManager.replay((UserScramCredentialRecord) message);",
          "",
          "[Removed Lines]",
          "1575:                 aclControlManager.replay((AccessControlEntryRecord) message, snapshotId);",
          "1578:                 aclControlManager.replay((RemoveAccessControlEntryRecord) message, snapshotId);",
          "",
          "[Added Lines]",
          "1541:                 aclControlManager.replay((AccessControlEntryRecord) message);",
          "1544:                 aclControlManager.replay((RemoveAccessControlEntryRecord) message);",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1702:     private final ScramControlManager scramControlManager;",
          "",
          "[Removed Lines]",
          "1708:     private final Optional<ClusterMetadataAuthorizer> authorizer;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "1755:     private long lastCommittedTimestamp = -1;",
          "",
          "[Removed Lines]",
          "1761:     private boolean needToCompleteAuthorizerLoad;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "1834:         Optional<CreateTopicPolicy> createTopicPolicy,",
          "1835:         Optional<AlterConfigPolicy> alterConfigPolicy,",
          "1836:         ConfigurationValidator configurationValidator,",
          "1838:         Map<String, Object> staticConfig,",
          "1839:         BootstrapMetadata bootstrapMetadata,",
          "1840:         int maxRecordsPerBatch,",
          "",
          "[Removed Lines]",
          "1837:         Optional<ClusterMetadataAuthorizer> authorizer,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "1908:             setLogContext(logContext).",
          "1909:             setSnapshotRegistry(snapshotRegistry).",
          "1910:             build();",
          "1913:         this.aclControlManager = new AclControlManager.Builder().",
          "1914:             setLogContext(logContext).",
          "1915:             setSnapshotRegistry(snapshotRegistry).",
          "1917:             build();",
          "1918:         this.logReplayTracker = new LogReplayTracker.Builder().",
          "1919:             setLogContext(logContext).",
          "",
          "[Removed Lines]",
          "1911:         this.authorizer = authorizer;",
          "1912:         authorizer.ifPresent(a -> a.setAclMutator(this));",
          "1916:             setClusterMetadataAuthorizer(authorizer).",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "1923:         this.maxRecordsPerBatch = maxRecordsPerBatch;",
          "1924:         this.metaLogListener = new QuorumMetaLogListener();",
          "1925:         this.curClaimEpoch = -1;",
          "1927:         this.zkRecordConsumer = new MigrationRecordConsumer();",
          "1928:         this.zkMigrationEnabled = zkMigrationEnabled;",
          "1929:         this.recordRedactor = new RecordRedactor(configSchema);",
          "",
          "[Removed Lines]",
          "1926:         this.needToCompleteAuthorizerLoad = authorizer.isPresent();",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "1932:         resetToEmptyState();",
          "1937:         this.raftClient.register(metaLogListener);",
          "1938:     }",
          "",
          "[Removed Lines]",
          "1934:         log.info(\"Creating new QuorumController with clusterId {}, authorizer {}.{}\",",
          "1935:                 clusterId, authorizer, zkMigrationEnabled ? \" ZK migration mode is enabled.\" : \"\");",
          "",
          "[Added Lines]",
          "1883:         log.info(\"Creating new QuorumController with clusterId {}.{}\",",
          "1884:                 clusterId, zkMigrationEnabled ? \" ZK migration mode is enabled.\" : \"\");",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/image/AclsDelta.java||metadata/src/main/java/org/apache/kafka/image/AclsDelta.java": [
          "File: metadata/src/main/java/org/apache/kafka/image/AclsDelta.java -> metadata/src/main/java/org/apache/kafka/image/AclsDelta.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "41:     private final AclsImage image;",
          "42:     private final Map<Uuid, Optional<StandardAcl>> changes = new LinkedHashMap<>();",
          "43:     private final Set<StandardAcl> deleted = new HashSet<>();",
          "46:     public AclsDelta(AclsImage image) {",
          "47:         this.image = image;",
          "",
          "[Removed Lines]",
          "44:     private boolean isSnapshotDelta = false;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "67:     }",
          "69:     void finishSnapshot() {",
          "71:     }",
          "73:     public void handleMetadataVersionChange(MetadataVersion newVersion) {",
          "75:     }",
          "81:     public void replay(AccessControlEntryRecord record) {",
          "82:         StandardAclWithId aclWithId = StandardAclWithId.fromRecord(record);",
          "83:         changes.put(aclWithId.id(), Optional.of(aclWithId.acl()));",
          "",
          "[Removed Lines]",
          "70:         this.isSnapshotDelta = true;",
          "77:     public boolean isSnapshotDelta() {",
          "78:         return isSnapshotDelta;",
          "79:     }",
          "",
          "[Added Lines]",
          "69:         for (Entry<Uuid, StandardAcl> entry : image.acls().entrySet()) {",
          "70:             if (!changes.containsKey(entry.getKey())) {",
          "71:                 changes.put(entry.getKey(), Optional.empty());",
          "72:             }",
          "73:         }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "106:     public AclsImage apply() {",
          "107:         Map<Uuid, StandardAcl> newAcls = new HashMap<>();",
          "116:             }",
          "117:         }",
          "118:         for (Entry<Uuid, Optional<StandardAcl>> entry : changes.entrySet()) {",
          "",
          "[Removed Lines]",
          "108:         if (!isSnapshotDelta) {",
          "109:             for (Entry<Uuid, StandardAcl> entry : image.acls().entrySet()) {",
          "110:                 Optional<StandardAcl> change = changes.get(entry.getKey());",
          "111:                 if (change == null) {",
          "112:                     newAcls.put(entry.getKey(), entry.getValue());",
          "113:                 } else if (change.isPresent()) {",
          "114:                     newAcls.put(entry.getKey(), change.get());",
          "115:                 }",
          "",
          "[Added Lines]",
          "107:         for (Entry<Uuid, StandardAcl> entry : image.acls().entrySet()) {",
          "108:             Optional<StandardAcl> change = changes.get(entry.getKey());",
          "109:             if (change == null) {",
          "110:                 newAcls.put(entry.getKey(), entry.getValue());",
          "111:             } else if (change.isPresent()) {",
          "112:                 newAcls.put(entry.getKey(), change.get());",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "128:     @Override",
          "129:     public String toString() {",
          "131:             \", changes=\" + changes.entrySet().stream().",
          "132:                 map(e -> \"\" + e.getKey() + \"=\" + e.getValue()).",
          "133:                 collect(Collectors.joining(\", \")) + \")\";",
          "",
          "[Removed Lines]",
          "130:         return \"AclsDelta(isSnapshotDelta=\" + isSnapshotDelta +",
          "",
          "[Added Lines]",
          "127:         return \"AclsDelta(\" +",
          "",
          "---------------"
        ],
        "metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java||metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java": [
          "File: metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java -> metadata/src/test/java/org/apache/kafka/controller/AclControlManagerTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "212:         for (StandardAclWithId acl : TEST_ACLS) {",
          "213:             AccessControlEntryRecord record = acl.toRecord();",
          "214:             assertTrue(loadedAcls.add(new ApiMessageAndVersion(record, (short) 0)));",
          "216:         }",
          "",
          "[Removed Lines]",
          "215:             manager.replay(acl.toRecord(), Optional.empty());",
          "",
          "[Added Lines]",
          "215:             manager.replay(acl.toRecord());",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "241:         AclControlManager manager = new AclControlManager.Builder().build();",
          "242:         MockClusterMetadataAuthorizer authorizer = new MockClusterMetadataAuthorizer();",
          "243:         authorizer.loadSnapshot(manager.idToAcl());",
          "245:         manager.replay(new RemoveAccessControlEntryRecord().",
          "247:         assertTrue(manager.idToAcl().isEmpty());",
          "248:     }",
          "",
          "[Removed Lines]",
          "244:         manager.replay(StandardAclWithIdTest.TEST_ACLS.get(0).toRecord(), Optional.empty());",
          "246:             setId(TEST_ACLS.get(0).id()), Optional.empty());",
          "",
          "[Added Lines]",
          "244:         manager.replay(StandardAclWithIdTest.TEST_ACLS.get(0).toRecord());",
          "246:             setId(TEST_ACLS.get(0).id()));",
          "",
          "---------------"
        ],
        "metadata/src/test/java/org/apache/kafka/controller/MockAclControlManager.java||metadata/src/test/java/org/apache/kafka/controller/MockAclControlManager.java": [
          "File: metadata/src/test/java/org/apache/kafka/controller/MockAclControlManager.java -> metadata/src/test/java/org/apache/kafka/controller/MockAclControlManager.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "metadata/src/test/java/org/apache/kafka/controller/MockAclMutator.java||metadata/src/test/java/org/apache/kafka/controller/MockAclMutator.java": [
          "File: metadata/src/test/java/org/apache/kafka/controller/MockAclMutator.java -> metadata/src/test/java/org/apache/kafka/controller/MockAclMutator.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.kafka.controller;",
          "20: import org.apache.kafka.common.Uuid;",
          "21: import org.apache.kafka.common.acl.AclBinding;",
          "22: import org.apache.kafka.common.acl.AclBindingFilter;",
          "23: import org.apache.kafka.metadata.RecordTestUtils;",
          "24: import org.apache.kafka.metadata.authorizer.AclMutator;",
          "25: import org.apache.kafka.metadata.authorizer.StandardAcl;",
          "26: import org.apache.kafka.metadata.authorizer.StandardAuthorizer;",
          "27: import org.apache.kafka.server.authorizer.AclCreateResult;",
          "28: import org.apache.kafka.server.authorizer.AclDeleteResult;",
          "30: import java.util.HashMap;",
          "31: import java.util.List;",
          "32: import java.util.Map;",
          "33: import java.util.Map.Entry;",
          "34: import java.util.concurrent.CompletableFuture;",
          "42: public class MockAclMutator implements AclMutator {",
          "43:     private final StandardAuthorizer authorizer;",
          "44:     private final AclControlManager aclControl;",
          "46:     public MockAclMutator(",
          "47:         StandardAuthorizer authorizer",
          "48:     ) {",
          "49:         this.authorizer = authorizer;",
          "50:         this.aclControl = new AclControlManager.Builder().build();",
          "51:     }",
          "53:     private void syncIdToAcl(",
          "54:         Map<Uuid, StandardAcl> prevIdToAcl,",
          "55:         Map<Uuid, StandardAcl> nextIdToAcl",
          "56:     ) {",
          "57:         for (Entry<Uuid, StandardAcl> entry : prevIdToAcl.entrySet()) {",
          "58:             if (!entry.getValue().equals(nextIdToAcl.get(entry.getKey()))) {",
          "59:                 authorizer.removeAcl(entry.getKey());",
          "60:             }",
          "61:         }",
          "62:         for (Entry<Uuid, StandardAcl> entry : nextIdToAcl.entrySet()) {",
          "63:             if (!entry.getValue().equals(prevIdToAcl.get(entry.getKey()))) {",
          "64:                 authorizer.addAcl(entry.getKey(), entry.getValue());",
          "65:             }",
          "66:         }",
          "67:     }",
          "69:     @Override",
          "70:     public synchronized CompletableFuture<List<AclCreateResult>> createAcls(",
          "71:         ControllerRequestContext context,",
          "72:         List<AclBinding> aclBindings",
          "73:     ) {",
          "74:         Map<Uuid, StandardAcl> prevIdToAcl = new HashMap<>(aclControl.idToAcl());",
          "75:         ControllerResult<List<AclCreateResult>> result = aclControl.createAcls(aclBindings);",
          "76:         RecordTestUtils.replayAll(aclControl, result.records());",
          "77:         syncIdToAcl(prevIdToAcl, aclControl.idToAcl());",
          "78:         return CompletableFuture.completedFuture(result.response());",
          "79:     }",
          "81:     @Override",
          "82:     public synchronized CompletableFuture<List<AclDeleteResult>> deleteAcls(",
          "83:         ControllerRequestContext context,",
          "84:         List<AclBindingFilter> aclBindingFilters",
          "85:     ) {",
          "86:         Map<Uuid, StandardAcl> prevIdToAcl = new HashMap<>(aclControl.idToAcl());",
          "87:         ControllerResult<List<AclDeleteResult>> result = aclControl.deleteAcls(aclBindingFilters);",
          "88:         RecordTestUtils.replayAll(aclControl, result.records());",
          "89:         syncIdToAcl(prevIdToAcl, aclControl.idToAcl());",
          "90:         return CompletableFuture.completedFuture(result.response());",
          "91:     }",
          "92: }",
          "",
          "---------------"
        ],
        "metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java||metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java": [
          "File: metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java -> metadata/src/test/java/org/apache/kafka/controller/QuorumControllerTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "979:     private static final Map<Integer, Long> ALL_ZERO_BROKER_EPOCHS =",
          "980:         IntStream.of(0, 1, 2, 3).boxed().collect(Collectors.toMap(identity(), __ -> 0L));",
          "1020:     @Test",
          "1021:     public void testFatalMetadataReplayErrorOnActive() throws Throwable {",
          "1022:         try (",
          "",
          "[Removed Lines]",
          "982:     @Test",
          "983:     public void testQuorumControllerCompletesAuthorizerInitialLoad() throws Throwable {",
          "984:         final int numControllers = 3;",
          "985:         List<StandardAuthorizer> authorizers = new ArrayList<>(numControllers);",
          "986:         for (int i = 0; i < numControllers; i++) {",
          "987:             StandardAuthorizer authorizer = new StandardAuthorizer();",
          "988:             authorizer.configure(Collections.emptyMap());",
          "989:             authorizers.add(authorizer);",
          "990:         }",
          "991:         try (",
          "992:             LocalLogManagerTestEnv logEnv = new LocalLogManagerTestEnv.Builder(numControllers).",
          "993:                 setSharedLogDataInitializer(sharedLogData -> {",
          "994:                     sharedLogData.setInitialMaxReadOffset(2);",
          "995:                 }).",
          "996:                 build()",
          "997:         ) {",
          "998:             logEnv.appendInitialRecords(generateTestRecords(FOO_ID, ALL_ZERO_BROKER_EPOCHS));",
          "999:             logEnv.logManagers().forEach(m -> m.setMaxReadOffset(2));",
          "1000:             try (",
          "1001:                 QuorumControllerTestEnv controlEnv = new QuorumControllerTestEnv.Builder(logEnv).",
          "1002:                     setControllerBuilderInitializer(controllerBuilder -> {",
          "1003:                         controllerBuilder.setAuthorizer(authorizers.get(controllerBuilder.nodeId()));",
          "1004:                     }).",
          "1005:                     build()",
          "1006:             ) {",
          "1007:                 assertInitialLoadFuturesNotComplete(authorizers);",
          "1008:                 logEnv.logManagers().get(0).setMaxReadOffset(Long.MAX_VALUE);",
          "1009:                 QuorumController active = controlEnv.activeController();",
          "1010:                 active.unregisterBroker(ANONYMOUS_CONTEXT, 3).get();",
          "1011:                 assertInitialLoadFuturesNotComplete(authorizers.stream().skip(1).collect(Collectors.toList()));",
          "1012:                 logEnv.logManagers().forEach(m -> m.setMaxReadOffset(Long.MAX_VALUE));",
          "1013:                 TestUtils.waitForCondition(() -> {",
          "1014:                     return authorizers.stream().allMatch(a -> a.initialLoadFuture().isDone());",
          "1015:                 }, \"Failed to complete initial authorizer load for all controllers.\");",
          "1016:             }",
          "1017:         }",
          "1018:     }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "metadata/src/test/java/org/apache/kafka/metadata/RecordTestUtils.java||metadata/src/test/java/org/apache/kafka/metadata/RecordTestUtils.java": [
          "File: metadata/src/test/java/org/apache/kafka/metadata/RecordTestUtils.java -> metadata/src/test/java/org/apache/kafka/metadata/RecordTestUtils.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "39: import java.util.Iterator;",
          "40: import java.util.List;",
          "41: import java.util.Objects;",
          "43: import java.util.Set;",
          "44: import java.util.function.Function;",
          "45: import java.util.function.Supplier;",
          "",
          "[Removed Lines]",
          "42: import java.util.Optional;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "70:                     try {",
          "71:                         Method method = target.getClass().getMethod(\"replay\",",
          "72:                             record.getClass(),",
          "84:                     }",
          "85:                 }",
          "86:             } catch (InvocationTargetException e) {",
          "",
          "[Removed Lines]",
          "73:                             Optional.class);",
          "74:                         method.invoke(target, record, Optional.empty());",
          "75:                     } catch (NoSuchMethodException t) {",
          "76:                         try {",
          "77:                             Method method = target.getClass().getMethod(\"replay\",",
          "78:                                 record.getClass(),",
          "79:                                 long.class);",
          "80:                             method.invoke(target, record, 0L);",
          "81:                         } catch (NoSuchMethodException i) {",
          "83:                         }",
          "",
          "[Added Lines]",
          "72:                             long.class);",
          "73:                         method.invoke(target, record, 0L);",
          "74:                     } catch (NoSuchMethodException i) {",
          "",
          "---------------"
        ],
        "metadata/src/test/java/org/apache/kafka/metadata/authorizer/MockAclMutator.java||metadata/src/test/java/org/apache/kafka/metadata/authorizer/MockAclMutator.java": [
          "File: metadata/src/test/java/org/apache/kafka/metadata/authorizer/MockAclMutator.java -> metadata/src/test/java/org/apache/kafka/metadata/authorizer/MockAclMutator.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f4996629239a49afb570a725642fb0311dd42e71",
      "candidate_info": {
        "commit_hash": "f4996629239a49afb570a725642fb0311dd42e71",
        "repo": "apache/kafka",
        "commit_url": "https://github.com/apache/kafka/commit/f4996629239a49afb570a725642fb0311dd42e71",
        "files": [
          "core/src/main/scala/kafka/zk/migration/ZkTopicMigrationClient.scala",
          "core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala",
          "core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala",
          "metadata/src/main/java/org/apache/kafka/image/TopicDelta.java",
          "metadata/src/main/java/org/apache/kafka/metadata/PartitionRegistration.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/TopicMigrationClient.java",
          "metadata/src/test/java/org/apache/kafka/metadata/migration/CapturingTopicMigrationClient.java",
          "metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java"
        ],
        "message": "KAFKA-15003: Fix ZK sync logic for partition assignments (#13735)\n\nFixed the metadata change events in the Migration component to check correctly for the diff in\nexisting topic changes and replicate the metadata to the Zookeeper. Also, made the diff check\nexhaustive enough to handle the partial writes in Zookeeper when we're try to replicate changes\nusing a snapshot in the event of Controller failover.\n\nAdd migration client and integration tests to verify the change.\n\nCo-authored-by: Akhilesh Chaganti <akhileshchg@users.noreply.github.com>",
        "before_after_code_files": [
          "core/src/main/scala/kafka/zk/migration/ZkTopicMigrationClient.scala||core/src/main/scala/kafka/zk/migration/ZkTopicMigrationClient.scala",
          "core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala||core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala",
          "core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala",
          "metadata/src/main/java/org/apache/kafka/image/TopicDelta.java||metadata/src/main/java/org/apache/kafka/image/TopicDelta.java",
          "metadata/src/main/java/org/apache/kafka/metadata/PartitionRegistration.java||metadata/src/main/java/org/apache/kafka/metadata/PartitionRegistration.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/TopicMigrationClient.java||metadata/src/main/java/org/apache/kafka/metadata/migration/TopicMigrationClient.java",
          "metadata/src/test/java/org/apache/kafka/metadata/migration/CapturingTopicMigrationClient.java||metadata/src/test/java/org/apache/kafka/metadata/migration/CapturingTopicMigrationClient.java",
          "metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java||metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala",
            "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java",
            "metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java||metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java"
          ],
          "candidate": [
            "core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala",
            "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java",
            "metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java||metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java"
          ]
        }
      },
      "candidate_diff": {
        "core/src/main/scala/kafka/zk/migration/ZkTopicMigrationClient.scala||core/src/main/scala/kafka/zk/migration/ZkTopicMigrationClient.scala": [
          "File: core/src/main/scala/kafka/zk/migration/ZkTopicMigrationClient.scala -> core/src/main/scala/kafka/zk/migration/ZkTopicMigrationClient.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "113:         zkClient.defaultAcls(path),",
          "114:         CreateMode.PERSISTENT)",
          "115:     }",
          "116:     val createPartitionsZNode = {",
          "117:       val path = TopicPartitionsZNode.path(topicName)",
          "118:       CreateRequest(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "116:     val createPartitionZNodeReqs = createTopicPartitionZNodesRequests(topicName, partitions, state)",
          "118:     val requests = Seq(createTopicZNode) ++ createPartitionZNodeReqs",
          "119:     val (migrationZkVersion, responses) = zkClient.retryMigrationRequestsUntilConnected(requests, state)",
          "120:     val resultCodes = responses.map { response => response.path -> response.resultCode }.toMap",
          "121:     if (resultCodes(TopicZNode.path(topicName)).equals(Code.NODEEXISTS)) {",
          "123:       state",
          "124:     } else if (resultCodes.forall { case (_, code) => code.equals(Code.OK) }) {",
          "126:       state.withMigrationZkVersion(migrationZkVersion)",
          "127:     } else {",
          "129:       throw new MigrationClientException(s\"Failed to create or update topic $topicName. ZK operations had results $resultCodes\")",
          "130:     }",
          "131:   }",
          "133:   private def createTopicPartitionZNodesRequests(",
          "134:     topicName: String,",
          "135:     partitions: util.Map[Integer, PartitionRegistration],",
          "136:     state: ZkMigrationLeadershipState",
          "137:   ): Seq[CreateRequest] = {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "122:         CreateMode.PERSISTENT)",
          "123:     }",
          "126:       val topicPartition = new TopicPartition(topicName, partitionId)",
          "127:       Seq(",
          "128:         createTopicPartition(topicPartition),",
          "",
          "[Removed Lines]",
          "125:     val createPartitionZNodeReqs = partitions.asScala.flatMap { case (partitionId, partition) =>",
          "",
          "[Added Lines]",
          "147:     val createPartitionZNodeReqs = partitions.asScala.toSeq.flatMap { case (partitionId, partition) =>",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "130:       )",
          "131:     }",
          "146:   }",
          "148:   private def recursiveChildren(path: String, acc: ArrayBuffer[String]): Unit = {",
          "",
          "[Removed Lines]",
          "133:     val requests = Seq(createTopicZNode, createPartitionsZNode) ++ createPartitionZNodeReqs",
          "134:     val (migrationZkVersion, responses) = zkClient.retryMigrationRequestsUntilConnected(requests, state)",
          "135:     val resultCodes = responses.map { response => response.path -> response.resultCode }.toMap",
          "136:     if (resultCodes(TopicZNode.path(topicName)).equals(Code.NODEEXISTS)) {",
          "138:       state",
          "139:     } else if (resultCodes.forall { case (_, code) => code.equals(Code.OK) }) {",
          "141:       state.withMigrationZkVersion(migrationZkVersion)",
          "142:     } else {",
          "144:       throw new MigrationClientException(s\"Failed to create or update topic $topicName. ZK operations had results $resultCodes\")",
          "145:     }",
          "",
          "[Added Lines]",
          "155:     Seq(createPartitionsZNode) ++ createPartitionZNodeReqs",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "159:     buffer.toSeq",
          "160:   }",
          "162:   override def deleteTopic(",
          "163:     topicName: String,",
          "164:     state: ZkMigrationLeadershipState",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "172:   override def updateTopic(",
          "173:     topicName: String,",
          "174:     topicId: Uuid,",
          "175:     partitions: util.Map[Integer, PartitionRegistration],",
          "176:     state: ZkMigrationLeadershipState",
          "177:   ): ZkMigrationLeadershipState = wrapZkException {",
          "178:     val assignments = partitions.asScala.map { case (partitionId, partition) =>",
          "179:       new TopicPartition(topicName, partitionId) ->",
          "180:         ReplicaAssignment(partition.replicas, partition.addingReplicas, partition.removingReplicas)",
          "181:     }",
          "182:     val request = SetDataRequest(",
          "183:       TopicZNode.path(topicName),",
          "184:       TopicZNode.encode(Some(topicId), assignments),",
          "185:       ZkVersion.MatchAnyVersion",
          "186:     )",
          "187:     val (migrationZkVersion, responses) = zkClient.retryMigrationRequestsUntilConnected(Seq(request), state)",
          "188:     val resultCodes = responses.map { response => response.path -> response.resultCode }.toMap",
          "189:     if (resultCodes.forall { case (_, code) => code.equals(Code.OK) } ) {",
          "190:       state.withMigrationZkVersion(migrationZkVersion)",
          "191:     } else {",
          "192:       throw new MigrationClientException(s\"Failed to update topic metadata: $topicName. ZK transaction had results $resultCodes\")",
          "193:     }",
          "194:   }",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "181:     }",
          "182:   }",
          "184:   override def updateTopicPartitions(",
          "185:     topicPartitions: util.Map[String, util.Map[Integer, PartitionRegistration]],",
          "186:     state: ZkMigrationLeadershipState",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "218:   override def createTopicPartitions(topicPartitions: util.Map[String, util.Map[Integer, PartitionRegistration]], state: ZkMigrationLeadershipState)",
          "219:   :ZkMigrationLeadershipState = wrapZkException {",
          "220:     val requests = topicPartitions.asScala.toSeq.flatMap { case (topicName, partitions) =>",
          "221:       createTopicPartitionZNodesRequests(topicName, partitions, state)",
          "222:     }",
          "224:     val (migrationZkVersion, responses) = zkClient.retryMigrationRequestsUntilConnected(requests, state)",
          "225:     val resultCodes = responses.map { response => response.path -> response.resultCode }.toMap",
          "226:     if (resultCodes.forall { case (_, code) => code.equals(Code.OK) || code.equals(Code.NODEEXISTS) }) {",
          "227:       state.withMigrationZkVersion(migrationZkVersion)",
          "228:     } else {",
          "229:       throw new MigrationClientException(s\"Failed to create partition states: $topicPartitions. ZK transaction had results $resultCodes\")",
          "230:     }",
          "231:   }",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "204:     }",
          "205:   }",
          "207:   private def createTopicPartition(",
          "208:     topicPartition: TopicPartition",
          "209:   ): CreateRequest = wrapZkException {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "256:   override def deleteTopicPartitions(",
          "257:     topicPartitions: util.Map[String, util.Set[Integer]],",
          "258:     state: ZkMigrationLeadershipState",
          "259:   ): ZkMigrationLeadershipState = {",
          "260:     val requests = topicPartitions.asScala.flatMap { case (topicName, partitionIds) =>",
          "261:       partitionIds.asScala.map { partitionId =>",
          "262:         val topicPartition = new TopicPartition(topicName, partitionId)",
          "263:         val path = TopicPartitionZNode.path(topicPartition)",
          "264:         DeleteRequest(path, ZkVersion.MatchAnyVersion)",
          "265:       }",
          "266:     }",
          "267:     if (requests.isEmpty) {",
          "268:       state",
          "269:     } else {",
          "270:       val (migrationZkVersion, responses) = zkClient.retryMigrationRequestsUntilConnected(requests.toSeq, state)",
          "271:       val resultCodes = responses.map { response => response.path -> response.resultCode }.toMap",
          "272:       if (resultCodes.forall { case (_, code) => code.equals(Code.OK) }) {",
          "273:         state.withMigrationZkVersion(migrationZkVersion)",
          "274:       } else {",
          "275:         throw new MigrationClientException(s\"Failed to delete partition states: $topicPartitions. ZK transaction had results $resultCodes\")",
          "276:       }",
          "277:     }",
          "278:   }",
          "",
          "---------------"
        ],
        "core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala||core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala": [
          "File: core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala -> core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import kafka.utils.{PasswordEncoder, TestUtils}",
          "27: import org.apache.kafka.clients.admin._",
          "28: import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord}",
          "30: import org.apache.kafka.common.acl.AclOperation.{DESCRIBE, READ, WRITE}",
          "31: import org.apache.kafka.common.acl.AclPermissionType.ALLOW",
          "32: import org.apache.kafka.common.acl.{AccessControlEntry, AclBinding}",
          "",
          "[Removed Lines]",
          "29: import org.apache.kafka.common.Uuid",
          "",
          "[Added Lines]",
          "29: import org.apache.kafka.common.{TopicPartition, Uuid}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "385:     }",
          "386:   }",
          "388:   def allocateProducerId(bootstrapServers: String): Unit = {",
          "389:     val props = new Properties()",
          "390:     props.put(\"bootstrap.servers\", bootstrapServers)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "388:   @ClusterTest(clusterType = Type.ZK, brokers = 3, metadataVersion = MetadataVersion.IBP_3_4_IV0, serverProperties = Array(",
          "389:     new ClusterConfigProperty(key = \"inter.broker.listener.name\", value = \"EXTERNAL\"),",
          "390:     new ClusterConfigProperty(key = \"listeners\", value = \"PLAINTEXT://localhost:0,EXTERNAL://localhost:0\"),",
          "391:     new ClusterConfigProperty(key = \"advertised.listeners\", value = \"PLAINTEXT://localhost:0,EXTERNAL://localhost:0\"),",
          "392:     new ClusterConfigProperty(key = \"listener.security.protocol.map\", value = \"EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT\"),",
          "393:   ))",
          "394:   def testNewAndChangedTopicsInDualWrite(zkCluster: ClusterInstance): Unit = {",
          "396:     val topicName = \"test\"",
          "397:     var admin = zkCluster.createAdminClient()",
          "398:     val zkClient = zkCluster.asInstanceOf[ZkClusterInstance].getUnderlying().zkClient",
          "401:     val clusterId = zkCluster.clusterId()",
          "402:     val kraftCluster = new KafkaClusterTestKit.Builder(",
          "403:       new TestKitNodes.Builder().",
          "404:         setBootstrapMetadataVersion(MetadataVersion.IBP_3_4_IV0).",
          "405:         setClusterId(Uuid.fromString(clusterId)).",
          "406:         setNumBrokerNodes(0).",
          "407:         setNumControllerNodes(1).build())",
          "408:       .setConfigProp(KafkaConfig.MigrationEnabledProp, \"true\")",
          "409:       .setConfigProp(KafkaConfig.ZkConnectProp, zkCluster.asInstanceOf[ZkClusterInstance].getUnderlying.zkConnect)",
          "410:       .build()",
          "411:     try {",
          "412:       kraftCluster.format()",
          "413:       kraftCluster.startup()",
          "414:       val readyFuture = kraftCluster.controllers().values().asScala.head.controller.waitForReadyBrokers(3)",
          "417:       log.info(\"Restart brokers in migration mode\")",
          "418:       val clientProps = kraftCluster.controllerClientProperties()",
          "419:       val voters = clientProps.get(RaftConfig.QUORUM_VOTERS_CONFIG)",
          "420:       zkCluster.config().serverProperties().put(KafkaConfig.MigrationEnabledProp, \"true\")",
          "421:       zkCluster.config().serverProperties().put(RaftConfig.QUORUM_VOTERS_CONFIG, voters)",
          "422:       zkCluster.config().serverProperties().put(KafkaConfig.ControllerListenerNamesProp, \"CONTROLLER\")",
          "423:       zkCluster.config().serverProperties().put(KafkaConfig.ListenerSecurityProtocolMapProp, \"CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT\")",
          "424:       zkCluster.rollingBrokerRestart()",
          "425:       zkCluster.waitForReadyBrokers()",
          "426:       readyFuture.get(30, TimeUnit.SECONDS)",
          "429:       log.info(\"Waiting for ZK migration to begin\")",
          "430:       TestUtils.waitUntilTrue(() => zkClient.getControllerId.contains(3000), \"Timed out waiting for KRaft controller to take over\")",
          "433:       log.info(\"Create new topic with AdminClient\")",
          "434:       admin = zkCluster.createAdminClient()",
          "435:       val newTopics = new util.ArrayList[NewTopic]()",
          "436:       newTopics.add(new NewTopic(topicName, 2, 3.toShort))",
          "437:       val createTopicResult = admin.createTopics(newTopics)",
          "438:       createTopicResult.all().get(60, TimeUnit.SECONDS)",
          "440:       val existingPartitions = Seq(new TopicPartition(topicName, 0), new TopicPartition(topicName, 1))",
          "442:       verifyTopicPartitionMetadata(topicName, existingPartitions, zkClient)",
          "444:       log.info(\"Create new partitions with AdminClient\")",
          "445:       admin.createPartitions(Map(topicName -> NewPartitions.increaseTo(3)).asJava).all().get(60, TimeUnit.SECONDS)",
          "448:       verifyTopicPartitionMetadata(topicName, existingPartitions ++ Seq(new TopicPartition(topicName, 2)), zkClient)",
          "449:     } finally {",
          "450:       zkCluster.stop()",
          "451:       kraftCluster.close()",
          "452:     }",
          "453:   }",
          "455:   def verifyTopicPartitionMetadata(topicName: String, partitions: Seq[TopicPartition], zkClient: KafkaZkClient): Unit = {",
          "456:     val (topicIdReplicaAssignment, success) = TestUtils.computeUntilTrue(",
          "457:       zkClient.getReplicaAssignmentAndTopicIdForTopics(Set(topicName)).headOption) {",
          "458:       x => x.exists(_.assignment.size == partitions.size)",
          "459:     }",
          "460:     assertTrue(success, \"Unable to find topic metadata in Zk\")",
          "461:     TestUtils.waitUntilTrue(() =>{",
          "462:       val lisrMap = zkClient.getTopicPartitionStates(partitions.toSeq)",
          "463:       lisrMap.size == partitions.size &&",
          "464:         lisrMap.forall { case (tp, lisr) =>",
          "465:           lisr.leaderAndIsr.leader >= 0 &&",
          "466:             topicIdReplicaAssignment.exists(_.assignment(tp).replicas == lisr.leaderAndIsr.isr)",
          "467:         }",
          "468:     }, \"Unable to find topic partition metadata\")",
          "469:   }",
          "",
          "---------------"
        ],
        "core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala": [
          "File: core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala -> core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "101:   }",
          "103:   @Test",
          "105:     assertEquals(0, migrationState.migrationZkVersion())",
          "107:     val partitions = Map(",
          "",
          "[Removed Lines]",
          "104:   def testCreateNewPartitions(): Unit = {",
          "",
          "[Added Lines]",
          "104:   def testCreateNewTopic(): Unit = {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "364:       }",
          "365:     }",
          "366:   }",
          "367: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "368:   @Test",
          "369:   def testUpdateExistingTopicWithNewAndChangedPartitions(): Unit = {",
          "370:     assertEquals(0, migrationState.migrationZkVersion())",
          "372:     val topicId = Uuid.randomUuid()",
          "373:     val partitions = Map(",
          "374:       0 -> new PartitionRegistration(Array(0, 1, 2), Array(0, 1, 2), Array(), Array(), 0, LeaderRecoveryState.RECOVERED, 0, -1),",
          "375:       1 -> new PartitionRegistration(Array(1, 2, 3), Array(1, 2, 3), Array(), Array(), 1, LeaderRecoveryState.RECOVERED, 0, -1)",
          "376:     ).map { case (k, v) => Integer.valueOf(k) -> v }.asJava",
          "377:     migrationState = migrationClient.topicClient().createTopic(\"test\", topicId, partitions, migrationState)",
          "378:     assertEquals(1, migrationState.migrationZkVersion())",
          "382:     val changedPartitions = Map(",
          "383:       0 -> new PartitionRegistration(Array(1, 2, 3), Array(1, 2, 3), Array(), Array(), 0, LeaderRecoveryState.RECOVERED, 0, -1),",
          "384:       1 -> new PartitionRegistration(Array(0, 1, 2), Array(0, 1, 2), Array(), Array(), 1, LeaderRecoveryState.RECOVERED, 0, -1)",
          "385:     ).map { case (k, v) => Integer.valueOf(k) -> v }.asJava",
          "386:     migrationState = migrationClient.topicClient().updateTopic(\"test\", topicId, changedPartitions, migrationState)",
          "387:     assertEquals(2, migrationState.migrationZkVersion())",
          "390:     val topicReplicaAssignmentFromZk = zkClient.getReplicaAssignmentAndTopicIdForTopics(Set(\"test\"))",
          "391:     assertEquals(1, topicReplicaAssignmentFromZk.size)",
          "392:     assertEquals(Some(topicId), topicReplicaAssignmentFromZk.head.topicId);",
          "393:     topicReplicaAssignmentFromZk.head.assignment.foreach { case (tp, assignment) =>",
          "394:       tp.partition() match {",
          "395:         case p if p <=1 =>",
          "396:           assertEquals(changedPartitions.get(p).replicas.toSeq, assignment.replicas)",
          "397:           assertEquals(changedPartitions.get(p).addingReplicas.toSeq, assignment.addingReplicas)",
          "398:           assertEquals(changedPartitions.get(p).removingReplicas.toSeq, assignment.removingReplicas)",
          "399:         case p => fail(s\"Found unknown partition $p\")",
          "400:       }",
          "401:     }",
          "404:     val newPartition = Map(",
          "405:       2 -> new PartitionRegistration(Array(2, 3, 4), Array(2, 3, 4), Array(), Array(), 1, LeaderRecoveryState.RECOVERED, 0, -1)",
          "406:     ).map { case (k, v) => int2Integer(k) -> v }.asJava",
          "407:     migrationState = migrationClient.topicClient().createTopicPartitions(Map(\"test\" -> newPartition).asJava, migrationState)",
          "408:     assertEquals(3, migrationState.migrationZkVersion())",
          "411:     val newPartitionFromZk = zkClient.getTopicPartitionState(new TopicPartition(\"test\", 2))",
          "412:     assertTrue(newPartitionFromZk.isDefined)",
          "413:     newPartitionFromZk.foreach { part =>",
          "414:       val expectedPartition = newPartition.get(2)",
          "415:       assertEquals(expectedPartition.leader, part.leaderAndIsr.leader)",
          "417:       assertEquals(expectedPartition.partitionEpoch + 1, part.leaderAndIsr.partitionEpoch)",
          "418:       assertEquals(expectedPartition.leaderEpoch, part.leaderAndIsr.leaderEpoch)",
          "419:       assertEquals(expectedPartition.leaderRecoveryState, part.leaderAndIsr.leaderRecoveryState)",
          "420:       assertEquals(expectedPartition.isr.toList, part.leaderAndIsr.isr)",
          "421:     }",
          "422:   }",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/image/TopicDelta.java||metadata/src/main/java/org/apache/kafka/image/TopicDelta.java": [
          "File: metadata/src/main/java/org/apache/kafka/image/TopicDelta.java -> metadata/src/main/java/org/apache/kafka/image/TopicDelta.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: import java.util.Map.Entry;",
          "30: import java.util.Map;",
          "31: import java.util.Set;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32: import java.util.stream.Collectors;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "49:         return partitionChanges;",
          "50:     }",
          "52:     public String name() {",
          "53:         return image.name();",
          "54:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "53:     public Map<Integer, PartitionRegistration> newPartitions() {",
          "54:         return partitionChanges",
          "55:             .entrySet()",
          "56:             .stream()",
          "57:             .filter(entry -> !image.partitions().containsKey(entry.getKey()))",
          "58:             .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));",
          "59:     }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "92:         return new TopicImage(image.name(), image.id(), newPartitions);",
          "93:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "104:     public boolean hasPartitionsWithAssignmentChanges() {",
          "105:         for (Entry<Integer, PartitionRegistration> entry : partitionChanges.entrySet()) {",
          "106:             int partitionId = entry.getKey();",
          "108:             if (!image.partitions().containsKey(partitionId))",
          "109:                 return true;",
          "110:             PartitionRegistration previousPartition = image.partitions().get(partitionId);",
          "111:             PartitionRegistration currentPartition = entry.getValue();",
          "112:             if (!previousPartition.hasSameAssignment(currentPartition))",
          "113:                 return true;",
          "114:         }",
          "115:         return false;",
          "116:     }",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/metadata/PartitionRegistration.java||metadata/src/main/java/org/apache/kafka/metadata/PartitionRegistration.java": [
          "File: metadata/src/main/java/org/apache/kafka/metadata/PartitionRegistration.java -> metadata/src/main/java/org/apache/kafka/metadata/PartitionRegistration.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "235:         builder.append(\")\");",
          "236:         return builder.toString();",
          "237:     }",
          "238: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "239:     public boolean hasSameAssignment(PartitionRegistration registration) {",
          "240:         return Arrays.equals(this.replicas, registration.replicas) &&",
          "241:             Arrays.equals(this.addingReplicas, registration.addingReplicas) &&",
          "242:             Arrays.equals(this.removingReplicas, registration.removingReplicas);",
          "243:     }",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java": [
          "File: metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "65:     private static final String UPDATE_PRODUCER_ID = \"UpdateProducerId\";",
          "66:     private static final String CREATE_TOPIC = \"CreateTopic\";",
          "67:     private static final String DELETE_TOPIC = \"DeleteTopic\";",
          "68:     private static final String UPDATE_PARTITON = \"UpdatePartition\";",
          "69:     private static final String UPDATE_BROKER_CONFIG = \"UpdateBrokerConfig\";",
          "70:     private static final String DELETE_BROKER_CONFIG = \"DeleteBrokerConfig\";",
          "71:     private static final String UPDATE_TOPIC_CONFIG = \"UpdateTopicConfig\";",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "67:     private static final String UPDATE_TOPIC = \"UpdateTopic\";",
          "70:     private static final String DELETE_PARTITION = \"DeletePartition\";",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "98:         KRaftMigrationOperationConsumer operationConsumer",
          "99:     ) {",
          "100:         if (delta.topicsDelta() != null) {",
          "102:         }",
          "103:         if (delta.configsDelta() != null) {",
          "104:             handleConfigsDelta(image.configs(), delta.configsDelta(), operationConsumer);",
          "",
          "[Removed Lines]",
          "101:             handleTopicsDelta(previousImage.topics().topicIdToNameView()::get, delta.topicsDelta(), operationConsumer);",
          "",
          "[Added Lines]",
          "103:             handleTopicsDelta(previousImage.topics().topicIdToNameView()::get, image.topics(), delta.topicsDelta(), operationConsumer);",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "122:     void handleTopicsSnapshot(TopicsImage topicsImage, KRaftMigrationOperationConsumer operationConsumer) {",
          "123:         Map<Uuid, String> deletedTopics = new HashMap<>();",
          "125:         Map<Uuid, Map<Integer, PartitionRegistration>> changedPartitions = new HashMap<>();",
          "127:         migrationClient.topicClient().iterateTopics(",
          "128:             EnumSet.of(",
          "",
          "[Removed Lines]",
          "124:         Set<Uuid> createdTopics = new HashSet<>(topicsImage.topicsById().keySet());",
          "",
          "[Added Lines]",
          "126:         Set<Uuid> topicsInZk = new HashSet<>();",
          "127:         Set<Uuid> newTopics = new HashSet<>(topicsImage.topicsById().keySet());",
          "128:         Set<Uuid> changedTopics = new HashSet<>();",
          "129:         Map<Uuid, Set<Integer>> partitionsInZk = new HashMap<>();",
          "130:         Map<String, Set<Integer>> extraneousPartitionsInZk = new HashMap<>();",
          "132:         Map<Uuid, Map<Integer, PartitionRegistration>> newPartitions = new HashMap<>();",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "137:                         deletedTopics.put(topicId, topicName);",
          "138:                     } else {",
          "140:                     }",
          "141:                 }",
          "",
          "[Removed Lines]",
          "139:                         createdTopics.remove(topicId);",
          "",
          "[Added Lines]",
          "146:                         if (!newTopics.remove(topicId)) return;",
          "147:                         topicsInZk.add(topicId);",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "144:                 public void visitPartition(TopicIdPartition topicIdPartition, PartitionRegistration partitionRegistration) {",
          "145:                     TopicImage topic = topicsImage.getTopic(topicIdPartition.topicId());",
          "146:                     if (topic == null) {",
          "148:                     }",
          "151:                     PartitionRegistration kraftPartition = topic.partitions().get(topicIdPartition.partition());",
          "155:                     }",
          "156:                 }",
          "157:             });",
          "160:             TopicImage topic = topicsImage.getTopic(topicId);",
          "161:             operationConsumer.accept(",
          "162:                 CREATE_TOPIC,",
          "",
          "[Removed Lines]",
          "147:                         return; // topic deleted in KRaft",
          "152:                     if (!kraftPartition.equals(partitionRegistration)) {",
          "153:                         changedPartitions.computeIfAbsent(topicIdPartition.topicId(), __ -> new HashMap<>())",
          "154:                             .put(topicIdPartition.partition(), kraftPartition);",
          "159:         createdTopics.forEach(topicId -> {",
          "",
          "[Added Lines]",
          "155:                         return; // The topic was deleted in KRaft. Handled by deletedTopics",
          "161:                     partitionsInZk",
          "162:                         .computeIfAbsent(topic.id(), __ -> new HashSet<>())",
          "163:                         .add(topicIdPartition.partition());",
          "167:                     if (kraftPartition != null) {",
          "168:                         if (!kraftPartition.equals(partitionRegistration)) {",
          "169:                             changedPartitions.computeIfAbsent(topicIdPartition.topicId(), __ -> new HashMap<>())",
          "170:                                 .put(topicIdPartition.partition(), kraftPartition);",
          "171:                         }",
          "174:                         if (!kraftPartition.hasSameAssignment(partitionRegistration)) {",
          "175:                             changedTopics.add(topic.id());",
          "176:                         }",
          "182:         topicsInZk.forEach(topicId -> {",
          "183:             TopicImage topic = topicsImage.getTopic(topicId);",
          "184:             Set<Integer> topicPartitionsInZk = partitionsInZk.computeIfAbsent(topicId, __ -> new HashSet<>());",
          "185:             if (!topicPartitionsInZk.equals(topic.partitions().keySet())) {",
          "186:                 Map<Integer, PartitionRegistration> newTopicPartitions = new HashMap<>(topic.partitions());",
          "188:                 topicPartitionsInZk.forEach(newTopicPartitions::remove);",
          "189:                 newPartitions.put(topicId, newTopicPartitions);",
          "192:                 topicPartitionsInZk.removeAll(topic.partitions().keySet());",
          "193:                 if (!topicPartitionsInZk.isEmpty()) {",
          "194:                     extraneousPartitionsInZk.put(topic.name(), topicPartitionsInZk);",
          "195:                 }",
          "196:                 changedTopics.add(topicId);",
          "197:             }",
          "198:         });",
          "200:         newTopics.forEach(topicId -> {",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "165:             );",
          "166:         });",
          "168:         deletedTopics.forEach((topicId, topicName) -> {",
          "169:             operationConsumer.accept(",
          "170:                 DELETE_TOPIC,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "209:         changedTopics.forEach(topicId -> {",
          "210:             TopicImage topic = topicsImage.getTopic(topicId);",
          "211:             operationConsumer.accept(",
          "212:                 UPDATE_TOPIC,",
          "213:                 \"Changed Topic \" + topic.name() + \", ID \" + topicId,",
          "214:                 migrationState -> migrationClient.topicClient().updateTopic(topic.name(), topicId, topic.partitions(), migrationState)",
          "215:             );",
          "216:         });",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "179:             );",
          "180:         });",
          "183:             TopicImage topic = topicsImage.getTopic(topicId);",
          "184:             operationConsumer.accept(",
          "185:                 UPDATE_PARTITON,",
          "186:                 \"Updating Partitions for Topic \" + topic.name() + \", ID \" + topicId,",
          "187:                 migrationState -> migrationClient.topicClient().updateTopicPartitions(",
          "189:                     migrationState));",
          "190:         });",
          "191:     }",
          "193:     void handleTopicsDelta(",
          "194:         Function<Uuid, String> deletedTopicNameResolver,",
          "195:         TopicsDelta topicsDelta,",
          "196:         KRaftMigrationOperationConsumer operationConsumer",
          "197:     ) {",
          "",
          "[Removed Lines]",
          "182:         changedPartitions.forEach((topicId, paritionMap) -> {",
          "188:                     Collections.singletonMap(topic.name(), paritionMap),",
          "",
          "[Added Lines]",
          "232:         newPartitions.forEach((topicId, partitionMap) -> {",
          "233:             TopicImage topic = topicsImage.getTopic(topicId);",
          "234:             operationConsumer.accept(",
          "235:                 UPDATE_PARTITON,",
          "236:                 \"Creating additional partitions for Topic \" + topic.name() + \", ID \" + topicId,",
          "237:                 migrationState -> migrationClient.topicClient().updateTopicPartitions(",
          "238:                     Collections.singletonMap(topic.name(), partitionMap),",
          "239:                     migrationState));",
          "240:         });",
          "242:         changedPartitions.forEach((topicId, partitionMap) -> {",
          "248:                     Collections.singletonMap(topic.name(), partitionMap),",
          "249:                     migrationState));",
          "250:         });",
          "252:         extraneousPartitionsInZk.forEach((topicName, partitions) -> {",
          "253:             operationConsumer.accept(",
          "254:                 DELETE_PARTITION,",
          "255:                 \"Deleting extraneous Partitions \" + partitions + \" for Topic \" + topicName,",
          "256:                 migrationState -> migrationClient.topicClient().deleteTopicPartitions(",
          "257:                     Collections.singletonMap(topicName, partitions),",
          "264:         TopicsImage topicsImage,",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "212:                         topicDelta.partitionChanges(),",
          "213:                         migrationState));",
          "214:             } else {",
          "221:             }",
          "222:         });",
          "223:     }",
          "",
          "[Removed Lines]",
          "215:                 operationConsumer.accept(",
          "216:                     UPDATE_PARTITON,",
          "217:                     \"Updating Partitions for Topic \" + topicDelta.name() + \", ID \" + topicId,",
          "218:                     migrationState -> migrationClient.topicClient().updateTopicPartitions(",
          "219:                         Collections.singletonMap(topicDelta.name(), topicDelta.partitionChanges()),",
          "220:                         migrationState));",
          "",
          "[Added Lines]",
          "285:                 if (topicDelta.hasPartitionsWithAssignmentChanges())",
          "286:                     operationConsumer.accept(",
          "287:                         UPDATE_TOPIC,",
          "288:                         \"Updating Topic \" + topicDelta.name() + \", ID \" + topicId,",
          "289:                         migrationState -> migrationClient.topicClient().updateTopic(",
          "290:                             topicDelta.name(),",
          "291:                             topicId,",
          "292:                             topicsImage.getTopic(topicId).partitions(),",
          "293:                             migrationState));",
          "294:                 Map<Integer, PartitionRegistration> newPartitions = topicDelta.newPartitions();",
          "295:                 Map<Integer, PartitionRegistration> changedPartitions = topicDelta.partitionChanges();",
          "296:                 if (!newPartitions.isEmpty()) {",
          "297:                     operationConsumer.accept(",
          "298:                         UPDATE_PARTITON,",
          "299:                         \"Create new partitions for Topic \" + topicDelta.name() + \", ID \" + topicId,",
          "300:                         migrationState -> migrationClient.topicClient().createTopicPartitions(",
          "301:                             Collections.singletonMap(topicDelta.name(), newPartitions),",
          "302:                             migrationState));",
          "303:                     newPartitions.keySet().forEach(changedPartitions::remove);",
          "304:                 }",
          "305:                 if (!changedPartitions.isEmpty()) {",
          "307:                     final Map<Integer, PartitionRegistration> finalChangedPartitions = changedPartitions;",
          "308:                     operationConsumer.accept(",
          "309:                         UPDATE_PARTITON,",
          "310:                         \"Updating Partitions for Topic \" + topicDelta.name() + \", ID \" + topicId,",
          "311:                         migrationState -> migrationClient.topicClient().updateTopicPartitions(",
          "312:                             Collections.singletonMap(topicDelta.name(), finalChangedPartitions),",
          "313:                             migrationState));",
          "314:                 }",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/metadata/migration/TopicMigrationClient.java||metadata/src/main/java/org/apache/kafka/metadata/migration/TopicMigrationClient.java": [
          "File: metadata/src/main/java/org/apache/kafka/metadata/migration/TopicMigrationClient.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/TopicMigrationClient.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import java.util.EnumSet;",
          "25: import java.util.List;",
          "26: import java.util.Map;",
          "28: public interface TopicMigrationClient {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: import java.util.Set;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "53:         ZkMigrationLeadershipState state",
          "54:     );",
          "56:     ZkMigrationLeadershipState updateTopicPartitions(",
          "57:         Map<String, Map<Integer, PartitionRegistration>> topicPartitions,",
          "58:         ZkMigrationLeadershipState state",
          "59:     );",
          "60: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "57:     ZkMigrationLeadershipState updateTopic(",
          "58:         String topicName,",
          "59:         Uuid topicId,",
          "60:         Map<Integer, PartitionRegistration> topicPartitions,",
          "61:         ZkMigrationLeadershipState state",
          "62:     );",
          "64:     ZkMigrationLeadershipState createTopicPartitions(",
          "65:         Map<String, Map<Integer, PartitionRegistration>> topicPartitions,",
          "66:         ZkMigrationLeadershipState state",
          "67:     );",
          "74:     ZkMigrationLeadershipState deleteTopicPartitions(",
          "75:         Map<String, Set<Integer>> topicPartitions,",
          "76:         ZkMigrationLeadershipState state",
          "77:     );",
          "",
          "---------------"
        ],
        "metadata/src/test/java/org/apache/kafka/metadata/migration/CapturingTopicMigrationClient.java||metadata/src/test/java/org/apache/kafka/metadata/migration/CapturingTopicMigrationClient.java": [
          "File: metadata/src/test/java/org/apache/kafka/metadata/migration/CapturingTopicMigrationClient.java -> metadata/src/test/java/org/apache/kafka/metadata/migration/CapturingTopicMigrationClient.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: public class CapturingTopicMigrationClient implements TopicMigrationClient {",
          "31:     public List<String> deletedTopics = new ArrayList<>();",
          "32:     public List<String> createdTopics = new ArrayList<>();",
          "33:     public LinkedHashMap<String, Set<Integer>> updatedTopicPartitions = new LinkedHashMap<>();",
          "35:     public void reset() {",
          "36:         createdTopics.clear();",
          "37:         updatedTopicPartitions.clear();",
          "38:         deletedTopics.clear();",
          "39:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33:     public LinkedHashMap<String, Map<Integer, PartitionRegistration>> updatedTopics = new LinkedHashMap<>();",
          "34:     public LinkedHashMap<String, Set<Integer>> newTopicPartitions = new LinkedHashMap<>();",
          "36:     public LinkedHashMap<String, Set<Integer>> deletedTopicPartitions = new LinkedHashMap<>();",
          "43:         updatedTopics.clear();",
          "44:         deletedTopicPartitions.clear();",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "56:         return state;",
          "57:     }",
          "59:     @Override",
          "60:     public ZkMigrationLeadershipState updateTopicPartitions(Map<String, Map<Integer, PartitionRegistration>> topicPartitions, ZkMigrationLeadershipState state) {",
          "61:         topicPartitions.forEach((topicName, partitionMap) ->",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "65:     @Override",
          "66:     public ZkMigrationLeadershipState updateTopic(",
          "67:         String topicName,",
          "68:         Uuid topicId,",
          "69:         Map<Integer, PartitionRegistration> topicPartitions,",
          "70:         ZkMigrationLeadershipState state",
          "71:     ) {",
          "72:         updatedTopics.put(topicName, topicPartitions);",
          "73:         return state;",
          "74:     }",
          "76:     @Override",
          "77:     public ZkMigrationLeadershipState createTopicPartitions(Map<String, Map<Integer, PartitionRegistration>> topicPartitions, ZkMigrationLeadershipState state) {",
          "78:         topicPartitions.forEach((topicName, partitionMap) ->",
          "79:             newTopicPartitions.put(topicName, partitionMap.keySet())",
          "80:         );",
          "81:         return state;",
          "82:     }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "63:         );",
          "64:         return state;",
          "65:     }",
          "66: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "92:     @Override",
          "93:     public ZkMigrationLeadershipState deleteTopicPartitions(Map<String, Set<Integer>> topicPartitions, ZkMigrationLeadershipState state) {",
          "94:         deletedTopicPartitions.putAll(topicPartitions);",
          "95:         return state;",
          "96:     }",
          "",
          "---------------"
        ],
        "metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java||metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java": [
          "File: metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java -> metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: package org.apache.kafka.metadata.migration;",
          "19: import org.apache.kafka.common.config.ConfigResource;",
          "20: import org.apache.kafka.image.AclsImage;",
          "21: import org.apache.kafka.image.AclsImageTest;",
          "22: import org.apache.kafka.image.ClientQuotasImage;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import org.apache.kafka.common.TopicIdPartition;",
          "21: import org.apache.kafka.common.TopicPartition;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "33: import org.junit.jupiter.api.Test;",
          "35: import java.util.Arrays;",
          "36: import java.util.EnumSet;",
          "37: import java.util.HashMap;",
          "38: import java.util.List;",
          "39: import java.util.Map;",
          "41: import static org.junit.jupiter.api.Assertions.assertEquals;",
          "42: import static org.junit.jupiter.api.Assertions.assertTrue;",
          "45: public class KRaftMigrationZkWriterTest {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "38: import java.util.Collections;",
          "43: import java.util.stream.IntStream;",
          "51:     @Test",
          "52:     public void testExtraneousZkPartitions() {",
          "53:         CapturingTopicMigrationClient topicClient = new CapturingTopicMigrationClient() {",
          "54:             @Override",
          "55:             public void iterateTopics(EnumSet<TopicVisitorInterest> interests, TopicVisitor visitor) {",
          "56:                 Map<Integer, List<Integer>> assignments = new HashMap<>();",
          "57:                 assignments.put(0, Arrays.asList(2, 3, 4));",
          "58:                 assignments.put(1, Arrays.asList(3, 4, 5));",
          "59:                 assignments.put(2, Arrays.asList(2, 4, 5));",
          "60:                 assignments.put(3, Arrays.asList(1, 2, 3)); // This one is not in KRaft",
          "61:                 visitor.visitTopic(\"foo\", TopicsImageTest.FOO_UUID, assignments);",
          "64:                 IntStream.of(0, 2, 3).forEach(partitionId -> {",
          "65:                     visitor.visitPartition(",
          "66:                         new TopicIdPartition(TopicsImageTest.FOO_UUID, new TopicPartition(\"foo\", partitionId)),",
          "67:                         TopicsImageTest.IMAGE1.getPartition(TopicsImageTest.FOO_UUID, partitionId)",
          "68:                     );",
          "69:                 });",
          "71:             }",
          "72:         };",
          "74:         CapturingConfigMigrationClient configClient = new CapturingConfigMigrationClient();",
          "75:         CapturingAclMigrationClient aclClient = new CapturingAclMigrationClient();",
          "76:         CapturingMigrationClient migrationClient = CapturingMigrationClient.newBuilder()",
          "77:             .setBrokersInZk(0)",
          "78:             .setTopicMigrationClient(topicClient)",
          "79:             .setConfigMigrationClient(configClient)",
          "80:             .build();",
          "82:         KRaftMigrationZkWriter writer = new KRaftMigrationZkWriter(migrationClient);",
          "84:         MetadataImage image = new MetadataImage(",
          "85:             MetadataProvenance.EMPTY,",
          "86:             FeaturesImage.EMPTY,",
          "87:             ClusterImage.EMPTY,",
          "88:             TopicsImageTest.IMAGE1,     // This includes \"foo\" with 3 partitions",
          "89:             ConfigurationsImage.EMPTY,",
          "90:             ClientQuotasImage.EMPTY,",
          "91:             ProducerIdsImage.EMPTY,",
          "92:             AclsImage.EMPTY,",
          "93:             ScramImage.EMPTY",
          "94:         );",
          "96:         writer.handleSnapshot(image, (opType, opLog, operation) -> {",
          "97:             operation.apply(ZkMigrationLeadershipState.EMPTY);",
          "98:         });",
          "99:         assertEquals(topicClient.updatedTopics.get(\"foo\").size(), 3);",
          "100:         assertEquals(topicClient.deletedTopicPartitions.get(\"foo\"), Collections.singleton(3));",
          "101:         assertEquals(topicClient.updatedTopicPartitions.get(\"foo\"), Collections.singleton(1));",
          "102:     }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "137:             (logMsg, operation) -> operation.apply(ZkMigrationLeadershipState.EMPTY));",
          "138:         writer.handleSnapshot(image, consumer);",
          "139:         assertEquals(1, opCounts.remove(\"CreateTopic\"));",
          "140:         assertEquals(0, opCounts.size());",
          "141:         assertEquals(\"bar\", topicClient.createdTopics.get(0));",
          "142:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "198:         assertEquals(1, opCounts.remove(\"UpdatePartition\"));",
          "199:         assertEquals(1, opCounts.remove(\"UpdateTopic\"));",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "116bc000c8c6533552321fe1d395629c2aa00bd9",
      "candidate_info": {
        "commit_hash": "116bc000c8c6533552321fe1d395629c2aa00bd9",
        "repo": "apache/kafka",
        "commit_url": "https://github.com/apache/kafka/commit/116bc000c8c6533552321fe1d395629c2aa00bd9",
        "files": [
          "core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala"
        ],
        "message": "MINOR: fix scala compile issue (#15343)\n\nReviewers: David Jacot <djacot@confluent.io>",
        "before_after_code_files": [
          "core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala"
          ],
          "candidate": [
            "core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala"
          ]
        }
      },
      "candidate_diff": {
        "core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala": [
          "File: core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala -> core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "233:     val errorLogs = mutable.Buffer[String]()",
          "235:     kraftMigrationZkWriter.handleSnapshot(image, (_, _, operation) => {",
          "236:       migrationState = operation.apply(migrationState)",
          "237:     })",
          "",
          "[Removed Lines]",
          "234:     val kraftMigrationZkWriter = new KRaftMigrationZkWriter(migrationClient, errorLogs.append)",
          "",
          "[Added Lines]",
          "234:     val kraftMigrationZkWriter = new KRaftMigrationZkWriter(migrationClient, msg => errorLogs.append(msg))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "276:   def testAclUpdateAndDelete(): Unit = {",
          "277:     zkClient.createAclPaths()",
          "278:     val errorLogs = mutable.Buffer[String]()",
          "281:     val topicName = \"topic-\" + Uuid.randomUuid()",
          "282:     val otherName = \"other-\" + Uuid.randomUuid()",
          "",
          "[Removed Lines]",
          "279:     val kraftMigrationZkWriter = new KRaftMigrationZkWriter(migrationClient, errorLogs.append)",
          "",
          "[Added Lines]",
          "279:     val kraftMigrationZkWriter = new KRaftMigrationZkWriter(migrationClient, msg => errorLogs.append(msg))",
          "",
          "---------------"
        ]
      }
    }
  ]
}