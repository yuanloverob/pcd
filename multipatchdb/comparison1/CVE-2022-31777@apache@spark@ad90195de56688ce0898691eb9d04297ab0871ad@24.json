{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "671539de00c1da817859de66345e122cac01a2ee",
      "candidate_info": {
        "commit_hash": "671539de00c1da817859de66345e122cac01a2ee",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/671539de00c1da817859de66345e122cac01a2ee",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InlineCTE.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PushdownPredicatesAndPruneColumnsForCTEDef.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceCTERefWithRepartition.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisTest.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala",
          "sql/core/src/test/resources/sql-tests/inputs/subquery/scalar-subquery/scalar-subquery-select.sql",
          "sql/core/src/test/resources/sql-tests/results/subquery/scalar-subquery/scalar-subquery-select.sql.out",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23a.sf100/explain.txt",
          "sql/core/src/test/resources/tpcds-plan-stability/approved-plans-v1_4/q23b.sf100/explain.txt",
          "sql/core/src/test/scala/org/apache/spark/sql/CTEInlineSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ],
        "message": "[SPARK-37670][SQL] Support predicate pushdown and column pruning for de-duped CTEs\n\nThis PR adds predicate push-down and column pruning to CTEs that are not inlined as well as fixes a few potential correctness issues:\n  1) Replace (previously not inlined) CTE refs with Repartition operations at the end of logical plan optimization so that WithCTE is not carried over to physical plan. As a result, we can simplify the logic of physical planning, as well as avoid a correctness issue where the logical link of a physical plan node can point to `WithCTE` and lead to unexpected behaviors in AQE, e.g., class cast exceptions in DPP.\n  2) Pull (not inlined) CTE defs from subqueries up to the main query level, in order to avoid creating copies of the same CTE def during predicate push-downs and other transformations.\n  3) Make CTE IDs more deterministic by starting from 0 for each query.\n\nImprove de-duped CTEs' performance with predicate pushdown and column pruning; fixes de-duped CTEs' correctness issues.\n\nNo.\n\nAdded UTs.\n\nCloses #34929 from maryannxue/cte-followup.\n\nLead-authored-by: Maryann Xue <maryann.xue@gmail.com>\nCo-authored-by: Wenchen Fan <wenchen@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 175e429cca29c2314ee029bf009ed5222c0bffad)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InlineCTE.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InlineCTE.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PushdownPredicatesAndPruneColumnsForCTEDef.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PushdownPredicatesAndPruneColumnsForCTEDef.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceCTERefWithRepartition.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceCTERefWithRepartition.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisTest.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisTest.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala",
          "sql/core/src/test/resources/sql-tests/inputs/subquery/scalar-subquery/scalar-subquery-select.sql||sql/core/src/test/resources/sql-tests/inputs/subquery/scalar-subquery/scalar-subquery-select.sql",
          "sql/core/src/test/scala/org/apache/spark/sql/CTEInlineSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/CTEInlineSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CTESubstitution.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "69:     if (cteDefs.isEmpty) {",
          "70:       substituted",
          "71:     } else if (substituted eq lastSubstituted.get) {",
          "73:     } else {",
          "74:       var done = false",
          "75:       substituted.resolveOperatorsWithPruning(_ => !done) {",
          "76:         case p if p eq lastSubstituted.get =>",
          "77:           done = true",
          "79:       }",
          "80:     }",
          "81:   }",
          "",
          "[Removed Lines]",
          "72:       WithCTE(substituted, cteDefs.toSeq)",
          "78:           WithCTE(p, cteDefs.toSeq)",
          "",
          "[Added Lines]",
          "72:       WithCTE(substituted, cteDefs.sortBy(_.id).toSeq)",
          "78:           WithCTE(p, cteDefs.sortBy(_.id).toSeq)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "203:       cteDefs: mutable.ArrayBuffer[CTERelationDef]): Seq[(String, CTERelationDef)] = {",
          "204:     val resolvedCTERelations = new mutable.ArrayBuffer[(String, CTERelationDef)](relations.size)",
          "205:     for ((name, relation) <- relations) {",
          "206:       val innerCTEResolved = if (isLegacy) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "206:       val lastCTEDefCount = cteDefs.length",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "211:       } else {",
          "214:         traverseAndSubstituteCTE(relation, isCommand, cteDefs)._1",
          "215:       }",
          "217:       val substituted =",
          "218:         substituteCTE(innerCTEResolved, isLegacy || isCommand, resolvedCTERelations.toSeq)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "227:       if (cteDefs.length > lastCTEDefCount) {",
          "235:         for (i <- lastCTEDefCount until cteDefs.length) {",
          "236:           val substituted =",
          "237:             substituteCTE(cteDefs(i).child, isLegacy || isCommand, resolvedCTERelations.toSeq)",
          "238:           cteDefs(i) = cteDefs(i).copy(child = substituted)",
          "239:         }",
          "240:       }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import org.apache.spark.sql.catalyst.expressions._",
          "23: import org.apache.spark.sql.catalyst.expressions.SubExprUtils._",
          "24: import org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression",
          "26: import org.apache.spark.sql.catalyst.plans._",
          "27: import org.apache.spark.sql.catalyst.plans.logical._",
          "28: import org.apache.spark.sql.catalyst.trees.TreeNodeTag",
          "",
          "[Removed Lines]",
          "25: import org.apache.spark.sql.catalyst.optimizer.{BooleanSimplification, DecorrelateInnerQuery}",
          "",
          "[Added Lines]",
          "25: import org.apache.spark.sql.catalyst.optimizer.{BooleanSimplification, DecorrelateInnerQuery, InlineCTE}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "95:   def checkAnalysis(plan: LogicalPlan): Unit = {",
          "100:       case p if p.analyzed => // Skip already analyzed sub-plans",
          "",
          "[Removed Lines]",
          "98:     plan.foreachUp {",
          "",
          "[Added Lines]",
          "99:     val inlineCTE = InlineCTE(alwaysInline = true)",
          "100:     inlineCTE(plan).foreachUp {",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InlineCTE.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InlineCTE.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InlineCTE.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InlineCTE.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "40:   override def apply(plan: LogicalPlan): LogicalPlan = {",
          "41:     if (!plan.isInstanceOf[Subquery] && plan.containsPattern(CTE)) {",
          "42:       val cteMap = mutable.HashMap.empty[Long, (CTERelationDef, Int)]",
          "43:       buildCTEMap(plan, cteMap)",
          "45:     } else {",
          "46:       plan",
          "47:     }",
          "48:   }",
          "",
          "[Removed Lines]",
          "39: object InlineCTE extends Rule[LogicalPlan] {",
          "44:       inlineCTE(plan, cteMap, forceInline = false)",
          "50:   private def shouldInline(cteDef: CTERelationDef, refCount: Int): Boolean = {",
          "",
          "[Added Lines]",
          "41: case class InlineCTE(alwaysInline: Boolean = false) extends Rule[LogicalPlan] {",
          "47:       val notInlined = mutable.ArrayBuffer.empty[CTERelationDef]",
          "48:       val inlined = inlineCTE(plan, cteMap, notInlined)",
          "51:       if (notInlined.isEmpty) {",
          "52:         inlined",
          "53:       } else {",
          "54:         WithCTE(inlined, notInlined.toSeq)",
          "55:       }",
          "61:   private def shouldInline(cteDef: CTERelationDef, refCount: Int): Boolean = alwaysInline || {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "93:   private def inlineCTE(",
          "94:       plan: LogicalPlan,",
          "95:       cteMap: mutable.HashMap[Long, (CTERelationDef, Int)],",
          "98:       case WithCTE(child, cteDefs) =>",
          "100:         cteDefs.foreach { cteDef =>",
          "101:           val (cte, refCount) = cteMap(cteDef.id)",
          "102:           if (refCount > 0) {",
          "104:             cteMap.update(cteDef.id, (inlined, refCount))",
          "106:               notInlined.append(inlined)",
          "107:             }",
          "108:           }",
          "109:         }",
          "112:       case ref: CTERelationRef =>",
          "113:         val (cteDef, refCount) = cteMap(ref.cteId)",
          "115:           if (ref.outputSet == cteDef.outputSet) {",
          "116:             cteDef.child",
          "117:           } else {",
          "",
          "[Removed Lines]",
          "96:       forceInline: Boolean): LogicalPlan = {",
          "97:     val (stripped, notInlined) = plan match {",
          "99:         val notInlined = mutable.ArrayBuffer.empty[CTERelationDef]",
          "103:             val inlined = cte.copy(child = inlineCTE(cte.child, cteMap, forceInline))",
          "105:             if (!forceInline && !shouldInline(inlined, refCount)) {",
          "110:         (inlineCTE(child, cteMap, forceInline), notInlined.toSeq)",
          "114:         val newRef = if (forceInline || shouldInline(cteDef, refCount)) {",
          "",
          "[Added Lines]",
          "107:       notInlined: mutable.ArrayBuffer[CTERelationDef]): LogicalPlan = {",
          "108:     plan match {",
          "113:             val inlined = cte.copy(child = inlineCTE(cte.child, cteMap, notInlined))",
          "115:             if (!shouldInline(inlined, refCount)) {",
          "120:         inlineCTE(child, cteMap, notInlined)",
          "124:         if (shouldInline(cteDef, refCount)) {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "125:         } else {",
          "126:           ref",
          "127:         }",
          "130:       case _ if plan.containsPattern(CTE) =>",
          "133:           .transformExpressionsWithPruning(_.containsAllPatterns(PLAN_EXPRESSION, CTE)) {",
          "134:             case e: SubqueryExpression =>",
          "136:           }",
          "146:     }",
          "147:   }",
          "148: }",
          "",
          "[Removed Lines]",
          "128:         (newRef, Seq.empty)",
          "131:         val newPlan = plan",
          "132:           .withNewChildren(plan.children.map(child => inlineCTE(child, cteMap, forceInline)))",
          "135:               e.withNewPlan(inlineCTE(e.plan, cteMap, forceInline = e.isCorrelated))",
          "137:         (newPlan, Seq.empty)",
          "139:       case _ => (plan, Seq.empty)",
          "140:     }",
          "142:     if (notInlined.isEmpty) {",
          "143:       stripped",
          "144:     } else {",
          "145:       WithCTE(stripped, notInlined)",
          "",
          "[Added Lines]",
          "140:         plan",
          "141:           .withNewChildren(plan.children.map(child => inlineCTE(child, cteMap, notInlined)))",
          "144:               e.withNewPlan(inlineCTE(e.plan, cteMap, notInlined))",
          "147:       case _ => plan",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "128:         OptimizeUpdateFields,",
          "129:         SimplifyExtractValueOps,",
          "130:         OptimizeCsvJsonExprs,",
          "132:         extendedOperatorOptimizationRules",
          "134:     val operatorOptimizationBatch: Seq[Batch] = {",
          "",
          "[Removed Lines]",
          "131:         CombineConcats) ++",
          "",
          "[Added Lines]",
          "131:         CombineConcats,",
          "132:         PushdownPredicatesAndPruneColumnsForCTEDef) ++",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "147:     }",
          "149:     val batches = (Batch(\"Eliminate Distinct\", Once, EliminateDistinct) ::",
          "",
          "[Removed Lines]",
          "154:     Batch(\"Finish Analysis\", Once,",
          "155:       EliminateResolvedHint,",
          "156:       EliminateSubqueryAliases,",
          "157:       EliminateView,",
          "158:       InlineCTE,",
          "159:       ReplaceExpressions,",
          "160:       RewriteNonCorrelatedExists,",
          "161:       PullOutGroupingExpressions,",
          "162:       ComputeCurrentTime,",
          "163:       ReplaceCurrentLike(catalogManager),",
          "164:       SpecialDatetimeValues,",
          "165:       RewriteAsOfJoin) ::",
          "",
          "[Added Lines]",
          "151:     Batch(\"Finish Analysis\", Once, FinishAnalysis) ::",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "174:     Batch(\"Union\", Once,",
          "175:       RemoveNoopOperators,",
          "176:       CombineUnions,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "160:     Batch(\"Inline CTE\", Once,",
          "161:       InlineCTE()) ::",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "207:       RemoveLiteralFromGroupExpressions,",
          "208:       RemoveRepetitionFromGroupExpressions) :: Nil ++",
          "209:     operatorOptimizationBatch) :+",
          "212:     Batch(\"Pre CBO Rules\", Once, preCBORules: _*) :+",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "198:     Batch(\"Clean Up Temporary CTE Info\", Once, CleanUpTempCTEInfo) :+",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "267:   def nonExcludableRules: Seq[String] =",
          "276:       RewriteDistinctAggregates.ruleName ::",
          "277:       ReplaceDeduplicateWithAggregate.ruleName ::",
          "278:       ReplaceIntersectWithSemiJoin.ruleName ::",
          "",
          "[Removed Lines]",
          "268:     EliminateDistinct.ruleName ::",
          "269:       EliminateResolvedHint.ruleName ::",
          "270:       EliminateSubqueryAliases.ruleName ::",
          "271:       EliminateView.ruleName ::",
          "272:       ReplaceExpressions.ruleName ::",
          "273:       ComputeCurrentTime.ruleName ::",
          "274:       SpecialDatetimeValues.ruleName ::",
          "275:       ReplaceCurrentLike(catalogManager).ruleName ::",
          "",
          "[Added Lines]",
          "257:     FinishAnalysis.ruleName ::",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "286:       RewritePredicateSubquery.ruleName ::",
          "287:       NormalizeFloatingNumbers.ruleName ::",
          "288:       ReplaceUpdateFieldsExpression.ruleName ::",
          "291:       RewriteLateralSubquery.ruleName :: Nil",
          "",
          "[Removed Lines]",
          "289:       PullOutGroupingExpressions.ruleName ::",
          "290:       RewriteAsOfJoin.ruleName ::",
          "",
          "[Added Lines]",
          "276:   object FinishAnalysis extends Rule[LogicalPlan] {",
          "281:     private val rules = Seq(",
          "282:       EliminateResolvedHint,",
          "283:       EliminateSubqueryAliases,",
          "284:       EliminateView,",
          "285:       ReplaceExpressions,",
          "286:       RewriteNonCorrelatedExists,",
          "287:       PullOutGroupingExpressions,",
          "288:       ComputeCurrentTime,",
          "289:       ReplaceCurrentLike(catalogManager),",
          "290:       SpecialDatetimeValues,",
          "291:       RewriteAsOfJoin)",
          "293:     override def apply(plan: LogicalPlan): LogicalPlan = {",
          "294:       rules.foldLeft(plan) { case (sp, rule) => rule.apply(sp) }",
          "295:         .transformAllExpressionsWithPruning(_.containsPattern(PLAN_EXPRESSION)) {",
          "296:           case s: SubqueryExpression =>",
          "297:             val Subquery(newPlan, _) = apply(Subquery.fromExpression(s))",
          "298:             s.withNewPlan(newPlan)",
          "299:         }",
          "300:     }",
          "301:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PushdownPredicatesAndPruneColumnsForCTEDef.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PushdownPredicatesAndPruneColumnsForCTEDef.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PushdownPredicatesAndPruneColumnsForCTEDef.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PushdownPredicatesAndPruneColumnsForCTEDef.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.sql.catalyst.optimizer",
          "20: import scala.collection.mutable",
          "22: import org.apache.spark.sql.catalyst.expressions.{And, Attribute, AttributeSet, Expression, Literal, Or, SubqueryExpression}",
          "23: import org.apache.spark.sql.catalyst.planning.ScanOperation",
          "24: import org.apache.spark.sql.catalyst.plans.logical._",
          "25: import org.apache.spark.sql.catalyst.rules.Rule",
          "26: import org.apache.spark.sql.catalyst.trees.TreePattern.CTE",
          "32: object PushdownPredicatesAndPruneColumnsForCTEDef extends Rule[LogicalPlan] {",
          "35:   private type CTEMap = mutable.HashMap[Long, (CTERelationDef, Int, Seq[Expression], AttributeSet)]",
          "37:   override def apply(plan: LogicalPlan): LogicalPlan = {",
          "38:     if (!plan.isInstanceOf[Subquery] && plan.containsPattern(CTE)) {",
          "39:       val cteMap = new CTEMap",
          "40:       gatherPredicatesAndAttributes(plan, cteMap)",
          "41:       pushdownPredicatesAndAttributes(plan, cteMap)",
          "42:     } else {",
          "43:       plan",
          "44:     }",
          "45:   }",
          "47:   private def restoreCTEDefAttrs(",
          "48:       input: Seq[Expression],",
          "49:       mapping: Map[Attribute, Expression]): Seq[Expression] = {",
          "50:     input.map(e => e.transform {",
          "51:       case a: Attribute =>",
          "52:         mapping.keys.find(_.semanticEquals(a)).map(mapping).getOrElse(a)",
          "53:     })",
          "54:   }",
          "63:   private def gatherPredicatesAndAttributes(plan: LogicalPlan, cteMap: CTEMap): Unit = {",
          "64:     plan match {",
          "65:       case WithCTE(child, cteDefs) =>",
          "66:         cteDefs.zipWithIndex.foreach { case (cteDef, precedence) =>",
          "67:           gatherPredicatesAndAttributes(cteDef.child, cteMap)",
          "68:           cteMap.put(cteDef.id, (cteDef, precedence, Seq.empty, AttributeSet.empty))",
          "69:         }",
          "70:         gatherPredicatesAndAttributes(child, cteMap)",
          "72:       case ScanOperation(projects, predicates, ref: CTERelationRef) =>",
          "73:         val (cteDef, precedence, preds, attrs) = cteMap(ref.cteId)",
          "74:         val attrMapping = ref.output.zip(cteDef.output).map{ case (r, d) => r -> d }.toMap",
          "75:         val newPredicates = if (isTruePredicate(preds)) {",
          "76:           preds",
          "77:         } else {",
          "79:           val filteredPredicates = restoreCTEDefAttrs(predicates.filter(_.find {",
          "80:             case s: SubqueryExpression => s.plan.find {",
          "81:               case r: CTERelationRef =>",
          "85:                 !cteMap.contains(r.cteId) || cteMap(r.cteId)._2 >= precedence",
          "86:               case _ => false",
          "87:             }.nonEmpty",
          "88:             case _ => false",
          "89:           }.isEmpty), attrMapping).filter(_.references.forall(cteDef.outputSet.contains))",
          "90:           if (filteredPredicates.isEmpty) {",
          "91:             Seq(Literal.TrueLiteral)",
          "92:           } else {",
          "93:             preds :+ filteredPredicates.reduce(And)",
          "94:           }",
          "95:         }",
          "96:         val newAttributes = attrs ++",
          "97:           AttributeSet(restoreCTEDefAttrs(projects.flatMap(_.references), attrMapping)) ++",
          "98:           AttributeSet(restoreCTEDefAttrs(predicates.flatMap(_.references), attrMapping))",
          "100:         cteMap.update(ref.cteId, (cteDef, precedence, newPredicates, newAttributes))",
          "101:         plan.subqueriesAll.foreach(s => gatherPredicatesAndAttributes(s, cteMap))",
          "103:       case _ =>",
          "104:         plan.children.foreach(c => gatherPredicatesAndAttributes(c, cteMap))",
          "105:         plan.subqueries.foreach(s => gatherPredicatesAndAttributes(s, cteMap))",
          "106:     }",
          "107:   }",
          "121:   private def pushdownPredicatesAndAttributes(",
          "122:       plan: LogicalPlan,",
          "123:       cteMap: CTEMap): LogicalPlan = plan.transformWithSubqueries {",
          "124:     case cteDef @ CTERelationDef(child, id, originalPlanWithPredicates) =>",
          "125:       val (_, _, newPreds, newAttrSet) = cteMap(id)",
          "126:       val originalPlan = originalPlanWithPredicates.map(_._1).getOrElse(child)",
          "127:       val preds = originalPlanWithPredicates.map(_._2).getOrElse(Seq.empty)",
          "128:       if (!isTruePredicate(newPreds) &&",
          "129:           newPreds.exists(newPred => !preds.exists(_.semanticEquals(newPred)))) {",
          "130:         val newCombinedPred = newPreds.reduce(Or)",
          "131:         val newChild = if (needsPruning(originalPlan, newAttrSet)) {",
          "132:           Project(newAttrSet.toSeq, originalPlan)",
          "133:         } else {",
          "134:           originalPlan",
          "135:         }",
          "136:         CTERelationDef(Filter(newCombinedPred, newChild), id, Some((originalPlan, newPreds)))",
          "137:       } else if (needsPruning(cteDef.child, newAttrSet)) {",
          "138:         CTERelationDef(Project(newAttrSet.toSeq, cteDef.child), id, Some((originalPlan, preds)))",
          "139:       } else {",
          "140:         cteDef",
          "141:       }",
          "143:     case cteRef @ CTERelationRef(cteId, _, output, _) =>",
          "144:       val (cteDef, _, _, newAttrSet) = cteMap(cteId)",
          "145:       if (newAttrSet.size < output.size) {",
          "146:         val indices = newAttrSet.toSeq.map(cteDef.output.indexOf)",
          "147:         val newOutput = indices.map(output)",
          "148:         cteRef.copy(output = newOutput)",
          "149:       } else {",
          "152:         cteRef",
          "153:       }",
          "154:   }",
          "156:   private def isTruePredicate(predicates: Seq[Expression]): Boolean = {",
          "157:     predicates.length == 1 && predicates.head == Literal.TrueLiteral",
          "158:   }",
          "160:   private def needsPruning(sourcePlan: LogicalPlan, attributeSet: AttributeSet): Boolean = {",
          "161:     attributeSet.size < sourcePlan.outputSet.size && attributeSet.subsetOf(sourcePlan.outputSet)",
          "162:   }",
          "163: }",
          "169: object CleanUpTempCTEInfo extends Rule[LogicalPlan] {",
          "170:   override def apply(plan: LogicalPlan): LogicalPlan =",
          "171:     plan.transformWithPruning(_.containsPattern(CTE)) {",
          "172:       case cteDef @ CTERelationDef(_, _, Some(_)) =>",
          "173:         cteDef.copy(originalPlanWithPredicates = None)",
          "174:     }",
          "175: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceCTERefWithRepartition.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceCTERefWithRepartition.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceCTERefWithRepartition.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceCTERefWithRepartition.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.sql.catalyst.optimizer",
          "20: import scala.collection.mutable",
          "22: import org.apache.spark.sql.catalyst.analysis.DeduplicateRelations",
          "23: import org.apache.spark.sql.catalyst.expressions.{Alias, SubqueryExpression}",
          "24: import org.apache.spark.sql.catalyst.plans.Inner",
          "25: import org.apache.spark.sql.catalyst.plans.logical._",
          "26: import org.apache.spark.sql.catalyst.rules.Rule",
          "27: import org.apache.spark.sql.catalyst.trees.TreePattern.{CTE, PLAN_EXPRESSION}",
          "36: object ReplaceCTERefWithRepartition extends Rule[LogicalPlan] {",
          "38:   override def apply(plan: LogicalPlan): LogicalPlan = plan match {",
          "39:     case _: Subquery => plan",
          "40:     case _ =>",
          "41:       replaceWithRepartition(plan, mutable.HashMap.empty[Long, LogicalPlan])",
          "42:   }",
          "44:   private def replaceWithRepartition(",
          "45:       plan: LogicalPlan,",
          "46:       cteMap: mutable.HashMap[Long, LogicalPlan]): LogicalPlan = plan match {",
          "47:     case WithCTE(child, cteDefs) =>",
          "48:       cteDefs.foreach { cteDef =>",
          "49:         val inlined = replaceWithRepartition(cteDef.child, cteMap)",
          "50:         val withRepartition = if (inlined.isInstanceOf[RepartitionOperation]) {",
          "53:           inlined",
          "54:         } else {",
          "55:           Repartition(conf.numShufflePartitions, shuffle = true, inlined)",
          "56:         }",
          "57:         cteMap.put(cteDef.id, withRepartition)",
          "58:       }",
          "59:       replaceWithRepartition(child, cteMap)",
          "61:     case ref: CTERelationRef =>",
          "62:       val cteDefPlan = cteMap(ref.cteId)",
          "63:       if (ref.outputSet == cteDefPlan.outputSet) {",
          "64:         cteDefPlan",
          "65:       } else {",
          "66:         val ctePlan = DeduplicateRelations(",
          "67:           Join(cteDefPlan, cteDefPlan, Inner, None, JoinHint(None, None))).children(1)",
          "68:         val projectList = ref.output.zip(ctePlan.output).map { case (tgtAttr, srcAttr) =>",
          "69:           Alias(srcAttr, tgtAttr.name)(exprId = tgtAttr.exprId)",
          "70:         }",
          "71:         Project(projectList, ctePlan)",
          "72:       }",
          "74:     case _ if plan.containsPattern(CTE) =>",
          "75:       plan",
          "76:         .withNewChildren(plan.children.map(c => replaceWithRepartition(c, cteMap)))",
          "77:         .transformExpressionsWithPruning(_.containsAllPatterns(PLAN_EXPRESSION, CTE)) {",
          "78:           case e: SubqueryExpression =>",
          "79:             e.withNewPlan(replaceWithRepartition(e.plan, cteMap))",
          "80:         }",
          "82:     case _ => plan",
          "83:   }",
          "84: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "448:     subqueries ++ subqueries.flatMap(_.subqueriesAll)",
          "449:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "456:   def transformWithSubqueries(f: PartialFunction[PlanType, PlanType]): PlanType =",
          "457:     transformDownWithSubqueries(f)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "465:     }",
          "466:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "482:   def transformDownWithSubqueries(f: PartialFunction[PlanType, PlanType]): PlanType = {",
          "483:     val g: PartialFunction[PlanType, PlanType] = new PartialFunction[PlanType, PlanType] {",
          "484:       override def isDefinedAt(x: PlanType): Boolean = true",
          "486:       override def apply(plan: PlanType): PlanType = {",
          "487:         val transformed = f.applyOrElse[PlanType, PlanType](plan, identity)",
          "488:         transformed transformExpressionsDown {",
          "489:           case planExpression: PlanExpression[PlanType] =>",
          "490:             val newPlan = planExpression.plan.transformDownWithSubqueries(f)",
          "491:             planExpression.withNewPlan(newPlan)",
          "492:         }",
          "493:       }",
          "494:     }",
          "496:     transformDown(g)",
          "497:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "665:   final override val nodePatterns: Seq[TreePattern] = Seq(CTE)",
          "",
          "[Removed Lines]",
          "663: case class CTERelationDef(child: LogicalPlan, id: Long = CTERelationDef.newId) extends UnaryNode {",
          "",
          "[Added Lines]",
          "667: case class CTERelationDef(",
          "668:     child: LogicalPlan,",
          "669:     id: Long = CTERelationDef.newId,",
          "670:     originalPlanWithPredicates: Option[(LogicalPlan, Seq[Expression])] = None) extends UnaryNode {",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisTest.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisTest.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisTest.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisTest.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "108:         case v: View if v.isTempViewStoringAnalyzedPlan => v.child",
          "109:       }",
          "110:       val actualPlan = if (inlineCTE) {",
          "112:       } else {",
          "113:         transformed",
          "114:       }",
          "",
          "[Removed Lines]",
          "111:         InlineCTE(transformed)",
          "",
          "[Added Lines]",
          "111:         val inlineCTE = InlineCTE()",
          "112:         inlineCTE(transformed)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/QueryExecution.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import java.util.UUID",
          "22: import java.util.concurrent.atomic.AtomicLong",
          "26: import org.apache.hadoop.fs.Path",
          "28: import org.apache.spark.internal.Logging",
          "",
          "[Removed Lines]",
          "24: import scala.collection.mutable",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "32: import org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker",
          "33: import org.apache.spark.sql.catalyst.expressions.codegen.ByteCodeStats",
          "34: import org.apache.spark.sql.catalyst.plans.QueryPlan",
          "36: import org.apache.spark.sql.catalyst.rules.{PlanChangeLogger, Rule}",
          "37: import org.apache.spark.sql.catalyst.util.StringUtils.PlanStringConcat",
          "38: import org.apache.spark.sql.catalyst.util.truncatedString",
          "",
          "[Removed Lines]",
          "35: import org.apache.spark.sql.catalyst.plans.logical.{AppendData, Command, CommandResult, CreateTableAsSelect, CTERelationDef, LogicalPlan, OverwriteByExpression, OverwritePartitionsDynamic, ReplaceTableAsSelect, ReturnAnswer}",
          "",
          "[Added Lines]",
          "33: import org.apache.spark.sql.catalyst.plans.logical.{AppendData, Command, CommandResult, CreateTableAsSelect, LogicalPlan, OverwriteByExpression, OverwritePartitionsDynamic, ReplaceTableAsSelect, ReturnAnswer}",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "65:   protected def planner = sparkSession.sessionState.planner",
          "78:   def assertAnalyzed(): Unit = analyzed",
          "80:   def assertSupported(): Unit = {",
          "",
          "[Removed Lines]",
          "68:   private val cteMap = mutable.HashMap.empty[Long, CTERelationDef]",
          "70:   def withCteMap[T](f: => T): T = {",
          "71:     val old = QueryExecution.currentCteMap.get()",
          "72:     QueryExecution.currentCteMap.set(cteMap)",
          "73:     try f finally {",
          "74:       QueryExecution.currentCteMap.set(old)",
          "75:     }",
          "76:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "148:   private def assertOptimized(): Unit = optimizedPlan",
          "153:     assertOptimized()",
          "",
          "[Removed Lines]",
          "150:   lazy val sparkPlan: SparkPlan = withCteMap {",
          "",
          "[Added Lines]",
          "137:   lazy val sparkPlan: SparkPlan = {",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "166:     assertOptimized()",
          "",
          "[Removed Lines]",
          "163:   lazy val executedPlan: SparkPlan = withCteMap {",
          "",
          "[Added Lines]",
          "150:   lazy val executedPlan: SparkPlan = {",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "497:     val preparationRules = preparations(session, Option(InsertAdaptiveSparkPlan(context)), true)",
          "498:     prepareForExecution(preparationRules, sparkPlan.clone())",
          "499:   }",
          "504: }",
          "",
          "[Removed Lines]",
          "501:   private val currentCteMap = new ThreadLocal[mutable.HashMap[Long, CTERelationDef]]()",
          "503:   def cteMap: mutable.HashMap[Long, CTERelationDef] = currentCteMap.get()",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "76:       ColumnPruning,",
          "77:       PushPredicateThroughNonJoin,",
          "78:       RemoveNoopOperators) :+",
          "81:   override def nonExcludableRules: Seq[String] = super.nonExcludableRules :+",
          "82:     ExtractPythonUDFFromJoinCondition.ruleName :+",
          "",
          "[Removed Lines]",
          "79:     Batch(\"User Provided Optimizers\", fixedPoint, experimentalMethods.extraOptimizations: _*)",
          "",
          "[Added Lines]",
          "79:     Batch(\"User Provided Optimizers\", fixedPoint, experimentalMethods.extraOptimizations: _*) :+",
          "80:     Batch(\"Replace CTE with Repartition\", Once, ReplaceCTERefWithRepartition)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlanner.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:       JoinSelection ::",
          "45:       InMemoryScans ::",
          "46:       SparkScripts ::",
          "48:       BasicOperators :: Nil)",
          "",
          "[Removed Lines]",
          "47:       WithCTEStrategy ::",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: import org.apache.spark.sql.catalyst.planning._",
          "30: import org.apache.spark.sql.catalyst.plans._",
          "31: import org.apache.spark.sql.catalyst.plans.logical._",
          "33: import org.apache.spark.sql.catalyst.streaming.{InternalOutputModes, StreamingRelationV2}",
          "34: import org.apache.spark.sql.errors.{QueryCompilationErrors, QueryExecutionErrors}",
          "35: import org.apache.spark.sql.execution.aggregate.AggUtils",
          "",
          "[Removed Lines]",
          "32: import org.apache.spark.sql.catalyst.plans.physical.RoundRobinPartitioning",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "675:     }",
          "676:   }",
          "708:   object BasicOperators extends Strategy {",
          "709:     def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match {",
          "710:       case d: DataWritingCommand => DataWritingCommandExec(d, planLater(d.query)) :: Nil",
          "",
          "[Removed Lines]",
          "681:   object WithCTEStrategy extends Strategy {",
          "682:     override def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match {",
          "683:       case WithCTE(plan, cteDefs) =>",
          "684:         val cteMap = QueryExecution.cteMap",
          "685:         cteDefs.foreach { cteDef =>",
          "686:           cteMap.put(cteDef.id, cteDef)",
          "687:         }",
          "688:         planLater(plan) :: Nil",
          "690:       case r: CTERelationRef =>",
          "691:         val ctePlan = QueryExecution.cteMap(r.cteId).child",
          "692:         val projectList = r.output.zip(ctePlan.output).map { case (tgtAttr, srcAttr) =>",
          "693:           Alias(srcAttr, tgtAttr.name)(exprId = tgtAttr.exprId)",
          "694:         }",
          "695:         val newPlan = Project(projectList, ctePlan)",
          "699:         exchange.ShuffleExchangeExec(",
          "700:           RoundRobinPartitioning(conf.numShufflePartitions),",
          "701:           planLater(newPlan),",
          "702:           REPARTITION_BY_COL) :: Nil",
          "704:       case _ => Nil",
          "705:     }",
          "706:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "148:     collapseCodegenStagesRule",
          "149:   )",
          "154:     val optimized = queryStageOptimizerRules.foldLeft(plan) { case (latestPlan, rule) =>",
          "155:       val applied = rule.apply(latestPlan)",
          "156:       val result = rule match {",
          "",
          "[Removed Lines]",
          "151:   private def optimizeQueryStage(",
          "152:       plan: SparkPlan,",
          "153:       isFinalStage: Boolean): SparkPlan = context.qe.withCteMap {",
          "",
          "[Added Lines]",
          "151:   private def optimizeQueryStage(plan: SparkPlan, isFinalStage: Boolean): SparkPlan = {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "645:     logicalPlan.invalidateStatsCache()",
          "646:     val optimized = optimizer.execute(logicalPlan)",
          "647:     val sparkPlan = context.session.sessionState.planner.plan(ReturnAnswer(optimized)).next()",
          "",
          "[Removed Lines]",
          "643:   private def reOptimize(",
          "644:       logicalPlan: LogicalPlan): (SparkPlan, LogicalPlan) = context.qe.withCteMap {",
          "",
          "[Added Lines]",
          "641:   private def reOptimize(logicalPlan: LogicalPlan): (SparkPlan, LogicalPlan) = {",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/subquery/scalar-subquery/scalar-subquery-select.sql||sql/core/src/test/resources/sql-tests/inputs/subquery/scalar-subquery/scalar-subquery-select.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/subquery/scalar-subquery/scalar-subquery-select.sql -> sql/core/src/test/resources/sql-tests/inputs/subquery/scalar-subquery/scalar-subquery-select.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "145: SELECT t1c, t1d, (SELECT c + d FROM (SELECT t1c AS c, t1d AS d)) FROM t1;",
          "146: SELECT t1c, (SELECT SUM(c) FROM (SELECT t1c AS c)) FROM t1;",
          "147: SELECT t1a, (SELECT SUM(t2b) FROM t2 JOIN (SELECT t1a AS a) ON t2a = a) FROM t1;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "149: -- CTE in correlated scalar subqueries",
          "150: CREATE OR REPLACE TEMPORARY VIEW t1 AS VALUES (0, 1), (1, 2) t1(c1, c2);",
          "151: CREATE OR REPLACE TEMPORARY VIEW t2 AS VALUES (0, 2), (0, 3) t2(c1, c2);",
          "153: -- Single row subquery",
          "154: SELECT c1, (WITH t AS (SELECT 1 AS a) SELECT a + c1 FROM t) FROM t1;",
          "155: -- Correlation in CTE.",
          "156: SELECT c1, (WITH t AS (SELECT * FROM t2 WHERE c1 = t1.c1) SELECT SUM(c2) FROM t) FROM t1;",
          "157: -- Multiple CTE definitions.",
          "158: SELECT c1, (",
          "159:     WITH t3 AS (SELECT c1 + 1 AS c1, c2 + 1 AS c2 FROM t2),",
          "160:     t4 AS (SELECT * FROM t3 WHERE t1.c1 = c1)",
          "161:     SELECT SUM(c2) FROM t4",
          "162: ) FROM t1;",
          "163: -- Multiple CTE references.",
          "164: SELECT c1, (",
          "165:     WITH t AS (SELECT * FROM t2)",
          "166:     SELECT SUM(c2) FROM (SELECT c1, c2 FROM t UNION SELECT c2, c1 FROM t) r(c1, c2)",
          "167:     WHERE c1 = t1.c1",
          "168: ) FROM t1;",
          "169: -- Reference CTE in both the main query and the subquery.",
          "170: WITH v AS (SELECT * FROM t2)",
          "171: SELECT * FROM t1 WHERE c1 > (",
          "172:     WITH t AS (SELECT * FROM t2)",
          "173:     SELECT COUNT(*) FROM v WHERE c1 = t1.c1 AND c1 > (SELECT SUM(c2) FROM t WHERE c1 = v.c1)",
          "174: );",
          "175: -- Single row subquery that references CTE in the main query.",
          "176: WITH t AS (SELECT 1 AS a)",
          "177: SELECT c1, (SELECT a FROM t WHERE a = c1) FROM t1;",
          "178: -- Multiple CTE references with non-deterministic CTEs.",
          "179: WITH",
          "180: v1 AS (SELECT c1, c2, rand(0) c3 FROM t1),",
          "181: v2 AS (SELECT c1, c2, rand(0) c4 FROM v1 WHERE c3 IN (SELECT c3 FROM v1))",
          "182: SELECT c1, (",
          "183:     WITH v3 AS (SELECT c1, c2, rand(0) c5 FROM t2)",
          "184:     SELECT COUNT(*) FROM (",
          "185:         SELECT * FROM v2 WHERE c1 > 0",
          "186:         UNION SELECT * FROM v2 WHERE c2 > 0",
          "187:         UNION SELECT * FROM v3 WHERE c2 > 0",
          "188:     ) WHERE c1 = v1.c1",
          "189: ) FROM v1;",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/CTEInlineSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/CTEInlineSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/CTEInlineSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/CTEInlineSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql",
          "21: import org.apache.spark.sql.execution.adaptive._",
          "22: import org.apache.spark.sql.execution.exchange.ReusedExchangeExec",
          "23: import org.apache.spark.sql.internal.SQLConf",
          "",
          "[Removed Lines]",
          "20: import org.apache.spark.sql.catalyst.plans.logical.WithCTE",
          "",
          "[Added Lines]",
          "20: import org.apache.spark.sql.catalyst.expressions.{And, GreaterThan, LessThan, Literal, Or}",
          "21: import org.apache.spark.sql.catalyst.plans.logical.{Filter, Project, RepartitionOperation, WithCTE}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "42:          \"\"\".stripMargin)",
          "43:       checkAnswer(df, Nil)",
          "44:       assert(",
          "46:         \"Non-deterministic With-CTE with multiple references should be not inlined.\")",
          "47:     }",
          "48:   }",
          "",
          "[Removed Lines]",
          "45:         df.queryExecution.optimizedPlan.exists(_.isInstanceOf[WithCTE]),",
          "",
          "[Added Lines]",
          "46:         df.queryExecution.optimizedPlan.exists(_.isInstanceOf[RepartitionOperation]),",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "59:          \"\"\".stripMargin)",
          "60:       checkAnswer(df, Nil)",
          "61:       assert(",
          "63:         \"Non-deterministic With-CTE with multiple references should be not inlined.\")",
          "64:     }",
          "65:   }",
          "",
          "[Removed Lines]",
          "62:         df.queryExecution.optimizedPlan.exists(_.isInstanceOf[WithCTE]),",
          "",
          "[Added Lines]",
          "63:         df.queryExecution.optimizedPlan.exists(_.isInstanceOf[RepartitionOperation]),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "79:         df.queryExecution.analyzed.exists(_.isInstanceOf[WithCTE]),",
          "80:         \"With-CTE should not be inlined in analyzed plan.\")",
          "81:       assert(",
          "83:         \"With-CTE with one reference should be inlined in optimized plan.\")",
          "84:     }",
          "85:   }",
          "",
          "[Removed Lines]",
          "82:         !df.queryExecution.optimizedPlan.exists(_.isInstanceOf[WithCTE]),",
          "",
          "[Added Lines]",
          "83:         !df.queryExecution.optimizedPlan.exists(_.isInstanceOf[RepartitionOperation]),",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "107:         \"With-CTE should contain 2 CTE defs after analysis.\")",
          "108:       assert(",
          "109:         df.queryExecution.optimizedPlan.collect {",
          "112:         \"With-CTE should contain 2 CTE def after optimization.\")",
          "113:     }",
          "114:   }",
          "",
          "[Removed Lines]",
          "110:           case WithCTE(_, cteDefs) => cteDefs",
          "111:         }.head.length == 2,",
          "",
          "[Added Lines]",
          "111:           case r: RepartitionOperation => r",
          "112:         }.length == 6,",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "136:         \"With-CTE should contain 2 CTE defs after analysis.\")",
          "137:       assert(",
          "138:         df.queryExecution.optimizedPlan.collect {",
          "141:         \"One CTE def should be inlined after optimization.\")",
          "142:     }",
          "143:   }",
          "",
          "[Removed Lines]",
          "139:           case WithCTE(_, cteDefs) => cteDefs",
          "140:         }.head.length == 1,",
          "",
          "[Added Lines]",
          "140:           case r: RepartitionOperation => r",
          "141:         }.length == 4,",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "163:         \"With-CTE should contain 2 CTE defs after analysis.\")",
          "164:       assert(",
          "165:         df.queryExecution.optimizedPlan.collect {",
          "167:         }.isEmpty,",
          "168:         \"CTEs with one reference should all be inlined after optimization.\")",
          "169:     }",
          "",
          "[Removed Lines]",
          "166:           case WithCTE(_, cteDefs) => cteDefs",
          "",
          "[Added Lines]",
          "167:           case r: RepartitionOperation => r",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "248:         \"With-CTE should contain 2 CTE defs after analysis.\")",
          "249:       assert(",
          "250:         df.queryExecution.optimizedPlan.collect {",
          "252:         }.isEmpty,",
          "253:         \"Deterministic CTEs should all be inlined after optimization.\")",
          "254:     }",
          "",
          "[Removed Lines]",
          "251:           case WithCTE(_, cteDefs) => cteDefs",
          "",
          "[Added Lines]",
          "252:           case r: RepartitionOperation => r",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "272:       assert(ex.message.contains(\"Table or view not found: v1\"))",
          "273:     }",
          "274:   }",
          "275: }",
          "277: class CTEInlineSuiteAEOff extends CTEInlineSuiteBase with DisableAdaptiveExecutionSuite",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "277:   test(\"CTE Predicate push-down and column pruning\") {",
          "278:     withView(\"t\") {",
          "279:       Seq((0, 1), (1, 2)).toDF(\"c1\", \"c2\").createOrReplaceTempView(\"t\")",
          "280:       val df = sql(",
          "281:         s\"\"\"with",
          "282:            |v as (",
          "283:            |  select c1, c2, 's' c3, rand() c4 from t",
          "284:            |),",
          "285:            |vv as (",
          "286:            |  select v1.c1, v1.c2, rand() c5 from v v1, v v2",
          "287:            |  where v1.c1 > 0 and v1.c3 = 's' and v1.c2 = v2.c2",
          "288:            |)",
          "289:            |select vv1.c1, vv1.c2, vv2.c1, vv2.c2 from vv vv1, vv vv2",
          "290:            |where vv1.c2 > 0 and vv2.c2 > 0 and vv1.c1 = vv2.c1",
          "291:          \"\"\".stripMargin)",
          "292:       checkAnswer(df, Row(1, 2, 1, 2) :: Nil)",
          "293:       assert(",
          "294:         df.queryExecution.analyzed.collect {",
          "295:           case WithCTE(_, cteDefs) => cteDefs",
          "296:         }.head.length == 2,",
          "297:         \"With-CTE should contain 2 CTE defs after analysis.\")",
          "298:       val cteRepartitions = df.queryExecution.optimizedPlan.collect {",
          "299:         case r: RepartitionOperation => r",
          "300:       }",
          "301:       assert(cteRepartitions.length == 6,",
          "302:         \"CTE should not be inlined after optimization.\")",
          "303:       val distinctCteRepartitions = cteRepartitions.map(_.canonicalized).distinct",
          "305:       assert(distinctCteRepartitions.length == 2)",
          "306:       assert(distinctCteRepartitions(1).collectFirst {",
          "307:         case p: Project if p.projectList.length == 3 => p",
          "308:       }.isDefined, \"CTE columns should be pruned.\")",
          "309:       assert(distinctCteRepartitions(1).collectFirst {",
          "310:         case f: Filter if f.condition.semanticEquals(GreaterThan(f.output(1), Literal(0))) => f",
          "311:       }.isDefined, \"Predicate 'c2 > 0' should be pushed down to the CTE def 'v'.\")",
          "312:       assert(distinctCteRepartitions(0).collectFirst {",
          "313:         case f: Filter if f.condition.find(_.semanticEquals(f.output(0))).isDefined => f",
          "314:       }.isDefined, \"CTE 'vv' definition contains predicate 'c1 > 0'.\")",
          "315:       assert(distinctCteRepartitions(1).collectFirst {",
          "316:         case f: Filter if f.condition.find(_.semanticEquals(f.output(0))).isDefined => f",
          "317:       }.isEmpty, \"Predicate 'c1 > 0' should be not pushed down to the CTE def 'v'.\")",
          "319:       assert(",
          "320:         collectWithSubqueries(df.queryExecution.executedPlan) {",
          "321:           case r: ReusedExchangeExec => r",
          "322:         }.length == 2,",
          "323:         \"CTE repartition is reused.\")",
          "324:     }",
          "325:   }",
          "327:   test(\"CTE Predicate push-down and column pruning - combined predicate\") {",
          "328:     withView(\"t\") {",
          "329:       Seq((0, 1, 2), (1, 2, 3)).toDF(\"c1\", \"c2\", \"c3\").createOrReplaceTempView(\"t\")",
          "330:       val df = sql(",
          "331:         s\"\"\"with",
          "332:            |v as (",
          "333:            |  select c1, c2, c3, rand() c4 from t",
          "334:            |),",
          "335:            |vv as (",
          "336:            |  select v1.c1, v1.c2, rand() c5 from v v1, v v2",
          "337:            |  where v1.c1 > 0 and v2.c3 < 5 and v1.c2 = v2.c2",
          "338:            |)",
          "339:            |select vv1.c1, vv1.c2, vv2.c1, vv2.c2 from vv vv1, vv vv2",
          "340:            |where vv1.c2 > 0 and vv2.c2 > 0 and vv1.c1 = vv2.c1",
          "341:          \"\"\".stripMargin)",
          "342:       checkAnswer(df, Row(1, 2, 1, 2) :: Nil)",
          "343:       assert(",
          "344:         df.queryExecution.analyzed.collect {",
          "345:           case WithCTE(_, cteDefs) => cteDefs",
          "346:         }.head.length == 2,",
          "347:         \"With-CTE should contain 2 CTE defs after analysis.\")",
          "348:       val cteRepartitions = df.queryExecution.optimizedPlan.collect {",
          "349:         case r: RepartitionOperation => r",
          "350:       }",
          "351:       assert(cteRepartitions.length == 6,",
          "352:         \"CTE should not be inlined after optimization.\")",
          "353:       val distinctCteRepartitions = cteRepartitions.map(_.canonicalized).distinct",
          "355:       assert(distinctCteRepartitions.length == 2)",
          "356:       assert(distinctCteRepartitions(1).collectFirst {",
          "357:         case p: Project if p.projectList.length == 3 => p",
          "358:       }.isDefined, \"CTE columns should be pruned.\")",
          "359:       assert(",
          "360:         distinctCteRepartitions(1).collectFirst {",
          "361:           case f: Filter",
          "362:               if f.condition.semanticEquals(",
          "363:                 And(",
          "364:                   GreaterThan(f.output(1), Literal(0)),",
          "365:                   Or(",
          "366:                     GreaterThan(f.output(0), Literal(0)),",
          "367:                     LessThan(f.output(2), Literal(5))))) =>",
          "368:             f",
          "369:         }.isDefined,",
          "370:         \"Predicate 'c2 > 0 AND (c1 > 0 OR c3 < 5)' should be pushed down to the CTE def 'v'.\")",
          "372:       assert(",
          "373:         collectWithSubqueries(df.queryExecution.executedPlan) {",
          "374:           case r: ReusedExchangeExec => r",
          "375:         }.length == 2,",
          "376:         \"CTE repartition is reused.\")",
          "377:     }",
          "378:   }",
          "380:   test(\"Views with CTEs - 1 temp view\") {",
          "381:     withView(\"t\", \"t2\") {",
          "382:       Seq((0, 1), (1, 2)).toDF(\"c1\", \"c2\").createOrReplaceTempView(\"t\")",
          "383:       sql(",
          "384:         s\"\"\"with",
          "385:            |v as (",
          "386:            |  select c1 + c2 c3 from t",
          "387:            |)",
          "388:            |select sum(c3) s from v",
          "389:          \"\"\".stripMargin).createOrReplaceTempView(\"t2\")",
          "390:       val df = sql(",
          "391:         s\"\"\"with",
          "392:            |v as (",
          "393:            |  select c1 * c2 c3 from t",
          "394:            |)",
          "395:            |select sum(c3) from v except select s from t2",
          "396:          \"\"\".stripMargin)",
          "397:       checkAnswer(df, Row(2) :: Nil)",
          "398:     }",
          "399:   }",
          "401:   test(\"Views with CTEs - 2 temp views\") {",
          "402:     withView(\"t\", \"t2\", \"t3\") {",
          "403:       Seq((0, 1), (1, 2)).toDF(\"c1\", \"c2\").createOrReplaceTempView(\"t\")",
          "404:       sql(",
          "405:         s\"\"\"with",
          "406:            |v as (",
          "407:            |  select c1 + c2 c3 from t",
          "408:            |)",
          "409:            |select sum(c3) s from v",
          "410:          \"\"\".stripMargin).createOrReplaceTempView(\"t2\")",
          "411:       sql(",
          "412:         s\"\"\"with",
          "413:            |v as (",
          "414:            |  select c1 * c2 c3 from t",
          "415:            |)",
          "416:            |select sum(c3) s from v",
          "417:          \"\"\".stripMargin).createOrReplaceTempView(\"t3\")",
          "418:       val df = sql(\"select s from t3 except select s from t2\")",
          "419:       checkAnswer(df, Row(2) :: Nil)",
          "420:     }",
          "421:   }",
          "423:   test(\"Views with CTEs - temp view + sql view\") {",
          "424:     withTable(\"t\") {",
          "425:       withView (\"t2\", \"t3\") {",
          "426:         Seq((0, 1), (1, 2)).toDF(\"c1\", \"c2\").write.saveAsTable(\"t\")",
          "427:         sql(",
          "428:           s\"\"\"with",
          "429:              |v as (",
          "430:              |  select c1 + c2 c3 from t",
          "431:              |)",
          "432:              |select sum(c3) s from v",
          "433:            \"\"\".stripMargin).createOrReplaceTempView(\"t2\")",
          "434:         sql(",
          "435:           s\"\"\"create view t3 as",
          "436:              |with",
          "437:              |v as (",
          "438:              |  select c1 * c2 c3 from t",
          "439:              |)",
          "440:              |select sum(c3) s from v",
          "441:            \"\"\".stripMargin)",
          "442:         val df = sql(\"select s from t3 except select s from t2\")",
          "443:         checkAnswer(df, Row(2) :: Nil)",
          "444:       }",
          "445:     }",
          "446:   }",
          "448:   test(\"Union of Dataframes with CTEs\") {",
          "449:     val a = spark.sql(\"with t as (select 1 as n) select * from t \")",
          "450:     val b = spark.sql(\"with t as (select 2 as n) select * from t \")",
          "451:     val df = a.union(b)",
          "452:     checkAnswer(df, Row(1) :: Row(2) :: Nil)",
          "453:   }",
          "455:   test(\"CTE definitions out of original order when not inlined\") {",
          "456:     withView(\"t1\", \"t2\") {",
          "457:       Seq((1, 2, 10, 100), (2, 3, 20, 200)).toDF(\"workspace_id\", \"issue_id\", \"shard_id\", \"field_id\")",
          "458:         .createOrReplaceTempView(\"issue_current\")",
          "459:       withSQLConf(SQLConf.OPTIMIZER_EXCLUDED_RULES.key ->",
          "460:           \"org.apache.spark.sql.catalyst.optimizer.InlineCTE\") {",
          "461:         val df = sql(",
          "462:           \"\"\"",
          "463:             |WITH cte_0 AS (",
          "464:             |  SELECT workspace_id, issue_id, shard_id, field_id FROM issue_current",
          "465:             |),",
          "466:             |cte_1 AS (",
          "467:             |  WITH filtered_source_table AS (",
          "468:             |    SELECT * FROM cte_0 WHERE shard_id in ( 10 )",
          "469:             |  )",
          "470:             |  SELECT source_table.workspace_id, field_id FROM cte_0 source_table",
          "471:             |  INNER JOIN (",
          "472:             |    SELECT workspace_id, issue_id FROM filtered_source_table GROUP BY 1, 2",
          "473:             |  ) target_table",
          "474:             |  ON source_table.issue_id = target_table.issue_id",
          "475:             |  AND source_table.workspace_id = target_table.workspace_id",
          "476:             |  WHERE source_table.shard_id IN ( 10 )",
          "477:             |)",
          "478:             |SELECT * FROM cte_1",
          "479:         \"\"\".stripMargin)",
          "480:         checkAnswer(df, Row(1, 100) :: Nil)",
          "481:       }",
          "482:     }",
          "483:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "593:         |select * from q1 union all select * from q2\"\"\".stripMargin),",
          "594:       Row(5, \"5\") :: Row(4, \"4\") :: Nil)",
          "596:   }",
          "598:   test(\"Allow only a single WITH clause per query\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "597:     withSQLConf(SQLConf.LEGACY_CTE_PRECEDENCE_POLICY.key -> \"CORRECTED\") {",
          "598:       checkAnswer(",
          "599:         sql(",
          "600:           \"\"\"",
          "601:             |with temp1 as (select 1 col),",
          "602:             |temp2 as (",
          "603:             |  with temp1 as (select col + 1 AS col from temp1),",
          "604:             |  temp3 as (select col + 1 from temp1)",
          "605:             |  select * from temp3",
          "606:             |)",
          "607:             |select * from temp2",
          "608:             |\"\"\".stripMargin),",
          "609:         Row(3))",
          "610:       }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b18d582c7a07a43ce2d25708bb8116ffc98cf8b2",
      "candidate_info": {
        "commit_hash": "b18d582c7a07a43ce2d25708bb8116ffc98cf8b2",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/b18d582c7a07a43ce2d25708bb8116ffc98cf8b2",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala"
        ],
        "message": "[SPARK-40280][SQL][FOLLOWUP][3.3] Fix 'ParquetFilterSuite' issue\n\n### What changes were proposed in this pull request?\n\n### Why are the changes needed?\n\nFix 'ParquetFilterSuite' issue after merging #37747 :\nThe `org.apache.parquet.filter2.predicate.Operators.In` was added in the parquet 1.12.3, but spark branch-3.3 uses the parquet 1.12.2. Use `Operators.And` instead of `Operators.In`.\n\n### Does this PR introduce _any_ user-facing change?\n\n### How was this patch tested?\n\nCloses #37847 from zzcclp/SPARK-40280-hotfix-3.3.\n\nAuthored-by: Zhichao Zhang <zhangzc@apache.org>\nSigned-off-by: huaxingao <huaxin_gao@apple.com>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "400:       withSQLConf(SQLConf.PARQUET_FILTER_PUSHDOWN_INFILTERTHRESHOLD.key -> s\"$threshold\") {",
          "401:         checkFilterPredicate(",
          "402:           In(intAttr, Array(2, 3, 4, 5, 6, 7).map(Literal.apply)),",
          "404:           Seq(Row(2), Row(3), Row(4)))",
          "405:       }",
          "406:     }",
          "",
          "[Removed Lines]",
          "403:           if (threshold == 3) classOf[FilterIn[_]] else classOf[Operators.Or],",
          "",
          "[Added Lines]",
          "403:           if (threshold == 3) classOf[Operators.And] else classOf[Operators.Or],",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "485:       withSQLConf(SQLConf.PARQUET_FILTER_PUSHDOWN_INFILTERTHRESHOLD.key -> s\"$threshold\") {",
          "486:         checkFilterPredicate(",
          "487:           In(longAttr, Array(2L, 3L, 4L, 5L, 6L, 7L).map(Literal.apply)),",
          "489:           Seq(Row(2L), Row(3L), Row(4L)))",
          "490:       }",
          "491:     }",
          "",
          "[Removed Lines]",
          "488:           if (threshold == 3) classOf[FilterIn[_]] else classOf[Operators.Or],",
          "",
          "[Added Lines]",
          "488:           if (threshold == 3) classOf[Operators.And] else classOf[Operators.Or],",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "3e5407dbbfb8ea955e9c44df1893ad24a9449a28",
      "candidate_info": {
        "commit_hash": "3e5407dbbfb8ea955e9c44df1893ad24a9449a28",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/3e5407dbbfb8ea955e9c44df1893ad24a9449a28",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManagerSuite.scala"
        ],
        "message": "[SPARK-38809][SS] Implement option to skip null values in symmetric hash implementation of stream-stream joins\n\n### What changes were proposed in this pull request?\n\nIn the symmetric has join state manager, we can receive entries with null values for a key and that can cause the `removeByValue` and get iterators to fail and run into the NullPointerException. This is possible if the state recovered is written from an old spark version or its corrupted on disk or due to issues with the iterators. Since we don't have a utility to query this state, we would like to provide a conf option to skip nulls for the symmetric hash implementation in stream stream joins.\n\n### Why are the changes needed?\n\nWithout these changes, if we encounter null values for stream-stream joins, the executor task will repeatedly fail with NullPointerException and will terminate the stage and eventually the query as well. This change allows the user to set a config option to continue iterating by skipping null values for symmetric hash based implementation of stream-stream joins.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nAdded unit tests to test the new functionality by adding nulls in between and forcing the iteration/get calls with nulls in the mix and tested the behavior with the config disabled as well as enabled.\nSample output:\n```\n[info] SymmetricHashJoinStateManagerSuite:\n15:07:50.627 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[info] - StreamingJoinStateManager V1 - all operations (588 milliseconds)\n[info] - StreamingJoinStateManager V2 - all operations (251 milliseconds)\n15:07:52.669 WARN org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager: `keyWithIndexToValue` returns a null value for indices with range from startIndex=3 and endIndex=4.\n15:07:52.671 WARN org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager: `keyWithIndexToValue` returns a null value for indices with range from startIndex=3 and endIndex=3.\n15:07:52.672 WARN org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager: `keyWithIndexToValue` returns a null value for indices with range from startIndex=1 and endIndex=3.\n15:07:52.672 WARN org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager: `keyWithIndexToValue` returns a null value for indices with range from startIndex=1 and endIndex=1.\n[info] - StreamingJoinStateManager V1 - all operations with nulls (252 milliseconds)\n15:07:52.896 WARN org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager: `keyWithIndexToValue` returns a null value for indices with range from startIndex=3 and endIndex=4.\n15:07:52.897 WARN org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager: `keyWithIndexToValue` returns a null value for indices with range from startIndex=3 and endIndex=3.\n15:07:52.898 WARN org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager: `keyWithIndexToValue` returns a null value for indices with range from startIndex=1 and endIndex=3.\n15:07:52.898 WARN org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager: `keyWithIndexToValue` returns a null value for indices with range from startIndex=1 and endIndex=1.\n[info] - StreamingJoinStateManager V2 - all operations with nulls (221 milliseconds)\n15:07:53.114 WARN org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager: `keyWithIndexToValue` returns a null value for indices with range from startIndex=5 and endIndex=6.\n15:07:53.116 WARN org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager: `keyWithIndexToValue` returns a null value for indices with range from startIndex=3 and endIndex=6.\n15:07:53.331 WARN org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager: `keyWithIndexToValue` returns a null value for indices with range from startIndex=5 and endIndex=6.\n15:07:53.331 WARN org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager: `keyWithIndexToValue` returns a null value for indices with range from startIndex=3 and endIndex=3.\n[info] - StreamingJoinStateManager V1 - all operations with nulls in middle (435 milliseconds)\n15:07:53.549 WARN org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager: `keyWithIndexToValue` returns a null value for indices with range from startIndex=5 and endIndex=6.\n15:07:53.551 WARN org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager: `keyWithIndexToValue` returns a null value for indices with range from startIndex=3 and endIndex=6.\n15:07:53.785 WARN org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager: `keyWithIndexToValue` returns a null value for indices with range from startIndex=5 and endIndex=6.\n15:07:53.785 WARN org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManager: `keyWithIndexToValue` returns a null value for indices with range from startIndex=3 and endIndex=3.\n[info] - StreamingJoinStateManager V2 - all operations with nulls in middle (456 milliseconds)\n[info] - SPARK-35689: StreamingJoinStateManager V1 - printable key of keyWithIndexToValue (390 milliseconds)\n[info] - SPARK-35689: StreamingJoinStateManager V2 - printable key of keyWithIndexToValue (216 milliseconds)\n15:07:54.640 WARN org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite:\n\n===== POSSIBLE THREAD LEAK IN SUITE o.a.s.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite, threads: rpc-boss-3-1 (daemon=true), shuffle-boss-6-1 (daemon=true) =====\n[info] Run completed in 5 seconds, 714 milliseconds.\n[info] Total number of tests run: 8\n[info] Suites: completed 1, aborted 0\n[info] Tests: succeeded 8, failed 0, canceled 0, ignored 0, pending 0\n[info] All tests passed.\n```\n\nCloses #36090 from anishshri-db/bfix/SPARK-38809.\n\nAuthored-by: Anish Shrigondekar <anish.shrigondekar@databricks.com>\nSigned-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>\n(cherry picked from commit 61c489ea7ef51d7d0217f770ec358ed7a7b76b42)\nSigned-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManagerSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManagerSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1897:       .booleanConf",
          "1898:       .createWithDefault(false)",
          "1900:   val VARIABLE_SUBSTITUTE_ENABLED =",
          "1901:     buildConf(\"spark.sql.variable.substitute\")",
          "1902:       .doc(\"This enables substitution using syntax like `${var}`, `${system:var}`, \" +",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1905:   val STATE_STORE_SKIP_NULLS_FOR_STREAM_STREAM_JOINS =",
          "1906:   buildConf(\"spark.sql.streaming.stateStore.skipNullsForStreamStreamJoins.enabled\")",
          "1907:     .internal()",
          "1908:     .doc(\"When true, this config will skip null values in hash based stream-stream joins.\")",
          "1909:     .version(\"3.3.0\")",
          "1910:     .booleanConf",
          "1911:     .createWithDefault(false)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3867:   def stateStoreFormatValidationEnabled: Boolean = getConf(STATE_STORE_FORMAT_VALIDATION_ENABLED)",
          "3869:   def checkpointLocation: Option[String] = getConf(CHECKPOINT_LOCATION)",
          "3871:   def isUnsupportedOperationCheckEnabled: Boolean = getConf(UNSUPPORTED_OPERATION_CHECK_ENABLED)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3882:   def stateStoreSkipNullsForStreamStreamJoins: Boolean =",
          "3883:     getConf(STATE_STORE_SKIP_NULLS_FOR_STREAM_STREAM_JOINS)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStoreConf.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "52:   val formatValidationCheckValue: Boolean =",
          "53:     extraOptions.getOrElse(StateStoreConf.FORMAT_VALIDATION_CHECK_VALUE_CONFIG, \"true\") == \"true\"",
          "56:   val compressionCodec: String = sqlConf.stateStoreCompressionCodec",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "56:   val skipNullsForStreamStreamJoins: Boolean = sqlConf.stateStoreSkipNullsForStreamStreamJoins",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "233:           if (hasMoreValuesForCurrentKey) {",
          "235:             val valuePair = keyWithIndexToValue.get(currentKey, index)",
          "237:               return valuePair",
          "238:             } else {",
          "239:               index += 1",
          "",
          "[Removed Lines]",
          "236:             if (removalCondition(valuePair.value)) {",
          "",
          "[Added Lines]",
          "240:             if (valuePair == null && storeConf.skipNullsForStreamStreamJoins) {",
          "241:               index += 1",
          "242:             } else if (removalCondition(valuePair.value)) {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "601:     def getAll(key: UnsafeRow, numValues: Long): Iterator[KeyWithIndexAndValue] = {",
          "604:       new NextIterator[KeyWithIndexAndValue] {",
          "605:         override protected def getNext(): KeyWithIndexAndValue = {",
          "610:             val keyWithIndex = keyWithIndexRow(key, index)",
          "611:             val valuePair = valueRowConverter.convertValue(stateStore.get(keyWithIndex))",
          "615:           }",
          "616:         }",
          "618:         override protected def close(): Unit = {}",
          "",
          "[Removed Lines]",
          "602:       val keyWithIndexAndValue = new KeyWithIndexAndValue()",
          "603:       var index = 0",
          "606:           if (index >= numValues) {",
          "607:             finished = true",
          "608:             null",
          "609:           } else {",
          "612:             keyWithIndexAndValue.withNew(key, index, valuePair)",
          "613:             index += 1",
          "614:             keyWithIndexAndValue",
          "",
          "[Added Lines]",
          "611:         private val keyWithIndexAndValue = new KeyWithIndexAndValue()",
          "612:         private var index: Long = 0L",
          "614:         private def hasMoreValues = index < numValues",
          "616:           while (hasMoreValues) {",
          "619:             if (valuePair == null && storeConf.skipNullsForStreamStreamJoins) {",
          "620:               index += 1",
          "621:             } else {",
          "622:               keyWithIndexAndValue.withNew(key, index, valuePair)",
          "623:               index += 1",
          "624:               return keyWithIndexAndValue",
          "625:             }",
          "628:           finished = true",
          "629:           return null",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManagerSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManagerSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManagerSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManagerSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: import org.apache.spark.sql.catalyst.plans.logical.EventTimeWatermark",
          "30: import org.apache.spark.sql.execution.streaming.StatefulOperatorStateInfo",
          "31: import org.apache.spark.sql.execution.streaming.StreamingSymmetricHashJoinHelper.LeftSide",
          "32: import org.apache.spark.sql.streaming.StreamTest",
          "33: import org.apache.spark.sql.types._",
          "34: import org.apache.spark.unsafe.types.UTF8String",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32: import org.apache.spark.sql.internal.SQLConf",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "52:     }",
          "53:   }",
          "55:   SymmetricHashJoinStateManager.supportedVersions.foreach { version =>",
          "56:     test(s\"SPARK-35689: StreamingJoinStateManager V${version} - \" +",
          "57:         \"printable key of keyWithIndexToValue\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "56:   SymmetricHashJoinStateManager.supportedVersions.foreach { version =>",
          "57:     test(s\"StreamingJoinStateManager V${version} - all operations with nulls in middle\") {",
          "58:       testAllOperationsWithNullsInMiddle(version)",
          "59:     }",
          "60:   }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "167:     }",
          "168:   }",
          "170:   val watermarkMetadata = new MetadataBuilder().putLong(EventTimeWatermark.delayKey, 10).build()",
          "171:   val inputValueSchema = new StructType()",
          "172:     .add(StructField(\"time\", IntegerType, metadata = watermarkMetadata))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "178:   private def testAllOperationsWithNullsInMiddle(stateFormatVersion: Int): Unit = {",
          "181:     withJoinStateManager(inputValueAttribs, joinKeyExprs, stateFormatVersion) { manager =>",
          "182:       implicit val mgr = manager",
          "184:       val ex = intercept[Exception] {",
          "185:         appendAndTest(40, 50, 200, 300)",
          "186:         assert(numRows === 3)",
          "187:         updateNumValues(40, 4) // create a null at the end",
          "188:         append(40, 400)",
          "189:         updateNumValues(40, 7) // create nulls in between and end",
          "190:         removeByValue(50)",
          "191:       }",
          "192:       assert(ex.isInstanceOf[NullPointerException])",
          "193:       assert(getNumValues(40) === 7)        // we should get 7 with no nulls skipped",
          "195:       removeByValue(300)",
          "196:       assert(getNumValues(40) === 1)         // only 400 should remain",
          "197:       assert(get(40) === Seq(400))",
          "198:       removeByValue(400)",
          "199:       assert(get(40) === Seq.empty)",
          "200:       assert(numRows === 0)                        // ensure all elements removed",
          "201:     }",
          "205:     withJoinStateManager(inputValueAttribs, joinKeyExprs, stateFormatVersion, true) { manager =>",
          "206:       implicit val mgr = manager",
          "208:       appendAndTest(40, 50, 200, 300)",
          "209:       assert(numRows === 3)",
          "210:       updateNumValues(40, 4) // create a null at the end",
          "211:       append(40, 400)",
          "212:       updateNumValues(40, 7) // create nulls in between and end",
          "214:       removeByValue(50)",
          "215:       assert(getNumValues(40) === 3)       // we should now get (400, 200, 300) with nulls skipped",
          "217:       removeByValue(300)",
          "218:       assert(getNumValues(40) === 1)         // only 400 should remain",
          "219:       assert(get(40) === Seq(400))",
          "220:       removeByValue(400)",
          "221:       assert(get(40) === Seq.empty)",
          "222:       assert(numRows === 0)                        // ensure all elements removed",
          "223:     }",
          "224:   }",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "205:     manager.updateNumValuesTestOnly(toJoinKeyRow(key), numValues)",
          "206:   }",
          "208:   def get(key: Int)(implicit manager: SymmetricHashJoinStateManager): Seq[Int] = {",
          "209:     manager.get(toJoinKeyRow(key)).map(toValueInt).toSeq.sorted",
          "210:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "264:   def getNumValues(key: Int)",
          "265:                   (implicit manager: SymmetricHashJoinStateManager): Int = {",
          "266:     manager.get(toJoinKeyRow(key)).size",
          "267:   }",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "232:     manager.metrics.numKeys",
          "233:   }",
          "236:   def withJoinStateManager(",
          "241:     withTempDir { file =>",
          "251:       }",
          "252:     }",
          "253:     StateStore.stop()",
          "",
          "[Removed Lines]",
          "237:     inputValueAttribs: Seq[Attribute],",
          "238:     joinKeyExprs: Seq[Expression],",
          "239:     stateFormatVersion: Int)(f: SymmetricHashJoinStateManager => Unit): Unit = {",
          "242:       val storeConf = new StateStoreConf()",
          "243:       val stateInfo = StatefulOperatorStateInfo(file.getAbsolutePath, UUID.randomUUID, 0, 0, 5)",
          "244:       val manager = new SymmetricHashJoinStateManager(",
          "245:         LeftSide, inputValueAttribs, joinKeyExprs, Some(stateInfo), storeConf, new Configuration,",
          "246:         partitionId = 0, stateFormatVersion)",
          "247:       try {",
          "248:         f(manager)",
          "249:       } finally {",
          "250:         manager.abortIfNeeded()",
          "",
          "[Added Lines]",
          "297:       inputValueAttribs: Seq[Attribute],",
          "298:       joinKeyExprs: Seq[Expression],",
          "299:       stateFormatVersion: Int,",
          "300:       skipNullsForStreamStreamJoins: Boolean = false)",
          "301:       (f: SymmetricHashJoinStateManager => Unit): Unit = {",
          "304:       withSQLConf(SQLConf.STATE_STORE_SKIP_NULLS_FOR_STREAM_STREAM_JOINS.key ->",
          "305:         skipNullsForStreamStreamJoins.toString) {",
          "306:         val storeConf = new StateStoreConf(spark.sqlContext.conf)",
          "307:         val stateInfo = StatefulOperatorStateInfo(file.getAbsolutePath, UUID.randomUUID, 0, 0, 5)",
          "308:         val manager = new SymmetricHashJoinStateManager(",
          "309:           LeftSide, inputValueAttribs, joinKeyExprs, Some(stateInfo), storeConf, new Configuration,",
          "310:           partitionId = 0, stateFormatVersion)",
          "311:         try {",
          "312:           f(manager)",
          "313:         } finally {",
          "314:           manager.abortIfNeeded()",
          "315:         }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c9d56758a8c28a44161f63eb5c8763ab92616a56",
      "candidate_info": {
        "commit_hash": "c9d56758a8c28a44161f63eb5c8763ab92616a56",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/c9d56758a8c28a44161f63eb5c8763ab92616a56",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQuerySuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala"
        ],
        "message": "[SPARK-39856][TESTS][INFRA] Skip q72 at TPC-DS build at GitHub Actions\n\n### What changes were proposed in this pull request?\n\nThis PR reverts https://github.com/apache/spark/commit/7358253755762f9bfe6cedc1a50ec14616cfeace, https://github.com/apache/spark/commit/ae1f6a26ed39b297ace8d6c9420b72a3c01a3291 and https://github.com/apache/spark/commit/72b55ccf8327c00e173ab6130fdb428ad0d5aacc because they do not help fixing the TPC-DS build.\n\nIn addition, this PR skips the problematic query in GitHub Actions to avoid OOM.\n\n### Why are the changes needed?\n\nTo make the build pass.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, dev and test-only.\n\n### How was this patch tested?\n\nCI in this PR should test it out.\n\nCloses #37289 from HyukjinKwon/SPARK-39856-followup.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit de9a4b0747a4127e320f80f5e1bf431429da70a9)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/TPCDSQuerySuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/TPCDSQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/TPCDSQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/TPCDSQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: @ExtendedSQLTest",
          "30: class TPCDSQuerySuite extends BenchmarkQueryTest with TPCDSBase {",
          "33:     val queryString = resourceToString(s\"tpcds/$name.sql\",",
          "34:       classLoader = Thread.currentThread().getContextClassLoader)",
          "35:     test(name) {",
          "",
          "[Removed Lines]",
          "32:   tpcdsQueries.foreach { name =>",
          "",
          "[Added Lines]",
          "33:   tpcdsQueries.filterNot(sys.env.contains(\"GITHUB_ACTIONS\") && _ == \"q72\").foreach { name =>",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39:     }",
          "40:   }",
          "43:     val queryString = resourceToString(s\"tpcds-v2.7.0/$name.sql\",",
          "44:       classLoader = Thread.currentThread().getContextClassLoader)",
          "45:     test(s\"$name-v2.7\") {",
          "",
          "[Removed Lines]",
          "42:   tpcdsQueriesV2_7_0.foreach { name =>",
          "",
          "[Added Lines]",
          "44:   tpcdsQueriesV2_7_0.filterNot(sys.env.contains(\"GITHUB_ACTIONS\") && _ == \"q72\").foreach { name =>",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/TPCDSQueryTestSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "64:   override protected def sparkConf: SparkConf = super.sparkConf",
          "67:   protected override def createSparkSession: TestSparkSession = {",
          "68:     new TestSparkSession(new SparkContext(\"local[1]\", this.getClass.getSimpleName, sparkConf))",
          "",
          "[Removed Lines]",
          "65:     .set(SQLConf.SHUFFLE_PARTITIONS.key, 32.toString)",
          "",
          "[Added Lines]",
          "65:     .set(SQLConf.SHUFFLE_PARTITIONS.key, \"1\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "105:       query: String,",
          "106:       goldenFile: File,",
          "107:       conf: Map[String, String]): Unit = {",
          "108:     withSQLConf(conf.toSeq: _*) {",
          "109:       try {",
          "110:         val (schema, output) = handleExceptions(getNormalizedResult(spark, query))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "108:     val shouldSortResults = sortMergeJoinConf != conf  // Sort for other joins",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "142:         assertResult(expectedSchema, s\"Schema did not match\\n$queryString\") {",
          "143:           schema",
          "144:         }",
          "154:         }",
          "155:       } catch {",
          "156:         case e: Throwable =>",
          "",
          "[Removed Lines]",
          "146:         val expectSorted = expectedOutput.split(\"\\n\").sorted.map(_.trim)",
          "147:           .mkString(\"\\n\").replaceAll(\"\\\\s+$\", \"\")",
          "148:           .replaceAll(\"\"\"([0-9]+.[0-9]{10})([0-9]*)\"\"\", \"$1\")",
          "149:         val outputSorted = output.sorted.map(_.trim).mkString(\"\\n\")",
          "150:           .replaceAll(\"\\\\s+$\", \"\")",
          "151:           .replaceAll(\"\"\"([0-9]+.[0-9]{10})([0-9]*)\"\"\", \"$1\")",
          "152:         assertResult(expectSorted, s\"Result did not match\\n$queryString\") {",
          "153:           outputSorted",
          "",
          "[Added Lines]",
          "146:         if (shouldSortResults) {",
          "147:           val expectSorted = expectedOutput.split(\"\\n\").sorted.map(_.trim)",
          "148:             .mkString(\"\\n\").replaceAll(\"\\\\s+$\", \"\")",
          "149:           val outputSorted = output.sorted.map(_.trim).mkString(\"\\n\").replaceAll(\"\\\\s+$\", \"\")",
          "150:           assertResult(expectSorted, s\"Result did not match\\n$queryString\") {",
          "151:             outputSorted",
          "152:           }",
          "153:         } else {",
          "154:           assertResult(expectedOutput, s\"Result did not match\\n$queryString\") {",
          "155:             outputString",
          "156:           }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "76f40eef8b97e23f4a16e471366ae410a3e6cc20",
      "candidate_info": {
        "commit_hash": "76f40eef8b97e23f4a16e471366ae410a3e6cc20",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/76f40eef8b97e23f4a16e471366ae410a3e6cc20",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala"
        ],
        "message": "[SPARK-38829][SQL][3.3] Remove TimestampNTZ type support in Parquet for Spark 3.3\n\n### What changes were proposed in this pull request?\n\nThis is a follow-up for https://github.com/apache/spark/pull/36094.\nI added `Utils.isTesting` whenever we perform schema conversion or row conversion for TimestampNTZType.\n\nI verified that the tests, e.g. ParquetIOSuite, fail with unsupported data type when running in non-testing mode:\n```\n[info]   Cause: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 40.0 failed 1 times, most recent failure: Lost task 1.0 in stage 40.0 (TID 66) (ip-10-110-16-208.us-west-2.compute.internal executor driver): org.apache.spark.sql.AnalysisException: Unsupported data type timestamp_ntz\n[info] \tat org.apache.spark.sql.errors.QueryCompilationErrors$.cannotConvertDataTypeToParquetTypeError(QueryCompilationErrors.scala:1304)\n[info] \tat org.apache.spark.sql.execution.datasources.parquet.SparkToParquetSchemaConverter.convertField(ParquetSchemaConverter.scala:707)\n[info] \tat org.apache.spark.sql.execution.datasources.parquet.SparkToParquetSchemaConverter.convertField(ParquetSchemaConverter.scala:479)\n[info] \tat org.apache.spark.sql.execution.datasources.parquet.SparkToParquetSchemaConverter.$anonfun$convert$1(ParquetSchemaConverter.scala:471)\n```\n\n### Why are the changes needed?\nWe have to disable TimestampNTZType as other parts of the codebase do not yet support this type.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, the TimestampNTZ type is not released yet.\n\n### How was this patch tested?\n\nI tested the changes manually by rerunning the test suites that verify TimestampNTZType in the non-testing mode.\n\nCloses #36137 from sadikovi/SPARK-38829-parquet-ntz-off.\n\nAuthored-by: Ivan Sadikov <ivan.sadikov@databricks.com>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "41: import org.apache.spark.sql.internal.SQLConf",
          "42: import org.apache.spark.sql.types._",
          "43: import org.apache.spark.unsafe.types.UTF8String",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "44: import org.apache.spark.util.Utils",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "487:     parquetType.asPrimitiveType().getPrimitiveTypeName == INT64 &&",
          "488:       parquetType.getLogicalTypeAnnotation.isInstanceOf[TimestampLogicalTypeAnnotation] &&",
          "489:       !parquetType.getLogicalTypeAnnotation",
          "",
          "[Removed Lines]",
          "490:         .asInstanceOf[TimestampLogicalTypeAnnotation].isAdjustedToUTC",
          "",
          "[Added Lines]",
          "491:         .asInstanceOf[TimestampLogicalTypeAnnotation].isAdjustedToUTC &&",
          "493:       Utils.isTesting",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: import org.apache.spark.sql.errors.QueryCompilationErrors",
          "31: import org.apache.spark.sql.internal.SQLConf",
          "32: import org.apache.spark.sql.types._",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33: import org.apache.spark.util.Utils",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "253:             if (timestamp.isAdjustedToUTC) {",
          "254:               TimestampType",
          "255:             } else {",
          "257:             }",
          "258:           case _ => illegalType()",
          "259:         }",
          "",
          "[Removed Lines]",
          "256:               TimestampNTZType",
          "",
          "[Added Lines]",
          "258:               if (Utils.isTesting) TimestampNTZType else TimestampType",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "547:               .as(LogicalTypeAnnotation.timestampType(true, TimeUnit.MILLIS)).named(field.name)",
          "548:         }",
          "551:         Types.primitive(INT64, repetition)",
          "552:           .as(LogicalTypeAnnotation.timestampType(false, TimeUnit.MICROS)).named(field.name)",
          "553:       case BinaryType =>",
          "",
          "[Removed Lines]",
          "550:       case TimestampNTZType =>",
          "",
          "[Added Lines]",
          "553:       case TimestampNTZType if Utils.isTesting =>",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "39: import org.apache.spark.sql.internal.SQLConf",
          "40: import org.apache.spark.sql.internal.SQLConf.LegacyBehaviorPolicy",
          "41: import org.apache.spark.sql.types._",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42: import org.apache.spark.util.Utils",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "227:               recordConsumer.addLong(millis)",
          "228:         }",
          "233:         (row: SpecializedGetters, ordinal: Int) => recordConsumer.addLong(row.getLong(ordinal))",
          "",
          "[Removed Lines]",
          "230:       case TimestampNTZType =>",
          "",
          "[Added Lines]",
          "232:       case TimestampNTZType if Utils.isTesting =>",
          "",
          "---------------"
        ]
      }
    }
  ]
}