{
  "cve_id": "CVE-2022-40754",
  "cve_desc": "In Apache Airflow 2.3.0 through 2.3.4, there was an open redirect in the webserver's `/confirm` endpoint.",
  "repo": "apache/airflow",
  "patch_hash": "3871f00230b7064e6234aa65b8a00ee2b6d33e4c",
  "patch_info": {
    "commit_hash": "3871f00230b7064e6234aa65b8a00ee2b6d33e4c",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/3871f00230b7064e6234aa65b8a00ee2b6d33e4c",
    "files": [
      "airflow/www/views.py"
    ],
    "message": "Fix UI redirect (#26409)\n\nCo-authored-by: Konstantin Weddige <konstantin.weddige@lutrasecurity.com>\n(cherry picked from commit 56e7555c42f013f789a4b718676ff09b4a9d5135)",
    "before_after_code_files": [
      "airflow/www/views.py||airflow/www/views.py"
    ]
  },
  "patch_diff": {
    "airflow/www/views.py||airflow/www/views.py": [
      "File: airflow/www/views.py -> airflow/www/views.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "2329:         task_id = args.get('task_id')",
      "2330:         dag_run_id = args.get('dag_run_id')",
      "2331:         state = args.get('state')",
      "2334:         if 'map_index' not in args:",
      "2335:             map_indexes: list[int] | None = None",
      "",
      "[Removed Lines]",
      "2332:         origin = args.get('origin')",
      "",
      "[Added Lines]",
      "2332:         origin = get_safe_url(args.get('origin'))",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "38d3d4f0f11a88260e4a849a8bed5755b5490878",
      "candidate_info": {
        "commit_hash": "38d3d4f0f11a88260e4a849a8bed5755b5490878",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/38d3d4f0f11a88260e4a849a8bed5755b5490878",
        "files": [
          "airflow/cli/cli_parser.py",
          "tests/cli/commands/test_dag_command.py"
        ],
        "message": "Require dag_id arg for dags list-runs (#26357)\n\nWhile it would ideal to transition to a positional arg like was\nattempted in #25978, this unfortunately does result in a breaking change\nso we cannot do it now.\n\nBy instead marking the existing arg as required, we maintain backcompat\nwhile also providing a helpful error message to the user if they forget\nit.\n\nThis reverts commit ed6ea72f181a1d381cc1ff6c801f10cc0bc0d830.\n\n(cherry picked from commit f6c579c1c0efb8cdd2eaf905909cda7bc7314f88)",
        "before_after_code_files": [
          "airflow/cli/cli_parser.py||airflow/cli/cli_parser.py",
          "tests/cli/commands/test_dag_command.py||tests/cli/commands/test_dag_command.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/cli_parser.py||airflow/cli/cli_parser.py": [
          "File: airflow/cli/cli_parser.py -> airflow/cli/cli_parser.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "263: )",
          "265: # list_dag_runs",
          "266: ARG_NO_BACKFILL = Arg(",
          "267:     (\"--no-backfill\",), help=\"filter all the backfill dagruns given the dag id\", action=\"store_true\"",
          "268: )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "266: ARG_DAG_ID_REQ_FLAG = Arg(",
          "267:     (\"-d\", \"--dag-id\"), required=True, help=\"The id of the dag\"",
          "268: )  # TODO: convert this to a positional arg in Airflow 3",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1008:         ),",
          "1009:         func=lazy_load_command('airflow.cli.commands.dag_command.dag_list_dag_runs'),",
          "1010:         args=(",
          "1012:             ARG_NO_BACKFILL,",
          "1013:             ARG_STATE,",
          "1014:             ARG_OUTPUT,",
          "",
          "[Removed Lines]",
          "1011:             ARG_DAG_ID,",
          "",
          "[Added Lines]",
          "1014:             ARG_DAG_ID_REQ_FLAG,",
          "",
          "---------------"
        ],
        "tests/cli/commands/test_dag_command.py||tests/cli/commands/test_dag_command.py": [
          "File: tests/cli/commands/test_dag_command.py -> tests/cli/commands/test_dag_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "499:             [",
          "500:                 'dags',",
          "501:                 'list-runs',",
          "502:                 '--no-backfill',",
          "503:                 '--start-date',",
          "504:                 DEFAULT_DATE.isoformat(),",
          "505:                 '--end-date',",
          "506:                 timezone.make_aware(datetime.max).isoformat(),",
          "508:             ]",
          "509:         )",
          "510:         dag_command.dag_list_dag_runs(args)",
          "",
          "[Removed Lines]",
          "507:                 'example_bash_operator',",
          "",
          "[Added Lines]",
          "502:                 '--dag-id',",
          "503:                 'example_bash_operator',",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "95eda4bc0ef6a4e003f1b5a684d090a4b4255738",
      "candidate_info": {
        "commit_hash": "95eda4bc0ef6a4e003f1b5a684d090a4b4255738",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/95eda4bc0ef6a4e003f1b5a684d090a4b4255738",
        "files": [
          "README.md",
          "docs/apache-airflow/installation/supported-versions.rst",
          "docs/docker-stack/README.md",
          "docs/docker-stack/docker-examples/extending/add-apt-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-build-essential-extend/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-providers/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-pypi-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/custom-providers/Dockerfile",
          "docs/docker-stack/docker-examples/extending/embedding-dags/Dockerfile",
          "docs/docker-stack/docker-examples/extending/writable-directory/Dockerfile",
          "docs/docker-stack/entrypoint.rst",
          "scripts/ci/pre_commit/pre_commit_supported_versions.py",
          "setup.py"
        ],
        "message": "Update Airflow version",
        "before_after_code_files": [
          "scripts/ci/pre_commit/pre_commit_supported_versions.py||scripts/ci/pre_commit/pre_commit_supported_versions.py",
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/pre_commit/pre_commit_supported_versions.py||scripts/ci/pre_commit/pre_commit_supported_versions.py": [
          "File: scripts/ci/pre_commit/pre_commit_supported_versions.py -> scripts/ci/pre_commit/pre_commit_supported_versions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: HEADERS = (\"Version\", \"Current Patch/Minor\", \"State\", \"First Release\", \"Limited Support\", \"EOL/Terminated\")",
          "27: SUPPORTED_VERSIONS = (",
          "29:     (\"1.10\", \"1.10.15\", \"EOL\", \"Aug 27, 2018\", \"Dec 17, 2020\", \"June 17, 2021\"),",
          "30:     (\"1.9\", \"1.9.0\", \"EOL\", \"Jan 03, 2018\", \"Aug 27, 2018\", \"Aug 27, 2018\"),",
          "31:     (\"1.8\", \"1.8.2\", \"EOL\", \"Mar 19, 2017\", \"Jan 03, 2018\", \"Jan 03, 2018\"),",
          "",
          "[Removed Lines]",
          "28:     (\"2\", \"2.3.4\", \"Supported\", \"Dec 17, 2020\", \"TBD\", \"TBD\"),",
          "",
          "[Added Lines]",
          "28:     (\"2\", \"2.4.0\", \"Supported\", \"Dec 17, 2020\", \"TBD\", \"TBD\"),",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "48: logger = logging.getLogger(__name__)",
          "52: AIRFLOW_SOURCES_ROOT = Path(__file__).parent.resolve()",
          "53: PROVIDERS_ROOT = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"providers\"",
          "",
          "[Removed Lines]",
          "50: version = '2.4.0b1'",
          "",
          "[Added Lines]",
          "50: version = '2.4.0'",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
      "candidate_info": {
        "commit_hash": "6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
        "files": [
          "airflow/utils/log/file_task_handler.py"
        ],
        "message": "Properly build URL to retrieve logs independently from system (#26337)\n\nThe previous way of building the path depended on the OS path\nbut it was really used to build the URL so we should use\nurllib instead of os.path.join.\n\n(cherry picked from commit 18386026c28939fa6d91d198c5489c295a05dcd2)",
        "before_after_code_files": [
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import warnings",
          "22: from pathlib import Path",
          "23: from typing import TYPE_CHECKING, Optional",
          "25: from airflow.configuration import AirflowConfigException, conf",
          "26: from airflow.exceptions import RemovedInAirflow3Warning",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24: from urllib.parse import urljoin",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "194:         else:",
          "195:             import httpx",
          "199:             )",
          "200:             log += f\"*** Log file does not exist: {location}\\n\"",
          "201:             log += f\"*** Fetching from: {url}\\n\"",
          "",
          "[Removed Lines]",
          "197:             url = os.path.join(\"http://{ti.hostname}:{worker_log_server_port}/log\", log_relative_path).format(",
          "198:                 ti=ti, worker_log_server_port=conf.get('logging', 'WORKER_LOG_SERVER_PORT')",
          "",
          "[Added Lines]",
          "198:             url = urljoin(",
          "199:                 f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log\", log_relative_path",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c06d8dbe9dddfb924041747cc84ae2d12799e0da",
      "candidate_info": {
        "commit_hash": "c06d8dbe9dddfb924041747cc84ae2d12799e0da",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c06d8dbe9dddfb924041747cc84ae2d12799e0da",
        "files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py"
        ],
        "message": "Add better progress in CI for constraints generation. (#26253)\n\nCurrently constraints generation is not really showing good progress\nwhile the packages are being removed/installed. This adds progress\nthat shows that something happens.\n\n(cherry picked from commit d6f473b31d9902290dc3a204657b4a7ed8d7843b)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "65:     get_extra_docker_flags,",
          "66:     perform_environment_checks,",
          "67: )",
          "69: from airflow_breeze.utils.python_versions import get_python_version_list",
          "70: from airflow_breeze.utils.run_utils import (",
          "71:     RunCommandResult,",
          "",
          "[Removed Lines]",
          "68: from airflow_breeze.utils.parallel import check_async_run_results, run_with_pool",
          "",
          "[Added Lines]",
          "68: from airflow_breeze.utils.parallel import GenericRegexpProgressMatcher, check_async_run_results, run_with_pool",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "305:     )",
          "308: def run_generate_constraints_in_parallel(",
          "309:     shell_params_list: List[ShellParams],",
          "310:     python_version_list: List[str],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "308: CONSTRAINT_PROGRESS_MATCHER = (",
          "309:     r'Found|Uninstalling|uninstalled|Collecting|Downloading|eta|Running|Installing|built|Attempting'",
          "310: )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "320:             f\"Constraints {shell_params.airflow_constraints_mode}:{shell_params.python}\"",
          "321:             for shell_params in shell_params_list",
          "322:         ]",
          "324:             results = [",
          "325:                 pool.apply_async(",
          "326:                     run_generate_constraints,",
          "",
          "[Removed Lines]",
          "323:         with run_with_pool(parallelism=parallelism, all_params=all_params) as (pool, outputs):",
          "",
          "[Added Lines]",
          "328:         with run_with_pool(",
          "329:             parallelism=parallelism,",
          "330:             all_params=all_params,",
          "331:             progress_matcher=GenericRegexpProgressMatcher(",
          "332:                 regexp=CONSTRAINT_PROGRESS_MATCHER, lines_to_search=6",
          "333:             ),",
          "334:         ) as (pool, outputs):",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "63a5276e925361a4ab33a5b172da87cc69ba86ee",
      "candidate_info": {
        "commit_hash": "63a5276e925361a4ab33a5b172da87cc69ba86ee",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/63a5276e925361a4ab33a5b172da87cc69ba86ee",
        "files": [
          "airflow/models/dag.py",
          "airflow/models/dagrun.py",
          "airflow/www/views.py",
          "tests/models/test_dag.py"
        ],
        "message": "Respect max_active_runs for dataset-triggered dags (#26348)\n\nCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>\nCo-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>\n(cherry picked from commit b99d1cd5d32aea5721c512d6052b6b7b3e0dfefb)",
        "before_after_code_files": [
          "airflow/models/dag.py||airflow/models/dag.py",
          "airflow/models/dagrun.py||airflow/models/dagrun.py",
          "airflow/www/views.py||airflow/www/views.py",
          "tests/models/test_dag.py||tests/models/test_dag.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [
            "airflow/www/views.py||airflow/www/views.py"
          ],
          "candidate": [
            "airflow/www/views.py||airflow/www/views.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "54: from dateutil.relativedelta import relativedelta",
          "55: from pendulum.tz.timezone import Timezone",
          "56: from sqlalchemy import Boolean, Column, ForeignKey, Index, Integer, String, Text, and_, case, func, not_, or_",
          "57: from sqlalchemy.orm import backref, joinedload, relationship",
          "58: from sqlalchemy.orm.query import Query",
          "59: from sqlalchemy.orm.session import Session",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "57: from sqlalchemy.ext.associationproxy import association_proxy",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3066:         \"DagScheduleDatasetReference\",",
          "3067:         cascade='all, delete, delete-orphan',",
          "3068:     )",
          "3069:     task_outlet_dataset_references = relationship(",
          "3070:         \"TaskOutletDatasetReference\",",
          "3071:         cascade='all, delete, delete-orphan',",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3070:     schedule_datasets = association_proxy('schedule_dataset_references', 'dataset')",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "3235:         transaction is committed it will be unlocked.",
          "3236:         \"\"\"",
          "3237:         # these dag ids are triggered by datasets, and they are ready to go.",
          "3239:             x.dag_id: (x.first_queued_time, x.last_queued_time)",
          "3240:             for x in session.query(",
          "3241:                 DagScheduleDatasetReference.dag_id,",
          "",
          "[Removed Lines]",
          "3238:         dataset_triggered_dag_info_list = {",
          "",
          "[Added Lines]",
          "3240:         dataset_triggered_dag_info = {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "3247:             .having(func.count() == func.sum(case((DDRQ.target_dag_id.is_not(None), 1), else_=0)))",
          "3248:             .all()",
          "3249:         }",
          "3256:         query = (",
          "3257:             session.query(cls)",
          "3258:             .filter(",
          "",
          "[Removed Lines]",
          "3250:         dataset_triggered_dag_ids = list(dataset_triggered_dag_info_list.keys())",
          "3252:         # TODO[HA]: Bake this query, it is run _A lot_",
          "3253:         # We limit so that _one_ scheduler doesn't try to do all the creation",
          "3254:         # of dag runs",
          "",
          "[Added Lines]",
          "3252:         dataset_triggered_dag_ids = set(dataset_triggered_dag_info.keys())",
          "3253:         if dataset_triggered_dag_ids:",
          "3254:             exclusion_list = {",
          "3255:                 x.dag_id",
          "3256:                 for x in (",
          "3257:                     session.query(DagModel.dag_id)",
          "3258:                     .join(DagRun.dag_model)",
          "3259:                     .filter(DagRun.state.in_((DagRunState.QUEUED, DagRunState.RUNNING)))",
          "3260:                     .filter(DagModel.dag_id.in_(dataset_triggered_dag_ids))",
          "3261:                     .group_by(DagModel.dag_id)",
          "3262:                     .having(func.count() >= func.max(DagModel.max_active_runs))",
          "3263:                     .all()",
          "3264:                 )",
          "3265:             }",
          "3266:             if exclusion_list:",
          "3267:                 dataset_triggered_dag_ids -= exclusion_list",
          "3268:                 dataset_triggered_dag_info = {",
          "3269:                     k: v for k, v in dataset_triggered_dag_info.items() if k not in exclusion_list",
          "3270:                 }",
          "3272:         # We limit so that _one_ scheduler doesn't try to do all the creation of dag runs",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "3271:         return (",
          "3272:             with_row_locks(query, of=cls, session=session, **skip_locked(session=session)),",
          "3274:         )",
          "3276:     def calculate_dagrun_date_fields(",
          "",
          "[Removed Lines]",
          "3273:             dataset_triggered_dag_info_list,",
          "",
          "[Added Lines]",
          "3290:             dataset_triggered_dag_info,",
          "",
          "---------------"
        ],
        "airflow/models/dagrun.py||airflow/models/dagrun.py": [
          "File: airflow/models/dagrun.py -> airflow/models/dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "163:     task_instances = relationship(",
          "164:         TI, back_populates=\"dag_run\", cascade='save-update, merge, delete, delete-orphan'",
          "165:     )",
          "167:     DEFAULT_DAGRUNS_TO_EXAMINE = airflow_conf.getint(",
          "168:         'scheduler',",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "166:     dag_model = relationship(",
          "167:         \"DagModel\",",
          "168:         primaryjoin=\"foreign(DagRun.dag_id) == DagModel.dag_id\",",
          "169:         uselist=False,",
          "170:         viewonly=True,",
          "171:     )",
          "",
          "---------------"
        ],
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1145:         owner_links = session.query(DagOwnerAttributes).filter_by(dag_id=dag_id).all()",
          "1147:         attrs_to_avoid = [",
          "1148:             \"schedule_dataset_references\",",
          "1149:             \"task_outlet_dataset_references\",",
          "1150:             \"NUM_DAGS_PER_DAGRUN_QUERY\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1148:             \"schedule_datasets\",",
          "",
          "---------------"
        ],
        "tests/models/test_dag.py||tests/models/test_dag.py": [
          "File: tests/models/test_dag.py -> tests/models/test_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "65: from airflow.utils.weight_rule import WeightRule",
          "66: from tests.models import DEFAULT_DATE",
          "67: from tests.test_utils.asserts import assert_queries_count",
          "69: from tests.test_utils.mapping import expand_mapped_task",
          "70: from tests.test_utils.timetables import cron_timetable, delta_timetable",
          "",
          "[Removed Lines]",
          "68: from tests.test_utils.db import clear_db_dags, clear_db_runs",
          "",
          "[Added Lines]",
          "68: from tests.test_utils.db import clear_db_dags, clear_db_datasets, clear_db_runs",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2104: class TestDagModel:",
          "2105:     def test_dags_needing_dagruns_not_too_early(self):",
          "2106:         dag = DAG(dag_id='far_future_dag', start_date=timezone.datetime(2038, 1, 1))",
          "2107:         EmptyOperator(task_id='dummy', dag=dag, owner='airflow')",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2105:     def _clean(self):",
          "2106:         clear_db_dags()",
          "2107:         clear_db_datasets()",
          "2108:         clear_db_runs()",
          "2110:     def setup_method(self):",
          "2111:         self._clean()",
          "2113:     def teardown_method(self):",
          "2114:         self._clean()",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2125:         session.rollback()",
          "2126:         session.close()",
          "2128:     def test_max_active_runs_not_none(self):",
          "2129:         dag = DAG(dag_id='test_max_active_runs_not_none', start_date=timezone.datetime(2038, 1, 1))",
          "2130:         EmptyOperator(task_id='dummy', dag=dag, owner='airflow')",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2139:     def test_dags_needing_dagruns_datasets(self, dag_maker, session):",
          "2140:         dataset = Dataset(uri='hello')",
          "2141:         with dag_maker(",
          "2142:             session=session,",
          "2143:             dag_id='my_dag',",
          "2144:             max_active_runs=1,",
          "2145:             schedule=[dataset],",
          "2146:             start_date=pendulum.now().add(days=-2),",
          "2147:         ) as dag:",
          "2148:             EmptyOperator(task_id='dummy')",
          "2150:         # there's no queue record yet, so no runs needed at this time.",
          "2151:         query, _ = DagModel.dags_needing_dagruns(session)",
          "2152:         dag_models = query.all()",
          "2153:         assert dag_models == []",
          "2155:         # add queue records so we'll need a run",
          "2156:         dag_model = session.query(DagModel).filter(DagModel.dag_id == dag.dag_id).one()",
          "2157:         dataset_model: DatasetModel = dag_model.schedule_datasets[0]",
          "2158:         session.add(DatasetDagRunQueue(dataset_id=dataset_model.id, target_dag_id=dag_model.dag_id))",
          "2159:         session.flush()",
          "2160:         query, _ = DagModel.dags_needing_dagruns(session)",
          "2161:         dag_models = query.all()",
          "2162:         assert dag_models == [dag_model]",
          "2164:         # create run so we don't need a run anymore (due to max active runs)",
          "2165:         dag_maker.create_dagrun(",
          "2166:             run_type=DagRunType.DATASET_TRIGGERED,",
          "2167:             state=DagRunState.QUEUED,",
          "2168:             execution_date=pendulum.now('UTC'),",
          "2169:         )",
          "2170:         query, _ = DagModel.dags_needing_dagruns(session)",
          "2171:         dag_models = query.all()",
          "2172:         assert dag_models == []",
          "2174:         # increase max active runs and we should now need another run",
          "2175:         dag_maker.dag_model.max_active_runs = 2",
          "2176:         session.flush()",
          "2177:         query, _ = DagModel.dags_needing_dagruns(session)",
          "2178:         dag_models = query.all()",
          "2179:         assert dag_models == [dag_model]",
          "",
          "---------------"
        ]
      }
    }
  ]
}