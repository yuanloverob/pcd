{
  "cve_id": "CVE-2023-42780",
  "cve_desc": "Apache Airflow, versions prior to 2.7.2, contains a security vulnerability that allows authenticated users of Airflow to list warnings for all DAGs, even if the user had no permission to see those DAGs. It would reveal the dag_ids and the stack-traces of import errors for those DAGs with import errors.\nUsers of Apache Airflow are advised to upgrade to version 2.7.2 or newer to mitigate the risk associated with this vulnerability.\n\n",
  "repo": "apache/airflow",
  "patch_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
  "patch_info": {
    "commit_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ],
    "message": "Fix dag warning endpoint permissions (#34355)\n\n* Fix dag warning endpoint permissions\n\n* update the query to have an accurate result for total entries and pagination\n\n* add unit tests\n\n* Update test_dag_warning_endpoint.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 3570bbfbea69e2965f91b9964ce28bc268c68129)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_warning_endpoint.py -> airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "16: # under the License.",
      "17: from __future__ import annotations",
      "19: from sqlalchemy import select",
      "20: from sqlalchemy.orm import Session",
      "22: from airflow.api_connexion import security",
      "23: from airflow.api_connexion.parameters import apply_sorting, check_limit, format_parameters",
      "24: from airflow.api_connexion.schemas.dag_warning_schema import (",
      "25:     DagWarningCollection,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "19: from flask import g",
      "24: from airflow.api_connexion.exceptions import PermissionDenied",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "28: from airflow.api_connexion.types import APIResponse",
      "29: from airflow.models.dagwarning import DagWarning as DagWarningModel",
      "30: from airflow.security import permissions",
      "31: from airflow.utils.db import get_query_count",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from airflow.utils.airflow_flask_app import get_airflow_app",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "52:     allowed_filter_attrs = [\"dag_id\", \"warning_type\", \"message\", \"timestamp\"]",
      "53:     query = select(DagWarningModel)",
      "54:     if dag_id:",
      "55:         query = query.where(DagWarningModel.dag_id == dag_id)",
      "56:     if warning_type:",
      "57:         query = query.where(DagWarningModel.warning_type == warning_type)",
      "58:     total_entries = get_query_count(query, session=session)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "58:         if not get_airflow_app().appbuilder.sm.can_read_dag(dag_id, g.user):",
      "59:             raise PermissionDenied(detail=f\"User not allowed to access this DAG: {dag_id}\")",
      "61:     else:",
      "62:         readable_dags = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
      "63:         query = query.where(DagWarningModel.dag_id.in_(readable_dags))",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_warning_endpoint.py -> tests/api_connexion/endpoints/test_dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35:         app,  # type:ignore",
      "36:         username=\"test\",",
      "37:         role_name=\"Test\",",
      "39:     )",
      "40:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "42:     yield minimal_app_for_api",
      "44:     delete_user(app, username=\"test\")  # type: ignore",
      "45:     delete_user(app, username=\"test_no_permissions\")  # type: ignore",
      "48: class TestBaseDagWarning:",
      "",
      "[Removed Lines]",
      "38:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING)],  # type: ignore",
      "",
      "[Added Lines]",
      "38:         permissions=[",
      "39:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "40:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
      "41:         ],  # type: ignore",
      "44:     create_user(",
      "45:         app,  # type:ignore",
      "46:         username=\"test_with_dag2_read\",",
      "47:         role_name=\"TestWithDag2Read\",",
      "48:         permissions=[",
      "49:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "50:             (permissions.ACTION_CAN_READ, f\"{permissions.RESOURCE_DAG_PREFIX}dag2\"),",
      "51:         ],  # type: ignore",
      "52:     )",
      "58:     delete_user(app, username=\"test_with_dag2_read\")  # type: ignore",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "147:             \"/api/v1/dagWarnings\", environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"}",
      "148:         )",
      "149:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "164:     def test_should_raise_403_forbidden_when_user_has_no_dag_read_permission(self):",
      "165:         response = self.client.get(",
      "166:             \"/api/v1/dagWarnings\",",
      "167:             environ_overrides={\"REMOTE_USER\": \"test_with_dag2_read\"},",
      "168:             query_string={\"dag_id\": \"dag1\"},",
      "169:         )",
      "170:         assert response.status_code == 403",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "ce2d236584b469f88700b423ce9d2bef9801a23c",
      "candidate_info": {
        "commit_hash": "ce2d236584b469f88700b423ce9d2bef9801a23c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ce2d236584b469f88700b423ce9d2bef9801a23c",
        "files": [
          "airflow/jobs/backfill_job_runner.py",
          "airflow/jobs/scheduler_job_runner.py"
        ],
        "message": "Refactor unneeded 'continue' jumps in jobs (#33846)\n\n(cherry picked from commit e2dc0a9be1dd48c0841c2f07cb5870920d397c95)",
        "before_after_code_files": [
          "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py",
          "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py": [
          "File: airflow/jobs/backfill_job_runner.py -> airflow/jobs/backfill_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "652:                             _per_task_process(key, ti, session)",
          "653:                             try:",
          "654:                                 session.commit()",
          "657:                             except OperationalError:",
          "658:                                 self.log.error(",
          "659:                                     \"Failed to commit task state due to operational error. \"",
          "",
          "[Removed Lines]",
          "655:                                 # break the retry loop",
          "656:                                 break",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "665:                                 if i == max_attempts - 1:",
          "666:                                     raise",
          "667:                                 # retry the loop",
          "668:             except (NoAvailablePoolSlot, DagConcurrencyLimitReached, TaskConcurrencyLimitReached) as e:",
          "669:                 self.log.debug(e)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "666:                             else:",
          "667:                                 # break the retry loop",
          "668:                                 break",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "811:         for dagrun_info in dagrun_infos:",
          "812:             for dag in self._get_dag_with_subdags():",
          "813:                 dag_run = self._get_dag_run(dagrun_info, dag, session=session)",
          "820:         processed_dag_run_dates = self._process_backfill_task_instances(",
          "821:             ti_status=ti_status,",
          "",
          "[Removed Lines]",
          "814:                 if dag_run is None:",
          "815:                     continue",
          "816:                 tis_map = self._task_instances_for_dag_run(dag, dag_run, session=session)",
          "817:                 ti_status.active_runs.append(dag_run)",
          "818:                 ti_status.to_run.update(tis_map or {})",
          "",
          "[Added Lines]",
          "815:                 if dag_run is not None:",
          "816:                     tis_map = self._task_instances_for_dag_run(dag, dag_run, session=session)",
          "817:                     ti_status.active_runs.append(dag_run)",
          "818:                     ti_status.to_run.update(tis_map or {})",
          "",
          "---------------"
        ],
        "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py": [
          "File: airflow/jobs/scheduler_job_runner.py -> airflow/jobs/scheduler_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "891:             )",
          "892:             for dag_run in paused_runs:",
          "893:                 dag = self.dagbag.get_dag(dag_run.dag_id, session=session)",
          "901:         except Exception as e:  # should not fail the scheduler",
          "902:             self.log.exception(\"Failed to update dag run state for paused dags due to %s\", e)",
          "",
          "[Removed Lines]",
          "894:                 if dag is None:",
          "895:                     continue",
          "897:                 dag_run.dag = dag",
          "898:                 _, callback_to_run = dag_run.update_state(execute_callbacks=False, session=session)",
          "899:                 if callback_to_run:",
          "900:                     self._send_dag_callbacks_to_processor(dag, callback_to_run)",
          "",
          "[Added Lines]",
          "894:                 if dag is not None:",
          "895:                     dag_run.dag = dag",
          "896:                     _, callback_to_run = dag_run.update_state(execute_callbacks=False, session=session)",
          "897:                     if callback_to_run:",
          "898:                         self._send_dag_callbacks_to_processor(dag, callback_to_run)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1067:         )",
          "1068:         for dag_run, callback_to_run in callback_tuples:",
          "1069:             dag = cached_get_dag(dag_run.dag_id)",
          "1072:                 self.log.error(\"DAG '%s' not found in serialized_dag table\", dag_run.dag_id)",
          "1078:         with prohibit_commit(session) as guard:",
          "1079:             # Without this, the session has an invalid view of the DB",
          "",
          "[Removed Lines]",
          "1071:             if not dag:",
          "1073:                 continue",
          "1074:             # Sending callbacks there as in standalone_dag_processor they are adding to the database,",
          "1075:             # so it must be done outside of prohibit_commit.",
          "1076:             self._send_dag_callbacks_to_processor(dag, callback_to_run)",
          "",
          "[Added Lines]",
          "1068:             if dag:",
          "1069:                 # Sending callbacks there as in standalone_dag_processor they are adding to the database,",
          "1070:                 # so it must be done outside of prohibit_commit.",
          "1071:                 self._send_dag_callbacks_to_processor(dag, callback_to_run)",
          "1072:             else:",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2324041b324932cf0661679b59d9480c60cd2577",
      "candidate_info": {
        "commit_hash": "2324041b324932cf0661679b59d9480c60cd2577",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/2324041b324932cf0661679b59d9480c60cd2577",
        "files": [
          "airflow/operators/python.py",
          "tests/operators/test_python.py"
        ],
        "message": "Deprecate numeric type python version in PythonVirtualEnvOperator (#34359)\n\n* Remove float type python version in PythonVirtualEnvOperator\n\nRemove float type python version in PythonVirtualEnvOperator\n\n* Remove int type python version in PythonVirtualEnvOperator\n\n* change deprecated to removed\n\nchange deprecated to removed\n\n* change removal to deprecation\n\nchange removal to deprecation\n\n* fix typo\n\nfix typo\n\n* fix line too long\n\nfix line too long\n\n* change condition statement\n\nchange condition statement\n\n* Use 'is not'\n\n---------\n\nCo-authored-by: kyeonghoon Kim <kyeonghoon.kim@bagelcode.com>\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit b23d3f964b2699d4c7f579e22d50fabc9049d1b6)",
        "before_after_code_files": [
          "airflow/operators/python.py||airflow/operators/python.py",
          "tests/operators/test_python.py||tests/operators/test_python.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/operators/python.py||airflow/operators/python.py": [
          "File: airflow/operators/python.py -> airflow/operators/python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "530:         python_callable: Callable,",
          "531:         requirements: None | Iterable[str] | str = None,",
          "533:         use_dill: bool = False,",
          "534:         system_site_packages: bool = True,",
          "535:         pip_install_options: list[str] | None = None,",
          "",
          "[Removed Lines]",
          "532:         python_version: str | int | float | None = None,",
          "",
          "[Added Lines]",
          "532:         python_version: str | None = None,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "552:                 \"major versions for PythonVirtualenvOperator. Please use string_args.\"",
          "553:                 f\"Sys version: {sys.version_info}. Venv version: {python_version}\"",
          "554:             )",
          "555:         if not is_venv_installed():",
          "556:             raise AirflowException(\"PythonVirtualenvOperator requires virtualenv, please install it.\")",
          "557:         if not requirements:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "555:         if python_version is not None and not isinstance(python_version, str):",
          "556:             warnings.warn(",
          "557:                 \"Passing non-string types (e.g. int or float) as python_version \"",
          "558:                 \"is deprecated. Please use string value instead.\",",
          "559:                 RemovedInAirflow3Warning,",
          "560:                 stacklevel=2,",
          "561:             )",
          "",
          "---------------"
        ],
        "tests/operators/test_python.py||tests/operators/test_python.py": [
          "File: tests/operators/test_python.py -> tests/operators/test_python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "963:                 return",
          "964:             raise Exception",
          "968:     def test_without_dill(self):",
          "969:         def f(a):",
          "",
          "[Removed Lines]",
          "966:         self.run_as_task(f, python_version=3, use_dill=False, requirements=[\"dill\"])",
          "",
          "[Added Lines]",
          "966:         self.run_as_task(f, python_version=\"3\", use_dill=False, requirements=[\"dill\"])",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b4d8553566c360bf0ed97fd880a1e39382c47c12",
      "candidate_info": {
        "commit_hash": "b4d8553566c360bf0ed97fd880a1e39382c47c12",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b4d8553566c360bf0ed97fd880a1e39382c47c12",
        "files": [
          "airflow/decorators/base.py",
          "airflow/executors/debug_executor.py",
          "airflow/plugins_manager.py",
          "airflow/providers_manager.py",
          "airflow/serialization/serialized_objects.py",
          "airflow/template/templater.py"
        ],
        "message": "Refactor unneeded 'continue' jumps around the repo (#33849)\n\n(cherry picked from commit 668aace06f8e4366917878eafc20550d9129faea)",
        "before_after_code_files": [
          "airflow/decorators/base.py||airflow/decorators/base.py",
          "airflow/executors/debug_executor.py||airflow/executors/debug_executor.py",
          "airflow/plugins_manager.py||airflow/plugins_manager.py",
          "airflow/providers_manager.py||airflow/providers_manager.py",
          "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py",
          "airflow/template/templater.py||airflow/template/templater.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/decorators/base.py||airflow/decorators/base.py": [
          "File: airflow/decorators/base.py -> airflow/decorators/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "102:         kwargs_left = kwargs.copy()",
          "103:         for arg_name in self._mappable_function_argument_names:",
          "104:             value = kwargs_left.pop(arg_name, NOTSET)",
          "109:         if len(kwargs_left) == 1:",
          "110:             raise TypeError(f\"{func}() got an unexpected keyword argument {next(iter(kwargs_left))!r}\")",
          "111:         elif kwargs_left:",
          "",
          "[Removed Lines]",
          "105:             if func != \"expand\" or value is NOTSET or is_mappable(value):",
          "106:                 continue",
          "107:             tname = type(value).__name__",
          "108:             raise ValueError(f\"expand() got an unexpected type {tname!r} for keyword argument {arg_name!r}\")",
          "",
          "[Added Lines]",
          "105:             if func == \"expand\" and value is not NOTSET and not is_mappable(value):",
          "106:                 tname = type(value).__name__",
          "107:                 raise ValueError(",
          "108:                     f\"expand() got an unexpected type {tname!r} for keyword argument {arg_name!r}\"",
          "109:                 )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "147:         prefix = re2.split(r\"__\\d+$\", tg_task_id)[0]",
          "148:         for task_id in dag.task_ids:",
          "149:             match = re2.match(rf\"^{prefix}__(\\d+)$\", task_id)",
          "153:         yield 0  # Default if there's no matching task ID.",
          "155:     core = re2.split(r\"__\\d+$\", task_id)[0]",
          "",
          "[Removed Lines]",
          "150:             if match is None:",
          "151:                 continue",
          "152:             yield int(match.group(1))",
          "",
          "[Added Lines]",
          "151:             if match:",
          "152:                 yield int(match.group(1))",
          "",
          "---------------"
        ],
        "airflow/executors/debug_executor.py||airflow/executors/debug_executor.py": [
          "File: airflow/executors/debug_executor.py -> airflow/executors/debug_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "71:                 self.log.info(\"Setting %s to %s\", ti.key, TaskInstanceState.UPSTREAM_FAILED)",
          "72:                 ti.set_state(TaskInstanceState.UPSTREAM_FAILED)",
          "73:                 self.change_state(ti.key, TaskInstanceState.UPSTREAM_FAILED)",
          "77:                 self.log.info(\"Executor is terminated! Stopping %s to %s\", ti.key, TaskInstanceState.FAILED)",
          "78:                 ti.set_state(TaskInstanceState.FAILED)",
          "79:                 self.change_state(ti.key, TaskInstanceState.FAILED)",
          "84:     def _run_task(self, ti: TaskInstance) -> bool:",
          "85:         self.log.debug(\"Executing task: %s\", ti)",
          "",
          "[Removed Lines]",
          "74:                 continue",
          "76:             if self._terminated.is_set():",
          "80:                 continue",
          "82:             task_succeeded = self._run_task(ti)",
          "",
          "[Added Lines]",
          "74:             elif self._terminated.is_set():",
          "78:             else:",
          "79:                 task_succeeded = self._run_task(ti)",
          "",
          "---------------"
        ],
        "airflow/plugins_manager.py||airflow/plugins_manager.py": [
          "File: airflow/plugins_manager.py -> airflow/plugins_manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import os",
          "27: import sys",
          "28: import types",
          "29: from typing import TYPE_CHECKING, Any, Iterable",
          "31: try:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "29: from pathlib import Path",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "252:     log.debug(\"Loading plugins from directory: %s\", settings.PLUGINS_FOLDER)",
          "254:     for file_path in find_path_from_directory(settings.PLUGINS_FOLDER, \".airflowignore\"):",
          "259:             continue",
          "261:         try:",
          "262:             loader = importlib.machinery.SourceFileLoader(mod_name, file_path)",
          "",
          "[Removed Lines]",
          "255:         if not os.path.isfile(file_path):",
          "256:             continue",
          "257:         mod_name, file_ext = os.path.splitext(os.path.split(file_path)[-1])",
          "258:         if file_ext != \".py\":",
          "",
          "[Added Lines]",
          "256:         path = Path(file_path)",
          "257:         if not path.is_file() or path.suffix != \".py\":",
          "259:         mod_name = path.stem",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "287:         try:",
          "288:             plugin_instance = import_string(plugin.plugin_class)",
          "290:                 log.warning(\"Plugin %s is not a valid plugin\", plugin.name)",
          "293:         except ImportError:",
          "294:             log.exception(\"Failed to load plugin %s from class name %s\", plugin.name, plugin.plugin_class)",
          "298: def make_module(name: str, objects: list[Any]):",
          "",
          "[Removed Lines]",
          "289:             if not is_valid_plugin(plugin_instance):",
          "291:                 continue",
          "292:             register_plugin(plugin_instance)",
          "295:             continue",
          "",
          "[Added Lines]",
          "289:             if is_valid_plugin(plugin_instance):",
          "290:                 register_plugin(plugin_instance)",
          "291:             else:",
          "",
          "---------------"
        ],
        "airflow/providers_manager.py||airflow/providers_manager.py": [
          "File: airflow/providers_manager.py -> airflow/providers_manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "587:                 # The same path can appear in the __path__ twice, under non-normalized paths (ie.",
          "588:                 # /path/to/repo/airflow/providers and /path/to/repo/./airflow/providers)",
          "589:                 path = os.path.realpath(path)",
          "594:             except Exception as e:",
          "595:                 log.warning(f\"Error when loading 'provider.yaml' files from {path} airflow sources: {e}\")",
          "",
          "[Removed Lines]",
          "590:                 if path in seen:",
          "591:                     continue",
          "592:                 seen.add(path)",
          "593:                 self._add_provider_info_from_local_source_files_on_path(path)",
          "",
          "[Added Lines]",
          "590:                 if path not in seen:",
          "591:                     seen.add(path)",
          "592:                     self._add_provider_info_from_local_source_files_on_path(path)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "957:                     hook_class.__name__,",
          "958:                 )",
          "959:                 # In case of inherited hooks this might be happening several times",
          "970:     def _add_customized_fields(self, package_name: str, hook_class: type, customized_fields: dict):",
          "971:         try:",
          "",
          "[Removed Lines]",
          "960:                 continue",
          "961:             self._connection_form_widgets[prefixed_field_name] = ConnectionFormWidgetInfo(",
          "962:                 hook_class.__name__,",
          "963:                 package_name,",
          "964:                 field,",
          "965:                 field_identifier,",
          "966:                 hasattr(field.field_class.widget, \"input_type\")",
          "967:                 and field.field_class.widget.input_type == \"password\",",
          "968:             )",
          "",
          "[Added Lines]",
          "959:             else:",
          "960:                 self._connection_form_widgets[prefixed_field_name] = ConnectionFormWidgetInfo(",
          "961:                     hook_class.__name__,",
          "962:                     package_name,",
          "963:                     field,",
          "964:                     field_identifier,",
          "965:                     hasattr(field.field_class.widget, \"input_type\")",
          "966:                     and field.field_class.widget.input_type == \"password\",",
          "967:                 )",
          "",
          "---------------"
        ],
        "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py": [
          "File: airflow/serialization/serialized_objects.py -> airflow/serialization/serialized_objects.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "645:             return False",
          "647:         for attr in attrs:",
          "655:                 kwargs[attr] = val",
          "656:         return class_(**kwargs)",
          "",
          "[Removed Lines]",
          "648:             if attr not in param_dict:",
          "649:                 continue",
          "650:             val = param_dict[attr]",
          "651:             if is_serialized(val):",
          "652:                 deserialized_val = cls.deserialize(param_dict[attr])",
          "653:                 kwargs[attr] = deserialized_val",
          "654:             else:",
          "",
          "[Added Lines]",
          "648:             if attr in param_dict:",
          "649:                 val = param_dict[attr]",
          "650:                 if is_serialized(val):",
          "651:                     val = cls.deserialize(val)",
          "",
          "---------------"
        ],
        "airflow/template/templater.py||airflow/template/templater.py": [
          "File: airflow/template/templater.py -> airflow/template/templater.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "68:         if self.template_ext:",
          "69:             for field in self.template_fields:",
          "70:                 content = getattr(self, field, None)",
          "74:                     env = self.get_template_env()",
          "75:                     try:",
          "76:                         setattr(self, field, env.loader.get_source(env, content)[0])  # type: ignore",
          "",
          "[Removed Lines]",
          "71:                 if content is None:",
          "72:                     continue",
          "73:                 elif isinstance(content, str) and content.endswith(tuple(self.template_ext)):",
          "",
          "[Added Lines]",
          "71:                 if isinstance(content, str) and content.endswith(tuple(self.template_ext)):",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "38931c89bf2f5ef7cbb57dcfc275899e9c4c4677",
      "candidate_info": {
        "commit_hash": "38931c89bf2f5ef7cbb57dcfc275899e9c4c4677",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/38931c89bf2f5ef7cbb57dcfc275899e9c4c4677",
        "files": [
          "airflow/models/dag.py",
          "airflow/utils/log/trigger_handler.py"
        ],
        "message": "Replace dict.items by dict.values when key is not used in core (#33940)\n\n(cherry picked from commit 9d2cfb6261d26b7285b04323f216db2d8ad839e0)",
        "before_after_code_files": [
          "airflow/models/dag.py||airflow/models/dag.py",
          "airflow/utils/log/trigger_handler.py||airflow/utils/log/trigger_handler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "3283:         if not self.timetable.can_be_scheduled:",
          "3284:             return",
          "3287:             # As type can be an array, we would check if `null` is an allowed type or not",
          "3288:             if not v.has_value and (\"type\" not in v.schema or \"null\" not in v.schema[\"type\"]):",
          "3289:                 raise AirflowException(",
          "",
          "[Removed Lines]",
          "3286:         for k, v in self.params.items():",
          "",
          "[Added Lines]",
          "3286:         for v in self.params.values():",
          "",
          "---------------"
        ],
        "airflow/utils/log/trigger_handler.py||airflow/utils/log/trigger_handler.py": [
          "File: airflow/utils/log/trigger_handler.py -> airflow/utils/log/trigger_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "111:             del self.handlers[trigger_id]",
          "113:     def flush(self):",
          "115:             h.flush()",
          "117:     def close(self):",
          "",
          "[Removed Lines]",
          "114:         for _, h in self.handlers.items():",
          "",
          "[Added Lines]",
          "114:         for h in self.handlers.values():",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f8eb6ee2eb42e534302d14b6892c762c873fea96",
      "candidate_info": {
        "commit_hash": "f8eb6ee2eb42e534302d14b6892c762c873fea96",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f8eb6ee2eb42e534302d14b6892c762c873fea96",
        "files": [
          "airflow/cli/cli_config.py",
          "airflow/cli/commands/internal_api_command.py",
          "airflow/cli/commands/kubernetes_command.py",
          "airflow/cli/commands/webserver_command.py"
        ],
        "message": "Refactor unneeded 'continue' jumps in cli (#33845)\n\n(cherry picked from commit 05a0542b5ab1ec0f6eb86fa545bba524ad8db9f3)",
        "before_after_code_files": [
          "airflow/cli/cli_config.py||airflow/cli/cli_config.py",
          "airflow/cli/commands/internal_api_command.py||airflow/cli/commands/internal_api_command.py",
          "airflow/cli/commands/kubernetes_command.py||airflow/cli/commands/kubernetes_command.py",
          "airflow/cli/commands/webserver_command.py||airflow/cli/commands/webserver_command.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/cli_config.py||airflow/cli/cli_config.py": [
          "File: airflow/cli/cli_config.py -> airflow/cli/cli_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "92:         self.flags = flags",
          "93:         self.kwargs = {}",
          "94:         for k, v in locals().items():",
          "102:     def add_to_parser(self, parser: argparse.ArgumentParser):",
          "103:         \"\"\"Add this argument to an ArgumentParser.\"\"\"",
          "",
          "[Removed Lines]",
          "95:             if v is _UNSET:",
          "96:                 continue",
          "97:             if k in (\"self\", \"flags\"):",
          "98:                 continue",
          "100:             self.kwargs[k] = v",
          "",
          "[Added Lines]",
          "95:             if k not in (\"self\", \"flags\") and v is not _UNSET:",
          "96:                 self.kwargs[k] = v",
          "",
          "---------------"
        ],
        "airflow/cli/commands/internal_api_command.py||airflow/cli/commands/internal_api_command.py": [
          "File: airflow/cli/commands/internal_api_command.py -> airflow/cli/commands/internal_api_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "188:                     # Reading pid of gunicorn main process as it will be different that",
          "189:                     # the one of process spawned above.",
          "191:                         sleep(0.1)",
          "192:                         gunicorn_master_proc_pid = read_pid_from_pidfile(pid_file)",
          "196:                     # Run Gunicorn monitor",
          "197:                     gunicorn_master_proc = psutil.Process(gunicorn_master_proc_pid)",
          "",
          "[Removed Lines]",
          "190:                     while True:",
          "193:                         if gunicorn_master_proc_pid:",
          "194:                             break",
          "",
          "[Added Lines]",
          "190:                     gunicorn_master_proc_pid = None",
          "191:                     while not gunicorn_master_proc_pid:",
          "",
          "---------------"
        ],
        "airflow/cli/commands/kubernetes_command.py||airflow/cli/commands/kubernetes_command.py": [
          "File: airflow/cli/commands/kubernetes_command.py -> airflow/cli/commands/kubernetes_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "142:                     _delete_pod(pod.metadata.name, namespace)",
          "143:                 except ApiException as e:",
          "144:                     print(f\"Can't remove POD: {e}\", file=sys.stderr)",
          "147:         continue_token = pod_list.metadata._continue",
          "148:         if not continue_token:",
          "149:             break",
          "",
          "[Removed Lines]",
          "145:                 continue",
          "146:             print(f\"No action taken on pod {pod_name}\")",
          "",
          "[Added Lines]",
          "145:             else:",
          "146:                 print(f\"No action taken on pod {pod_name}\")",
          "",
          "---------------"
        ],
        "airflow/cli/commands/webserver_command.py||airflow/cli/commands/webserver_command.py": [
          "File: airflow/cli/commands/webserver_command.py -> airflow/cli/commands/webserver_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "492:                     # Reading pid of gunicorn master as it will be different that",
          "493:                     # the one of process spawned above.",
          "495:                         sleep(0.1)",
          "496:                         gunicorn_master_proc_pid = read_pid_from_pidfile(pid_file)",
          "500:                     # Run Gunicorn monitor",
          "501:                     gunicorn_master_proc = psutil.Process(gunicorn_master_proc_pid)",
          "",
          "[Removed Lines]",
          "494:                     while True:",
          "497:                         if gunicorn_master_proc_pid:",
          "498:                             break",
          "",
          "[Added Lines]",
          "494:                     gunicorn_master_proc_pid = None",
          "495:                     while not gunicorn_master_proc_pid:",
          "",
          "---------------"
        ]
      }
    }
  ]
}