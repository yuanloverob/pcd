{
  "cve_id": "CVE-2022-40604",
  "cve_desc": "In Apache Airflow 2.3.0 through 2.3.4, part of a url was unnecessarily formatted, allowing for possible information extraction.",
  "repo": "apache/airflow",
  "patch_hash": "6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
  "patch_info": {
    "commit_hash": "6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
    "files": [
      "airflow/utils/log/file_task_handler.py"
    ],
    "message": "Properly build URL to retrieve logs independently from system (#26337)\n\nThe previous way of building the path depended on the OS path\nbut it was really used to build the URL so we should use\nurllib instead of os.path.join.\n\n(cherry picked from commit 18386026c28939fa6d91d198c5489c295a05dcd2)",
    "before_after_code_files": [
      "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
    ]
  },
  "patch_diff": {
    "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
      "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "21: import warnings",
      "22: from pathlib import Path",
      "23: from typing import TYPE_CHECKING, Optional",
      "25: from airflow.configuration import AirflowConfigException, conf",
      "26: from airflow.exceptions import RemovedInAirflow3Warning",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "24: from urllib.parse import urljoin",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "194:         else:",
      "195:             import httpx",
      "199:             )",
      "200:             log += f\"*** Log file does not exist: {location}\\n\"",
      "201:             log += f\"*** Fetching from: {url}\\n\"",
      "",
      "[Removed Lines]",
      "197:             url = os.path.join(\"http://{ti.hostname}:{worker_log_server_port}/log\", log_relative_path).format(",
      "198:                 ti=ti, worker_log_server_port=conf.get('logging', 'WORKER_LOG_SERVER_PORT')",
      "",
      "[Added Lines]",
      "198:             url = urljoin(",
      "199:                 f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log\", log_relative_path",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "63a5276e925361a4ab33a5b172da87cc69ba86ee",
      "candidate_info": {
        "commit_hash": "63a5276e925361a4ab33a5b172da87cc69ba86ee",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/63a5276e925361a4ab33a5b172da87cc69ba86ee",
        "files": [
          "airflow/models/dag.py",
          "airflow/models/dagrun.py",
          "airflow/www/views.py",
          "tests/models/test_dag.py"
        ],
        "message": "Respect max_active_runs for dataset-triggered dags (#26348)\n\nCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>\nCo-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>\n(cherry picked from commit b99d1cd5d32aea5721c512d6052b6b7b3e0dfefb)",
        "before_after_code_files": [
          "airflow/models/dag.py||airflow/models/dag.py",
          "airflow/models/dagrun.py||airflow/models/dagrun.py",
          "airflow/www/views.py||airflow/www/views.py",
          "tests/models/test_dag.py||tests/models/test_dag.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "54: from dateutil.relativedelta import relativedelta",
          "55: from pendulum.tz.timezone import Timezone",
          "56: from sqlalchemy import Boolean, Column, ForeignKey, Index, Integer, String, Text, and_, case, func, not_, or_",
          "57: from sqlalchemy.orm import backref, joinedload, relationship",
          "58: from sqlalchemy.orm.query import Query",
          "59: from sqlalchemy.orm.session import Session",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "57: from sqlalchemy.ext.associationproxy import association_proxy",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3066:         \"DagScheduleDatasetReference\",",
          "3067:         cascade='all, delete, delete-orphan',",
          "3068:     )",
          "3069:     task_outlet_dataset_references = relationship(",
          "3070:         \"TaskOutletDatasetReference\",",
          "3071:         cascade='all, delete, delete-orphan',",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3070:     schedule_datasets = association_proxy('schedule_dataset_references', 'dataset')",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "3235:         transaction is committed it will be unlocked.",
          "3236:         \"\"\"",
          "3237:         # these dag ids are triggered by datasets, and they are ready to go.",
          "3239:             x.dag_id: (x.first_queued_time, x.last_queued_time)",
          "3240:             for x in session.query(",
          "3241:                 DagScheduleDatasetReference.dag_id,",
          "",
          "[Removed Lines]",
          "3238:         dataset_triggered_dag_info_list = {",
          "",
          "[Added Lines]",
          "3240:         dataset_triggered_dag_info = {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "3247:             .having(func.count() == func.sum(case((DDRQ.target_dag_id.is_not(None), 1), else_=0)))",
          "3248:             .all()",
          "3249:         }",
          "3256:         query = (",
          "3257:             session.query(cls)",
          "3258:             .filter(",
          "",
          "[Removed Lines]",
          "3250:         dataset_triggered_dag_ids = list(dataset_triggered_dag_info_list.keys())",
          "3252:         # TODO[HA]: Bake this query, it is run _A lot_",
          "3253:         # We limit so that _one_ scheduler doesn't try to do all the creation",
          "3254:         # of dag runs",
          "",
          "[Added Lines]",
          "3252:         dataset_triggered_dag_ids = set(dataset_triggered_dag_info.keys())",
          "3253:         if dataset_triggered_dag_ids:",
          "3254:             exclusion_list = {",
          "3255:                 x.dag_id",
          "3256:                 for x in (",
          "3257:                     session.query(DagModel.dag_id)",
          "3258:                     .join(DagRun.dag_model)",
          "3259:                     .filter(DagRun.state.in_((DagRunState.QUEUED, DagRunState.RUNNING)))",
          "3260:                     .filter(DagModel.dag_id.in_(dataset_triggered_dag_ids))",
          "3261:                     .group_by(DagModel.dag_id)",
          "3262:                     .having(func.count() >= func.max(DagModel.max_active_runs))",
          "3263:                     .all()",
          "3264:                 )",
          "3265:             }",
          "3266:             if exclusion_list:",
          "3267:                 dataset_triggered_dag_ids -= exclusion_list",
          "3268:                 dataset_triggered_dag_info = {",
          "3269:                     k: v for k, v in dataset_triggered_dag_info.items() if k not in exclusion_list",
          "3270:                 }",
          "3272:         # We limit so that _one_ scheduler doesn't try to do all the creation of dag runs",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "3271:         return (",
          "3272:             with_row_locks(query, of=cls, session=session, **skip_locked(session=session)),",
          "3274:         )",
          "3276:     def calculate_dagrun_date_fields(",
          "",
          "[Removed Lines]",
          "3273:             dataset_triggered_dag_info_list,",
          "",
          "[Added Lines]",
          "3290:             dataset_triggered_dag_info,",
          "",
          "---------------"
        ],
        "airflow/models/dagrun.py||airflow/models/dagrun.py": [
          "File: airflow/models/dagrun.py -> airflow/models/dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "163:     task_instances = relationship(",
          "164:         TI, back_populates=\"dag_run\", cascade='save-update, merge, delete, delete-orphan'",
          "165:     )",
          "167:     DEFAULT_DAGRUNS_TO_EXAMINE = airflow_conf.getint(",
          "168:         'scheduler',",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "166:     dag_model = relationship(",
          "167:         \"DagModel\",",
          "168:         primaryjoin=\"foreign(DagRun.dag_id) == DagModel.dag_id\",",
          "169:         uselist=False,",
          "170:         viewonly=True,",
          "171:     )",
          "",
          "---------------"
        ],
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1145:         owner_links = session.query(DagOwnerAttributes).filter_by(dag_id=dag_id).all()",
          "1147:         attrs_to_avoid = [",
          "1148:             \"schedule_dataset_references\",",
          "1149:             \"task_outlet_dataset_references\",",
          "1150:             \"NUM_DAGS_PER_DAGRUN_QUERY\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1148:             \"schedule_datasets\",",
          "",
          "---------------"
        ],
        "tests/models/test_dag.py||tests/models/test_dag.py": [
          "File: tests/models/test_dag.py -> tests/models/test_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "65: from airflow.utils.weight_rule import WeightRule",
          "66: from tests.models import DEFAULT_DATE",
          "67: from tests.test_utils.asserts import assert_queries_count",
          "69: from tests.test_utils.mapping import expand_mapped_task",
          "70: from tests.test_utils.timetables import cron_timetable, delta_timetable",
          "",
          "[Removed Lines]",
          "68: from tests.test_utils.db import clear_db_dags, clear_db_runs",
          "",
          "[Added Lines]",
          "68: from tests.test_utils.db import clear_db_dags, clear_db_datasets, clear_db_runs",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2104: class TestDagModel:",
          "2105:     def test_dags_needing_dagruns_not_too_early(self):",
          "2106:         dag = DAG(dag_id='far_future_dag', start_date=timezone.datetime(2038, 1, 1))",
          "2107:         EmptyOperator(task_id='dummy', dag=dag, owner='airflow')",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2105:     def _clean(self):",
          "2106:         clear_db_dags()",
          "2107:         clear_db_datasets()",
          "2108:         clear_db_runs()",
          "2110:     def setup_method(self):",
          "2111:         self._clean()",
          "2113:     def teardown_method(self):",
          "2114:         self._clean()",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2125:         session.rollback()",
          "2126:         session.close()",
          "2128:     def test_max_active_runs_not_none(self):",
          "2129:         dag = DAG(dag_id='test_max_active_runs_not_none', start_date=timezone.datetime(2038, 1, 1))",
          "2130:         EmptyOperator(task_id='dummy', dag=dag, owner='airflow')",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2139:     def test_dags_needing_dagruns_datasets(self, dag_maker, session):",
          "2140:         dataset = Dataset(uri='hello')",
          "2141:         with dag_maker(",
          "2142:             session=session,",
          "2143:             dag_id='my_dag',",
          "2144:             max_active_runs=1,",
          "2145:             schedule=[dataset],",
          "2146:             start_date=pendulum.now().add(days=-2),",
          "2147:         ) as dag:",
          "2148:             EmptyOperator(task_id='dummy')",
          "2150:         # there's no queue record yet, so no runs needed at this time.",
          "2151:         query, _ = DagModel.dags_needing_dagruns(session)",
          "2152:         dag_models = query.all()",
          "2153:         assert dag_models == []",
          "2155:         # add queue records so we'll need a run",
          "2156:         dag_model = session.query(DagModel).filter(DagModel.dag_id == dag.dag_id).one()",
          "2157:         dataset_model: DatasetModel = dag_model.schedule_datasets[0]",
          "2158:         session.add(DatasetDagRunQueue(dataset_id=dataset_model.id, target_dag_id=dag_model.dag_id))",
          "2159:         session.flush()",
          "2160:         query, _ = DagModel.dags_needing_dagruns(session)",
          "2161:         dag_models = query.all()",
          "2162:         assert dag_models == [dag_model]",
          "2164:         # create run so we don't need a run anymore (due to max active runs)",
          "2165:         dag_maker.create_dagrun(",
          "2166:             run_type=DagRunType.DATASET_TRIGGERED,",
          "2167:             state=DagRunState.QUEUED,",
          "2168:             execution_date=pendulum.now('UTC'),",
          "2169:         )",
          "2170:         query, _ = DagModel.dags_needing_dagruns(session)",
          "2171:         dag_models = query.all()",
          "2172:         assert dag_models == []",
          "2174:         # increase max active runs and we should now need another run",
          "2175:         dag_maker.dag_model.max_active_runs = 2",
          "2176:         session.flush()",
          "2177:         query, _ = DagModel.dags_needing_dagruns(session)",
          "2178:         dag_models = query.all()",
          "2179:         assert dag_models == [dag_model]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0967259373dcbb2f10d9b8cc06f44009e92a503c",
      "candidate_info": {
        "commit_hash": "0967259373dcbb2f10d9b8cc06f44009e92a503c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0967259373dcbb2f10d9b8cc06f44009e92a503c",
        "files": [
          "airflow/models/dagbag.py",
          "tests/dags/test_invalid_dup_task.py",
          "tests/jobs/test_scheduler_job.py",
          "tests/models/test_dagbag.py"
        ],
        "message": "Clear autoregistered DAGs if there are any import errors (#26398)\n\nWe need to clear any autoregistered DAGs that may have been already\nregistered if we encounter any import errors while parsing a given DAG\nfile.\n\nThis maintains the behavior before we autoregistered DAGs.\n\n(cherry picked from commit 01e3fb7eac6cceb1fa4cc68ee1e4fe3682edbf0a)",
        "before_after_code_files": [
          "airflow/models/dagbag.py||airflow/models/dagbag.py",
          "tests/dags/test_invalid_dup_task.py||tests/dags/test_invalid_dup_task.py",
          "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py",
          "tests/models/test_dagbag.py||tests/models/test_dagbag.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dagbag.py||airflow/models/dagbag.py": [
          "File: airflow/models/dagbag.py -> airflow/models/dagbag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "326:                 loader.exec_module(new_module)",
          "327:                 return [new_module]",
          "328:             except Exception as e:",
          "329:                 self.log.exception(\"Failed to import: %s\", filepath)",
          "330:                 if self.dagbag_import_error_tracebacks:",
          "331:                     self.import_errors[filepath] = traceback.format_exc(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "329:                 DagContext.autoregistered_dags.clear()",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "391:                     current_module = importlib.import_module(mod_name)",
          "392:                     mods.append(current_module)",
          "393:                 except Exception as e:",
          "394:                     fileloc = os.path.join(filepath, zip_info.filename)",
          "395:                     self.log.exception(\"Failed to import: %s\", fileloc)",
          "396:                     if self.dagbag_import_error_tracebacks:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "395:                     DagContext.autoregistered_dags.clear()",
          "",
          "---------------"
        ],
        "tests/dags/test_invalid_dup_task.py||tests/dags/test_invalid_dup_task.py": [
          "File: tests/dags/test_invalid_dup_task.py -> tests/dags/test_invalid_dup_task.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "19: from datetime import datetime",
          "21: from airflow import DAG",
          "22: from airflow.operators.empty import EmptyOperator",
          "24: with DAG(",
          "25:     \"test_invalid_dup_task\",",
          "26:     start_date=datetime(2021, 1, 1),",
          "27:     schedule=\"@once\",",
          "28: ):",
          "29:     EmptyOperator(task_id=\"hi\")",
          "30:     EmptyOperator(task_id=\"hi\")",
          "",
          "---------------"
        ],
        "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py": [
          "File: tests/jobs/test_scheduler_job.py -> tests/jobs/test_scheduler_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2766:         ignored_files = {",
          "2767:             'no_dags.py',",
          "2768:             'test_invalid_cron.py',",
          "2769:             'test_ignore_this.py',",
          "2770:             'test_invalid_param.py',",
          "2771:             'test_nested_dag.py',",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2769:             'test_invalid_dup_task.py',",
          "",
          "---------------"
        ],
        "tests/models/test_dagbag.py||tests/models/test_dagbag.py": [
          "File: tests/models/test_dagbag.py -> tests/models/test_dagbag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "357:             assert dag, f\"{dag_id} was bagged\"",
          "358:             assert dag.fileloc.endswith(path)",
          "360:     @patch.object(DagModel, \"get_current\")",
          "361:     def test_refresh_py_dag(self, mock_dagmodel):",
          "362:         \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "360:     def test_dag_registration_with_failure(self):",
          "361:         dagbag = models.DagBag(dag_folder=os.devnull, include_examples=False)",
          "362:         found = dagbag.process_file(str(TEST_DAGS_FOLDER / 'test_invalid_dup_task.py'))",
          "363:         assert [] == found",
          "365:     @pytest.fixture()",
          "366:     def zip_with_valid_dag_and_dup_tasks(self, tmp_path: pathlib.Path) -> Iterator[str]:",
          "367:         failing_dag_file = TEST_DAGS_FOLDER / 'test_invalid_dup_task.py'",
          "368:         working_dag_file = TEST_DAGS_FOLDER / 'test_example_bash_operator.py'",
          "369:         zipped = os.path.join(tmp_path, \"test_zip_invalid_dup_task.zip\")",
          "370:         with zipfile.ZipFile(zipped, \"w\") as zf:",
          "371:             zf.write(failing_dag_file, os.path.basename(failing_dag_file))",
          "372:             zf.write(working_dag_file, os.path.basename(working_dag_file))",
          "373:         yield zipped",
          "374:         os.unlink(zipped)",
          "376:     def test_dag_registration_with_failure_zipped(self, zip_with_valid_dag_and_dup_tasks):",
          "377:         dagbag = models.DagBag(dag_folder=os.devnull, include_examples=False)",
          "378:         found = dagbag.process_file(zip_with_valid_dag_and_dup_tasks)",
          "379:         assert 1 == len(found)",
          "380:         assert ['test_example_bash_operator'] == [dag.dag_id for dag in found]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7f6e0d7728f370aff3a17d104dd656502ed19581",
      "candidate_info": {
        "commit_hash": "7f6e0d7728f370aff3a17d104dd656502ed19581",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7f6e0d7728f370aff3a17d104dd656502ed19581",
        "files": [
          "dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py"
        ],
        "message": "Fix case when SHELL variable is not set in kubernetes tests (#26235)\n\nWhen SHELL variable is not set, kubernetes tests will fall back\nto using 'bash'\n\n(cherry picked from commit b7db311336bbe909ee7f559af2b1da7635c54823)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py||dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py||dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py -> dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1332:     env = get_k8s_env(python=python, kubernetes_version=kubernetes_version, executor=executor)",
          "1333:     kubectl_cluster_name = get_kubectl_cluster_name(python=python, kubernetes_version=kubernetes_version)",
          "1334:     get_console(output=output).print(f\"\\n[info]Running tests with {kubectl_cluster_name} cluster.\")",
          "1336:     extra_shell_args: List[str] = []",
          "1337:     if shell_binary.endswith(\"zsh\"):",
          "1338:         extra_shell_args.append('--no-rcs')",
          "",
          "[Removed Lines]",
          "1335:     shell_binary = env['SHELL']",
          "",
          "[Added Lines]",
          "1335:     shell_binary = env.get('SHELL', shutil.which('bash'))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a166fb8a3ed485ed07c492265203895006ae7cea",
      "candidate_info": {
        "commit_hash": "a166fb8a3ed485ed07c492265203895006ae7cea",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a166fb8a3ed485ed07c492265203895006ae7cea",
        "files": [
          "airflow/api_connexion/openapi/v1.yaml",
          "airflow/www/static/js/types/api-generated.ts"
        ],
        "message": "Move the deserialization of custom XCom Backend to 2.4.0 (#26392)\n\nThe change has been cherry-picked so we need to update the docs.\n\n(cherry picked from commit 41e48aaf4dbec24c924a03198b224a0b81c432d4)",
        "before_after_code_files": [
          "airflow/www/static/js/types/api-generated.ts||airflow/www/static/js/types/api-generated.ts"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/static/js/types/api-generated.ts||airflow/www/static/js/types/api-generated.ts": [
          "File: airflow/www/static/js/types/api-generated.ts -> airflow/www/static/js/types/api-generated.ts"
        ]
      }
    },
    {
      "candidate_hash": "b013ce9a951dac95f691adb5164d7e6140fedaea",
      "candidate_info": {
        "commit_hash": "b013ce9a951dac95f691adb5164d7e6140fedaea",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b013ce9a951dac95f691adb5164d7e6140fedaea",
        "files": [
          "airflow/models/dataset.py",
          "airflow/www/static/js/datasetUtils.js",
          "airflow/www/templates/airflow/dataset_next_run_modal.html",
          "airflow/www/views.py",
          "tests/www/views/test_views_grid.py"
        ],
        "message": "Fix Dataset bugs in grid view (#26356)\n\n* Fix bugs in grid view\n\n* Add DatasetModel.uri to group_by\n\n(cherry picked from commit 9a9ed5b1f6c3caf1525eb73c4d1440752f737f5e)",
        "before_after_code_files": [
          "airflow/models/dataset.py||airflow/models/dataset.py",
          "airflow/www/static/js/datasetUtils.js||airflow/www/static/js/datasetUtils.js",
          "airflow/www/templates/airflow/dataset_next_run_modal.html||airflow/www/templates/airflow/dataset_next_run_modal.html",
          "airflow/www/views.py||airflow/www/views.py",
          "tests/www/views/test_views_grid.py||tests/www/views/test_views_grid.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dataset.py||airflow/models/dataset.py": [
          "File: airflow/models/dataset.py -> airflow/models/dataset.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "160:     created_at = Column(UtcDateTime, default=timezone.utcnow, nullable=False)",
          "161:     updated_at = Column(UtcDateTime, default=timezone.utcnow, onupdate=timezone.utcnow, nullable=False)",
          "165:     __tablename__ = \"task_outlet_dataset_reference\"",
          "166:     __table_args__ = (",
          "",
          "[Removed Lines]",
          "163:     dataset = relationship(\"DatasetModel\")",
          "",
          "[Added Lines]",
          "163:     dataset = relationship(\"DatasetModel\", back_populates=\"producing_tasks\")",
          "",
          "---------------"
        ],
        "airflow/www/static/js/datasetUtils.js||airflow/www/static/js/datasetUtils.js": [
          "File: airflow/www/static/js/datasetUtils.js -> airflow/www/static/js/datasetUtils.js",
          "--- Hunk 1 ---",
          "[Context before]",
          "38:     uriCell.append(datasetLink);",
          "40:     const timeCell = document.createElement('td');",
          "43:     row.append(uriCell);",
          "44:     row.append(timeCell);",
          "",
          "[Removed Lines]",
          "41:     if (d.created_at) timeCell.append(isoDateToTimeEl(d.created_at));",
          "",
          "[Added Lines]",
          "41:     if (d.lastUpdate) timeCell.append(isoDateToTimeEl(d.lastUpdate));",
          "",
          "---------------"
        ],
        "airflow/www/templates/airflow/dataset_next_run_modal.html||airflow/www/templates/airflow/dataset_next_run_modal.html": [
          "File: airflow/www/templates/airflow/dataset_next_run_modal.html -> airflow/www/templates/airflow/dataset_next_run_modal.html",
          "--- Hunk 1 ---",
          "[Context before]",
          "40:             <thead>",
          "41:               <tr>",
          "42:                 <th>Dataset URI</th>",
          "44:               </tr>",
          "45:             </thead>",
          "46:             <tbody id=\"datasets_tbody\">",
          "",
          "[Removed Lines]",
          "43:                 <th>Timestamp</th>",
          "",
          "[Added Lines]",
          "43:                 <th>Latest Update</th>",
          "",
          "---------------"
        ],
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "96: from airflow.models.dag import DAG, get_dataset_triggered_next_run_info",
          "97: from airflow.models.dagcode import DagCode",
          "98: from airflow.models.dagrun import DagRun, DagRunType",
          "100: from airflow.models.operator import Operator",
          "101: from airflow.models.serialized_dag import SerializedDagModel",
          "102: from airflow.models.taskinstance import TaskInstance",
          "",
          "[Removed Lines]",
          "99: from airflow.models.dataset import DagScheduleDatasetReference, DatasetDagRunQueue, DatasetModel",
          "",
          "[Added Lines]",
          "99: from airflow.models.dataset import DagScheduleDatasetReference, DatasetDagRunQueue, DatasetEvent, DatasetModel",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3481:                 for info in session.query(",
          "3482:                     DatasetModel.id,",
          "3483:                     DatasetModel.uri,",
          "3485:                 )",
          "3487:                 .join(",
          "3488:                     DatasetDagRunQueue,",
          "3489:                     and_(",
          "3491:                         DatasetDagRunQueue.target_dag_id == DagScheduleDatasetReference.dag_id,",
          "3492:                     ),",
          "3493:                     isouter=True,",
          "3494:                 )",
          "3495:                 .filter(DagScheduleDatasetReference.dag_id == dag_id)",
          "3497:                 .all()",
          "3498:             ]",
          "3499:         return (",
          "",
          "[Removed Lines]",
          "3484:                     DatasetDagRunQueue.created_at,",
          "3486:                 .join(DagScheduleDatasetReference, DatasetModel.id == DagScheduleDatasetReference.dataset_id)",
          "3490:                         DatasetDagRunQueue.dataset_id == DagScheduleDatasetReference.dataset_id,",
          "3496:                 .order_by(DatasetModel.id)",
          "",
          "[Added Lines]",
          "3484:                     func.max(DatasetEvent.timestamp).label(\"lastUpdate\"),",
          "3486:                 .join(DagScheduleDatasetReference, DagScheduleDatasetReference.dataset_id == DatasetModel.id)",
          "3490:                         DatasetDagRunQueue.dataset_id == DatasetModel.id,",
          "3495:                 .join(",
          "3496:                     DatasetEvent,",
          "3497:                     and_(",
          "3498:                         DatasetEvent.dataset_id == DatasetModel.id,",
          "3499:                         DatasetEvent.timestamp > DatasetDagRunQueue.created_at,",
          "3500:                     ),",
          "3501:                     isouter=True,",
          "3502:                 )",
          "3504:                 .group_by(DatasetModel.id, DatasetModel.uri)",
          "3505:                 .order_by(DatasetModel.uri)",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_grid.py||tests/www/views/test_views_grid.py": [
          "File: tests/www/views/test_views_grid.py -> tests/www/views/test_views_grid.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: from airflow.lineage.entities import File",
          "27: from airflow.models import DagBag",
          "28: from airflow.models.dagrun import DagRun",
          "30: from airflow.operators.empty import EmptyOperator",
          "31: from airflow.utils.state import DagRunState, TaskInstanceState",
          "32: from airflow.utils.task_group import TaskGroup",
          "",
          "[Removed Lines]",
          "29: from airflow.models.dataset import DatasetDagRunQueue, DatasetModel",
          "",
          "[Added Lines]",
          "29: from airflow.models.dataset import DatasetDagRunQueue, DatasetEvent, DatasetModel",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "342:         ds1_id = session.query(DatasetModel.id).filter_by(uri=datasets[0].uri).scalar()",
          "343:         ds2_id = session.query(DatasetModel.id).filter_by(uri=datasets[1].uri).scalar()",
          "344:         ddrq = DatasetDagRunQueue(",
          "346:         )",
          "347:         session.add(ddrq)",
          "348:         session.commit()",
          "350:         resp = admin_client.get(f'/object/next_run_datasets/{DAG_ID}', follow_redirects=True)",
          "352:     assert resp.status_code == 200, resp.json",
          "353:     assert resp.json == [",
          "356:     ]",
          "",
          "[Removed Lines]",
          "345:             target_dag_id=DAG_ID, dataset_id=ds1_id, created_at=pendulum.DateTime(2022, 8, 1, tzinfo=UTC)",
          "354:         {'id': ds1_id, 'uri': 's3://bucket/key/1', 'created_at': \"2022-08-01T00:00:00+00:00\"},",
          "355:         {'id': ds2_id, 'uri': 's3://bucket/key/2', 'created_at': None},",
          "",
          "[Added Lines]",
          "345:             target_dag_id=DAG_ID, dataset_id=ds1_id, created_at=pendulum.DateTime(2022, 8, 2, tzinfo=UTC)",
          "348:         dataset_events = [",
          "349:             DatasetEvent(",
          "350:                 dataset_id=ds1_id,",
          "351:                 extra={},",
          "352:                 timestamp=pendulum.DateTime(2022, 8, 1, 1, tzinfo=UTC),",
          "353:             ),",
          "354:             DatasetEvent(",
          "355:                 dataset_id=ds1_id,",
          "356:                 extra={},",
          "357:                 timestamp=pendulum.DateTime(2022, 8, 2, 1, tzinfo=UTC),",
          "358:             ),",
          "359:             DatasetEvent(",
          "360:                 dataset_id=ds1_id,",
          "361:                 extra={},",
          "362:                 timestamp=pendulum.DateTime(2022, 8, 2, 2, tzinfo=UTC),",
          "363:             ),",
          "364:         ]",
          "365:         session.add_all(dataset_events)",
          "372:         {'id': ds1_id, 'uri': 's3://bucket/key/1', 'lastUpdate': \"2022-08-02T02:00:00+00:00\"},",
          "373:         {'id': ds2_id, 'uri': 's3://bucket/key/2', 'lastUpdate': None},",
          "",
          "---------------"
        ]
      }
    }
  ]
}