{
  "cve_id": "CVE-2023-50943",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows a potential attacker to poison the XCom data by bypassing the protection of \"enable_xcom_pickling=False\" configuration setting resulting in poisoned data after XCom deserialization. This vulnerability is considered low since it requires a DAG author to exploit it. Users are recommended to upgrade to version 2.8.1 or later, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
  "patch_info": {
    "commit_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "files": [
      "airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py"
    ],
    "message": "Stop deserializing pickle when enable_xcom_pickling is False (#36255)\n\n* Stop deserializing pickle when enable_xcom_pickling is False\n\n* Fix unit tests\n\n(cherry picked from commit 63e97abec5d56bc62a293c93f5227f364561e51c)",
    "before_after_code_files": [
      "airflow/models/xcom.py||airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py||tests/models/test_xcom.py"
    ]
  },
  "patch_diff": {
    "airflow/models/xcom.py||airflow/models/xcom.py": [
      "File: airflow/models/xcom.py -> airflow/models/xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "685:             except pickle.UnpicklingError:",
      "686:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "687:         else:",
      "693:     @staticmethod",
      "694:     def deserialize_value(result: XCom) -> Any:",
      "",
      "[Removed Lines]",
      "688:             try:",
      "689:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "690:             except (json.JSONDecodeError, UnicodeDecodeError):",
      "691:                 return pickle.loads(result.value)",
      "",
      "[Added Lines]",
      "688:             # Since xcom_pickling is disabled, we should only try to deserialize with JSON",
      "689:             return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "",
      "---------------"
    ],
    "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py": [
      "File: tests/api_connexion/schemas/test_xcom_schema.py -> tests/api_connexion/schemas/test_xcom_schema.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from airflow.models import DagRun, XCom",
      "31: from airflow.utils.dates import parse_execution_date",
      "32: from airflow.utils.session import create_session",
      "34: pytestmark = pytest.mark.db_test",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "188:     default_time = \"2016-04-02T21:00:00+00:00\"",
      "189:     default_time_parsed = parse_execution_date(default_time)",
      "191:     def test_serialize(self, create_xcom, session):",
      "192:         create_xcom(",
      "193:             dag_id=\"test_dag\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "192:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "208:             \"map_index\": -1,",
      "209:         }",
      "211:     def test_deserialize(self):",
      "212:         xcom_dump = {",
      "213:             \"key\": \"test_key\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "213:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------"
    ],
    "tests/models/test_xcom.py||tests/models/test_xcom.py": [
      "File: tests/models/test_xcom.py -> tests/models/test_xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "140:             ret_value = XCom.get_value(key=\"xcom_test3\", ti_key=ti_key, session=session)",
      "141:         assert ret_value == {\"key\": \"value\"}",
      "144:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"}):",
      "145:             XCom.set(",
      "146:                 key=\"xcom_test3\",",
      "",
      "[Removed Lines]",
      "143:     def test_xcom_deserialize_with_pickle_to_json_switch(self, task_instance, session):",
      "",
      "[Added Lines]",
      "143:     def test_xcom_deserialize_pickle_when_xcom_pickling_is_disabled(self, task_instance, session):",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "151:                 session=session,",
      "152:             )",
      "153:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"False\"}):",
      "163:     @conf_vars({(\"core\", \"xcom_enable_pickling\"): \"False\"})",
      "164:     def test_xcom_disable_pickle_type_fail_on_non_json(self, task_instance, session):",
      "",
      "[Removed Lines]",
      "154:             ret_value = XCom.get_one(",
      "155:                 key=\"xcom_test3\",",
      "156:                 dag_id=task_instance.dag_id,",
      "157:                 task_id=task_instance.task_id,",
      "158:                 run_id=task_instance.run_id,",
      "159:                 session=session,",
      "160:             )",
      "161:         assert ret_value == {\"key\": \"value\"}",
      "",
      "[Added Lines]",
      "154:             with pytest.raises(UnicodeDecodeError):",
      "155:                 XCom.get_one(",
      "156:                     key=\"xcom_test3\",",
      "157:                     dag_id=task_instance.dag_id,",
      "158:                     task_id=task_instance.task_id,",
      "159:                     run_id=task_instance.run_id,",
      "160:                     session=session,",
      "161:                 )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "07e30c9f9c1e8eda82360d66ba74970ebb64a65a",
      "candidate_info": {
        "commit_hash": "07e30c9f9c1e8eda82360d66ba74970ebb64a65a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/07e30c9f9c1e8eda82360d66ba74970ebb64a65a",
        "files": [
          "setup.cfg"
        ],
        "message": "Fix configuration of mypy plugins to point to paths not modules (#36563)\n\nThe configuration of our MyPy plugins was wrongly pointing to\nmodules rather than paths. This caused problems in the environment\nwhere you had no PYTHONPATH set pointing to the root of your\nAirflow sources. One of the side effects was that MyPy Plugin\nfor IntelliJ failed with \"invalid plugin\" error.\n\nThis PR changes the plugins to use relative paths instead - which\nshould work when mypy is invoked from the root of the\nproject (which in general is how our mypy gets invoked anyway and\nis the default settings for most IDE integrations.\n\n(cherry picked from commit 8fba23fc8450d13c3a241252b547b95c0e258782)",
        "before_after_code_files": [
          "setup.cfg||setup.cfg"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.cfg||setup.cfg": [
          "File: setup.cfg -> setup.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "205: warn_redundant_casts = True",
          "206: warn_unused_ignores = False",
          "207: plugins =",
          "210: pretty = True",
          "211: show_error_codes = True",
          "212: # Mypy since 0.991 warns about type annotations being present in an untyped",
          "",
          "[Removed Lines]",
          "208:   dev.mypy.plugin.decorators,",
          "209:   dev.mypy.plugin.outputs",
          "",
          "[Added Lines]",
          "208:   dev/mypy/plugin/decorators.py,",
          "209:   dev/mypy/plugin/outputs.py",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "99ec731487e57cf3245953fda4d9a5746677a4c7",
      "candidate_info": {
        "commit_hash": "99ec731487e57cf3245953fda4d9a5746677a4c7",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/99ec731487e57cf3245953fda4d9a5746677a4c7",
        "files": [
          "dev/breeze/src/airflow_breeze/commands/common_package_installation_options.py",
          "images/breeze/output_release-management_install-provider-packages.svg",
          "images/breeze/output_release-management_install-provider-packages.txt",
          "images/breeze/output_release-management_verify-provider-packages.svg",
          "images/breeze/output_release-management_verify-provider-packages.txt",
          "images/breeze/output_shell.svg",
          "images/breeze/output_shell.txt",
          "images/breeze/output_start-airflow.svg",
          "images/breeze/output_start-airflow.txt",
          "scripts/in_container/install_airflow_and_providers.py"
        ],
        "message": "Fix --use-airflow-version constraints (#36378)\n\nWhen `--use-airflow-version` is a numeric or rc version, the constraints\nshould be specific for that version when installing airflow. For example\nwhen we install 2.7.3rc1, `constraints-2.7.3rc1` should be used.\n\nThis has been lost when fixing version in CI.\n\nThis PR introduces these fixes:\n\n* default varlue for airflow constraints is DEFAULT_AIRFLOW_CONSTRAINTS_BRANCH\n\n* when --use-airflow-version is numeric version and default value is\n  used for constraints (DEFAULT_AIRFLOW_CONSTRAINTS_BRANCH) then it\n  is replaced with `constraints-VERSION`\n\n* when we print out constraints used, we print which are the\n  constraints used by Airflow and which by providers.\n\n(cherry picked from commit 5ddd67a9a670caf210fbcd15e033561ebe4404d8)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/common_package_installation_options.py||dev/breeze/src/airflow_breeze/commands/common_package_installation_options.py",
          "scripts/in_container/install_airflow_and_providers.py||scripts/in_container/install_airflow_and_providers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/common_package_installation_options.py||dev/breeze/src/airflow_breeze/commands/common_package_installation_options.py": [
          "File: dev/breeze/src/airflow_breeze/commands/common_package_installation_options.py -> dev/breeze/src/airflow_breeze/commands/common_package_installation_options.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import click",
          "22: from airflow_breeze.global_constants import ALLOWED_CONSTRAINTS_MODES_CI, ALLOWED_CONSTRAINTS_MODES_PROD",
          "23: from airflow_breeze.utils.custom_param_types import BetterChoice",
          "25: option_airflow_constraints_reference = click.option(",
          "26:     \"--airflow-constraints-reference\",",
          "29:     envvar=\"AIRFLOW_CONSTRAINTS_REFERENCE\",",
          "30: )",
          "31: option_airflow_constraints_location = click.option(",
          "",
          "[Removed Lines]",
          "27:     help=\"Constraint reference to use for airflow installation (used in calculated constraints URL). \"",
          "28:     \"Can be 'default' in which case the default constraints-reference is used.\",",
          "",
          "[Added Lines]",
          "22: from airflow_breeze.branch_defaults import DEFAULT_AIRFLOW_CONSTRAINTS_BRANCH",
          "28:     default=DEFAULT_AIRFLOW_CONSTRAINTS_BRANCH,",
          "29:     help=\"Constraint reference to use for airflow installation (used in calculated constraints URL).\",",
          "",
          "---------------"
        ],
        "scripts/in_container/install_airflow_and_providers.py||scripts/in_container/install_airflow_and_providers.py": [
          "File: scripts/in_container/install_airflow_and_providers.py -> scripts/in_container/install_airflow_and_providers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "76:     constraints_reference: str | None,",
          "77:     github_repository: str,",
          "78:     python_version: str,",
          "79: ):",
          "80:     constraints_base = f\"https://raw.githubusercontent.com/{github_repository}/{constraints_reference}\"",
          "81:     location = f\"{constraints_base}/{constraints_mode}-{python_version}.txt\"",
          "83:     return location",
          "",
          "[Removed Lines]",
          "82:     console.print(f\"[info]Determined constraints as: {location}\")",
          "",
          "[Added Lines]",
          "79:     providers: bool,",
          "83:     console.print(f\"[info]Determined {'providers' if providers else 'airflow'} constraints as: {location}\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "126:         constraints_reference=airflow_constraints_reference,",
          "127:         github_repository=github_repository,",
          "128:         python_version=python_version,",
          "129:     )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "130:         providers=False,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "157:         constraints_reference=providers_constraints_reference,",
          "158:         python_version=python_version,",
          "159:         github_repository=github_repository,",
          "160:     )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "162:         providers=True,",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b3031114b8d0d3aff361475acbcd2d987b11298d",
      "candidate_info": {
        "commit_hash": "b3031114b8d0d3aff361475acbcd2d987b11298d",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b3031114b8d0d3aff361475acbcd2d987b11298d",
        "files": [
          ".github/actions/build-ci-images/action.yml",
          "generated/provider_dependencies.json",
          "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py"
        ],
        "message": "Remove additional generation of dependencies when building CI images (#36283)\n\nWhen generated dependencies are not properly updated, we had a special\nstep where the dependencies were generated \"just in case\" before CI\nimage was built, because otherwise building the CI image could have\nfailed with strange \"failed because of conflicting dependencies\"\nwithout a clue what was the root cause.\n\nHowever, the pre-commit did not return error exit code - because for the\npre-commit, it is enough that a file is modified during pre-commit to\nfail the pre-commit in general.\n\nThat had a nasty side effect because the built CI image actually already\ncontained properly generated dependencies (by this step), and it did not\nproperly detected cases where the ones in the repository were added\nmanually and not generated with pre-commit.\n\nThis PR fixes it - instead of generating and building such image in\nCI it will now fail the CI image building step but with clear\ninstructions what to do.\n\nThe CI job step uses now regular breeze command rather than running\nthe script manually but also the script returns error code in case\nthe generated dependencies have been updated.\n\n(cherry picked from commit 33a2fbef9ff656c3522ea8dea5fff2e2c2645abf)",
        "before_after_code_files": [
          "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py||scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py||scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py": [
          "File: scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py -> scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "213:         DEPENDENCIES_JSON_FILE_PATH.write_text(json.dumps(unique_sorted_dependencies, indent=2) + \"\\n\")",
          "214:         if os.environ.get(\"CI\"):",
          "215:             console.print()",
          "217:             console.print(",
          "220:             )",
          "221:             console.print()",
          "222:         else:",
          "223:             console.print()",
          "",
          "[Removed Lines]",
          "216:             console.print(f\"[info]Written {DEPENDENCIES_JSON_FILE_PATH}\")",
          "218:                 f\"[yellow]You will need to run breeze locally and commit \"",
          "219:                 f\"{DEPENDENCIES_JSON_FILE_PATH.relative_to(AIRFLOW_SOURCES_ROOT)}!\\n\"",
          "",
          "[Added Lines]",
          "216:             console.print(f\"[info]There is a need to regenerate {DEPENDENCIES_JSON_FILE_PATH}\")",
          "218:                 f\"[red]You need to run the following command locally and commit generated \"",
          "219:                 f\"{DEPENDENCIES_JSON_FILE_PATH.relative_to(AIRFLOW_SOURCES_ROOT)} file:\\n\"",
          "221:             console.print(\"breeze static-checks --type update-providers-dependencies --all-files\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "227:             )",
          "228:             console.print(f\"[info]Written {DEPENDENCIES_JSON_FILE_PATH}\")",
          "229:             console.print()",
          "230:     else:",
          "231:         console.print(",
          "232:             \"[green]No need to regenerate dependencies!\\n[/]\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "231:         sys.exit(1)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d8537857036a36dcfda171395092f932770d6dda",
      "candidate_info": {
        "commit_hash": "d8537857036a36dcfda171395092f932770d6dda",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/d8537857036a36dcfda171395092f932770d6dda",
        "files": [
          "dev/README_RELEASE_PROVIDER_PACKAGES.md",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py"
        ],
        "message": "Automate rcN calculation when releasing provider packages (#36441)\n\nThis change will automatically generate the **right** rcN package\nwhen prepareing the packages for PyPI. This allows to have pretty\nmuch continuous release process for voting over the provider packages.\n\nSimply when an rcN candidate is not released, it will be automatically\nincluded in the next wave of packages with rcN+1 version - unless during\nprovider package generation the version will be bumped to MAJOR or MINOR\ndue to new changes.\n\nThis allows for the workflow where in every new wave we always generate\nall provider packages ready for release.\n\n(cherry picked from commit 6f5a50ea10842c2bb4b6bdc1e28dfaa680536d5a)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py||dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "563:         shutil.rmtree(DIST_DIR, ignore_errors=True)",
          "564:         DIST_DIR.mkdir(parents=True, exist_ok=True)",
          "565:     for provider_id in packages_list:",
          "566:         try:",
          "567:             basic_provider_checks(provider_id)",
          "570:             get_console().print()",
          "571:             with ci_group(f\"Preparing provider package [special]{provider_id}\"):",
          "572:                 get_console().print()",
          "573:                 target_provider_root_sources_path = copy_provider_sources_to_target(provider_id)",
          "574:                 generate_build_files(",
          "575:                     provider_id=provider_id,",
          "577:                     target_provider_root_sources_path=target_provider_root_sources_path,",
          "578:                 )",
          "579:                 cleanup_build_remnants(target_provider_root_sources_path)",
          "",
          "[Removed Lines]",
          "568:             if not skip_tag_check and should_skip_the_package(provider_id, version_suffix_for_pypi):",
          "569:                 continue",
          "576:                     version_suffix=version_suffix_for_pypi,",
          "",
          "[Added Lines]",
          "566:         package_version = version_suffix_for_pypi",
          "569:             if not skip_tag_check:",
          "570:                 should_skip, package_version = should_skip_the_package(provider_id, package_version)",
          "571:                 if should_skip:",
          "572:                     continue",
          "579:                     version_suffix=package_version,",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py||dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py": [
          "File: dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py -> dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "155:     get_console().print(f\"\\n[info]Generated package build files for {provider_id}[/]\\n\")",
          "161:     For RC and official releases we check if the \"officially released\" version exists",
          "162:     and skip the released if it was. This allows to skip packages that have not been",
          "163:     marked for release in this wave. For \"dev\" suffixes, we always build all packages.",
          "164:     \"\"\"",
          "167:         if tag_exists_for_provider(provider_id, current_tag):",
          "173: def cleanup_build_remnants(target_provider_root_sources_path: Path):",
          "",
          "[Removed Lines]",
          "158: def should_skip_the_package(provider_id: str, version_suffix: str) -> bool:",
          "159:     \"\"\"Return True if the package should be skipped.",
          "165:     if version_suffix.startswith(\"rc\") or version_suffix == \"\":",
          "166:         current_tag = get_latest_provider_tag(provider_id, version_suffix)",
          "168:             get_console().print(f\"[warning]The tag {current_tag} exists. Skipping the package.[/]\")",
          "169:             return True",
          "170:     return False",
          "",
          "[Added Lines]",
          "158: def should_skip_the_package(provider_id: str, version_suffix: str) -> tuple[bool, str]:",
          "159:     \"\"\"Return True, version if the package should be skipped and False, good version suffix if not.",
          "165:     if version_suffix != \"\" and not version_suffix.startswith(\"rc\"):",
          "166:         return False, version_suffix",
          "167:     if version_suffix == \"\":",
          "168:         current_tag = get_latest_provider_tag(provider_id, \"\")",
          "170:             get_console().print(f\"[warning]The 'final' tag {current_tag} exists. Skipping the package.[/]\")",
          "171:             return True, version_suffix",
          "172:         return False, version_suffix",
          "173:     # version_suffix starts with \"rc\"",
          "174:     current_version = int(version_suffix[2:])",
          "175:     while True:",
          "176:         current_tag = get_latest_provider_tag(provider_id, f\"rc{current_version}\")",
          "177:         if tag_exists_for_provider(provider_id, current_tag):",
          "178:             current_version += 1",
          "179:             get_console().print(f\"[warning]The tag {current_tag} exists. Checking rc{current_version}.[/]\")",
          "180:         else:",
          "181:             return False, f\"rc{current_version}\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cdca4cae474cdf57fd0f567b26c1c91ac97d0876",
      "candidate_info": {
        "commit_hash": "cdca4cae474cdf57fd0f567b26c1c91ac97d0876",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cdca4cae474cdf57fd0f567b26c1c91ac97d0876",
        "files": [
          "airflow/utils/log/file_task_handler.py"
        ],
        "message": "Add log lookup exception for empty op subtypes (#35536)\n\n* Add log lookup exception for empty op subtypes\n\n* Use exception catching approach instead to preserve tests\n\n(cherry picked from commit ddcaef45593a5411859327ab2d16ed648073b986)",
        "before_after_code_files": [
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: from typing import TYPE_CHECKING, Any, Callable, Iterable",
          "30: from urllib.parse import urljoin",
          "32: import pendulum",
          "34: from airflow.configuration import conf",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32: import httpx",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "80: def _fetch_logs_from_service(url, log_relative_path):",
          "83:     from airflow.utils.jwt_signer import JWTSigner",
          "85:     timeout = conf.getint(\"webserver\", \"log_fetch_timeout_sec\", fallback=None)",
          "",
          "[Removed Lines]",
          "81:     import httpx",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "170:     \"\"\"",
          "172:     trigger_should_wrap = True",
          "174:     def __init__(self, base_log_folder: str, filename_template: str | None = None):",
          "175:         super().__init__()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "172:     inherits_from_empty_operator_log_message = (",
          "173:         \"Operator inherits from empty operator and thus does not have logs\"",
          "174:     )",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "555:                 messages.append(f\"Found logs served from host {url}\")",
          "556:                 logs.append(response.text)",
          "557:         except Exception as e:",
          "560:         return messages, logs",
          "562:     def _read_remote_logs(self, ti, try_number, metadata=None) -> tuple[list[str], list[str]]:",
          "",
          "[Removed Lines]",
          "558:             messages.append(f\"Could not read served logs: {e}\")",
          "559:             logger.exception(\"Could not read served logs\")",
          "",
          "[Added Lines]",
          "560:             if isinstance(e, httpx.UnsupportedProtocol) and ti.task.inherits_from_empty_operator is True:",
          "561:                 messages.append(self.inherits_from_empty_operator_log_message)",
          "562:             else:",
          "563:                 messages.append(f\"Could not read served logs: {e}\")",
          "564:                 logger.exception(\"Could not read served logs\")",
          "",
          "---------------"
        ]
      }
    }
  ]
}