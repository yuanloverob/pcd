{
  "cve_id": "CVE-2023-42780",
  "cve_desc": "Apache Airflow, versions prior to 2.7.2, contains a security vulnerability that allows authenticated users of Airflow to list warnings for all DAGs, even if the user had no permission to see those DAGs. It would reveal the dag_ids and the stack-traces of import errors for those DAGs with import errors.\nUsers of Apache Airflow are advised to upgrade to version 2.7.2 or newer to mitigate the risk associated with this vulnerability.\n\n",
  "repo": "apache/airflow",
  "patch_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
  "patch_info": {
    "commit_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ],
    "message": "Fix dag warning endpoint permissions (#34355)\n\n* Fix dag warning endpoint permissions\n\n* update the query to have an accurate result for total entries and pagination\n\n* add unit tests\n\n* Update test_dag_warning_endpoint.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 3570bbfbea69e2965f91b9964ce28bc268c68129)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_warning_endpoint.py -> airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "16: # under the License.",
      "17: from __future__ import annotations",
      "19: from sqlalchemy import select",
      "20: from sqlalchemy.orm import Session",
      "22: from airflow.api_connexion import security",
      "23: from airflow.api_connexion.parameters import apply_sorting, check_limit, format_parameters",
      "24: from airflow.api_connexion.schemas.dag_warning_schema import (",
      "25:     DagWarningCollection,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "19: from flask import g",
      "24: from airflow.api_connexion.exceptions import PermissionDenied",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "28: from airflow.api_connexion.types import APIResponse",
      "29: from airflow.models.dagwarning import DagWarning as DagWarningModel",
      "30: from airflow.security import permissions",
      "31: from airflow.utils.db import get_query_count",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from airflow.utils.airflow_flask_app import get_airflow_app",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "52:     allowed_filter_attrs = [\"dag_id\", \"warning_type\", \"message\", \"timestamp\"]",
      "53:     query = select(DagWarningModel)",
      "54:     if dag_id:",
      "55:         query = query.where(DagWarningModel.dag_id == dag_id)",
      "56:     if warning_type:",
      "57:         query = query.where(DagWarningModel.warning_type == warning_type)",
      "58:     total_entries = get_query_count(query, session=session)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "58:         if not get_airflow_app().appbuilder.sm.can_read_dag(dag_id, g.user):",
      "59:             raise PermissionDenied(detail=f\"User not allowed to access this DAG: {dag_id}\")",
      "61:     else:",
      "62:         readable_dags = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
      "63:         query = query.where(DagWarningModel.dag_id.in_(readable_dags))",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_warning_endpoint.py -> tests/api_connexion/endpoints/test_dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35:         app,  # type:ignore",
      "36:         username=\"test\",",
      "37:         role_name=\"Test\",",
      "39:     )",
      "40:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "42:     yield minimal_app_for_api",
      "44:     delete_user(app, username=\"test\")  # type: ignore",
      "45:     delete_user(app, username=\"test_no_permissions\")  # type: ignore",
      "48: class TestBaseDagWarning:",
      "",
      "[Removed Lines]",
      "38:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING)],  # type: ignore",
      "",
      "[Added Lines]",
      "38:         permissions=[",
      "39:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "40:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
      "41:         ],  # type: ignore",
      "44:     create_user(",
      "45:         app,  # type:ignore",
      "46:         username=\"test_with_dag2_read\",",
      "47:         role_name=\"TestWithDag2Read\",",
      "48:         permissions=[",
      "49:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "50:             (permissions.ACTION_CAN_READ, f\"{permissions.RESOURCE_DAG_PREFIX}dag2\"),",
      "51:         ],  # type: ignore",
      "52:     )",
      "58:     delete_user(app, username=\"test_with_dag2_read\")  # type: ignore",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "147:             \"/api/v1/dagWarnings\", environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"}",
      "148:         )",
      "149:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "164:     def test_should_raise_403_forbidden_when_user_has_no_dag_read_permission(self):",
      "165:         response = self.client.get(",
      "166:             \"/api/v1/dagWarnings\",",
      "167:             environ_overrides={\"REMOTE_USER\": \"test_with_dag2_read\"},",
      "168:             query_string={\"dag_id\": \"dag1\"},",
      "169:         )",
      "170:         assert response.status_code == 403",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "53c99eb6b828606fc6cc3283173a634db43f7006",
      "candidate_info": {
        "commit_hash": "53c99eb6b828606fc6cc3283173a634db43f7006",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/53c99eb6b828606fc6cc3283173a634db43f7006",
        "files": [
          "airflow/jobs/triggerer_job_runner.py"
        ],
        "message": "Add cancel_trigger_ids to to_cancel dequeue in batch (#33944)\n\n(cherry picked from commit f63a94d3029485d5657232ac50efa96b2c9226a4)",
        "before_after_code_files": [
          "airflow/jobs/triggerer_job_runner.py||airflow/jobs/triggerer_job_runner.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/triggerer_job_runner.py||airflow/jobs/triggerer_job_runner.py": [
          "File: airflow/jobs/triggerer_job_runner.py -> airflow/jobs/triggerer_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "680:             self.set_trigger_logging_metadata(new_trigger_orm.task_instance, new_id, new_trigger_instance)",
          "681:             self.to_create.append((new_id, new_trigger_instance))",
          "682:         # Enqueue orphaned triggers for cancellation",
          "686:     def set_trigger_logging_metadata(self, ti: TaskInstance, trigger_id, trigger):",
          "687:         \"\"\"",
          "",
          "[Removed Lines]",
          "683:         for old_id in cancel_trigger_ids:",
          "684:             self.to_cancel.append(old_id)",
          "",
          "[Added Lines]",
          "683:         self.to_cancel.extend(cancel_trigger_ids)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "eb9274e85952f5ce7cf27ab693eadc7fb7cdce9e",
      "candidate_info": {
        "commit_hash": "eb9274e85952f5ce7cf27ab693eadc7fb7cdce9e",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/eb9274e85952f5ce7cf27ab693eadc7fb7cdce9e",
        "files": [
          "airflow/www/views.py"
        ],
        "message": "Use iterative loop to look for mapped parent (#34622)\n\n(cherry picked from commit d9ba152c15dd50baa1fef41a63424225ba8ddd47)",
        "before_after_code_files": [
          "airflow/www/views.py||airflow/www/views.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "121: from airflow.utils.session import NEW_SESSION, create_session, provide_session",
          "122: from airflow.utils.state import DagRunState, State, TaskInstanceState",
          "123: from airflow.utils.strings import to_boolean",
          "125: from airflow.utils.timezone import td_format, utcnow",
          "126: from airflow.version import version",
          "127: from airflow.www import auth, utils as wwwutils",
          "",
          "[Removed Lines]",
          "124: from airflow.utils.task_group import MappedTaskGroup, TaskGroup, task_group_to_dict",
          "",
          "[Added Lines]",
          "124: from airflow.utils.task_group import TaskGroup, task_group_to_dict",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "426:             }",
          "433:         # Task Group",
          "434:         task_group = item",
          "437:         children = [",
          "438:             task_group_to_grid(child, grouped_tis, is_parent_mapped=group_is_mapped)",
          "",
          "[Removed Lines]",
          "428:         def check_group_is_mapped(tg: TaskGroup | None) -> bool:",
          "429:             if tg is None:",
          "430:                 return False",
          "431:             return isinstance(tg, MappedTaskGroup) or check_group_is_mapped(tg.parent_group)",
          "435:         group_is_mapped = check_group_is_mapped(task_group)",
          "",
          "[Added Lines]",
          "430:         group_is_mapped = next(task_group.iter_mapped_task_groups(), None) is not None",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "03f8ff5153708befc4bfa60f9bad7670250f29bf",
      "candidate_info": {
        "commit_hash": "03f8ff5153708befc4bfa60f9bad7670250f29bf",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/03f8ff5153708befc4bfa60f9bad7670250f29bf",
        "files": [
          "airflow/example_dags/example_params_trigger_ui.py",
          "airflow/example_dags/example_params_ui_tutorial.py",
          "airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py",
          "airflow/providers/cncf/kubernetes/utils/pod_manager.py",
          "airflow/providers/databricks/operators/databricks_repos.py",
          "airflow/providers/docker/operators/docker.py",
          "airflow/providers/singularity/operators/singularity.py",
          "airflow/utils/log/json_formatter.py",
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "scripts/in_container/run_provider_yaml_files_check.py"
        ],
        "message": "Refactor: Improve detection of duplicates and list sorting (#33675)\n\n(cherry picked from commit 2dbb9633240777d658031d32217255849150684b)",
        "before_after_code_files": [
          "airflow/example_dags/example_params_trigger_ui.py||airflow/example_dags/example_params_trigger_ui.py",
          "airflow/example_dags/example_params_ui_tutorial.py||airflow/example_dags/example_params_ui_tutorial.py",
          "airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py||airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py",
          "airflow/providers/cncf/kubernetes/utils/pod_manager.py||airflow/providers/cncf/kubernetes/utils/pod_manager.py",
          "airflow/providers/databricks/operators/databricks_repos.py||airflow/providers/databricks/operators/databricks_repos.py",
          "airflow/providers/docker/operators/docker.py||airflow/providers/docker/operators/docker.py",
          "airflow/providers/singularity/operators/singularity.py||airflow/providers/singularity/operators/singularity.py",
          "airflow/utils/log/json_formatter.py||airflow/utils/log/json_formatter.py",
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "scripts/in_container/run_provider_yaml_files_check.py||scripts/in_container/run_provider_yaml_files_check.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/example_dags/example_params_trigger_ui.py||airflow/example_dags/example_params_trigger_ui.py": [
          "File: airflow/example_dags/example_params_trigger_ui.py -> airflow/example_dags/example_params_trigger_ui.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: with DAG(",
          "35:     dag_id=Path(__file__).stem,",
          "37:     doc_md=__doc__,",
          "38:     schedule=None,",
          "39:     start_date=datetime.datetime(2022, 3, 4),",
          "",
          "[Removed Lines]",
          "36:     description=__doc__[0 : __doc__.find(\".\")],",
          "",
          "[Added Lines]",
          "36:     description=__doc__.partition(\".\")[0],",
          "",
          "---------------"
        ],
        "airflow/example_dags/example_params_ui_tutorial.py||airflow/example_dags/example_params_ui_tutorial.py": [
          "File: airflow/example_dags/example_params_ui_tutorial.py -> airflow/example_dags/example_params_ui_tutorial.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "36: with DAG(",
          "37:     dag_id=Path(__file__).stem,",
          "39:     doc_md=__doc__,",
          "40:     schedule=None,",
          "41:     start_date=datetime.datetime(2022, 3, 4),",
          "",
          "[Removed Lines]",
          "38:     description=__doc__[0 : __doc__.find(\".\")],",
          "",
          "[Added Lines]",
          "38:     description=__doc__.partition(\".\")[0],",
          "",
          "---------------"
        ],
        "airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py||airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py": [
          "File: airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py -> airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "185:         :param line: k8s log line",
          "186:         :return: timestamp and log message",
          "187:         \"\"\"",
          "190:             self.log.error(",
          "191:                 \"Error parsing timestamp (no timestamp in message: %r). \"",
          "192:                 \"Will continue execution but won't update timestamp\",",
          "193:                 line,",
          "194:             )",
          "195:             return None, line",
          "198:         return timestamp, message",
          "200:     def _task_status(self, event):",
          "",
          "[Removed Lines]",
          "188:         split_at = line.find(\" \")",
          "189:         if split_at == -1:",
          "196:         timestamp = line[:split_at]",
          "197:         message = line[split_at + 1 :].rstrip()",
          "",
          "[Added Lines]",
          "188:         timestamp, sep, message = line.strip().partition(\" \")",
          "189:         if not sep:",
          "",
          "---------------"
        ],
        "airflow/providers/cncf/kubernetes/utils/pod_manager.py||airflow/providers/cncf/kubernetes/utils/pod_manager.py": [
          "File: airflow/providers/cncf/kubernetes/utils/pod_manager.py -> airflow/providers/cncf/kubernetes/utils/pod_manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "524:         :param line: k8s log line",
          "525:         :return: timestamp and log message",
          "526:         \"\"\"",
          "529:             self.log.error(",
          "530:                 \"Error parsing timestamp (no timestamp in message %r). \"",
          "531:                 \"Will continue execution but won't update timestamp\",",
          "532:                 line,",
          "533:             )",
          "534:             return None, line",
          "537:         try:",
          "538:             last_log_time = cast(DateTime, pendulum.parse(timestamp))",
          "539:         except ParserError:",
          "",
          "[Removed Lines]",
          "527:         split_at = line.find(\" \")",
          "528:         if split_at == -1:",
          "535:         timestamp = line[:split_at]",
          "536:         message = line[split_at + 1 :].rstrip()",
          "",
          "[Added Lines]",
          "527:         timestamp, sep, message = line.strip().partition(\" \")",
          "528:         if not sep:",
          "",
          "---------------"
        ],
        "airflow/providers/databricks/operators/databricks_repos.py||airflow/providers/databricks/operators/databricks_repos.py": [
          "File: airflow/providers/databricks/operators/databricks_repos.py -> airflow/providers/databricks/operators/databricks_repos.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "105:     def __detect_repo_provider__(url):",
          "106:         provider = None",
          "107:         try:",
          "113:             provider = DatabricksReposCreateOperator.__git_providers__.get(netloc)",
          "114:             if provider is None and DatabricksReposCreateOperator.__aws_code_commit_regexp__.match(netloc):",
          "115:                 provider = \"awsCodeCommit\"",
          "",
          "[Removed Lines]",
          "108:             netloc = urlsplit(url).netloc",
          "109:             idx = netloc.rfind(\"@\")",
          "110:             if idx != -1:",
          "111:                 netloc = netloc[(idx + 1) :]",
          "112:             netloc = netloc.lower()",
          "",
          "[Added Lines]",
          "108:             netloc = urlsplit(url).netloc.lower()",
          "109:             _, _, netloc = netloc.rpartition(\"@\")",
          "",
          "---------------"
        ],
        "airflow/providers/docker/operators/docker.py||airflow/providers/docker/operators/docker.py": [
          "File: airflow/providers/docker/operators/docker.py -> airflow/providers/docker/operators/docker.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "487:         :return: the command (or commands)",
          "488:         \"\"\"",
          "490:             command = ast.literal_eval(command)",
          "491:         return command",
          "",
          "[Removed Lines]",
          "489:         if isinstance(command, str) and command.strip().find(\"[\") == 0:",
          "",
          "[Added Lines]",
          "489:         if isinstance(command, str) and command.strip().startswith(\"[\"):",
          "",
          "---------------"
        ],
        "airflow/providers/singularity/operators/singularity.py||airflow/providers/singularity/operators/singularity.py": [
          "File: airflow/providers/singularity/operators/singularity.py -> airflow/providers/singularity/operators/singularity.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "167:         self.log.info(\"Output from command %s\", result[\"message\"])",
          "169:     def _get_command(self) -> Any | None:",
          "171:             commands = ast.literal_eval(self.command)",
          "172:         else:",
          "173:             commands = self.command",
          "",
          "[Removed Lines]",
          "170:         if self.command is not None and self.command.strip().find(\"[\") == 0:  # type: ignore",
          "",
          "[Added Lines]",
          "170:         if self.command is not None and self.command.strip().startswith(\"[\"):  # type: ignore",
          "",
          "---------------"
        ],
        "airflow/utils/log/json_formatter.py||airflow/utils/log/json_formatter.py": [
          "File: airflow/utils/log/json_formatter.py -> airflow/utils/log/json_formatter.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "37:         self.extras = extras",
          "39:     def usesTime(self):",
          "42:     def format(self, record):",
          "43:         super().format(record)",
          "",
          "[Removed Lines]",
          "40:         return self.json_fields.count(\"asctime\") > 0",
          "",
          "[Added Lines]",
          "40:         return \"asctime\" in self.json_fields",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py": [
          "File: dev/breeze/src/airflow_breeze/utils/selective_checks.py -> dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "677:         # this should be hard-coded as we want to have very specific sequence of tests",
          "678:         sorting_order = [\"Core\", \"Providers[-amazon,google]\", \"Other\", \"Providers[amazon]\", \"WWW\"]",
          "694:     @cached_property",
          "695:     def basic_checks_only(self) -> bool:",
          "",
          "[Removed Lines]",
          "680:         def sort_key(t: str) -> str:",
          "681:             # Put the test types in the order we want them to run",
          "682:             if t in sorting_order:",
          "683:                 return str(sorting_order.index(t))",
          "684:             else:",
          "685:                 return str(len(sorting_order)) + t",
          "687:         return \" \".join(",
          "688:             sorted(",
          "689:                 current_test_types,",
          "690:                 key=sort_key,",
          "691:             )",
          "692:         )",
          "",
          "[Added Lines]",
          "679:         sort_key = {item: i for i, item in enumerate(sorting_order)}",
          "680:         # Put the test types in the order we want them to run",
          "681:         return \" \".join(sorted(current_test_types, key=lambda x: (sort_key.get(x, len(sorting_order)), x)))",
          "",
          "---------------"
        ],
        "scripts/in_container/run_provider_yaml_files_check.py||scripts/in_container/run_provider_yaml_files_check.py": [
          "File: scripts/in_container/run_provider_yaml_files_check.py -> scripts/in_container/run_provider_yaml_files_check.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "261:         yaml_files.items(), [\"sensors\", \"operators\", \"hooks\", \"triggers\"]",
          "262:     ):",
          "263:         resource_data = provider_data.get(resource_type, [])",
          "274: def check_completeness_of_list_of_transfers(yaml_files: dict[str, dict]):",
          "",
          "[Removed Lines]",
          "264:         current_integrations = [r.get(\"integration-name\", \"\") for r in resource_data]",
          "265:         if len(current_integrations) != len(set(current_integrations)):",
          "266:             for integration in current_integrations:",
          "267:                 if current_integrations.count(integration) > 1:",
          "268:                     errors.append(",
          "269:                         f\"Duplicated content of '{resource_type}/integration-name/{integration}' \"",
          "270:                         f\"in file: {yaml_file_path}\"",
          "271:                     )",
          "",
          "[Added Lines]",
          "264:         count_integrations = Counter(r.get(\"integration-name\", \"\") for r in resource_data)",
          "265:         for integration, count in count_integrations.items():",
          "266:             if count > 1:",
          "267:                 errors.append(",
          "268:                     f\"Duplicated content of '{resource_type}/integration-name/{integration}' \"",
          "269:                     f\"in file: {yaml_file_path}\"",
          "270:                 )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "343:     for yaml_file_path, provider_data in yaml_files.items():",
          "344:         resource_data = provider_data.get(resource_type, [])",
          "347:             (r.get(\"source-integration-name\", \"\"), r.get(\"target-integration-name\", \"\"))",
          "348:             for r in resource_data",
          "361: def check_invalid_integration(yaml_files: dict[str, dict]):",
          "",
          "[Removed Lines]",
          "346:         source_target_integrations = [",
          "349:         ]",
          "350:         if len(source_target_integrations) != len(set(source_target_integrations)):",
          "351:             for integration_couple in source_target_integrations:",
          "352:                 if source_target_integrations.count(integration_couple) > 1:",
          "353:                     errors.append(",
          "354:                         f\"Duplicated content of \\n\"",
          "355:                         f\" '{resource_type}/source-integration-name/{integration_couple[0]}' \"",
          "356:                         f\" '{resource_type}/target-integration-name/{integration_couple[1]}' \"",
          "357:                         f\"in file: {yaml_file_path}\"",
          "358:                     )",
          "",
          "[Added Lines]",
          "345:         count_integrations = Counter(",
          "348:         )",
          "349:         for (source, target), count in count_integrations.items():",
          "350:             if count > 1:",
          "351:                 errors.append(",
          "352:                     f\"Duplicated content of \\n\"",
          "353:                     f\" '{resource_type}/source-integration-name/{source}' \"",
          "354:                     f\" '{resource_type}/target-integration-name/{target}' \"",
          "355:                     f\"in file: {yaml_file_path}\"",
          "356:                 )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "447: def check_unique_provider_name(yaml_files: dict[str, dict]):",
          "450:     if duplicates:",
          "451:         errors.append(f\"Provider name must be unique. Duplicates: {duplicates}\")",
          "",
          "[Removed Lines]",
          "448:     provider_names = [d[\"name\"] for d in yaml_files.values()]",
          "449:     duplicates = {x for x in provider_names if provider_names.count(x) > 1}",
          "",
          "[Added Lines]",
          "446:     name_counter = Counter(d[\"name\"] for d in yaml_files.values())",
          "447:     duplicates = {k for k, v in name_counter.items() if v > 1}",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4618ae4a45f6ff76df986e41477016ad78eb8681",
      "candidate_info": {
        "commit_hash": "4618ae4a45f6ff76df986e41477016ad78eb8681",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4618ae4a45f6ff76df986e41477016ad78eb8681",
        "files": [
          "airflow/jobs/job.py",
          "airflow/settings.py",
          "airflow/utils/state.py",
          "airflow/www/jest-setup.js",
          "airflow/www/utils.py",
          "docs/apache-airflow/img/task_lifecycle_diagram.png",
          "tests/cli/commands/test_jobs_command.py",
          "tests/jobs/test_scheduler_job.py",
          "tests/models/test_dagrun.py",
          "tests/www/views/test_views_cluster_activity.py",
          "tests/www/views/test_views_home.py"
        ],
        "message": "Refactor: remove unused state - SHUTDOWN (#33746)\n\nCo-authored-by: daniel.dylag <danieldylag1990@gmail.com>\n(cherry picked from commit 7faa72795185c1af4b21f207ce7e0735c4365d46)",
        "before_after_code_files": [
          "airflow/jobs/job.py||airflow/jobs/job.py",
          "airflow/settings.py||airflow/settings.py",
          "airflow/utils/state.py||airflow/utils/state.py",
          "airflow/www/jest-setup.js||airflow/www/jest-setup.js",
          "airflow/www/utils.py||airflow/www/utils.py",
          "tests/cli/commands/test_jobs_command.py||tests/cli/commands/test_jobs_command.py",
          "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py",
          "tests/models/test_dagrun.py||tests/models/test_dagrun.py",
          "tests/www/views/test_views_cluster_activity.py||tests/www/views/test_views_cluster_activity.py",
          "tests/www/views/test_views_home.py||tests/www/views/test_views_home.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/job.py||airflow/jobs/job.py": [
          "File: airflow/jobs/job.py -> airflow/jobs/job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "190:             session.merge(self)",
          "191:             previous_heartbeat = self.latest_heartbeat",
          "194:                 # TODO: Make sure it is AIP-44 compliant",
          "195:                 self.kill()",
          "",
          "[Removed Lines]",
          "193:             if self.state in (JobState.SHUTDOWN, JobState.RESTARTING):",
          "",
          "[Added Lines]",
          "193:             if self.state == JobState.RESTARTING:",
          "",
          "---------------"
        ],
        "airflow/settings.py||airflow/settings.py": [
          "File: airflow/settings.py -> airflow/settings.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "100:     \"restarting\": \"violet\",",
          "101:     \"running\": \"lime\",",
          "102:     \"scheduled\": \"tan\",",
          "104:     \"skipped\": \"hotpink\",",
          "105:     \"success\": \"green\",",
          "106:     \"up_for_reschedule\": \"turquoise\",",
          "",
          "[Removed Lines]",
          "103:     \"shutdown\": \"blue\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/utils/state.py||airflow/utils/state.py": [
          "File: airflow/utils/state.py -> airflow/utils/state.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26:     RUNNING = \"running\"",
          "27:     SUCCESS = \"success\"",
          "29:     RESTARTING = \"restarting\"",
          "30:     FAILED = \"failed\"",
          "",
          "[Removed Lines]",
          "28:     SHUTDOWN = \"shutdown\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "51:     QUEUED = \"queued\"  # Executor has enqueued the task",
          "52:     RUNNING = \"running\"  # Task is executing",
          "53:     SUCCESS = \"success\"  # Task completed",
          "55:     RESTARTING = \"restarting\"  # External request to restart (e.g. cleared when running)",
          "56:     FAILED = \"failed\"  # Task errored out",
          "57:     UP_FOR_RETRY = \"up_for_retry\"  # Task failed but has retries left",
          "",
          "[Removed Lines]",
          "54:     SHUTDOWN = \"shutdown\"  # External request to shut down (e.g. marked failed when running)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "95:     REMOVED = TaskInstanceState.REMOVED",
          "96:     SCHEDULED = TaskInstanceState.SCHEDULED",
          "97:     QUEUED = TaskInstanceState.QUEUED",
          "99:     RESTARTING = TaskInstanceState.RESTARTING",
          "100:     UP_FOR_RETRY = TaskInstanceState.UP_FOR_RETRY",
          "101:     UP_FOR_RESCHEDULE = TaskInstanceState.UP_FOR_RESCHEDULE",
          "",
          "[Removed Lines]",
          "98:     SHUTDOWN = TaskInstanceState.SHUTDOWN",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "120:         TaskInstanceState.QUEUED: \"gray\",",
          "121:         TaskInstanceState.RUNNING: \"lime\",",
          "122:         TaskInstanceState.SUCCESS: \"green\",",
          "124:         TaskInstanceState.RESTARTING: \"violet\",",
          "125:         TaskInstanceState.FAILED: \"red\",",
          "126:         TaskInstanceState.UP_FOR_RETRY: \"gold\",",
          "",
          "[Removed Lines]",
          "123:         TaskInstanceState.SHUTDOWN: \"blue\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "169:             TaskInstanceState.SCHEDULED,",
          "170:             TaskInstanceState.QUEUED,",
          "171:             TaskInstanceState.RUNNING,",
          "173:             TaskInstanceState.RESTARTING,",
          "174:             TaskInstanceState.UP_FOR_RETRY,",
          "175:             TaskInstanceState.UP_FOR_RESCHEDULE,",
          "",
          "[Removed Lines]",
          "172:             TaskInstanceState.SHUTDOWN,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "195:     A list of states indicating that a task or dag is a success state.",
          "196:     \"\"\"",
          "203:     adoptable_states = frozenset(",
          "204:         [TaskInstanceState.QUEUED, TaskInstanceState.RUNNING, TaskInstanceState.RESTARTING]",
          "205:     )",
          "",
          "[Removed Lines]",
          "198:     terminating_states = frozenset([TaskInstanceState.SHUTDOWN, TaskInstanceState.RESTARTING])",
          "199:     \"\"\"",
          "200:     A list of states indicating that a task has been terminated.",
          "201:     \"\"\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/www/jest-setup.js||airflow/www/jest-setup.js": [
          "File: airflow/www/jest-setup.js -> airflow/www/jest-setup.js",
          "--- Hunk 1 ---",
          "[Context before]",
          "49:   restarting: \"violet\",",
          "50:   running: \"lime\",",
          "51:   scheduled: \"tan\",",
          "53:   skipped: \"hotpink\",",
          "54:   success: \"green\",",
          "55:   up_for_reschedule: \"turquoise\",",
          "",
          "[Removed Lines]",
          "52:   shutdown: \"blue\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/www/utils.py||airflow/www/utils.py": [
          "File: airflow/www/utils.py -> airflow/www/utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "108:     TaskInstanceState.SCHEDULED,",
          "109:     TaskInstanceState.DEFERRED,",
          "110:     TaskInstanceState.RUNNING,",
          "112:     TaskInstanceState.RESTARTING,",
          "113:     None,",
          "114:     TaskInstanceState.SUCCESS,",
          "",
          "[Removed Lines]",
          "111:     TaskInstanceState.SHUTDOWN,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/cli/commands/test_jobs_command.py||tests/cli/commands/test_jobs_command.py": [
          "File: tests/cli/commands/test_jobs_command.py -> tests/cli/commands/test_jobs_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: from airflow.jobs.job import Job",
          "27: from airflow.jobs.scheduler_job_runner import SchedulerJobRunner",
          "28: from airflow.utils.session import create_session",
          "30: from tests.test_utils.db import clear_db_jobs",
          "",
          "[Removed Lines]",
          "29: from airflow.utils.state import State",
          "",
          "[Added Lines]",
          "29: from airflow.utils.state import JobState, State",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "109:             for _ in range(3):",
          "110:                 scheduler_job = Job()",
          "111:                 job_runner = SchedulerJobRunner(job=scheduler_job)",
          "113:                 session.add(scheduler_job)",
          "114:                 scheduler_jobs.append(scheduler_job)",
          "115:                 job_runners.append(job_runner)",
          "",
          "[Removed Lines]",
          "112:                 scheduler_job.state = State.SHUTDOWN",
          "",
          "[Added Lines]",
          "112:                 scheduler_job.state = JobState.FAILED",
          "",
          "---------------"
        ],
        "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py": [
          "File: tests/jobs/test_scheduler_job.py -> tests/jobs/test_scheduler_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "59: from airflow.utils import timezone",
          "60: from airflow.utils.file import list_py_file_paths",
          "61: from airflow.utils.session import create_session, provide_session",
          "63: from airflow.utils.types import DagRunType",
          "64: from tests.listeners import dag_listener",
          "65: from tests.listeners.test_listeners import get_listener_manager",
          "",
          "[Removed Lines]",
          "62: from airflow.utils.state import DagRunState, State, TaskInstanceState",
          "",
          "[Added Lines]",
          "62: from airflow.utils.state import DagRunState, JobState, State, TaskInstanceState",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "4560:                 local_job = Job(dag_id=ti.dag_id)",
          "4561:                 LocalTaskJobRunner(job=local_job, task_instance=ti)",
          "4564:                 session.add(local_job)",
          "4565:                 session.flush()",
          "",
          "[Removed Lines]",
          "4562:                 local_job.state = State.SHUTDOWN",
          "",
          "[Added Lines]",
          "4562:                 local_job.state = TaskInstanceState.FAILED",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "4622:                 ti.queued_by_job_id = 999",
          "4624:                 local_job = Job(dag_id=ti.dag_id)",
          "4627:                 session.add(local_job)",
          "4628:                 session.flush()",
          "",
          "[Removed Lines]",
          "4625:                 local_job.state = State.SHUTDOWN",
          "",
          "[Added Lines]",
          "4625:                 local_job.state = TaskInstanceState.FAILED",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "4684:             local_job = Job(dag_id=ti.dag_id)",
          "4685:             LocalTaskJobRunner(job=local_job, task_instance=ti)",
          "4687:             session.add(local_job)",
          "4688:             session.flush()",
          "",
          "[Removed Lines]",
          "4686:             local_job.state = State.SHUTDOWN",
          "",
          "[Added Lines]",
          "4686:             local_job.state = JobState.FAILED",
          "",
          "---------------"
        ],
        "tests/models/test_dagrun.py||tests/models/test_dagrun.py": [
          "File: tests/models/test_dagrun.py -> tests/models/test_dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "336:         dr.update_state(session=session)",
          "337:         assert dr.state == DagRunState.FAILED",
          "341:         with dag:",
          "342:             op1 = EmptyOperator(task_id=\"upstream_task\")",
          "343:             op2 = EmptyOperator(task_id=\"downstream_task\")",
          "",
          "[Removed Lines]",
          "339:     def test_dagrun_no_deadlock_with_shutdown(self, session):",
          "340:         dag = DAG(\"test_dagrun_no_deadlock_with_shutdown\", start_date=DEFAULT_DATE)",
          "",
          "[Added Lines]",
          "339:     def test_dagrun_no_deadlock_with_restarting(self, session):",
          "340:         dag = DAG(\"test_dagrun_no_deadlock_with_restarting\", start_date=DEFAULT_DATE)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "351:             start_date=DEFAULT_DATE,",
          "352:         )",
          "353:         upstream_ti = dr.get_task_instance(task_id=\"upstream_task\")",
          "356:         dr.update_state()",
          "357:         assert dr.state == DagRunState.RUNNING",
          "",
          "[Removed Lines]",
          "354:         upstream_ti.set_state(TaskInstanceState.SHUTDOWN, session=session)",
          "",
          "[Added Lines]",
          "354:         upstream_ti.set_state(TaskInstanceState.RESTARTING, session=session)",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_cluster_activity.py||tests/www/views/test_views_cluster_activity.py": [
          "File: tests/www/views/test_views_cluster_activity.py -> tests/www/views/test_views_cluster_activity.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "115:             \"restarting\": 0,",
          "116:             \"running\": 0,",
          "117:             \"scheduled\": 0,",
          "119:             \"skipped\": 0,",
          "120:             \"success\": 2,",
          "121:             \"up_for_reschedule\": 0,",
          "",
          "[Removed Lines]",
          "118:             \"shutdown\": 0,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "144:             \"restarting\": 0,",
          "145:             \"running\": 0,",
          "146:             \"scheduled\": 0,",
          "148:             \"skipped\": 0,",
          "149:             \"success\": 0,",
          "150:             \"up_for_reschedule\": 0,",
          "",
          "[Removed Lines]",
          "147:             \"shutdown\": 0,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_home.py||tests/www/views/test_views_home.py": [
          "File: tests/www/views/test_views_home.py -> tests/www/views/test_views_home.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "57:             '\"null\": \"lightblue\", \"queued\": \"gray\", '",
          "58:             '\"removed\": \"lightgrey\", \"restarting\": \"violet\", \"running\": \"lime\", '",
          "59:             '\"scheduled\": \"tan\", '",
          "61:             '\"success\": \"green\", \"up_for_reschedule\": \"turquoise\", '",
          "62:             '\"up_for_retry\": \"gold\", \"upstream_failed\": \"orange\"};'",
          "63:         )",
          "",
          "[Removed Lines]",
          "60:             '\"shutdown\": \"blue\", \"skipped\": \"hotpink\", '",
          "",
          "[Added Lines]",
          "60:             '\"skipped\": \"hotpink\", '",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "822135f86612d44ebc0515dd4d1b79e40d685d63",
      "candidate_info": {
        "commit_hash": "822135f86612d44ebc0515dd4d1b79e40d685d63",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/822135f86612d44ebc0515dd4d1b79e40d685d63",
        "files": [
          "airflow/settings.py",
          "docs/apache-airflow/howto/set-up-database.rst"
        ],
        "message": "Change links to SQLAlchemy 1.4 (#34288)\n\n(cherry picked from commit ed552e294596b27288fb040abd1634cb77e419c9)",
        "before_after_code_files": [
          "airflow/settings.py||airflow/settings.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/settings.py||airflow/settings.py": [
          "File: airflow/settings.py -> airflow/settings.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "309:         # Typically, this is a simple statement like \"SELECT 1\", but may also make use",
          "310:         # of some DBAPI-specific method to test the connection for liveness.",
          "311:         # More information here:",
          "313:         pool_pre_ping = conf.getboolean(\"database\", \"SQL_ALCHEMY_POOL_PRE_PING\", fallback=True)",
          "315:         log.debug(",
          "",
          "[Removed Lines]",
          "312:         # https://docs.sqlalchemy.org/en/13/core/pooling.html#disconnect-handling-pessimistic",
          "",
          "[Added Lines]",
          "312:         # https://docs.sqlalchemy.org/en/14/core/pooling.html#disconnect-handling-pessimistic",
          "",
          "---------------"
        ]
      }
    }
  ]
}