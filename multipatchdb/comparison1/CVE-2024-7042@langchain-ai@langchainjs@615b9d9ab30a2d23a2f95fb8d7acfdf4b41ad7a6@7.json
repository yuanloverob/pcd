{
  "cve_id": "CVE-2024-7042",
  "cve_desc": "A vulnerability in the GraphCypherQAChain class of langchain-ai/langchainjs versions 0.2.5 and all versions with this class allows for prompt injection, leading to SQL injection. This vulnerability permits unauthorized data manipulation, data exfiltration, denial of service (DoS) by deleting all data, breaches in multi-tenant security environments, and data integrity issues. Attackers can create, update, or delete nodes and relationships without proper authorization, extract sensitive data, disrupt services, access data across different tenants, and compromise the integrity of the database.",
  "repo": "langchain-ai/langchainjs",
  "patch_hash": "615b9d9ab30a2d23a2f95fb8d7acfdf4b41ad7a6",
  "patch_info": {
    "commit_hash": "615b9d9ab30a2d23a2f95fb8d7acfdf4b41ad7a6",
    "repo": "langchain-ai/langchainjs",
    "commit_url": "https://github.com/langchain-ai/langchainjs/commit/615b9d9ab30a2d23a2f95fb8d7acfdf4b41ad7a6",
    "files": [
      "examples/package.json",
      "examples/src/indexes/vector_stores/lancedb/fromDocs.ts",
      "examples/src/indexes/vector_stores/lancedb/fromTexts.ts",
      "libs/langchain-community/package.json",
      "libs/langchain-community/src/vectorstores/lancedb.ts",
      "libs/langchain-community/src/vectorstores/tests/lancedb.int.test.ts",
      "yarn.lock"
    ],
    "message": "feat(community): Remove required param from LanceDB integration (#6706)\n\nCo-authored-by: jacoblee93 <jacoblee93@gmail.com>",
    "before_after_code_files": [
      "examples/src/indexes/vector_stores/lancedb/fromDocs.ts||examples/src/indexes/vector_stores/lancedb/fromDocs.ts",
      "examples/src/indexes/vector_stores/lancedb/fromTexts.ts||examples/src/indexes/vector_stores/lancedb/fromTexts.ts",
      "libs/langchain-community/src/vectorstores/lancedb.ts||libs/langchain-community/src/vectorstores/lancedb.ts",
      "libs/langchain-community/src/vectorstores/tests/lancedb.int.test.ts||libs/langchain-community/src/vectorstores/tests/lancedb.int.test.ts",
      "yarn.lock||yarn.lock"
    ]
  },
  "patch_diff": {
    "examples/src/indexes/vector_stores/lancedb/fromDocs.ts||examples/src/indexes/vector_stores/lancedb/fromDocs.ts": [
      "File: examples/src/indexes/vector_stores/lancedb/fromDocs.ts -> examples/src/indexes/vector_stores/lancedb/fromDocs.ts",
      "--- Hunk 1 ---",
      "[Context before]",
      "4: import fs from \"node:fs/promises\";",
      "5: import path from \"node:path\";",
      "6: import os from \"node:os\";",
      "10: const loader = new TextLoader(\"src/document_loaders/example_data/example.txt\");",
      "11: const docs = await loader.load();",
      "13: export const run = async () => {",
      "14:   const dir = await fs.mkdtemp(path.join(os.tmpdir(), \"lancedb-\"));",
      "26:   const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);",
      "27:   console.log(resultOne);",
      "",
      "[Removed Lines]",
      "7: import { connect } from \"vectordb\";",
      "15:   const db = await connect(dir);",
      "16:   const table = await db.createTable(\"vectors\", [",
      "17:     { vector: Array(1536), text: \"sample\", source: \"a\" },",
      "18:   ]);",
      "20:   const vectorStore = await LanceDB.fromDocuments(",
      "21:     docs,",
      "22:     new OpenAIEmbeddings(),",
      "23:     { table }",
      "24:   );",
      "",
      "[Added Lines]",
      "13:   const vectorStore = await LanceDB.fromDocuments(docs, new OpenAIEmbeddings());",
      "15:   const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);",
      "16:   console.log(resultOne);",
      "24: };",
      "26: export const run_with_existing_table = async () => {",
      "29:   const vectorStore = await LanceDB.fromDocuments(docs, new OpenAIEmbeddings());",
      "",
      "---------------"
    ],
    "examples/src/indexes/vector_stores/lancedb/fromTexts.ts||examples/src/indexes/vector_stores/lancedb/fromTexts.ts": [
      "File: examples/src/indexes/vector_stores/lancedb/fromTexts.ts -> examples/src/indexes/vector_stores/lancedb/fromTexts.ts",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: import { LanceDB } from \"@langchain/community/vectorstores/lancedb\";",
      "2: import { OpenAIEmbeddings } from \"@langchain/openai\";",
      "4: import * as fs from \"node:fs/promises\";",
      "5: import * as path from \"node:path\";",
      "6: import os from \"node:os\";",
      "8: export const run = async () => {",
      "15:   const vectorStore = await LanceDB.fromTexts(",
      "16:     [\"Hello world\", \"Bye bye\", \"hello nice world\"],",
      "17:     [{ id: 2 }, { id: 1 }, { id: 3 }],",
      "20:   );",
      "22:   const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);",
      "",
      "[Removed Lines]",
      "3: import { connect } from \"vectordb\";",
      "9:   const dir = await fs.mkdtemp(path.join(os.tmpdir(), \"lancedb-\"));",
      "10:   const db = await connect(dir);",
      "11:   const table = await db.createTable(\"vectors\", [",
      "12:     { vector: Array(1536), text: \"sample\", id: 1 },",
      "13:   ]);",
      "18:     new OpenAIEmbeddings(),",
      "19:     { table }",
      "",
      "[Added Lines]",
      "8:   const vectorStore = await LanceDB.fromTexts(",
      "9:     [\"Hello world\", \"Bye bye\", \"hello nice world\"],",
      "10:     [{ id: 2 }, { id: 1 }, { id: 3 }],",
      "11:     new OpenAIEmbeddings()",
      "12:   );",
      "14:   const resultOne = await vectorStore.similaritySearch(\"hello world\", 1);",
      "15:   console.log(resultOne);",
      "17: };",
      "19: export const run_with_existing_table = async () => {",
      "20:   const dir = await fs.mkdtemp(path.join(os.tmpdir(), \"lancedb-\"));",
      "24:     new OpenAIEmbeddings()",
      "",
      "---------------"
    ],
    "libs/langchain-community/src/vectorstores/lancedb.ts||libs/langchain-community/src/vectorstores/lancedb.ts": [
      "File: libs/langchain-community/src/vectorstores/lancedb.ts -> libs/langchain-community/src/vectorstores/lancedb.ts",
      "--- Hunk 1 ---",
      "[Context before]",
      "2: import type { EmbeddingsInterface } from \"@langchain/core/embeddings\";",
      "3: import { VectorStore } from \"@langchain/core/vectorstores\";",
      "4: import { Document } from \"@langchain/core/documents\";",
      "",
      "[Removed Lines]",
      "1: import { Table } from \"vectordb\";",
      "",
      "[Added Lines]",
      "1: import { connect, Table, Connection, WriteMode } from \"vectordb\";",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "10: export type LanceDBArgs = {",
      "12:   textKey?: string;",
      "13: };",
      "",
      "[Removed Lines]",
      "11:   table: Table;",
      "",
      "[Added Lines]",
      "11:   table?: Table;",
      "13:   uri?: string;",
      "14:   tableName?: string;",
      "15:   mode?: WriteMode;",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "20: export class LanceDB extends VectorStore {",
      "23:   private textKey: string;",
      "28:     this.embeddings = embeddings;",
      "30:   }",
      "",
      "[Removed Lines]",
      "21:   private table: Table;",
      "25:   constructor(embeddings: EmbeddingsInterface, args: LanceDBArgs) {",
      "26:     super(embeddings, args);",
      "27:     this.table = args.table;",
      "29:     this.textKey = args.textKey || \"text\";",
      "",
      "[Added Lines]",
      "24:   private table?: Table;",
      "28:   private uri: string;",
      "30:   private tableName: string;",
      "32:   private mode?: WriteMode;",
      "34:   constructor(embeddings: EmbeddingsInterface, args?: LanceDBArgs) {",
      "35:     super(embeddings, args || {});",
      "36:     this.table = args?.table;",
      "38:     this.textKey = args?.textKey || \"text\";",
      "39:     this.uri = args?.uri || \"~/lancedb\";",
      "40:     this.tableName = args?.tableName || \"langchain\";",
      "41:     this.mode = args?.mode || WriteMode.Overwrite;",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "71:       });",
      "72:       data.push(record);",
      "73:     }",
      "74:     await this.table.add(data);",
      "75:   }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "86:     if (!this.table) {",
      "87:       const db: Connection = await connect(this.uri);",
      "88:       this.table = await db.createTable(this.tableName, data, {",
      "89:         writeMode: this.mode,",
      "90:       });",
      "92:       return;",
      "93:     }",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "85:     query: number[],",
      "86:     k: number",
      "87:   ): Promise<[Document, number][]> {",
      "88:     const results = await this.table.search(query).limit(k).execute();",
      "90:     const docsAndScore: [Document, number][] = [];",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "108:     if (!this.table) {",
      "109:       throw new Error(",
      "110:         \"Table not found. Please add vectors to the table first.\"",
      "111:       );",
      "112:     }",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "119:     texts: string[],",
      "120:     metadatas: object[] | object,",
      "121:     embeddings: EmbeddingsInterface,",
      "123:   ): Promise<LanceDB> {",
      "124:     const docs: Document[] = [];",
      "125:     for (let i = 0; i < texts.length; i += 1) {",
      "",
      "[Removed Lines]",
      "122:     dbConfig: LanceDBArgs",
      "",
      "[Added Lines]",
      "147:     dbConfig?: LanceDBArgs",
      "",
      "---------------",
      "--- Hunk 7 ---",
      "[Context before]",
      "143:   static async fromDocuments(",
      "144:     docs: Document[],",
      "145:     embeddings: EmbeddingsInterface,",
      "147:   ): Promise<LanceDB> {",
      "148:     const instance = new this(embeddings, dbConfig);",
      "149:     await instance.addDocuments(docs);",
      "",
      "[Removed Lines]",
      "146:     dbConfig: LanceDBArgs",
      "",
      "[Added Lines]",
      "171:     dbConfig?: LanceDBArgs",
      "",
      "---------------"
    ],
    "libs/langchain-community/src/vectorstores/tests/lancedb.int.test.ts||libs/langchain-community/src/vectorstores/tests/lancedb.int.test.ts": [
      "File: libs/langchain-community/src/vectorstores/tests/lancedb.int.test.ts -> libs/langchain-community/src/vectorstores/tests/lancedb.int.test.ts",
      "--- Hunk 1 ---",
      "[Context before]",
      "45:     expect(resultsTwo.length).toBe(5);",
      "46:   });",
      "47: });",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "49: describe(\"LanceDB empty schema\", () => {",
      "50:   test(\"Test fromTexts + addDocuments\", async () => {",
      "51:     const embeddings = new OpenAIEmbeddings();",
      "52:     const vectorStore = await LanceDB.fromTexts(",
      "53:       [\"hello bye\", \"hello world\", \"bye bye\"],",
      "54:       [{ id: 1 }, { id: 2 }, { id: 3 }],",
      "55:       embeddings",
      "56:     );",
      "58:     const results = await vectorStore.similaritySearch(\"hello bye\", 10);",
      "59:     expect(results.length).toBe(3);",
      "61:     await vectorStore.addDocuments([",
      "62:       new Document({",
      "63:         pageContent: \"a new world\",",
      "64:         metadata: { id: 4 },",
      "65:       }),",
      "66:     ]);",
      "68:     const resultsTwo = await vectorStore.similaritySearch(\"hello bye\", 10);",
      "69:     expect(resultsTwo.length).toBe(4);",
      "70:   });",
      "71: });",
      "",
      "---------------"
    ],
    "yarn.lock||yarn.lock": [
      "File: yarn.lock -> yarn.lock",
      "--- Hunk 1 ---",
      "[Context before]",
      "251:   languageName: node",
      "252:   linkType: hard",
      "272: \"@apify/consts@npm:^2.13.0, @apify/consts@npm:^2.9.0\":",
      "273:   version: 2.13.0",
      "274:   resolution: \"@apify/consts@npm:2.13.0\"",
      "",
      "[Removed Lines]",
      "254: \"@apache-arrow/ts@npm:^12.0.0\":",
      "255:   version: 12.0.0",
      "256:   resolution: \"@apache-arrow/ts@npm:12.0.0\"",
      "257:   dependencies:",
      "258:     \"@types/command-line-args\": 5.2.0",
      "259:     \"@types/command-line-usage\": 5.0.2",
      "260:     \"@types/node\": 18.14.5",
      "261:     \"@types/pad-left\": 2.1.1",
      "262:     command-line-args: 5.2.1",
      "263:     command-line-usage: 6.1.3",
      "264:     flatbuffers: 23.3.3",
      "265:     json-bignum: ^0.0.3",
      "266:     pad-left: ^2.1.0",
      "267:     tslib: ^2.5.0",
      "268:   checksum: 67b2791e14d5377b1d160a0d8390decc386e013c517713f8b9c100737a0e478a394086d91a8c846848d4e30289070a119d8e65191998f4c2555b18a29564df50",
      "269:   languageName: node",
      "270:   linkType: hard",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "11112:   languageName: node",
      "11113:   linkType: hard",
      "11115: \"@langchain/anthropic@*, @langchain/anthropic@workspace:*, @langchain/anthropic@workspace:libs/langchain-anthropic\":",
      "11116:   version: 0.0.0-use.local",
      "11117:   resolution: \"@langchain/anthropic@workspace:libs/langchain-anthropic\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "11097: \"@lancedb/vectordb-darwin-arm64@npm:0.4.20\":",
      "11098:   version: 0.4.20",
      "11099:   resolution: \"@lancedb/vectordb-darwin-arm64@npm:0.4.20\"",
      "11100:   conditions: os=darwin & cpu=arm64",
      "11101:   languageName: node",
      "11102:   linkType: hard",
      "11104: \"@lancedb/vectordb-darwin-x64@npm:0.4.20\":",
      "11105:   version: 0.4.20",
      "11106:   resolution: \"@lancedb/vectordb-darwin-x64@npm:0.4.20\"",
      "11107:   conditions: os=darwin & cpu=x64",
      "11108:   languageName: node",
      "11109:   linkType: hard",
      "11111: \"@lancedb/vectordb-linux-arm64-gnu@npm:0.4.20\":",
      "11112:   version: 0.4.20",
      "11113:   resolution: \"@lancedb/vectordb-linux-arm64-gnu@npm:0.4.20\"",
      "11114:   conditions: os=linux & cpu=arm64",
      "11115:   languageName: node",
      "11116:   linkType: hard",
      "11118: \"@lancedb/vectordb-linux-x64-gnu@npm:0.4.20\":",
      "11119:   version: 0.4.20",
      "11120:   resolution: \"@lancedb/vectordb-linux-x64-gnu@npm:0.4.20\"",
      "11121:   conditions: os=linux & cpu=x64",
      "11122:   languageName: node",
      "11123:   linkType: hard",
      "11125: \"@lancedb/vectordb-win32-x64-msvc@npm:0.4.20\":",
      "11126:   version: 0.4.20",
      "11127:   resolution: \"@lancedb/vectordb-win32-x64-msvc@npm:0.4.20\"",
      "11128:   conditions: os=win32 & cpu=x64",
      "11129:   languageName: node",
      "11130:   linkType: hard",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "11577:     typesense: ^1.5.3",
      "11578:     usearch: ^1.1.1",
      "11579:     uuid: ^10.0.0",
      "11581:     voy-search: 0.6.2",
      "11582:     weaviate-ts-client: ^1.4.0",
      "11583:     web-auth-library: ^1.0.3",
      "",
      "[Removed Lines]",
      "11580:     vectordb: ^0.1.4",
      "",
      "[Added Lines]",
      "11597:     vectordb: ^0.9.0",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "13039:   languageName: node",
      "13040:   linkType: hard",
      "13042: \"@neondatabase/serverless@npm:0.6.0\":",
      "13043:   version: 0.6.0",
      "13044:   resolution: \"@neondatabase/serverless@npm:0.6.0\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "13059: \"@neon-rs/load@npm:^0.0.74\":",
      "13060:   version: 0.0.74",
      "13061:   resolution: \"@neon-rs/load@npm:0.0.74\"",
      "13062:   checksum: d26ec9b08cdf1a7c5aeefe98f77112d205d11b4005a7934b21fe8fd27528847e08e4749e7e6c3fc05ae9f701175a58c11a095ae6af449634df3991a2c82e1dfa",
      "13063:   languageName: node",
      "13064:   linkType: hard",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "20774:   languageName: node",
      "20775:   linkType: hard",
      "20797: \"apache-arrow@npm:^12.0.1\":",
      "20798:   version: 12.0.1",
      "20799:   resolution: \"apache-arrow@npm:12.0.1\"",
      "",
      "[Removed Lines]",
      "20777: \"apache-arrow@npm:^12.0.0\":",
      "20778:   version: 12.0.0",
      "20779:   resolution: \"apache-arrow@npm:12.0.0\"",
      "20780:   dependencies:",
      "20781:     \"@types/command-line-args\": 5.2.0",
      "20782:     \"@types/command-line-usage\": 5.0.2",
      "20783:     \"@types/node\": 18.14.5",
      "20784:     \"@types/pad-left\": 2.1.1",
      "20785:     command-line-args: 5.2.1",
      "20786:     command-line-usage: 6.1.3",
      "20787:     flatbuffers: 23.3.3",
      "20788:     json-bignum: ^0.0.3",
      "20789:     pad-left: ^2.1.0",
      "20790:     tslib: ^2.5.0",
      "20791:   bin:",
      "20792:     arrow2csv: bin/arrow2csv.js",
      "20793:   checksum: 3285189517c2b298cda42852321ce127754918513116eade6e4914c57983f68b6ba96605cfaa2202796d3d6e14755d3b3758f76c1374492affa3d95714eaca40",
      "20794:   languageName: node",
      "20795:   linkType: hard",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "27133:     typescript: ~5.1.6",
      "27134:     typesense: ^1.5.3",
      "27135:     uuid: ^10.0.0",
      "27137:     voy-search: 0.6.2",
      "27138:     weaviate-ts-client: ^2.0.0",
      "27139:     zod: ^3.22.4",
      "",
      "[Removed Lines]",
      "27136:     vectordb: ^0.1.4",
      "",
      "[Added Lines]",
      "27140:     vectordb: ^0.9.0",
      "",
      "---------------",
      "--- Hunk 7 ---",
      "[Context before]",
      "42444:   languageName: node",
      "42445:   linkType: hard",
      "42454:   languageName: node",
      "42455:   linkType: hard",
      "",
      "[Removed Lines]",
      "42447: \"vectordb@npm:^0.1.4\":",
      "42448:   version: 0.1.4",
      "42449:   resolution: \"vectordb@npm:0.1.4\"",
      "42450:   dependencies:",
      "42451:     \"@apache-arrow/ts\": ^12.0.0",
      "42452:     apache-arrow: ^12.0.0",
      "42453:   checksum: 8a40abf4466479b0b9e61687416b5ab232458401917bf9a1d5f3d8ea8c8320ecc5691174f4d4c0cfef0bb6c16328a9088419fd90ac85fd7267dbccdd1f9e55d7",
      "",
      "[Added Lines]",
      "42451: \"vectordb@npm:^0.9.0\":",
      "42452:   version: 0.9.0",
      "42453:   resolution: \"vectordb@npm:0.9.0\"",
      "42454:   dependencies:",
      "42455:     \"@lancedb/vectordb-darwin-arm64\": 0.4.20",
      "42456:     \"@lancedb/vectordb-darwin-x64\": 0.4.20",
      "42457:     \"@lancedb/vectordb-linux-arm64-gnu\": 0.4.20",
      "42458:     \"@lancedb/vectordb-linux-x64-gnu\": 0.4.20",
      "42459:     \"@lancedb/vectordb-win32-x64-msvc\": 0.4.20",
      "42460:     \"@neon-rs/load\": ^0.0.74",
      "42461:     axios: ^1.4.0",
      "42462:   peerDependencies:",
      "42463:     \"@apache-arrow/ts\": ^14.0.2",
      "42464:     apache-arrow: ^14.0.2",
      "42465:   dependenciesMeta:",
      "42466:     \"@lancedb/vectordb-darwin-arm64\":",
      "42467:       optional: true",
      "42468:     \"@lancedb/vectordb-darwin-x64\":",
      "42469:       optional: true",
      "42470:     \"@lancedb/vectordb-linux-arm64-gnu\":",
      "42471:       optional: true",
      "42472:     \"@lancedb/vectordb-linux-x64-gnu\":",
      "42473:       optional: true",
      "42474:     \"@lancedb/vectordb-win32-x64-msvc\":",
      "42475:       optional: true",
      "42476:   conditions: (os=darwin | os=linux | os=win32) & (cpu=x64 | cpu=arm64)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "b23506236182bbba12d8d322e7417cbf4fd899b3",
      "candidate_info": {
        "commit_hash": "b23506236182bbba12d8d322e7417cbf4fd899b3",
        "repo": "langchain-ai/langchainjs",
        "commit_url": "https://github.com/langchain-ai/langchainjs/commit/b23506236182bbba12d8d322e7417cbf4fd899b3",
        "files": [
          "examples/package.json",
          "langchain-core/package.json",
          "langchain/package.json",
          "libs/langchain-community/package.json",
          "yarn.lock"
        ],
        "message": "chore(core,langchain,community): Relax langsmith deps (#7556)",
        "before_after_code_files": [
          "yarn.lock||yarn.lock"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "yarn.lock||yarn.lock"
          ],
          "candidate": [
            "yarn.lock||yarn.lock"
          ]
        }
      },
      "candidate_diff": {
        "yarn.lock||yarn.lock": [
          "File: yarn.lock -> yarn.lock",
          "--- Hunk 1 ---",
          "[Context before]",
          "12010:     jsdom: ^22.1.0",
          "12011:     jsonwebtoken: ^9.0.2",
          "12012:     langchain: \">=0.2.3 <0.3.0 || >=0.3.4 <0.4.0\"",
          "12014:     llmonitor: ^0.5.9",
          "12015:     lodash: ^4.17.21",
          "12016:     lunary: ^0.7.10",
          "",
          "[Removed Lines]",
          "12013:     langsmith: ^0.2.8",
          "",
          "[Added Lines]",
          "12013:     langsmith: \">=0.2.8 <0.4.0\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "12448:     jest: ^29.5.0",
          "12449:     jest-environment-node: ^29.6.4",
          "12450:     js-tiktoken: ^1.0.12",
          "12452:     ml-matrix: ^6.10.4",
          "12453:     mustache: ^4.2.0",
          "12454:     p-queue: ^6.6.2",
          "",
          "[Removed Lines]",
          "12451:     langsmith: ^0.2.8",
          "",
          "[Added Lines]",
          "12451:     langsmith: \">=0.2.8 <0.4.0\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "27852:     ioredis: ^5.3.2",
          "27853:     js-yaml: ^4.1.0",
          "27854:     langchain: \"workspace:*\"",
          "27856:     mongodb: ^6.3.0",
          "27857:     pg: ^8.11.0",
          "27858:     pickleparser: ^0.2.1",
          "",
          "[Removed Lines]",
          "27855:     langsmith: ^0.2.8",
          "",
          "[Added Lines]",
          "27855:     langsmith: \">=0.2.8 <0.4.0\"",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "33464:     js-tiktoken: ^1.0.12",
          "33465:     js-yaml: ^4.1.0",
          "33466:     jsonpointer: ^5.0.1",
          "33468:     openai: ^4.41.1",
          "33469:     openapi-types: ^12.1.3",
          "33470:     p-retry: 4",
          "",
          "[Removed Lines]",
          "33467:     langsmith: ^0.2.8",
          "",
          "[Added Lines]",
          "33467:     langsmith: \">=0.2.8 <0.4.0\"",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "33549:   languageName: unknown",
          "33550:   linkType: soft",
          "33555:   dependencies:",
          "33556:     \"@types/uuid\": ^10.0.0",
          "33557:     commander: ^10.0.1",
          "",
          "[Removed Lines]",
          "33552: \"langsmith@npm:^0.2.8\":",
          "33553:   version: 0.2.8",
          "33554:   resolution: \"langsmith@npm:0.2.8\"",
          "",
          "[Added Lines]",
          "33552: \"langsmith@npm:>=0.2.8 <0.4.0\":",
          "33553:   version: 0.2.15",
          "33554:   resolution: \"langsmith@npm:0.2.15\"",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "33564:   peerDependenciesMeta:",
          "33565:     openai:",
          "33566:       optional: true",
          "33568:   languageName: node",
          "33569:   linkType: hard",
          "",
          "[Removed Lines]",
          "33567:   checksum: 8695df08a09b9885b0308c66fbf9802edbe20e286fec3db8faa75ed1893a7aafae014441e311677bb60abb33af49da7f7d8404f55fffbdad5aec61cf65215fc8",
          "",
          "[Added Lines]",
          "33567:   checksum: 72dbc86c72e186b3228ec754abc1c98fd17e62aee337375c228ba83da6dd6aaa0eea0acb0ca2c754e6c33f69ab0e11d39a122a7e51b6b33100ce95395aa57e9a",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ded94d5bc77d53a78865d3a7645dbdfeac2735da",
      "candidate_info": {
        "commit_hash": "ded94d5bc77d53a78865d3a7645dbdfeac2735da",
        "repo": "langchain-ai/langchainjs",
        "commit_url": "https://github.com/langchain-ai/langchainjs/commit/ded94d5bc77d53a78865d3a7645dbdfeac2735da",
        "files": [
          "docs/core_docs/docs/integrations/llms/replicate.mdx",
          "libs/langchain-community/package.json",
          "yarn.lock"
        ],
        "message": "fix(community): Relax Replicate peer dep (#7656)",
        "before_after_code_files": [
          "yarn.lock||yarn.lock"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "yarn.lock||yarn.lock"
          ],
          "candidate": [
            "yarn.lock||yarn.lock"
          ]
        }
      },
      "candidate_diff": {
        "yarn.lock||yarn.lock": [
          "File: yarn.lock -> yarn.lock",
          "--- Hunk 1 ---",
          "[Context before]",
          "12034:     pyodide: ^0.26.2",
          "12035:     redis: ^4.6.6",
          "12036:     release-it: ^17.6.0",
          "12038:     rollup: ^3.19.1",
          "12039:     sonix-speech-recognition: ^2.1.1",
          "12040:     srt-parser-2: ^1.2.3",
          "",
          "[Removed Lines]",
          "12037:     replicate: ^0.29.4",
          "",
          "[Added Lines]",
          "12037:     replicate: ^1.0.1",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "12166:     puppeteer: \"*\"",
          "12167:     pyodide: \">=0.24.1 <0.27.0\"",
          "12168:     redis: \"*\"",
          "12170:     sonix-speech-recognition: ^2.1.1",
          "12171:     srt-parser-2: ^1.2.3",
          "12172:     typeorm: ^0.3.20",
          "",
          "[Removed Lines]",
          "12169:     replicate: ^0.29.4",
          "",
          "[Added Lines]",
          "12169:     replicate: \"*\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "39506:   languageName: node",
          "39507:   linkType: hard",
          "39512:   dependencies:",
          "39513:     readable-stream: \">=4.0.0\"",
          "39514:   dependenciesMeta:",
          "39515:     readable-stream:",
          "39516:       optional: true",
          "39518:   languageName: node",
          "39519:   linkType: hard",
          "",
          "[Removed Lines]",
          "39509: \"replicate@npm:^0.29.4\":",
          "39510:   version: 0.29.4",
          "39511:   resolution: \"replicate@npm:0.29.4\"",
          "39517:   checksum: 9405e19f619134a312aa77b3c04156549e4c8ba5e0711a494b99358abd0378646c22cd9bf07e6f9c8ab4a2f80b69ba22ed0a5b8ec0610684e9fa5d413e3b5729",
          "",
          "[Added Lines]",
          "39509: \"replicate@npm:^1.0.1\":",
          "39510:   version: 1.0.1",
          "39511:   resolution: \"replicate@npm:1.0.1\"",
          "39517:   checksum: b41d6663f229a7f49a4026ef5690716806c337c303946f7ed42f56d90dfdfde699a59a4ff2a5042f7f1a8184794791703c328fc115ef6d9560721c0b052471a1",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4e999b14617d6ec1f93f5f82666d42ccca7769ec",
      "candidate_info": {
        "commit_hash": "4e999b14617d6ec1f93f5f82666d42ccca7769ec",
        "repo": "langchain-ai/langchainjs",
        "commit_url": "https://github.com/langchain-ai/langchainjs/commit/4e999b14617d6ec1f93f5f82666d42ccca7769ec",
        "files": [
          "libs/langchain-community/package.json",
          "yarn.lock"
        ],
        "message": "chore(community): Bump faker dev dep to 8.4.1 (#7290)",
        "before_after_code_files": [
          "yarn.lock||yarn.lock"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "yarn.lock||yarn.lock"
          ],
          "candidate": [
            "yarn.lock||yarn.lock"
          ]
        }
      },
      "candidate_diff": {
        "yarn.lock||yarn.lock": [
          "File: yarn.lock -> yarn.lock",
          "--- Hunk 1 ---",
          "[Context before]",
          "10048:   languageName: node",
          "10049:   linkType: hard",
          "10051: \"@faker-js/faker@npm:^7.6.0\":",
          "10052:   version: 7.6.0",
          "10053:   resolution: \"@faker-js/faker@npm:7.6.0\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "10051: \"@faker-js/faker@npm:8.4.1, @faker-js/faker@npm:^8.4.1\":",
          "10052:   version: 8.4.1",
          "10053:   resolution: \"@faker-js/faker@npm:8.4.1\"",
          "10054:   checksum: d802d531f8929562715adc279cfec763c9a4bc596ec67b0ce43fd0ae61b285d2b0eec6f1f4aa852452a63721a842fe7e81926dce7bd92acca94b01e2a1f55f5a",
          "10055:   languageName: node",
          "10056:   linkType: hard",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "10069:   languageName: node",
          "10070:   linkType: hard",
          "10079: \"@fastify/busboy@npm:^1.2.1\":",
          "10080:   version: 1.2.1",
          "10081:   resolution: \"@fastify/busboy@npm:1.2.1\"",
          "",
          "[Removed Lines]",
          "10072: \"@faker-js/faker@npm:^8.4.1\":",
          "10073:   version: 8.4.1",
          "10074:   resolution: \"@faker-js/faker@npm:8.4.1\"",
          "10075:   checksum: d802d531f8929562715adc279cfec763c9a4bc596ec67b0ce43fd0ae61b285d2b0eec6f1f4aa852452a63721a842fe7e81926dce7bd92acca94b01e2a1f55f5a",
          "10076:   languageName: node",
          "10077:   linkType: hard",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "11709:     \"@cloudflare/workers-types\": ^4.20230922.0",
          "11710:     \"@datastax/astra-db-ts\": ^1.0.1",
          "11711:     \"@elastic/elasticsearch\": ^8.4.0",
          "11713:     \"@getmetal/metal-sdk\": ^4.0.0",
          "11714:     \"@getzep/zep-cloud\": ^1.0.6",
          "11715:     \"@getzep/zep-js\": ^0.9.0",
          "",
          "[Removed Lines]",
          "11712:     \"@faker-js/faker\": ^7.6.0",
          "",
          "[Added Lines]",
          "11712:     \"@faker-js/faker\": 8.4.1",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "78a69951d4626892fed2c035693f10e069c4f989",
      "candidate_info": {
        "commit_hash": "78a69951d4626892fed2c035693f10e069c4f989",
        "repo": "langchain-ai/langchainjs",
        "commit_url": "https://github.com/langchain-ai/langchainjs/commit/78a69951d4626892fed2c035693f10e069c4f989",
        "files": [
          "docs/core_docs/docs/integrations/chat/mistral.ipynb",
          "docs/core_docs/docs/integrations/llms/mistral.ipynb",
          "docs/core_docs/docs/integrations/text_embedding/mistralai.ipynb",
          "libs/langchain-mistralai/package.json",
          "libs/langchain-mistralai/src/chat_models.ts",
          "libs/langchain-mistralai/src/embeddings.ts",
          "libs/langchain-mistralai/src/llms.ts",
          "libs/langchain-mistralai/src/tests/chat_models.int.test.ts",
          "libs/langchain-mistralai/src/tests/chat_models.standard.int.test.ts",
          "libs/langchain-mistralai/src/tests/chat_models.test.ts",
          "libs/langchain-mistralai/src/tests/embeddings.int.test.ts",
          "libs/langchain-mistralai/src/tests/llms.int.test.ts",
          "libs/langchain-mistralai/src/utils.ts",
          "yarn.lock"
        ],
        "message": "feat(mistral): Mistral 1.3.1 migration (#7218)\n\nCo-authored-by: Ashtian <ashtian.delacruz@mail.utoronto.ca>\nCo-authored-by: BaNg-W <bang344251871@gmail.com>\nCo-authored-by: CarterMorris <CarterMorris@users.noreply.github.com>\nCo-authored-by: BaNg-W <BaNg-W@users.noreply.github.com>\nCo-authored-by: BaNg-W <114012080+BaNg-W@users.noreply.github.com>\nCo-authored-by: jacoblee93 <jacoblee93@gmail.com>",
        "before_after_code_files": [
          "libs/langchain-mistralai/src/chat_models.ts||libs/langchain-mistralai/src/chat_models.ts",
          "libs/langchain-mistralai/src/embeddings.ts||libs/langchain-mistralai/src/embeddings.ts",
          "libs/langchain-mistralai/src/llms.ts||libs/langchain-mistralai/src/llms.ts",
          "libs/langchain-mistralai/src/tests/chat_models.int.test.ts||libs/langchain-mistralai/src/tests/chat_models.int.test.ts",
          "libs/langchain-mistralai/src/tests/chat_models.standard.int.test.ts||libs/langchain-mistralai/src/tests/chat_models.standard.int.test.ts",
          "libs/langchain-mistralai/src/tests/chat_models.test.ts||libs/langchain-mistralai/src/tests/chat_models.test.ts",
          "libs/langchain-mistralai/src/tests/embeddings.int.test.ts||libs/langchain-mistralai/src/tests/embeddings.int.test.ts",
          "libs/langchain-mistralai/src/tests/llms.int.test.ts||libs/langchain-mistralai/src/tests/llms.int.test.ts",
          "libs/langchain-mistralai/src/utils.ts||libs/langchain-mistralai/src/utils.ts",
          "yarn.lock||yarn.lock"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "yarn.lock||yarn.lock"
          ],
          "candidate": [
            "yarn.lock||yarn.lock"
          ]
        }
      },
      "candidate_diff": {
        "libs/langchain-mistralai/src/chat_models.ts||libs/langchain-mistralai/src/chat_models.ts": [
          "File: libs/langchain-mistralai/src/chat_models.ts -> libs/langchain-mistralai/src/chat_models.ts",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: import { v4 as uuidv4 } from \"uuid\";",
          "2: import {",
          "13: import {",
          "14:   MessageType,",
          "16:   MessageContent,",
          "17:   AIMessage,",
          "18:   HumanMessage,",
          "19:   HumanMessageChunk,",
          "",
          "[Removed Lines]",
          "3:   ChatCompletionResponse,",
          "4:   Function as MistralAIFunction,",
          "5:   ToolCalls as MistralAIToolCalls,",
          "6:   ResponseFormat,",
          "7:   ChatCompletionResponseChunk,",
          "8:   ChatRequest,",
          "9:   Tool as MistralAITool,",
          "10:   Message as MistralAIMessage,",
          "11:   TokenUsage as MistralAITokenUsage,",
          "12: } from \"@mistralai/mistralai\";",
          "15:   type BaseMessage,",
          "",
          "[Added Lines]",
          "2: import { Mistral as MistralClient } from \"@mistralai/mistralai\";",
          "4:   ChatCompletionRequest as MistralAIChatCompletionRequest,",
          "5:   ChatCompletionRequestToolChoice as MistralAIToolChoice,",
          "6:   Messages as MistralAIMessage,",
          "7: } from \"@mistralai/mistralai/models/components/chatcompletionrequest.js\";",
          "8: import { ContentChunk as MistralAIContentChunk } from \"@mistralai/mistralai/models/components/contentchunk.js\";",
          "9: import { Tool as MistralAITool } from \"@mistralai/mistralai/models/components/tool.js\";",
          "10: import { ToolCall as MistralAIToolCall } from \"@mistralai/mistralai/models/components/toolcall.js\";",
          "11: import { ChatCompletionStreamRequest as MistralAIChatCompletionStreamRequest } from \"@mistralai/mistralai/models/components/chatcompletionstreamrequest.js\";",
          "12: import { UsageInfo as MistralAITokenUsage } from \"@mistralai/mistralai/models/components/usageinfo.js\";",
          "13: import { CompletionEvent as MistralAIChatCompletionEvent } from \"@mistralai/mistralai/models/components/completionevent.js\";",
          "14: import { ChatCompletionResponse as MistralAIChatCompletionResponse } from \"@mistralai/mistralai/models/components/chatcompletionresponse.js\";",
          "16:   type BeforeRequestHook,",
          "17:   type RequestErrorHook,",
          "18:   type ResponseHook,",
          "19:   HTTPClient as MistralAIHTTPClient,",
          "20: } from \"@mistralai/mistralai/lib/http.js\";",
          "21: import {",
          "22:   BaseMessage,",
          "25:   MessageContentComplex,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "21:   ToolMessageChunk,",
          "22:   ChatMessageChunk,",
          "23:   FunctionMessageChunk,",
          "25:   isAIMessage,",
          "26: } from \"@langchain/core/messages\";",
          "27: import type {",
          "28:   BaseLanguageModelInput,",
          "29:   BaseLanguageModelCallOptions,",
          "31:   StructuredOutputMethodOptions,",
          "32:   FunctionDefinition,",
          "33: } from \"@langchain/core/language_models/base\";",
          "",
          "[Removed Lines]",
          "24:   OpenAIToolCall,",
          "30:   StructuredOutputMethodParams,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "44:   ChatGenerationChunk,",
          "45:   ChatResult,",
          "46: } from \"@langchain/core/outputs\";",
          "47: import { getEnvironmentVariable } from \"@langchain/core/utils/env\";",
          "48: import { NewTokenIndices } from \"@langchain/core/callbacks/base\";",
          "49: import { z } from \"zod\";",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "54: import { AsyncCaller } from \"@langchain/core/utils/async_caller\";",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "65: } from \"@langchain/core/runnables\";",
          "66: import { zodToJsonSchema } from \"zod-to-json-schema\";",
          "67: import { ToolCallChunk } from \"@langchain/core/messages/tool\";",
          "70: interface TokenUsage {",
          "71:   completionTokens?: number;",
          "",
          "[Removed Lines]",
          "68: import { _convertToolCallIdToMistralCompatible } from \"./utils.js\";",
          "",
          "[Added Lines]",
          "76: import {",
          "77:   _convertToolCallIdToMistralCompatible,",
          "78:   _mistralContentChunkToMessageContentComplex,",
          "79: } from \"./utils.js\";",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "73:   totalTokens?: number;",
          "74: }",
          "85: export interface ChatMistralAICallOptions",
          "86:   extends Omit<BaseLanguageModelCallOptions, \"stop\"> {",
          "",
          "[Removed Lines]",
          "76: export type MistralAIToolChoice = \"auto\" | \"any\" | \"none\";",
          "78: type MistralAIToolInput = { type: string; function: MistralAIFunction };",
          "80: type ChatMistralAIToolType =",
          "81:   | MistralAIToolInput",
          "82:   | MistralAITool",
          "83:   | BindToolsInput;",
          "",
          "[Added Lines]",
          "87: type ChatMistralAIToolType = MistralAIToolCall | MistralAITool | BindToolsInput;",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "120:   model?: string;",
          "124:   endpoint?: string;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "134:   serverURL?: string;",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "167:   seed?: number;",
          "168: }",
          "170: function convertMessagesToMistralMessages(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "182:   beforeRequestHooks?: BeforeRequestHook[];",
          "187:   requestErrorHooks?: RequestErrorHook[];",
          "192:   responseHooks?: ResponseHook[];",
          "197:   httpClient?: MistralAIHTTPClient;",
          "203:   presencePenalty?: number;",
          "209:   frequencyPenalty?: number;",
          "213:   numCompletions?: number;",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "187:     }",
          "188:   };",
          "191:     if (typeof content === \"string\") {",
          "192:       return content;",
          "193:     }",
          "194:     throw new Error(",
          "196:         content,",
          "197:         null,",
          "198:         2",
          "",
          "[Removed Lines]",
          "190:   const getContent = (content: MessageContent): string => {",
          "195:       `ChatMistralAI does not support non text message content. Received: ${JSON.stringify(",
          "",
          "[Added Lines]",
          "236:   const getContent = (",
          "237:     content: MessageContent,",
          "238:     type: MessageType",
          "239:   ): string | MistralAIContentChunk[] => {",
          "240:     const _generateContentChunk = (",
          "241:       complex: MessageContentComplex,",
          "242:       role: string",
          "243:     ): MistralAIContentChunk => {",
          "244:       if (",
          "245:         complex.type === \"image_url\" &&",
          "246:         (role === \"user\" || role === \"assistant\")",
          "247:       ) {",
          "248:         return {",
          "249:           type: complex.type,",
          "250:           imageUrl: complex?.image_url,",
          "251:         };",
          "252:       }",
          "254:       if (complex.type === \"text\") {",
          "255:         return {",
          "256:           type: complex.type,",
          "257:           text: complex?.text,",
          "258:         };",
          "259:       }",
          "261:       throw new Error(",
          "262:         `ChatMistralAI only supports messages of \"image_url\" for roles \"user\" and \"assistant\", and \"text\" for all others.\\n\\nReceived: ${JSON.stringify(",
          "263:           content,",
          "264:           null,",
          "265:           2",
          "266:         )}`",
          "267:       );",
          "268:     };",
          "274:     if (Array.isArray(content)) {",
          "275:       const mistralRole = getRole(type);",
          "278:       const newContent: MistralAIContentChunk[] = [];",
          "279:       content.forEach((messageContentComplex) => {",
          "281:         if (",
          "282:           messageContentComplex.type === \"text\" ||",
          "283:           messageContentComplex.type === \"image_url\"",
          "284:         ) {",
          "285:           newContent.push(",
          "286:             _generateContentChunk(messageContentComplex, mistralRole)",
          "287:           );",
          "288:         } else {",
          "289:           throw new Error(",
          "290:             `Mistral only supports types \"text\" or \"image_url\" for complex message types.`",
          "291:           );",
          "292:         }",
          "293:       });",
          "294:       return newContent;",
          "295:     }",
          "298:       `Message content must be a string or an array.\\n\\nReceived: ${JSON.stringify(",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "200:     );",
          "201:   };",
          "204:     if (isAIMessage(message) && !!message.tool_calls?.length) {",
          "205:       return message.tool_calls",
          "206:         .map((toolCall) => ({",
          "207:           ...toolCall,",
          "208:           id: _convertToolCallIdToMistralCompatible(toolCall.id ?? \"\"),",
          "209:         }))",
          "211:     }",
          "222:   };",
          "224:   return messages.map((message) => {",
          "225:     const toolCalls = getTools(message);",
          "227:     if (\"tool_call_id\" in message && typeof message.tool_call_id === \"string\") {",
          "228:       return {",
          "230:         content,",
          "231:         name: message.name,",
          "235:       };",
          "236:     }",
          "238:     return {",
          "240:       content,",
          "242:     };",
          "243:   }) as MistralAIMessage[];",
          "244: }",
          "246: function mistralAIResponseToChatMessage(",
          "248:   usage?: MistralAITokenUsage",
          "249: ): BaseMessage {",
          "250:   const { message } = choice;",
          "257:   }",
          "258:   switch (message.role) {",
          "259:     case \"assistant\": {",
          "260:       const toolCalls = [];",
          "",
          "[Removed Lines]",
          "203:   const getTools = (message: BaseMessage): MistralAIToolCalls[] | undefined => {",
          "210:         .map(convertLangChainToolCallToOpenAI) as MistralAIToolCalls[];",
          "212:     if (!message.additional_kwargs.tool_calls?.length) {",
          "213:       return undefined;",
          "214:     }",
          "215:     const toolCalls: Omit<OpenAIToolCall, \"index\">[] =",
          "216:       message.additional_kwargs.tool_calls;",
          "217:     return toolCalls?.map((toolCall) => ({",
          "218:       id: _convertToolCallIdToMistralCompatible(toolCall.id),",
          "219:       type: \"function\",",
          "220:       function: toolCall.function,",
          "221:     }));",
          "226:     const content = toolCalls === undefined ? getContent(message.content) : \"\";",
          "229:         role: getRole(message._getType()),",
          "232:         tool_call_id: _convertToolCallIdToMistralCompatible(",
          "233:           message.tool_call_id",
          "234:         ),",
          "239:       role: getRole(message._getType()),",
          "241:       tool_calls: toolCalls,",
          "247:   choice: ChatCompletionResponse[\"choices\"][0],",
          "254:   let rawToolCalls: MistralAIToolCalls[] = [];",
          "255:   if (\"tool_calls\" in message && Array.isArray(message.tool_calls)) {",
          "256:     rawToolCalls = message.tool_calls as MistralAIToolCalls[];",
          "",
          "[Added Lines]",
          "306:   const getTools = (message: BaseMessage): MistralAIToolCall[] | undefined => {",
          "313:         .map(convertLangChainToolCallToOpenAI) as MistralAIToolCall[];",
          "315:     return undefined;",
          "320:     const content = getContent(message.content, message.getType());",
          "323:         role: getRole(message.getType()),",
          "326:         toolCallId: _convertToolCallIdToMistralCompatible(message.tool_call_id),",
          "329:     } else if (isAIMessage(message)) {",
          "330:       if (toolCalls === undefined) {",
          "331:         return {",
          "332:           role: getRole(message.getType()),",
          "333:           content,",
          "334:         };",
          "335:       } else {",
          "336:         return {",
          "337:           role: getRole(message.getType()),",
          "338:           toolCalls,",
          "339:         };",
          "340:       }",
          "344:       role: getRole(message.getType()),",
          "351:   choice: NonNullable<MistralAIChatCompletionResponse[\"choices\"]>[0],",
          "355:   if (message === undefined) {",
          "356:     throw new Error(\"No message found in response\");",
          "357:   }",
          "361:   let rawToolCalls: MistralAIToolCall[] = [];",
          "362:   if (\"toolCalls\" in message && Array.isArray(message.toolCalls)) {",
          "363:     rawToolCalls = message.toolCalls;",
          "365:   const content = _mistralContentChunkToMessageContentComplex(message.content);",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "272:         }",
          "273:       }",
          "274:       return new AIMessage({",
          "276:         tool_calls: toolCalls,",
          "277:         invalid_tool_calls: invalidToolCalls,",
          "286:         usage_metadata: usage",
          "287:           ? {",
          "291:             }",
          "292:           : undefined,",
          "293:       });",
          "294:     }",
          "295:     default:",
          "297:   }",
          "298: }",
          "300: function _convertDeltaToMessageChunk(",
          "301:   delta: {",
          "305:   },",
          "306:   usage?: MistralAITokenUsage | null",
          "307: ) {",
          "309:     if (usage) {",
          "310:       return new AIMessageChunk({",
          "311:         content: \"\",",
          "312:         usage_metadata: usage",
          "313:           ? {",
          "317:             }",
          "318:           : undefined,",
          "319:       });",
          "",
          "[Removed Lines]",
          "275:         content: message.content ?? \"\",",
          "278:         additional_kwargs: {",
          "279:           tool_calls: rawToolCalls.length",
          "280:             ? rawToolCalls.map((toolCall) => ({",
          "281:                 ...toolCall,",
          "282:                 type: \"function\",",
          "283:               }))",
          "284:             : undefined,",
          "285:         },",
          "288:               input_tokens: usage.prompt_tokens,",
          "289:               output_tokens: usage.completion_tokens,",
          "290:               total_tokens: usage.total_tokens,",
          "296:       return new HumanMessage(message.content ?? \"\");",
          "302:     role?: string | undefined;",
          "303:     content?: string | undefined;",
          "304:     tool_calls?: MistralAIToolCalls[] | undefined;",
          "308:   if (!delta.content && !delta.tool_calls) {",
          "314:               input_tokens: usage.prompt_tokens,",
          "315:               output_tokens: usage.completion_tokens,",
          "316:               total_tokens: usage.total_tokens,",
          "",
          "[Added Lines]",
          "383:         content,",
          "386:         additional_kwargs: {},",
          "389:               input_tokens: usage.promptTokens,",
          "390:               output_tokens: usage.completionTokens,",
          "391:               total_tokens: usage.totalTokens,",
          "397:       return new HumanMessage({ content });",
          "403:     role?: string | null | undefined;",
          "404:     content?: string | MistralAIContentChunk[] | null | undefined;",
          "405:     toolCalls?: MistralAIToolCall[] | null | undefined;",
          "409:   if (!delta.content && !delta.toolCalls) {",
          "415:               input_tokens: usage.promptTokens,",
          "416:               output_tokens: usage.completionTokens,",
          "417:               total_tokens: usage.totalTokens,",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "329:           ...toolCall,",
          "330:           index,",
          "331:           id: toolCall.id ?? uuidv4().replace(/-/g, \"\"),",
          "",
          "[Removed Lines]",
          "326:   const rawToolCallChunksWithIndex = delta.tool_calls?.length",
          "327:     ? delta.tool_calls?.map(",
          "328:         (toolCall, index): OpenAIToolCall => ({",
          "",
          "[Added Lines]",
          "427:   const rawToolCallChunksWithIndex = delta.toolCalls?.length",
          "428:     ? delta.toolCalls?.map(",
          "429:         (toolCall, index): MistralAIToolCall & { index: number } => ({",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "338:   if (delta.role) {",
          "339:     role = delta.role;",
          "340:   }",
          "342:   let additional_kwargs;",
          "343:   const toolCallChunks: ToolCallChunk[] = [];",
          "344:   if (rawToolCallChunksWithIndex !== undefined) {",
          "348:     for (const rawToolCallChunk of rawToolCallChunksWithIndex) {",
          "349:       toolCallChunks.push({",
          "350:         name: rawToolCallChunk.function?.name,",
          "352:         id: rawToolCallChunk.id,",
          "353:         index: rawToolCallChunk.index,",
          "354:         type: \"tool_call_chunk\",",
          "",
          "[Removed Lines]",
          "341:   const content = delta.content ?? \"\";",
          "345:     additional_kwargs = {",
          "346:       tool_calls: rawToolCallChunksWithIndex,",
          "347:     };",
          "351:         args: rawToolCallChunk.function?.arguments,",
          "",
          "[Added Lines]",
          "442:   const content = _mistralContentChunkToMessageContentComplex(delta.content);",
          "448:       const rawArgs = rawToolCallChunk.function?.arguments;",
          "449:       const args =",
          "450:         rawArgs === undefined || typeof rawArgs === \"string\"",
          "451:           ? rawArgs",
          "452:           : JSON.stringify(rawArgs);",
          "455:         args,",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "367:       additional_kwargs,",
          "368:       usage_metadata: usage",
          "369:         ? {",
          "373:           }",
          "374:         : undefined,",
          "375:     });",
          "",
          "[Removed Lines]",
          "370:             input_tokens: usage.prompt_tokens,",
          "371:             output_tokens: usage.completion_tokens,",
          "372:             total_tokens: usage.total_tokens,",
          "",
          "[Added Lines]",
          "474:             input_tokens: usage.promptTokens,",
          "475:             output_tokens: usage.completionTokens,",
          "476:             total_tokens: usage.totalTokens,",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "749:   lc_namespace = [\"langchain\", \"chat_models\", \"mistralai\"];",
          "753:   model = \"mistral-small-latest\";",
          "755:   apiKey: string;",
          "759:   temperature = 0.7;",
          "",
          "[Removed Lines]",
          "751:   modelName = \"mistral-small-latest\";",
          "757:   endpoint?: string;",
          "",
          "[Added Lines]",
          "862:   endpoint: string;",
          "864:   serverURL?: string;",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "776:   seed?: number;",
          "778:   lc_serializable = true;",
          "780:   streamUsage = true;",
          "782:   constructor(fields?: ChatMistralAIInput) {",
          "783:     super(fields ?? {});",
          "784:     const apiKey = fields?.apiKey ?? getEnvironmentVariable(\"MISTRAL_API_KEY\");",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "885:   maxRetries?: number;",
          "891:   beforeRequestHooks?: Array<BeforeRequestHook>;",
          "893:   requestErrorHooks?: Array<RequestErrorHook>;",
          "895:   responseHooks?: Array<ResponseHook>;",
          "897:   httpClient?: MistralAIHTTPClient;",
          "899:   presencePenalty?: number;",
          "901:   frequencyPenalty?: number;",
          "903:   numCompletions?: number;",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "789:     }",
          "790:     this.apiKey = apiKey;",
          "791:     this.streaming = fields?.streaming ?? this.streaming;",
          "793:     this.temperature = fields?.temperature ?? this.temperature;",
          "794:     this.topP = fields?.topP ?? this.topP;",
          "795:     this.maxTokens = fields?.maxTokens ?? this.maxTokens;",
          "797:     this.safePrompt = fields?.safePrompt ?? this.safePrompt;",
          "798:     this.randomSeed = fields?.seed ?? fields?.randomSeed ?? this.seed;",
          "799:     this.seed = this.randomSeed;",
          "802:     this.streamUsage = fields?.streamUsage ?? this.streamUsage;",
          "803:   }",
          "805:   get lc_secrets(): { [key: string]: string } | undefined {",
          "",
          "[Removed Lines]",
          "792:     this.endpoint = fields?.endpoint;",
          "796:     this.safeMode = fields?.safeMode ?? this.safeMode;",
          "800:     this.modelName = fields?.model ?? fields?.modelName ?? this.model;",
          "801:     this.model = this.modelName;",
          "",
          "[Added Lines]",
          "915:     this.serverURL = fields?.serverURL ?? this.serverURL;",
          "922:     this.maxRetries = fields?.maxRetries;",
          "923:     this.httpClient = fields?.httpClient;",
          "924:     this.model = fields?.model ?? fields?.modelName ?? this.model;",
          "926:     this.beforeRequestHooks =",
          "927:       fields?.beforeRequestHooks ?? this.beforeRequestHooks;",
          "928:     this.requestErrorHooks =",
          "929:       fields?.requestErrorHooks ?? this.requestErrorHooks;",
          "930:     this.responseHooks = fields?.responseHooks ?? this.responseHooks;",
          "931:     this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;",
          "932:     this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;",
          "933:     this.numCompletions = fields?.numCompletions ?? this.numCompletions;",
          "934:     this.addAllHooksToHttpClient();",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "835:   invocationParams(",
          "836:     options?: this[\"ParsedCallOptions\"]",
          "838:     const { response_format, tools, tool_choice } = options ?? {};",
          "839:     const mistralAITools: Array<MistralAITool> | undefined = tools?.length",
          "840:       ? _convertToolToMistralTool(tools)",
          "841:       : undefined;",
          "843:       model: this.model,",
          "844:       tools: mistralAITools,",
          "845:       temperature: this.temperature,",
          "846:       maxTokens: this.maxTokens,",
          "847:       topP: this.topP,",
          "848:       randomSeed: this.seed,",
          "850:       safePrompt: this.safePrompt,",
          "851:       toolChoice: tool_choice,",
          "853:     };",
          "854:     return params;",
          "855:   }",
          "",
          "[Removed Lines]",
          "837:   ): Omit<ChatRequest, \"messages\"> {",
          "842:     const params: Omit<ChatRequest, \"messages\"> = {",
          "849:       safeMode: this.safeMode,",
          "852:       responseFormat: response_format as ResponseFormat,",
          "",
          "[Added Lines]",
          "969:   ): Omit<",
          "970:     MistralAIChatCompletionRequest | MistralAIChatCompletionStreamRequest,",
          "971:     \"messages\"",
          "972:   > {",
          "977:     const params: Omit<MistralAIChatCompletionRequest, \"messages\"> = {",
          "986:       responseFormat: response_format,",
          "987:       presencePenalty: this.presencePenalty,",
          "988:       frequencyPenalty: this.frequencyPenalty,",
          "989:       n: this.numCompletions,",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "872:   async completionWithRetry(",
          "874:     streaming: true",
          "877:   async completionWithRetry(",
          "879:     streaming: false",
          "882:   async completionWithRetry(",
          "884:     streaming: boolean",
          "885:   ): Promise<",
          "887:   > {",
          "892:       try {",
          "893:         let res:",
          "896:         if (streaming) {",
          "898:         } else {",
          "900:         }",
          "901:         return res;",
          "903:       } catch (e: any) {",
          "905:           e.status = 400;",
          "906:         }",
          "907:         throw e;",
          "",
          "[Removed Lines]",
          "873:     input: ChatRequest,",
          "875:   ): Promise<AsyncGenerator<ChatCompletionResponseChunk>>;",
          "878:     input: ChatRequest,",
          "880:   ): Promise<ChatCompletionResponse>;",
          "883:     input: ChatRequest,",
          "886:     ChatCompletionResponse | AsyncGenerator<ChatCompletionResponseChunk>",
          "888:     const { MistralClient } = await this.imports();",
          "889:     const client = new MistralClient(this.apiKey, this.endpoint);",
          "891:     return this.caller.call(async () => {",
          "894:           | ChatCompletionResponse",
          "895:           | AsyncGenerator<ChatCompletionResponseChunk>;",
          "897:           res = client.chatStream(input);",
          "899:           res = await client.chat(input);",
          "904:         if (e.message?.includes(\"status: 400\")) {",
          "",
          "[Added Lines]",
          "1010:     input: MistralAIChatCompletionStreamRequest,",
          "1012:   ): Promise<AsyncIterable<MistralAIChatCompletionEvent>>;",
          "1015:     input: MistralAIChatCompletionRequest,",
          "1017:   ): Promise<MistralAIChatCompletionResponse>;",
          "1020:     input:",
          "1021:       | MistralAIChatCompletionRequest",
          "1022:       | MistralAIChatCompletionStreamRequest,",
          "1025:     | MistralAIChatCompletionResponse",
          "1026:     | AsyncIterable<MistralAIChatCompletionEvent>",
          "1028:     const caller = new AsyncCaller({",
          "1029:       maxRetries: this.maxRetries,",
          "1030:     });",
          "1031:     const client = new MistralClient({",
          "1032:       apiKey: this.apiKey,",
          "1033:       serverURL: this.serverURL,",
          "1035:       ...(this.httpClient ? { httpClient: this.httpClient } : {}),",
          "1036:     });",
          "1038:     return caller.call(async () => {",
          "1041:           | MistralAIChatCompletionResponse",
          "1042:           | AsyncIterable<MistralAIChatCompletionEvent>;",
          "1044:           res = await client.chat.stream(input);",
          "1046:           res = await client.chat.complete(input);",
          "1051:         if (",
          "1052:           e.message?.includes(\"status: 400\") ||",
          "1053:           e.message?.toLowerCase().includes(\"status 400\") ||",
          "1054:           e.message?.includes(\"validation failed\")",
          "1055:         ) {",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "931:     if (this.streaming || shouldStream) {",
          "",
          "[Removed Lines]",
          "928:     const shouldStream = !!options.signal ?? !!options.timeout;",
          "",
          "[Added Lines]",
          "1079:     const shouldStream = options.signal ?? !!options.timeout;",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "951:     const response = await this.completionWithRetry(input, false);",
          "959:     if (completionTokens) {",
          "960:       tokenUsage.completionTokens =",
          "",
          "[Removed Lines]",
          "953:     const {",
          "954:       completion_tokens: completionTokens,",
          "955:       prompt_tokens: promptTokens,",
          "956:       total_tokens: totalTokens,",
          "957:     } = response?.usage ?? {};",
          "",
          "[Added Lines]",
          "1104:     const { completionTokens, promptTokens, totalTokens } =",
          "1105:       response?.usage ?? {};",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "977:       if (!(\"message\" in part)) {",
          "978:         throw new Error(\"No message found in the choice.\");",
          "979:       }",
          "981:       const generation: ChatGeneration = {",
          "982:         text,",
          "983:         message: mistralAIResponseToChatMessage(part, response?.usage),",
          "984:       };",
          "987:       }",
          "988:       generations.push(generation);",
          "989:     }",
          "",
          "[Removed Lines]",
          "980:       const text = part.message?.content ?? \"\";",
          "985:       if (part.finish_reason) {",
          "986:         generation.generationInfo = { finish_reason: part.finish_reason };",
          "",
          "[Added Lines]",
          "1128:       let text = part.message?.content ?? \"\";",
          "1129:       if (Array.isArray(text)) {",
          "1130:         text = text[0].type === \"text\" ? text[0].text : \"\";",
          "1131:       }",
          "1136:       if (part.finishReason) {",
          "1137:         generation.generationInfo = { finishReason: part.finishReason };",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "1006:     };",
          "1008:     const streamIterable = await this.completionWithRetry(input, true);",
          "1010:       if (options.signal?.aborted) {",
          "1011:         throw new Error(\"AbortError\");",
          "1012:       }",
          "",
          "[Removed Lines]",
          "1009:     for await (const data of streamIterable) {",
          "",
          "[Added Lines]",
          "1160:     for await (const { data } of streamIterable) {",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "1033:         continue;",
          "1034:       }",
          "1035:       const generationChunk = new ChatGenerationChunk({",
          "1036:         message,",
          "1038:         generationInfo: newTokenIndices,",
          "1039:       });",
          "1040:       yield generationChunk;",
          "",
          "[Removed Lines]",
          "1037:         text: delta.content ?? \"\",",
          "",
          "[Added Lines]",
          "1186:       let text = delta.content ?? \"\";",
          "1187:       if (Array.isArray(text)) {",
          "1188:         text = text[0].type === \"text\" ? text[0].text : \"\";",
          "1189:       }",
          "1192:         text,",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "1050:     }",
          "1051:   }",
          "1054:   _combineLLMOutput() {",
          "1055:     return [];",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1208:   addAllHooksToHttpClient() {",
          "1209:     try {",
          "1211:       this.removeAllHooksFromHttpClient();",
          "1214:       const hasHooks = [",
          "1215:         this.beforeRequestHooks,",
          "1216:         this.requestErrorHooks,",
          "1217:         this.responseHooks,",
          "1218:       ].some((hook) => hook && hook.length > 0);",
          "1219:       if (hasHooks && !this.httpClient) {",
          "1220:         this.httpClient = new MistralAIHTTPClient();",
          "1221:       }",
          "1223:       if (this.beforeRequestHooks) {",
          "1224:         for (const hook of this.beforeRequestHooks) {",
          "1225:           this.httpClient?.addHook(\"beforeRequest\", hook);",
          "1226:         }",
          "1227:       }",
          "1229:       if (this.requestErrorHooks) {",
          "1230:         for (const hook of this.requestErrorHooks) {",
          "1231:           this.httpClient?.addHook(\"requestError\", hook);",
          "1232:         }",
          "1233:       }",
          "1235:       if (this.responseHooks) {",
          "1236:         for (const hook of this.responseHooks) {",
          "1237:           this.httpClient?.addHook(\"response\", hook);",
          "1238:         }",
          "1239:       }",
          "1240:     } catch {",
          "1241:       throw new Error(\"Error in adding all hooks\");",
          "1242:     }",
          "1243:   }",
          "1245:   removeAllHooksFromHttpClient() {",
          "1246:     try {",
          "1247:       if (this.beforeRequestHooks) {",
          "1248:         for (const hook of this.beforeRequestHooks) {",
          "1249:           this.httpClient?.removeHook(\"beforeRequest\", hook);",
          "1250:         }",
          "1251:       }",
          "1253:       if (this.requestErrorHooks) {",
          "1254:         for (const hook of this.requestErrorHooks) {",
          "1255:           this.httpClient?.removeHook(\"requestError\", hook);",
          "1256:         }",
          "1257:       }",
          "1259:       if (this.responseHooks) {",
          "1260:         for (const hook of this.responseHooks) {",
          "1261:           this.httpClient?.removeHook(\"response\", hook);",
          "1262:         }",
          "1263:       }",
          "1264:     } catch {",
          "1265:       throw new Error(\"Error in removing hooks\");",
          "1266:     }",
          "1267:   }",
          "1269:   removeHookFromHttpClient(",
          "1270:     hook: BeforeRequestHook | RequestErrorHook | ResponseHook",
          "1271:   ) {",
          "1272:     try {",
          "1273:       this.httpClient?.removeHook(\"beforeRequest\", hook as BeforeRequestHook);",
          "1274:       this.httpClient?.removeHook(\"requestError\", hook as RequestErrorHook);",
          "1275:       this.httpClient?.removeHook(\"response\", hook as ResponseHook);",
          "1276:     } catch {",
          "1277:       throw new Error(\"Error in removing hook\");",
          "1278:     }",
          "1279:   }",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "1060:     RunOutput extends Record<string, any> = Record<string, any>",
          "1061:   >(",
          "1062:     outputSchema:",
          "1064:       | z.ZodType<RunOutput>",
          "1066:       | Record<string, any>,",
          "",
          "[Removed Lines]",
          "1063:       | StructuredOutputMethodParams<RunOutput, false>",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 26 ---",
          "[Context before]",
          "1072:     RunOutput extends Record<string, any> = Record<string, any>",
          "1073:   >(",
          "1074:     outputSchema:",
          "1076:       | z.ZodType<RunOutput>",
          "1078:       | Record<string, any>,",
          "",
          "[Removed Lines]",
          "1075:       | StructuredOutputMethodParams<RunOutput, true>",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 27 ---",
          "[Context before]",
          "1084:     RunOutput extends Record<string, any> = Record<string, any>",
          "1085:   >(",
          "1086:     outputSchema:",
          "1088:       | z.ZodType<RunOutput>",
          "1090:       | Record<string, any>,",
          "",
          "[Removed Lines]",
          "1087:       | StructuredOutputMethodParams<RunOutput, boolean>",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 28 ---",
          "[Context before]",
          "1096:         { raw: BaseMessage; parsed: RunOutput }",
          "1097:       > {",
          "1114:     let llm: Runnable<BaseLanguageModelInput>;",
          "1115:     let outputParser: BaseLLMOutputParser<RunOutput>;",
          "",
          "[Removed Lines]",
          "1099:     let schema: z.ZodType<RunOutput> | Record<string, any>;",
          "1100:     let name;",
          "1101:     let method;",
          "1102:     let includeRaw;",
          "1103:     if (isStructuredOutputMethodParams(outputSchema)) {",
          "1104:       schema = outputSchema.schema;",
          "1105:       name = outputSchema.name;",
          "1106:       method = outputSchema.method;",
          "1107:       includeRaw = outputSchema.includeRaw;",
          "1108:     } else {",
          "1109:       schema = outputSchema;",
          "1110:       name = config?.name;",
          "1111:       method = config?.method;",
          "1112:       includeRaw = config?.includeRaw;",
          "1113:     }",
          "",
          "[Added Lines]",
          "1324:     const schema: z.ZodType<RunOutput> | Record<string, any> = outputSchema;",
          "1325:     const name = config?.name;",
          "1326:     const method = config?.method;",
          "1327:     const includeRaw = config?.includeRaw;",
          "",
          "---------------",
          "--- Hunk 29 ---",
          "[Context before]",
          "1205:       parsedWithFallback,",
          "1206:     ]);",
          "1207:   }",
          "1214: }",
          "1216: function isZodSchema<",
          "",
          "[Removed Lines]",
          "1210:   private async imports() {",
          "1211:     const { default: MistralClient } = await import(\"@mistralai/mistralai\");",
          "1212:     return { MistralClient };",
          "1213:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 30 ---",
          "[Context before]",
          "1224:   return typeof (input as z.ZodType<RunOutput>)?.parse === \"function\";",
          "1225: }",
          "",
          "[Removed Lines]",
          "1227: function isStructuredOutputMethodParams(",
          "1228:   x: unknown",
          "1230: ): x is StructuredOutputMethodParams<Record<string, any>> {",
          "1231:   return (",
          "1232:     x !== undefined &&",
          "1234:     typeof (x as StructuredOutputMethodParams<Record<string, any>>).schema ===",
          "1235:       \"object\"",
          "1236:   );",
          "1237: }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "libs/langchain-mistralai/src/embeddings.ts||libs/langchain-mistralai/src/embeddings.ts": [
          "File: libs/langchain-mistralai/src/embeddings.ts -> libs/langchain-mistralai/src/embeddings.ts",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: import { getEnvironmentVariable } from \"@langchain/core/utils/env\";",
          "2: import { Embeddings, type EmbeddingsParams } from \"@langchain/core/embeddings\";",
          "3: import { chunkArray } from \"@langchain/core/utils/chunk_array\";",
          "",
          "[Removed Lines]",
          "4: import { EmbeddingResponse } from \"@mistralai/mistralai\";",
          "",
          "[Added Lines]",
          "4: import { EmbeddingRequest as MistralAIEmbeddingsRequest } from \"@mistralai/mistralai/src/models/components/embeddingrequest.js\";",
          "5: import { EmbeddingResponse as MistralAIEmbeddingsResponse } from \"@mistralai/mistralai/src/models/components/embeddingresponse.js\";",
          "6: import {",
          "7:   BeforeRequestHook,",
          "8:   RequestErrorHook,",
          "9:   ResponseHook,",
          "10:   HTTPClient as MistralAIHTTPClient,",
          "11: } from \"@mistralai/mistralai/lib/http.js\";",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "31:   encodingFormat?: string;",
          "35:   endpoint?: string;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "47:   serverURL?: string;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "46:   stripNewLines?: boolean;",
          "47: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "63:   beforeRequestHooks?: BeforeRequestHook[];",
          "68:   requestErrorHooks?: RequestErrorHook[];",
          "73:   responseHooks?: ResponseHook[];",
          "78:   httpClient?: MistralAIHTTPClient;",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "66:   apiKey: string;",
          "70:   constructor(fields?: Partial<MistralAIEmbeddingsParams>) {",
          "71:     super(fields ?? {});",
          "",
          "[Removed Lines]",
          "68:   endpoint?: string;",
          "",
          "[Added Lines]",
          "103:   endpoint: string;",
          "105:   serverURL?: string;",
          "107:   beforeRequestHooks?: Array<BeforeRequestHook>;",
          "109:   requestErrorHooks?: Array<RequestErrorHook>;",
          "111:   responseHooks?: Array<ResponseHook>;",
          "113:   httpClient?: MistralAIHTTPClient;",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "74:       throw new Error(\"API key missing for MistralAI, but it is required.\");",
          "75:     }",
          "76:     this.apiKey = apiKey;",
          "78:     this.modelName = fields?.model ?? fields?.modelName ?? this.model;",
          "79:     this.model = this.modelName;",
          "80:     this.encodingFormat = fields?.encodingFormat ?? this.encodingFormat;",
          "81:     this.batchSize = fields?.batchSize ?? this.batchSize;",
          "82:     this.stripNewLines = fields?.stripNewLines ?? this.stripNewLines;",
          "83:   }",
          "",
          "[Removed Lines]",
          "77:     this.endpoint = fields?.endpoint;",
          "",
          "[Added Lines]",
          "122:     this.serverURL = fields?.serverURL ?? this.serverURL;",
          "128:     this.beforeRequestHooks =",
          "129:       fields?.beforeRequestHooks ?? this.beforeRequestHooks;",
          "130:     this.requestErrorHooks =",
          "131:       fields?.requestErrorHooks ?? this.requestErrorHooks;",
          "132:     this.responseHooks = fields?.responseHooks ?? this.responseHooks;",
          "133:     this.httpClient = fields?.httpClient ?? this.httpClient;",
          "134:     this.addAllHooksToHttpClient();",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "105:       const batch = batches[i];",
          "106:       const { data: batchResponse } = batchResponses[i];",
          "107:       for (let j = 0; j < batch.length; j += 1) {",
          "109:       }",
          "110:     }",
          "111:     return embeddings;",
          "",
          "[Removed Lines]",
          "108:         embeddings.push(batchResponse[j].embedding);",
          "",
          "[Added Lines]",
          "160:         embeddings.push(batchResponse[j].embedding ?? []);",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "121:     const { data } = await this.embeddingWithRetry(",
          "122:       this.stripNewLines ? text.replace(/\\n/g, \" \") : text",
          "123:     );",
          "125:   }",
          "134:   private async embeddingWithRetry(",
          "139:     return this.caller.call(async () => {",
          "144:       return res;",
          "145:     });",
          "146:   }",
          "149:   private async imports() {",
          "152:   }",
          "153: }",
          "",
          "[Removed Lines]",
          "124:     return data[0].embedding;",
          "135:     input: string | Array<string>",
          "136:   ): Promise<EmbeddingResponse> {",
          "137:     const { MistralClient } = await this.imports();",
          "138:     const client = new MistralClient(this.apiKey, this.endpoint);",
          "140:       const res = await client.embeddings({",
          "141:         model: this.model,",
          "142:         input,",
          "143:       });",
          "150:     const { default: MistralClient } = await import(\"@mistralai/mistralai\");",
          "151:     return { MistralClient };",
          "",
          "[Added Lines]",
          "176:     return data[0].embedding ?? [];",
          "187:     inputs: string | Array<string>",
          "188:   ): Promise<MistralAIEmbeddingsResponse> {",
          "189:     const { Mistral } = await this.imports();",
          "190:     const client = new Mistral({",
          "191:       apiKey: this.apiKey,",
          "192:       serverURL: this.serverURL,",
          "194:       ...(this.httpClient ? { httpClient: this.httpClient } : {}),",
          "195:     });",
          "196:     const embeddingsRequest: MistralAIEmbeddingsRequest = {",
          "197:       model: this.model,",
          "198:       inputs,",
          "199:       encodingFormat: this.encodingFormat,",
          "200:     };",
          "202:       const res = await client.embeddings.create(embeddingsRequest);",
          "207:   addAllHooksToHttpClient() {",
          "208:     try {",
          "210:       this.removeAllHooksFromHttpClient();",
          "213:       const hasHooks = [",
          "214:         this.beforeRequestHooks,",
          "215:         this.requestErrorHooks,",
          "216:         this.responseHooks,",
          "217:       ].some((hook) => hook && hook.length > 0);",
          "218:       if (hasHooks && !this.httpClient) {",
          "219:         this.httpClient = new MistralAIHTTPClient();",
          "220:       }",
          "222:       if (this.beforeRequestHooks) {",
          "223:         for (const hook of this.beforeRequestHooks) {",
          "224:           this.httpClient?.addHook(\"beforeRequest\", hook);",
          "225:         }",
          "226:       }",
          "228:       if (this.requestErrorHooks) {",
          "229:         for (const hook of this.requestErrorHooks) {",
          "230:           this.httpClient?.addHook(\"requestError\", hook);",
          "231:         }",
          "232:       }",
          "234:       if (this.responseHooks) {",
          "235:         for (const hook of this.responseHooks) {",
          "236:           this.httpClient?.addHook(\"response\", hook);",
          "237:         }",
          "238:       }",
          "239:     } catch {",
          "240:       throw new Error(\"Error in adding all hooks\");",
          "241:     }",
          "242:   }",
          "244:   removeAllHooksFromHttpClient() {",
          "245:     try {",
          "246:       if (this.beforeRequestHooks) {",
          "247:         for (const hook of this.beforeRequestHooks) {",
          "248:           this.httpClient?.removeHook(\"beforeRequest\", hook);",
          "249:         }",
          "250:       }",
          "252:       if (this.requestErrorHooks) {",
          "253:         for (const hook of this.requestErrorHooks) {",
          "254:           this.httpClient?.removeHook(\"requestError\", hook);",
          "255:         }",
          "256:       }",
          "258:       if (this.responseHooks) {",
          "259:         for (const hook of this.responseHooks) {",
          "260:           this.httpClient?.removeHook(\"response\", hook);",
          "261:         }",
          "262:       }",
          "263:     } catch {",
          "264:       throw new Error(\"Error in removing hooks\");",
          "265:     }",
          "266:   }",
          "268:   removeHookFromHttpClient(",
          "269:     hook: BeforeRequestHook | RequestErrorHook | ResponseHook",
          "270:   ) {",
          "271:     try {",
          "272:       this.httpClient?.removeHook(\"beforeRequest\", hook as BeforeRequestHook);",
          "273:       this.httpClient?.removeHook(\"requestError\", hook as RequestErrorHook);",
          "274:       this.httpClient?.removeHook(\"response\", hook as ResponseHook);",
          "275:     } catch {",
          "276:       throw new Error(\"Error in removing hook\");",
          "277:     }",
          "278:   }",
          "282:     const { Mistral } = await import(\"@mistralai/mistralai\");",
          "283:     return { Mistral };",
          "",
          "---------------"
        ],
        "libs/langchain-mistralai/src/llms.ts||libs/langchain-mistralai/src/llms.ts": [
          "File: libs/langchain-mistralai/src/llms.ts -> libs/langchain-mistralai/src/llms.ts",
          "--- Hunk 1 ---",
          "[Context before]",
          "2: import { BaseLLMParams, LLM } from \"@langchain/core/language_models/llms\";",
          "3: import { type BaseLanguageModelCallOptions } from \"@langchain/core/language_models/base\";",
          "4: import { GenerationChunk, LLMResult } from \"@langchain/core/outputs\";",
          "5: import {",
          "11: import { getEnvironmentVariable } from \"@langchain/core/utils/env\";",
          "12: import { chunkArray } from \"@langchain/core/utils/chunk_array\";",
          "13: import { AsyncCaller } from \"@langchain/core/utils/async_caller\";",
          "",
          "[Removed Lines]",
          "6:   ChatCompletionResponse,",
          "7:   ChatCompletionResponseChoice,",
          "8:   ChatCompletionResponseChunk,",
          "9:   type CompletionRequest,",
          "10: } from \"@mistralai/mistralai\";",
          "",
          "[Added Lines]",
          "5: import { FIMCompletionRequest as MistralAIFIMCompletionRequest } from \"@mistralai/mistralai/models/components/fimcompletionrequest.js\";",
          "6: import { FIMCompletionStreamRequest as MistralAIFIMCompletionStreamRequest } from \"@mistralai/mistralai/models/components/fimcompletionstreamrequest.js\";",
          "7: import { FIMCompletionResponse as MistralAIFIMCompletionResponse } from \"@mistralai/mistralai/models/components/fimcompletionresponse.js\";",
          "8: import { ChatCompletionChoice as MistralAIChatCompletionChoice } from \"@mistralai/mistralai/models/components/chatcompletionchoice.js\";",
          "9: import { CompletionEvent as MistralAIChatCompletionEvent } from \"@mistralai/mistralai/models/components/completionevent.js\";",
          "10: import { CompletionChunk as MistralAICompetionChunk } from \"@mistralai/mistralai/models/components/completionchunk.js\";",
          "12:   BeforeRequestHook,",
          "13:   RequestErrorHook,",
          "14:   ResponseHook,",
          "15:   HTTPClient as MistralAIHTTPClient,",
          "16: } from \"@mistralai/mistralai/lib/http.js\";",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "35:   apiKey?: string;",
          "39:   endpoint?: string;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "50:   serverURL?: string;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "71:   batchSize?: number;",
          "72: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "87:   beforeRequestHooks?: BeforeRequestHook[];",
          "92:   requestErrorHooks?: RequestErrorHook[];",
          "97:   responseHooks?: ResponseHook[];",
          "102:   httpClient?: MistralAIHTTPClient;",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "103:   apiKey: string;",
          "107:   maxRetries?: number;",
          "109:   maxConcurrency?: number;",
          "111:   constructor(fields?: MistralAIInput) {",
          "112:     super(fields ?? {});",
          "",
          "[Removed Lines]",
          "105:   endpoint?: string;",
          "",
          "[Added Lines]",
          "139:   endpoint: string;",
          "141:   serverURL?: string;",
          "147:   beforeRequestHooks?: Array<BeforeRequestHook>;",
          "149:   requestErrorHooks?: Array<RequestErrorHook>;",
          "151:   responseHooks?: Array<ResponseHook>;",
          "153:   httpClient?: MistralAIHTTPClient;",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "118:     this.randomSeed = fields?.randomSeed ?? this.randomSeed;",
          "119:     this.batchSize = fields?.batchSize ?? this.batchSize;",
          "120:     this.streaming = fields?.streaming ?? this.streaming;",
          "122:     this.maxRetries = fields?.maxRetries;",
          "123:     this.maxConcurrency = fields?.maxConcurrency;",
          "125:     const apiKey = fields?.apiKey ?? getEnvironmentVariable(\"MISTRAL_API_KEY\");",
          "126:     if (!apiKey) {",
          "",
          "[Removed Lines]",
          "121:     this.endpoint = fields?.endpoint;",
          "",
          "[Added Lines]",
          "165:     this.serverURL = fields?.serverURL ?? this.serverURL;",
          "168:     this.beforeRequestHooks =",
          "169:       fields?.beforeRequestHooks ?? this.beforeRequestHooks;",
          "170:     this.requestErrorHooks =",
          "171:       fields?.requestErrorHooks ?? this.requestErrorHooks;",
          "172:     this.responseHooks = fields?.responseHooks ?? this.responseHooks;",
          "173:     this.httpClient = fields?.httpClient ?? this.httpClient;",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "130:       );",
          "131:     }",
          "132:     this.apiKey = apiKey;",
          "133:   }",
          "135:   get lc_secrets(): { [key: string]: string } | undefined {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "184:     this.addAllHooksToHttpClient();",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "151:   invocationParams(",
          "152:     options: this[\"ParsedCallOptions\"]",
          "154:     return {",
          "155:       model: this.model,",
          "156:       suffix: options.suffix,",
          "",
          "[Removed Lines]",
          "153:   ): Omit<CompletionRequest, \"prompt\"> {",
          "",
          "[Added Lines]",
          "205:   ): Omit<",
          "206:     MistralAIFIMCompletionRequest | MistralAIFIMCompletionStreamRequest,",
          "207:     \"prompt\"",
          "208:   > {",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "177:       prompt,",
          "178:     };",
          "179:     const result = await this.completionWithRetry(params, options, false);",
          "181:   }",
          "183:   async _generate(",
          "",
          "[Removed Lines]",
          "180:     return result.choices[0].message.content ?? \"\";",
          "",
          "[Added Lines]",
          "235:     let content = result?.choices?.[0].message.content ?? \"\";",
          "236:     if (Array.isArray(content)) {",
          "237:       content = content[0].type === \"text\" ? content[0].text : \"\";",
          "238:     }",
          "239:     return content;",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "186:     runManager?: CallbackManagerForLLMRun",
          "187:   ): Promise<LLMResult> {",
          "188:     const subPrompts = chunkArray(prompts, this.batchSize);",
          "191:     const params = this.invocationParams(options);",
          "",
          "[Removed Lines]",
          "189:     const choices: ChatCompletionResponseChoice[][] = [];",
          "",
          "[Added Lines]",
          "248:     const choices: MistralAIChatCompletionChoice[][] = [];",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "194:       const data = await (async () => {",
          "195:         if (this.streaming) {",
          "196:           const responseData: Array<",
          "199:             >",
          "200:           > = [];",
          "201:           for (let x = 0; x < subPrompts[i].length; x += 1) {",
          "203:             let response:",
          "205:               | undefined;",
          "206:             const stream = await this.completionWithRetry(",
          "207:               {",
          "",
          "[Removed Lines]",
          "197:             { choices: ChatCompletionResponseChoice[] } & Partial<",
          "198:               Omit<ChatCompletionResponse, \"choices\">",
          "202:             const choices: ChatCompletionResponseChoice[] = [];",
          "204:               | Omit<ChatCompletionResponse, \"choices\" | \"usage\">",
          "",
          "[Added Lines]",
          "256:             { choices: MistralAIChatCompletionChoice[] } & Partial<",
          "257:               Omit<MistralAICompetionChunk, \"choices\">",
          "261:             const choices: MistralAIChatCompletionChoice[] = [];",
          "263:               | Omit<MistralAICompetionChunk, \"choices\" | \"usage\">",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "211:               options,",
          "212:               true",
          "213:             );",
          "216:               if (!response) {",
          "217:                 response = {",
          "219:                   object: \"chat.completion\",",
          "222:                 };",
          "223:               }",
          "227:                 if (!choices[part.index]) {",
          "228:                   choices[part.index] = {",
          "229:                     index: part.index,",
          "230:                     message: {",
          "234:                     },",
          "236:                   };",
          "237:                 } else {",
          "238:                   const choice = choices[part.index];",
          "241:                 }",
          "243:                   prompt: part.index,",
          "244:                   completion: part.index,",
          "245:                 });",
          "",
          "[Removed Lines]",
          "214:             for await (const message of stream) {",
          "218:                   id: message.id,",
          "220:                   created: message.created,",
          "221:                   model: message.model,",
          "226:               for (const part of message.choices) {",
          "231:                       role: part.delta.role ?? \"assistant\",",
          "232:                       content: part.delta.content ?? \"\",",
          "233:                       tool_calls: null,",
          "235:                     finish_reason: part.finish_reason,",
          "239:                   choice.message.content += part.delta.content ?? \"\";",
          "240:                   choice.finish_reason = part.finish_reason;",
          "242:                 void runManager?.handleLLMNewToken(part.delta.content ?? \"\", {",
          "",
          "[Added Lines]",
          "273:             for await (const { data } of stream) {",
          "277:                   id: data.id,",
          "279:                   created: data.created,",
          "280:                   model: data.model,",
          "285:               for (const part of data.choices) {",
          "286:                 let content = part.delta.content ?? \"\";",
          "288:                 if (Array.isArray(content)) {",
          "289:                   let strContent = \"\";",
          "290:                   for (const contentChunk of content) {",
          "291:                     if (contentChunk.type === \"text\") {",
          "292:                       strContent += contentChunk.text;",
          "293:                     } else if (contentChunk.type === \"image_url\") {",
          "294:                       const imageURL =",
          "295:                         typeof contentChunk.imageUrl === \"string\"",
          "296:                           ? contentChunk.imageUrl",
          "297:                           : contentChunk.imageUrl.url;",
          "298:                       strContent += imageURL;",
          "299:                     }",
          "300:                   }",
          "301:                   content = strContent;",
          "302:                 }",
          "307:                       role: \"assistant\",",
          "308:                       content,",
          "309:                       toolCalls: null,",
          "311:                     finishReason: part.finishReason ?? \"length\",",
          "315:                   choice.message.content += content;",
          "316:                   choice.finishReason = part.finishReason ?? \"length\";",
          "318:                 void runManager?.handleLLMNewToken(content, {",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "255:           }",
          "256:           return responseData;",
          "257:         } else {",
          "259:           for (let x = 0; x < subPrompts[i].length; x += 1) {",
          "260:             const res = await this.completionWithRetry(",
          "261:               {",
          "",
          "[Removed Lines]",
          "258:           const responseData: Array<ChatCompletionResponse> = [];",
          "",
          "[Added Lines]",
          "334:           const responseData: Array<MistralAIFIMCompletionResponse> = [];",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "271:         }",
          "272:       })();",
          "275:     }",
          "277:     const generations = choices.map((promptChoices) =>",
          "284:     );",
          "285:     return {",
          "286:       generations,",
          "",
          "[Removed Lines]",
          "274:       choices.push(...data.map((d) => d.choices));",
          "278:       promptChoices.map((choice) => ({",
          "279:         text: choice.message.content ?? \"\",",
          "280:         generationInfo: {",
          "281:           finishReason: choice.finish_reason,",
          "282:         },",
          "283:       }))",
          "",
          "[Added Lines]",
          "350:       choices.push(...data.map((d) => d.choices ?? []));",
          "354:       promptChoices.map((choice) => {",
          "355:         let text = choice.message?.content ?? \"\";",
          "356:         if (Array.isArray(text)) {",
          "357:           text = text[0].type === \"text\" ? text[0].text : \"\";",
          "358:         }",
          "359:         return {",
          "360:           text,",
          "361:           generationInfo: {",
          "362:             finishReason: choice.finishReason,",
          "363:           },",
          "364:         };",
          "365:       })",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "288:   }",
          "290:   async completionWithRetry(",
          "292:     options: this[\"ParsedCallOptions\"],",
          "293:     stream: false",
          "296:   async completionWithRetry(",
          "298:     options: this[\"ParsedCallOptions\"],",
          "299:     stream: true",
          "302:   async completionWithRetry(",
          "304:     options: this[\"ParsedCallOptions\"],",
          "305:     stream: boolean",
          "306:   ): Promise<",
          "309:   > {",
          "311:     const caller = new AsyncCaller({",
          "312:       maxConcurrency: options.maxConcurrency || this.maxConcurrency,",
          "313:       maxRetries: this.maxRetries,",
          "314:     });",
          "321:     return caller.callWithOptions(",
          "322:       {",
          "323:         signal: options.signal,",
          "324:       },",
          "325:       async () => {",
          "330:         }",
          "331:       }",
          "332:     );",
          "",
          "[Removed Lines]",
          "291:     request: CompletionRequest,",
          "294:   ): Promise<ChatCompletionResponse>;",
          "297:     request: CompletionRequest,",
          "300:   ): Promise<AsyncGenerator<ChatCompletionResponseChunk, void>>;",
          "303:     request: CompletionRequest,",
          "307:     | ChatCompletionResponse",
          "308:     | AsyncGenerator<ChatCompletionResponseChunk, void, unknown>",
          "310:     const { MistralClient } = await this.imports();",
          "315:     const client = new MistralClient(",
          "316:       this.apiKey,",
          "317:       this.endpoint,",
          "318:       this.maxRetries,",
          "319:       options.timeout",
          "320:     );",
          "326:         if (stream) {",
          "327:           return client.completionStream(request);",
          "328:         } else {",
          "329:           return client.completion(request);",
          "",
          "[Added Lines]",
          "373:     request: MistralAIFIMCompletionRequest,",
          "376:   ): Promise<MistralAIFIMCompletionResponse>;",
          "379:     request: MistralAIFIMCompletionStreamRequest,",
          "382:   ): Promise<AsyncIterable<MistralAIChatCompletionEvent>>;",
          "385:     request:",
          "386:       | MistralAIFIMCompletionRequest",
          "387:       | MistralAIFIMCompletionStreamRequest,",
          "391:     MistralAIFIMCompletionResponse | AsyncIterable<MistralAIChatCompletionEvent>",
          "393:     const { Mistral } = await this.imports();",
          "398:     const client = new Mistral({",
          "399:       apiKey: this.apiKey,",
          "400:       serverURL: this.serverURL,",
          "401:       timeoutMs: options.timeout,",
          "403:       ...(this.httpClient ? { httpClient: this.httpClient } : {}),",
          "404:     });",
          "410:         try {",
          "411:           let res:",
          "412:             | MistralAIFIMCompletionResponse",
          "413:             | AsyncIterable<MistralAIChatCompletionEvent>;",
          "414:           if (stream) {",
          "415:             res = await client.fim.stream(request);",
          "416:           } else {",
          "417:             res = await client.fim.complete(request);",
          "418:           }",
          "419:           return res;",
          "421:         } catch (e: any) {",
          "422:           if (",
          "423:             e.message?.includes(\"status: 400\") ||",
          "424:             e.message?.toLowerCase().includes(\"status 400\") ||",
          "425:             e.message?.includes(\"validation failed\")",
          "426:           ) {",
          "427:             e.status = 400;",
          "428:           }",
          "429:           throw e;",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "342:       prompt,",
          "343:     };",
          "344:     const stream = await this.completionWithRetry(params, options, true);",
          "346:       const choice = data?.choices[0];",
          "347:       if (!choice) {",
          "348:         continue;",
          "349:       }",
          "350:       const chunk = new GenerationChunk({",
          "352:         generationInfo: {",
          "354:           tokenUsage: data.usage,",
          "355:         },",
          "356:       });",
          "",
          "[Removed Lines]",
          "345:     for await (const data of stream) {",
          "351:         text: choice.delta.content ?? \"\",",
          "353:           finishReason: choice.finish_reason,",
          "",
          "[Added Lines]",
          "445:     for await (const message of stream) {",
          "446:       const { data } = message;",
          "451:       let text = choice.delta.content ?? \"\";",
          "452:       if (Array.isArray(text)) {",
          "453:         text = text[0].type === \"text\" ? text[0].text : \"\";",
          "454:       }",
          "456:         text,",
          "458:           finishReason: choice.finishReason,",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "363:     }",
          "364:   }",
          "367:   private async imports() {",
          "370:   }",
          "371: }",
          "",
          "[Removed Lines]",
          "368:     const { default: MistralClient } = await import(\"@mistralai/mistralai\");",
          "369:     return { MistralClient };",
          "",
          "[Added Lines]",
          "471:   addAllHooksToHttpClient() {",
          "472:     try {",
          "474:       this.removeAllHooksFromHttpClient();",
          "477:       const hasHooks = [",
          "478:         this.beforeRequestHooks,",
          "479:         this.requestErrorHooks,",
          "480:         this.responseHooks,",
          "481:       ].some((hook) => hook && hook.length > 0);",
          "482:       if (hasHooks && !this.httpClient) {",
          "483:         this.httpClient = new MistralAIHTTPClient();",
          "484:       }",
          "486:       if (this.beforeRequestHooks) {",
          "487:         for (const hook of this.beforeRequestHooks) {",
          "488:           this.httpClient?.addHook(\"beforeRequest\", hook);",
          "489:         }",
          "490:       }",
          "492:       if (this.requestErrorHooks) {",
          "493:         for (const hook of this.requestErrorHooks) {",
          "494:           this.httpClient?.addHook(\"requestError\", hook);",
          "495:         }",
          "496:       }",
          "498:       if (this.responseHooks) {",
          "499:         for (const hook of this.responseHooks) {",
          "500:           this.httpClient?.addHook(\"response\", hook);",
          "501:         }",
          "502:       }",
          "503:     } catch {",
          "504:       throw new Error(\"Error in adding all hooks\");",
          "505:     }",
          "506:   }",
          "508:   removeAllHooksFromHttpClient() {",
          "509:     try {",
          "510:       if (this.beforeRequestHooks) {",
          "511:         for (const hook of this.beforeRequestHooks) {",
          "512:           this.httpClient?.removeHook(\"beforeRequest\", hook);",
          "513:         }",
          "514:       }",
          "516:       if (this.requestErrorHooks) {",
          "517:         for (const hook of this.requestErrorHooks) {",
          "518:           this.httpClient?.removeHook(\"requestError\", hook);",
          "519:         }",
          "520:       }",
          "522:       if (this.responseHooks) {",
          "523:         for (const hook of this.responseHooks) {",
          "524:           this.httpClient?.removeHook(\"response\", hook);",
          "525:         }",
          "526:       }",
          "527:     } catch {",
          "528:       throw new Error(\"Error in removing hooks\");",
          "529:     }",
          "530:   }",
          "532:   removeHookFromHttpClient(",
          "533:     hook: BeforeRequestHook | RequestErrorHook | ResponseHook",
          "534:   ) {",
          "535:     try {",
          "536:       this.httpClient?.removeHook(\"beforeRequest\", hook as BeforeRequestHook);",
          "537:       this.httpClient?.removeHook(\"requestError\", hook as RequestErrorHook);",
          "538:       this.httpClient?.removeHook(\"response\", hook as ResponseHook);",
          "539:     } catch {",
          "540:       throw new Error(\"Error in removing hook\");",
          "541:     }",
          "542:   }",
          "546:     const { Mistral } = await import(\"@mistralai/mistralai\");",
          "547:     return { Mistral };",
          "",
          "---------------"
        ],
        "libs/langchain-mistralai/src/tests/chat_models.int.test.ts||libs/langchain-mistralai/src/tests/chat_models.int.test.ts": [
          "File: libs/langchain-mistralai/src/tests/chat_models.int.test.ts -> libs/langchain-mistralai/src/tests/chat_models.int.test.ts",
          "--- Hunk 1 ---",
          "[Context before]",
          "5: import {",
          "6:   AIMessage,",
          "7:   AIMessageChunk,",
          "9:   HumanMessage,",
          "10:   ToolMessage,",
          "11: } from \"@langchain/core/messages\";",
          "12: import { zodToJsonSchema } from \"zod-to-json-schema\";",
          "13: import { ChatMistralAI } from \"../chat_models.js\";",
          "16:   const model = new ChatMistralAI({",
          "17:     model: \"mistral-tiny\",",
          "18:   });",
          "",
          "[Removed Lines]",
          "8:   BaseMessage,",
          "15: test(\"Test ChatMistralAI can invoke\", async () => {",
          "",
          "[Added Lines]",
          "9:   SystemMessage,",
          "12: import { ContentChunk as MistralAIContentChunk } from \"@mistralai/mistralai/models/components/contentchunk.js\";",
          "13: import { HTTPClient } from \"@mistralai/mistralai/lib/http.js\";",
          "16: import { _mistralContentChunkToMessageContentComplex } from \"../utils.js\";",
          "18: test(\"Test ChatMistralAI can invoke hello\", async () => {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "81:   const chain = prompt.pipe(model);",
          "82:   const response = await chain.invoke({});",
          "93: });",
          "96:   const tools = [",
          "97:     {",
          "98:       type: \"function\",",
          "",
          "[Removed Lines]",
          "83:   expect(\"tool_calls\" in response.additional_kwargs).toBe(true);",
          "85:   expect(response.additional_kwargs.tool_calls?.[0].function.name).toBe(",
          "86:     \"calculator\"",
          "87:   );",
          "88:   expect(",
          "89:     JSON.parse(",
          "90:       response.additional_kwargs.tool_calls?.[0].function.arguments ?? \"{}\"",
          "91:     ).calculator",
          "92:   ).toBeDefined();",
          "95: test(\"Can call tools\", async () => {",
          "",
          "[Added Lines]",
          "86:   expect(\"tool_calls\" in response).toBe(true);",
          "88:   expect(response.tool_calls?.[0].name).toBe(\"calculator\");",
          "89:   expect(response.tool_calls?.[0].args?.calculator).toBeDefined();",
          "92: test(\"Can call tools using raw tools\", async () => {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "130:   const response = await chain.invoke({});",
          "132:   expect(response.tool_calls?.length).toEqual(1);",
          "147: });",
          "149: test(\"Can call .stream with tool calling\", async () => {",
          "",
          "[Removed Lines]",
          "133:   expect(response.tool_calls?.[0].args).toEqual(",
          "134:     JSON.parse(",
          "135:       response.additional_kwargs.tool_calls?.[0].function.arguments ?? \"{}\"",
          "136:     )",
          "137:   );",
          "138:   expect(\"tool_calls\" in response.additional_kwargs).toBe(true);",
          "139:   expect(response.additional_kwargs.tool_calls?.[0].function.name).toBe(",
          "140:     \"calculator\"",
          "141:   );",
          "142:   expect(",
          "143:     JSON.parse(",
          "144:       response.additional_kwargs.tool_calls?.[0].function.arguments ?? \"{}\"",
          "145:     ).calculator",
          "146:   ).toBeDefined();",
          "",
          "[Added Lines]",
          "130:   expect(response.tool_calls?.[0].name).toBe(\"calculator\");",
          "131:   expect(response.tool_calls?.[0].args?.calculator).toBeDefined();",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "180:   const chain = prompt.pipe(model);",
          "181:   const response = await chain.stream({});",
          "183:   for await (const chunk of response) {",
          "185:     finalRes = chunk;",
          "",
          "[Removed Lines]",
          "182:   let finalRes: BaseMessage | null = null;",
          "",
          "[Added Lines]",
          "167:   let finalRes: AIMessageChunk | null = null;",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "188:     throw new Error(\"No final response found\");",
          "189:   }",
          "201: });",
          "203: test(\"Can use json mode response format\", async () => {",
          "",
          "[Removed Lines]",
          "191:   expect(\"tool_calls\" in finalRes.additional_kwargs).toBe(true);",
          "193:   expect(finalRes.additional_kwargs.tool_calls?.[0].function.name).toBe(",
          "194:     \"calculator\"",
          "195:   );",
          "196:   expect(",
          "197:     JSON.parse(",
          "198:       finalRes.additional_kwargs.tool_calls?.[0].function.arguments ?? \"{}\"",
          "199:     ).calculator",
          "200:   ).toBeDefined();",
          "",
          "[Added Lines]",
          "176:   expect(\"tool_calls\" in finalRes).toBe(true);",
          "178:   expect(finalRes.tool_calls?.[0].name).toBe(\"calculator\");",
          "179:   expect(finalRes.tool_calls?.[0].args.calculator).toBeDefined();",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "303:   const chain = prompt.pipe(model);",
          "304:   const response = await chain.stream({});",
          "306:   for await (const chunk of response) {",
          "308:     finalRes = finalRes.concat(chunk);",
          "",
          "[Removed Lines]",
          "305:   let finalRes: BaseMessage[] = [];",
          "",
          "[Added Lines]",
          "284:   let finalRes: AIMessageChunk[] = [];",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "311:     throw new Error(\"No final response found\");",
          "312:   }",
          "319:   expect(person).toBeDefined();",
          "320:   expect(person.name).toBeDefined();",
          "321:   expect(person.age).toBeDefined();",
          "",
          "[Removed Lines]",
          "314:   expect(finalRes[0].additional_kwargs.tool_calls?.[0]).toBeDefined();",
          "315:   const toolCall = finalRes[0].additional_kwargs.tool_calls?.[0];",
          "316:   expect(toolCall?.function.name).toBe(\"person_traits\");",
          "317:   const args = JSON.parse(toolCall?.function.arguments ?? \"{}\");",
          "318:   const { person } = args;",
          "",
          "[Added Lines]",
          "293:   expect(finalRes[0].tool_calls?.[0]).toBeDefined();",
          "294:   const toolCall = finalRes[0].tool_calls?.[0];",
          "295:   expect(toolCall?.name).toBe(\"person_traits\");",
          "296:   const person = toolCall?.args?.person;",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "406:     ]);",
          "407:     const chain = prompt.pipe(modelWithStructuredOutput);",
          "408:     const result = await chain.invoke({});",
          "410:     expect(\"operation\" in result).toBe(true);",
          "411:     expect(\"number1\" in result).toBe(true);",
          "412:     expect(\"number2\" in result).toBe(true);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "387:     console.log(result);",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "609:       throw new Error(\"raw not in result\");",
          "610:     }",
          "611:     const { raw } = result as { raw: AIMessage };",
          "634:   });",
          "635: });",
          "",
          "[Removed Lines]",
          "612:     expect(raw.additional_kwargs.tool_calls?.length).toBeGreaterThan(0);",
          "613:     expect(raw.additional_kwargs.tool_calls?.[0].function.name).toBe(",
          "614:       \"calculator\"",
          "615:     );",
          "616:     expect(",
          "617:       \"operation\" in",
          "618:         JSON.parse(",
          "619:           raw.additional_kwargs.tool_calls?.[0].function.arguments ?? \"\"",
          "620:         )",
          "621:     ).toBe(true);",
          "622:     expect(",
          "623:       \"number1\" in",
          "624:         JSON.parse(",
          "625:           raw.additional_kwargs.tool_calls?.[0].function.arguments ?? \"\"",
          "626:         )",
          "627:     ).toBe(true);",
          "628:     expect(",
          "629:       \"number2\" in",
          "630:         JSON.parse(",
          "631:           raw.additional_kwargs.tool_calls?.[0].function.arguments ?? \"\"",
          "632:         )",
          "633:     ).toBe(true);",
          "",
          "[Added Lines]",
          "590:     expect(raw.tool_calls?.length).toBeGreaterThan(0);",
          "591:     expect(raw.tool_calls?.[0].name).toBe(\"calculator\");",
          "592:     expect(\"operation\" in (raw.tool_calls?.[0]?.args ?? {})).toBe(true);",
          "593:     expect(\"number1\" in (raw.tool_calls?.[0]?.args ?? {})).toBe(true);",
          "594:     expect(\"number2\" in (raw.tool_calls?.[0]?.args ?? {})).toBe(true);",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "812:     expect(fullMessage.toLowerCase()).toContain(\"world\");",
          "813:   });",
          "816:     class CodeSandbox extends StructuredTool {",
          "817:       name = \"code_sandbox\";",
          "",
          "[Removed Lines]",
          "815:   test(\"Can call tools using structured tools codestral-latest\", async () => {",
          "",
          "[Added Lines]",
          "776:   test(\"Can call tools using codestral-latest structured tools\", async () => {",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "850:         \"Write a function that takes in a single argument and logs it to the console. Ensure the code is in Python.\",",
          "851:     });",
          "856:       throw new Error(\"No tool call found\");",
          "857:     }",
          "863:   });",
          "864: });",
          "",
          "[Removed Lines]",
          "853:     expect(\"tool_calls\" in response.additional_kwargs).toBe(true);",
          "855:     if (!response.additional_kwargs.tool_calls?.[0]) {",
          "858:     const sandboxTool = response.additional_kwargs.tool_calls[0];",
          "859:     expect(sandboxTool.function.name).toBe(\"code_sandbox\");",
          "860:     const parsedArgs = JSON.parse(sandboxTool.function.arguments);",
          "861:     expect(parsedArgs.code).toBeDefined();",
          "",
          "[Added Lines]",
          "814:     expect(\"tool_calls\" in response).toBe(true);",
          "816:     if (!response.tool_calls?.[0]) {",
          "819:     const sandboxTool = response.tool_calls[0];",
          "820:     expect(sandboxTool.name).toBe(\"code_sandbox\");",
          "821:     expect(sandboxTool.args?.code).toBeDefined();",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "957:   const castMessage = response.raw as AIMessage;",
          "958:   expect(castMessage.tool_calls).toHaveLength(1);",
          "959: });",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "921: test(\"Test ChatMistralAI can invoke with MessageContent input types\", async () => {",
          "922:   const model = new ChatMistralAI({",
          "923:     model: \"pixtral-12b-2409\",",
          "924:   });",
          "925:   const messagesListContent = [",
          "926:     new SystemMessage({",
          "927:       content: \"List the top 5 countries in Europe with the highest GDP\",",
          "928:     }),",
          "929:     new HumanMessage({",
          "930:       content: [",
          "931:         {",
          "932:           type: \"text\",",
          "933:           text: \"Here is an infographic with European GPDs\",",
          "934:         },",
          "935:         {",
          "936:           type: \"image_url\",",
          "937:           image_url: \"https://mistral.ai/images/news/pixtral-12b/gdp.png\",",
          "938:         },",
          "939:       ],",
          "940:     }),",
          "941:   ];",
          "942:   const response = await model.invoke(messagesListContent);",
          "943:   console.log(\"response\", response);",
          "944:   expect(response.content.length).toBeGreaterThan(1);",
          "945: });",
          "947: test(\"Mistral ContentChunk to MessageContentComplex conversion\", () => {",
          "948:   const mistralMessages = [",
          "949:     {",
          "950:       type: \"text\",",
          "951:       text: \"Test message\",",
          "952:     },",
          "953:     {",
          "954:       type: \"image_url\",",
          "955:       imageUrl: \"https://mistral.ai/images/news/pixtral-12b/gdp.png\",",
          "956:     },",
          "957:     {",
          "958:       type: \"image_url\",",
          "959:       imageUrl: {",
          "960:         url: \"https://mistral.ai/images/news/pixtral-12b/gdp.png\",",
          "961:         detail: \"high\",",
          "962:       },",
          "963:     },",
          "964:     {",
          "965:       type: \"image_url\",",
          "966:       imageUrl: {",
          "967:         url: \"https://mistral.ai/images/news/pixtral-12b/gdp.png\",",
          "968:         detail: \"medium\",",
          "969:       },",
          "970:     },",
          "971:     {",
          "972:       type: \"image_url\",",
          "973:       imageUrl: {",
          "974:         url: \"https://mistral.ai/images/news/pixtral-12b/gdp.png\",",
          "975:       },",
          "976:     },",
          "977:   ] as MistralAIContentChunk[];",
          "979:   expect(_mistralContentChunkToMessageContentComplex(mistralMessages)).toEqual([",
          "980:     {",
          "981:       type: \"text\",",
          "982:       text: \"Test message\",",
          "983:     },",
          "984:     {",
          "985:       type: \"image_url\",",
          "986:       image_url: \"https://mistral.ai/images/news/pixtral-12b/gdp.png\",",
          "987:     },",
          "988:     {",
          "989:       type: \"image_url\",",
          "990:       image_url: {",
          "991:         url: \"https://mistral.ai/images/news/pixtral-12b/gdp.png\",",
          "992:         detail: \"high\",",
          "993:       },",
          "994:     },",
          "995:     {",
          "996:       type: \"image_url\",",
          "997:       image_url: {",
          "998:         url: \"https://mistral.ai/images/news/pixtral-12b/gdp.png\",",
          "999:       },",
          "1000:     },",
          "1001:     {",
          "1002:       type: \"image_url\",",
          "1003:       image_url: {",
          "1004:         url: \"https://mistral.ai/images/news/pixtral-12b/gdp.png\",",
          "1005:       },",
          "1006:     },",
          "1007:   ]);",
          "1008: });",
          "1010: test(\"Test ChatMistralAI can register BeforeRequestHook function\", async () => {",
          "1011:   const model = new ChatMistralAI({",
          "1012:     model: \"mistral-tiny\",",
          "1013:   });",
          "1014:   const prompt = ChatPromptTemplate.fromMessages([",
          "1015:     [\"system\", \"You are a helpful assistant\"],",
          "1016:     [\"human\", \"{input}\"],",
          "1017:   ]);",
          "1019:   let count = 0;",
          "1020:   const addCount = () => {",
          "1021:     count += 1;",
          "1022:   };",
          "1024:   const beforeRequestHook = (): void => {",
          "1025:     addCount();",
          "1026:   };",
          "1027:   model.beforeRequestHooks = [beforeRequestHook];",
          "1028:   model.addAllHooksToHttpClient();",
          "1030:   await prompt.pipe(model).invoke({",
          "1031:     input: \"Hello\",",
          "1032:   });",
          "1034:   expect(count).toEqual(1);",
          "1035: });",
          "1037: test(\"Test ChatMistralAI can register RequestErrorHook function\", async () => {",
          "1038:   const fetcher = (): Promise<Response> =>",
          "1039:     Promise.reject(new Error(\"Intended fetcher error\"));",
          "1040:   const customHttpClient = new HTTPClient({ fetcher });",
          "1042:   const model = new ChatMistralAI({",
          "1043:     model: \"mistral-tiny\",",
          "1044:     httpClient: customHttpClient,",
          "1045:     maxRetries: 0,",
          "1046:   });",
          "1047:   const prompt = ChatPromptTemplate.fromMessages([",
          "1048:     [\"system\", \"You are a helpful assistant\"],",
          "1049:     [\"human\", \"{input}\"],",
          "1050:   ]);",
          "1052:   let count = 0;",
          "1053:   const addCount = () => {",
          "1054:     count += 1;",
          "1055:   };",
          "1057:   const RequestErrorHook = (): void => {",
          "1058:     addCount();",
          "1059:     console.log(\"In request error hook\");",
          "1060:   };",
          "1061:   model.requestErrorHooks = [RequestErrorHook];",
          "1062:   model.addAllHooksToHttpClient();",
          "1064:   try {",
          "1065:     await prompt.pipe(model).invoke({",
          "1066:       input: \"Hello\",",
          "1067:     });",
          "1068:   } catch (e: unknown) {",
          "1070:   }",
          "1073:   expect(count).toEqual(1);",
          "1074: });",
          "1076: test(\"Test ChatMistralAI can register ResponseHook function\", async () => {",
          "1077:   const model = new ChatMistralAI({",
          "1078:     model: \"mistral-tiny\",",
          "1079:   });",
          "1080:   const prompt = ChatPromptTemplate.fromMessages([",
          "1081:     [\"system\", \"You are a helpful assistant\"],",
          "1082:     [\"human\", \"{input}\"],",
          "1083:   ]);",
          "1085:   let count = 0;",
          "1086:   const addCount = () => {",
          "1087:     count += 1;",
          "1088:   };",
          "1090:   const ResponseHook = (): void => {",
          "1091:     addCount();",
          "1092:   };",
          "1093:   model.responseHooks = [ResponseHook];",
          "1094:   model.addAllHooksToHttpClient();",
          "1096:   await prompt.pipe(model).invoke({",
          "1097:     input: \"Hello\",",
          "1098:   });",
          "1100:   expect(count).toEqual(1);",
          "1101: });",
          "1103: test(\"Test ChatMistralAI can register multiple hook functions with success\", async () => {",
          "1104:   const model = new ChatMistralAI({",
          "1105:     model: \"mistral-tiny\",",
          "1106:   });",
          "1107:   const prompt = ChatPromptTemplate.fromMessages([",
          "1108:     [\"system\", \"You are a helpful assistant\"],",
          "1109:     [\"human\", \"{input}\"],",
          "1110:   ]);",
          "1112:   let count = 0;",
          "1113:   const addCount = () => {",
          "1114:     count += 1;",
          "1115:   };",
          "1117:   const beforeRequestHook = (): void => {",
          "1118:     addCount();",
          "1119:   };",
          "1120:   const ResponseHook = (): void => {",
          "1121:     addCount();",
          "1122:   };",
          "1123:   model.beforeRequestHooks = [beforeRequestHook];",
          "1124:   model.responseHooks = [ResponseHook];",
          "1125:   model.addAllHooksToHttpClient();",
          "1127:   await prompt.pipe(model).invoke({",
          "1128:     input: \"Hello\",",
          "1129:   });",
          "1131:   expect(count).toEqual(2);",
          "1132: });",
          "1134: test(\"Test ChatMistralAI can register multiple hook functions with error\", async () => {",
          "1135:   const fetcher = (): Promise<Response> =>",
          "1136:     Promise.reject(new Error(\"Intended fetcher error\"));",
          "1137:   const customHttpClient = new HTTPClient({ fetcher });",
          "1139:   const model = new ChatMistralAI({",
          "1140:     model: \"mistral-tiny\",",
          "1141:     httpClient: customHttpClient,",
          "1142:     maxRetries: 0,",
          "1143:   });",
          "1145:   const prompt = ChatPromptTemplate.fromMessages([",
          "1146:     [\"system\", \"You are a helpful assistant\"],",
          "1147:     [\"human\", \"{input}\"],",
          "1148:   ]);",
          "1150:   let count = 0;",
          "1151:   const addCount = () => {",
          "1152:     count += 1;",
          "1153:   };",
          "1155:   const beforeRequestHook = (): void => {",
          "1156:     addCount();",
          "1157:   };",
          "1158:   const RequestErrorHook = (): void => {",
          "1159:     addCount();",
          "1160:   };",
          "1161:   model.beforeRequestHooks = [beforeRequestHook];",
          "1162:   model.requestErrorHooks = [RequestErrorHook];",
          "1163:   model.addAllHooksToHttpClient();",
          "1165:   try {",
          "1166:     await prompt.pipe(model).invoke({",
          "1167:       input: \"Hello\",",
          "1168:     });",
          "1169:   } catch (e: unknown) {",
          "1171:   }",
          "1173:   expect(count).toEqual(2);",
          "1174: });",
          "1176: test(\"Test ChatMistralAI can remove hook\", async () => {",
          "1177:   const model = new ChatMistralAI({",
          "1178:     model: \"mistral-tiny\",",
          "1179:   });",
          "1180:   const prompt = ChatPromptTemplate.fromMessages([",
          "1181:     [\"system\", \"You are a helpful assistant\"],",
          "1182:     [\"human\", \"{input}\"],",
          "1183:   ]);",
          "1185:   let count = 0;",
          "1186:   const addCount = () => {",
          "1187:     count += 1;",
          "1188:   };",
          "1190:   const beforeRequestHook = (): void => {",
          "1191:     addCount();",
          "1192:   };",
          "1193:   model.beforeRequestHooks = [beforeRequestHook];",
          "1194:   model.addAllHooksToHttpClient();",
          "1196:   await prompt.pipe(model).invoke({",
          "1197:     input: \"Hello\",",
          "1198:   });",
          "1200:   expect(count).toEqual(1);",
          "1202:   model.removeHookFromHttpClient(beforeRequestHook);",
          "1204:   await prompt.pipe(model).invoke({",
          "1205:     input: \"Hello\",",
          "1206:   });",
          "1208:   expect(count).toEqual(1);",
          "1209: });",
          "1211: test(\"Test ChatMistralAI can remove all hooks\", async () => {",
          "1212:   const model = new ChatMistralAI({",
          "1213:     model: \"mistral-tiny\",",
          "1214:   });",
          "1215:   const prompt = ChatPromptTemplate.fromMessages([",
          "1216:     [\"system\", \"You are a helpful assistant\"],",
          "1217:     [\"human\", \"{input}\"],",
          "1218:   ]);",
          "1220:   let count = 0;",
          "1221:   const addCount = () => {",
          "1222:     count += 1;",
          "1223:   };",
          "1225:   const beforeRequestHook = (): void => {",
          "1226:     addCount();",
          "1227:   };",
          "1228:   const ResponseHook = (): void => {",
          "1229:     addCount();",
          "1230:   };",
          "1231:   model.beforeRequestHooks = [beforeRequestHook];",
          "1232:   model.responseHooks = [ResponseHook];",
          "1233:   model.addAllHooksToHttpClient();",
          "1235:   await prompt.pipe(model).invoke({",
          "1236:     input: \"Hello\",",
          "1237:   });",
          "1239:   expect(count).toEqual(2);",
          "1241:   model.removeAllHooksFromHttpClient();",
          "1243:   await prompt.pipe(model).invoke({",
          "1244:     input: \"Hello\",",
          "1245:   });",
          "1247:   expect(count).toEqual(2);",
          "1248: });",
          "",
          "---------------"
        ],
        "libs/langchain-mistralai/src/tests/chat_models.standard.int.test.ts||libs/langchain-mistralai/src/tests/chat_models.standard.int.test.ts": [
          "File: libs/langchain-mistralai/src/tests/chat_models.standard.int.test.ts -> libs/langchain-mistralai/src/tests/chat_models.standard.int.test.ts",
          "--- Hunk 1 ---",
          "[Context before]",
          "24:     });",
          "25:   }",
          "28:     this.skipTestMessage(",
          "30:       \"ChatMistralAI\",",
          "32:     );",
          "33:   }",
          "34: }",
          "",
          "[Removed Lines]",
          "27:   async testCacheComplexMessageTypes() {",
          "29:       \"testCacheComplexMessageTypes\",",
          "31:       \"Complex message types not properly implemented\"",
          "",
          "[Added Lines]",
          "27:   async testToolMessageHistoriesListContent() {",
          "29:       \"testToolMessageHistoriesListContent\",",
          "31:       \"tool_use message blocks not supported\"",
          "",
          "---------------"
        ],
        "libs/langchain-mistralai/src/tests/chat_models.test.ts||libs/langchain-mistralai/src/tests/chat_models.test.ts": [
          "File: libs/langchain-mistralai/src/tests/chat_models.test.ts -> libs/langchain-mistralai/src/tests/chat_models.test.ts",
          "--- Hunk 1 ---",
          "[Context before]",
          "2: import {",
          "3:   _isValidMistralToolCallId,",
          "4:   _convertToolCallIdToMistralCompatible,",
          "5: } from \"../utils.js\";",
          "7: describe(\"Mistral Tool Call ID Conversion\", () => {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "5:   _mistralContentChunkToMessageContentComplex,",
          "",
          "---------------"
        ],
        "libs/langchain-mistralai/src/tests/embeddings.int.test.ts||libs/langchain-mistralai/src/tests/embeddings.int.test.ts": [
          "File: libs/langchain-mistralai/src/tests/embeddings.int.test.ts -> libs/langchain-mistralai/src/tests/embeddings.int.test.ts",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: import { test } from \"@jest/globals\";",
          "2: import { MistralAIEmbeddings } from \"../embeddings.js\";",
          "4: test(\"Test MistralAIEmbeddings can embed query\", async () => {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2: import { HTTPClient } from \"@mistralai/mistralai/lib/http.js\";",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "21:   expect(embeddings[0].length).toBe(1024);",
          "22:   expect(embeddings[1].length).toBe(1024);",
          "23: });",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: test(\"Test MistralAIEmbeddings can register BeforeRequestHook function\", async () => {",
          "27:   const model = new MistralAIEmbeddings({",
          "28:     model: \"mistral-embed\",",
          "29:   });",
          "31:   let count = 0;",
          "32:   const addCount = () => {",
          "33:     count += 1;",
          "34:   };",
          "36:   const beforeRequestHook = (): void => {",
          "37:     addCount();",
          "38:   };",
          "39:   model.beforeRequestHooks = [beforeRequestHook];",
          "40:   model.addAllHooksToHttpClient();",
          "42:   await model.embedQuery(\"Hello\");",
          "44:   expect(count).toEqual(1);",
          "45: });",
          "47: test(\"Test MistralAIEmbeddings can register RequestErrorHook function\", async () => {",
          "48:   const fetcher = (): Promise<Response> =>",
          "49:     Promise.reject(new Error(\"Intended fetcher error\"));",
          "50:   const customHttpClient = new HTTPClient({ fetcher });",
          "52:   const model = new MistralAIEmbeddings({",
          "53:     model: \"mistral-embed\",",
          "54:     httpClient: customHttpClient,",
          "55:     maxRetries: 0,",
          "56:   });",
          "58:   let count = 0;",
          "59:   const addCount = () => {",
          "60:     count += 1;",
          "61:   };",
          "63:   const RequestErrorHook = (): void => {",
          "64:     addCount();",
          "65:     console.log(\"In request error hook\");",
          "66:   };",
          "67:   model.requestErrorHooks = [RequestErrorHook];",
          "68:   model.addAllHooksToHttpClient();",
          "70:   try {",
          "71:     await model.embedQuery(\"Hello\");",
          "72:   } catch (e: unknown) {",
          "74:   }",
          "77:   expect(count).toEqual(1);",
          "78: });",
          "80: test(\"Test MistralAIEmbeddings can register ResponseHook function\", async () => {",
          "81:   const model = new MistralAIEmbeddings({",
          "82:     model: \"mistral-embed\",",
          "83:   });",
          "85:   let count = 0;",
          "86:   const addCount = () => {",
          "87:     count += 1;",
          "88:   };",
          "90:   const ResponseHook = (): void => {",
          "91:     addCount();",
          "92:   };",
          "93:   model.responseHooks = [ResponseHook];",
          "94:   model.addAllHooksToHttpClient();",
          "96:   await model.embedQuery(\"Hello\");",
          "98:   expect(count).toEqual(1);",
          "99: });",
          "101: test(\"Test MistralAIEmbeddings can register multiple hook functions with success\", async () => {",
          "102:   const model = new MistralAIEmbeddings({",
          "103:     model: \"mistral-embed\",",
          "104:   });",
          "106:   let count = 0;",
          "107:   const addCount = () => {",
          "108:     count += 1;",
          "109:   };",
          "111:   const beforeRequestHook = (): void => {",
          "112:     addCount();",
          "113:   };",
          "114:   const ResponseHook = (): void => {",
          "115:     addCount();",
          "116:   };",
          "117:   model.beforeRequestHooks = [beforeRequestHook];",
          "118:   model.responseHooks = [ResponseHook];",
          "119:   model.addAllHooksToHttpClient();",
          "121:   await model.embedQuery(\"Hello\");",
          "123:   expect(count).toEqual(2);",
          "124: });",
          "126: test(\"Test MistralAIEmbeddings can register multiple hook functions with error\", async () => {",
          "127:   const fetcher = (): Promise<Response> =>",
          "128:     Promise.reject(new Error(\"Intended fetcher error\"));",
          "129:   const customHttpClient = new HTTPClient({ fetcher });",
          "131:   const model = new MistralAIEmbeddings({",
          "132:     model: \"mistral-embed\",",
          "133:     httpClient: customHttpClient,",
          "134:     maxRetries: 0,",
          "135:   });",
          "137:   let count = 0;",
          "138:   const addCount = () => {",
          "139:     count += 1;",
          "140:   };",
          "142:   const beforeRequestHook = (): void => {",
          "143:     addCount();",
          "144:   };",
          "145:   const RequestErrorHook = (): void => {",
          "146:     addCount();",
          "147:   };",
          "148:   model.beforeRequestHooks = [beforeRequestHook];",
          "149:   model.requestErrorHooks = [RequestErrorHook];",
          "150:   model.addAllHooksToHttpClient();",
          "152:   try {",
          "153:     await model.embedQuery(\"Hello\");",
          "154:   } catch (e: unknown) {",
          "156:   }",
          "158:   expect(count).toEqual(2);",
          "159: });",
          "161: test(\"Test MistralAIEmbeddings can remove hook\", async () => {",
          "162:   const model = new MistralAIEmbeddings({",
          "163:     model: \"mistral-embed\",",
          "164:   });",
          "166:   let count = 0;",
          "167:   const addCount = () => {",
          "168:     count += 1;",
          "169:   };",
          "171:   const beforeRequestHook = (): void => {",
          "172:     addCount();",
          "173:   };",
          "174:   model.beforeRequestHooks = [beforeRequestHook];",
          "175:   model.addAllHooksToHttpClient();",
          "177:   await model.embedQuery(\"Hello\");",
          "179:   expect(count).toEqual(1);",
          "181:   model.removeHookFromHttpClient(beforeRequestHook);",
          "183:   await model.embedQuery(\"Hello\");",
          "185:   expect(count).toEqual(1);",
          "186: });",
          "188: test(\"Test MistralAIEmbeddings can remove all hooks\", async () => {",
          "189:   const model = new MistralAIEmbeddings({",
          "190:     model: \"mistral-embed\",",
          "191:   });",
          "193:   let count = 0;",
          "194:   const addCount = () => {",
          "195:     count += 1;",
          "196:   };",
          "198:   const beforeRequestHook = (): void => {",
          "199:     addCount();",
          "200:   };",
          "201:   const ResponseHook = (): void => {",
          "202:     addCount();",
          "203:   };",
          "204:   model.beforeRequestHooks = [beforeRequestHook];",
          "205:   model.responseHooks = [ResponseHook];",
          "206:   model.addAllHooksToHttpClient();",
          "208:   await model.embedQuery(\"Hello\");",
          "210:   expect(count).toEqual(2);",
          "212:   model.removeAllHooksFromHttpClient();",
          "214:   await model.embedQuery(\"Hello\");",
          "216:   expect(count).toEqual(2);",
          "217: });",
          "",
          "---------------"
        ],
        "libs/langchain-mistralai/src/tests/llms.int.test.ts||libs/langchain-mistralai/src/tests/llms.int.test.ts": [
          "File: libs/langchain-mistralai/src/tests/llms.int.test.ts -> libs/langchain-mistralai/src/tests/llms.int.test.ts",
          "--- Hunk 1 ---",
          "[Context before]",
          "3: import { test, expect } from \"@jest/globals\";",
          "4: import { CallbackManager } from \"@langchain/core/callbacks/manager\";",
          "5: import { MistralAI } from \"../llms.js\";",
          "8: const originalBackground = process.env.LANGCHAIN_CALLBACKS_BACKGROUND;",
          "11:   const model = new MistralAI({",
          "12:     maxTokens: 5,",
          "13:     model: \"codestral-latest\",",
          "",
          "[Removed Lines]",
          "10: test(\"Test MistralAI\", async () => {",
          "",
          "[Added Lines]",
          "5: import { HTTPClient } from \"@mistralai/mistralai/lib/http.js\";",
          "11: test(\"Test MistralAI default\", async () => {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "173:   }",
          "174:   expect(i).toBeGreaterThan(5);",
          "175: });",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "178: test(\"Test MistralAI can register BeforeRequestHook function\", async () => {",
          "179:   const model = new MistralAI({",
          "180:     model: \"codestral-latest\",",
          "181:   });",
          "183:   let count = 0;",
          "184:   const addCount = () => {",
          "185:     count += 1;",
          "186:   };",
          "188:   const beforeRequestHook = (): void => {",
          "189:     addCount();",
          "190:   };",
          "191:   model.beforeRequestHooks = [beforeRequestHook];",
          "192:   model.addAllHooksToHttpClient();",
          "194:   await model.invoke(\"Log 'Hello world' to the console in javascript: .\");",
          "196:   expect(count).toEqual(1);",
          "197: });",
          "199: test(\"Test MistralAI can register RequestErrorHook function\", async () => {",
          "200:   const fetcher = (): Promise<Response> =>",
          "201:     Promise.reject(new Error(\"Intended fetcher error\"));",
          "202:   const customHttpClient = new HTTPClient({ fetcher });",
          "204:   const model = new MistralAI({",
          "205:     model: \"codestral-latest\",",
          "206:     httpClient: customHttpClient,",
          "207:     maxRetries: 0,",
          "208:   });",
          "210:   let count = 0;",
          "211:   const addCount = () => {",
          "212:     count += 1;",
          "213:   };",
          "215:   const RequestErrorHook = (): void => {",
          "216:     addCount();",
          "217:     console.log(\"In request error hook\");",
          "218:   };",
          "219:   model.requestErrorHooks = [RequestErrorHook];",
          "220:   model.addAllHooksToHttpClient();",
          "222:   try {",
          "223:     await model.invoke(\"Log 'Hello world' to the console in javascript: .\");",
          "224:   } catch (e: unknown) {",
          "226:   }",
          "229:   expect(count).toEqual(1);",
          "230: });",
          "232: test(\"Test MistralAI can register ResponseHook function\", async () => {",
          "233:   const model = new MistralAI({",
          "234:     model: \"codestral-latest\",",
          "235:   });",
          "237:   let count = 0;",
          "238:   const addCount = () => {",
          "239:     count += 1;",
          "240:   };",
          "242:   const ResponseHook = (): void => {",
          "243:     addCount();",
          "244:   };",
          "245:   model.responseHooks = [ResponseHook];",
          "246:   model.addAllHooksToHttpClient();",
          "248:   await model.invoke(\"Log 'Hello world' to the console in javascript: .\");",
          "250:   expect(count).toEqual(1);",
          "251: });",
          "253: test(\"Test MistralAI can register multiple hook functions with success\", async () => {",
          "254:   const model = new MistralAI({",
          "255:     model: \"codestral-latest\",",
          "256:   });",
          "258:   let count = 0;",
          "259:   const addCount = () => {",
          "260:     count += 1;",
          "261:   };",
          "263:   const beforeRequestHook = (): void => {",
          "264:     addCount();",
          "265:   };",
          "266:   const ResponseHook = (): void => {",
          "267:     addCount();",
          "268:   };",
          "269:   model.beforeRequestHooks = [beforeRequestHook];",
          "270:   model.responseHooks = [ResponseHook];",
          "271:   model.addAllHooksToHttpClient();",
          "273:   await model.invoke(\"Log 'Hello world' to the console in javascript: \");",
          "275:   expect(count).toEqual(2);",
          "276: });",
          "278: test(\"Test MistralAI can register multiple hook functions with error\", async () => {",
          "279:   const fetcher = (): Promise<Response> =>",
          "280:     Promise.reject(new Error(\"Intended fetcher error\"));",
          "281:   const customHttpClient = new HTTPClient({ fetcher });",
          "283:   const model = new MistralAI({",
          "284:     model: \"codestral-latest\",",
          "285:     httpClient: customHttpClient,",
          "286:     maxRetries: 0,",
          "287:   });",
          "289:   let count = 0;",
          "290:   const addCount = () => {",
          "291:     count += 1;",
          "292:   };",
          "294:   const beforeRequestHook = (): void => {",
          "295:     addCount();",
          "296:   };",
          "297:   const RequestErrorHook = (): void => {",
          "298:     addCount();",
          "299:   };",
          "300:   model.beforeRequestHooks = [beforeRequestHook];",
          "301:   model.requestErrorHooks = [RequestErrorHook];",
          "302:   model.addAllHooksToHttpClient();",
          "304:   try {",
          "305:     await model.invoke(\"Log 'Hello world' to the console in javascript: \");",
          "306:   } catch (e: unknown) {",
          "308:   }",
          "310:   expect(count).toEqual(2);",
          "311: });",
          "313: test(\"Test MistralAI can remove hook\", async () => {",
          "314:   const model = new MistralAI({",
          "315:     model: \"codestral-latest\",",
          "316:   });",
          "318:   let count = 0;",
          "319:   const addCount = () => {",
          "320:     count += 1;",
          "321:   };",
          "323:   const beforeRequestHook = (): void => {",
          "324:     addCount();",
          "325:   };",
          "326:   model.beforeRequestHooks = [beforeRequestHook];",
          "327:   model.addAllHooksToHttpClient();",
          "329:   await model.invoke(\"Log 'Hello world' to the console in javascript: \");",
          "331:   expect(count).toEqual(1);",
          "333:   model.removeHookFromHttpClient(beforeRequestHook);",
          "335:   await model.invoke(\"Log 'Hello world' to the console in javascript: \");",
          "337:   expect(count).toEqual(1);",
          "338: });",
          "340: test(\"Test MistralAI can remove all hooks\", async () => {",
          "341:   const model = new MistralAI({",
          "342:     model: \"codestral-latest\",",
          "343:   });",
          "345:   let count = 0;",
          "346:   const addCount = () => {",
          "347:     count += 1;",
          "348:   };",
          "350:   const beforeRequestHook = (): void => {",
          "351:     addCount();",
          "352:   };",
          "353:   const ResponseHook = (): void => {",
          "354:     addCount();",
          "355:   };",
          "356:   model.beforeRequestHooks = [beforeRequestHook];",
          "357:   model.responseHooks = [ResponseHook];",
          "358:   model.addAllHooksToHttpClient();",
          "360:   await model.invoke(\"Log 'Hello world' to the console in javascript: \");",
          "362:   expect(count).toEqual(2);",
          "364:   model.removeAllHooksFromHttpClient();",
          "366:   await model.invoke(\"Log 'Hello world' to the console in javascript: \");",
          "368:   expect(count).toEqual(2);",
          "369: });",
          "",
          "---------------"
        ],
        "libs/langchain-mistralai/src/utils.ts||libs/langchain-mistralai/src/utils.ts": [
          "File: libs/langchain-mistralai/src/utils.ts -> libs/langchain-mistralai/src/utils.ts",
          "--- Hunk 1 ---",
          "[Context before]",
          "2: const TOOL_CALL_ID_PATTERN = /^[a-zA-Z0-9]{9}$/;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: import { ContentChunk as MistralAIContentChunk } from \"@mistralai/mistralai/models/components/contentchunk.js\";",
          "2: import { MessageContentComplex } from \"@langchain/core/messages\";",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "44:     }",
          "45:   }",
          "46: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "51: export function _mistralContentChunkToMessageContentComplex(",
          "52:   content: string | MistralAIContentChunk[] | null | undefined",
          "53: ): string | MessageContentComplex[] {",
          "54:   if (!content) {",
          "55:     return \"\";",
          "56:   }",
          "57:   if (typeof content === \"string\") {",
          "58:     return content;",
          "59:   }",
          "60:   return content.map((contentChunk) => {",
          "62:     if (contentChunk.type === \"image_url\") {",
          "63:       if (",
          "64:         typeof contentChunk.imageUrl !== \"string\" &&",
          "65:         contentChunk.imageUrl?.detail",
          "66:       ) {",
          "67:         const { detail } = contentChunk.imageUrl;",
          "70:         if (detail !== \"high\" && detail !== \"auto\" && detail !== \"low\") {",
          "71:           return {",
          "72:             type: contentChunk.type,",
          "73:             image_url: {",
          "74:               url: contentChunk.imageUrl.url,",
          "75:             },",
          "76:           };",
          "77:         }",
          "78:       }",
          "79:       return {",
          "80:         type: contentChunk.type,",
          "81:         image_url: contentChunk.imageUrl,",
          "82:       };",
          "83:     }",
          "84:     return contentChunk;",
          "85:   });",
          "86: }",
          "",
          "---------------"
        ],
        "yarn.lock||yarn.lock": [
          "File: yarn.lock -> yarn.lock",
          "--- Hunk 1 ---",
          "[Context before]",
          "12375:     \"@langchain/core\": \"workspace:*\"",
          "12376:     \"@langchain/scripts\": \">=0.1.0 <0.2.0\"",
          "12377:     \"@langchain/standard-tests\": 0.0.0",
          "12379:     \"@swc/core\": ^1.3.90",
          "12380:     \"@swc/jest\": ^0.2.29",
          "12381:     \"@tsconfig/recommended\": ^1.0.3",
          "",
          "[Removed Lines]",
          "12378:     \"@mistralai/mistralai\": ^0.4.0",
          "",
          "[Added Lines]",
          "12378:     \"@mistralai/mistralai\": ^1.3.1",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "12397:     ts-jest: ^29.1.0",
          "12398:     typescript: <5.2.0",
          "12399:     uuid: ^10.0.0",
          "12401:     zod-to-json-schema: ^3.22.4",
          "12402:   peerDependencies:",
          "12404:   languageName: unknown",
          "12405:   linkType: soft",
          "",
          "[Removed Lines]",
          "12400:     zod: ^3.22.4",
          "12403:     \"@langchain/core\": \">=0.2.21 <0.4.0\"",
          "",
          "[Added Lines]",
          "12400:     zod: ^3.23.8",
          "12403:     \"@langchain/core\": \">=0.3.7 <0.4.0\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "13150:   languageName: node",
          "13151:   linkType: hard",
          "13159:   languageName: node",
          "13160:   linkType: hard",
          "",
          "[Removed Lines]",
          "13153: \"@mistralai/mistralai@npm:^0.4.0\":",
          "13154:   version: 0.4.0",
          "13155:   resolution: \"@mistralai/mistralai@npm:0.4.0\"",
          "13156:   dependencies:",
          "13157:     node-fetch: ^2.6.7",
          "13158:   checksum: 1b03fc0b55164c02e5fb29fb2d09ebe4ad44346fc313f7fb3ab09e48f73f975763d1ac9654098d433ea17d7caa20654b2b15510822276acc9fa46db461a254a6",
          "",
          "[Added Lines]",
          "13153: \"@mistralai/mistralai@npm:^1.3.1\":",
          "13154:   version: 1.3.1",
          "13155:   resolution: \"@mistralai/mistralai@npm:1.3.1\"",
          "13156:   peerDependencies:",
          "13157:     zod: \">= 3\"",
          "13158:   checksum: 9e31a2f760706a9f54347ba2cb2b7784d4f93eb4ff5d87cc7cfac9b7a1a1816f21da2328f5f5e13c11ed8953f1d71f2a2e09d12123ac17d171c189d21b87a977",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "6d025f5f72163ddac8c0b27e2bdb15676691cc91",
      "candidate_info": {
        "commit_hash": "6d025f5f72163ddac8c0b27e2bdb15676691cc91",
        "repo": "langchain-ai/langchainjs",
        "commit_url": "https://github.com/langchain-ai/langchainjs/commit/6d025f5f72163ddac8c0b27e2bdb15676691cc91",
        "files": [
          "docs/core_docs/docs/integrations/platforms/index.mdx",
          "libs/langchain-xai/package.json",
          "yarn.lock"
        ],
        "message": "fix(docs): Add xAI to platforms page (#7170)",
        "before_after_code_files": [
          "yarn.lock||yarn.lock"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "yarn.lock||yarn.lock"
          ],
          "candidate": [
            "yarn.lock||yarn.lock"
          ]
        }
      },
      "candidate_diff": {
        "yarn.lock||yarn.lock": [
          "File: yarn.lock -> yarn.lock",
          "--- Hunk 1 ---",
          "[Context before]",
          "12851:     ts-jest: ^29.1.0",
          "12852:     typescript: <5.2.0",
          "12853:     zod: ^3.22.4",
          "12854:   peerDependencies:",
          "12855:     \"@langchain/core\": \">=0.2.21 <0.4.0\"",
          "12856:   languageName: unknown",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "12854:     zod-to-json-schema: ^3.23.1",
          "",
          "---------------"
        ]
      }
    }
  ]
}