{
  "cve_id": "CVE-2023-45348",
  "cve_desc": "Apache Airflow, versions 2.7.0 and 2.7.1, is affected by a vulnerability that allows an authenticated user to retrieve sensitive configuration information when the \"expose_config\" option is set to \"non-sensitive-only\". The `expose_config` option is False by default.\nIt is recommended to upgrade to a version that is not affected.",
  "repo": "apache/airflow",
  "patch_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
  "patch_info": {
    "commit_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
    "files": [
      "airflow/api_connexion/endpoints/config_endpoint.py",
      "airflow/configuration.py",
      "tests/api_connexion/endpoints/test_config_endpoint.py"
    ],
    "message": "Check if the lower of provided values are sensitives in config endpoint (#34712)\n\n* Check if the lower of provided values are sensitives in config endpoint\n\n* update unit test\n\n* ensure that all values in sensitive dict are in lower characters\n\n(cherry picked from commit f044589b685855a8fce8f5376bea2564c5a001f7)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py",
      "airflow/configuration.py||airflow/configuration.py",
      "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py": [
      "File: airflow/api_connexion/endpoints/config_endpoint.py -> airflow/api_connexion/endpoints/config_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "123:                 \"Config not found.\", detail=f\"The option [{section}/{option}] is not found in config.\"",
      "124:             )",
      "127:             value = \"< hidden >\"",
      "128:         else:",
      "129:             value = conf.get(section, option)",
      "",
      "[Removed Lines]",
      "126:         if (section, option) in conf.sensitive_config_values:",
      "",
      "[Added Lines]",
      "126:         if (section.lower(), option.lower()) in conf.sensitive_config_values:",
      "",
      "---------------"
    ],
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "311:             for s, s_c in self.configuration_description.items()",
      "312:             for k, item in s_c.get(\"options\").items()  # type: ignore[union-attr]",
      "313:         }",
      "315:         depr_option = {self.deprecated_options[x][:-1] for x in sensitive if x in self.deprecated_options}",
      "316:         depr_section = {",
      "317:             (self.deprecated_sections[s][0], k) for s, k in sensitive if s in self.deprecated_sections",
      "",
      "[Removed Lines]",
      "314:         sensitive = {(section, key) for (section, key), v in flattened.items() if v.get(\"sensitive\") is True}",
      "",
      "[Added Lines]",
      "314:         sensitive = {",
      "315:             (section.lower(), key.lower())",
      "316:             for (section, key), v in flattened.items()",
      "317:             if v.get(\"sensitive\") is True",
      "318:         }",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_config_endpoint.py -> tests/api_connexion/endpoints/test_config_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "247:         return_value=MOCK_CONF_WITH_SENSITIVE_VALUE,",
      "248:     )",
      "249:     @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "251:         response = self.client.get(",
      "253:             headers={\"Accept\": \"text/plain\"},",
      "254:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "255:         )",
      "256:         assert response.status_code == 200",
      "257:         expected = textwrap.dedent(",
      "261:         \"\"\"",
      "262:         )",
      "263:         assert expected == response.data.decode()",
      "",
      "[Removed Lines]",
      "250:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict):",
      "252:             \"/api/v1/config/section/core/option/sql_alchemy_conn\",",
      "258:             \"\"\"\\",
      "259:         [core]",
      "260:         sql_alchemy_conn = < hidden >",
      "",
      "[Added Lines]",
      "250:     @pytest.mark.parametrize(",
      "251:         \"section, option\",",
      "252:         [",
      "253:             (\"core\", \"sql_alchemy_conn\"),",
      "254:             (\"core\", \"SQL_ALCHEMY_CONN\"),",
      "255:             (\"corE\", \"sql_alchemy_conn\"),",
      "256:             (\"CORE\", \"sql_alchemy_conn\"),",
      "257:         ],",
      "258:     )",
      "259:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict, section, option):",
      "261:             f\"/api/v1/config/section/{section}/option/{option}\",",
      "267:             f\"\"\"\\",
      "268:         [{section}]",
      "269:         {option} = < hidden >",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "8501fcb3a1254ad2029d7f95015a5d6525a814d8",
      "candidate_info": {
        "commit_hash": "8501fcb3a1254ad2029d7f95015a5d6525a814d8",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8501fcb3a1254ad2029d7f95015a5d6525a814d8",
        "files": [
          "airflow/cli/commands/internal_api_command.py",
          "airflow/cli/commands/webserver_command.py",
          "airflow/dag_processing/manager.py",
          "airflow/models/dagbag.py",
          "docs/exts/docs_build/dev_index_generator.py"
        ],
        "message": "Refactor os.path.splitext to Path.* (#34352)\n\n(cherry picked from commit 4869575b2c538b54cbc9368791a924f7cd5f7ce8)",
        "before_after_code_files": [
          "airflow/cli/commands/internal_api_command.py||airflow/cli/commands/internal_api_command.py",
          "airflow/cli/commands/webserver_command.py||airflow/cli/commands/webserver_command.py",
          "airflow/dag_processing/manager.py||airflow/dag_processing/manager.py",
          "airflow/models/dagbag.py||airflow/models/dagbag.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/commands/internal_api_command.py||airflow/cli/commands/internal_api_command.py": [
          "File: airflow/cli/commands/internal_api_command.py -> airflow/cli/commands/internal_api_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import sys",
          "25: import textwrap",
          "26: from contextlib import suppress",
          "27: from tempfile import gettempdir",
          "28: from time import sleep",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: from pathlib import Path",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "171:             handle = setup_logging(log_file)",
          "174:             with open(stdout, \"a\") as stdout, open(stderr, \"a\") as stderr:",
          "175:                 stdout.truncate(0)",
          "176:                 stderr.truncate(0)",
          "178:                 ctx = daemon.DaemonContext(",
          "180:                     files_preserve=[handle],",
          "181:                     stdout=stdout,",
          "182:                     stderr=stderr,",
          "",
          "[Removed Lines]",
          "173:             base, ext = os.path.splitext(pid_file)",
          "179:                     pidfile=TimeoutPIDLockFile(f\"{base}-monitor{ext}\", -1),",
          "",
          "[Added Lines]",
          "174:             pid_path = Path(pid_file)",
          "175:             pidlock_path = pid_path.with_name(f\"{pid_path.stem}-monitor{pid_path.suffix}\")",
          "182:                     pidfile=TimeoutPIDLockFile(pidlock_path, -1),",
          "",
          "---------------"
        ],
        "airflow/cli/commands/webserver_command.py||airflow/cli/commands/webserver_command.py": [
          "File: airflow/cli/commands/webserver_command.py -> airflow/cli/commands/webserver_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import time",
          "27: import types",
          "28: from contextlib import suppress",
          "29: from time import sleep",
          "30: from typing import NoReturn",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "29: from pathlib import Path",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "475:             handle = setup_logging(log_file)",
          "478:             with open(stdout, \"a\") as stdout, open(stderr, \"a\") as stderr:",
          "479:                 stdout.truncate(0)",
          "480:                 stderr.truncate(0)",
          "482:                 ctx = daemon.DaemonContext(",
          "484:                     files_preserve=[handle],",
          "485:                     stdout=stdout,",
          "486:                     stderr=stderr,",
          "",
          "[Removed Lines]",
          "477:             base, ext = os.path.splitext(pid_file)",
          "483:                     pidfile=TimeoutPIDLockFile(f\"{base}-monitor{ext}\", -1),",
          "",
          "[Added Lines]",
          "478:             pid_path = Path(pid_file)",
          "479:             pidlock_path = pid_path.with_name(f\"{pid_path.stem}-monitor{pid_path.suffix}\")",
          "486:                     pidfile=TimeoutPIDLockFile(pidlock_path, -1),",
          "",
          "---------------"
        ],
        "airflow/dag_processing/manager.py||airflow/dag_processing/manager.py": [
          "File: airflow/dag_processing/manager.py -> airflow/dag_processing/manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "847:             last_runtime = self.get_last_runtime(file_path)",
          "848:             num_dags = self.get_last_dag_count(file_path)",
          "849:             num_errors = self.get_last_error_count(file_path)",
          "853:             processor_pid = self.get_pid(file_path)",
          "854:             processor_start_time = self.get_start_time(file_path)",
          "855:             runtime = (now - processor_start_time) if processor_start_time else None",
          "",
          "[Removed Lines]",
          "850:             file_name = os.path.basename(file_path)",
          "851:             file_name = os.path.splitext(file_name)[0].replace(os.sep, \".\")",
          "",
          "[Added Lines]",
          "850:             file_name = Path(file_path).stem",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1039:             run_count=self.get_run_count(processor.file_path) + 1,",
          "1040:         )",
          "1041:         self._file_stats[processor.file_path] = stat",
          "1044:         Stats.timing(f\"dag_processing.last_duration.{file_name}\", last_duration)",
          "1045:         Stats.timing(\"dag_processing.last_duration\", last_duration, tags={\"file_name\": file_name})",
          "",
          "[Removed Lines]",
          "1043:         file_name = os.path.splitext(os.path.basename(processor.file_path))[0].replace(os.sep, \".\")",
          "",
          "[Added Lines]",
          "1040:         file_name = Path(processor.file_path).stem",
          "",
          "---------------"
        ],
        "airflow/models/dagbag.py||airflow/models/dagbag.py": [
          "File: airflow/models/dagbag.py -> airflow/models/dagbag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "28: import warnings",
          "29: import zipfile",
          "30: from datetime import datetime, timedelta",
          "31: from typing import TYPE_CHECKING, NamedTuple",
          "33: from sqlalchemy.exc import OperationalError",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "31: from pathlib import Path",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "56: from airflow.utils.types import NOTSET, ArgNotSet",
          "58: if TYPE_CHECKING:",
          "61:     from airflow.models.dag import DAG",
          "",
          "[Removed Lines]",
          "59:     import pathlib",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "94:     def __init__(",
          "95:         self,",
          "97:         include_examples: bool | ArgNotSet = NOTSET,",
          "98:         safe_mode: bool | ArgNotSet = NOTSET,",
          "99:         read_dags_from_db: bool = False,",
          "",
          "[Removed Lines]",
          "96:         dag_folder: str | pathlib.Path | None = None,",
          "",
          "[Added Lines]",
          "95:         dag_folder: str | Path | None = None,",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "325:             return []",
          "327:         self.log.debug(\"Importing %s\", filepath)",
          "329:         path_hash = hashlib.sha1(filepath.encode(\"utf-8\")).hexdigest()",
          "330:         mod_name = f\"unusual_prefix_{path_hash}_{org_mod_name}\"",
          "332:         if mod_name in sys.modules:",
          "",
          "[Removed Lines]",
          "328:         org_mod_name, _ = os.path.splitext(os.path.split(filepath)[-1])",
          "",
          "[Added Lines]",
          "328:         org_mod_name = Path(filepath).stem",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "378:         mods = []",
          "379:         with zipfile.ZipFile(filepath) as current_zip_file:",
          "380:             for zip_info in current_zip_file.infolist():",
          "386:                     continue",
          "391:                 self.log.debug(\"Reading %s from %s\", zip_info.filename, filepath)",
          "",
          "[Removed Lines]",
          "381:                 head, _ = os.path.split(zip_info.filename)",
          "382:                 mod_name, ext = os.path.splitext(zip_info.filename)",
          "383:                 if ext not in [\".py\", \".pyc\"]:",
          "384:                     continue",
          "385:                 if head:",
          "388:                 if mod_name == \"__init__\":",
          "389:                     self.log.warning(\"Found __init__.%s at root of %s\", ext, filepath)",
          "",
          "[Added Lines]",
          "380:                 zip_path = Path(zip_info.filename)",
          "381:                 if zip_path.suffix not in [\".py\", \".pyc\"] or len(zip_path.parts) > 1:",
          "384:                 if zip_path.stem == \"__init__\":",
          "385:                     self.log.warning(\"Found %s at root of %s\", zip_path.name, filepath)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "400:                         )",
          "401:                     continue",
          "403:                 if mod_name in sys.modules:",
          "404:                     del sys.modules[mod_name]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "399:                 mod_name = zip_path.stem",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "517:     def collect_dags(",
          "518:         self,",
          "520:         only_if_updated: bool = True,",
          "521:         include_examples: bool = conf.getboolean(\"core\", \"LOAD_EXAMPLES\"),",
          "522:         safe_mode: bool = conf.getboolean(\"core\", \"DAG_DISCOVERY_SAFE_MODE\"),",
          "",
          "[Removed Lines]",
          "519:         dag_folder: str | pathlib.Path | None = None,",
          "",
          "[Added Lines]",
          "516:         dag_folder: str | Path | None = None,",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5778fbbc658fff20913c92e5e587291907afad43",
      "candidate_info": {
        "commit_hash": "5778fbbc658fff20913c92e5e587291907afad43",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/5778fbbc658fff20913c92e5e587291907afad43",
        "files": [
          "dev/breeze/src/airflow_breeze/global_constants.py",
          "dev/breeze/src/airflow_breeze/utils/suspended_providers.py",
          "docs/conf.py",
          "scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py",
          "scripts/in_container/run_provider_yaml_files_check.py",
          "tests/always/test_example_dags.py"
        ],
        "message": "Refactor: path.rglob(\u2026) does not need ** (#33669)\n\n(cherry picked from commit 8aa22291821327111a1bc5f0647b764880d4b9e9)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py",
          "dev/breeze/src/airflow_breeze/utils/suspended_providers.py||dev/breeze/src/airflow_breeze/utils/suspended_providers.py",
          "scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py||scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py",
          "scripts/in_container/run_provider_yaml_files_check.py||scripts/in_container/run_provider_yaml_files_check.py",
          "tests/always/test_example_dags.py||tests/always/test_example_dags.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py": [
          "File: dev/breeze/src/airflow_breeze/global_constants.py -> dev/breeze/src/airflow_breeze/global_constants.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "297:     \"statsd\",",
          "298:     \"trino\",",
          "299: ]",
          "302: with Path(AIRFLOW_SOURCES_ROOT, \"generated\", \"provider_dependencies.json\").open() as f:",
          "303:     PROVIDER_DEPENDENCIES = json.load(f)",
          "",
          "[Removed Lines]",
          "300: ALL_PROVIDER_YAML_FILES = Path(AIRFLOW_SOURCES_ROOT).glob(\"airflow/providers/**/provider.yaml\")",
          "",
          "[Added Lines]",
          "300: ALL_PROVIDER_YAML_FILES = Path(AIRFLOW_SOURCES_ROOT, \"airflow\", \"providers\").rglob(\"provider.yaml\")",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/suspended_providers.py||dev/breeze/src/airflow_breeze/utils/suspended_providers.py": [
          "File: dev/breeze/src/airflow_breeze/utils/suspended_providers.py -> dev/breeze/src/airflow_breeze/utils/suspended_providers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27:     skipped when running tests (without any prefix - for example apache/beam, yandex, google etc.).",
          "28:     \"\"\"",
          "29:     suspended_providers = []",
          "31:         provider_yaml = yaml.safe_load(provider_path.read_text())",
          "32:         if provider_yaml.get(\"suspended\"):",
          "33:             suspended_providers.append(",
          "",
          "[Removed Lines]",
          "30:     for provider_path in AIRFLOW_PROVIDERS_ROOT.glob(\"**/provider.yaml\"):",
          "",
          "[Added Lines]",
          "30:     for provider_path in AIRFLOW_PROVIDERS_ROOT.rglob(\"provider.yaml\"):",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "43:     Yields the ids of suspended providers.",
          "44:     \"\"\"",
          "45:     suspended_provider_ids = []",
          "47:         provider_yaml = yaml.safe_load(provider_path.read_text())",
          "48:         if provider_yaml.get(\"suspended\"):",
          "49:             suspended_provider_ids.append(",
          "",
          "[Removed Lines]",
          "46:     for provider_path in AIRFLOW_PROVIDERS_ROOT.glob(\"**/provider.yaml\"):",
          "",
          "[Added Lines]",
          "46:     for provider_path in AIRFLOW_PROVIDERS_ROOT.rglob(\"provider.yaml\"):",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py||scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py": [
          "File: scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py -> scripts/ci/pre_commit/pre_commit_update_common_sql_api_stubs.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "319:     shutil.rmtree(OUT_DIR, ignore_errors=True)",
          "321:     subprocess.run(",
          "323:         cwd=AIRFLOW_SOURCES_ROOT_PATH,",
          "324:     )",
          "325:     total_removals, total_additions = 0, 0",
          "",
          "[Removed Lines]",
          "322:         [\"stubgen\", *[os.fspath(path) for path in COMMON_SQL_ROOT.rglob(\"**/*.py\")]],",
          "",
          "[Added Lines]",
          "322:         [\"stubgen\", *[os.fspath(path) for path in COMMON_SQL_ROOT.rglob(\"*.py\")]],",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "327:     if _force_override:",
          "328:         console.print(\"\\n[yellow]The committed stub APIs are force-updated\\n\")",
          "329:     # reformat the generated stubs first",
          "331:         write_pyi_file(stub_path, stub_path.read_text(encoding=\"utf-8\"))",
          "333:         _new_removals, _new_additions = compare_stub_files(stub_path, force_override=_force_override)",
          "334:         total_removals += _new_removals",
          "335:         total_additions += _new_additions",
          "",
          "[Removed Lines]",
          "330:     for stub_path in OUT_DIR.rglob(\"**/*.pyi\"):",
          "332:     for stub_path in OUT_DIR.rglob(\"**/*.pyi\"):",
          "",
          "[Added Lines]",
          "330:     for stub_path in OUT_DIR.rglob(\"*.pyi\"):",
          "332:     for stub_path in OUT_DIR.rglob(\"*.pyi\"):",
          "",
          "---------------"
        ],
        "scripts/in_container/run_provider_yaml_files_check.py||scripts/in_container/run_provider_yaml_files_check.py": [
          "File: scripts/in_container/run_provider_yaml_files_check.py -> scripts/in_container/run_provider_yaml_files_check.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "427:     console.print(suspended_logos)",
          "428:     expected_logo_urls = {",
          "429:         f\"/{f.relative_to(DOCS_DIR).as_posix()}\"",
          "431:         if f.is_file() and not f\"/{f.relative_to(DOCS_DIR).as_posix()}\".startswith(tuple(suspended_logos))",
          "432:     }",
          "",
          "[Removed Lines]",
          "430:         for f in DOCS_DIR.glob(\"integration-logos/**/*\")",
          "",
          "[Added Lines]",
          "430:         for f in (DOCS_DIR / \"integration-logos\").rglob(\"*\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "490:     ProvidersManager().initialize_providers_configuration()",
          "491:     architecture = Architecture.get_current()",
          "492:     console.print(f\"Verifying packages on {architecture} architecture. Platform: {platform.machine()}.\")",
          "494:     all_provider_files = sorted(str(path) for path in provider_files_pattern)",
          "495:     if len(sys.argv) > 1:",
          "496:         paths = [os.fspath(ROOT_DIR / f) for f in sorted(sys.argv[1:])]",
          "",
          "[Removed Lines]",
          "493:     provider_files_pattern = pathlib.Path(ROOT_DIR).glob(\"airflow/providers/**/provider.yaml\")",
          "",
          "[Added Lines]",
          "493:     provider_files_pattern = pathlib.Path(ROOT_DIR, \"airflow\", \"providers\").rglob(\"provider.yaml\")",
          "",
          "---------------"
        ],
        "tests/always/test_example_dags.py||tests/always/test_example_dags.py": [
          "File: tests/always/test_example_dags.py -> tests/always/test_example_dags.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "38:     skipped when running tests (without any prefix - for example apache/beam, yandex, google etc.).",
          "39:     \"\"\"",
          "40:     suspended_providers = []",
          "42:         provider_yaml = yaml.safe_load(provider_path.read_text())",
          "43:         if provider_yaml.get(\"suspended\"):",
          "44:             suspended_providers.append(",
          "",
          "[Removed Lines]",
          "41:     for provider_path in AIRFLOW_PROVIDERS_ROOT.glob(\"**/provider.yaml\"):",
          "",
          "[Added Lines]",
          "41:     for provider_path in AIRFLOW_PROVIDERS_ROOT.rglob(\"provider.yaml\"):",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ac19ec55b52574c012fedd041eba957887758613",
      "candidate_info": {
        "commit_hash": "ac19ec55b52574c012fedd041eba957887758613",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ac19ec55b52574c012fedd041eba957887758613",
        "files": [
          "airflow/models/dag.py"
        ],
        "message": "Schedule default value description (#34291)\n\n* added clarification around schedule parameter\n\n* rewriting explanation of default value\n\n* Update airflow/models/dag.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n* Update dag.py\n\n* Fix trailing whitespace\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 5ab7517258716445ac583c41656fdb17f87f57a8)",
        "before_after_code_files": [
          "airflow/models/dag.py||airflow/models/dag.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "304:     :param description: The description for the DAG to e.g. be shown on the webserver",
          "305:     :param schedule: Defines the rules according to which DAG runs are scheduled. Can",
          "306:         accept cron string, timedelta object, Timetable, or list of Dataset objects.",
          "308:     :param start_date: The timestamp from which the scheduler will",
          "309:         attempt to backfill",
          "310:     :param end_date: A date beyond which your DAG won't run, leave to None",
          "",
          "[Removed Lines]",
          "307:         See also :doc:`/howto/timetable`.",
          "",
          "[Added Lines]",
          "307:         If this is not provided, the DAG will be set to the default",
          "308:         schedule ``timedelta(days=1)``. See also :doc:`/howto/timetable`.",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1511b4d8530594acf336d72dee0c790c3ca50cbd",
      "candidate_info": {
        "commit_hash": "1511b4d8530594acf336d72dee0c790c3ca50cbd",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/1511b4d8530594acf336d72dee0c790c3ca50cbd",
        "files": [
          "airflow/configuration.py"
        ],
        "message": "Refactor unneeded 'continue' jumps in configuration (#33844)\n\n(cherry picked from commit 3ae6b4e86fe807c00bd736c59df58733df2b9bf9)",
        "before_after_code_files": [
          "airflow/configuration.py||airflow/configuration.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [
            "airflow/configuration.py||airflow/configuration.py"
          ],
          "candidate": [
            "airflow/configuration.py||airflow/configuration.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/configuration.py||airflow/configuration.py": [
          "File: airflow/configuration.py -> airflow/configuration.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "19: import datetime",
          "20: import functools",
          "21: import io",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: import contextlib",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1473:             # if they are not provided through env, cmd and secret",
          "1474:             hidden = \"< hidden >\"",
          "1475:             for section, key in self.sensitive_config_values:",
          "1485:         return config_sources",
          "",
          "[Removed Lines]",
          "1476:                 if not config_sources.get(section):",
          "1477:                     continue",
          "1478:                 if config_sources[section].get(key, None):",
          "1479:                     if display_source:",
          "1480:                         source = config_sources[section][key][1]",
          "1481:                         config_sources[section][key] = (hidden, source)",
          "1482:                     else:",
          "1483:                         config_sources[section][key] = hidden",
          "",
          "[Added Lines]",
          "1477:                 if config_sources.get(section):",
          "1478:                     if config_sources[section].get(key, None):",
          "1479:                         if display_source:",
          "1480:                             source = config_sources[section][key][1]",
          "1481:                             config_sources[section][key] = (hidden, source)",
          "1482:                         else:",
          "1483:                             config_sources[section][key] = hidden",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1650:         configs: Iterable[tuple[str, ConfigParser]],",
          "1651:     ) -> bool:",
          "1652:         for config_type, config in configs:",
          "1659:                         return True",
          "1664:     @staticmethod",
          "1665:     def _deprecated_variable_is_set(deprecated_section: str, deprecated_key: str) -> bool:",
          "",
          "[Removed Lines]",
          "1653:             if config_type == \"default\":",
          "1654:                 continue",
          "1655:             try:",
          "1656:                 deprecated_section_array = config.items(section=deprecated_section, raw=True)",
          "1657:                 for key_candidate, _ in deprecated_section_array:",
          "1658:                     if key_candidate == deprecated_key:",
          "1660:             except NoSectionError:",
          "1661:                 pass",
          "1662:         return False",
          "",
          "[Added Lines]",
          "1653:             if config_type != \"default\":",
          "1654:                 with contextlib.suppress(NoSectionError):",
          "1655:                     deprecated_section_array = config.items(section=deprecated_section, raw=True)",
          "1656:                     if any(key == deprecated_key for key, _ in deprecated_section_array):",
          "1658:         else:",
          "1659:             return False",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ed36b2890773953f2df1c3851a7ed02e3822c5f9",
      "candidate_info": {
        "commit_hash": "ed36b2890773953f2df1c3851a7ed02e3822c5f9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ed36b2890773953f2df1c3851a7ed02e3822c5f9",
        "files": [
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/tests/test_docker_command_utils.py"
        ],
        "message": "Avoid WSL2 ones when finding a context for Breeze (#34538)\n\n* Avoid WSL2 ones when finding a context for Breeze\n\n* fixup! Avoid WSL2 ones when finding a context for Breeze\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 2e764fb0978fde33f59918416bd2732294c4bf23)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/tests/test_docker_command_utils.py||dev/breeze/tests/test_docker_command_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import copy",
          "21: import os",
          "22: import random",
          "23: import re",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import json",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "817:     :return: name of the docker context to use",
          "818:     \"\"\"",
          "821:         get_console().print(\"[warning]Could not detect docker builder. Using default.[/]\")",
          "822:         return \"default\"",
          "825:         get_console().print(\"[warning]Could not detect docker builder. Using default.[/]\")",
          "826:         return \"default\"",
          "836:     get_console().print(",
          "837:         f\"[warning]Could not use any of the preferred docker contexts {PREFERRED_CONTEXTS}.\\n\"",
          "838:         f\"Using {fallback_context} as context.[/]\"",
          "",
          "[Removed Lines]",
          "819:     output = run_command([\"docker\", \"context\", \"ls\", \"-q\"], capture_output=True, check=False, text=True)",
          "820:     if output.returncode != 0:",
          "823:     context_list = output.stdout.splitlines()",
          "824:     if not context_list:",
          "827:     elif len(context_list) == 1:",
          "828:         get_console().print(f\"[info]Using {context_list[0]} as context.[/]\")",
          "829:         return context_list[0]",
          "830:     else:",
          "831:         for preferred_context in PREFERRED_CONTEXTS:",
          "832:             if preferred_context in context_list:",
          "833:                 get_console().print(f\"[info]Using {preferred_context} as context.[/]\")",
          "834:                 return preferred_context",
          "835:     fallback_context = context_list[0]",
          "",
          "[Added Lines]",
          "820:     result = run_command(",
          "821:         [\"docker\", \"context\", \"ls\", \"--format=json\"],",
          "822:         capture_output=True,",
          "823:         check=False,",
          "824:         text=True,",
          "825:     )",
          "826:     if result.returncode != 0:",
          "829:     context_json = json.loads(result.stdout)",
          "830:     if isinstance(context_json, dict):",
          "831:         # In case there is one context it is returned as dict not array of dicts \u00af\\_(\u30c4)_/\u00af",
          "832:         context_json = [context_json]",
          "833:     known_contexts = {info[\"Name\"]: info for info in context_json}",
          "834:     if not known_contexts:",
          "837:     for preferred_context_name in PREFERRED_CONTEXTS:",
          "838:         try:",
          "839:             context = known_contexts[preferred_context_name]",
          "840:         except KeyError:",
          "841:             continue",
          "842:         # On Windows, some contexts are used for WSL2. We don't want to use those.",
          "843:         if context[\"DockerEndpoint\"] == \"npipe:////./pipe/dockerDesktopLinuxEngine\":",
          "844:             continue",
          "845:         get_console().print(f\"[info]Using {preferred_context_name} as context.[/]\")",
          "846:         return preferred_context_name",
          "847:     fallback_context = next(iter(known_contexts))",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_docker_command_utils.py||dev/breeze/tests/test_docker_command_utils.py": [
          "File: dev/breeze/tests/test_docker_command_utils.py -> dev/breeze/tests/test_docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "195:     )",
          "198: @pytest.mark.parametrize(",
          "199:     \"context_output, selected_context, console_output\",",
          "200:     [",
          "201:         (",
          "202:             \"default\",",
          "203:             \"default\",",
          "204:             \"[info]Using default as context\",",
          "205:         ),",
          "211:     ],",
          "212: )",
          "213: def test_autodetect_docker_context(context_output: str, selected_context: str, console_output: str):",
          "",
          "[Removed Lines]",
          "206:         (\"\", \"default\", \"[warning]Could not detect docker builder\"),",
          "207:         (\"a\\nb\", \"a\", \"[warning]Could not use any of the preferred docker contexts\"),",
          "208:         (\"a\\ndesktop-linux\", \"desktop-linux\", \"[info]Using desktop-linux as context\"),",
          "209:         (\"a\\ndefault\", \"default\", \"[info]Using default as context\"),",
          "210:         (\"a\\ndefault\\ndesktop-linux\", \"desktop-linux\", \"[info]Using desktop-linux as context\"),",
          "",
          "[Added Lines]",
          "198: def _fake_ctx(name: str) -> dict[str, str]:",
          "199:     return {",
          "200:         \"Name\": name,",
          "201:         \"DockerEndpoint\": f\"unix://{name}\",",
          "202:     }",
          "209:             json.dumps([_fake_ctx(\"default\")]),",
          "211:             \"[info]Using default as context\",",
          "212:         ),",
          "213:         (\"[]\", \"default\", \"[warning]Could not detect docker builder\"),",
          "214:         (",
          "215:             json.dumps([_fake_ctx(\"a\"), _fake_ctx(\"b\")]),",
          "216:             \"a\",",
          "217:             \"[warning]Could not use any of the preferred docker contexts\",",
          "218:         ),",
          "219:         (",
          "220:             json.dumps([_fake_ctx(\"a\"), _fake_ctx(\"desktop-linux\")]),",
          "221:             \"desktop-linux\",",
          "222:             \"[info]Using desktop-linux as context\",",
          "223:         ),",
          "224:         (",
          "225:             json.dumps([_fake_ctx(\"a\"), _fake_ctx(\"default\")]),",
          "229:         (",
          "230:             json.dumps([_fake_ctx(\"a\"), _fake_ctx(\"default\"), _fake_ctx(\"desktop-linux\")]),",
          "231:             \"desktop-linux\",",
          "232:             \"[info]Using desktop-linux as context\",",
          "233:         ),",
          "",
          "---------------"
        ]
      }
    }
  ]
}