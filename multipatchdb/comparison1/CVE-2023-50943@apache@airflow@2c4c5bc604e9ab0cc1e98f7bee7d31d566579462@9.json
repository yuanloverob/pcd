{
  "cve_id": "CVE-2023-50943",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows a potential attacker to poison the XCom data by bypassing the protection of \"enable_xcom_pickling=False\" configuration setting resulting in poisoned data after XCom deserialization. This vulnerability is considered low since it requires a DAG author to exploit it. Users are recommended to upgrade to version 2.8.1 or later, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
  "patch_info": {
    "commit_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "files": [
      "airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py"
    ],
    "message": "Stop deserializing pickle when enable_xcom_pickling is False (#36255)\n\n* Stop deserializing pickle when enable_xcom_pickling is False\n\n* Fix unit tests\n\n(cherry picked from commit 63e97abec5d56bc62a293c93f5227f364561e51c)",
    "before_after_code_files": [
      "airflow/models/xcom.py||airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py||tests/models/test_xcom.py"
    ]
  },
  "patch_diff": {
    "airflow/models/xcom.py||airflow/models/xcom.py": [
      "File: airflow/models/xcom.py -> airflow/models/xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "685:             except pickle.UnpicklingError:",
      "686:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "687:         else:",
      "693:     @staticmethod",
      "694:     def deserialize_value(result: XCom) -> Any:",
      "",
      "[Removed Lines]",
      "688:             try:",
      "689:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "690:             except (json.JSONDecodeError, UnicodeDecodeError):",
      "691:                 return pickle.loads(result.value)",
      "",
      "[Added Lines]",
      "688:             # Since xcom_pickling is disabled, we should only try to deserialize with JSON",
      "689:             return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "",
      "---------------"
    ],
    "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py": [
      "File: tests/api_connexion/schemas/test_xcom_schema.py -> tests/api_connexion/schemas/test_xcom_schema.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from airflow.models import DagRun, XCom",
      "31: from airflow.utils.dates import parse_execution_date",
      "32: from airflow.utils.session import create_session",
      "34: pytestmark = pytest.mark.db_test",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "188:     default_time = \"2016-04-02T21:00:00+00:00\"",
      "189:     default_time_parsed = parse_execution_date(default_time)",
      "191:     def test_serialize(self, create_xcom, session):",
      "192:         create_xcom(",
      "193:             dag_id=\"test_dag\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "192:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "208:             \"map_index\": -1,",
      "209:         }",
      "211:     def test_deserialize(self):",
      "212:         xcom_dump = {",
      "213:             \"key\": \"test_key\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "213:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------"
    ],
    "tests/models/test_xcom.py||tests/models/test_xcom.py": [
      "File: tests/models/test_xcom.py -> tests/models/test_xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "140:             ret_value = XCom.get_value(key=\"xcom_test3\", ti_key=ti_key, session=session)",
      "141:         assert ret_value == {\"key\": \"value\"}",
      "144:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"}):",
      "145:             XCom.set(",
      "146:                 key=\"xcom_test3\",",
      "",
      "[Removed Lines]",
      "143:     def test_xcom_deserialize_with_pickle_to_json_switch(self, task_instance, session):",
      "",
      "[Added Lines]",
      "143:     def test_xcom_deserialize_pickle_when_xcom_pickling_is_disabled(self, task_instance, session):",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "151:                 session=session,",
      "152:             )",
      "153:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"False\"}):",
      "163:     @conf_vars({(\"core\", \"xcom_enable_pickling\"): \"False\"})",
      "164:     def test_xcom_disable_pickle_type_fail_on_non_json(self, task_instance, session):",
      "",
      "[Removed Lines]",
      "154:             ret_value = XCom.get_one(",
      "155:                 key=\"xcom_test3\",",
      "156:                 dag_id=task_instance.dag_id,",
      "157:                 task_id=task_instance.task_id,",
      "158:                 run_id=task_instance.run_id,",
      "159:                 session=session,",
      "160:             )",
      "161:         assert ret_value == {\"key\": \"value\"}",
      "",
      "[Added Lines]",
      "154:             with pytest.raises(UnicodeDecodeError):",
      "155:                 XCom.get_one(",
      "156:                     key=\"xcom_test3\",",
      "157:                     dag_id=task_instance.dag_id,",
      "158:                     task_id=task_instance.task_id,",
      "159:                     run_id=task_instance.run_id,",
      "160:                     session=session,",
      "161:                 )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "4452e301ebbfd2995c3751c2b0d4c8e2d3455756",
      "candidate_info": {
        "commit_hash": "4452e301ebbfd2995c3751c2b0d4c8e2d3455756",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4452e301ebbfd2995c3751c2b0d4c8e2d3455756",
        "files": [
          "airflow/decorators/base.py",
          "docs/apache-airflow/tutorial/taskflow.rst",
          "tests/decorators/test_python.py"
        ],
        "message": "Make sure `multiple_outputs` is inferred correctly even when using `TypedDict` (#36652)\n\n* Use `issubclass()` to check if return type is a dictionary\n\n* Compare type to `typing.Mapping` instead of `typing.Dict`\n\n* Add documentation\n\n(cherry picked from commit e11b91c8e01b38023f209983a81aee23439a34a3)",
        "before_after_code_files": [
          "airflow/decorators/base.py||airflow/decorators/base.py",
          "tests/decorators/test_python.py||tests/decorators/test_python.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/decorators/base.py||airflow/decorators/base.py": [
          "File: airflow/decorators/base.py -> airflow/decorators/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27:     Callable,",
          "28:     ClassVar,",
          "29:     Collection,",
          "31:     Generic,",
          "32:     Iterator,",
          "33:     Mapping,",
          "",
          "[Removed Lines]",
          "30:     Dict,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "351:         except TypeError:  # Can't evaluate return type.",
          "352:             return False",
          "353:         ttype = getattr(return_type, \"__origin__\", return_type)",
          "356:     def __attrs_post_init__(self):",
          "357:         if \"self\" in self.function_signature.parameters:",
          "",
          "[Removed Lines]",
          "354:         return ttype is dict or ttype is Dict",
          "",
          "[Added Lines]",
          "353:         return issubclass(ttype, Mapping)",
          "",
          "---------------"
        ],
        "tests/decorators/test_python.py||tests/decorators/test_python.py": [
          "File: tests/decorators/test_python.py -> tests/decorators/test_python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "98:         assert identity_dict_with_decorator_call(5, 5).operator.multiple_outputs is True",
          "100:     def test_infer_multiple_outputs_forward_annotation(self):",
          "101:         if TYPE_CHECKING:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "100:     @pytest.mark.skipif(sys.version_info < (3, 8), reason=\"PEP 589 is implemented in Python 3.8\")",
          "101:     def test_infer_multiple_outputs_typed_dict(self):",
          "102:         from typing import TypedDict",
          "104:         class TypeDictClass(TypedDict):",
          "105:             pass",
          "107:         @task_decorator",
          "108:         def t1() -> TypeDictClass:",
          "109:             return {}",
          "111:         assert t1().operator.multiple_outputs is True",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b9447b1e086a989ca3dffac599654e5ffcba0e44",
      "candidate_info": {
        "commit_hash": "b9447b1e086a989ca3dffac599654e5ffcba0e44",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b9447b1e086a989ca3dffac599654e5ffcba0e44",
        "files": [
          "airflow/www/templates/airflow/traceback.html"
        ],
        "message": "Improve the error message displayed when there is a webserver error (#36570)\n\nThe error message displayed when errors happen in the webserver did\nnot properly communicated, that the user MUST look at the logs and\nprovide more information to investigate the root cause (after looking\nat the logs themselves). We have a number of users just copy&pasting\nthe generic error message without understanding that this is not nearly\nenough to help them.\n\nThis PR proposes a bit more explicit call to look at the logs and\nexplains why details are not displayed (for security reasons).\n\n(cherry picked from commit 574f86ee1e323d2a1613284f4bb77b6c4a3d3d0a)",
        "before_after_code_files": [
          "airflow/www/templates/airflow/traceback.html||airflow/www/templates/airflow/traceback.html"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/templates/airflow/traceback.html||airflow/www/templates/airflow/traceback.html": [
          "File: airflow/www/templates/airflow/traceback.html -> airflow/www/templates/airflow/traceback.html",
          "--- Hunk 1 ---",
          "[Context before]",
          "27:       <h1> Ooops! </h1>",
          "28:       <div>",
          "29:           <pre>",
          "50: Python version: {{ python_version }}",
          "51: Airflow version: {{ airflow_version }}",
          "",
          "[Removed Lines]",
          "30: Something bad has happened.",
          "32: Airflow is used by many users, and it is very likely that others had similar problems and you can easily find",
          "33: a solution to your problem.",
          "35: Consider following these steps:",
          "48:     Make sure however, to include all relevant details and results of your investigation so far.",
          "",
          "[Added Lines]",
          "30: Something bad has happened. For security reasons detailed information about the error is not logged.",
          "41:     All those resources might help you to find a solution to your problem.",
          "46:     get the logs with errors, describe results of your investigation so far, and consider creating a",
          "47:     <b><a href=\"https://github.com/apache/airflow/issues/new/choose\">bug report</a></b> including this information.",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4afcb7fda79c03eb66ed3e7188c9646a77ce7f2b",
      "candidate_info": {
        "commit_hash": "4afcb7fda79c03eb66ed3e7188c9646a77ce7f2b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4afcb7fda79c03eb66ed3e7188c9646a77ce7f2b",
        "files": [
          "airflow/models/dagrun.py"
        ],
        "message": "Remove dot value (#36712)\n\nThe update_state function in the DagRun class fails as ever since an update was made to Airflow, the state of the DagRun is returned as a string instead of an actual DagRunState. However, should this be fixed in the future, this change would still not affect anything since the state is called as a string here, which would return the value without needing to specify \".value\" anyway.\n\nCo-authored-by: fuat.cakici <fuat.cakici@knab.nl>\n(cherry picked from commit dfa695a48ec53f97b9443205d00d06ce0c67271d)",
        "before_after_code_files": [
          "airflow/models/dagrun.py||airflow/models/dagrun.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dagrun.py||airflow/models/dagrun.py": [
          "File: airflow/models/dagrun.py -> airflow/models/dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1044:         duration = self.end_date - self.start_date",
          "1045:         timer_params = {\"dt\": duration, \"tags\": self.stats_tags}",
          "1049:     @provide_session",
          "1050:     def verify_integrity(self, *, session: Session = NEW_SESSION) -> None:",
          "",
          "[Removed Lines]",
          "1046:         Stats.timing(f\"dagrun.duration.{self.state.value}.{self.dag_id}\", **timer_params)",
          "1047:         Stats.timing(f\"dagrun.duration.{self.state.value}\", **timer_params)",
          "",
          "[Added Lines]",
          "1046:         Stats.timing(f\"dagrun.duration.{self.state}.{self.dag_id}\", **timer_params)",
          "1047:         Stats.timing(f\"dagrun.duration.{self.state}\", **timer_params)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "119ba542541f754dc2247bf9de8ea5fa7aa3e8f9",
      "candidate_info": {
        "commit_hash": "119ba542541f754dc2247bf9de8ea5fa7aa3e8f9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/119ba542541f754dc2247bf9de8ea5fa7aa3e8f9",
        "files": [
          "airflow/models/dag.py",
          "tests/models/test_dag.py"
        ],
        "message": "Fix Callback exception when a removed task is the last one in the task instance list (#36693)\n\n* Fix Callback exception when a removed task is the last one in the task instance list\n\n* Add test_dag_handle_callback_with_removed_task\n\n* Remove extra break line\n\n* Merge TIs filters\n\n* Fix static check\n\n* Revert changes\n\n(cherry picked from commit 8c1c09bab34be8234d9e152ed7e4c9b925c08459)",
        "before_after_code_files": [
          "airflow/models/dag.py||airflow/models/dag.py",
          "tests/models/test_dag.py||tests/models/test_dag.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1462:             # context for the callback.",
          "1463:             if dag.partial:",
          "1464:                 tis = [ti for ti in tis if not ti.state == State.NONE]",
          "1465:             ti = tis[-1]  # get first TaskInstance of DagRun",
          "1466:             ti.task = dag.get_task(ti.task_id)",
          "1467:             context = ti.get_template_context(session=session)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1465:             # filter out removed tasks",
          "1466:             tis = [ti for ti in tis if ti.state != TaskInstanceState.REMOVED]",
          "",
          "---------------"
        ],
        "tests/models/test_dag.py||tests/models/test_dag.py": [
          "File: tests/models/test_dag.py -> tests/models/test_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1512:         dag.clear()",
          "1513:         self._clean_up(dag_id)",
          "1515:     def test_next_dagrun_after_fake_scheduled_previous(self):",
          "1516:         \"\"\"",
          "1517:         Test scheduling a dag where there is a prior DagRun",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1515:     def test_dag_handle_callback_with_removed_task(self, dag_maker, session):",
          "1516:         \"\"\"",
          "1517:         Tests avoid crashes when a removed task is the last one in the list of task instance",
          "1518:         \"\"\"",
          "1519:         dag_id = \"test_dag_callback_with_removed_task\"",
          "1520:         mock_callback = mock.MagicMock()",
          "1521:         with DAG(",
          "1522:             dag_id=dag_id,",
          "1523:             on_success_callback=mock_callback,",
          "1524:             on_failure_callback=mock_callback,",
          "1525:         ) as dag:",
          "1526:             EmptyOperator(task_id=\"faketastic\")",
          "1527:             task_removed = EmptyOperator(task_id=\"removed_task\")",
          "1529:         with create_session() as session:",
          "1530:             dag_run = dag.create_dagrun(State.RUNNING, TEST_DATE, run_type=DagRunType.MANUAL, session=session)",
          "1531:             dag._remove_task(task_removed.task_id)",
          "1532:             tis = dag_run.get_task_instances(session=session)",
          "1533:             tis[-1].state = TaskInstanceState.REMOVED",
          "1534:             assert dag_run.get_task_instance(task_removed.task_id).state == TaskInstanceState.REMOVED",
          "1536:             # should not raise any exception",
          "1537:             dag.handle_callback(dag_run, success=True)",
          "1538:             dag.handle_callback(dag_run, success=False)",
          "1540:         dag.clear()",
          "1541:         self._clean_up(dag_id)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "38573856e51b8988b8298af3394461a805d79a6a",
      "candidate_info": {
        "commit_hash": "38573856e51b8988b8298af3394461a805d79a6a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/38573856e51b8988b8298af3394461a805d79a6a",
        "files": [
          "airflow/settings.py",
          "tests/core/test_settings.py"
        ],
        "message": "Better error message when sqlite URL uses relative path (#36774)\n\nWhen sqlite URL uses relative path, the error printed is quite\ncryptic:\n\n```\nsqlalchemy.exc.OperationalError: (sqlite3.OperationalError) unable to open database file\n```\n\nThis might easily happen for example when you are in a hurry and put\nrelative value in your AIRFLOW_HOME.\n\nThis PR checks if sql is relative and throws more appropriate and\nexplicit message what is wrong.\n\n(cherry picked from commit 082055e23a38169a613f639296e144234f15d28c)",
        "before_after_code_files": [
          "airflow/settings.py||airflow/settings.py",
          "tests/core/test_settings.py||tests/core/test_settings.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/settings.py||airflow/settings.py": [
          "File: airflow/settings.py -> airflow/settings.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "192:     global PLUGINS_FOLDER",
          "193:     global DONOT_MODIFY_HANDLERS",
          "194:     SQL_ALCHEMY_CONN = conf.get(\"database\", \"SQL_ALCHEMY_CONN\")",
          "195:     DAGS_FOLDER = os.path.expanduser(conf.get(\"core\", \"DAGS_FOLDER\"))",
          "197:     PLUGINS_FOLDER = conf.get(\"core\", \"plugins_folder\", fallback=os.path.join(AIRFLOW_HOME, \"plugins\"))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "195:     if SQL_ALCHEMY_CONN.startswith(\"sqlite\") and not SQL_ALCHEMY_CONN.startswith(\"sqlite:////\"):",
          "196:         from airflow.exceptions import AirflowConfigException",
          "198:         raise AirflowConfigException(",
          "199:             f\"Cannot use relative path: `{SQL_ALCHEMY_CONN}` to connect to sqlite. \"",
          "200:             \"Please use absolute path such as `sqlite:////tmp/airflow.db`.\"",
          "201:         )",
          "",
          "---------------"
        ],
        "tests/core/test_settings.py||tests/core/test_settings.py": [
          "File: tests/core/test_settings.py -> tests/core/test_settings.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import sys",
          "22: import tempfile",
          "23: from unittest import mock",
          "26: import pytest",
          "29: from tests.test_utils.config import conf_vars",
          "31: SETTINGS_FILE_POLICY = \"\"\"",
          "",
          "[Removed Lines]",
          "24: from unittest.mock import MagicMock, call",
          "28: from airflow.exceptions import AirflowClusterPolicyViolation",
          "",
          "[Added Lines]",
          "24: from unittest.mock import MagicMock, call, patch",
          "28: from airflow.exceptions import AirflowClusterPolicyViolation, AirflowConfigException",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "214:         session_lifetime_config = settings.get_session_lifetime_config()",
          "215:         default_timeout_minutes = 30 * 24 * 60",
          "216:         assert session_lifetime_config == default_timeout_minutes",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "219: def test_sqlite_relative_path():",
          "220:     from airflow import settings",
          "222:     with patch(\"airflow.settings.conf.get\") as conf_get_mock:",
          "223:         conf_get_mock.return_value = \"sqlite:///./relative_path.db\"",
          "224:         with pytest.raises(AirflowConfigException) as exc:",
          "225:             settings.configure_vars()",
          "226:         assert \"Cannot use relative path:\" in str(exc.value)",
          "",
          "---------------"
        ]
      }
    }
  ]
}