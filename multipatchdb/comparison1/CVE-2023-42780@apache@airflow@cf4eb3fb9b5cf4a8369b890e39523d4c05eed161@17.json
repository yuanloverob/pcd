{
  "cve_id": "CVE-2023-42780",
  "cve_desc": "Apache Airflow, versions prior to 2.7.2, contains a security vulnerability that allows authenticated users of Airflow to list warnings for all DAGs, even if the user had no permission to see those DAGs. It would reveal the dag_ids and the stack-traces of import errors for those DAGs with import errors.\nUsers of Apache Airflow are advised to upgrade to version 2.7.2 or newer to mitigate the risk associated with this vulnerability.\n\n",
  "repo": "apache/airflow",
  "patch_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
  "patch_info": {
    "commit_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ],
    "message": "Fix dag warning endpoint permissions (#34355)\n\n* Fix dag warning endpoint permissions\n\n* update the query to have an accurate result for total entries and pagination\n\n* add unit tests\n\n* Update test_dag_warning_endpoint.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 3570bbfbea69e2965f91b9964ce28bc268c68129)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_warning_endpoint.py -> airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "16: # under the License.",
      "17: from __future__ import annotations",
      "19: from sqlalchemy import select",
      "20: from sqlalchemy.orm import Session",
      "22: from airflow.api_connexion import security",
      "23: from airflow.api_connexion.parameters import apply_sorting, check_limit, format_parameters",
      "24: from airflow.api_connexion.schemas.dag_warning_schema import (",
      "25:     DagWarningCollection,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "19: from flask import g",
      "24: from airflow.api_connexion.exceptions import PermissionDenied",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "28: from airflow.api_connexion.types import APIResponse",
      "29: from airflow.models.dagwarning import DagWarning as DagWarningModel",
      "30: from airflow.security import permissions",
      "31: from airflow.utils.db import get_query_count",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from airflow.utils.airflow_flask_app import get_airflow_app",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "52:     allowed_filter_attrs = [\"dag_id\", \"warning_type\", \"message\", \"timestamp\"]",
      "53:     query = select(DagWarningModel)",
      "54:     if dag_id:",
      "55:         query = query.where(DagWarningModel.dag_id == dag_id)",
      "56:     if warning_type:",
      "57:         query = query.where(DagWarningModel.warning_type == warning_type)",
      "58:     total_entries = get_query_count(query, session=session)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "58:         if not get_airflow_app().appbuilder.sm.can_read_dag(dag_id, g.user):",
      "59:             raise PermissionDenied(detail=f\"User not allowed to access this DAG: {dag_id}\")",
      "61:     else:",
      "62:         readable_dags = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
      "63:         query = query.where(DagWarningModel.dag_id.in_(readable_dags))",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_warning_endpoint.py -> tests/api_connexion/endpoints/test_dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35:         app,  # type:ignore",
      "36:         username=\"test\",",
      "37:         role_name=\"Test\",",
      "39:     )",
      "40:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "42:     yield minimal_app_for_api",
      "44:     delete_user(app, username=\"test\")  # type: ignore",
      "45:     delete_user(app, username=\"test_no_permissions\")  # type: ignore",
      "48: class TestBaseDagWarning:",
      "",
      "[Removed Lines]",
      "38:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING)],  # type: ignore",
      "",
      "[Added Lines]",
      "38:         permissions=[",
      "39:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "40:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
      "41:         ],  # type: ignore",
      "44:     create_user(",
      "45:         app,  # type:ignore",
      "46:         username=\"test_with_dag2_read\",",
      "47:         role_name=\"TestWithDag2Read\",",
      "48:         permissions=[",
      "49:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "50:             (permissions.ACTION_CAN_READ, f\"{permissions.RESOURCE_DAG_PREFIX}dag2\"),",
      "51:         ],  # type: ignore",
      "52:     )",
      "58:     delete_user(app, username=\"test_with_dag2_read\")  # type: ignore",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "147:             \"/api/v1/dagWarnings\", environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"}",
      "148:         )",
      "149:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "164:     def test_should_raise_403_forbidden_when_user_has_no_dag_read_permission(self):",
      "165:         response = self.client.get(",
      "166:             \"/api/v1/dagWarnings\",",
      "167:             environ_overrides={\"REMOTE_USER\": \"test_with_dag2_read\"},",
      "168:             query_string={\"dag_id\": \"dag1\"},",
      "169:         )",
      "170:         assert response.status_code == 403",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "8f424fdd88c070f5124fd3dc9ccdd6c10f40f386",
      "candidate_info": {
        "commit_hash": "8f424fdd88c070f5124fd3dc9ccdd6c10f40f386",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8f424fdd88c070f5124fd3dc9ccdd6c10f40f386",
        "files": [
          "tests/cli/commands/test_task_command.py",
          "tests/dag_processing/test_job_runner.py",
          "tests/providers/amazon/aws/hooks/test_eks.py",
          "tests/providers/google/cloud/operators/test_datafusion.py",
          "tests/providers/google/cloud/operators/test_dataproc.py",
          "tests/serialization/test_dag_serialization.py",
          "tests/utils/test_helpers.py",
          "tests/utils/test_python_virtualenv.py"
        ],
        "message": "Replace sequence concatination by unpacking in Airflow tests (#33935)\n\n(cherry picked from commit 732eba98b2f485646646574052c1c3c57a94e07f)",
        "before_after_code_files": [
          "tests/cli/commands/test_task_command.py||tests/cli/commands/test_task_command.py",
          "tests/dag_processing/test_job_runner.py||tests/dag_processing/test_job_runner.py",
          "tests/providers/amazon/aws/hooks/test_eks.py||tests/providers/amazon/aws/hooks/test_eks.py",
          "tests/providers/google/cloud/operators/test_datafusion.py||tests/providers/google/cloud/operators/test_datafusion.py",
          "tests/providers/google/cloud/operators/test_dataproc.py||tests/providers/google/cloud/operators/test_dataproc.py",
          "tests/serialization/test_dag_serialization.py||tests/serialization/test_dag_serialization.py",
          "tests/utils/test_helpers.py||tests/utils/test_helpers.py",
          "tests/utils/test_python_virtualenv.py||tests/utils/test_python_virtualenv.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/cli/commands/test_task_command.py||tests/cli/commands/test_task_command.py": [
          "File: tests/cli/commands/test_task_command.py -> tests/cli/commands/test_task_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "818:             session.commit()",
          "820:             assert session.query(TaskInstance).filter_by(pool=pool_name).first() is None",
          "822:             assert session.query(TaskInstance).filter_by(pool=pool_name).first() is not None",
          "824:             session.delete(pool)",
          "",
          "[Removed Lines]",
          "821:             task_command.task_run(self.parser.parse_args(self.task_args + [\"--pool\", pool_name]))",
          "",
          "[Added Lines]",
          "821:             task_command.task_run(self.parser.parse_args([*self.task_args, \"--pool\", pool_name]))",
          "",
          "---------------"
        ],
        "tests/dag_processing/test_job_runner.py||tests/dag_processing/test_job_runner.py": [
          "File: tests/dag_processing/test_job_runner.py -> tests/dag_processing/test_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "499:             [\"file_3.py\", \"file_2.py\", \"file_1.py\"]",
          "500:         )",
          "503:         manager.processor.add_new_file_path_to_queue()",
          "504:         assert manager.processor._file_path_queue == collections.deque(",
          "505:             [\"file_4.py\", \"file_3.py\", \"file_2.py\", \"file_1.py\"]",
          "",
          "[Removed Lines]",
          "502:         manager.processor.set_file_paths(dag_files + [\"file_4.py\"])",
          "",
          "[Added Lines]",
          "502:         manager.processor.set_file_paths([*dag_files, \"file_4.py\"])",
          "",
          "---------------"
        ],
        "tests/providers/amazon/aws/hooks/test_eks.py||tests/providers/amazon/aws/hooks/test_eks.py": [
          "File: tests/providers/amazon/aws/hooks/test_eks.py -> tests/providers/amazon/aws/hooks/test_eks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1167:         test_inputs = dict(",
          "1168:             deepcopy(",
          "1173:                     (ClusterAttributes.CLUSTER_NAME, cluster_name),",
          "1174:                     (FargateProfileAttributes.FARGATE_PROFILE_NAME, fargate_profile_name),",
          "1175:                 ]",
          "1178:             )",
          "1179:         )",
          "",
          "[Removed Lines]",
          "1169:                 # Required Constants",
          "1170:                 [POD_EXECUTION_ROLE_ARN]",
          "1171:                 # Required Variables",
          "1172:                 + [",
          "1176:                 # Test Case Values",
          "1177:                 + [(FargateProfileAttributes.SELECTORS, selectors)]",
          "",
          "[Added Lines]",
          "1169:                 [",
          "1170:                     # Required Constants",
          "1171:                     POD_EXECUTION_ROLE_ARN,",
          "1172:                     # Required Variables",
          "1175:                     # Test Case Values",
          "1176:                     (FargateProfileAttributes.SELECTORS, selectors),",
          "",
          "---------------"
        ],
        "tests/providers/google/cloud/operators/test_datafusion.py||tests/providers/google/cloud/operators/test_datafusion.py": [
          "File: tests/providers/google/cloud/operators/test_datafusion.py -> tests/providers/google/cloud/operators/test_datafusion.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "238:         )",
          "240:         mock_hook.return_value.wait_for_pipeline_state.assert_called_once_with(",
          "242:             pipeline_id=PIPELINE_ID,",
          "243:             pipeline_name=PIPELINE_NAME,",
          "244:             namespace=NAMESPACE,",
          "",
          "[Removed Lines]",
          "241:             success_states=SUCCESS_STATES + [PipelineStates.RUNNING],",
          "",
          "[Added Lines]",
          "241:             success_states=[*SUCCESS_STATES, PipelineStates.RUNNING],",
          "",
          "---------------"
        ],
        "tests/providers/google/cloud/operators/test_dataproc.py||tests/providers/google/cloud/operators/test_dataproc.py": [
          "File: tests/providers/google/cloud/operators/test_dataproc.py -> tests/providers/google/cloud/operators/test_dataproc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "461:             \"labels\": LABELS,",
          "462:             \"virtual_cluster_config\": None,",
          "463:         }",
          "465:             call.hook().create_cluster(**create_cluster_args),",
          "466:         ]",
          "",
          "[Removed Lines]",
          "464:         expected_calls = self.extra_links_expected_calls_base + [",
          "",
          "[Added Lines]",
          "464:         expected_calls = [",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "510:             \"labels\": LABELS,",
          "511:             \"virtual_cluster_config\": VIRTUAL_CLUSTER_CONFIG,",
          "512:         }",
          "514:             call.hook().create_cluster(**create_cluster_args),",
          "515:         ]",
          "",
          "[Removed Lines]",
          "513:         expected_calls = self.extra_links_expected_calls_base + [",
          "",
          "[Added Lines]",
          "514:         expected_calls = [",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "784:             \"graceful_decommission_timeout\": {\"seconds\": 600},",
          "785:             \"update_mask\": UPDATE_MASK,",
          "786:         }",
          "789:         ]",
          "791:         op = DataprocScaleClusterOperator(",
          "",
          "[Removed Lines]",
          "787:         expected_calls = self.extra_links_expected_calls_base + [",
          "788:             call.hook().update_cluster(**update_cluster_args)",
          "",
          "[Added Lines]",
          "789:         expected_calls = [",
          "791:             call.hook().update_cluster(**update_cluster_args),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1224:             \"timeout\": TIMEOUT,",
          "1225:             \"metadata\": METADATA,",
          "1226:         }",
          "1229:         ]",
          "1231:         op = DataprocUpdateClusterOperator(",
          "",
          "[Removed Lines]",
          "1227:         expected_calls = self.extra_links_expected_calls_base + [",
          "1228:             call.hook().update_cluster(**update_cluster_args)",
          "",
          "[Added Lines]",
          "1230:         expected_calls = [",
          "1232:             call.hook().update_cluster(**update_cluster_args),",
          "",
          "---------------"
        ],
        "tests/serialization/test_dag_serialization.py||tests/serialization/test_dag_serialization.py": [
          "File: tests/serialization/test_dag_serialization.py -> tests/serialization/test_dag_serialization.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1481:             pass",
          "1483:         class DummyTask(BaseOperator):",
          "1486:         execution_date = datetime(2020, 1, 1)",
          "1487:         with DAG(dag_id=\"test_error_on_unregistered_ti_dep_serialization\", start_date=execution_date) as dag:",
          "",
          "[Removed Lines]",
          "1484:             deps = frozenset(list(BaseOperator.deps) + [DummyTriggerRule()])",
          "",
          "[Added Lines]",
          "1484:             deps = frozenset([*BaseOperator.deps, DummyTriggerRule()])",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1508:         from test_plugin import CustomTestTriggerRule",
          "1510:         class DummyTask(BaseOperator):",
          "1513:         execution_date = datetime(2020, 1, 1)",
          "1514:         with DAG(dag_id=\"test_serialize_custom_ti_deps\", start_date=execution_date) as dag:",
          "",
          "[Removed Lines]",
          "1511:             deps = frozenset(list(BaseOperator.deps) + [CustomTestTriggerRule()])",
          "",
          "[Added Lines]",
          "1511:             deps = frozenset([*BaseOperator.deps, CustomTestTriggerRule()])",
          "",
          "---------------"
        ],
        "tests/utils/test_helpers.py||tests/utils/test_helpers.py": [
          "File: tests/utils/test_helpers.py -> tests/utils/test_helpers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "80:         assert list(helpers.chunks([1, 2, 3], 2)) == [[1, 2], [3]]",
          "82:     def test_reduce_in_chunks(self):",
          "87:         assert helpers.reduce_in_chunks(lambda x, y: x + y[0] * y[1], [1, 2, 3, 4], 0, 2) == 14",
          "",
          "[Removed Lines]",
          "83:         assert helpers.reduce_in_chunks(lambda x, y: x + [y], [1, 2, 3, 4, 5], []) == [[1, 2, 3, 4, 5]]",
          "85:         assert helpers.reduce_in_chunks(lambda x, y: x + [y], [1, 2, 3, 4, 5], [], 2) == [[1, 2], [3, 4], [5]]",
          "",
          "[Added Lines]",
          "83:         assert helpers.reduce_in_chunks(lambda x, y: [*x, y], [1, 2, 3, 4, 5], []) == [[1, 2, 3, 4, 5]]",
          "85:         assert helpers.reduce_in_chunks(lambda x, y: [*x, y], [1, 2, 3, 4, 5], [], 2) == [[1, 2], [3, 4], [5]]",
          "",
          "---------------"
        ],
        "tests/utils/test_python_virtualenv.py||tests/utils/test_python_virtualenv.py": [
          "File: tests/utils/test_python_virtualenv.py -> tests/utils/test_python_virtualenv.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "61:             [sys.executable, \"-m\", \"virtualenv\", \"/VENV\", \"--system-site-packages\", \"--python=pythonVER\"]",
          "62:         )",
          "63:         mock_execute_in_subprocess.assert_called_with(",
          "65:         )",
          "67:     @mock.patch(\"airflow.utils.python_virtualenv.execute_in_subprocess\")",
          "",
          "[Removed Lines]",
          "64:             [\"/VENV/bin/pip\", \"install\"] + pip_install_options + [\"apache-beam[gcp]\"]",
          "",
          "[Added Lines]",
          "64:             [\"/VENV/bin/pip\", \"install\", *pip_install_options, \"apache-beam[gcp]\"]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7cbdcf2b959067ad2e28f9ef360ba9e6cd937f06",
      "candidate_info": {
        "commit_hash": "7cbdcf2b959067ad2e28f9ef360ba9e6cd937f06",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7cbdcf2b959067ad2e28f9ef360ba9e6cd937f06",
        "files": [
          "airflow/decorators/base.py"
        ],
        "message": "Avoid top-level airflow import to avoid circular dependency (#34586)\n\n(cherry picked from commit d1b7bca6a36c146119fb5746019116c4d1e15275)",
        "before_after_code_files": [
          "airflow/decorators/base.py||airflow/decorators/base.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/decorators/base.py||airflow/decorators/base.py": [
          "File: airflow/decorators/base.py -> airflow/decorators/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "41: import typing_extensions",
          "42: from sqlalchemy.orm import Session",
          "45: from airflow.exceptions import AirflowException",
          "46: from airflow.models.abstractoperator import DEFAULT_RETRIES, DEFAULT_RETRY_DELAY",
          "47: from airflow.models.baseoperator import (",
          "",
          "[Removed Lines]",
          "44: from airflow import Dataset",
          "",
          "[Added Lines]",
          "44: from airflow.datasets import Dataset",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a87761af96f553606e8e6e1843e94e623af08261",
      "candidate_info": {
        "commit_hash": "a87761af96f553606e8e6e1843e94e623af08261",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a87761af96f553606e8e6e1843e94e623af08261",
        "files": [
          "airflow/www/views.py"
        ],
        "message": "Fix the required permissions to clear a TI from the UI (#34123)\n\n(cherry picked from commit 2f5777c082189e6495f0fea44bb9050549c0056b)",
        "before_after_code_files": [
          "airflow/www/views.py||airflow/www/views.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2312:     @auth.has_access(",
          "2313:         [",
          "2314:             (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),",
          "2316:         ]",
          "2317:     )",
          "2318:     @action_logging",
          "",
          "[Removed Lines]",
          "2315:             (permissions.ACTION_CAN_DELETE, permissions.RESOURCE_TASK_INSTANCE),",
          "",
          "[Added Lines]",
          "2315:             (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_TASK_INSTANCE),",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2407:     @auth.has_access(",
          "2408:         [",
          "2409:             (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),",
          "2411:         ]",
          "2412:     )",
          "2413:     @action_logging",
          "",
          "[Removed Lines]",
          "2410:             (permissions.ACTION_CAN_DELETE, permissions.RESOURCE_TASK_INSTANCE),",
          "",
          "[Added Lines]",
          "2410:             (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_TASK_INSTANCE),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0af9a5b21cf84adfe4107945c0f68bada12eb359",
      "candidate_info": {
        "commit_hash": "0af9a5b21cf84adfe4107945c0f68bada12eb359",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0af9a5b21cf84adfe4107945c0f68bada12eb359",
        "files": [
          "airflow/api/__init__.py",
          "airflow/cli/commands/standalone_command.py",
          "airflow/executors/local_executor.py",
          "airflow/jobs/triggerer_job_runner.py",
          "airflow/triggers/external_task.py",
          "airflow/www/extensions/init_security.py"
        ],
        "message": "Move the try outside the loop when this is possible in Airflow core (#33975)\n\n* Move the try outside the loop when this is possible in Airflow core\n\n* Use supress instead of except pass\n\n(cherry picked from commit 8918b435be8c683bbd6bb2ffa871dbd31d476f48)",
        "before_after_code_files": [
          "airflow/api/__init__.py||airflow/api/__init__.py",
          "airflow/cli/commands/standalone_command.py||airflow/cli/commands/standalone_command.py",
          "airflow/executors/local_executor.py||airflow/executors/local_executor.py",
          "airflow/jobs/triggerer_job_runner.py||airflow/jobs/triggerer_job_runner.py",
          "airflow/triggers/external_task.py||airflow/triggers/external_task.py",
          "airflow/www/extensions/init_security.py||airflow/www/extensions/init_security.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api/__init__.py||airflow/api/__init__.py": [
          "File: airflow/api/__init__.py -> airflow/api/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "36:         pass",
          "38:     backends = []",
          "41:             auth = import_module(backend.strip())",
          "42:             log.info(\"Loaded API auth backend: %s\", backend)",
          "43:             backends.append(auth)",
          "47:     return backends",
          "",
          "[Removed Lines]",
          "39:     for backend in auth_backends.split(\",\"):",
          "40:         try:",
          "44:         except ImportError as err:",
          "45:             log.critical(\"Cannot import %s for API authentication due to: %s\", backend, err)",
          "46:             raise AirflowException(err)",
          "",
          "[Added Lines]",
          "39:     try:",
          "40:         for backend in auth_backends.split(\",\"):",
          "44:     except ImportError as err:",
          "45:         log.critical(\"Cannot import %s for API authentication due to: %s\", backend, err)",
          "46:         raise AirflowException(err)",
          "",
          "---------------"
        ],
        "airflow/cli/commands/standalone_command.py||airflow/cli/commands/standalone_command.py": [
          "File: airflow/cli/commands/standalone_command.py -> airflow/cli/commands/standalone_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "91:             command.start()",
          "92:         # Run output loop",
          "93:         shown_ready = False",
          "96:                 # Print all the current lines onto the screen",
          "97:                 self.update_output()",
          "98:                 # Print info banner when all components are ready and the",
          "",
          "[Removed Lines]",
          "94:         while True:",
          "95:             try:",
          "",
          "[Added Lines]",
          "94:         try:",
          "95:             while True:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "108:                     shown_ready = True",
          "109:                 # Ensure we idle-sleep rather than fast-looping",
          "110:                 time.sleep(0.1)",
          "113:         # Stop subcommand threads",
          "114:         self.print_output(\"standalone\", \"Shutting down components\")",
          "115:         for command in self.subcommands.values():",
          "",
          "[Removed Lines]",
          "111:             except KeyboardInterrupt:",
          "112:                 break",
          "",
          "[Added Lines]",
          "111:         except KeyboardInterrupt:",
          "112:             pass",
          "",
          "---------------"
        ],
        "airflow/executors/local_executor.py||airflow/executors/local_executor.py": [
          "File: airflow/executors/local_executor.py -> airflow/executors/local_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: \"\"\"",
          "25: from __future__ import annotations",
          "27: import logging",
          "28: import os",
          "29: import subprocess",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: import contextlib",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "330:         def sync(self):",
          "331:             \"\"\"Sync will get called periodically by the heartbeat method.\"\"\"",
          "334:                     results = self.executor.result_queue.get_nowait()",
          "335:                     try:",
          "336:                         self.executor.change_state(*results)",
          "337:                     finally:",
          "338:                         self.executor.result_queue.task_done()",
          "342:         def end(self):",
          "343:             \"\"\"",
          "",
          "[Removed Lines]",
          "332:             while True:",
          "333:                 try:",
          "339:                 except Empty:",
          "340:                     break",
          "",
          "[Added Lines]",
          "333:             with contextlib.suppress(Empty):",
          "334:                 while True:",
          "",
          "---------------"
        ],
        "airflow/jobs/triggerer_job_runner.py||airflow/jobs/triggerer_job_runner.py": [
          "File: airflow/jobs/triggerer_job_runner.py -> airflow/jobs/triggerer_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "460:         \"\"\"",
          "461:         watchdog = asyncio.create_task(self.block_watchdog())",
          "462:         last_status = time.time()",
          "465:                 # Run core logic",
          "466:                 await self.create_triggers()",
          "467:                 await self.cancel_triggers()",
          "",
          "[Removed Lines]",
          "463:         while not self.stop:",
          "464:             try:",
          "",
          "[Added Lines]",
          "463:         try:",
          "464:             while not self.stop:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "473:                     count = len(self.triggers)",
          "474:                     self.log.info(\"%i triggers currently running\", count)",
          "475:                     last_status = time.time()",
          "479:         # Wait for watchdog to complete",
          "480:         await watchdog",
          "",
          "[Removed Lines]",
          "476:             except Exception:",
          "477:                 self.stop = True",
          "478:                 raise",
          "",
          "[Added Lines]",
          "476:         except Exception:",
          "477:             self.stop = True",
          "478:             raise",
          "",
          "---------------"
        ],
        "airflow/triggers/external_task.py||airflow/triggers/external_task.py": [
          "File: airflow/triggers/external_task.py -> airflow/triggers/external_task.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "90:         If dag with specified name was not in the running state after _timeout_sec seconds",
          "91:         after starting execution process of the trigger, terminate with status 'timeout'.",
          "92:         \"\"\"",
          "95:                 delta = utcnow() - self.trigger_start_time",
          "96:                 if delta.total_seconds() < self._timeout_sec:",
          "97:                     # mypy confuses typing here",
          "",
          "[Removed Lines]",
          "93:         while True:",
          "94:             try:",
          "",
          "[Added Lines]",
          "93:         try:",
          "94:             while True:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "107:                     return",
          "108:                 self.log.info(\"Task is still running, sleeping for %s seconds...\", self.poll_interval)",
          "109:                 await asyncio.sleep(self.poll_interval)",
          "114:     @sync_to_async",
          "115:     @provide_session",
          "",
          "[Removed Lines]",
          "110:             except Exception:",
          "111:                 yield TriggerEvent({\"status\": \"failed\"})",
          "112:                 return",
          "",
          "[Added Lines]",
          "110:         except Exception:",
          "111:             yield TriggerEvent({\"status\": \"failed\"})",
          "",
          "---------------"
        ],
        "airflow/www/extensions/init_security.py||airflow/www/extensions/init_security.py": [
          "File: airflow/www/extensions/init_security.py -> airflow/www/extensions/init_security.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "56:         pass",
          "58:     app.api_auth = []",
          "61:             auth = import_module(backend.strip())",
          "62:             auth.init_app(app)",
          "63:             app.api_auth.append(auth)",
          "69: def init_check_user_active(app):",
          "",
          "[Removed Lines]",
          "59:     for backend in auth_backends.split(\",\"):",
          "60:         try:",
          "64:         except ImportError as err:",
          "65:             log.critical(\"Cannot import %s for API authentication due to: %s\", backend, err)",
          "66:             raise AirflowException(err)",
          "",
          "[Added Lines]",
          "59:     try:",
          "60:         for backend in auth_backends.split(\",\"):",
          "64:     except ImportError as err:",
          "65:         log.critical(\"Cannot import %s for API authentication due to: %s\", backend, err)",
          "66:         raise AirflowException(err)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "174c8a02d11926356ea6f808709c50edc8129d0b",
      "candidate_info": {
        "commit_hash": "174c8a02d11926356ea6f808709c50edc8129d0b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/174c8a02d11926356ea6f808709c50edc8129d0b",
        "files": [
          "airflow/www/views.py"
        ],
        "message": "Fix is_parent_mapped value by checking if any of the parent tg is mapped (#34587)\n\n(cherry picked from commit 97916ba45ccf73185a5fbf50270a493369da0344)",
        "before_after_code_files": [
          "airflow/www/views.py||airflow/www/views.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "426:             }",
          "428:         # Task Group",
          "429:         task_group = item",
          "432:         children = [",
          "433:             task_group_to_grid(child, grouped_tis, is_parent_mapped=group_is_mapped)",
          "",
          "[Removed Lines]",
          "430:         group_is_mapped = isinstance(task_group, MappedTaskGroup)",
          "",
          "[Added Lines]",
          "428:         def check_group_is_mapped(tg: TaskGroup | None) -> bool:",
          "429:             if tg is None:",
          "430:                 return False",
          "431:             return isinstance(tg, MappedTaskGroup) or check_group_is_mapped(tg.parent_group)",
          "435:         group_is_mapped = check_group_is_mapped(task_group)",
          "",
          "---------------"
        ]
      }
    }
  ]
}