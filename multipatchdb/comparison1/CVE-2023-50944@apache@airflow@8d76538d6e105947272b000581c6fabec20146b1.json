{
  "cve_id": "CVE-2023-50944",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows an authenticated user to access the source code of a DAG to which they don't have access.\u00a0This vulnerability is considered low since it requires an authenticated user to exploit it. Users are recommended to upgrade to version 2.8.1, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "8d76538d6e105947272b000581c6fabec20146b1",
  "patch_info": {
    "commit_hash": "8d76538d6e105947272b000581c6fabec20146b1",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/8d76538d6e105947272b000581c6fabec20146b1",
    "files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ],
    "message": "Check DAG read permission before accessing DAG code (#36257)\n\n(cherry picked from commit 30ea37e0d247ce54c2d25b115e807fdb0074d795)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py||airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_source_endpoint.py -> airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "17: from __future__ import annotations",
      "19: from http import HTTPStatus",
      "21: from flask import Response, current_app, request",
      "22: from itsdangerous import BadSignature, URLSafeSerializer",
      "24: from airflow.api_connexion import security",
      "26: from airflow.api_connexion.schemas.dag_source_schema import dag_source_schema",
      "27: from airflow.auth.managers.models.resource_details import DagAccessEntity",
      "28: from airflow.models.dagcode import DagCode",
      "31: @security.requires_access_dag(\"GET\", DagAccessEntity.CODE)",
      "33:     \"\"\"Get source code using file token.\"\"\"",
      "34:     secret_key = current_app.config[\"SECRET_KEY\"]",
      "35:     auth_s = URLSafeSerializer(secret_key)",
      "36:     try:",
      "37:         path = auth_s.loads(file_token)",
      "39:     except (BadSignature, FileNotFoundError):",
      "40:         raise NotFound(\"Dag source not found\")",
      "",
      "[Removed Lines]",
      "25: from airflow.api_connexion.exceptions import NotFound",
      "32: def get_dag_source(*, file_token: str) -> Response:",
      "38:         dag_source = DagCode.code(path)",
      "",
      "[Added Lines]",
      "20: from typing import TYPE_CHECKING",
      "26: from airflow.api_connexion.exceptions import NotFound, PermissionDenied",
      "28: from airflow.api_connexion.security import get_readable_dags",
      "30: from airflow.models.dag import DagModel",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "34: if TYPE_CHECKING:",
      "35:     from sqlalchemy.orm import Session",
      "39: @provide_session",
      "40: def get_dag_source(*, file_token: str, session: Session = NEW_SESSION) -> Response:",
      "46:         dag_ids = session.query(DagModel.dag_id).filter(DagModel.fileloc == path).all()",
      "47:         readable_dags = get_readable_dags()",
      "48:         # Check if user has read access to all the DAGs defined in the file",
      "49:         if any(dag_id[0] not in readable_dags for dag_id in dag_ids):",
      "50:             raise PermissionDenied()",
      "51:         dag_source = DagCode.code(path, session=session)",
      "",
      "---------------"
    ],
    "airflow/models/dagcode.py||airflow/models/dagcode.py": [
      "File: airflow/models/dagcode.py -> airflow/models/dagcode.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "177:         return cls.code(fileloc)",
      "179:     @classmethod",
      "181:         \"\"\"Return source code for this DagCode object.",
      "183:         :return: source code as string",
      "184:         \"\"\"",
      "187:     @staticmethod",
      "188:     def _get_code_from_file(fileloc):",
      "",
      "[Removed Lines]",
      "180:     def code(cls, fileloc) -> str:",
      "185:         return cls._get_code_from_db(fileloc)",
      "",
      "[Added Lines]",
      "180:     @provide_session",
      "181:     def code(cls, fileloc, session: Session = NEW_SESSION) -> str:",
      "186:         return cls._get_code_from_db(fileloc, session)",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_source_endpoint.py -> tests/api_connexion/endpoints/test_dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35: ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))",
      "36: EXAMPLE_DAG_FILE = os.path.join(\"airflow\", \"example_dags\", \"example_bash_operator.py\")",
      "39: @pytest.fixture(scope=\"module\")",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "37: EXAMPLE_DAG_ID = \"example_bash_operator\"",
      "38: TEST_DAG_ID = \"latest_only\"",
      "39: NOT_READABLE_DAG_ID = \"latest_only_with_trigger\"",
      "40: TEST_MULTIPLE_DAGS_ID = \"dataset_produces_1\"",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "45:         role_name=\"Test\",",
      "46:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_CODE)],  # type: ignore",
      "47:     )",
      "48:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "50:     yield app",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "52:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "53:         TEST_DAG_ID,",
      "54:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "55:     )",
      "56:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "57:         EXAMPLE_DAG_ID,",
      "58:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "59:     )",
      "60:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "61:         TEST_MULTIPLE_DAGS_ID,",
      "62:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "63:     )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "80:     def test_should_respond_200_text(self, url_safe_serializer):",
      "81:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "82:         dagbag.sync_to_db()",
      "87:         response = self.client.get(",
      "88:             url, headers={\"Accept\": \"text/plain\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "89:         )",
      "",
      "[Removed Lines]",
      "83:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "84:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "86:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "99:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "100:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "102:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "95:     def test_should_respond_200_json(self, url_safe_serializer):",
      "96:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "97:         dagbag.sync_to_db()",
      "102:         response = self.client.get(",
      "103:             url, headers={\"Accept\": \"application/json\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "104:         )",
      "",
      "[Removed Lines]",
      "98:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "99:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "101:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "114:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "115:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "117:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "110:     def test_should_respond_406(self, url_safe_serializer):",
      "111:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "112:         dagbag.sync_to_db()",
      "116:         response = self.client.get(",
      "117:             url, headers={\"Accept\": \"image/webp\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "118:         )",
      "",
      "[Removed Lines]",
      "113:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "115:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "129:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "131:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "151:             environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"},",
      "152:         )",
      "153:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "171:     def test_should_respond_403_not_readable(self, url_safe_serializer):",
      "172:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "173:         dagbag.sync_to_db()",
      "174:         dag: DAG = dagbag.dags[NOT_READABLE_DAG_ID]",
      "176:         response = self.client.get(",
      "177:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "178:             headers={\"Accept\": \"text/plain\"},",
      "179:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "180:         )",
      "181:         read_dag = self.client.get(",
      "182:             f\"/api/v1/dags/{NOT_READABLE_DAG_ID}\",",
      "183:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "184:         )",
      "185:         assert response.status_code == 403",
      "186:         assert read_dag.status_code == 403",
      "188:     def test_should_respond_403_some_dags_not_readable_in_the_file(self, url_safe_serializer):",
      "189:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "190:         dagbag.sync_to_db()",
      "191:         dag: DAG = dagbag.dags[TEST_MULTIPLE_DAGS_ID]",
      "193:         response = self.client.get(",
      "194:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "195:             headers={\"Accept\": \"text/plain\"},",
      "196:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "197:         )",
      "199:         read_dag = self.client.get(",
      "200:             f\"/api/v1/dags/{TEST_MULTIPLE_DAGS_ID}\",",
      "201:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "202:         )",
      "203:         assert response.status_code == 403",
      "204:         assert read_dag.status_code == 200",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "30ea37e0d247ce54c2d25b115e807fdb0074d795",
      "candidate_info": {
        "commit_hash": "30ea37e0d247ce54c2d25b115e807fdb0074d795",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/30ea37e0d247ce54c2d25b115e807fdb0074d795",
        "files": [
          "airflow/api_connexion/endpoints/dag_source_endpoint.py",
          "airflow/models/dagcode.py",
          "tests/api_connexion/endpoints/test_dag_source_endpoint.py"
        ],
        "message": "Check DAG read permission before accessing DAG code (#36257)",
        "before_after_code_files": [
          "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
          "airflow/models/dagcode.py||airflow/models/dagcode.py",
          "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "diff_branch_cherry_pick": 1,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
            "airflow/models/dagcode.py||airflow/models/dagcode.py",
            "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
          ],
          "candidate": [
            "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
            "airflow/models/dagcode.py||airflow/models/dagcode.py",
            "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py": [
          "File: airflow/api_connexion/endpoints/dag_source_endpoint.py -> airflow/api_connexion/endpoints/dag_source_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: from __future__ import annotations",
          "19: from http import HTTPStatus",
          "21: from flask import Response, current_app, request",
          "22: from itsdangerous import BadSignature, URLSafeSerializer",
          "24: from airflow.api_connexion import security",
          "26: from airflow.api_connexion.schemas.dag_source_schema import dag_source_schema",
          "27: from airflow.auth.managers.models.resource_details import DagAccessEntity",
          "28: from airflow.models.dagcode import DagCode",
          "31: @security.requires_access_dag(\"GET\", DagAccessEntity.CODE)",
          "33:     \"\"\"Get source code using file token.\"\"\"",
          "34:     secret_key = current_app.config[\"SECRET_KEY\"]",
          "35:     auth_s = URLSafeSerializer(secret_key)",
          "36:     try:",
          "37:         path = auth_s.loads(file_token)",
          "39:     except (BadSignature, FileNotFoundError):",
          "40:         raise NotFound(\"Dag source not found\")",
          "",
          "[Removed Lines]",
          "25: from airflow.api_connexion.exceptions import NotFound",
          "32: def get_dag_source(*, file_token: str) -> Response:",
          "38:         dag_source = DagCode.code(path)",
          "",
          "[Added Lines]",
          "20: from typing import TYPE_CHECKING",
          "26: from airflow.api_connexion.exceptions import NotFound, PermissionDenied",
          "28: from airflow.api_connexion.security import get_readable_dags",
          "30: from airflow.models.dag import DagModel",
          "32: from airflow.utils.session import NEW_SESSION, provide_session",
          "34: if TYPE_CHECKING:",
          "35:     from sqlalchemy.orm import Session",
          "39: @provide_session",
          "40: def get_dag_source(*, file_token: str, session: Session = NEW_SESSION) -> Response:",
          "46:         dag_ids = session.query(DagModel.dag_id).filter(DagModel.fileloc == path).all()",
          "47:         readable_dags = get_readable_dags()",
          "48:         # Check if user has read access to all the DAGs defined in the file",
          "49:         if any(dag_id[0] not in readable_dags for dag_id in dag_ids):",
          "50:             raise PermissionDenied()",
          "51:         dag_source = DagCode.code(path, session=session)",
          "",
          "---------------"
        ],
        "airflow/models/dagcode.py||airflow/models/dagcode.py": [
          "File: airflow/models/dagcode.py -> airflow/models/dagcode.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "177:         return cls.code(fileloc)",
          "179:     @classmethod",
          "181:         \"\"\"Return source code for this DagCode object.",
          "183:         :return: source code as string",
          "184:         \"\"\"",
          "187:     @staticmethod",
          "188:     def _get_code_from_file(fileloc):",
          "",
          "[Removed Lines]",
          "180:     def code(cls, fileloc) -> str:",
          "185:         return cls._get_code_from_db(fileloc)",
          "",
          "[Added Lines]",
          "180:     @provide_session",
          "181:     def code(cls, fileloc, session: Session = NEW_SESSION) -> str:",
          "186:         return cls._get_code_from_db(fileloc, session)",
          "",
          "---------------"
        ],
        "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_dag_source_endpoint.py -> tests/api_connexion/endpoints/test_dag_source_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "35: ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))",
          "36: EXAMPLE_DAG_FILE = os.path.join(\"airflow\", \"example_dags\", \"example_bash_operator.py\")",
          "39: @pytest.fixture(scope=\"module\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "37: EXAMPLE_DAG_ID = \"example_bash_operator\"",
          "38: TEST_DAG_ID = \"latest_only\"",
          "39: NOT_READABLE_DAG_ID = \"latest_only_with_trigger\"",
          "40: TEST_MULTIPLE_DAGS_ID = \"dataset_produces_1\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "45:         role_name=\"Test\",",
          "46:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_CODE)],  # type: ignore",
          "47:     )",
          "48:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
          "50:     yield app",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "52:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
          "53:         TEST_DAG_ID,",
          "54:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
          "55:     )",
          "56:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
          "57:         EXAMPLE_DAG_ID,",
          "58:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
          "59:     )",
          "60:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
          "61:         TEST_MULTIPLE_DAGS_ID,",
          "62:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
          "63:     )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "80:     def test_should_respond_200_text(self, url_safe_serializer):",
          "81:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
          "82:         dagbag.sync_to_db()",
          "87:         response = self.client.get(",
          "88:             url, headers={\"Accept\": \"text/plain\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
          "89:         )",
          "",
          "[Removed Lines]",
          "83:         first_dag: DAG = next(iter(dagbag.dags.values()))",
          "84:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
          "86:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
          "",
          "[Added Lines]",
          "99:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
          "100:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
          "102:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "95:     def test_should_respond_200_json(self, url_safe_serializer):",
          "96:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
          "97:         dagbag.sync_to_db()",
          "102:         response = self.client.get(",
          "103:             url, headers={\"Accept\": \"application/json\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
          "104:         )",
          "",
          "[Removed Lines]",
          "98:         first_dag: DAG = next(iter(dagbag.dags.values()))",
          "99:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
          "101:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
          "",
          "[Added Lines]",
          "114:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
          "115:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
          "117:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "110:     def test_should_respond_406(self, url_safe_serializer):",
          "111:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
          "112:         dagbag.sync_to_db()",
          "116:         response = self.client.get(",
          "117:             url, headers={\"Accept\": \"image/webp\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
          "118:         )",
          "",
          "[Removed Lines]",
          "113:         first_dag: DAG = next(iter(dagbag.dags.values()))",
          "115:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
          "",
          "[Added Lines]",
          "129:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
          "131:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "151:             environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"},",
          "152:         )",
          "153:         assert response.status_code == 403",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "171:     def test_should_respond_403_not_readable(self, url_safe_serializer):",
          "172:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
          "173:         dagbag.sync_to_db()",
          "174:         dag: DAG = dagbag.dags[NOT_READABLE_DAG_ID]",
          "176:         response = self.client.get(",
          "177:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
          "178:             headers={\"Accept\": \"text/plain\"},",
          "179:             environ_overrides={\"REMOTE_USER\": \"test\"},",
          "180:         )",
          "181:         read_dag = self.client.get(",
          "182:             f\"/api/v1/dags/{NOT_READABLE_DAG_ID}\",",
          "183:             environ_overrides={\"REMOTE_USER\": \"test\"},",
          "184:         )",
          "185:         assert response.status_code == 403",
          "186:         assert read_dag.status_code == 403",
          "188:     def test_should_respond_403_some_dags_not_readable_in_the_file(self, url_safe_serializer):",
          "189:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
          "190:         dagbag.sync_to_db()",
          "191:         dag: DAG = dagbag.dags[TEST_MULTIPLE_DAGS_ID]",
          "193:         response = self.client.get(",
          "194:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
          "195:             headers={\"Accept\": \"text/plain\"},",
          "196:             environ_overrides={\"REMOTE_USER\": \"test\"},",
          "197:         )",
          "199:         read_dag = self.client.get(",
          "200:             f\"/api/v1/dags/{TEST_MULTIPLE_DAGS_ID}\",",
          "201:             environ_overrides={\"REMOTE_USER\": \"test\"},",
          "202:         )",
          "203:         assert response.status_code == 403",
          "204:         assert read_dag.status_code == 200",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "75dc15795033fa23b5edd8d084d38c3bf1de7744",
      "candidate_info": {
        "commit_hash": "75dc15795033fa23b5edd8d084d38c3bf1de7744",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/75dc15795033fa23b5edd8d084d38c3bf1de7744",
        "files": [
          "dev/README_RELEASE_AIRFLOW.md"
        ],
        "message": "Add steps to update constraints and adjust version for chicken-egg (#36288)\n\nWhen Chicken-egg providers are released, we also have to do some\nmanual adjustments of constraints and version of airflow in\nthe v2-8-test branch.\n\n(cherry picked from commit 35117aa9e931ec6b9e4c5669cd23014bd38c7514)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "ca1534b9ef8ae2277e3af4b5b70da0379e2fd894",
      "candidate_info": {
        "commit_hash": "ca1534b9ef8ae2277e3af4b5b70da0379e2fd894",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ca1534b9ef8ae2277e3af4b5b70da0379e2fd894",
        "files": [
          "docs/exts/operators_and_hooks_ref.py",
          "docs/exts/templates/deferrable_operators_list.rst.jinja2"
        ],
        "message": "fix: Repair defferable operator list in docs (#36476)\n\n(cherry picked from commit 7cd326ccbd5da03900b01695c74a805ced980855)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "8986f8f3beba11cf568e0f014883b997e4042f7b",
      "candidate_info": {
        "commit_hash": "8986f8f3beba11cf568e0f014883b997e4042f7b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8986f8f3beba11cf568e0f014883b997e4042f7b",
        "files": [
          "dev/breeze/src/airflow_breeze/params/shell_params.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "scripts/in_container/install_airflow_and_providers.py"
        ],
        "message": "Fix using extras when `--use-airflow-version` is used in Breeze (#36280)\n\nWhen we are installing a released version of Airflow in Breeze, we can\npass additional extras to install (For example, we need to pass celery\nextra in order to start airflow with celery executor.\n\nThe extras could be specified as:\n\n```\nbreeze start-airflow --use-airflow-version 2.8.0rc4  \\\n  --executor CeleryExecutor --airflow-extras \"celery\"\n\n```\n\nHowever recent refactors caused a problem that the extras added were\nspecified after version (which is rejected by newer versions of `pip`).\n\nThis PR fixes it and also moves the place where CeleryExecutor use\ntriggers adding celery extra when`--use-airflow-version` is used.\n\nThe warning about this is better visible when moving to Shell Params.\n\n(cherry picked from commit 329780649543ab7b9d593a2e2428073fbd4cf274)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/params/shell_params.py||dev/breeze/src/airflow_breeze/params/shell_params.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "scripts/in_container/install_airflow_and_providers.py||scripts/in_container/install_airflow_and_providers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/params/shell_params.py||dev/breeze/src/airflow_breeze/params/shell_params.py": [
          "File: dev/breeze/src/airflow_breeze/params/shell_params.py -> dev/breeze/src/airflow_breeze/params/shell_params.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "329:         if self.executor == \"CeleryExecutor\":",
          "330:             compose_file_list.append(DOCKER_COMPOSE_DIR / \"integration-celery.yml\")",
          "332:         compose_file_list.append(DOCKER_COMPOSE_DIR / \"base.yml\")",
          "333:         self.add_docker_in_docker(compose_file_list)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "331:             if self.use_airflow_version:",
          "332:                 current_extras = self.airflow_extras",
          "333:                 if \"celery\" not in current_extras.split(\",\"):",
          "334:                     get_console().print(",
          "335:                         \"[warning]Adding `celery` extras as it is implicitly needed by celery executor\"",
          "336:                     )",
          "337:                     self.airflow_extras = \",\".join(current_extras.split(\",\") + [\"celery\"])",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "751:             f\"Changing the executor to {SEQUENTIAL_EXECUTOR}.\\n\"",
          "752:         )",
          "753:         shell_params.executor = SEQUENTIAL_EXECUTOR",
          "768:     if shell_params.restart:",
          "769:         bring_compose_project_down(preserve_volumes=False, shell_params=shell_params)",
          "770:     if shell_params.include_mypy_volume:",
          "",
          "[Removed Lines]",
          "755:     if shell_params.executor == \"CeleryExecutor\" and shell_params.use_airflow_version:",
          "756:         if shell_params.airflow_extras and \"celery\" not in shell_params.airflow_extras.split():",
          "757:             get_console().print(",
          "758:                 f\"\\n[warning]CeleryExecutor requires airflow_extras: celery. \"",
          "759:                 f\"Adding celery to extras: '{shell_params.airflow_extras}'.\\n\"",
          "760:             )",
          "761:             shell_params.airflow_extras += \",celery\"",
          "762:         elif not shell_params.airflow_extras:",
          "763:             get_console().print(",
          "764:                 \"\\n[warning]CeleryExecutor requires airflow_extras: celery. \"",
          "765:                 \"Setting airflow extras to 'celery'.\\n\"",
          "766:             )",
          "767:             shell_params.airflow_extras = \"celery\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "scripts/in_container/install_airflow_and_providers.py||scripts/in_container/install_airflow_and_providers.py": [
          "File: scripts/in_container/install_airflow_and_providers.py -> scripts/in_container/install_airflow_and_providers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "253:         )",
          "254:     else:",
          "255:         console.print(f\"\\nInstalling airflow via apache-airflow=={use_airflow_version}\")",
          "257:         airflow_constraints_location = get_airflow_constraints_location(",
          "258:             airflow_skip_constraints=airflow_skip_constraints,",
          "259:             airflow_constraints_mode=airflow_constraints_mode,",
          "",
          "[Removed Lines]",
          "256:         airflow_package_spec = f\"apache-airflow=={use_airflow_version}{airflow_extras}\"",
          "",
          "[Added Lines]",
          "256:         airflow_package_spec = f\"apache-airflow{airflow_extras}=={use_airflow_version}\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "56b368fef7e2346bbe9047083f428b1ba5364155",
      "candidate_info": {
        "commit_hash": "56b368fef7e2346bbe9047083f428b1ba5364155",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/56b368fef7e2346bbe9047083f428b1ba5364155",
        "files": [
          "dev/airflow-github"
        ],
        "message": "Tracking airflow-github changelog activities using progress bar (#36610)\n\n* Tracking airflow-github changelog activities using progress bar\n\n* review comments from ephraimbuddy\n\n(cherry picked from commit b3b8ae99ca1ef0524854148ca8a549232e2f3ae0)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "357e81051d87e8266a0d04abe6be9ecc3520af74",
      "candidate_info": {
        "commit_hash": "357e81051d87e8266a0d04abe6be9ecc3520af74",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/357e81051d87e8266a0d04abe6be9ecc3520af74",
        "files": [
          "docs/apache-airflow/static/gh-jira-links.js"
        ],
        "message": "Fix multiple issues in release notes does not have links (#36503)\n\n(cherry picked from commit c87ef553bc1624ced15aeea2cbf3e012eee66de1)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "3c7032513e7dadfaaad93efd47922f0d84bdfda2",
      "candidate_info": {
        "commit_hash": "3c7032513e7dadfaaad93efd47922f0d84bdfda2",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/3c7032513e7dadfaaad93efd47922f0d84bdfda2",
        "files": [
          "airflow/operators/python.py",
          "tests/operators/test_python.py"
        ],
        "message": "Allow PythonVirtualenvOperator.skip_on_exit_code to be zero (#36361)\n\n(cherry picked from commit b2f1882c584feceda02d3ee5af086b5098701518)",
        "before_after_code_files": [
          "airflow/operators/python.py||airflow/operators/python.py",
          "tests/operators/test_python.py||tests/operators/test_python.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/operators/python.py||airflow/operators/python.py": [
          "File: airflow/operators/python.py -> airflow/operators/python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "386:             skip_on_exit_code",
          "387:             if isinstance(skip_on_exit_code, Container)",
          "388:             else [skip_on_exit_code]",
          "390:             else []",
          "391:         )",
          "",
          "[Removed Lines]",
          "389:             if skip_on_exit_code",
          "",
          "[Added Lines]",
          "389:             if skip_on_exit_code is not None",
          "",
          "---------------"
        ],
        "tests/operators/test_python.py||tests/operators/test_python.py": [
          "File: tests/operators/test_python.py -> tests/operators/test_python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "847:         assert set(context) == declared_keys",
          "849:     @pytest.mark.parametrize(",
          "851:         [",
          "853:             ({\"skip_on_exit_code\": 100}, 100, TaskInstanceState.SKIPPED),",
          "856:             ({\"skip_on_exit_code\": 100}, 101, TaskInstanceState.FAILED),",
          "857:             ({\"skip_on_exit_code\": [100, 102]}, 101, TaskInstanceState.FAILED),",
          "859:         ],",
          "860:     )",
          "862:         def f(exit_code):",
          "863:             if exit_code != 0:",
          "864:                 raise SystemExit(exit_code)",
          "866:         if expected_state == TaskInstanceState.FAILED:",
          "867:             with pytest.raises(CalledProcessError):",
          "869:         else:",
          "870:             ti = self.run_as_task(",
          "871:                 f,",
          "872:                 return_ti=True,",
          "873:                 op_kwargs={\"exit_code\": actual_exit_code},",
          "875:             )",
          "876:             assert ti.state == expected_state",
          "",
          "[Removed Lines]",
          "850:         \"extra_kwargs, actual_exit_code, expected_state\",",
          "852:             (None, 99, TaskInstanceState.FAILED),",
          "854:             ({\"skip_on_exit_code\": [100]}, 100, TaskInstanceState.SKIPPED),",
          "855:             ({\"skip_on_exit_code\": (100, 101)}, 100, TaskInstanceState.SKIPPED),",
          "858:             ({\"skip_on_exit_code\": None}, 0, TaskInstanceState.SUCCESS),",
          "861:     def test_on_skip_exit_code(self, extra_kwargs, actual_exit_code, expected_state):",
          "868:                 self.run_as_task(f, op_kwargs={\"exit_code\": actual_exit_code}, **(extra_kwargs or {}))",
          "",
          "[Added Lines]",
          "850:         \"kwargs, actual_exit_code, expected_state\",",
          "852:             ({}, 0, TaskInstanceState.SUCCESS),",
          "853:             ({}, 100, TaskInstanceState.FAILED),",
          "854:             ({}, 101, TaskInstanceState.FAILED),",
          "855:             ({\"skip_on_exit_code\": None}, 0, TaskInstanceState.SUCCESS),",
          "856:             ({\"skip_on_exit_code\": None}, 100, TaskInstanceState.FAILED),",
          "857:             ({\"skip_on_exit_code\": None}, 101, TaskInstanceState.FAILED),",
          "858:             ({\"skip_on_exit_code\": 100}, 0, TaskInstanceState.SUCCESS),",
          "861:             ({\"skip_on_exit_code\": 0}, 0, TaskInstanceState.SKIPPED),",
          "862:             ({\"skip_on_exit_code\": [100]}, 0, TaskInstanceState.SUCCESS),",
          "863:             ({\"skip_on_exit_code\": [100]}, 100, TaskInstanceState.SKIPPED),",
          "864:             ({\"skip_on_exit_code\": [100]}, 101, TaskInstanceState.FAILED),",
          "866:             ({\"skip_on_exit_code\": (100,)}, 0, TaskInstanceState.SUCCESS),",
          "867:             ({\"skip_on_exit_code\": (100,)}, 100, TaskInstanceState.SKIPPED),",
          "868:             ({\"skip_on_exit_code\": (100,)}, 101, TaskInstanceState.FAILED),",
          "871:     def test_on_skip_exit_code(self, kwargs, actual_exit_code, expected_state):",
          "878:                 self.run_as_task(f, op_kwargs={\"exit_code\": actual_exit_code}, **kwargs)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "097659eb5b4d8bef0fc9471a319adff984af96b7",
      "candidate_info": {
        "commit_hash": "097659eb5b4d8bef0fc9471a319adff984af96b7",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/097659eb5b4d8bef0fc9471a319adff984af96b7",
        "files": [
          "dev/README_RELEASE_AIRFLOW.md"
        ],
        "message": "Move removal of chicken-egg providers to before releasing image (#36286)\n\nOnce Airflow is relesed to PyPI we should remove chicken-egg\nproviders for that release and cherry-pick them to v2-*-test in\norder to prepare container images in case the image contains the\nproviders as default extras.\n\n(cherry picked from commit 663dfd007000824a71f84f6db84a43654a2dbe4f)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "7037d0e8db841ede59ded319ba75d12d2112bd4b",
      "candidate_info": {
        "commit_hash": "7037d0e8db841ede59ded319ba75d12d2112bd4b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7037d0e8db841ede59ded319ba75d12d2112bd4b",
        "files": [
          "airflow/providers/mysql/provider.yaml",
          "generated/provider_dependencies.json"
        ],
        "message": "Bump min version of mysql-connector-python (#36668)\n\nAs part of extraction from #36537 I noticed that mysql-connector-python\ncauses also a lot of backtracking.\n\nThis PR bumps minimum version of the mysql-connection-python from\nREALLY OLD (2018) to JUST OLD (2022).\n\n(cherry picked from commit c29632aab4cf13a861bfffcbd11f1c9964d8910a)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "3b5a460d6620d5c883e7b26edf40c053f63406ca",
      "candidate_info": {
        "commit_hash": "3b5a460d6620d5c883e7b26edf40c053f63406ca",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/3b5a460d6620d5c883e7b26edf40c053f63406ca",
        "files": [
          "docs/apache-airflow/howto/set-up-database.rst"
        ],
        "message": "Remove Redshift mention from the list of managed Postgres backends (#36217)\n\n(cherry picked from commit 01fd0d31b46682f4d700aaacf19cfe7a0fe9a057)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "2cad58ad4e58f7b4c18b0b903de307faae265565",
      "candidate_info": {
        "commit_hash": "2cad58ad4e58f7b4c18b0b903de307faae265565",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/2cad58ad4e58f7b4c18b0b903de307faae265565",
        "files": [
          ".pre-commit-config.yaml"
        ],
        "message": "Updating version of airflow pre-commit should happen when version change (#36293)\n\n(cherry picked from commit b5cd96a27f30bb5515299eaf8c15204291030c37)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "b3031114b8d0d3aff361475acbcd2d987b11298d",
      "candidate_info": {
        "commit_hash": "b3031114b8d0d3aff361475acbcd2d987b11298d",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b3031114b8d0d3aff361475acbcd2d987b11298d",
        "files": [
          ".github/actions/build-ci-images/action.yml",
          "generated/provider_dependencies.json",
          "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py"
        ],
        "message": "Remove additional generation of dependencies when building CI images (#36283)\n\nWhen generated dependencies are not properly updated, we had a special\nstep where the dependencies were generated \"just in case\" before CI\nimage was built, because otherwise building the CI image could have\nfailed with strange \"failed because of conflicting dependencies\"\nwithout a clue what was the root cause.\n\nHowever, the pre-commit did not return error exit code - because for the\npre-commit, it is enough that a file is modified during pre-commit to\nfail the pre-commit in general.\n\nThat had a nasty side effect because the built CI image actually already\ncontained properly generated dependencies (by this step), and it did not\nproperly detected cases where the ones in the repository were added\nmanually and not generated with pre-commit.\n\nThis PR fixes it - instead of generating and building such image in\nCI it will now fail the CI image building step but with clear\ninstructions what to do.\n\nThe CI job step uses now regular breeze command rather than running\nthe script manually but also the script returns error code in case\nthe generated dependencies have been updated.\n\n(cherry picked from commit 33a2fbef9ff656c3522ea8dea5fff2e2c2645abf)",
        "before_after_code_files": [
          "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py||scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py||scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py": [
          "File: scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py -> scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "213:         DEPENDENCIES_JSON_FILE_PATH.write_text(json.dumps(unique_sorted_dependencies, indent=2) + \"\\n\")",
          "214:         if os.environ.get(\"CI\"):",
          "215:             console.print()",
          "217:             console.print(",
          "220:             )",
          "221:             console.print()",
          "222:         else:",
          "223:             console.print()",
          "",
          "[Removed Lines]",
          "216:             console.print(f\"[info]Written {DEPENDENCIES_JSON_FILE_PATH}\")",
          "218:                 f\"[yellow]You will need to run breeze locally and commit \"",
          "219:                 f\"{DEPENDENCIES_JSON_FILE_PATH.relative_to(AIRFLOW_SOURCES_ROOT)}!\\n\"",
          "",
          "[Added Lines]",
          "216:             console.print(f\"[info]There is a need to regenerate {DEPENDENCIES_JSON_FILE_PATH}\")",
          "218:                 f\"[red]You need to run the following command locally and commit generated \"",
          "219:                 f\"{DEPENDENCIES_JSON_FILE_PATH.relative_to(AIRFLOW_SOURCES_ROOT)} file:\\n\"",
          "221:             console.print(\"breeze static-checks --type update-providers-dependencies --all-files\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "227:             )",
          "228:             console.print(f\"[info]Written {DEPENDENCIES_JSON_FILE_PATH}\")",
          "229:             console.print()",
          "230:     else:",
          "231:         console.print(",
          "232:             \"[green]No need to regenerate dependencies!\\n[/]\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "231:         sys.exit(1)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e9b3201d583fc4ea6c96e67084e154436879fd22",
      "candidate_info": {
        "commit_hash": "e9b3201d583fc4ea6c96e67084e154436879fd22",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e9b3201d583fc4ea6c96e67084e154436879fd22",
        "files": [
          "docs/apache-airflow/authoring-and-scheduling/dynamic-task-mapping.rst"
        ],
        "message": "Add branching based on mapped task group example to dynamic-task-mapping.rst (#36480)\n\n* Add branching based on mapped task group example to dynamic-task-mapping.rst\n\nBased on trying to solve [this stack overflow question](https://stackoverflow.com/questions/77730116/branching-not-working-in-airflow-as-expected/77730300#77730300), it seems impossible to reliably branch mapped tasks based on the result of an upstream task. However, it's possible to do this in a mapped task group, which this example demonstrates.\n\n* trying to force blacken-docs\n\n(cherry picked from commit 0d9a26ceefb2a5661f19cce292e05939d3f2a0c1)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "63c4546bb7e7d9b3c393755737796332184c6eb2",
      "candidate_info": {
        "commit_hash": "63c4546bb7e7d9b3c393755737796332184c6eb2",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/63c4546bb7e7d9b3c393755737796332184c6eb2",
        "files": [
          "airflow/providers/amazon/provider.yaml",
          "generated/provider_dependencies.json",
          "setup.py"
        ],
        "message": "Bump min version of amazon-provider related dependencies (#36660)\n\nThis is a regular bump of Amazon-provider related dependencies.\nThe way how botocore releases are done, they are putting a lot of\nstrain on `pip` to resolve the right set of dependencies, including\nlong backtracking, when there are too man versions available.\n\nTherefore, from time to time, we are bumping minimum version of\nAmazon-related dependencies to limit the impact frequent releases\nof boto and botocore has. Also it is generally fine to update min\nversion of dependencies for providers because at the very least\nusers can still use previously released providers in case they\nhave problem with those dependencies, also many of the updated\ndependencies contain fixes and feature we implicitly depend on and\nbumping them regulary is a good way to make sure all the functionalities\nof the Amazon provider are working as expected.\n\nAnother reason for the bump is that as of 1.33 version botocore and\nboto version stopped being shifted by 3 (previously boto3 1.28 was\nthe version corresponding to botocore 1.31). As of version 1.33 this\nproblem has been solved. See https://github.com/boto/boto3/issues/2702\n\nWatchtower min version is bumped to version 3 (which is 12 months old\neven if before we opted for much older (more than 2 years old) and again\nif users want to use older version of watchtower, they can opt for\nprevious provider version.\n\nThis change saves 5-6 minutes of backtracking when `pip` try to\nfind the right version of dependencies when upgrading to newer version.\n\nExtracted from #36537\n\n(cherry picked from commit 298c37d355eeadfccbd655efb2922d39ba17052c)",
        "before_after_code_files": [
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "409: ]",
          "411: # make sure to update providers/amazon/provider.yaml botocore min version when you update it here",
          "414: _devel_only_amazon = [",
          "417:     f\"mypy-boto3-rds>={_MIN_BOTO3_VERSION}\",",
          "418:     f\"mypy-boto3-redshift-data>={_MIN_BOTO3_VERSION}\",",
          "419:     f\"mypy-boto3-s3>={_MIN_BOTO3_VERSION}\",",
          "421: ]",
          "423: _devel_only_azure = [",
          "",
          "[Removed Lines]",
          "412: _MIN_BOTO3_VERSION = \"1.28.0\"",
          "415:     \"aws_xray_sdk\",",
          "416:     \"moto[cloudformation,glue]>=4.2.9\",",
          "420:     f\"mypy-boto3-appflow>={_MIN_BOTO3_VERSION}\",",
          "",
          "[Added Lines]",
          "412: _MIN_BOTO3_VERSION = \"1.33.0\"",
          "415:     \"aws_xray_sdk>=2.12.0\",",
          "416:     \"moto[cloudformation,glue]>=4.2.12\",",
          "417:     f\"mypy-boto3-appflow>={_MIN_BOTO3_VERSION}\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "010bb695b8d3157f937e489ce6a20935b801b9d9",
      "candidate_info": {
        "commit_hash": "010bb695b8d3157f937e489ce6a20935b801b9d9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/010bb695b8d3157f937e489ce6a20935b801b9d9",
        "files": [
          ".pre-commit-config.yaml",
          "STATIC_CODE_CHECKS.rst",
          "airflow/contrib/operators/__init__.py",
          "airflow/contrib/secrets/__init__.py",
          "airflow/contrib/sensors/__init__.py",
          "airflow/contrib/utils/__init__.py",
          "pyproject.toml",
          "scripts/ci/pre_commit/pre_commit_ruff_format.py",
          "setup.py"
        ],
        "message": "Upgrade to latest ruff and remove ISC001 warning from output (#36649)\n\nThis PR upgrades to latest ruff, and removes the ISC001 warning that\nwarns us against potential conflict between ruff and ruff-formatter\nwhen two strings in one line get concatenated.\n\nThis warning makes sense if you run both ruff and formatting at the\nsame time, but in our case we are doing it in two separate\nsteps - one step is to run ruff linting and the second step is to\nrun formatting and running formatting already runs after linting\nis complete.\n\nThis warnign is pretty misleading as it distracts from real formatting\nissues you might have.\n\nThere is - unfortunately - no standard way to remove the warning\nso we have to do it a little \"around\" - rather than running\nthe pre-commit directly from ruff website, we run our local pre-commit\nwith few lines of Python code that runs ruff through shell and\ngreps out the ISC001 warning. We also force color to make\nsure the output is still coloured.\n\n(cherry picked from commit 11c46fd2ec165da32202b464be7c2df5cca4d6c0)",
        "before_after_code_files": [
          "airflow/contrib/operators/__init__.py||airflow/contrioperators/__init__.py",
          "airflow/contrib/secrets/__init__.py||airflow/contrisecrets/__init__.py",
          "airflow/contrib/sensors/__init__.py||airflow/contrisensors/__init__.py",
          "airflow/contrib/utils/__init__.py||airflow/contriutils/__init__.py",
          "scripts/ci/pre_commit/pre_commit_ruff_format.py||scripts/ci/pre_commit/pre_commit_ruff_format.py",
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/contrib/operators/__init__.py||airflow/contrioperators/__init__.py": [
          "File: airflow/contrib/operators/__init__.py -> airflow/contrioperators/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "180:         \"DataprocDeleteClusterOperator\": (",
          "181:             \"airflow.providers.google.cloud.operators.dataproc.DataprocDeleteClusterOperator\"",
          "182:         ),",
          "186:         \"DataprocInstantiateWorkflowTemplateOperator\": (",
          "187:             \"airflow.providers.google.cloud.operators.dataproc.DataprocInstantiateWorkflowTemplateOperator\"",
          "188:         ),",
          "",
          "[Removed Lines]",
          "183:         \"DataprocInstantiateInlineWorkflowTemplateOperator\":",
          "184:             \"airflow.providers.google.cloud.operators.dataproc.\"",
          "185:             \"DataprocInstantiateInlineWorkflowTemplateOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "240:         \"DataProcSparkSqlOperator\": (",
          "241:             \"airflow.providers.google.cloud.operators.dataproc.DataprocSubmitSparkSqlJobOperator\"",
          "242:         ),",
          "246:         \"DataprocWorkflowTemplateInstantiateOperator\": (",
          "247:             \"airflow.providers.google.cloud.operators.dataproc.DataprocInstantiateWorkflowTemplateOperator\"",
          "248:         ),",
          "",
          "[Removed Lines]",
          "243:         \"DataprocWorkflowTemplateInstantiateInlineOperator\":",
          "244:             \"airflow.providers.google.cloud.operators.dataproc.\"",
          "245:             \"DataprocInstantiateInlineWorkflowTemplateOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "351:         \"ComputeEngineCopyInstanceTemplateOperator\": (",
          "352:             \"airflow.providers.google.cloud.operators.compute.ComputeEngineCopyInstanceTemplateOperator\"",
          "353:         ),",
          "357:         \"ComputeEngineSetMachineTypeOperator\": (",
          "358:             \"airflow.providers.google.cloud.operators.compute.ComputeEngineSetMachineTypeOperator\"",
          "359:         ),",
          "",
          "[Removed Lines]",
          "354:         \"ComputeEngineInstanceGroupUpdateManagerTemplateOperator\":",
          "355:             \"airflow.providers.google.cloud.operators.compute.\"",
          "356:             \"ComputeEngineInstanceGroupUpdateManagerTemplateOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "364:             \"airflow.providers.google.cloud.operators.compute.ComputeEngineStopInstanceOperator\"",
          "365:         ),",
          "366:         \"GceBaseOperator\": \"airflow.providers.google.cloud.operators.compute.ComputeEngineBaseOperator\",",
          "370:         \"GceInstanceStartOperator\": (",
          "371:             \"airflow.providers.google.cloud.operators.compute.ComputeEngineStartInstanceOperator\"",
          "372:         ),",
          "",
          "[Removed Lines]",
          "367:         \"GceInstanceGroupManagerUpdateTemplateOperator\":",
          "368:             \"airflow.providers.google.cloud.operators.compute.\"",
          "369:             \"ComputeEngineInstanceGroupUpdateManagerTemplateOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "513:         ),",
          "514:     },",
          "515:     \"gcp_natural_language_operator\": {",
          "540:     },",
          "541:     \"gcp_spanner_operator\": {",
          "542:         \"SpannerDeleteDatabaseInstanceOperator\": (",
          "",
          "[Removed Lines]",
          "516:         \"CloudNaturalLanguageAnalyzeEntitiesOperator\":",
          "517:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "518:             \"CloudNaturalLanguageAnalyzeEntitiesOperator\",",
          "519:         \"CloudNaturalLanguageAnalyzeEntitySentimentOperator\":",
          "520:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "521:             \"CloudNaturalLanguageAnalyzeEntitySentimentOperator\",",
          "522:         \"CloudNaturalLanguageAnalyzeSentimentOperator\":",
          "523:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "524:             \"CloudNaturalLanguageAnalyzeSentimentOperator\",",
          "525:         \"CloudNaturalLanguageClassifyTextOperator\":",
          "526:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "527:             \"CloudNaturalLanguageClassifyTextOperator\",",
          "528:         \"CloudLanguageAnalyzeEntitiesOperator\":",
          "529:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "530:             \"CloudNaturalLanguageAnalyzeEntitiesOperator\",",
          "531:         \"CloudLanguageAnalyzeEntitySentimentOperator\":",
          "532:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "533:             \"CloudNaturalLanguageAnalyzeEntitySentimentOperator\",",
          "534:         \"CloudLanguageAnalyzeSentimentOperator\":",
          "535:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "536:             \"CloudNaturalLanguageAnalyzeSentimentOperator\",",
          "537:         \"CloudLanguageClassifyTextOperator\":",
          "538:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "539:             \"CloudNaturalLanguageClassifyTextOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "692:         ),",
          "693:     },",
          "694:     \"gcp_transfer_operator\": {",
          "755:     },",
          "756:     \"gcp_translate_operator\": {",
          "757:         \"CloudTranslateTextOperator\": (",
          "",
          "[Removed Lines]",
          "695:         \"CloudDataTransferServiceCancelOperationOperator\":",
          "696:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "697:             \"CloudDataTransferServiceCancelOperationOperator\",",
          "698:         \"CloudDataTransferServiceCreateJobOperator\":",
          "699:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "700:             \"CloudDataTransferServiceCreateJobOperator\",",
          "701:         \"CloudDataTransferServiceDeleteJobOperator\":",
          "702:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "703:             \"CloudDataTransferServiceDeleteJobOperator\",",
          "704:         \"CloudDataTransferServiceGCSToGCSOperator\":",
          "705:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "706:             \"CloudDataTransferServiceGCSToGCSOperator\",",
          "707:         \"CloudDataTransferServiceGetOperationOperator\":",
          "708:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "709:             \"CloudDataTransferServiceGetOperationOperator\",",
          "710:         \"CloudDataTransferServiceListOperationsOperator\":",
          "711:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "712:             \"CloudDataTransferServiceListOperationsOperator\",",
          "713:         \"CloudDataTransferServicePauseOperationOperator\":",
          "714:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "715:             \"CloudDataTransferServicePauseOperationOperator\",",
          "716:         \"CloudDataTransferServiceResumeOperationOperator\":",
          "717:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "718:             \"CloudDataTransferServiceResumeOperationOperator\",",
          "719:         \"CloudDataTransferServiceS3ToGCSOperator\":",
          "720:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "721:             \"CloudDataTransferServiceS3ToGCSOperator\",",
          "722:         \"CloudDataTransferServiceUpdateJobOperator\":",
          "723:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "724:             \"CloudDataTransferServiceUpdateJobOperator\",",
          "725:         \"GcpTransferServiceJobCreateOperator\":",
          "726:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "727:             \"CloudDataTransferServiceCreateJobOperator\",",
          "728:         \"GcpTransferServiceJobDeleteOperator\":",
          "729:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "730:             \"CloudDataTransferServiceDeleteJobOperator\",",
          "731:         \"GcpTransferServiceJobUpdateOperator\":",
          "732:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "733:             \"CloudDataTransferServiceUpdateJobOperator\",",
          "734:         \"GcpTransferServiceOperationCancelOperator\":",
          "735:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "736:             \"CloudDataTransferServiceCancelOperationOperator\",",
          "737:         \"GcpTransferServiceOperationGetOperator\":",
          "738:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "739:             \"CloudDataTransferServiceGetOperationOperator\",",
          "740:         \"GcpTransferServiceOperationPauseOperator\":",
          "741:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "742:             \"CloudDataTransferServicePauseOperationOperator\",",
          "743:         \"GcpTransferServiceOperationResumeOperator\":",
          "744:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "745:             \"CloudDataTransferServiceResumeOperationOperator\",",
          "746:         \"GcpTransferServiceOperationsListOperator\":",
          "747:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "748:             \"CloudDataTransferServiceListOperationsOperator\",",
          "749:         \"GoogleCloudStorageToGoogleCloudStorageTransferOperator\":",
          "750:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "751:             \"CloudDataTransferServiceGCSToGCSOperator\",",
          "752:         \"S3ToGoogleCloudStorageTransferOperator\":",
          "753:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "754:             \"CloudDataTransferServiceS3ToGCSOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "767:         ),",
          "768:     },",
          "769:     \"gcp_video_intelligence_operator\": {",
          "779:     },",
          "780:     \"gcp_vision_operator\": {",
          "781:         \"CloudVisionAddProductToProductSetOperator\": (",
          "",
          "[Removed Lines]",
          "770:         \"CloudVideoIntelligenceDetectVideoExplicitContentOperator\":",
          "771:             \"airflow.providers.google.cloud.operators.video_intelligence.\"",
          "772:             \"CloudVideoIntelligenceDetectVideoExplicitContentOperator\",",
          "773:         \"CloudVideoIntelligenceDetectVideoLabelsOperator\":",
          "774:             \"airflow.providers.google.cloud.operators.video_intelligence.\"",
          "775:             \"CloudVideoIntelligenceDetectVideoLabelsOperator\",",
          "776:         \"CloudVideoIntelligenceDetectVideoShotsOperator\":",
          "777:             \"airflow.providers.google.cloud.operators.video_intelligence.\"",
          "778:             \"CloudVideoIntelligenceDetectVideoShotsOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "945:         \"JiraOperator\": \"airflow.providers.atlassian.jira.operators.jira.JiraOperator\",",
          "946:     },",
          "947:     \"kubernetes_pod_operator\": {",
          "951:     },",
          "952:     \"mlengine_operator\": {",
          "953:         \"MLEngineManageModelOperator\": (",
          "",
          "[Removed Lines]",
          "948:         \"KubernetesPodOperator\": (",
          "949:             \"airflow.providers.cncf.kubernetes.operators.pod.KubernetesPodOperator\"",
          "950:         ),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "997:         \"OpsgenieAlertOperator\": \"airflow.providers.opsgenie.operators.opsgenie.OpsgenieCreateAlertOperator\",",
          "998:     },",
          "999:     \"oracle_to_azure_data_lake_transfer\": {",
          "1003:     },",
          "1004:     \"oracle_to_oracle_transfer\": {",
          "1005:         \"OracleToOracleOperator\": (",
          "",
          "[Removed Lines]",
          "1000:         \"OracleToAzureDataLakeOperator\":",
          "1001:             \"airflow.providers.microsoft.azure.transfers.\"",
          "1002:             \"oracle_to_azure_data_lake.OracleToAzureDataLakeOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1056:         \"S3ToGCSOperator\": \"airflow.providers.google.cloud.transfers.s3_to_gcs.S3ToGCSOperator\",",
          "1057:     },",
          "1058:     \"s3_to_gcs_transfer_operator\": {",
          "1062:     },",
          "1063:     \"s3_to_sftp_operator\": {",
          "1064:         \"S3ToSFTPOperator\": \"airflow.providers.amazon.aws.transfers.s3_to_sftp.S3ToSFTPOperator\",",
          "",
          "[Removed Lines]",
          "1059:         \"CloudDataTransferServiceS3ToGCSOperator\":",
          "1060:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "1061:             \"CloudDataTransferServiceS3ToGCSOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/contrib/secrets/__init__.py||airflow/contrisecrets/__init__.py": [
          "File: airflow/contrib/secrets/__init__.py -> airflow/contrisecrets/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: warnings.warn(",
          "27:     \"This module is deprecated. Please use airflow.providers.*.secrets.\",",
          "28:     RemovedInAirflow3Warning,",
          "30: )",
          "32: __deprecated_classes = {",
          "",
          "[Removed Lines]",
          "29:     stacklevel=2",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/contrib/sensors/__init__.py||airflow/contrisensors/__init__.py": [
          "File: airflow/contrib/sensors/__init__.py -> airflow/contrisensors/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "85:         \"FTPSSensor\": \"airflow.providers.ftp.sensors.ftp.FTPSSensor\",",
          "86:     },",
          "87:     \"gcp_transfer_sensor\": {",
          "94:     },",
          "95:     \"gcs_sensor\": {",
          "96:         \"GCSObjectExistenceSensor\": \"airflow.providers.google.cloud.sensors.gcs.GCSObjectExistenceSensor\",",
          "",
          "[Removed Lines]",
          "88:         \"CloudDataTransferServiceJobStatusSensor\":",
          "89:             \"airflow.providers.google.cloud.sensors.cloud_storage_transfer_service.\"",
          "90:             \"CloudDataTransferServiceJobStatusSensor\",",
          "91:         \"GCPTransferServiceWaitForJobStatusSensor\":",
          "92:             \"airflow.providers.google.cloud.sensors.cloud_storage_transfer_service.\"",
          "93:             \"CloudDataTransferServiceJobStatusSensor\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/contrib/utils/__init__.py||airflow/contriutils/__init__.py": [
          "File: airflow/contrib/utils/__init__.py -> airflow/contriutils/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: from airflow.utils.deprecation_tools import add_deprecated_classes",
          "26: warnings.warn(",
          "30: )",
          "32: __deprecated_classes = {",
          "",
          "[Removed Lines]",
          "27:     \"This module is deprecated. Please use `airflow.utils`.\",",
          "28:     RemovedInAirflow3Warning,",
          "29:     stacklevel=2",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_ruff_format.py||scripts/ci/pre_commit/pre_commit_ruff_format.py": [
          "File: scripts/ci/pre_commit/pre_commit_ruff_format.py -> scripts/ci/pre_commit/pre_commit_ruff_format.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: import os",
          "21: import subprocess",
          "23: ruff_format_cmd = \"ruff format --force-exclude 2>&1 | grep -v '`ISC001`. To avoid unexpected behavior'\"",
          "24: envcopy = os.environ.copy()",
          "25: envcopy[\"CLICOLOR_FORCE\"] = \"1\"",
          "26: subprocess.run(ruff_format_cmd, shell=True, check=True, env=envcopy)",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "466: _devel_only_static_checks = [",
          "467:     \"pre-commit\",",
          "468:     \"black\",",
          "470:     \"yamllint\",",
          "471: ]",
          "",
          "[Removed Lines]",
          "469:     \"ruff>=0.0.219\",",
          "",
          "[Added Lines]",
          "469:     \"ruff==0.1.11\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8317580ccf671d344bc45cea151f641800826a23",
      "candidate_info": {
        "commit_hash": "8317580ccf671d344bc45cea151f641800826a23",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8317580ccf671d344bc45cea151f641800826a23",
        "files": [
          "airflow/operators/python.py"
        ],
        "message": "Fix PythonVirtualenvOperator tests (#36367)\n\n(cherry picked from commit af4b51cdfc8f98bec9922facd165ea8a6440c12b)",
        "before_after_code_files": [
          "airflow/operators/python.py||airflow/operators/python.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/operators/python.py||airflow/operators/python.py": [
          "File: airflow/operators/python.py -> airflow/operators/python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "472:                 else:",
          "473:                     raise",
          "475:             return self._read_result(output_path)",
          "477:     def determine_kwargs(self, context: Mapping[str, Any]) -> Mapping[str, Any]:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "475:             if 0 in self.skip_on_exit_code:",
          "476:                 raise AirflowSkipException(\"Process exited with code 0. Skipping.\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4f9aa6132e16cf51e4d5a3c2dd6920323c1b0d72",
      "candidate_info": {
        "commit_hash": "4f9aa6132e16cf51e4d5a3c2dd6920323c1b0d72",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4f9aa6132e16cf51e4d5a3c2dd6920323c1b0d72",
        "files": [
          "airflow/operators/trigger_dagrun.py"
        ],
        "message": "explicit string cast required to force integer-type run_ids to be passed as strings instead of integers (#36756)\n\n(cherry picked from commit e2335a00cea898d83e17b3eb69959656daae883e)",
        "before_after_code_files": [
          "airflow/operators/trigger_dagrun.py||airflow/operators/trigger_dagrun.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/operators/trigger_dagrun.py||airflow/operators/trigger_dagrun.py": [
          "File: airflow/operators/trigger_dagrun.py -> airflow/operators/trigger_dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "157:             raise AirflowException(\"conf parameter should be JSON Serializable\")",
          "159:         if self.trigger_run_id:",
          "161:         else:",
          "162:             run_id = DagRun.generate_run_id(DagRunType.MANUAL, parsed_execution_date)",
          "",
          "[Removed Lines]",
          "160:             run_id = self.trigger_run_id",
          "",
          "[Added Lines]",
          "160:             run_id = str(self.trigger_run_id)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "38573856e51b8988b8298af3394461a805d79a6a",
      "candidate_info": {
        "commit_hash": "38573856e51b8988b8298af3394461a805d79a6a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/38573856e51b8988b8298af3394461a805d79a6a",
        "files": [
          "airflow/settings.py",
          "tests/core/test_settings.py"
        ],
        "message": "Better error message when sqlite URL uses relative path (#36774)\n\nWhen sqlite URL uses relative path, the error printed is quite\ncryptic:\n\n```\nsqlalchemy.exc.OperationalError: (sqlite3.OperationalError) unable to open database file\n```\n\nThis might easily happen for example when you are in a hurry and put\nrelative value in your AIRFLOW_HOME.\n\nThis PR checks if sql is relative and throws more appropriate and\nexplicit message what is wrong.\n\n(cherry picked from commit 082055e23a38169a613f639296e144234f15d28c)",
        "before_after_code_files": [
          "airflow/settings.py||airflow/settings.py",
          "tests/core/test_settings.py||tests/core/test_settings.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/settings.py||airflow/settings.py": [
          "File: airflow/settings.py -> airflow/settings.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "192:     global PLUGINS_FOLDER",
          "193:     global DONOT_MODIFY_HANDLERS",
          "194:     SQL_ALCHEMY_CONN = conf.get(\"database\", \"SQL_ALCHEMY_CONN\")",
          "195:     DAGS_FOLDER = os.path.expanduser(conf.get(\"core\", \"DAGS_FOLDER\"))",
          "197:     PLUGINS_FOLDER = conf.get(\"core\", \"plugins_folder\", fallback=os.path.join(AIRFLOW_HOME, \"plugins\"))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "195:     if SQL_ALCHEMY_CONN.startswith(\"sqlite\") and not SQL_ALCHEMY_CONN.startswith(\"sqlite:////\"):",
          "196:         from airflow.exceptions import AirflowConfigException",
          "198:         raise AirflowConfigException(",
          "199:             f\"Cannot use relative path: `{SQL_ALCHEMY_CONN}` to connect to sqlite. \"",
          "200:             \"Please use absolute path such as `sqlite:////tmp/airflow.db`.\"",
          "201:         )",
          "",
          "---------------"
        ],
        "tests/core/test_settings.py||tests/core/test_settings.py": [
          "File: tests/core/test_settings.py -> tests/core/test_settings.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import sys",
          "22: import tempfile",
          "23: from unittest import mock",
          "26: import pytest",
          "29: from tests.test_utils.config import conf_vars",
          "31: SETTINGS_FILE_POLICY = \"\"\"",
          "",
          "[Removed Lines]",
          "24: from unittest.mock import MagicMock, call",
          "28: from airflow.exceptions import AirflowClusterPolicyViolation",
          "",
          "[Added Lines]",
          "24: from unittest.mock import MagicMock, call, patch",
          "28: from airflow.exceptions import AirflowClusterPolicyViolation, AirflowConfigException",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "214:         session_lifetime_config = settings.get_session_lifetime_config()",
          "215:         default_timeout_minutes = 30 * 24 * 60",
          "216:         assert session_lifetime_config == default_timeout_minutes",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "219: def test_sqlite_relative_path():",
          "220:     from airflow import settings",
          "222:     with patch(\"airflow.settings.conf.get\") as conf_get_mock:",
          "223:         conf_get_mock.return_value = \"sqlite:///./relative_path.db\"",
          "224:         with pytest.raises(AirflowConfigException) as exc:",
          "225:             settings.configure_vars()",
          "226:         assert \"Cannot use relative path:\" in str(exc.value)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4d27170e3c5b237abcba1aa6e54d122bcb37efac",
      "candidate_info": {
        "commit_hash": "4d27170e3c5b237abcba1aa6e54d122bcb37efac",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4d27170e3c5b237abcba1aa6e54d122bcb37efac",
        "files": [
          "airflow/decorators/__init__.pyi"
        ],
        "message": "Make `kubernetes` decorator type annotation consistent with operator (#36405)\n\n(cherry picked from commit 8af63683640358eaad2a9ed8d3a4ea26bfbee29a)",
        "before_after_code_files": [
          "airflow/decorators/__init__.pyi||airflow/decorators/__init__.pyi"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/decorators/__init__.pyi||airflow/decorators/__init__.pyi": [
          "File: airflow/decorators/__init__.pyi -> airflow/decorators/__init__.pyi",
          "--- Hunk 1 ---",
          "[Context before]",
          "461:     def kubernetes(",
          "462:         self,",
          "469:         ports: list[k8s.V1ContainerPort] | None = None,",
          "470:         volume_mounts: list[k8s.V1VolumeMount] | None = None,",
          "471:         volumes: list[k8s.V1Volume] | None = None,",
          "473:         env_from: list[k8s.V1EnvFromSource] | None = None,",
          "474:         secrets: list[Secret] | None = None,",
          "475:         in_cluster: bool | None = None,",
          "476:         cluster_context: str | None = None,",
          "477:         labels: dict | None = None,",
          "480:         get_logs: bool = True,",
          "481:         image_pull_policy: str | None = None,",
          "482:         annotations: dict | None = None,",
          "483:         container_resources: k8s.V1ResourceRequirements | None = None,",
          "484:         affinity: k8s.V1Affinity | None = None,",
          "486:         node_selector: dict | None = None,",
          "487:         image_pull_secrets: list[k8s.V1LocalObjectReference] | None = None,",
          "488:         service_account_name: str | None = None,",
          "490:         hostnetwork: bool = False,",
          "491:         tolerations: list[k8s.V1Toleration] | None = None,",
          "493:         dnspolicy: str | None = None,",
          "494:         schedulername: str | None = None,",
          "495:         init_containers: list[k8s.V1Container] | None = None,",
          "496:         log_events_on_failure: bool = False,",
          "497:         do_xcom_push: bool = False,",
          "498:         pod_template_file: str | None = None,",
          "499:         priority_class_name: str | None = None,",
          "500:         pod_runtime_info_envs: list[k8s.V1EnvVar] | None = None,",
          "501:         termination_grace_period: int | None = None,",
          "502:         configmaps: list[str] | None = None,",
          "504:     ) -> TaskDecorator:",
          "505:         \"\"\"Create a decorator to convert a callable to a Kubernetes Pod task.",
          "507:         :param kubernetes_conn_id: The Kubernetes cluster's",
          "508:             :ref:`connection ID <howto/connection:kubernetes>`.",
          "509:         :param namespace: Namespace to run within Kubernetes. Defaults to *default*.",
          "",
          "[Removed Lines]",
          "464:         image: str,",
          "465:         kubernetes_conn_id: str = ...,",
          "466:         namespace: str = \"default\",",
          "467:         name: str = ...,",
          "468:         random_name_suffix: bool = True,",
          "472:         env_vars: list[k8s.V1EnvVar] | None = None,",
          "478:         reattach_on_restart: bool = True,",
          "479:         startup_timeout_seconds: int = 120,",
          "485:         config_file: str = ...,",
          "489:         is_delete_operator_pod: bool = True,",
          "492:         security_context: dict | None = None,",
          "",
          "[Added Lines]",
          "464:         multiple_outputs: bool | None = None,",
          "465:         use_dill: bool = False,  # Added by _KubernetesDecoratedOperator.",
          "466:         # 'cmds' filled by _KubernetesDecoratedOperator.",
          "467:         kubernetes_conn_id: str | None = ...,",
          "468:         namespace: str | None = None,",
          "469:         image: str | None = None,",
          "470:         name: str | None = None,",
          "471:         random_name_suffix: bool = ...,",
          "472:         arguments: list[str] | None = None,",
          "476:         env_vars: list[k8s.V1EnvVar] | dict[str, str] | None = None,",
          "482:         reattach_on_restart: bool = ...,",
          "483:         startup_timeout_seconds: int = ...,",
          "484:         startup_check_interval_seconds: int = ...,",
          "486:         container_logs: Iterable[str] | str | Literal[True] = ...,",
          "491:         config_file: str | None = None,",
          "496:         host_aliases: list[k8s.V1HostAlias] | None = None,",
          "498:         security_context: k8s.V1PodSecurityContext | dict | None = None,",
          "499:         container_security_context: k8s.V1SecurityContext | dict | None = None,",
          "501:         dns_config: k8s.V1PodDNSConfig | None = None,",
          "502:         hostname: str | None = None,",
          "503:         subdomain: str | None = None,",
          "505:         full_pod_spec: k8s.V1Pod | None = None,",
          "510:         pod_template_dict: dict | None = None,",
          "515:         skip_on_exit_code: int | Container[int] | None = None,",
          "516:         base_container_name: str | None = None,",
          "517:         deferrable: bool = ...,",
          "518:         poll_interval: float = ...,",
          "519:         log_pod_spec_on_failure: bool = ...,",
          "520:         on_finish_action: str = ...,",
          "521:         termination_message_policy: str = ...,",
          "522:         active_deadline_seconds: int | None = None,",
          "523:         progress_callback: Callable[[str], None] | None = None,",
          "528:         :param multiple_outputs: If set, function return value will be unrolled to multiple XCom values.",
          "529:             Dict will unroll to XCom values with keys as XCom keys. Defaults to False.",
          "530:         :param use_dill: Whether to use dill or pickle for serialization",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "514:             (DNS-1123 subdomain, containing only ``[a-z0-9.-]``). Defaults to",
          "515:             ``k8s_airflow_pod_{RANDOM_UUID}``.",
          "516:         :param random_name_suffix: If *True*, will generate a random suffix.",
          "517:         :param ports: Ports for the launched pod.",
          "518:         :param volume_mounts: *volumeMounts* for the launched pod.",
          "519:         :param volumes: Volumes for the launched pod. Includes *ConfigMaps* and",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "541:         :param arguments: arguments of the entrypoint. (templated)",
          "542:             The docker image's CMD is used if this is not provided.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "533:             a new pod for each try.",
          "534:         :param labels: Labels to apply to the pod. (templated)",
          "535:         :param startup_timeout_seconds: Timeout in seconds to startup the pod.",
          "536:         :param get_logs: Get the stdout of the container as logs of the tasks.",
          "537:         :param image_pull_policy: Specify a policy to cache or always pull an",
          "538:             image.",
          "539:         :param annotations: Non-identifying metadata you can attach to the pod.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "562:         :param startup_check_interval_seconds: interval in seconds to check if the pod has already started",
          "564:         :param container_logs: list of containers whose logs will be published to stdout",
          "565:             Takes a sequence of containers, a single container name or True.",
          "566:             If True, all the containers logs are published. Works in conjunction with ``get_logs`` param.",
          "567:             The default value is the base container.",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "548:             pod. If more than one secret is required, provide a comma separated",
          "549:             list, e.g. ``secret_a,secret_b``.",
          "550:         :param service_account_name: Name of the service account.",
          "554:         :param hostnetwork: If *True*, enable host networking on the pod.",
          "555:         :param tolerations: A list of Kubernetes tolerations.",
          "556:         :param security_context: Security options the pod should run with",
          "557:             (PodSecurityContext).",
          "558:         :param dnspolicy: DNS policy for the pod.",
          "559:         :param schedulername: Specify a scheduler name for the pod",
          "560:         :param init_containers: Init containers for the launched pod.",
          "561:         :param log_events_on_failure: Log the pod's events if a failure occurs.",
          "562:         :param do_xcom_push: If *True*, the content of",
          "563:             ``/airflow/xcom/return.json`` in the container will also be pushed",
          "564:             to an XCom when the container completes.",
          "565:         :param pod_template_file: Path to pod template file (templated)",
          "566:         :param priority_class_name: Priority class name for the launched pod.",
          "567:         :param pod_runtime_info_envs: A list of environment variables",
          "568:             to be set in the container.",
          "",
          "[Removed Lines]",
          "551:         :param is_delete_operator_pod: What to do when the pod reaches its final",
          "552:             state, or the execution is interrupted. If *True* (default), delete",
          "553:             the pod; otherwise leave the pod.",
          "",
          "[Added Lines]",
          "583:         :param host_aliases: A list of host aliases to apply to the containers in the pod.",
          "587:         :param container_security_context: security options the container should run with.",
          "589:         :param dns_config: dns configuration (ip addresses, searches, options) for the pod.",
          "590:         :param hostname: hostname for the pod.",
          "591:         :param subdomain: subdomain for the pod.",
          "593:         :param full_pod_spec: The complete podSpec",
          "600:         :param pod_template_dict: pod template dictionary (templated)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "572:             ConfigMaps to populate the environment variables with. The contents",
          "573:             of the target ConfigMap's Data field will represent the key-value",
          "574:             pairs as environment variables. Extends env_from.",
          "575:         \"\"\"",
          "576:     @overload",
          "577:     def sensor(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "610:         :param skip_on_exit_code: If task exits with this exit code, leave the task",
          "611:             in ``skipped`` state (default: None). If set to ``None``, any non-zero",
          "612:             exit code will be treated as a failure.",
          "613:         :param base_container_name: The name of the base container in the pod. This container's logs",
          "614:             will appear as part of this task's logs if get_logs is True. Defaults to None. If None,",
          "615:             will consult the class variable BASE_CONTAINER_NAME (which defaults to \"base\") for the base",
          "616:             container name to use.",
          "617:         :param deferrable: Run operator in the deferrable mode.",
          "618:         :param poll_interval: Polling period in seconds to check for the status. Used only in deferrable mode.",
          "619:         :param log_pod_spec_on_failure: Log the pod's specification if a failure occurs",
          "620:         :param on_finish_action: What to do when the pod reaches its final state, or the execution is interrupted.",
          "621:             If \"delete_pod\", the pod will be deleted regardless its state; if \"delete_succeeded_pod\",",
          "622:             only succeeded pod will be deleted. You can set to \"keep_pod\" to keep the pod.",
          "623:         :param termination_message_policy: The termination message policy of the base container.",
          "624:             Default value is \"File\"",
          "625:         :param active_deadline_seconds: The active_deadline_seconds which matches to active_deadline_seconds",
          "626:             in V1PodSpec.",
          "627:         :param progress_callback: Callback function for receiving k8s container logs.",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "718f802ffa39ce7b018446398177bf31bab1eb31",
      "candidate_info": {
        "commit_hash": "718f802ffa39ce7b018446398177bf31bab1eb31",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/718f802ffa39ce7b018446398177bf31bab1eb31",
        "files": [
          "docs/apache-airflow/administration-and-deployment/production-deployment.rst",
          "docs/apache-airflow/core-concepts/overview.rst"
        ],
        "message": "Add section about live-upgrading Airflow (#36637)\n\nOur users are often asking about live-upgrading Airflow and the answer\non what and how can be live-upgraded is not obvious and it depends on a\nnumber of factors - most importantly on the type of deployment you run\nand  type of executor you use.\n\nThis PR adds a basic description for it - following the recent update\nexplaining the different live-upgrade scenarios available.\n\n(cherry picked from commit ef1498831a54c76a928d0a56eaf4c232925c0c65)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "e3316c36b4159d5bc6ae73e4b77ba62045064184",
      "candidate_info": {
        "commit_hash": "e3316c36b4159d5bc6ae73e4b77ba62045064184",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e3316c36b4159d5bc6ae73e4b77ba62045064184",
        "files": [
          "dev/README_RELEASE_PROVIDER_PACKAGES.md",
          "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py"
        ],
        "message": "Fix preparing rc candidates for providers (#36465)\n\nThe last auto-upgrade RC implementd in #36441 had a bug - it was bumping\nrc even for providers that have been already released. This change fixes\nit - it skips packages that already have \"final\" tag present in the\nrepo. It also explicitely calls \"Apply template update\" as optional\nstep - only needed in case we modify templates and want to update\nautomatically generated documentation with it.\n\n(cherry picked from commit a3e5a971edf9e4e86c86d595daa04dccd277aabe)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py||dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py||dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py": [
          "File: dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py -> dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "172:         return False, version_suffix",
          "173:     # version_suffix starts with \"rc\"",
          "174:     current_version = int(version_suffix[2:])",
          "175:     while True:",
          "176:         current_tag = get_latest_provider_tag(provider_id, f\"rc{current_version}\")",
          "177:         if tag_exists_for_provider(provider_id, current_tag):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "175:     release_tag = get_latest_provider_tag(provider_id, \"\")",
          "176:     if tag_exists_for_provider(provider_id, release_tag):",
          "177:         get_console().print(f\"[warning]The tag {release_tag} exists. Provider is released. Skipping it.[/]\")",
          "178:         return True, \"\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "00fc3db79aebd0b9de2724340c703d44ae17e394",
      "candidate_info": {
        "commit_hash": "00fc3db79aebd0b9de2724340c703d44ae17e394",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/00fc3db79aebd0b9de2724340c703d44ae17e394",
        "files": [
          "dev/README_RELEASE_PROVIDER_PACKAGES.md"
        ],
        "message": "Remove pypitest from the release process (#36466)\n\n(cherry picked from commit c16b4218728a654c2a9d51a5a637ebf595f20136)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "815fb4aa8ea15e2594dce27184e221bb181295a9",
      "candidate_info": {
        "commit_hash": "815fb4aa8ea15e2594dce27184e221bb181295a9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/815fb4aa8ea15e2594dce27184e221bb181295a9",
        "files": [
          ".github/workflows/ci.yml"
        ],
        "message": "Increase timeout for cache building (#36762)\n\nWhen we build cach from the scratch, cache preparation can take\nlonger than 50 minutes (right now it's about an hour). Timeout\nincrease to 120 minutes should solve the problem that in such case\nthe PROD cache building gets cancelled and you neeed to re-run\nit to succeed.\n\n(cherry picked from commit c7ade012cb81158e2bc11b36febcfae82bc759ab)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "667ec3b6e6b3a7ed5e7a17f2bb8ee6ed9afa21d6",
      "candidate_info": {
        "commit_hash": "667ec3b6e6b3a7ed5e7a17f2bb8ee6ed9afa21d6",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/667ec3b6e6b3a7ed5e7a17f2bb8ee6ed9afa21d6",
        "files": [
          ".github/workflows/ci.yml",
          ".pre-commit-config.yaml",
          "STATIC_CODE_CHECKS.rst",
          "airflow/decorators/__init__.pyi",
          "airflow/models/taskreschedule.py",
          "airflow/operators/latest_only.py",
          "airflow/providers/apache/druid/hooks/druid.py",
          "airflow/providers/databricks/hooks/databricks_sql.py",
          "airflow/providers/exasol/hooks/exasol.py",
          "airflow/providers/google/cloud/hooks/cloud_run.py",
          "airflow/providers/google/cloud/triggers/cloud_run.py",
          "airflow/providers/google/cloud/utils/credentials_provider.py",
          "airflow/providers/google/common/hooks/base_google.py",
          "airflow/providers/google/common/utils/id_token_credentials.py",
          "airflow/providers/grpc/hooks/grpc.py",
          "airflow/providers/openlineage/plugins/facets.py",
          "airflow/providers/postgres/hooks/postgres.py",
          "airflow/providers/snowflake/hooks/snowflake.py",
          "airflow/providers/trino/operators/trino.py",
          "airflow/providers/vertica/hooks/vertica.py",
          "airflow/providers_manager.py",
          "airflow/sensors/base.py",
          "airflow/utils/operator_helpers.py",
          "dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "dev/breeze/tests/test_selective_checks.py",
          "images/breeze/output_static-checks.svg",
          "images/breeze/output_static-checks.txt",
          "scripts/ci/pre_commit/common_precommit_utils.py",
          "scripts/ci/pre_commit/pre_commit_mypy.py"
        ],
        "message": "Run mypy checks for full packages in CI (#36638)\n\nMyPy as used in our static checks has slightly different heuristics\nwhen running on on individual files and whole packages. This sometimes\ncauses semi-random failures when different set of files is produced\nwhen pre-commits split the files between parallel processes.\n\nThe regular `mypy-*` pre-commits work by passing filenames to mypy\nchecks, and when `--all-files` flag is passed to mypy, this means\nthat 2700 files are passed. In this case pre-commit will split such\nlong list of files to several sequential muypy executions. This\nis not very good because depending on the list of files passed,\nmypy can split the list diferently and results will be different\nwhen just list of files changes - so mypy might start detecting\nproblems that were not present before.\n\nThis PR introduces new `mypy` check that runs mypy for packages\nrather than individual files. We cannot run them for local\npre-commit runs, because in many cases, such package based\nmypy check will run for minutes when a single file changes,\ndue to cache invalidation rules - and we do not want to penalise\ncommits that are changing common airflow code (because such PRs\nwould invalidate a lot of mypy cache every time such common file\nchanges). So we still want to run file-based mypy for local\ncommits. But we do not want to pass 2700 files in CI, rather than\nthat on CI we want to run mypy checks \"per package\".\n\nThis PR introduces a new \"manual\" stage mypy pre-commit check that\nwill run \"package\" based mypy checks and adds selective check rules\nthat will decide properly when to run such tests and separate,\nmatrix-based CI job that will run such mypy checks - separately\nfor each of the packages: \"airflow\", \"providers\", \"docs\", \"dev\".\n\nAlso this job will skip providers checks in non-main branch and\nwill run all tests when \"full tests needed\" are requested.\n\nThis PR ignores some errors resulted from 3rd-party libraries used\nthat are randomply appearing when some files are modified (and fixes\nthe current main failures)\n\n(cherry picked from commit f7b663d9aff472d0a419e16c262fbae2a8a69ce1)",
        "before_after_code_files": [
          "airflow/decorators/__init__.pyi||airflow/decorators/__init__.pyi",
          "airflow/models/taskreschedule.py||airflow/models/taskreschedule.py",
          "airflow/operators/latest_only.py||airflow/operators/latest_only.py",
          "airflow/providers/apache/druid/hooks/druid.py||airflow/providers/apache/druid/hooks/druid.py",
          "airflow/providers/databricks/hooks/databricks_sql.py||airflow/providers/databricks/hooks/databricks_sql.py",
          "airflow/providers/exasol/hooks/exasol.py||airflow/providers/exasol/hooks/exasol.py",
          "airflow/providers/google/cloud/hooks/cloud_run.py||airflow/providers/google/cloud/hooks/cloud_run.py",
          "airflow/providers/google/cloud/triggers/cloud_run.py||airflow/providers/google/cloud/triggers/cloud_run.py",
          "airflow/providers/google/cloud/utils/credentials_provider.py||airflow/providers/google/cloud/utils/credentials_provider.py",
          "airflow/providers/google/common/hooks/base_google.py||airflow/providers/google/common/hooks/base_google.py",
          "airflow/providers/google/common/utils/id_token_credentials.py||airflow/providers/google/common/utils/id_token_credentials.py",
          "airflow/providers/grpc/hooks/grpc.py||airflow/providers/grpc/hooks/grpc.py",
          "airflow/providers/openlineage/plugins/facets.py||airflow/providers/openlineage/plugins/facets.py",
          "airflow/providers/postgres/hooks/postgres.py||airflow/providers/postgres/hooks/postgres.py",
          "airflow/providers/snowflake/hooks/snowflake.py||airflow/providers/snowflake/hooks/snowflake.py",
          "airflow/providers/trino/operators/trino.py||airflow/providers/trino/operators/trino.py",
          "airflow/providers/vertica/hooks/vertica.py||airflow/providers/vertica/hooks/vertica.py",
          "airflow/providers_manager.py||airflow/providers_manager.py",
          "airflow/sensors/base.py||airflow/sensors/base.py",
          "airflow/utils/operator_helpers.py||airflow/utils/operator_helpers.py",
          "dev/breeze/src/airflow_breeze/pre_commit_ids.py||dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "dev/breeze/tests/test_selective_checks.py||dev/breeze/tests/test_selective_checks.py",
          "scripts/ci/pre_commit/common_precommit_utils.py||scripts/ci/pre_commit/common_precommit_utils.py",
          "scripts/ci/pre_commit/pre_commit_mypy.py||scripts/ci/pre_commit/pre_commit_mypy.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/decorators/__init__.pyi||airflow/decorators/__init__.pyi": [
          "File: airflow/decorators/__init__.pyi -> airflow/decorators/__init__.pyi",
          "--- Hunk 1 ---",
          "[Context before]",
          "61: class TaskDecoratorCollection:",
          "62:     @overload",
          "64:         self,",
          "66:         multiple_outputs: bool | None = None,",
          "",
          "[Removed Lines]",
          "63:     def python(",
          "",
          "[Added Lines]",
          "63:     def python(  # type: ignore[misc]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "88:     def python(self, python_callable: Callable[FParams, FReturn]) -> Task[FParams, FReturn]: ...",
          "89:     # [END mixin_for_typing]",
          "90:     @overload",
          "92:         self,",
          "94:         multiple_outputs: bool | None = None,",
          "",
          "[Removed Lines]",
          "91:     def __call__(",
          "",
          "[Added Lines]",
          "91:     def __call__(  # type: ignore[misc]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "101:     def __call__(self, python_callable: Callable[FParams, FReturn]) -> Task[FParams, FReturn]:",
          "102:         \"\"\"Aliasing ``python``; signature should match exactly.\"\"\"",
          "103:     @overload",
          "105:         self,",
          "107:         multiple_outputs: bool | None = None,",
          "",
          "[Removed Lines]",
          "104:     def virtualenv(",
          "",
          "[Added Lines]",
          "104:     def virtualenv(  # type: ignore[misc]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "187:             such as transmission a large amount of XCom to TaskAPI.",
          "188:         \"\"\"",
          "189:     @overload",
          "191:         \"\"\"Create a decorator to wrap the decorated callable into a BranchPythonOperator.",
          "193:         For more information on how to use this decorator, see :ref:`concepts:branching`.",
          "",
          "[Removed Lines]",
          "190:     def branch(self, *, multiple_outputs: bool | None = None, **kwargs) -> TaskDecorator:",
          "",
          "[Added Lines]",
          "190:     def branch(  # type: ignore[misc]",
          "191:         self, *, multiple_outputs: bool | None = None, **kwargs",
          "192:     ) -> TaskDecorator:",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "199:     @overload",
          "200:     def branch(self, python_callable: Callable[FParams, FReturn]) -> Task[FParams, FReturn]: ...",
          "201:     @overload",
          "203:         self,",
          "205:         multiple_outputs: bool | None = None,",
          "",
          "[Removed Lines]",
          "202:     def branch_virtualenv(",
          "",
          "[Added Lines]",
          "204:     def branch_virtualenv(  # type: ignore[misc]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "292:         self, python_callable: Callable[FParams, FReturn]",
          "293:     ) -> Task[FParams, FReturn]: ...",
          "294:     @overload",
          "296:         self,",
          "298:         multiple_outputs: bool | None = None,",
          "",
          "[Removed Lines]",
          "295:     def short_circuit(",
          "",
          "[Added Lines]",
          "297:     def short_circuit(  # type: ignore[misc]",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "627:         :param progress_callback: Callback function for receiving k8s container logs.",
          "628:         \"\"\"",
          "629:     @overload",
          "631:         self,",
          "633:         poke_interval: float = ...,",
          "",
          "[Removed Lines]",
          "630:     def sensor(",
          "",
          "[Added Lines]",
          "632:     def sensor(  # type: ignore[misc]",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "664:     @overload",
          "665:     def sensor(self, python_callable: Callable[FParams, FReturn] | None = None) -> Task[FParams, FReturn]: ...",
          "666:     @overload",
          "668:         self,",
          "670:         multiple_outputs: bool | None = None,",
          "",
          "[Removed Lines]",
          "667:     def pyspark(",
          "",
          "[Added Lines]",
          "669:     def pyspark(  # type: ignore[misc]",
          "",
          "---------------"
        ],
        "airflow/models/taskreschedule.py||airflow/models/taskreschedule.py": [
          "File: airflow/models/taskreschedule.py -> airflow/models/taskreschedule.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "39:     from airflow.models.operator import Operator",
          "40:     from airflow.models.taskinstance import TaskInstance",
          "43: class TaskReschedule(Base):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "41:     from airflow.serialization.pydantic.taskinstance import TaskInstancePydantic",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "103:     @classmethod",
          "104:     def stmt_for_task_instance(",
          "105:         cls,",
          "108:         try_number: int | None = None,",
          "109:         descending: bool = False,",
          "",
          "[Removed Lines]",
          "106:         ti: TaskInstance,",
          "",
          "[Added Lines]",
          "107:         ti: TaskInstance | TaskInstancePydantic,",
          "",
          "---------------"
        ],
        "airflow/operators/latest_only.py||airflow/operators/latest_only.py": [
          "File: airflow/operators/latest_only.py -> airflow/operators/latest_only.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "45:     def choose_branch(self, context: Context) -> str | Iterable[str]:",
          "46:         # If the DAG Run is externally triggered, then return without",
          "47:         # skipping downstream tasks",
          "49:         if dag_run.external_trigger:",
          "50:             self.log.info(\"Externally triggered DAG_Run: allowing execution to proceed.\")",
          "51:             return list(context[\"task\"].get_direct_relative_ids(upstream=False))",
          "",
          "[Removed Lines]",
          "48:         dag_run: DagRun = context[\"dag_run\"]",
          "",
          "[Added Lines]",
          "48:         dag_run: DagRun = context[\"dag_run\"]  # type: ignore[assignment]",
          "",
          "---------------"
        ],
        "airflow/providers/apache/druid/hooks/druid.py||airflow/providers/apache/druid/hooks/druid.py": [
          "File: airflow/providers/apache/druid/hooks/druid.py -> airflow/providers/apache/druid/hooks/druid.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "200:         endpoint = conn.extra_dejson.get(\"endpoint\", \"druid/v2/sql\")",
          "201:         return f\"{conn_type}://{host}/{endpoint}\"",
          "204:         raise NotImplementedError()",
          "206:     def insert_rows(",
          "",
          "[Removed Lines]",
          "203:     def set_autocommit(self, conn: connect, autocommit: bool) -> NotImplementedError:",
          "",
          "[Added Lines]",
          "203:     def set_autocommit(self, conn: connect, autocommit: bool) -> None:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "211:         commit_every: int = 1000,",
          "212:         replace: bool = False,",
          "215:         raise NotImplementedError()",
          "",
          "[Removed Lines]",
          "214:     ) -> NotImplementedError:",
          "",
          "[Added Lines]",
          "214:     ) -> None:",
          "",
          "---------------"
        ],
        "airflow/providers/databricks/hooks/databricks_sql.py||airflow/providers/databricks/hooks/databricks_sql.py": [
          "File: airflow/providers/databricks/hooks/databricks_sql.py -> airflow/providers/databricks/hooks/databricks_sql.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "145:             )",
          "146:         return self._sql_conn",
          "149:     def run(",
          "150:         self,",
          "151:         sql: str | Iterable[str],",
          "",
          "[Removed Lines]",
          "148:     @overload",
          "",
          "[Added Lines]",
          "148:     @overload  # type: ignore[override]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "220:                 self.set_autocommit(conn, autocommit)",
          "222:                 with closing(conn.cursor()) as cur:",
          "224:                     if handler is not None:",
          "225:                         result = self._make_serializable(handler(cur))",
          "226:                         if return_single_query_results(sql, return_last, split_statements):",
          "",
          "[Removed Lines]",
          "223:                     self._run_command(cur, sql_statement, parameters)",
          "",
          "[Added Lines]",
          "223:                     self._run_command(cur, sql_statement, parameters)  # type: ignore[attr-defined]",
          "",
          "---------------"
        ],
        "airflow/providers/exasol/hooks/exasol.py||airflow/providers/exasol/hooks/exasol.py": [
          "File: airflow/providers/exasol/hooks/exasol.py -> airflow/providers/exasol/hooks/exasol.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "162:             )",
          "163:         return cols",
          "166:     def run(",
          "167:         self,",
          "168:         sql: str | Iterable[str],",
          "",
          "[Removed Lines]",
          "165:     @overload",
          "",
          "[Added Lines]",
          "165:     @overload  # type: ignore[override]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "232:                 with closing(conn.execute(sql_statement, parameters)) as exa_statement:",
          "233:                     self.log.info(\"Running statement: %s, parameters: %s\", sql_statement, parameters)",
          "234:                     if handler is not None:",
          "236:                         if return_single_query_results(sql, return_last, split_statements):",
          "237:                             _last_result = result",
          "238:                             _last_columns = self.get_description(exa_statement)",
          "",
          "[Removed Lines]",
          "235:                         result = handler(exa_statement)",
          "",
          "[Added Lines]",
          "235:                         result = self._make_common_data_structure(  # type: ignore[attr-defined]",
          "236:                             handler(exa_statement)",
          "237:                         )",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/hooks/cloud_run.py||airflow/providers/google/cloud/hooks/cloud_run.py": [
          "File: airflow/providers/google/cloud/hooks/cloud_run.py -> airflow/providers/google/cloud/hooks/cloud_run.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31:     RunJobRequest,",
          "32:     UpdateJobRequest,",
          "33: )",
          "36: from airflow.exceptions import AirflowException",
          "37: from airflow.providers.google.common.consts import CLIENT_INFO",
          "",
          "[Removed Lines]",
          "34: from google.longrunning import operations_pb2",
          "",
          "[Added Lines]",
          "34: from google.longrunning import operations_pb2  # type: ignore[attr-defined]",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/triggers/cloud_run.py||airflow/providers/google/cloud/triggers/cloud_run.py": [
          "File: airflow/providers/google/cloud/triggers/cloud_run.py -> airflow/providers/google/cloud/triggers/cloud_run.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: from airflow.triggers.base import BaseTrigger, TriggerEvent",
          "27: if TYPE_CHECKING:",
          "30: DEFAULT_BATCH_LOCATION = \"us-central1\"",
          "",
          "[Removed Lines]",
          "28:     from google.longrunning import operations_pb2",
          "",
          "[Added Lines]",
          "28:     from google.longrunning import operations_pb2  # type: ignore[attr-defined]",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/utils/credentials_provider.py||airflow/providers/google/cloud/utils/credentials_provider.py": [
          "File: airflow/providers/google/cloud/utils/credentials_provider.py -> airflow/providers/google/cloud/utils/credentials_provider.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: import google.auth",
          "31: import google.auth.credentials",
          "32: import google.oauth2.service_account",
          "34: from google.auth.environment_vars import CREDENTIALS, LEGACY_PROJECT, PROJECT",
          "36: from airflow.exceptions import AirflowException",
          "",
          "[Removed Lines]",
          "33: from google.auth import impersonated_credentials",
          "",
          "[Added Lines]",
          "33: from google.auth import impersonated_credentials  # type: ignore[attr-defined]",
          "",
          "---------------"
        ],
        "airflow/providers/google/common/hooks/base_google.py||airflow/providers/google/common/hooks/base_google.py": [
          "File: airflow/providers/google/common/hooks/base_google.py -> airflow/providers/google/common/hooks/base_google.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "36: import tenacity",
          "37: from asgiref.sync import sync_to_async",
          "38: from google.api_core.exceptions import Forbidden, ResourceExhausted, TooManyRequests",
          "40: from google.auth.environment_vars import CLOUD_SDK_CONFIG_DIR, CREDENTIALS",
          "41: from google.auth.exceptions import RefreshError",
          "42: from google.auth.transport import _http_client",
          "",
          "[Removed Lines]",
          "39: from google.auth import _cloud_sdk, compute_engine",
          "",
          "[Added Lines]",
          "39: from google.auth import _cloud_sdk, compute_engine  # type: ignore[attr-defined]",
          "",
          "---------------"
        ],
        "airflow/providers/google/common/utils/id_token_credentials.py||airflow/providers/google/common/utils/id_token_credentials.py": [
          "File: airflow/providers/google/common/utils/id_token_credentials.py -> airflow/providers/google/common/utils/id_token_credentials.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "37: import google.auth.transport",
          "38: from google.auth import credentials as google_auth_credentials, environment_vars, exceptions",
          "41: if TYPE_CHECKING:",
          "42:     import google.oauth2",
          "",
          "[Removed Lines]",
          "39: from google.oauth2 import credentials as oauth2_credentials, service_account",
          "",
          "[Added Lines]",
          "39: from google.oauth2 import credentials as oauth2_credentials, service_account  # type: ignore[attr-defined]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "146:     target_audience: str | None,",
          "147: ) -> google_auth_credentials.Credentials | None:",
          "148:     \"\"\"Gets the credentials and project ID from the Cloud SDK.\"\"\"",
          "151:     # Check if application default credentials exist.",
          "152:     credentials_filename = _cloud_sdk.get_application_default_credentials_path()",
          "",
          "[Removed Lines]",
          "149:     from google.auth import _cloud_sdk",
          "",
          "[Added Lines]",
          "149:     from google.auth import _cloud_sdk  # type: ignore[attr-defined]",
          "",
          "---------------"
        ],
        "airflow/providers/grpc/hooks/grpc.py||airflow/providers/grpc/hooks/grpc.py": [
          "File: airflow/providers/grpc/hooks/grpc.py -> airflow/providers/grpc/hooks/grpc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import grpc",
          "23: from google import auth as google_auth",
          "25: from google.auth.transport import (",
          "26:     grpc as google_auth_transport_grpc,",
          "27:     requests as google_auth_transport_requests,",
          "",
          "[Removed Lines]",
          "24: from google.auth import jwt as google_auth_jwt",
          "",
          "[Added Lines]",
          "24: from google.auth import jwt as google_auth_jwt  # type: ignore[attr-defined]",
          "",
          "---------------"
        ],
        "airflow/providers/openlineage/plugins/facets.py||airflow/providers/openlineage/plugins/facets.py": [
          "File: airflow/providers/openlineage/plugins/facets.py -> airflow/providers/openlineage/plugins/facets.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "28:     mapIndex: int",
          "29:     operatorClass: str",
          "33:     @classmethod",
          "34:     def from_task_instance(cls, task_instance):",
          "",
          "[Removed Lines]",
          "31:     _additional_skip_redact: list[str] = [\"operatorClass\"]",
          "",
          "[Added Lines]",
          "31:     _additional_skip_redact = [\"operatorClass\"]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "63:     properties: dict[str, object]",
          "64:     type: str = \"operator\"",
          "69: @define(slots=False)",
          "",
          "[Removed Lines]",
          "66:     _skip_redact: list[str] = [\"name\", \"type\"]",
          "",
          "[Added Lines]",
          "66:     _skip_redact = [\"name\", \"type\"]",
          "",
          "---------------"
        ],
        "airflow/providers/postgres/hooks/postgres.py||airflow/providers/postgres/hooks/postgres.py": [
          "File: airflow/providers/postgres/hooks/postgres.py -> airflow/providers/postgres/hooks/postgres.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "332:         if is_redshift:",
          "333:             authority = self._get_openlineage_redshift_authority_part(connection)",
          "334:         else:",
          "337:         return DatabaseInfo(",
          "338:             scheme=\"postgres\" if not is_redshift else \"redshift\",",
          "",
          "[Removed Lines]",
          "335:             authority = DbApiHook.get_openlineage_authority_part(connection, default_port=5432)",
          "",
          "[Added Lines]",
          "335:             authority = DbApiHook.get_openlineage_authority_part(  # type: ignore[attr-defined]",
          "336:                 connection, default_port=5432",
          "337:             )",
          "",
          "---------------"
        ],
        "airflow/providers/snowflake/hooks/snowflake.py||airflow/providers/snowflake/hooks/snowflake.py": [
          "File: airflow/providers/snowflake/hooks/snowflake.py -> airflow/providers/snowflake/hooks/snowflake.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "300:     def get_autocommit(self, conn):",
          "301:         return getattr(conn, \"autocommit_mode\", False)",
          "304:     def run(",
          "305:         self,",
          "306:         sql: str | Iterable[str],",
          "",
          "[Removed Lines]",
          "303:     @overload",
          "",
          "[Added Lines]",
          "303:     @overload  # type: ignore[override]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "385:             with self._get_cursor(conn, return_dictionaries) as cur:",
          "386:                 results = []",
          "387:                 for sql_statement in sql_list:",
          "390:                     if handler is not None:",
          "392:                         if return_single_query_results(sql, return_last, split_statements):",
          "393:                             _last_result = result",
          "394:                             _last_description = cur.description",
          "",
          "[Removed Lines]",
          "388:                     self._run_command(cur, sql_statement, parameters)",
          "391:                         result = handler(cur)",
          "",
          "[Added Lines]",
          "388:                     self._run_command(cur, sql_statement, parameters)  # type: ignore[attr-defined]",
          "391:                         result = self._make_common_data_structure(handler(cur))  # type: ignore[attr-defined]",
          "",
          "---------------"
        ],
        "airflow/providers/trino/operators/trino.py||airflow/providers/trino/operators/trino.py": [
          "File: airflow/providers/trino/operators/trino.py -> airflow/providers/trino/operators/trino.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "65:         )",
          "67:     def on_kill(self) -> None:",
          "70:             try:",
          "73:                     sql=f\"CALL system.runtime.kill_query(query_id => {query_id},message => 'Job \"",
          "74:                     f\"killed by \"",
          "75:                     f\"user');\",",
          "",
          "[Removed Lines]",
          "68:         if self._hook is not None and isinstance(self._hook, TrinoHook):",
          "69:             query_id = \"'\" + self._hook.query_id + \"'\"",
          "71:                 self.log.info(\"Stopping query run with queryId - %s\", self._hook.query_id)",
          "72:                 self._hook.run(",
          "",
          "[Added Lines]",
          "68:         if self._hook is not None and isinstance(self._hook, TrinoHook):  # type: ignore[attr-defined]",
          "69:             query_id = \"'\" + self._hook.query_id + \"'\"  # type: ignore[attr-defined]",
          "71:                 self.log.info(\"Stopping query run with queryId - %s\", self._hook.query_id)  # type: ignore[attr-defined]",
          "72:                 self._hook.run(  # type: ignore[attr-defined]",
          "",
          "---------------"
        ],
        "airflow/providers/vertica/hooks/vertica.py||airflow/providers/vertica/hooks/vertica.py": [
          "File: airflow/providers/vertica/hooks/vertica.py -> airflow/providers/vertica/hooks/vertica.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "132:         conn = connect(**conn_config)",
          "133:         return conn",
          "136:     def run(",
          "137:         self,",
          "138:         sql: str | Iterable[str],",
          "",
          "[Removed Lines]",
          "135:     @overload",
          "",
          "[Added Lines]",
          "135:     @overload  # type: ignore[override]",
          "",
          "---------------"
        ],
        "airflow/providers_manager.py||airflow/providers_manager.py": [
          "File: airflow/providers_manager.py -> airflow/providers_manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "404:         return ProvidersManager._initialized",
          "406:     @staticmethod",
          "408:         return ProvidersManager._initialization_stack_trace",
          "410:     def __init__(self):",
          "",
          "[Removed Lines]",
          "407:     def initialization_stack_trace() -> str:",
          "",
          "[Added Lines]",
          "407:     def initialization_stack_trace() -> str | None:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "418:         # Keeps dict of hooks keyed by connection type",
          "419:         self._hooks_dict: dict[str, HookInfo] = {}",
          "420:         self._fs_set: set[str] = set()",
          "422:         # keeps mapping between connection_types and hook class, package they come from",
          "423:         self._hook_provider_dict: dict[str, HookClassProvider] = {}",
          "424:         # Keeps dict of hooks keyed by connection type. They are lazy evaluated at access time",
          "",
          "[Removed Lines]",
          "421:         self._taskflow_decorators: dict[str, Callable] = LazyDictWithCache()",
          "",
          "[Added Lines]",
          "421:         self._taskflow_decorators: dict[str, Callable] = LazyDictWithCache()  # type: ignore[assignment]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1105:         \"\"\"Retrieve all configs defined in the providers.\"\"\"",
          "1106:         for provider_package, provider in self._provider_dict.items():",
          "1107:             if provider.data.get(\"config\"):",
          "1110:     def _discover_plugins(self) -> None:",
          "1111:         \"\"\"Retrieve all plugins defined in the providers.\"\"\"",
          "",
          "[Removed Lines]",
          "1108:                 self._provider_configs[provider_package] = provider.data.get(\"config\")",
          "",
          "[Added Lines]",
          "1108:                 self._provider_configs[provider_package] = (",
          "1109:                     provider.data.get(\"config\")  # type: ignore[assignment]",
          "1110:                 )",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1175:     @property",
          "1176:     def taskflow_decorators(self) -> dict[str, TaskDecorator]:",
          "1177:         self.initialize_providers_taskflow_decorator()",
          "1180:     @property",
          "1181:     def extra_links_class_names(self) -> list[str]:",
          "",
          "[Removed Lines]",
          "1178:         return self._taskflow_decorators",
          "",
          "[Added Lines]",
          "1180:         return self._taskflow_decorators  # type: ignore[return-value]",
          "",
          "---------------"
        ],
        "airflow/sensors/base.py||airflow/sensors/base.py": [
          "File: airflow/sensors/base.py -> airflow/sensors/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "212:         if self.reschedule:",
          "213:             # If reschedule, use the start date of the first try (first try can be either the very",
          "214:             # first execution of the task, or the first execution after the task was cleared.)",
          "216:             with create_session() as session:",
          "217:                 start_date = session.scalar(",
          "218:                     TaskReschedule.stmt_for_task_instance(",
          "",
          "[Removed Lines]",
          "215:             first_try_number = context[\"ti\"].max_tries - self.retries + 1",
          "",
          "[Added Lines]",
          "215:             max_tries: int = context[\"ti\"].max_tries or 0",
          "216:             retries: int = self.retries or 0",
          "217:             first_try_number = max_tries - retries + 1",
          "",
          "---------------"
        ],
        "airflow/utils/operator_helpers.py||airflow/utils/operator_helpers.py": [
          "File: airflow/utils/operator_helpers.py -> airflow/utils/operator_helpers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "173:     def unpacking(self) -> Mapping[str, Any]:",
          "174:         \"\"\"Dump the kwargs mapping to unpack with ``**`` in a function call.\"\"\"",
          "176:             return lazy_mapping_from_context(self._kwargs)",
          "177:         return self._kwargs",
          "",
          "[Removed Lines]",
          "175:         if self._wildcard and isinstance(self._kwargs, Context):",
          "",
          "[Added Lines]",
          "175:         if self._wildcard and isinstance(self._kwargs, Context):  # type: ignore[misc]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/pre_commit_ids.py||dev/breeze/src/airflow_breeze/pre_commit_ids.py": [
          "File: dev/breeze/src/airflow_breeze/pre_commit_ids.py -> dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "103:     \"lint-markdown\",",
          "104:     \"lint-openapi\",",
          "105:     \"mixed-line-ending\",",
          "106:     \"mypy-core\",",
          "107:     \"mypy-dev\",",
          "108:     \"mypy-docs\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "106:     \"mypy\",",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py": [
          "File: dev/breeze/src/airflow_breeze/utils/selective_checks.py -> dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "584:             )",
          "585:             return False",
          "587:     @cached_property",
          "588:     def needs_python_scans(self) -> bool:",
          "589:         return self._should_be_run(FileGroupForCi.PYTHON_PRODUCTION_FILES)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "587:     @cached_property",
          "588:     def mypy_packages(self) -> list[str]:",
          "589:         packages_to_run: list[str] = []",
          "590:         if (",
          "591:             self._matching_files(",
          "592:                 FileGroupForCi.ALL_AIRFLOW_PYTHON_FILES, CI_FILE_GROUP_MATCHES, CI_FILE_GROUP_EXCLUDES",
          "593:             )",
          "594:             or self.full_tests_needed",
          "595:         ):",
          "596:             packages_to_run.append(\"airflow\")",
          "597:         if (",
          "598:             self._matching_files(",
          "599:                 FileGroupForCi.ALL_PROVIDERS_PYTHON_FILES, CI_FILE_GROUP_MATCHES, CI_FILE_GROUP_EXCLUDES",
          "600:             )",
          "601:             or self._are_all_providers_affected()",
          "602:         ) and self._default_branch == \"main\":",
          "603:             packages_to_run.append(\"airflow/providers\")",
          "604:         if (",
          "605:             self._matching_files(",
          "606:                 FileGroupForCi.ALL_DOCS_PYTHON_FILES, CI_FILE_GROUP_MATCHES, CI_FILE_GROUP_EXCLUDES",
          "607:             )",
          "608:             or self.full_tests_needed",
          "609:         ):",
          "610:             packages_to_run.append(\"docs\")",
          "611:         if (",
          "612:             self._matching_files(",
          "613:                 FileGroupForCi.ALL_DEV_PYTHON_FILES, CI_FILE_GROUP_MATCHES, CI_FILE_GROUP_EXCLUDES",
          "614:             )",
          "615:             or self.full_tests_needed",
          "616:         ):",
          "617:             packages_to_run.append(\"dev\")",
          "618:         return packages_to_run",
          "620:     @cached_property",
          "621:     def needs_mypy(self) -> bool:",
          "622:         return self.mypy_packages != []",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "823:     def skip_pre_commits(self) -> str:",
          "824:         pre_commits_to_skip = set()",
          "825:         pre_commits_to_skip.add(\"identity\")",
          "826:         if self._default_branch != \"main\":",
          "827:             # Skip those tests on all \"release\" branches",
          "828:             pre_commits_to_skip.update(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "863:         # Skip all mypy \"individual\" file checks if we are running mypy checks in CI",
          "864:         # In the CI we always run mypy for the whole \"package\" rather than for `--all-files` because",
          "865:         # The pre-commit will semi-randomly skip such list of files into several groups and we want",
          "866:         # to make sure that such checks are always run in CI for whole \"group\" of files - i.e.",
          "867:         # whole package rather than for individual files. That's why we skip those checks in CI",
          "868:         # and run them via `mypy-all` command instead and dedicated CI job in matrix",
          "869:         # This will also speed up static-checks job usually as the jobs will be running in parallel",
          "870:         pre_commits_to_skip.update({\"mypy-providers\", \"mypy-core\", \"mypy-docs\", \"mypy-dev\"})",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "831:                     \"check-extra-packages-references\",",
          "832:                     \"check-provider-yaml-valid\",",
          "833:                     \"lint-helm-chart\",",
          "835:                 )",
          "836:             )",
          "837:         if self.full_tests_needed:",
          "838:             # when full tests are needed, we do not want to skip any checks and we should",
          "839:             # run all the pre-commits just to be sure everything is ok when some structural changes occurred",
          "840:             return \",\".join(sorted(pre_commits_to_skip))",
          "857:         if not self._matching_files(FileGroupForCi.WWW_FILES, CI_FILE_GROUP_MATCHES, CI_FILE_GROUP_EXCLUDES):",
          "858:             pre_commits_to_skip.add(\"ts-compile-format-lint-www\")",
          "859:         if not self._matching_files(",
          "",
          "[Removed Lines]",
          "834:                     \"mypy-providers\",",
          "841:         if not self._matching_files(",
          "842:             FileGroupForCi.ALL_PROVIDERS_PYTHON_FILES, CI_FILE_GROUP_MATCHES, CI_FILE_GROUP_EXCLUDES",
          "843:         ):",
          "844:             pre_commits_to_skip.add(\"mypy-providers\")",
          "845:         if not self._matching_files(",
          "846:             FileGroupForCi.ALL_AIRFLOW_PYTHON_FILES, CI_FILE_GROUP_MATCHES, CI_FILE_GROUP_EXCLUDES",
          "847:         ):",
          "848:             pre_commits_to_skip.add(\"mypy-core\")",
          "849:         if not self._matching_files(",
          "850:             FileGroupForCi.ALL_DOCS_PYTHON_FILES, CI_FILE_GROUP_MATCHES, CI_FILE_GROUP_EXCLUDES",
          "851:         ):",
          "852:             pre_commits_to_skip.add(\"mypy-docs\")",
          "853:         if not self._matching_files(",
          "854:             FileGroupForCi.ALL_DEV_PYTHON_FILES, CI_FILE_GROUP_MATCHES, CI_FILE_GROUP_EXCLUDES",
          "855:         ):",
          "856:             pre_commits_to_skip.add(\"mypy-dev\")",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_selective_checks.py||dev/breeze/tests/test_selective_checks.py": [
          "File: dev/breeze/tests/test_selective_checks.py -> dev/breeze/tests/test_selective_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "75:                     print_in_color(\"\\nOutput received:\")",
          "76:                     print_in_color(received_output_as_dict)",
          "77:                     print_in_color()",
          "79:                 else:",
          "80:                     print(",
          "81:                         f\"\\n[red]ERROR: The key '{expected_key}' missing but \"",
          "",
          "[Removed Lines]",
          "78:                     assert expected_value == received_value",
          "",
          "[Added Lines]",
          "78:                     assert received_value == expected_value",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "111:                     \"mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "112:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "113:                     \"parallel-test-types-list-as-string\": None,",
          "114:                 },",
          "115:                 id=\"No tests on simple change\",",
          "116:             )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "114:                     \"needs-mypy\": \"false\",",
          "115:                     \"mypy-packages\": \"[]\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "130:                     \"run-tests\": \"true\",",
          "131:                     \"run-amazon-tests\": \"false\",",
          "132:                     \"docs-build\": \"true\",",
          "134:                     \"mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "135:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "136:                     \"parallel-test-types-list-as-string\": \"API Always\",",
          "137:                 },",
          "138:                 id=\"Only API tests and DOCS should run\",",
          "139:             )",
          "",
          "[Removed Lines]",
          "133:                     \"skip-pre-commits\": \"check-provider-yaml-valid,identity,lint-helm-chart,mypy-dev,\"",
          "",
          "[Added Lines]",
          "135:                     \"skip-pre-commits\": \"check-provider-yaml-valid,identity,lint-helm-chart,mypy-core,mypy-dev,\"",
          "139:                     \"needs-mypy\": \"true\",",
          "140:                     \"mypy-packages\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "153:                     \"run-tests\": \"true\",",
          "154:                     \"run-amazon-tests\": \"false\",",
          "155:                     \"docs-build\": \"true\",",
          "157:                     \"mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "158:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "159:                     \"parallel-test-types-list-as-string\": \"Always Operators\",",
          "160:                 },",
          "161:                 id=\"Only Operator tests and DOCS should run\",",
          "162:             )",
          "",
          "[Removed Lines]",
          "156:                     \"skip-pre-commits\": \"check-provider-yaml-valid,identity,lint-helm-chart,mypy-dev,\"",
          "",
          "[Added Lines]",
          "160:                     \"skip-pre-commits\": \"check-provider-yaml-valid,identity,lint-helm-chart,mypy-core,mypy-dev,\"",
          "164:                     \"needs-mypy\": \"true\",",
          "165:                     \"mypy-packages\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "176:                     \"run-tests\": \"true\",",
          "177:                     \"run-amazon-tests\": \"false\",",
          "178:                     \"docs-build\": \"true\",",
          "180:                     \"mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "181:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "182:                     \"parallel-test-types-list-as-string\": \"Always BranchExternalPython BranchPythonVenv \"",
          "183:                     \"ExternalPython Operators PythonVenv\",",
          "184:                 },",
          "185:                 id=\"Only Python tests\",",
          "186:             )",
          "",
          "[Removed Lines]",
          "179:                     \"skip-pre-commits\": \"check-provider-yaml-valid,identity,lint-helm-chart,mypy-dev,\"",
          "",
          "[Added Lines]",
          "185:                     \"skip-pre-commits\": \"check-provider-yaml-valid,identity,lint-helm-chart,mypy-core,mypy-dev,\"",
          "190:                     \"needs-mypy\": \"true\",",
          "191:                     \"mypy-packages\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "200:                     \"run-tests\": \"true\",",
          "201:                     \"run-amazon-tests\": \"false\",",
          "202:                     \"docs-build\": \"true\",",
          "204:                     \"mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "205:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "206:                     \"parallel-test-types-list-as-string\": \"Always Serialization\",",
          "207:                 },",
          "208:                 id=\"Only Serialization tests\",",
          "209:             )",
          "",
          "[Removed Lines]",
          "203:                     \"skip-pre-commits\": \"check-provider-yaml-valid,identity,lint-helm-chart,mypy-dev,\"",
          "",
          "[Added Lines]",
          "211:                     \"skip-pre-commits\": \"check-provider-yaml-valid,identity,lint-helm-chart,mypy-core,mypy-dev,\"",
          "215:                     \"needs-mypy\": \"true\",",
          "216:                     \"mypy-packages\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "227:                     \"run-tests\": \"true\",",
          "228:                     \"run-amazon-tests\": \"true\",",
          "229:                     \"docs-build\": \"true\",",
          "231:                     \"ts-compile-format-lint-www\",",
          "232:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "233:                     \"parallel-test-types-list-as-string\": \"API Always Providers[amazon] \"",
          "234:                     \"Providers[common.sql,openlineage,pgvector,postgres] Providers[google]\",",
          "235:                 },",
          "236:                 id=\"API and providers tests and docs should run\",",
          "237:             )",
          "",
          "[Removed Lines]",
          "230:                     \"skip-pre-commits\": \"identity,lint-helm-chart,mypy-dev,mypy-docs,\"",
          "",
          "[Added Lines]",
          "240:                     \"skip-pre-commits\": \"identity,lint-helm-chart,mypy-core,mypy-dev,mypy-docs,mypy-providers,\"",
          "245:                     \"needs-mypy\": \"true\",",
          "246:                     \"mypy-packages\": \"['airflow', 'airflow/providers']\",",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "251:                     \"run-tests\": \"true\",",
          "252:                     \"run-amazon-tests\": \"false\",",
          "253:                     \"docs-build\": \"false\",",
          "255:                     \"ts-compile-format-lint-www\",",
          "256:                     \"run-kubernetes-tests\": \"false\",",
          "257:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "258:                     \"parallel-test-types-list-as-string\": \"Always Providers[apache.beam] Providers[google]\",",
          "259:                 },",
          "260:                 id=\"Selected Providers and docs should run\",",
          "261:             )",
          "",
          "[Removed Lines]",
          "254:                     \"skip-pre-commits\": \"identity,lint-helm-chart,mypy-core,mypy-dev,mypy-docs,\"",
          "",
          "[Added Lines]",
          "266:                     \"skip-pre-commits\": \"identity,lint-helm-chart,mypy-core,mypy-dev,mypy-docs,mypy-providers,\"",
          "271:                     \"needs-mypy\": \"true\",",
          "272:                     \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "280:                     \"run-kubernetes-tests\": \"false\",",
          "281:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "282:                     \"parallel-test-types-list-as-string\": None,",
          "283:                 },",
          "284:                 id=\"Only docs builds should run - no tests needed\",",
          "285:             )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "297:                     \"needs-mypy\": \"false\",",
          "298:                     \"mypy-packages\": \"[]\",",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "303:                     \"run-tests\": \"true\",",
          "304:                     \"run-amazon-tests\": \"true\",",
          "305:                     \"docs-build\": \"true\",",
          "307:                     \"run-kubernetes-tests\": \"true\",",
          "308:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "309:                     \"parallel-test-types-list-as-string\": \"Always Providers[amazon] \"",
          "310:                     \"Providers[common.sql,openlineage,pgvector,postgres] Providers[google]\",",
          "311:                 },",
          "312:                 id=\"Helm tests, providers (both upstream and downstream),\"",
          "313:                 \"kubernetes tests and docs should run\",",
          "",
          "[Removed Lines]",
          "306:                     \"skip-pre-commits\": \"identity,mypy-core,mypy-dev,mypy-docs,ts-compile-format-lint-www\",",
          "",
          "[Added Lines]",
          "322:                     \"skip-pre-commits\": \"identity,mypy-core,mypy-dev,mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "327:                     \"needs-mypy\": \"true\",",
          "328:                     \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "333:                     \"run-tests\": \"true\",",
          "334:                     \"run-amazon-tests\": \"true\",",
          "335:                     \"docs-build\": \"true\",",
          "337:                     \"run-kubernetes-tests\": \"true\",",
          "338:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "339:                     \"parallel-test-types-list-as-string\": \"Always \"",
          "340:                     \"Providers[airbyte,apache.livy,dbt.cloud,dingding,discord,http] Providers[amazon]\",",
          "341:                 },",
          "342:                 id=\"Helm tests, http and all relevant providers, kubernetes tests and \"",
          "343:                 \"docs should run even if unimportant files were added\",",
          "",
          "[Removed Lines]",
          "336:                     \"skip-pre-commits\": \"identity,mypy-core,mypy-dev,mypy-docs,ts-compile-format-lint-www\",",
          "",
          "[Added Lines]",
          "354:                     \"skip-pre-commits\": \"identity,mypy-core,mypy-dev,mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "359:                     \"needs-mypy\": \"true\",",
          "360:                     \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "362:                     \"run-tests\": \"true\",",
          "363:                     \"run-amazon-tests\": \"false\",",
          "364:                     \"docs-build\": \"true\",",
          "366:                     \"run-kubernetes-tests\": \"true\",",
          "367:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "368:                     \"parallel-test-types-list-as-string\": \"Always Providers[airbyte,http]\",",
          "369:                 },",
          "370:                 id=\"Helm tests, airbyte/http providers, kubernetes tests and \"",
          "371:                 \"docs should run even if unimportant files were added\",",
          "",
          "[Removed Lines]",
          "365:                     \"skip-pre-commits\": \"identity,mypy-core,mypy-dev,mypy-docs,ts-compile-format-lint-www\",",
          "",
          "[Added Lines]",
          "385:                     \"skip-pre-commits\": \"identity,mypy-core,mypy-dev,mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "389:                     \"needs-mypy\": \"true\",",
          "390:                     \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "389:                     \"needs-helm-tests\": \"true\",",
          "390:                     \"run-tests\": \"true\",",
          "391:                     \"docs-build\": \"true\",",
          "393:                     \"mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "394:                     \"run-amazon-tests\": \"false\",",
          "395:                     \"run-kubernetes-tests\": \"true\",",
          "396:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "397:                     \"parallel-test-types-list-as-string\": \"Always\",",
          "398:                 },",
          "399:                 id=\"Docs should run even if unimportant files were added and prod image \"",
          "400:                 \"should be build for chart changes\",",
          "",
          "[Removed Lines]",
          "392:                     \"skip-pre-commits\": \"check-provider-yaml-valid,identity,mypy-dev,\"",
          "",
          "[Added Lines]",
          "414:                     \"skip-pre-commits\": \"check-provider-yaml-valid,identity,mypy-core,mypy-dev,\"",
          "420:                     \"needs-mypy\": \"true\",",
          "421:                     \"mypy-packages\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "416:                     \"run-amazon-tests\": \"true\",",
          "417:                     \"docs-build\": \"true\",",
          "418:                     \"full-tests-needed\": \"true\",",
          "420:                     \"upgrade-to-newer-dependencies\": \"true\",",
          "421:                     \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "422:                 },",
          "423:                 id=\"Everything should run - including all providers and upgrading to \"",
          "424:                 \"newer requirements as setup.py changed and all Python versions\",",
          "",
          "[Removed Lines]",
          "419:                     \"skip-pre-commits\": \"identity\",",
          "",
          "[Added Lines]",
          "443:                     \"skip-pre-commits\": \"identity,mypy-core,mypy-dev,mypy-docs,mypy-providers\",",
          "446:                     \"needs-mypy\": \"true\",",
          "447:                     \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "440:                     \"run-amazon-tests\": \"true\",",
          "441:                     \"docs-build\": \"true\",",
          "442:                     \"full-tests-needed\": \"true\",",
          "444:                     \"upgrade-to-newer-dependencies\": \"true\",",
          "445:                     \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "446:                 },",
          "447:                 id=\"Everything should run and upgrading to newer requirements as dependencies change\",",
          "448:             )",
          "",
          "[Removed Lines]",
          "443:                     \"skip-pre-commits\": \"identity\",",
          "",
          "[Added Lines]",
          "469:                     \"skip-pre-commits\": \"identity,mypy-core,mypy-dev,mypy-docs,mypy-providers\",",
          "472:                     \"needs-mypy\": \"true\",",
          "473:                     \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "462:                 \"needs-helm-tests\": \"false\",",
          "463:                 \"run-tests\": \"true\",",
          "464:                 \"docs-build\": \"true\",",
          "466:                 \"run-kubernetes-tests\": \"false\",",
          "467:                 \"upgrade-to-newer-dependencies\": \"false\",",
          "468:                 \"run-amazon-tests\": \"true\",",
          "469:                 \"parallel-test-types-list-as-string\": \"Always Providers[amazon] \"",
          "470:                 \"Providers[apache.hive,cncf.kubernetes,common.sql,exasol,ftp,http,\"",
          "471:                 \"imap,microsoft.azure,mongo,mysql,openlineage,postgres,salesforce,ssh] Providers[google]\",",
          "472:             },",
          "473:             id=\"Providers tests run including amazon tests if amazon provider files changed\",",
          "474:         ),",
          "",
          "[Removed Lines]",
          "465:                 \"skip-pre-commits\": \"identity,lint-helm-chart,mypy-core,mypy-dev,mypy-docs,ts-compile-format-lint-www\",",
          "",
          "[Added Lines]",
          "493:                 \"skip-pre-commits\": \"identity,lint-helm-chart,mypy-core,mypy-dev,mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "500:                 \"needs-mypy\": \"true\",",
          "501:                 \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "486:                 \"run-tests\": \"true\",",
          "487:                 \"run-amazon-tests\": \"false\",",
          "488:                 \"docs-build\": \"false\",",
          "490:                 \"run-kubernetes-tests\": \"false\",",
          "491:                 \"upgrade-to-newer-dependencies\": \"false\",",
          "492:                 \"parallel-test-types-list-as-string\": \"Always Providers[airbyte,http]\",",
          "493:             },",
          "494:             id=\"Providers tests run without amazon tests if no amazon file changed\",",
          "495:         ),",
          "",
          "[Removed Lines]",
          "489:                 \"skip-pre-commits\": \"identity,lint-helm-chart,mypy-core,mypy-dev,mypy-docs,ts-compile-format-lint-www\",",
          "",
          "[Added Lines]",
          "519:                 \"skip-pre-commits\": \"identity,lint-helm-chart,mypy-core,mypy-dev,mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "523:                 \"needs-mypy\": \"true\",",
          "524:                 \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "509:                 \"run-tests\": \"true\",",
          "510:                 \"run-amazon-tests\": \"true\",",
          "511:                 \"docs-build\": \"true\",",
          "513:                 \"run-kubernetes-tests\": \"false\",",
          "514:                 \"upgrade-to-newer-dependencies\": \"false\",",
          "515:                 \"parallel-test-types-list-as-string\": \"Always Providers[amazon] \"",
          "516:                 \"Providers[apache.hive,cncf.kubernetes,common.sql,exasol,ftp,http,\"",
          "517:                 \"imap,microsoft.azure,mongo,mysql,openlineage,postgres,salesforce,ssh] Providers[google]\",",
          "518:             },",
          "519:             id=\"Providers tests run including amazon tests if amazon provider files changed\",",
          "520:         ),",
          "",
          "[Removed Lines]",
          "512:                 \"skip-pre-commits\": \"identity,lint-helm-chart,mypy-core,mypy-dev,mypy-docs,ts-compile-format-lint-www\",",
          "",
          "[Added Lines]",
          "544:                 \"skip-pre-commits\": \"identity,lint-helm-chart,mypy-core,mypy-dev,mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "550:                 \"needs-mypy\": \"true\",",
          "551:                 \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "537:                 \"run-amazon-tests\": \"false\",",
          "538:                 \"docs-build\": \"false\",",
          "539:                 \"run-kubernetes-tests\": \"false\",",
          "540:                 \"upgrade-to-newer-dependencies\": \"false\",",
          "541:                 \"parallel-test-types-list-as-string\": \"Always Providers[common.io]\",",
          "542:             },",
          "543:             id=\"Only Always and Common.IO tests should run when only common.io and tests/always changed\",",
          "544:         ),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "574:                 \"skip-pre-commits\": \"identity,lint-helm-chart,mypy-core,mypy-dev,mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "577:                 \"needs-mypy\": \"true\",",
          "578:                 \"mypy-packages\": \"['airflow', 'airflow/providers']\",",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "578:                     \"docs-build\": \"true\",",
          "579:                     \"docs-list-as-string\": ALL_DOCS_SELECTED_FOR_BUILD,",
          "580:                     \"full-tests-needed\": \"true\",",
          "582:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "583:                     \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "584:                 },",
          "585:                 id=\"Everything should run including all providers when full tests are needed\",",
          "586:             )",
          "",
          "[Removed Lines]",
          "581:                     \"skip-pre-commits\": \"identity\",",
          "",
          "[Added Lines]",
          "618:                     \"skip-pre-commits\": \"identity,mypy-core,mypy-dev,mypy-docs,mypy-providers\",",
          "621:                     \"needs-mypy\": \"true\",",
          "622:                     \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "605:                     \"docs-build\": \"true\",",
          "606:                     \"docs-list-as-string\": ALL_DOCS_SELECTED_FOR_BUILD,",
          "607:                     \"full-tests-needed\": \"true\",",
          "609:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "610:                     \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "611:                 },",
          "612:                 id=\"Everything should run including full providers when full \"",
          "613:                 \"tests are needed even with different label set as well\",",
          "",
          "[Removed Lines]",
          "608:                     \"skip-pre-commits\": \"identity\",",
          "",
          "[Added Lines]",
          "647:                     \"skip-pre-commits\": \"identity,mypy-core,mypy-dev,mypy-docs,mypy-providers\",",
          "650:                     \"needs-mypy\": \"true\",",
          "651:                     \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "630:                     \"docs-build\": \"true\",",
          "631:                     \"docs-list-as-string\": ALL_DOCS_SELECTED_FOR_BUILD,",
          "632:                     \"full-tests-needed\": \"true\",",
          "634:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "635:                     \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "636:                 },",
          "637:                 id=\"Everything should run including full providers when\"",
          "638:                 \"full tests are needed even if no files are changed\",",
          "",
          "[Removed Lines]",
          "633:                     \"skip-pre-commits\": \"identity\",",
          "",
          "[Added Lines]",
          "674:                     \"skip-pre-commits\": \"identity,mypy-core,mypy-dev,mypy-docs,mypy-providers\",",
          "677:                     \"needs-mypy\": \"true\",",
          "678:                     \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "655:                     \"docs-build\": \"true\",",
          "656:                     \"docs-list-as-string\": \"apache-airflow docker-stack\",",
          "657:                     \"full-tests-needed\": \"true\",",
          "661:                     \"skip-provider-tests\": \"true\",",
          "662:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "663:                     \"parallel-test-types-list-as-string\": \"API Always BranchExternalPython \"",
          "664:                     \"BranchPythonVenv CLI Core ExternalPython Operators Other PlainAsserts \"",
          "665:                     \"PythonVenv Serialization WWW\",",
          "666:                 },",
          "667:                 id=\"Everything should run except Providers and lint pre-commit \"",
          "668:                 \"when full tests are needed for non-main branch\",",
          "",
          "[Removed Lines]",
          "658:                     \"skip-pre-commits\": \"check-airflow-provider-compatibility,\"",
          "659:                     \"check-extra-packages-references,check-provider-yaml-valid,identity,\"",
          "660:                     \"lint-helm-chart,mypy-providers\",",
          "",
          "[Added Lines]",
          "701:                     \"skip-pre-commits\": \"check-airflow-provider-compatibility,check-extra-packages-references,check-provider-yaml-valid,identity,lint-helm-chart,mypy-core,mypy-dev,mypy-docs,mypy-providers\",",
          "707:                     \"needs-mypy\": \"true\",",
          "708:                     \"mypy-packages\": \"['airflow', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "707:                 \"upgrade-to-newer-dependencies\": \"false\",",
          "708:                 \"skip-provider-tests\": \"true\",",
          "709:                 \"parallel-test-types-list-as-string\": None,",
          "710:             },",
          "711:             id=\"Nothing should run if only non-important files changed\",",
          "712:         ),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "753:                 \"needs-mypy\": \"false\",",
          "754:                 \"mypy-packages\": \"[]\",",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "735:                 \"upgrade-to-newer-dependencies\": \"false\",",
          "736:                 \"skip-provider-tests\": \"true\",",
          "737:                 \"parallel-test-types-list-as-string\": \"Always\",",
          "738:             },",
          "739:             id=\"No Helm tests, No providers no lint charts, should run if \"",
          "740:             \"only chart/providers changed in non-main but PROD image should be built\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "783:                 \"needs-mypy\": \"false\",",
          "784:                 \"mypy-packages\": \"[]\",",
          "",
          "---------------",
          "--- Hunk 26 ---",
          "[Context before]",
          "759:                 \"docs-build\": \"true\",",
          "760:                 \"docs-list-as-string\": \"apache-airflow docker-stack\",",
          "761:                 \"full-tests-needed\": \"false\",",
          "765:                 \"run-kubernetes-tests\": \"true\",",
          "766:                 \"upgrade-to-newer-dependencies\": \"false\",",
          "767:                 \"skip-provider-tests\": \"true\",",
          "768:                 \"parallel-test-types-list-as-string\": \"Always CLI\",",
          "769:             },",
          "770:             id=\"Only CLI tests and Kubernetes tests should run if cli/chart files changed in non-main branch\",",
          "771:         ),",
          "",
          "[Removed Lines]",
          "762:                 \"skip-pre-commits\": \"check-airflow-provider-compatibility,check-extra-packages-references,\"",
          "763:                 \"check-provider-yaml-valid,identity,lint-helm-chart,\"",
          "764:                 \"mypy-dev,mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "",
          "[Added Lines]",
          "809:                 \"skip-pre-commits\": \"check-airflow-provider-compatibility,check-extra-packages-references,check-provider-yaml-valid,identity,lint-helm-chart,mypy-core,mypy-dev,mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "814:                 \"needs-mypy\": \"true\",",
          "815:                 \"mypy-packages\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 27 ---",
          "[Context before]",
          "788:                 \"run-kubernetes-tests\": \"false\",",
          "789:                 \"upgrade-to-newer-dependencies\": \"false\",",
          "790:                 \"skip-provider-tests\": \"true\",",
          "794:                 \"parallel-test-types-list-as-string\": \"API Always BranchExternalPython BranchPythonVenv \"",
          "795:                 \"CLI Core ExternalPython Operators Other PlainAsserts PythonVenv Serialization WWW\",",
          "796:             },",
          "797:             id=\"All tests except Providers and helm lint pre-commit \"",
          "798:             \"should run if core file changed in non-main branch\",",
          "",
          "[Removed Lines]",
          "791:                 \"skip-pre-commits\": \"check-airflow-provider-compatibility,check-extra-packages-references,\"",
          "792:                 \"check-provider-yaml-valid,identity,lint-helm-chart,\"",
          "793:                 \"mypy-dev,mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "",
          "[Added Lines]",
          "838:                 \"skip-pre-commits\": \"check-airflow-provider-compatibility,check-extra-packages-references,check-provider-yaml-valid,identity,lint-helm-chart,mypy-core,mypy-dev,mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "841:                 \"needs-mypy\": \"true\",",
          "842:                 \"mypy-packages\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 28 ---",
          "[Context before]",
          "832:                 \"mypy-core,mypy-dev,mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "833:                 \"skip-provider-tests\": \"true\",",
          "834:                 \"parallel-test-types-list-as-string\": None,",
          "835:             },",
          "836:             id=\"Nothing should run if only non-important files changed\",",
          "837:         ),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "882:                 \"needs-mypy\": \"false\",",
          "883:                 \"mypy-packages\": \"[]\",",
          "",
          "---------------",
          "--- Hunk 29 ---",
          "[Context before]",
          "847:                 \"run-tests\": \"true\",",
          "848:                 \"docs-build\": \"true\",",
          "849:                 \"docs-list-as-string\": ALL_DOCS_SELECTED_FOR_BUILD,",
          "852:                 \"upgrade-to-newer-dependencies\": \"false\",",
          "853:                 \"skip-provider-tests\": \"true\",",
          "854:                 \"parallel-test-types-list-as-string\": \"Always\",",
          "855:             },",
          "856:             id=\"Only Always and docs build should run if only system tests changed\",",
          "857:         ),",
          "",
          "[Removed Lines]",
          "850:                 \"skip-pre-commits\": \"check-provider-yaml-valid,identity,lint-helm-chart,\"",
          "851:                 \"mypy-dev,mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "",
          "[Added Lines]",
          "899:                 \"skip-pre-commits\": \"check-provider-yaml-valid,identity,lint-helm-chart,mypy-core,mypy-dev,mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "903:                 \"needs-mypy\": \"true\",",
          "904:                 \"mypy-packages\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 30 ---",
          "[Context before]",
          "877:                 \"cncf.kubernetes common.sql facebook google hashicorp microsoft.azure \"",
          "878:                 \"microsoft.mssql mysql openlineage oracle postgres \"",
          "879:                 \"presto salesforce samba sftp ssh trino\",",
          "881:                 \"run-kubernetes-tests\": \"true\",",
          "882:                 \"upgrade-to-newer-dependencies\": \"false\",",
          "883:                 \"skip-provider-tests\": \"false\",",
          "",
          "[Removed Lines]",
          "880:                 \"skip-pre-commits\": \"identity,mypy-dev,mypy-docs,ts-compile-format-lint-www\",",
          "",
          "[Added Lines]",
          "930:                 \"skip-pre-commits\": \"identity,mypy-core,mypy-dev,mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "",
          "---------------",
          "--- Hunk 31 ---",
          "[Context before]",
          "885:                 \"Providers[apache.beam,apache.cassandra,cncf.kubernetes,common.sql,facebook,hashicorp,\"",
          "886:                 \"microsoft.azure,microsoft.mssql,mysql,openlineage,oracle,postgres,presto,salesforce,\"",
          "887:                 \"samba,sftp,ssh,trino] Providers[google]\",",
          "888:             },",
          "889:             id=\"CLI tests and Google-related provider tests should run if cli/chart files changed but \"",
          "890:             \"prod image should be build too and k8s tests too\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "938:                 \"needs-mypy\": \"true\",",
          "939:                 \"mypy-packages\": \"['airflow', 'airflow/providers']\",",
          "",
          "---------------",
          "--- Hunk 32 ---",
          "[Context before]",
          "906:                 \"run-tests\": \"true\",",
          "907:                 \"docs-build\": \"true\",",
          "908:                 \"docs-list-as-string\": \"apache-airflow\",",
          "911:                 \"run-kubernetes-tests\": \"false\",",
          "912:                 \"upgrade-to-newer-dependencies\": \"false\",",
          "913:                 \"skip-provider-tests\": \"true\",",
          "914:                 \"parallel-test-types-list-as-string\": \"API Always CLI Operators WWW\",",
          "915:             },",
          "916:             id=\"No providers tests should run if only CLI/API/Operators/WWW file changed\",",
          "917:         ),",
          "",
          "[Removed Lines]",
          "909:                 \"skip-pre-commits\": \"check-provider-yaml-valid,identity,lint-helm-chart,mypy-dev,\"",
          "910:                 \"mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "",
          "[Added Lines]",
          "961:                 \"skip-pre-commits\": \"check-provider-yaml-valid,identity,lint-helm-chart,mypy-core,mypy-dev,mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "966:                 \"needs-mypy\": \"true\",",
          "967:                 \"mypy-packages\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 33 ---",
          "[Context before]",
          "927:                 \"run-tests\": \"true\",",
          "928:                 \"docs-build\": \"true\",",
          "929:                 \"docs-list-as-string\": ALL_DOCS_SELECTED_FOR_BUILD,",
          "932:                 \"run-kubernetes-tests\": \"false\",",
          "933:                 \"upgrade-to-newer-dependencies\": \"false\",",
          "934:                 \"skip-provider-tests\": \"false\",",
          "935:                 \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "936:             },",
          "937:             id=\"Tests for all providers should run if model file changed\",",
          "938:         ),",
          "",
          "[Removed Lines]",
          "930:                 \"skip-pre-commits\": \"check-provider-yaml-valid,identity,lint-helm-chart,mypy-dev,\"",
          "931:                 \"mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "",
          "[Added Lines]",
          "983:                 \"skip-pre-commits\": \"check-provider-yaml-valid,identity,lint-helm-chart,mypy-core,mypy-dev,mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "988:                 \"needs-mypy\": \"true\",",
          "989:                 \"mypy-packages\": \"['airflow', 'airflow/providers']\",",
          "",
          "---------------",
          "--- Hunk 34 ---",
          "[Context before]",
          "948:                 \"run-tests\": \"true\",",
          "949:                 \"docs-build\": \"true\",",
          "950:                 \"docs-list-as-string\": ALL_DOCS_SELECTED_FOR_BUILD,",
          "953:                 \"run-kubernetes-tests\": \"false\",",
          "954:                 \"upgrade-to-newer-dependencies\": \"false\",",
          "955:                 \"skip-provider-tests\": \"false\",",
          "956:                 \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "957:             },",
          "958:             id=\"Tests for all providers should run if any other than API/WWW/CLI/Operators file changed.\",",
          "959:         ),",
          "",
          "[Removed Lines]",
          "951:                 \"skip-pre-commits\": \"check-provider-yaml-valid,identity,lint-helm-chart,mypy-dev,\"",
          "952:                 \"mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "",
          "[Added Lines]",
          "1005:                 \"skip-pre-commits\": \"check-provider-yaml-valid,identity,lint-helm-chart,mypy-core,mypy-dev,mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "1010:                 \"needs-mypy\": \"true\",",
          "1011:                 \"mypy-packages\": \"['airflow', 'airflow/providers']\",",
          "",
          "---------------",
          "--- Hunk 35 ---",
          "[Context before]",
          "990:                 \"run-tests\": \"true\",",
          "991:                 \"docs-build\": \"true\",",
          "992:                 \"docs-list-as-string\": ALL_DOCS_SELECTED_FOR_BUILD,",
          "994:                 \"upgrade-to-newer-dependencies\": \"true\",",
          "995:                 \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "996:             },",
          "997:             id=\"All tests run on push even if unimportant file changed\",",
          "998:         ),",
          "",
          "[Removed Lines]",
          "993:                 \"skip-pre-commits\": \"identity\",",
          "",
          "[Added Lines]",
          "1048:                 \"skip-pre-commits\": \"identity,mypy-core,mypy-dev,mypy-docs,mypy-providers\",",
          "1051:                 \"needs-mypy\": \"true\",",
          "1052:                 \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 36 ---",
          "[Context before]",
          "1009:                 \"needs-helm-tests\": \"false\",",
          "1010:                 \"run-tests\": \"true\",",
          "1011:                 \"docs-build\": \"true\",",
          "1014:                 \"docs-list-as-string\": \"apache-airflow docker-stack\",",
          "1015:                 \"upgrade-to-newer-dependencies\": \"true\",",
          "1016:                 \"parallel-test-types-list-as-string\": \"API Always BranchExternalPython BranchPythonVenv \"",
          "1017:                 \"CLI Core ExternalPython Operators Other PlainAsserts PythonVenv Serialization WWW\",",
          "1018:             },",
          "1019:             id=\"All tests except Providers and Helm run on push\"",
          "1020:             \" even if unimportant file changed in non-main branch\",",
          "",
          "[Removed Lines]",
          "1012:                 \"skip-pre-commits\": \"check-airflow-provider-compatibility,check-extra-packages-references,\"",
          "1013:                 \"check-provider-yaml-valid,identity,lint-helm-chart,mypy-providers\",",
          "",
          "[Added Lines]",
          "1069:                 \"skip-pre-commits\": \"check-airflow-provider-compatibility,check-extra-packages-references,check-provider-yaml-valid,identity,lint-helm-chart,mypy-core,mypy-dev,mypy-docs,mypy-providers\",",
          "1074:                 \"needs-mypy\": \"true\",",
          "1075:                 \"mypy-packages\": \"['airflow', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 37 ---",
          "[Context before]",
          "1032:                 \"needs-helm-tests\": \"true\",",
          "1033:                 \"run-tests\": \"true\",",
          "1034:                 \"docs-build\": \"true\",",
          "1036:                 \"docs-list-as-string\": ALL_DOCS_SELECTED_FOR_BUILD,",
          "1037:                 \"upgrade-to-newer-dependencies\": \"true\",",
          "1038:                 \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "1039:             },",
          "1040:             id=\"All tests run on push if core file changed\",",
          "1041:         ),",
          "",
          "[Removed Lines]",
          "1035:                 \"skip-pre-commits\": \"identity\",",
          "",
          "[Added Lines]",
          "1093:                 \"skip-pre-commits\": \"identity,mypy-core,mypy-dev,mypy-docs,mypy-providers\",",
          "1097:                 \"needs-mypy\": \"true\",",
          "1098:                 \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 38 ---",
          "[Context before]",
          "1084:             \"needs-helm-tests\": \"true\",",
          "1085:             \"run-tests\": \"true\",",
          "1086:             \"docs-build\": \"true\",",
          "1088:             \"upgrade-to-newer-dependencies\": \"true\"",
          "1089:             if github_event in [GithubEvents.PUSH, GithubEvents.SCHEDULE]",
          "1090:             else \"false\",",
          "1091:             \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "1092:         },",
          "1093:         str(stderr),",
          "1094:     )",
          "",
          "[Removed Lines]",
          "1087:             \"skip-pre-commits\": \"identity\",",
          "",
          "[Added Lines]",
          "1147:             \"skip-pre-commits\": \"identity,mypy-core,mypy-dev,mypy-docs,mypy-providers\",",
          "1152:             \"needs-mypy\": \"true\",",
          "1153:             \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 39 ---",
          "[Context before]",
          "1577:         default_branch=\"main\",",
          "1578:     )",
          "1579:     assert_outputs_are_printed(expected_outputs, str(stderr))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1644: @pytest.mark.parametrize(",
          "1645:     \"files, expected_outputs, default_branch, pr_labels\",",
          "1646:     [",
          "1647:         pytest.param(",
          "1648:             (\"README.md\",),",
          "1649:             {",
          "1650:                 \"needs-mypy\": \"false\",",
          "1651:                 \"mypy-packages\": \"[]\",",
          "1652:             },",
          "1653:             \"main\",",
          "1654:             (),",
          "1655:             id=\"No mypy checks on non-python files\",",
          "1656:         ),",
          "1657:         pytest.param(",
          "1658:             (\"airflow/cli/file.py\",),",
          "1659:             {",
          "1660:                 \"needs-mypy\": \"true\",",
          "1661:                 \"mypy-packages\": \"['airflow']\",",
          "1662:             },",
          "1663:             \"main\",",
          "1664:             (),",
          "1665:             id=\"Airflow mypy checks on airflow regular files\",",
          "1666:         ),",
          "1667:         pytest.param(",
          "1668:             (\"airflow/models/file.py\",),",
          "1669:             {",
          "1670:                 \"needs-mypy\": \"true\",",
          "1671:                 \"mypy-packages\": \"['airflow', 'airflow/providers']\",",
          "1672:             },",
          "1673:             \"main\",",
          "1674:             (),",
          "1675:             id=\"Airflow mypy checks on airflow files that can trigger provider tests\",",
          "1676:         ),",
          "1677:         pytest.param(",
          "1678:             (\"airflow/providers/a_file.py\",),",
          "1679:             {",
          "1680:                 \"needs-mypy\": \"true\",",
          "1681:                 \"mypy-packages\": \"['airflow/providers']\",",
          "1682:             },",
          "1683:             \"main\",",
          "1684:             (),",
          "1685:             id=\"Airflow mypy checks on provider files\",",
          "1686:         ),",
          "1687:         pytest.param(",
          "1688:             (\"docs/a_file.py\",),",
          "1689:             {",
          "1690:                 \"needs-mypy\": \"true\",",
          "1691:                 \"mypy-packages\": \"['docs']\",",
          "1692:             },",
          "1693:             \"main\",",
          "1694:             (),",
          "1695:             id=\"Doc checks on doc files\",",
          "1696:         ),",
          "1697:         pytest.param(",
          "1698:             (\"dev/a_package/a_file.py\",),",
          "1699:             {",
          "1700:                 \"needs-mypy\": \"true\",",
          "1701:                 \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "1702:             },",
          "1703:             \"main\",",
          "1704:             (),",
          "1705:             id=\"All mypy checks on def files changed (full tests needed are implicit)\",",
          "1706:         ),",
          "1707:         pytest.param(",
          "1708:             (\"readme.md\",),",
          "1709:             {",
          "1710:                 \"needs-mypy\": \"true\",",
          "1711:                 \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "1712:             },",
          "1713:             \"main\",",
          "1714:             (\"full tests needed\",),",
          "1715:             id=\"All mypy checks on full tests needed\",",
          "1716:         ),",
          "1717:     ],",
          "1718: )",
          "1719: def test_mypy_matches(",
          "1720:     files: tuple[str, ...], expected_outputs: dict[str, str], default_branch: str, pr_labels: tuple[str, ...]",
          "1721: ):",
          "1722:     stderr = SelectiveChecks(",
          "1723:         files=files,",
          "1724:         commit_ref=\"HEAD\",",
          "1725:         default_branch=default_branch,",
          "1726:         github_event=GithubEvents.PULL_REQUEST,",
          "1727:         pr_labels=pr_labels,",
          "1728:     )",
          "1729:     assert_outputs_are_printed(expected_outputs, str(stderr))",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/common_precommit_utils.py||scripts/ci/pre_commit/common_precommit_utils.py": [
          "File: scripts/ci/pre_commit/common_precommit_utils.py -> scripts/ci/pre_commit/common_precommit_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "45:     raise RuntimeError(\"Couldn't find __version__ in AST\")",
          "50:     default_branch = os.environ.get(\"DEFAULT_BRANCH\")",
          "51:     if not default_branch or default_branch == \"main\":",
          "52:         return files",
          "56: def insert_documentation(file_path: Path, content: list[str], header: str, footer: str):",
          "",
          "[Removed Lines]",
          "48: def filter_out_providers_on_non_main_branch(files: list[str]) -> list[str]:",
          "49:     \"\"\"When running build on non-main branch do not take providers into account\"\"\"",
          "53:     return [file for file in files if not file.startswith(f\"airflow{os.sep}providers\")]",
          "",
          "[Added Lines]",
          "48: def pre_process_files(files: list[str]) -> list[str]:",
          "49:     \"\"\"Pre-process files passed to mypy.",
          "54:     \"\"\"",
          "58:     result = [file for file in files if not file.startswith(f\"airflow{os.sep}providers\")]",
          "59:     if \"airflow/providers\" in files:",
          "60:         if len(files) > 1:",
          "61:             raise RuntimeError(",
          "62:                 \"When running `airflow/providers` package, you cannot run any other packages because only \"",
          "63:                 \"airflow/providers package requires --namespace-packages flag to be set\"",
          "64:             )",
          "65:         result.append(\"--namespace-packages\")",
          "66:     if \"airflow\" in files:",
          "67:         if len(files) > 1:",
          "68:             raise RuntimeError(",
          "69:                 \"When running `airflow` package, you cannot run any other packages because only \"",
          "70:                 \"airflow/providers package requires --exclude airflow/providers/.* flag to be set\"",
          "71:             )",
          "72:         result.extend([\"--exclude\", \"airflow/providers/.*\"])",
          "73:     return result",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_mypy.py||scripts/ci/pre_commit/pre_commit_mypy.py": [
          "File: scripts/ci/pre_commit/pre_commit_mypy.py -> scripts/ci/pre_commit/pre_commit_mypy.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import os",
          "21: import sys",
          "22: from pathlib import Path",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import shlex",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "26: from common_precommit_utils import (",
          "27:     console,",
          "29:     initialize_breeze_precommit,",
          "30:     run_command_via_breeze_shell,",
          "31: )",
          "33: initialize_breeze_precommit(__name__, __file__)",
          "37:     print(\"No files to tests. Quitting\")",
          "38:     sys.exit(0)",
          "",
          "[Removed Lines]",
          "28:     filter_out_providers_on_non_main_branch,",
          "35: files_to_test = filter_out_providers_on_non_main_branch(sys.argv[1:])",
          "36: if files_to_test == [\"--namespace-packages\"]:",
          "",
          "[Added Lines]",
          "30:     pre_process_files,",
          "36: files_to_test = pre_process_files(sys.argv[1:])",
          "37: mypy_packages = os.environ.get(\"MYPY_PACKAGES\")",
          "38: if mypy_packages:",
          "39:     files_to_test += shlex.split(mypy_packages)",
          "40: if files_to_test == [\"--namespace-packages\"] or files_to_test == []:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "51:         \"MOUNT_SOURCES\": \"selected\",",
          "52:     },",
          "53: )",
          "54: if res.returncode != 0:",
          "55:     upgrading = os.environ.get(\"UPGRADE_TO_NEWER_DEPENDENCIES\", \"false\") != \"false\"",
          "56:     if upgrading:",
          "57:         console.print(",
          "59:         )",
          "60:     flag = \" --upgrade-to-newer-dependencies\" if upgrading else \"\"",
          "61:     console.print(",
          "68:     )",
          "69: sys.exit(res.returncode)",
          "",
          "[Removed Lines]",
          "58:             \"[yellow]You are running mypy with the image that has dependencies upgraded automatically.\"",
          "62:         \"[yellow]If you see strange stacktraces above, \"",
          "63:         f\"run `breeze ci-image build --python 3.8{flag}` and try again. \"",
          "64:         \"You can also run `breeze down --cleanup-mypy-cache` to clean up the cache used. \"",
          "65:         \"Still sometimes diff heuristic in mypy is behaving abnormal, to double check you can \"",
          "66:         \"call `breeze static-checks --type mypy-[dev|core|providers|docs] --all-files` \"",
          "67:         'and then commit via `git commit --no-verify -m \"commit message\"`. CI will do a full check.'",
          "",
          "[Added Lines]",
          "58: ci_environment = os.environ.get(\"CI\")",
          "60:     if mypy_packages and ci_environment:",
          "61:         console.print(",
          "62:             \"[yellow]You are running mypy with the packages selected. If you want to\"",
          "63:             \"reproduce it locally, you need to run the following command:\\n\"",
          "64:         )",
          "65:         console.print(",
          "66:             f'MYPY_PACKAGES=\"{mypy_packages}\" pre-commit run --hook-stage manual mypy --all-files\\n'",
          "67:         )",
          "71:             \"[yellow]You are running mypy with the image that has dependencies upgraded automatically.\\n\"",
          "75:         \"[yellow]If you see strange stacktraces above, and can't reproduce it, please run\"",
          "76:         \" this command and try again:\\n\"",
          "78:     console.print(f\"breeze ci-image build --python 3.8{flag}\\n\")",
          "79:     console.print(\"[yellow]You can also run `breeze down --cleanup-mypy-cache` to clean up the cache used.\\n\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "81d1fec83f659c3421a24eeb4d6d8e75d9681f86",
      "candidate_info": {
        "commit_hash": "81d1fec83f659c3421a24eeb4d6d8e75d9681f86",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/81d1fec83f659c3421a24eeb4d6d8e75d9681f86",
        "files": [
          "airflow/www/static/js/components/RunTypeIcon.tsx"
        ],
        "message": "Fix run type icon alignment with run type text (#36616)\n\n(cherry picked from commit 70be78f4e7cea07c759802ed9af51408d36184cf)",
        "before_after_code_files": [
          "airflow/www/static/js/components/RunTypeIcon.tsx||airflow/www/static/js/components/RunTypeIcon.tsx"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/static/js/components/RunTypeIcon.tsx||airflow/www/static/js/components/RunTypeIcon.tsx": [
          "File: airflow/www/static/js/components/RunTypeIcon.tsx -> airflow/www/static/js/components/RunTypeIcon.tsx",
          "--- Hunk 1 ---",
          "[Context before]",
          "29:   runType: DagRun[\"runType\"];",
          "30: }",
          "32: const DagRunTypeIcon = ({ runType, ...rest }: Props) => {",
          "33:   switch (runType) {",
          "34:     case \"manual\":",
          "36:     case \"backfill\":",
          "38:     case \"scheduled\":",
          "40:     case \"dataset_triggered\":",
          "42:     default:",
          "43:       return null;",
          "44:   }",
          "",
          "[Removed Lines]",
          "35:       return <MdPlayArrow style={{ display: \"inline\" }} {...rest} />;",
          "37:       return <RiArrowGoBackFill style={{ display: \"inline\" }} {...rest} />;",
          "39:       return <MdOutlineSchedule style={{ display: \"inline\" }} {...rest} />;",
          "41:       return <HiDatabase style={{ display: \"inline\" }} {...rest} />;",
          "",
          "[Added Lines]",
          "32: const iconStyle = {",
          "33:   display: \"inline\",",
          "34:   verticalAlign: \"bottom\",",
          "35: };",
          "40:       return <MdPlayArrow style={iconStyle} {...rest} />;",
          "42:       return <RiArrowGoBackFill style={iconStyle} {...rest} />;",
          "44:       return <MdOutlineSchedule style={iconStyle} {...rest} />;",
          "46:       return <HiDatabase style={iconStyle} {...rest} />;",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d8537857036a36dcfda171395092f932770d6dda",
      "candidate_info": {
        "commit_hash": "d8537857036a36dcfda171395092f932770d6dda",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/d8537857036a36dcfda171395092f932770d6dda",
        "files": [
          "dev/README_RELEASE_PROVIDER_PACKAGES.md",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py"
        ],
        "message": "Automate rcN calculation when releasing provider packages (#36441)\n\nThis change will automatically generate the **right** rcN package\nwhen prepareing the packages for PyPI. This allows to have pretty\nmuch continuous release process for voting over the provider packages.\n\nSimply when an rcN candidate is not released, it will be automatically\nincluded in the next wave of packages with rcN+1 version - unless during\nprovider package generation the version will be bumped to MAJOR or MINOR\ndue to new changes.\n\nThis allows for the workflow where in every new wave we always generate\nall provider packages ready for release.\n\n(cherry picked from commit 6f5a50ea10842c2bb4b6bdc1e28dfaa680536d5a)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py||dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "563:         shutil.rmtree(DIST_DIR, ignore_errors=True)",
          "564:         DIST_DIR.mkdir(parents=True, exist_ok=True)",
          "565:     for provider_id in packages_list:",
          "566:         try:",
          "567:             basic_provider_checks(provider_id)",
          "570:             get_console().print()",
          "571:             with ci_group(f\"Preparing provider package [special]{provider_id}\"):",
          "572:                 get_console().print()",
          "573:                 target_provider_root_sources_path = copy_provider_sources_to_target(provider_id)",
          "574:                 generate_build_files(",
          "575:                     provider_id=provider_id,",
          "577:                     target_provider_root_sources_path=target_provider_root_sources_path,",
          "578:                 )",
          "579:                 cleanup_build_remnants(target_provider_root_sources_path)",
          "",
          "[Removed Lines]",
          "568:             if not skip_tag_check and should_skip_the_package(provider_id, version_suffix_for_pypi):",
          "569:                 continue",
          "576:                     version_suffix=version_suffix_for_pypi,",
          "",
          "[Added Lines]",
          "566:         package_version = version_suffix_for_pypi",
          "569:             if not skip_tag_check:",
          "570:                 should_skip, package_version = should_skip_the_package(provider_id, package_version)",
          "571:                 if should_skip:",
          "572:                     continue",
          "579:                     version_suffix=package_version,",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py||dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py": [
          "File: dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py -> dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "155:     get_console().print(f\"\\n[info]Generated package build files for {provider_id}[/]\\n\")",
          "161:     For RC and official releases we check if the \"officially released\" version exists",
          "162:     and skip the released if it was. This allows to skip packages that have not been",
          "163:     marked for release in this wave. For \"dev\" suffixes, we always build all packages.",
          "164:     \"\"\"",
          "167:         if tag_exists_for_provider(provider_id, current_tag):",
          "173: def cleanup_build_remnants(target_provider_root_sources_path: Path):",
          "",
          "[Removed Lines]",
          "158: def should_skip_the_package(provider_id: str, version_suffix: str) -> bool:",
          "159:     \"\"\"Return True if the package should be skipped.",
          "165:     if version_suffix.startswith(\"rc\") or version_suffix == \"\":",
          "166:         current_tag = get_latest_provider_tag(provider_id, version_suffix)",
          "168:             get_console().print(f\"[warning]The tag {current_tag} exists. Skipping the package.[/]\")",
          "169:             return True",
          "170:     return False",
          "",
          "[Added Lines]",
          "158: def should_skip_the_package(provider_id: str, version_suffix: str) -> tuple[bool, str]:",
          "159:     \"\"\"Return True, version if the package should be skipped and False, good version suffix if not.",
          "165:     if version_suffix != \"\" and not version_suffix.startswith(\"rc\"):",
          "166:         return False, version_suffix",
          "167:     if version_suffix == \"\":",
          "168:         current_tag = get_latest_provider_tag(provider_id, \"\")",
          "170:             get_console().print(f\"[warning]The 'final' tag {current_tag} exists. Skipping the package.[/]\")",
          "171:             return True, version_suffix",
          "172:         return False, version_suffix",
          "173:     # version_suffix starts with \"rc\"",
          "174:     current_version = int(version_suffix[2:])",
          "175:     while True:",
          "176:         current_tag = get_latest_provider_tag(provider_id, f\"rc{current_version}\")",
          "177:         if tag_exists_for_provider(provider_id, current_tag):",
          "178:             current_version += 1",
          "179:             get_console().print(f\"[warning]The tag {current_tag} exists. Checking rc{current_version}.[/]\")",
          "180:         else:",
          "181:             return False, f\"rc{current_version}\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "42263f0bbb87c2833398ae70b05e864e22f042eb",
      "candidate_info": {
        "commit_hash": "42263f0bbb87c2833398ae70b05e864e22f042eb",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/42263f0bbb87c2833398ae70b05e864e22f042eb",
        "files": [
          ".github/workflows/ci.yml",
          ".pre-commit-config.yaml",
          ".rat-excludes",
          "BREEZE.rst",
          "STATIC_CODE_CHECKS.rst",
          "clients/README.md",
          "clients/gen/common.sh",
          "clients/gen/go.sh",
          "clients/gen/python.sh",
          "clients/python/.gitignore",
          "clients/python/.openapi-generator-ignore",
          "clients/python/CHANGELOG.md",
          "clients/python/INSTALL",
          "clients/python/LICENSE",
          "clients/python/NOTICE",
          "clients/python/README.md",
          "clients/python/pyproject.toml",
          "clients/python/test_python_client.py",
          "clients/python/version.txt",
          "dev/README_RELEASE_AIRFLOW.md",
          "dev/README_RELEASE_PYTHON_CLIENT.md",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "images/breeze/output_release-management.svg",
          "images/breeze/output_release-management.txt",
          "images/breeze/output_release-management_prepare-airflow-package.svg",
          "images/breeze/output_release-management_prepare-airflow-package.txt",
          "images/breeze/output_release-management_prepare-python-client.svg",
          "images/breeze/output_release-management_prepare-python-client.txt",
          "images/breeze/output_setup_check-all-params-in-groups.svg",
          "images/breeze/output_setup_check-all-params-in-groups.txt",
          "images/breeze/output_setup_regenerate-command-images.svg",
          "images/breeze/output_setup_regenerate-command-images.txt",
          "scripts/ci/docker-compose/local.yml",
          "scripts/ci/openapi/client_codegen_diff.sh"
        ],
        "message": "Generate Python client in reproducible way (#36763)\n\nClient source code and package generation was done using the code\ngenerated and committed to `airflow-client-python` and while the\nrepository with such code is useful to have, it's just a convenience\nrepo, because all sources are (and should be) generated from the\nAPI specification which is present in the Airflow repository.\n\nThis also made the reproducible builds and package generation not really\npossible, because we never knew if the source generated in the\n`airflow-client-python` repository has been generated and not tampered\nwith.\n\nWhile implementing it, it turned out that there were some issues in\nthe past that nade our client generation somewhat broken..\n\n* In 2.7.0 python client, we added the same code twice\n  (See https://github.com/apache/airflow-client-python/pull/93) on\n  top of \"airflow_client.client\" package, we also added copy of the\n  API client generated in \"airflow_client.airflow_client\" - that was\n  likely due to bad bash scripts and tools that were used to generate\n  it and errors during generation the clients.\n\n* We used to generate the code for \"client\" package and then moved\n  the \"client\" package to \"airflow_client.client\" package, while\n  manually modifying imports with `sed` (!?). That was likely due to\n  limitations in some old version of the client generator. However the\n  client generator we use now is capable of generating code directly in\n  the \"airflow_client.client\" package.\n\n* We also manually (via pre-commit) added Apache Licence to the\n  generated files. Whieh was completely unnecessary, because ASF rules\n  do not require licence headers to be added to code automatically\n  generated from a code that already has ASF licence.\n\n* We also generated source tarball packages from such generated code,\n  which was completely unnecessary - because sdist packages are already\n  fulfilling all the reqirements of such source pacakges - the code\n  in the packages is enough to build the package from the sources and\n  it does not contain any binary code, moreover the code is generated\n  out of the API specificiation, which means that anyone can take\n  the code and genearate the pacakged software from just sources in\n  sdist. Similarly as in case of provider packages, we do not need\n  to produce separate -source.tar.gz files.\n\nThis PR fixes all of it.\n\nFirst of all the source that lands in the source repository\n`airflow-client-python` and sdist/wheel packages are generated directly\nfrom the openapi specification.\n\nThey are generated using breeze release_management command from airflow\nsource  tagged with specific tag in the Airflow repo (including the\nsource of reproducible build date that is updated together with airflow\nrelease notes. This means that any PMC member can regenerate packages\n(binary identical) straight from the Airflow repository - without\ngoing through \"airflow-client-python\" repository.\n\nNo source tarball is generated - it is not needed, sdist is enough.\n\nThe `test_python_client.py` has been also moved over to Airflow repo\nand updated with handling case when expose_config is not enabled and\nit is used to automatically test the API client after it has been\ngenerated.\n\n(cherry picked from commit 9787440593196881b481466aa6d3cca4408f99e5)",
        "before_after_code_files": [
          "clients/gen/common.sh||clients/gen/common.sh",
          "clients/gen/go.sh||clients/gen/go.sh",
          "clients/gen/python.sh||clients/gen/python.sh",
          "clients/python/test_python_client.py||clients/python/test_python_client.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "scripts/ci/openapi/client_codegen_diff.sh||scripts/ci/openapi/client_codegen_diff.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "clients/gen/common.sh||clients/gen/common.sh": [
          "File: clients/gen/common.sh -> clients/gen/common.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "clients/gen/go.sh||clients/gen/go.sh": [
          "File: clients/gen/go.sh -> clients/gen/go.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "clients/gen/python.sh||clients/gen/python.sh": [
          "File: clients/gen/python.sh -> clients/gen/python.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "clients/python/test_python_client.py||clients/python/test_python_client.py": [
          "File: clients/python/test_python_client.py -> clients/python/test_python_client.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "17: #",
          "18: # PEP 723 compliant inline script metadata (not yet widely supported)",
          "19: # /// script",
          "20: # requires-python = \">=3.8\"",
          "21: # dependencies = [",
          "22: #   \"apache-airflow-client\",",
          "23: #   \"rich\",",
          "24: # ]",
          "25: # ///",
          "27: from __future__ import annotations",
          "29: import sys",
          "30: import uuid",
          "32: import airflow_client.client",
          "34: try:",
          "35:     # If you have rich installed, you will have nice colored output of the API responses",
          "36:     from rich import print",
          "37: except ImportError:",
          "38:     print(\"Output will not be colored. Please install rich to get colored output: `pip install rich`\")",
          "39:     pass",
          "40: from airflow_client.client.api import config_api, dag_api, dag_run_api",
          "41: from airflow_client.client.model.dag_run import DAGRun",
          "43: # The client must use the authentication and authorization parameters",
          "44: # in accordance with the API server security policy.",
          "45: # Examples for each auth method are provided below, use the example that",
          "46: # satisfies your auth use case.",
          "47: #",
          "48: # In case of the basic authentication below, make sure that Airflow is",
          "49: # configured also with the basic_auth as backend additionally to regular session backend needed",
          "50: # by the UI. In the `[api]` section of your `airflow.cfg` set:",
          "51: #",
          "52: # auth_backend = airflow.api.auth.backend.session,airflow.api.auth.backend.basic_auth",
          "53: #",
          "54: # Make sure that your user/name are configured properly - using the user/password that has admin",
          "55: # privileges in Airflow",
          "57: # Configure HTTP basic authorization: Basic",
          "58: configuration = airflow_client.client.Configuration(",
          "59:     host=\"http://localhost:8080/api/v1\", username=\"admin\", password=\"admin\"",
          "60: )",
          "62: # Make sure in the [core] section, the  `load_examples` config is set to True in your airflow.cfg",
          "63: # or AIRFLOW__CORE__LOAD_EXAMPLES environment variable set to True",
          "64: DAG_ID = \"example_bash_operator\"",
          "66: # Enter a context with an instance of the API client",
          "67: with airflow_client.client.ApiClient(configuration) as api_client:",
          "68:     errors = False",
          "70:     print(\"[blue]Getting DAG list\")",
          "71:     dag_api_instance = dag_api.DAGApi(api_client)",
          "72:     try:",
          "73:         api_response = dag_api_instance.get_dags()",
          "74:         print(api_response)",
          "75:     except airflow_client.client.OpenApiException as e:",
          "76:         print(f\"[red]Exception when calling DagAPI->get_dags: {e}\\n\")",
          "77:         errors = True",
          "78:     else:",
          "79:         print(\"[green]Getting DAG list successful\")",
          "81:     print(\"[blue]Getting Tasks for a DAG\")",
          "82:     try:",
          "83:         api_response = dag_api_instance.get_tasks(DAG_ID)",
          "84:         print(api_response)",
          "85:     except airflow_client.client.exceptions.OpenApiException as e:",
          "86:         print(f\"[red]Exception when calling DagAPI->get_tasks: {e}\\n\")",
          "87:         errors = True",
          "88:     else:",
          "89:         print(\"[green]Getting Tasks successful\")",
          "91:     print(\"[blue]Triggering a DAG run\")",
          "92:     dag_run_api_instance = dag_run_api.DAGRunApi(api_client)",
          "93:     try:",
          "94:         # Create a DAGRun object (no dag_id should be specified because it is read-only property of DAGRun)",
          "95:         # dag_run id is generated randomly to allow multiple executions of the script",
          "96:         dag_run = DAGRun(",
          "97:             dag_run_id=\"some_test_run_\" + uuid.uuid4().hex,",
          "98:         )",
          "99:         api_response = dag_run_api_instance.post_dag_run(DAG_ID, dag_run)",
          "100:         print(api_response)",
          "101:     except airflow_client.client.exceptions.OpenApiException as e:",
          "102:         print(f\"[red]Exception when calling DAGRunAPI->post_dag_run: {e}\\n\")",
          "103:         errors = True",
          "104:     else:",
          "105:         print(\"[green]Posting DAG Run successful\")",
          "107:     # Get current configuration. Note, this is disabled by default with most installation.",
          "108:     # You need to set `expose_config = True` in Airflow configuration in order to retrieve configuration.",
          "109:     conf_api_instance = config_api.ConfigApi(api_client)",
          "110:     try:",
          "111:         api_response = conf_api_instance.get_config()",
          "112:         print(api_response)",
          "113:     except airflow_client.client.OpenApiException as e:",
          "114:         if \"FORBIDDEN\" in str(e):",
          "115:             print(",
          "116:                 \"[yellow]You need to set `expose_config = True` in Airflow configuration\"",
          "117:                 \" in order to retrieve configuration.\"",
          "118:             )",
          "119:             print(\"[bright_blue]This is OK. Exposing config is disabled by default.\")",
          "120:         else:",
          "121:             print(f\"[red]Exception when calling DAGRunAPI->post_dag_run: {e}\\n\")",
          "122:             errors = True",
          "123:     else:",
          "124:         print(\"[green]Config retrieved successfully\")",
          "126:     if errors:",
          "127:         print(\"\\n[red]There were errors while running the script - see above for details\")",
          "128:         sys.exit(1)",
          "129:     else:",
          "130:         print(\"\\n[green]Everything went well\")",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "191:     show_default=True,",
          "192:     envvar=\"PACKAGE_FORMAT\",",
          "193: )",
          "195: if TYPE_CHECKING:",
          "196:     from packaging.version import Version",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "194: option_use_local_hatch = click.option(",
          "195:     \"--use-local-hatch\",",
          "196:     is_flag=True,",
          "197:     help=\"Use local hatch instead of docker to build the package. You need to have hatch installed.\",",
          "198: )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "279: AIRFLOW_BUILD_DOCKERFILE_IGNORE_PATH = AIRFLOW_SOURCES_ROOT / \"airflow-build-dockerfile.dockerignore\"",
          "322:     # This is security feature.",
          "323:     #",
          "324:     # Building the image needed to build airflow package including .git directory",
          "",
          "[Removed Lines]",
          "282: @release_management.command(",
          "283:     name=\"prepare-airflow-package\",",
          "284:     help=\"Prepare sdist/whl package of Airflow.\",",
          "285: )",
          "286: @click.option(",
          "287:     \"--use-local-hatch\",",
          "288:     is_flag=True,",
          "289:     help=\"Use local hatch instead of docker to build the package. You need to have hatch installed.\",",
          "290: )",
          "291: @option_package_format",
          "292: @option_version_suffix_for_pypi",
          "293: @option_verbose",
          "294: @option_dry_run",
          "295: def prepare_airflow_packages(",
          "296:     package_format: str,",
          "297:     version_suffix_for_pypi: str,",
          "298:     use_local_hatch: bool,",
          "299: ):",
          "300:     perform_environment_checks()",
          "301:     fix_ownership_using_docker()",
          "302:     cleanup_python_generated_files()",
          "303:     source_date_epoch = get_source_date_epoch()",
          "304:     if use_local_hatch:",
          "305:         hatch_build_command = [\"hatch\", \"build\", \"-c\", \"-t\", \"custom\"]",
          "306:         if package_format in [\"sdist\", \"both\"]:",
          "307:             hatch_build_command.extend([\"-t\", \"sdist\"])",
          "308:         if package_format in [\"wheel\", \"both\"]:",
          "309:             hatch_build_command.extend([\"-t\", \"wheel\"])",
          "310:         env_copy = os.environ.copy()",
          "311:         env_copy[\"SOURCE_DATE_EPOCH\"] = str(source_date_epoch)",
          "312:         run_command(",
          "313:             hatch_build_command,",
          "314:             check=True,",
          "315:             env=env_copy,",
          "316:         )",
          "317:         get_console().print(\"[success]Successfully prepared Airflow packages:\")",
          "318:         for file in sorted(DIST_DIR.glob(\"apache_airflow*\")):",
          "319:             get_console().print(file.name)",
          "320:         get_console().print()",
          "321:         return",
          "",
          "[Added Lines]",
          "287: def _build_local_build_image():",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "344:         cwd=AIRFLOW_SOURCES_ROOT,",
          "345:         env={\"DOCKER_CLI_HINTS\": \"false\"},",
          "346:     )",
          "347:     container_id = f\"airflow-build-{random.getrandbits(64):08x}\"",
          "348:     result = run_command(",
          "349:         cmd=[",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "315: def _build_airflow_packages_with_docker(",
          "316:     package_format: str, source_date_epoch: int, version_suffix_for_pypi: str",
          "317: ):",
          "318:     _build_local_build_image()",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "355:             \"-e\",",
          "356:             f\"VERSION_SUFFIX_FOR_PYPI={version_suffix_for_pypi}\",",
          "357:             \"-e\",",
          "358:             \"HOME=/opt/airflow/files/home\",",
          "359:             \"-e\",",
          "360:             \"GITHUB_ACTIONS\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "330:             f\"SOURCE_DATE_EPOCH={source_date_epoch}\",",
          "331:             \"-e\",",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "375:     DIST_DIR.mkdir(parents=True, exist_ok=True)",
          "376:     # Copy all files in the dist directory in container to the host dist directory (note '/.' in SRC)",
          "377:     run_command([\"docker\", \"cp\", f\"{container_id}:/opt/airflow/dist/.\", \"./dist\"], check=True)",
          "379:     get_console().print(\"[success]Successfully prepared Airflow packages:\")",
          "380:     for file in sorted(DIST_DIR.glob(\"apache_airflow*\")):",
          "381:         get_console().print(file.name)",
          "",
          "[Removed Lines]",
          "378:     run_command([\"docker\", \"rm\", \"--force\", container_id], check=True)",
          "",
          "[Added Lines]",
          "352:     run_command([\"docker\", \"rm\", \"--force\", container_id], check=False, stderr=DEVNULL, stdout=DEVNULL)",
          "355: def _build_airflow_packages_with_hatch(",
          "356:     package_format: str, source_date_epoch: int, version_suffix_for_pypi: str",
          "357: ):",
          "358:     hatch_build_command = [\"hatch\", \"build\", \"-c\", \"-t\", \"custom\"]",
          "359:     if package_format in [\"sdist\", \"both\"]:",
          "360:         hatch_build_command.extend([\"-t\", \"sdist\"])",
          "361:     if package_format in [\"wheel\", \"both\"]:",
          "362:         hatch_build_command.extend([\"-t\", \"wheel\"])",
          "363:     env_copy = os.environ.copy()",
          "364:     env_copy[\"SOURCE_DATE_EPOCH\"] = str(source_date_epoch)",
          "365:     env_copy[\"VERSION_SUFFIX_FOR_PYPI\"] = version_suffix_for_pypi",
          "366:     run_command(",
          "367:         hatch_build_command,",
          "368:         check=True,",
          "369:         env=env_copy,",
          "370:     )",
          "373: @release_management.command(",
          "374:     name=\"prepare-airflow-package\",",
          "375:     help=\"Prepare sdist/whl package of Airflow.\",",
          "376: )",
          "377: @option_package_format",
          "378: @option_version_suffix_for_pypi",
          "379: @option_use_local_hatch",
          "380: @option_verbose",
          "381: @option_dry_run",
          "382: def prepare_airflow_packages(",
          "383:     package_format: str,",
          "384:     version_suffix_for_pypi: str,",
          "385:     use_local_hatch: bool,",
          "386: ):",
          "387:     perform_environment_checks()",
          "388:     fix_ownership_using_docker()",
          "389:     cleanup_python_generated_files()",
          "390:     source_date_epoch = get_source_date_epoch()",
          "391:     if use_local_hatch:",
          "392:         _build_airflow_packages_with_hatch(",
          "393:             package_format=package_format,",
          "394:             source_date_epoch=source_date_epoch,",
          "395:             version_suffix_for_pypi=version_suffix_for_pypi,",
          "396:         )",
          "397:     else:",
          "398:         _build_airflow_packages_with_docker(",
          "399:             package_format=package_format,",
          "400:             source_date_epoch=source_date_epoch,",
          "401:             version_suffix_for_pypi=version_suffix_for_pypi,",
          "402:         )",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "2078:         type=no_version_file + \"-\" + suffix,",
          "2079:         comparable_version=Version(version),",
          "2080:     )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2107: PYTHON_CLIENT_DIR_PATH = AIRFLOW_SOURCES_ROOT / \"clients\" / \"python\"",
          "2108: PYTHON_CLIENT_DIST_DIR_PATH = PYTHON_CLIENT_DIR_PATH / \"dist\"",
          "2109: PYTHON_CLIENT_TMP_DIR = PYTHON_CLIENT_DIR_PATH / \"tmp\"",
          "2111: REPRODUCIBLE_BUILD_YAML = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"reproducible_build.yaml\"",
          "2113: VERSION_FILE = PYTHON_CLIENT_DIR_PATH / \"version.txt\"",
          "2114: SOURCE_API_YAML_PATH = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"api_connexion\" / \"openapi\" / \"v1.yaml\"",
          "2115: TARGET_API_YAML_PATH = PYTHON_CLIENT_DIR_PATH / \"v1.yaml\"",
          "2116: OPENAPI_GENERATOR_CLI_VER = \"5.4.0\"",
          "2118: GENERATED_CLIENT_DIRECTORIES_TO_COPY = [\"airflow_client\", \"docs\", \"test\"]",
          "2119: FILES_TO_COPY_TO_CLIENT_REPO = [",
          "2120:     \".gitignore\",",
          "2121:     \".openapi-generator-ignore\",",
          "2122:     \"CHANGELOG.md\",",
          "2123:     \"README.md\",",
          "2124:     \"INSTALL\",",
          "2125:     \"LICENSE\",",
          "2126:     \"NOTICE\",",
          "2127:     \"pyproject.toml\",",
          "2128:     \"test_python_client.py\",",
          "2129:     \"version.txt\",",
          "2130: ]",
          "2133: def _get_python_client_version(version_suffix_for_pypi):",
          "2134:     from packaging.version import Version",
          "2136:     python_client_version = VERSION_FILE.read_text().strip()",
          "2137:     version = Version(python_client_version)",
          "2138:     if version_suffix_for_pypi:",
          "2139:         if version.pre:",
          "2140:             currrent_suffix = version.pre[0] + str(version.pre[1])",
          "2141:             if currrent_suffix != version_suffix_for_pypi:",
          "2142:                 get_console().print(",
          "2143:                     f\"[error]The version suffix for PyPI ({version_suffix_for_pypi}) does not match the \"",
          "2144:                     f\"suffix in the version ({version})[/]\"",
          "2145:                 )",
          "2146:                 sys.exit(1)",
          "2147:     return version.base_version + version_suffix_for_pypi",
          "2150: def _generate_python_client_sources(python_client_version: str) -> None:",
          "2151:     get_console().print(f\"\\n[info]Generating client code in {PYTHON_CLIENT_TMP_DIR}[/]\")",
          "2152:     result = run_command(",
          "2153:         [",
          "2154:             \"docker\",",
          "2155:             \"run\",",
          "2156:             \"--rm\",",
          "2157:             \"-u\",",
          "2158:             f\"{os.getuid()}:{os.getgid()}\",",
          "2159:             \"-v\",",
          "2160:             f\"{TARGET_API_YAML_PATH}:/spec.yaml\",",
          "2161:             \"-v\",",
          "2162:             f\"{PYTHON_CLIENT_TMP_DIR}:/output\",",
          "2163:             f\"openapitools/openapi-generator-cli:v{OPENAPI_GENERATOR_CLI_VER}\",",
          "2164:             \"generate\",",
          "2165:             \"--input-spec\",",
          "2166:             \"/spec.yaml\",",
          "2167:             \"--generator-name\",",
          "2168:             \"python\",",
          "2169:             \"--git-user-id\",",
          "2170:             f\"{os.environ.get('GIT_USER')}\",",
          "2171:             \"--output\",",
          "2172:             \"/output\",",
          "2173:             \"--package-name\",",
          "2174:             \"airflow_client.client\",",
          "2175:             \"--git-repo-id\",",
          "2176:             \"airflow-client-python\",",
          "2177:             \"--additional-properties\",",
          "2178:             f'packageVersion=\"{python_client_version}\"',",
          "2179:         ],",
          "2180:         capture_output=True,",
          "2181:         text=True,",
          "2182:     )",
          "2183:     if result.returncode != 0:",
          "2184:         get_console().print(\"[error]Failed to generate client code[/]\")",
          "2185:         get_console().print(result.stdout, markup=False)",
          "2186:         get_console().print(result.stderr, markup=False, style=\"error\")",
          "2187:         sys.exit(result.returncode)",
          "2188:     get_console().print(f\"[success]Generated client code in {PYTHON_CLIENT_TMP_DIR}:[/]\")",
          "2189:     get_console().print(f\"\\n[info]Content of {PYTHON_CLIENT_TMP_DIR}:[/]\")",
          "2190:     for file in sorted(PYTHON_CLIENT_TMP_DIR.glob(\"*\")):",
          "2191:         get_console().print(f\"[info]  {file.name}[/]\")",
          "2192:     get_console().print()",
          "2195: def _copy_selected_sources_from_tmp_directory_to_clients_python():",
          "2196:     get_console().print(",
          "2197:         f\"[info]Copying selected sources: {GENERATED_CLIENT_DIRECTORIES_TO_COPY} from \"",
          "2198:         f\"{PYTHON_CLIENT_TMP_DIR} to {PYTHON_CLIENT_DIR_PATH}[/]\"",
          "2199:     )",
          "2200:     for dir in GENERATED_CLIENT_DIRECTORIES_TO_COPY:",
          "2201:         source_dir = PYTHON_CLIENT_TMP_DIR / dir",
          "2202:         target_dir = PYTHON_CLIENT_DIR_PATH / dir",
          "2203:         get_console().print(f\"[info]  Copying generated sources from {source_dir} to {target_dir}[/]\")",
          "2204:         shutil.rmtree(target_dir, ignore_errors=True)",
          "2205:         shutil.copytree(source_dir, target_dir)",
          "2206:         get_console().print(f\"[success]  Copied generated sources from {source_dir} to {target_dir}[/]\")",
          "2207:     get_console().print(",
          "2208:         f\"[info]Copied selected sources {GENERATED_CLIENT_DIRECTORIES_TO_COPY} from \"",
          "2209:         f\"{PYTHON_CLIENT_TMP_DIR} to {PYTHON_CLIENT_DIR_PATH}[/]\\n\"",
          "2210:     )",
          "2211:     get_console().print(f\"\\n[info]Content of {PYTHON_CLIENT_DIR_PATH}:[/]\")",
          "2212:     for file in sorted(PYTHON_CLIENT_DIR_PATH.glob(\"*\")):",
          "2213:         get_console().print(f\"[info]  {file.name}[/]\")",
          "2214:     get_console().print()",
          "2217: def _build_client_packages_with_hatch(source_date_epoch: int, package_format: str):",
          "2218:     command = [",
          "2219:         \"hatch\",",
          "2220:         \"build\",",
          "2221:         \"-c\",",
          "2222:     ]",
          "2223:     if package_format == \"sdist\" or package_format == \"both\":",
          "2224:         command += [\"-t\", \"sdist\"]",
          "2225:     if package_format == \"wheel\" or package_format == \"both\":",
          "2226:         command += [\"-t\", \"wheel\"]",
          "2227:     env_copy = os.environ.copy()",
          "2228:     env_copy[\"SOURCE_DATE_EPOCH\"] = str(source_date_epoch)",
          "2229:     run_command(",
          "2230:         cmd=command,",
          "2231:         cwd=PYTHON_CLIENT_DIR_PATH,",
          "2232:         env=env_copy,",
          "2233:         check=True,",
          "2234:     )",
          "2235:     shutil.copytree(PYTHON_CLIENT_DIST_DIR_PATH, DIST_DIR, dirs_exist_ok=True)",
          "2238: def _build_client_packages_with_docker(source_date_epoch: int, package_format: str):",
          "2239:     _build_local_build_image()",
          "2240:     command = \"hatch build -c \"",
          "2241:     if package_format == \"sdist\" or package_format == \"both\":",
          "2242:         command += \"-t sdist \"",
          "2243:     if package_format == \"wheel\" or package_format == \"both\":",
          "2244:         command += \"-t wheel \"",
          "2245:     container_id = f\"airflow-build-{random.getrandbits(64):08x}\"",
          "2246:     result = run_command(",
          "2247:         cmd=[",
          "2248:             \"docker\",",
          "2249:             \"run\",",
          "2250:             \"--name\",",
          "2251:             container_id,",
          "2252:             \"-t\",",
          "2253:             \"-e\",",
          "2254:             f\"SOURCE_DATE_EPOCH={source_date_epoch}\",",
          "2255:             \"-e\",",
          "2256:             \"HOME=/opt/airflow/files/home\",",
          "2257:             \"-e\",",
          "2258:             \"GITHUB_ACTIONS\",",
          "2259:             \"-w\",",
          "2260:             \"/opt/airflow/clients/python\",",
          "2261:             AIRFLOW_BUILD_IMAGE_TAG,",
          "2262:             \"bash\",",
          "2263:             \"-c\",",
          "2264:             command,",
          "2265:         ],",
          "2266:         check=False,",
          "2267:     )",
          "2268:     if result.returncode != 0:",
          "2269:         get_console().print(\"[error]Error preparing Python client packages[/]\")",
          "2270:         fix_ownership_using_docker()",
          "2271:         sys.exit(result.returncode)",
          "2272:     DIST_DIR.mkdir(parents=True, exist_ok=True)",
          "2273:     get_console().print()",
          "2274:     # Copy all files in the dist directory in container to the host dist directory (note '/.' in SRC)",
          "2275:     run_command([\"docker\", \"cp\", f\"{container_id}:/opt/airflow/clients/python/dist/.\", \"./dist\"], check=True)",
          "2276:     run_command([\"docker\", \"rm\", \"--force\", container_id], check=False, stdout=DEVNULL, stderr=DEVNULL)",
          "2279: @release_management.command(name=\"prepare-python-client\", help=\"Prepares python client packages.\")",
          "2280: @option_package_format",
          "2281: @option_version_suffix_for_pypi",
          "2282: @option_use_local_hatch",
          "2283: @click.option(",
          "2284:     \"--python-client-repo\",",
          "2285:     envvar=\"PYTHON_CLIENT_REPO\",",
          "2286:     type=click.Path(file_okay=False, dir_okay=True, path_type=Path, exists=True),",
          "2287:     help=\"Directory where the python client repo is checked out\",",
          "2288: )",
          "2289: @click.option(",
          "2290:     \"--only-publish-build-scripts\",",
          "2291:     envvar=\"ONLY_PUBLISH_BUILD_SCRIPTS\",",
          "2292:     is_flag=True,",
          "2293:     help=\"Only publish updated build scripts to puthon client repo, not generated client code.\",",
          "2294: )",
          "2295: @click.option(",
          "2296:     \"--security-schemes\",",
          "2297:     default=\"Basic,GoogleOpenID,Kerberos\",",
          "2298:     envvar=\"SECURITY_SCHEMES\",",
          "2299:     show_default=True,",
          "2300:     help=\"Security schemes to be added to the API documentation (coma separated)\",",
          "2301: )",
          "2302: @option_dry_run",
          "2303: @option_verbose",
          "2304: def prepare_python_client(",
          "2305:     package_format: str,",
          "2306:     version_suffix_for_pypi: str,",
          "2307:     use_local_hatch: bool,",
          "2308:     python_client_repo: Path | None,",
          "2309:     only_publish_build_scripts: bool,",
          "2310:     security_schemes: str,",
          "2311: ):",
          "2312:     shutil.rmtree(PYTHON_CLIENT_TMP_DIR, ignore_errors=True)",
          "2313:     PYTHON_CLIENT_TMP_DIR.mkdir(parents=True, exist_ok=True)",
          "2314:     shutil.copy(src=SOURCE_API_YAML_PATH, dst=TARGET_API_YAML_PATH)",
          "2315:     import yaml",
          "2317:     openapi_yaml = yaml.safe_load(TARGET_API_YAML_PATH.read_text())",
          "2319:     # Add security schemes to documentation",
          "2320:     security: list[dict[str, Any]] = []",
          "2321:     for scheme in security_schemes.split(\",\"):",
          "2322:         security.append({scheme: []})",
          "2323:     openapi_yaml[\"security\"] = security",
          "2324:     python_client_version = _get_python_client_version(version_suffix_for_pypi)",
          "2325:     TARGET_API_YAML_PATH.write_text(yaml.dump(openapi_yaml))",
          "2327:     _generate_python_client_sources(python_client_version=python_client_version)",
          "2328:     _copy_selected_sources_from_tmp_directory_to_clients_python()",
          "2330:     reproducible_build_yaml = yaml.safe_load(REPRODUCIBLE_BUILD_YAML.read_text())",
          "2331:     source_date_epoch = reproducible_build_yaml[\"source-date-epoch\"]",
          "2333:     if python_client_repo:",
          "2334:         if not only_publish_build_scripts:",
          "2335:             get_console().print(",
          "2336:                 f\"[info]Copying generated client from {PYTHON_CLIENT_DIR_PATH} to {python_client_repo}[/]\"",
          "2337:             )",
          "2338:             for dir in GENERATED_CLIENT_DIRECTORIES_TO_COPY:",
          "2339:                 source_dir = PYTHON_CLIENT_DIR_PATH / dir",
          "2340:                 target_dir = python_client_repo / dir",
          "2341:                 get_console().print(f\"[info]  Copying {source_dir} to {target_dir}[/]\")",
          "2342:                 shutil.rmtree(target_dir, ignore_errors=True)",
          "2343:                 shutil.copytree(source_dir, target_dir)",
          "2344:                 get_console().print(f\"[success]  Copied {source_dir} to {target_dir}[/]\")",
          "2345:             get_console().print(",
          "2346:                 f\"[info]Copied generated client from {PYTHON_CLIENT_DIR_PATH} to {python_client_repo}[/]\"",
          "2347:             )",
          "2348:         get_console().print(",
          "2349:             f\"[info]Copying build scripts from {PYTHON_CLIENT_DIR_PATH} to {python_client_repo}[/]\"",
          "2350:         )",
          "2351:         for file in FILES_TO_COPY_TO_CLIENT_REPO:",
          "2352:             source_file = PYTHON_CLIENT_DIR_PATH / file",
          "2353:             target_file = python_client_repo / file",
          "2354:             get_console().print(f\"[info]  Copying {source_file} to {target_file}[/]\")",
          "2355:             shutil.copy(source_file, target_file)",
          "2356:             get_console().print(f\"[success]  Copied {source_file} to {target_file}[/]\")",
          "2357:         get_console().print(",
          "2358:             f\"[success]Copied build scripts from {PYTHON_CLIENT_DIR_PATH} to {python_client_repo}[/]\"",
          "2359:         )",
          "2360:         spec_dir = python_client_repo / \"spec\"",
          "2361:         spec_dir.mkdir(parents=True, exist_ok=True)",
          "2362:         source_spec_file = PYTHON_CLIENT_DIR_PATH / \"v1.yaml\"",
          "2363:         target_spec_file = spec_dir / \"v1.yaml\"",
          "2364:         get_console().print(f\"[info]  Copying {source_spec_file} to {target_spec_file}[/]\")",
          "2365:         shutil.copy(source_spec_file, target_spec_file)",
          "2366:         get_console().print(f\"[success]  Copied {source_spec_file} to {target_spec_file}[/]\")",
          "2367:         get_console().print(",
          "2368:             f\"[success]Copied client code from {PYTHON_CLIENT_DIR_PATH} to {python_client_repo}[/]\\n\"",
          "2369:         )",
          "2370:     else:",
          "2371:         get_console().print(",
          "2372:             \"\\n[warning]No python client repo directory provided - skipping copying the generated client[/]\\n\"",
          "2373:         )",
          "2374:     get_console().print(f\"\\n[info]Building packages in {PYTHON_CLIENT_DIST_DIR_PATH}[/]\\n\")",
          "2375:     shutil.rmtree(PYTHON_CLIENT_DIST_DIR_PATH, ignore_errors=True)",
          "2376:     PYTHON_CLIENT_DIST_DIR_PATH.mkdir(parents=True, exist_ok=True)",
          "2377:     version = _get_python_client_version(version_suffix_for_pypi)",
          "2378:     original_version = VERSION_FILE.read_text().strip()",
          "2379:     if version_suffix_for_pypi:",
          "2380:         VERSION_FILE.write_text(version)",
          "2381:     try:",
          "2382:         if use_local_hatch:",
          "2383:             _build_client_packages_with_hatch(",
          "2384:                 source_date_epoch=source_date_epoch, package_format=package_format",
          "2385:             )",
          "2386:         else:",
          "2387:             _build_client_packages_with_docker(",
          "2388:                 source_date_epoch=source_date_epoch, package_format=package_format",
          "2389:             )",
          "2390:         get_console().print(f\"\\n[success]Built packages in {DIST_DIR}[/]\\n\")",
          "2391:     finally:",
          "2392:         if version_suffix_for_pypi:",
          "2393:             VERSION_FILE.write_text(original_version)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "45:     \"name\": \"Other release commands\",",
          "46:     \"commands\": [",
          "47:         \"add-back-references\",",
          "48:         \"publish-docs\",",
          "49:         \"generate-constraints\",",
          "50:         \"update-constraints\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "48:         \"prepare-python-client\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "57:             \"name\": \"Package flags\",",
          "58:             \"options\": [",
          "59:                 \"--package-format\",",
          "61:                 \"--version-suffix-for-pypi\",",
          "62:             ],",
          "63:         }",
          "64:     ],",
          "",
          "[Removed Lines]",
          "60:                 \"--use-local-hatch\",",
          "",
          "[Added Lines]",
          "62:                 \"--use-local-hatch\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "167:             ],",
          "168:         }",
          "169:     ],",
          "170:     \"breeze release-management generate-constraints\": [",
          "171:         {",
          "172:             \"name\": \"Generate constraints flags\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "171:     \"breeze release-management prepare-python-client\": [",
          "172:         {",
          "173:             \"name\": \"Python client preparation flags\",",
          "174:             \"options\": [",
          "175:                 \"--package-format\",",
          "176:                 \"--version-suffix-for-pypi\",",
          "177:                 \"--use-local-hatch\",",
          "178:                 \"--python-client-repo\",",
          "179:                 \"--only-publish-build-scripts\",",
          "180:                 \"--security-schemes\",",
          "181:             ],",
          "182:         }",
          "183:     ],",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "84:     (\"RELEASE_NOTES.rst\", \"/opt/airflow/RELEASE_NOTES.rst\"),",
          "85:     (\"airflow\", \"/opt/airflow/airflow\"),",
          "86:     (\"constraints\", \"/opt/airflow/constraints\"),",
          "87:     (\"dags\", \"/opt/airflow/dags\"),",
          "88:     (\"dev\", \"/opt/airflow/dev\"),",
          "89:     (\"docs\", \"/opt/airflow/docs\"),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "87:     (\"clients\", \"/opt/airflow/clients\"),",
          "",
          "---------------"
        ],
        "scripts/ci/openapi/client_codegen_diff.sh||scripts/ci/openapi/client_codegen_diff.sh": [
          "File: scripts/ci/openapi/client_codegen_diff.sh -> scripts/ci/openapi/client_codegen_diff.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "dd6572fb0350d0f8f5473a2d30163a550b7dff6a",
      "candidate_info": {
        "commit_hash": "dd6572fb0350d0f8f5473a2d30163a550b7dff6a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/dd6572fb0350d0f8f5473a2d30163a550b7dff6a",
        "files": [
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/src/airflow_breeze/utils/path_utils.py"
        ],
        "message": "Check executable permission for entrypoints at breeze start (#36482)\n\n* Check executable permission for entrypoints at breeze start\n\nSometimes our contributors check out Airflow repository on filesystems\nthat are not POSIX compliant and do not have support for executable bits\n(for example when you check-out the repository in Windows and attempt to\nmap it to a Linux VM). Breeze and building CI images will not\nwork in this case, but the error that you see might be misleading.\n\nThis PR performs additional environment check and informs you that\nyou should not do it, if executable bits are missing from entrypoints.\n\n* Update dev/breeze/src/airflow_breeze/utils/docker_command_utils.py\n\nCo-authored-by: Jens Scheffler <95105677+jscheffl@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Jens Scheffler <95105677+jscheffl@users.noreply.github.com>\n(cherry picked from commit 5551e14f6c0d8706bf8beaaab6a8fa5c80719a89)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/src/airflow_breeze/utils/path_utils.py||dev/breeze/src/airflow_breeze/utils/path_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: from airflow_breeze.utils.host_info_utils import get_host_group_id, get_host_os, get_host_user_id",
          "31: from airflow_breeze.utils.path_utils import (",
          "32:     AIRFLOW_SOURCES_ROOT,",
          "33:     cleanup_python_generated_files,",
          "34:     create_mypy_volume_if_needed,",
          "35: )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33:     SCRIPTS_DOCKER_DIR,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "481:         env_variables[\"AIRFLOW__CELERY__BROKER_URL\"] = url_map[params.celery_broker]",
          "484: def perform_environment_checks(quiet: bool = False):",
          "485:     check_docker_is_running()",
          "486:     check_docker_version(quiet)",
          "487:     check_docker_compose_version(quiet)",
          "490: def get_docker_syntax_version() -> str:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "485: def check_executable_entrypoint_permissions(quiet: bool = False):",
          "486:     \"\"\"",
          "487:     Checks if the user has executable permissions on the entrypoints in checked-out airflow repository..",
          "488:     \"\"\"",
          "489:     for entrypoint in SCRIPTS_DOCKER_DIR.glob(\"entrypoint*.sh\"):",
          "490:         if get_verbose() and not quiet:",
          "491:             get_console().print(f\"[info]Checking executable permissions on {entrypoint.as_posix()}[/]\")",
          "492:         if not os.access(entrypoint.as_posix(), os.X_OK):",
          "493:             get_console().print(",
          "494:                 f\"[error]You do not have executable permissions on {entrypoint}[/]\\n\"",
          "495:                 f\"You likely checked out airflow repo on a filesystem that does not support executable \"",
          "496:                 f\"permissions (for example on a Windows filesystem that is mapped to Linux VM). Airflow \"",
          "497:                 f\"repository should only be checked out on a filesystem that is POSIX compliant.\"",
          "498:             )",
          "499:             sys.exit(1)",
          "500:     if not quiet:",
          "501:         get_console().print(\"[success]Executable permissions on entrypoints are OK[/]\")",
          "508:     check_executable_entrypoint_permissions(quiet)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/path_utils.py||dev/breeze/src/airflow_breeze/utils/path_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/path_utils.py -> dev/breeze/src/airflow_breeze/utils/path_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "291: GENERATED_PROVIDER_PACKAGES_DIR = DIST_DIR / \"provider_packages\"",
          "292: DOCS_DIR = AIRFLOW_SOURCES_ROOT / \"docs\"",
          "293: SCRIPTS_CI_DIR = AIRFLOW_SOURCES_ROOT / \"scripts\" / \"ci\"",
          "294: SCRIPTS_CI_DOCKER_COMPOSE_DIR = SCRIPTS_CI_DIR / \"docker-compose\"",
          "295: SCRIPTS_CI_DOCKER_COMPOSE_LOCAL_YAML_FILE = SCRIPTS_CI_DOCKER_COMPOSE_DIR / \"local.yml\"",
          "296: GENERATED_DOCKER_COMPOSE_ENV_FILE = SCRIPTS_CI_DOCKER_COMPOSE_DIR / \"_generated_docker_compose.env\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "294: SCRIPTS_DOCKER_DIR = AIRFLOW_SOURCES_ROOT / \"scripts\" / \"docker\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1a92e92f4d8f339ee2e0d1c34e773e66fbbbff77",
      "candidate_info": {
        "commit_hash": "1a92e92f4d8f339ee2e0d1c34e773e66fbbbff77",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/1a92e92f4d8f339ee2e0d1c34e773e66fbbbff77",
        "files": [
          "docs/apache-airflow/templates-ref.rst"
        ],
        "message": "In docs use logical_date instead of deprecated execution_date (#36654)\n\n(cherry picked from commit a3ebc4d5bb95890df72f39f9ad97bcfa81496c42)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "9361ece8e0eeb306031ad15410637cd3e350828f",
      "candidate_info": {
        "commit_hash": "9361ece8e0eeb306031ad15410637cd3e350828f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/9361ece8e0eeb306031ad15410637cd3e350828f",
        "files": [
          "dev/README_RELEASE_PROVIDER_PACKAGES.md"
        ],
        "message": "Clarify how to find changelog for the RC candidates of providers (#36512)\n\n* Clarify how to find changelog for the RC candidates of providers\n\nWe used to have changelog in wheel package description but since\nthose changelogs are immutable in the released packages, we removed\nthe changelog from description some time ago and instead we publish\nlink to the changelog as \"project URL\". However the changelog\nlinks are not available until we publish the packages, so there is\nno easy way to see the changelog - and the \"Status of providers\"\nis the easiest (and pretty convenient) way of looking at what\nchanges are coming.\n\nThis PR removes mentioning that \"changelogs are available\" in the\nPyPI packages and explains that you should look for included PRs in\nthe \"Status issue\" until the providers are released.\n\n* Update dev/README_RELEASE_PROVIDER_PACKAGES.md\n\n(cherry picked from commit fd0facc9f0f8fc0bc630d84b82048e91407dabdc)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
      "candidate_info": {
        "commit_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
        "files": [
          "airflow/models/xcom.py",
          "tests/api_connexion/schemas/test_xcom_schema.py",
          "tests/models/test_xcom.py"
        ],
        "message": "Stop deserializing pickle when enable_xcom_pickling is False (#36255)\n\n* Stop deserializing pickle when enable_xcom_pickling is False\n\n* Fix unit tests\n\n(cherry picked from commit 63e97abec5d56bc62a293c93f5227f364561e51c)",
        "before_after_code_files": [
          "airflow/models/xcom.py||airflow/models/xcom.py",
          "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py",
          "tests/models/test_xcom.py||tests/models/test_xcom.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/xcom.py||airflow/models/xcom.py": [
          "File: airflow/models/xcom.py -> airflow/models/xcom.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "685:             except pickle.UnpicklingError:",
          "686:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
          "687:         else:",
          "693:     @staticmethod",
          "694:     def deserialize_value(result: XCom) -> Any:",
          "",
          "[Removed Lines]",
          "688:             try:",
          "689:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
          "690:             except (json.JSONDecodeError, UnicodeDecodeError):",
          "691:                 return pickle.loads(result.value)",
          "",
          "[Added Lines]",
          "688:             # Since xcom_pickling is disabled, we should only try to deserialize with JSON",
          "689:             return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
          "",
          "---------------"
        ],
        "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py": [
          "File: tests/api_connexion/schemas/test_xcom_schema.py -> tests/api_connexion/schemas/test_xcom_schema.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: from airflow.models import DagRun, XCom",
          "31: from airflow.utils.dates import parse_execution_date",
          "32: from airflow.utils.session import create_session",
          "34: pytestmark = pytest.mark.db_test",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33: from tests.test_utils.config import conf_vars",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "188:     default_time = \"2016-04-02T21:00:00+00:00\"",
          "189:     default_time_parsed = parse_execution_date(default_time)",
          "191:     def test_serialize(self, create_xcom, session):",
          "192:         create_xcom(",
          "193:             dag_id=\"test_dag\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "192:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "208:             \"map_index\": -1,",
          "209:         }",
          "211:     def test_deserialize(self):",
          "212:         xcom_dump = {",
          "213:             \"key\": \"test_key\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "213:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
          "",
          "---------------"
        ],
        "tests/models/test_xcom.py||tests/models/test_xcom.py": [
          "File: tests/models/test_xcom.py -> tests/models/test_xcom.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "140:             ret_value = XCom.get_value(key=\"xcom_test3\", ti_key=ti_key, session=session)",
          "141:         assert ret_value == {\"key\": \"value\"}",
          "144:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"}):",
          "145:             XCom.set(",
          "146:                 key=\"xcom_test3\",",
          "",
          "[Removed Lines]",
          "143:     def test_xcom_deserialize_with_pickle_to_json_switch(self, task_instance, session):",
          "",
          "[Added Lines]",
          "143:     def test_xcom_deserialize_pickle_when_xcom_pickling_is_disabled(self, task_instance, session):",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "151:                 session=session,",
          "152:             )",
          "153:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"False\"}):",
          "163:     @conf_vars({(\"core\", \"xcom_enable_pickling\"): \"False\"})",
          "164:     def test_xcom_disable_pickle_type_fail_on_non_json(self, task_instance, session):",
          "",
          "[Removed Lines]",
          "154:             ret_value = XCom.get_one(",
          "155:                 key=\"xcom_test3\",",
          "156:                 dag_id=task_instance.dag_id,",
          "157:                 task_id=task_instance.task_id,",
          "158:                 run_id=task_instance.run_id,",
          "159:                 session=session,",
          "160:             )",
          "161:         assert ret_value == {\"key\": \"value\"}",
          "",
          "[Added Lines]",
          "154:             with pytest.raises(UnicodeDecodeError):",
          "155:                 XCom.get_one(",
          "156:                     key=\"xcom_test3\",",
          "157:                     dag_id=task_instance.dag_id,",
          "158:                     task_id=task_instance.task_id,",
          "159:                     run_id=task_instance.run_id,",
          "160:                     session=session,",
          "161:                 )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7b5395bb64db9d131bf8e809adc332eabd9763c3",
      "candidate_info": {
        "commit_hash": "7b5395bb64db9d131bf8e809adc332eabd9763c3",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7b5395bb64db9d131bf8e809adc332eabd9763c3",
        "files": [
          "airflow/www/auth.py",
          "tests/www/test_auth.py"
        ],
        "message": "Redirect to index when user does not have permission to access a page (#36623)\n\n(cherry picked from commit 535c8be599f5e1a9455b6e6ab1840aa446ce3b1e)",
        "before_after_code_files": [
          "airflow/www/auth.py||airflow/www/auth.py",
          "tests/www/test_auth.py||tests/www/test_auth.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/auth.py||airflow/www/auth.py": [
          "File: airflow/www/auth.py -> airflow/www/auth.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: from functools import wraps",
          "23: from typing import TYPE_CHECKING, Callable, Sequence, TypeVar, cast",
          "26: from flask_appbuilder._compat import as_unicode",
          "27: from flask_appbuilder.const import (",
          "28:     FLAMSG_ERR_SEC_ACCESS_DENIED,",
          "",
          "[Removed Lines]",
          "25: from flask import flash, redirect, render_template, request",
          "",
          "[Added Lines]",
          "25: from flask import flash, redirect, render_template, request, url_for",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "176:             ),",
          "177:             403,",
          "178:         )",
          "179:     else:",
          "180:         access_denied = get_access_denied_message()",
          "181:         flash(access_denied, \"danger\")",
          "185: def has_access_cluster_activity(method: ResourceMethod) -> Callable[[T], T]:",
          "",
          "[Removed Lines]",
          "182:     return redirect(get_auth_manager().get_url_login(next=request.url))",
          "",
          "[Added Lines]",
          "179:     elif not get_auth_manager().is_logged_in():",
          "180:         return redirect(get_auth_manager().get_url_login(next=request.url))",
          "184:     return redirect(url_for(\"Airflow.index\"))",
          "",
          "---------------"
        ],
        "tests/www/test_auth.py||tests/www/test_auth.py": [
          "File: tests/www/test_auth.py -> tests/www/test_auth.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "206:             result = auth.has_access_dag_entities(\"GET\", dag_access_entity)(self.method_test)(None, items)",
          "208:         mock_call.assert_not_called()",
          "212: @pytest.mark.db_test",
          "",
          "[Removed Lines]",
          "209:         assert result.status_code == 302",
          "",
          "[Added Lines]",
          "209:         assert result.headers[\"Location\"] == \"/home\"",
          "211:     @pytest.mark.db_test",
          "212:     @patch(\"airflow.www.auth.get_auth_manager\")",
          "213:     def test_has_access_dag_entities_when_logged_out(self, mock_get_auth_manager, app, dag_access_entity):",
          "214:         auth_manager = Mock()",
          "215:         auth_manager.batch_is_authorized_dag.return_value = False",
          "216:         auth_manager.is_logged_in.return_value = False",
          "217:         auth_manager.get_url_login.return_value = \"login_url\"",
          "218:         mock_get_auth_manager.return_value = auth_manager",
          "219:         items = [Mock(dag_id=\"dag_1\"), Mock(dag_id=\"dag_2\")]",
          "221:         with app.test_request_context():",
          "222:             result = auth.has_access_dag_entities(\"GET\", dag_access_entity)(self.method_test)(None, items)",
          "224:         mock_call.assert_not_called()",
          "225:         assert result.headers[\"Location\"] == \"login_url\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ce43d4abeaf8b08dfe7c9022cea46a71efaa6a78",
      "candidate_info": {
        "commit_hash": "ce43d4abeaf8b08dfe7c9022cea46a71efaa6a78",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ce43d4abeaf8b08dfe7c9022cea46a71efaa6a78",
        "files": [
          "dev/breeze/src/airflow_breeze/utils/path_utils.py",
          "dev/breeze/tests/test_find_airflow_directory.py"
        ],
        "message": "Change detection mechanism for Breeze self-upgrade (#36635)\n\nBreeze auto-detects if it should upgrade itself - based on\nfinding Airflow directory it is in and calculating the hash of\nthe pyproject.toml it uses. Finding the airflow sources to\nact on was using setup.cfg from Airflow and checking the package\nname inside, but since we are about to remove setup.cfg, and\nmove all project configuration to pyproject.toml (see #36537), this\nmechanism will stop working.\n\nThis PR changes it by just checking if `airflow` subdir is present,\nand contains `__init__.py` with \"airflow\" inside. That should be\n\"good enough\" and fast, and also it should be backwards compatible\nin case new Breeze is used in older airflow sources.\n\n(cherry picked from commit c5de0db05e6ec5aff135d03de63f4683b3141a95)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/utils/path_utils.py||dev/breeze/src/airflow_breeze/utils/path_utils.py",
          "dev/breeze/tests/test_find_airflow_directory.py||dev/breeze/tests/test_find_airflow_directory.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/utils/path_utils.py||dev/breeze/src/airflow_breeze/utils/path_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/path_utils.py -> dev/breeze/src/airflow_breeze/utils/path_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: from airflow_breeze.utils.reinstall import reinstall_breeze, warn_dependencies_changed, warn_non_editable",
          "35: from airflow_breeze.utils.shared_options import get_verbose, set_forced_answer",
          "40: def search_upwards_for_airflow_sources_root(start_from: Path) -> Path | None:",
          "41:     root = Path(start_from.root)",
          "42:     d = start_from",
          "43:     while d != root:",
          "47:         d = d.parent",
          "48:     return None",
          "",
          "[Removed Lines]",
          "37: AIRFLOW_CFG_FILE = \"setup.cfg\"",
          "44:         attempt = d / AIRFLOW_CFG_FILE",
          "45:         if attempt.exists() and \"name = apache-airflow\\n\" in attempt.read_text():",
          "46:             return attempt.parent",
          "",
          "[Added Lines]",
          "37: PYPROJECT_TOML_FILE = \"pyproject.toml\"",
          "44:         airflow_candidate = d / \"airflow\"",
          "45:         airflow_candidate_init_py = airflow_candidate / \"__init__.py\"",
          "46:         if (",
          "47:             airflow_candidate.is_dir()",
          "48:             and airflow_candidate_init_py.is_file()",
          "49:             and \"airflow\" in airflow_candidate_init_py.read_text().lower()",
          "50:         ):",
          "51:             return airflow_candidate.parent",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "97:     return \"NOT FOUND\"",
          "101:     try:",
          "102:         the_hash = hashlib.new(\"blake2b\")",
          "104:         return the_hash.hexdigest()",
          "105:     except FileNotFoundError as e:",
          "106:         return f\"Missing file {e.filename}\"",
          "",
          "[Removed Lines]",
          "100: def get_sources_setup_metadata_hash(sources: Path) -> str:",
          "103:         the_hash.update((sources / \"dev\" / \"breeze\" / \"pyproject.toml\").read_bytes())",
          "",
          "[Added Lines]",
          "105: def get_pyproject_toml_hash(sources: Path) -> str:",
          "108:         the_hash.update((sources / \"dev\" / \"breeze\" / PYPROJECT_TOML_FILE).read_bytes())",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "109: def get_installation_sources_config_metadata_hash() -> str:",
          "110:     \"\"\"",
          "113:     This is used in order to determine if we need to upgrade Breeze, because some",
          "114:     setup files changed. Blake2b algorithm will not be flagged by security checkers",
          "",
          "[Removed Lines]",
          "111:     Retrieves hash of setup.py and setup.cfg files from the source of installation of Breeze.",
          "",
          "[Added Lines]",
          "116:     Retrieves hash of pyproject.toml from the source of installation of Breeze.",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "118:     installation_sources = get_installation_airflow_sources()",
          "119:     if installation_sources is None:",
          "120:         return \"NOT FOUND\"",
          "124: def get_used_sources_setup_metadata_hash() -> str:",
          "125:     \"\"\"",
          "126:     Retrieves hash of setup files from the currently used sources.",
          "127:     \"\"\"",
          "131: def set_forced_answer_for_upgrade_check():",
          "",
          "[Removed Lines]",
          "121:     return get_sources_setup_metadata_hash(installation_sources)",
          "128:     return get_sources_setup_metadata_hash(get_used_airflow_sources())",
          "",
          "[Added Lines]",
          "126:     return get_pyproject_toml_hash(installation_sources)",
          "133:     return get_pyproject_toml_hash(get_used_airflow_sources())",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_find_airflow_directory.py||dev/breeze/tests/test_find_airflow_directory.py": [
          "File: dev/breeze/tests/test_find_airflow_directory.py -> dev/breeze/tests/test_find_airflow_directory.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "42:     assert output == \"\"",
          "46: @mock.patch(\"airflow_breeze.utils.path_utils.Path.cwd\")",
          "47: def test_find_airflow_root_from_installation_dir(mock_cwd, capsys):",
          "48:     mock_cwd.return_value = ROOT_PATH",
          "",
          "[Removed Lines]",
          "45: @mock.patch(\"airflow_breeze.utils.path_utils.AIRFLOW_CFG_FILE\", \"bad_name.cfg\")",
          "",
          "[Added Lines]",
          "45: @mock.patch(\"airflow_breeze.utils.path_utils.PYPROJECT_TOML_FILE\", \"bad_name.toml\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0d12706dd9e8559942c303df8bb86d107ae9b039",
      "candidate_info": {
        "commit_hash": "0d12706dd9e8559942c303df8bb86d107ae9b039",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0d12706dd9e8559942c303df8bb86d107ae9b039",
        "files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/utils/docs_publisher.py",
          "dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py",
          "docs/exts/docs_build/docs_builder.py"
        ],
        "message": "Remove rendundant and unused code in docs building/publishing (#36346)\n\nCurrently docs building happens insid of the container image and code\ndoing that sits in `docs` folder, while publishing has already been\nmoved to `breeze` code (and is executed in the Breeze venv, not in the\ncontainer). Both building and publishing code were present in both\n(copy&pasted) and the parts of it not relevant to the `other` function\nhas not been used.\n\nWhile eventually we will move docs building also to `breeze` the first\nstep of that is to remove the redundancy and clean-up unused code, so\nthat we can make the transition cleaner.\n\n(cherry picked from commit bf90992dd48bce7de9f2a687860479e95575cd24)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/utils/docs_publisher.py||dev/breeze/src/airflow_breeze/utils/docs_publisher.py",
          "dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py||dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "109:     fix_ownership_using_docker,",
          "110:     perform_environment_checks,",
          "111: )",
          "112: from airflow_breeze.utils.github import download_constraints_file, get_active_airflow_versions",
          "113: from airflow_breeze.utils.packages import (",
          "114:     PackageSuspendedException,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "112: from airflow_breeze.utils.docs_publisher import DocsPublisher",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "139:     generate_providers_metadata_for_package,",
          "140:     get_related_providers,",
          "141: )",
          "143: from airflow_breeze.utils.python_versions import get_python_version_list",
          "144: from airflow_breeze.utils.run_utils import (",
          "145:     clean_www_assets,",
          "",
          "[Removed Lines]",
          "142: from airflow_breeze.utils.publish_docs_builder import PublishDocsBuilder",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1083:     verbose: bool,",
          "1084:     output: Output | None,",
          "1085: ) -> tuple[int, str]:",
          "1087:     builder.publish(override_versioned=override_versioned, airflow_site_dir=airflow_site_directory)",
          "1088:     return (",
          "1089:         0,",
          "",
          "[Removed Lines]",
          "1086:     builder = PublishDocsBuilder(package_name=package_name, output=output, verbose=verbose)",
          "",
          "[Added Lines]",
          "1086:     builder = DocsPublisher(package_name=package_name, output=output, verbose=verbose)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/docs_publisher.py||dev/breeze/src/airflow_breeze/utils/docs_publisher.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docs_publisher.py -> dev/breeze/src/airflow_breeze/utils/docs_publisher.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "19: import os",
          "20: import shutil",
          "21: from pathlib import Path",
          "23: from airflow_breeze.global_constants import get_airflow_version",
          "24: from airflow_breeze.utils.console import Output, get_console",
          "25: from airflow_breeze.utils.helm_chart_utils import chart_version",
          "26: from airflow_breeze.utils.packages import get_provider_packages_metadata, get_short_package_name",
          "27: from airflow_breeze.utils.publish_docs_helpers import pretty_format_path",
          "29: PROCESS_TIMEOUT = 15 * 60",
          "31: ROOT_PROJECT_DIR = Path(__file__).parents[5].resolve()",
          "32: DOCS_DIR = os.path.join(ROOT_PROJECT_DIR, \"docs\")",
          "35: class DocsPublisher:",
          "36:     \"\"\"Documentation builder for Airflow Docs Publishing.\"\"\"",
          "38:     def __init__(self, package_name: str, output: Output | None, verbose: bool):",
          "39:         self.package_name = package_name",
          "40:         self.output = output",
          "41:         self.verbose = verbose",
          "43:     @property",
          "44:     def is_versioned(self):",
          "45:         \"\"\"Is current documentation package versioned?\"\"\"",
          "46:         # Disable versioning. This documentation does not apply to any released product and we can update",
          "47:         # it as needed, i.e. with each new package of providers.",
          "48:         return self.package_name not in (\"apache-airflow-providers\", \"docker-stack\")",
          "50:     @property",
          "51:     def _build_dir(self) -> str:",
          "52:         if self.is_versioned:",
          "53:             version = \"stable\"",
          "54:             return f\"{DOCS_DIR}/_build/docs/{self.package_name}/{version}\"",
          "55:         else:",
          "56:             return f\"{DOCS_DIR}/_build/docs/{self.package_name}\"",
          "58:     @property",
          "59:     def _current_version(self):",
          "60:         if not self.is_versioned:",
          "61:             raise Exception(\"This documentation package is not versioned\")",
          "62:         if self.package_name == \"apache-airflow\":",
          "63:             return get_airflow_version()",
          "64:         if self.package_name.startswith(\"apache-airflow-providers-\"):",
          "65:             provider = get_provider_packages_metadata().get(get_short_package_name(self.package_name))",
          "66:             return provider[\"versions\"][0]",
          "67:         if self.package_name == \"helm-chart\":",
          "68:             return chart_version()",
          "69:         return Exception(f\"Unsupported package: {self.package_name}\")",
          "71:     @property",
          "72:     def _publish_dir(self) -> str:",
          "73:         if self.is_versioned:",
          "74:             return f\"docs-archive/{self.package_name}/{self._current_version}\"",
          "75:         else:",
          "76:             return f\"docs-archive/{self.package_name}\"",
          "78:     def publish(self, override_versioned: bool, airflow_site_dir: str):",
          "79:         \"\"\"Copy documentation packages files to airflow-site repository.\"\"\"",
          "80:         get_console(output=self.output).print(f\"Publishing docs for {self.package_name}\")",
          "81:         output_dir = os.path.join(airflow_site_dir, self._publish_dir)",
          "82:         pretty_source = pretty_format_path(self._build_dir, os.getcwd())",
          "83:         pretty_target = pretty_format_path(output_dir, airflow_site_dir)",
          "84:         get_console(output=self.output).print(f\"Copy directory: {pretty_source} => {pretty_target}\")",
          "85:         if os.path.exists(output_dir):",
          "86:             if self.is_versioned:",
          "87:                 if override_versioned:",
          "88:                     get_console(output=self.output).print(f\"Overriding previously existing {output_dir}! \")",
          "89:                 else:",
          "90:                     get_console(output=self.output).print(",
          "91:                         f\"Skipping previously existing {output_dir}! \"",
          "92:                         f\"Delete it manually if you want to regenerate it!\"",
          "93:                     )",
          "94:                     get_console(output=self.output).print()",
          "95:                     return",
          "96:             shutil.rmtree(output_dir)",
          "97:         shutil.copytree(self._build_dir, output_dir)",
          "98:         if self.is_versioned:",
          "99:             with open(os.path.join(output_dir, \"..\", \"stable.txt\"), \"w\") as stable_file:",
          "100:                 stable_file.write(self._current_version)",
          "101:         get_console(output=self.output).print()",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py||dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py": [
          "File: dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py -> dev/breeze/src/airflow_breeze/utils/publish_docs_builder.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "23d06bed8eaf99e4c44c4ba46c05238de1ed8c91",
      "candidate_info": {
        "commit_hash": "23d06bed8eaf99e4c44c4ba46c05238de1ed8c91",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/23d06bed8eaf99e4c44c4ba46c05238de1ed8c91",
        "files": [
          ".github/ISSUE_TEMPLATE/airflow_providers_bug_report.yml",
          "PROVIDERS.rst",
          "airflow/provider.yaml.schema.json",
          "airflow/providers/MANAGING_PROVIDERS_LIFECYCLE.rst",
          "airflow/providers/airbyte/provider.yaml",
          "airflow/providers/alibaba/provider.yaml",
          "airflow/providers/amazon/provider.yaml",
          "airflow/providers/apache/beam/provider.yaml",
          "airflow/providers/apache/cassandra/provider.yaml",
          "airflow/providers/apache/drill/provider.yaml",
          "airflow/providers/apache/druid/provider.yaml",
          "airflow/providers/apache/flink/provider.yaml",
          "airflow/providers/apache/hdfs/provider.yaml",
          "airflow/providers/apache/hive/provider.yaml",
          "airflow/providers/apache/impala/provider.yaml",
          "airflow/providers/apache/kafka/provider.yaml",
          "airflow/providers/apache/kylin/provider.yaml",
          "airflow/providers/apache/livy/provider.yaml",
          "airflow/providers/apache/pig/provider.yaml",
          "airflow/providers/apache/pinot/provider.yaml",
          "airflow/providers/apache/spark/provider.yaml",
          "airflow/providers/apache/sqoop/provider.yaml",
          "airflow/providers/apprise/provider.yaml",
          "airflow/providers/arangodb/provider.yaml",
          "airflow/providers/asana/provider.yaml",
          "airflow/providers/atlassian/jira/provider.yaml",
          "airflow/providers/celery/provider.yaml",
          "airflow/providers/cloudant/provider.yaml",
          "airflow/providers/cncf/kubernetes/provider.yaml",
          "airflow/providers/cohere/provider.yaml",
          "airflow/providers/common/io/provider.yaml",
          "airflow/providers/common/sql/provider.yaml",
          "airflow/providers/daskexecutor/provider.yaml",
          "airflow/providers/databricks/provider.yaml",
          "airflow/providers/datadog/provider.yaml",
          "airflow/providers/dbt/cloud/provider.yaml",
          "airflow/providers/dingding/provider.yaml",
          "airflow/providers/discord/provider.yaml",
          "airflow/providers/docker/provider.yaml",
          "airflow/providers/elasticsearch/provider.yaml",
          "airflow/providers/exasol/provider.yaml",
          "airflow/providers/facebook/provider.yaml",
          "airflow/providers/ftp/provider.yaml",
          "airflow/providers/github/provider.yaml",
          "airflow/providers/google/provider.yaml",
          "airflow/providers/grpc/provider.yaml",
          "airflow/providers/hashicorp/provider.yaml",
          "airflow/providers/http/provider.yaml",
          "airflow/providers/imap/provider.yaml",
          "airflow/providers/influxdb/provider.yaml",
          "airflow/providers/jdbc/provider.yaml",
          "airflow/providers/jenkins/provider.yaml",
          "airflow/providers/microsoft/azure/provider.yaml",
          "airflow/providers/microsoft/mssql/provider.yaml",
          "airflow/providers/microsoft/psrp/provider.yaml",
          "airflow/providers/microsoft/winrm/provider.yaml",
          "airflow/providers/mongo/provider.yaml",
          "airflow/providers/mysql/provider.yaml",
          "airflow/providers/neo4j/provider.yaml",
          "airflow/providers/odbc/provider.yaml",
          "airflow/providers/openai/provider.yaml",
          "airflow/providers/openfaas/provider.yaml",
          "airflow/providers/openlineage/provider.yaml",
          "airflow/providers/opensearch/provider.yaml",
          "airflow/providers/opsgenie/provider.yaml",
          "airflow/providers/oracle/provider.yaml",
          "airflow/providers/pagerduty/provider.yaml",
          "airflow/providers/papermill/provider.yaml",
          "airflow/providers/pgvector/provider.yaml",
          "airflow/providers/pinecone/provider.yaml",
          "airflow/providers/plexus/provider.yaml",
          "airflow/providers/postgres/provider.yaml",
          "airflow/providers/presto/provider.yaml",
          "airflow/providers/redis/provider.yaml",
          "airflow/providers/salesforce/provider.yaml",
          "airflow/providers/samba/provider.yaml",
          "airflow/providers/segment/provider.yaml",
          "airflow/providers/sendgrid/provider.yaml",
          "airflow/providers/sftp/provider.yaml",
          "airflow/providers/singularity/provider.yaml",
          "airflow/providers/slack/provider.yaml",
          "airflow/providers/smtp/provider.yaml",
          "airflow/providers/snowflake/provider.yaml",
          "airflow/providers/sqlite/provider.yaml",
          "airflow/providers/ssh/provider.yaml",
          "airflow/providers/tableau/provider.yaml",
          "airflow/providers/tabular/provider.yaml",
          "airflow/providers/telegram/provider.yaml",
          "airflow/providers/trino/provider.yaml",
          "airflow/providers/vertica/provider.yaml",
          "airflow/providers/weaviate/provider.yaml",
          "airflow/providers/yandex/provider.yaml",
          "airflow/providers/zendesk/provider.yaml",
          "dev/breeze/src/airflow_breeze/breeze.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/utils/packages.py",
          "dev/breeze/tests/test_packages.py",
          "docs/exts/docs_build/package_filter.py",
          "docs/exts/provider_yaml_utils.py",
          "generated/provider_dependencies.json",
          "images/breeze/output_build-docs.svg",
          "images/breeze/output_build-docs.txt",
          "images/breeze/output_release-management_add-back-references.svg",
          "images/breeze/output_release-management_add-back-references.txt",
          "images/breeze/output_release-management_generate-constraints.svg",
          "images/breeze/output_release-management_publish-docs.svg",
          "images/breeze/output_release-management_publish-docs.txt",
          "images/breeze/output_sbom_generate-providers-requirements.svg",
          "images/breeze/output_sbom_generate-providers-requirements.txt",
          "scripts/ci/pre_commit/pre_commit_check_provider_docs.py",
          "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py",
          "scripts/in_container/run_provider_yaml_files_check.py",
          "setup.py",
          "tests/always/test_example_dags.py"
        ],
        "message": "Speed up autocompletion of Breeze by simplifying provider state (#36499)\n\nSome recent changes, adding removed and suspended state for breeze\ncaused significant slow-down of autocompletion retrieval - as it\nturned out, because we loaded and parsed all provider yaml files\nduring auto-completion - in order to determine list of providers\navailable for some commands.\n\nWe already planned to replace the several states (suspended,\nnot-ready, removed) with a single state field - by doing it and\naddding the field to pre-commit generated \"provider_dependencies.json\"\nwe could switch to parsing the single provider_dependencies.json\nfile and retrieve provider list from there following the state stored\nin that json file.\n\nThis also simplifies state management following the recently\nadded state diagram by following the same state lifecycle:\n\n\"not-ready\" -> \"ready\" -> \"suspended\" -> \"removed\"",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/breeze.py||dev/breeze/src/airflow_breeze/breeze.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/utils/packages.py||dev/breeze/src/airflow_breeze/utils/packages.py",
          "dev/breeze/tests/test_packages.py||dev/breeze/tests/test_packages.py",
          "scripts/ci/pre_commit/pre_commit_check_provider_docs.py||scripts/ci/pre_commit/pre_commit_check_provider_docs.py",
          "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py||scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py",
          "scripts/in_container/run_provider_yaml_files_check.py||scripts/in_container/run_provider_yaml_files_check.py",
          "setup.py||setup.py",
          "tests/always/test_example_dags.py||tests/always/test_example_dags.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/breeze.py||dev/breeze/src/airflow_breeze/breeze.py": [
          "File: dev/breeze/src/airflow_breeze/breeze.py -> dev/breeze/src/airflow_breeze/breeze.py"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "450:         except PrepareReleaseDocsUserQuitException:",
          "451:             break",
          "452:         else:",
          "454:                 removed_packages.append(provider_id)",
          "455:             else:",
          "456:                 success_packages.append(provider_id)",
          "",
          "[Removed Lines]",
          "453:             if provider_metadata.get(\"removed\"):",
          "",
          "[Added Lines]",
          "453:             if provider_metadata[\"state\"] == \"removed\":",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "481:     if not provider_metadata:",
          "482:         get_console().print(f\"[error]The package {provider_package_id} is not a provider package. Exiting[/]\")",
          "483:         sys.exit(1)",
          "485:         get_console().print(",
          "486:             f\"[warning]The package: {provider_package_id} is scheduled for removal, but \"",
          "487:             f\"since you asked for it, it will be built [/]\\n\"",
          "488:         )",
          "490:         get_console().print(f\"[warning]The package: {provider_package_id} is suspended \" f\"skipping it [/]\\n\")",
          "491:         raise PackageSuspendedException()",
          "492:     return provider_metadata",
          "",
          "[Removed Lines]",
          "484:     if provider_metadata.get(\"removed\", False):",
          "489:     elif provider_metadata.get(\"suspended\"):",
          "",
          "[Added Lines]",
          "484:     if provider_metadata[\"state\"] == \"removed\":",
          "489:     elif provider_metadata.get(\"state\") == \"suspended\":",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/packages.py||dev/breeze/src/airflow_breeze/utils/packages.py": [
          "File: dev/breeze/src/airflow_breeze/utils/packages.py -> dev/breeze/src/airflow_breeze/utils/packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "156:     refresh_provider_metadata_from_yaml_file(provider_yaml_path)",
          "159: def get_provider_packages_metadata() -> dict[str, dict[str, Any]]:",
          "160:     \"\"\"",
          "161:     Load all data from providers files",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "159: @lru_cache(maxsize=1)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "208: @lru_cache",
          "209: def get_suspended_provider_ids() -> list[str]:",
          "217: @lru_cache",
          "",
          "[Removed Lines]",
          "210:     return [",
          "211:         provider_id",
          "212:         for provider_id, provider_metadata in get_provider_packages_metadata().items()",
          "213:         if provider_metadata.get(\"suspended\", False)",
          "214:     ]",
          "",
          "[Added Lines]",
          "211:     return get_available_packages(include_suspended=True, include_regular=False)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "222: @lru_cache",
          "223: def get_removed_provider_ids() -> list[str]:",
          "231: @lru_cache",
          "232: def get_not_ready_provider_ids() -> list[str]:",
          "240: def get_provider_requirements(provider_id: str) -> list[str]:",
          "",
          "[Removed Lines]",
          "224:     return [",
          "225:         provider_id",
          "226:         for provider_id, provider_metadata in get_provider_packages_metadata().items()",
          "227:         if provider_metadata.get(\"removed\", False)",
          "228:     ]",
          "233:     return [",
          "234:         provider_id",
          "235:         for provider_id, provider_metadata in get_provider_packages_metadata().items()",
          "236:         if provider_metadata.get(\"not-ready\", False)",
          "237:     ]",
          "",
          "[Added Lines]",
          "221:     return get_available_packages(include_removed=True, include_regular=False)",
          "226:     return get_available_packages(include_not_ready=True, include_regular=False)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "249:     include_suspended: bool = False,",
          "250:     include_removed: bool = False,",
          "251:     include_not_ready: bool = False,",
          "252: ) -> list[str]:",
          "253:     \"\"\"",
          "254:     Return provider ids for all packages that are available currently (not suspended).",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "241:     include_regular: bool = True,",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "256:     :rtype: object",
          "257:     :param include_suspended: whether the suspended packages should be included",
          "258:     :param include_removed: whether the removed packages should be included",
          "260:     :param include_non_provider_doc_packages: whether the non-provider doc packages should be included",
          "261:            (packages like apache-airflow, helm-chart, docker-stack)",
          "262:     :param include_all_providers: whether \"all-providers\" should be included ni the list.",
          "264:     \"\"\"",
          "272:     if include_non_provider_doc_packages:",
          "273:         available_packages.extend(REGULAR_DOC_PACKAGES)",
          "274:     if include_all_providers:",
          "275:         available_packages.append(\"all-providers\")",
          "281:     return sorted(set(available_packages))",
          "",
          "[Removed Lines]",
          "259:     :param include_not_ready: whether the not-ready ppackages should be included",
          "265:     provider_ids: list[str] = list(json.loads(PROVIDER_DEPENDENCIES_JSON_FILE_PATH.read_text()).keys())",
          "266:     available_packages = []",
          "267:     not_ready_provider_ids = get_not_ready_provider_ids()",
          "268:     if not include_not_ready:",
          "269:         provider_ids = [",
          "270:             provider_id for provider_id in provider_ids if provider_id not in not_ready_provider_ids",
          "271:         ]",
          "276:     available_packages.extend(provider_ids)",
          "277:     if include_suspended:",
          "278:         available_packages.extend(get_suspended_provider_ids())",
          "279:     if include_removed:",
          "280:         available_packages.extend(get_removed_provider_ids())",
          "",
          "[Added Lines]",
          "249:     :param include_not_ready: whether the not-ready packages should be included",
          "250:     :param include_regular: whether the regular packages should be included",
          "256:     provider_dependencies = json.loads(PROVIDER_DEPENDENCIES_JSON_FILE_PATH.read_text())",
          "258:     valid_states = set()",
          "259:     if include_not_ready:",
          "260:         valid_states.add(\"not-ready\")",
          "261:     if include_regular:",
          "262:         valid_states.add(\"ready\")",
          "263:     if include_suspended:",
          "264:         valid_states.add(\"suspended\")",
          "265:     if include_removed:",
          "266:         valid_states.add(\"removed\")",
          "267:     available_packages: list[str] = [",
          "268:         provider_id",
          "269:         for provider_id, provider_dependencies in provider_dependencies.items()",
          "270:         if provider_dependencies[\"state\"] in valid_states",
          "271:     ]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "499:         versions=provider_info[\"versions\"],",
          "500:         excluded_python_versions=provider_info.get(\"excluded-python-versions\") or [],",
          "501:         plugins=plugins,",
          "503:     )",
          "",
          "[Removed Lines]",
          "502:         removed=provider_info.get(\"removed\", False),",
          "",
          "[Added Lines]",
          "497:         removed=provider_info[\"state\"] == \"removed\",",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_packages.py||dev/breeze/tests/test_packages.py": [
          "File: dev/breeze/tests/test_packages.py -> dev/breeze/tests/test_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "110: def test_get_removed_providers():",
          "111:     # Modify it every time we schedule provider for removal or remove it",
          "115: def test_get_suspended_provider_ids():",
          "",
          "[Removed Lines]",
          "112:     assert [\"apache.sqoop\", \"daskexecutor\", \"plexus\"] == get_removed_provider_ids()",
          "",
          "[Added Lines]",
          "112:     assert [] == get_removed_provider_ids()",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "340:     assert provider_info_dict[\"name\"] == \"Amazon\"",
          "341:     assert provider_info_dict[\"package-name\"] == \"apache-airflow-providers-amazon\"",
          "342:     assert \"Amazon\" in provider_info_dict[\"description\"]",
          "344:     assert provider_info_dict[\"filesystems\"] == [\"airflow.providers.amazon.aws.fs.s3\"]",
          "345:     assert len(provider_info_dict[\"versions\"]) > 45",
          "346:     assert len(provider_info_dict[\"dependencies\"]) > 10",
          "",
          "[Removed Lines]",
          "343:     assert provider_info_dict[\"suspended\"] is False",
          "",
          "[Added Lines]",
          "343:     assert provider_info_dict[\"state\"] == \"ready\"",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_check_provider_docs.py||scripts/ci/pre_commit/pre_commit_check_provider_docs.py": [
          "File: scripts/ci/pre_commit/pre_commit_check_provider_docs.py -> scripts/ci/pre_commit/pre_commit_check_provider_docs.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "94:     for provider_file in AIRFLOW_PROVIDERS_DIR.rglob(\"provider.yaml\"):",
          "95:         provider_name = str(provider_file.parent.relative_to(AIRFLOW_PROVIDERS_DIR)).replace(os.sep, \".\")",
          "96:         provider_info = yaml.safe_load(provider_file.read_text())",
          "98:             ALL_PROVIDERS[provider_name] = provider_info",
          "",
          "[Removed Lines]",
          "97:         if not provider_info[\"suspended\"]:",
          "",
          "[Added Lines]",
          "97:         if provider_info[\"state\"] != \"suspended\":",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py||scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py": [
          "File: scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py -> scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "97:                     os.sep, \".\"",
          "98:                 )",
          "99:                 provider_info = yaml.safe_load(provider_file.read_text())",
          "103:                     suspended_paths.append(provider_file.parent.relative_to(AIRFLOW_PROVIDERS_DIR).as_posix())",
          "104:             path = Path(root, filename)",
          "105:             if path.is_file() and path.name.endswith(\".py\"):",
          "106:                 ALL_PROVIDER_FILES.append(Path(root, filename))",
          "",
          "[Removed Lines]",
          "100:                 if not provider_info[\"suspended\"]:",
          "101:                     ALL_PROVIDERS[provider_name] = provider_info",
          "102:                 else:",
          "",
          "[Added Lines]",
          "100:                 if provider_info[\"state\"] == \"suspended\":",
          "102:                 ALL_PROVIDERS[provider_name] = provider_info",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "174:             ALL_DEPENDENCIES[file_provider][\"cross-providers-deps\"].append(imported_provider)",
          "177: if __name__ == \"__main__\":",
          "178:     find_all_providers_and_provider_files()",
          "179:     num_files = len(ALL_PROVIDER_FILES)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "176: STATES: dict[str, str] = {}",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "182:     for file in ALL_PROVIDER_FILES:",
          "183:         check_if_different_provider_used(file)",
          "184:     for provider, provider_yaml_content in ALL_PROVIDERS.items():",
          "187:     if warnings:",
          "188:         console.print(\"[yellow]Warnings!\\n\")",
          "189:         for warning in warnings:",
          "",
          "[Removed Lines]",
          "185:         if not provider_yaml_content.get(\"suspended\"):",
          "186:             ALL_DEPENDENCIES[provider][\"deps\"].extend(provider_yaml_content[\"dependencies\"])",
          "",
          "[Added Lines]",
          "187:         ALL_DEPENDENCIES[provider][\"deps\"].extend(provider_yaml_content[\"dependencies\"])",
          "188:         STATES[provider] = provider_yaml_content[\"state\"]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "194:         for error in errors:",
          "195:             console.print(f\"[red] {error}\")",
          "196:         console.print(f\"[bright_blue]Total: {len(errors)} errors.\")",
          "198:     for key in sorted(ALL_DEPENDENCIES.keys()):",
          "199:         unique_sorted_dependencies[key][\"deps\"] = sorted(ALL_DEPENDENCIES[key][\"deps\"])",
          "200:         unique_sorted_dependencies[key][\"cross-providers-deps\"] = sorted(",
          "",
          "[Removed Lines]",
          "197:     unique_sorted_dependencies: dict[str, dict[str, list[str]]] = defaultdict(dict)",
          "",
          "[Added Lines]",
          "199:     unique_sorted_dependencies: dict[str, dict[str, list[str] | str]] = defaultdict(dict)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "202:         )",
          "203:         excluded_versions = ALL_PROVIDERS[key].get(\"excluded-python-versions\")",
          "204:         unique_sorted_dependencies[key][\"excluded-python-versions\"] = excluded_versions or []",
          "205:     if errors:",
          "206:         console.print()",
          "207:         console.print(\"[red]Errors found during verification. Exiting!\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "207:         unique_sorted_dependencies[key][\"state\"] = STATES[key]",
          "",
          "---------------"
        ],
        "scripts/in_container/run_provider_yaml_files_check.py||scripts/in_container/run_provider_yaml_files_check.py": [
          "File: scripts/in_container/run_provider_yaml_files_check.py -> scripts/in_container/run_provider_yaml_files_check.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "114:             jsonschema.validate(provider, schema=schema)",
          "115:         except jsonschema.ValidationError:",
          "116:             raise Exception(f\"Unable to parse: {rel_path}.\")",
          "118:             result[rel_path] = provider",
          "119:         else:",
          "120:             suspended_providers.add(provider[\"package-name\"])",
          "",
          "[Removed Lines]",
          "117:         if not provider.get(\"suspended\"):",
          "",
          "[Added Lines]",
          "117:         if provider[\"state\"] not in [\"suspended\", \"removed\"]:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "557:                 op[\"how-to-guide\"] for op in provider[\"transfers\"] if \"how-to-guide\" in op",
          "558:             )",
          "559:     if suspended_providers:",
          "561:         console.print(suspended_providers)",
          "563:     expected_doc_files = itertools.chain(",
          "",
          "[Removed Lines]",
          "560:         console.print(\"[yellow]Suspended providers:[/]\")",
          "",
          "[Added Lines]",
          "560:         console.print(\"[yellow]Suspended/Removed providers:[/]\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "680:     return num_providers, num_errors",
          "703: if __name__ == \"__main__\":",
          "704:     ProvidersManager().initialize_providers_configuration()",
          "705:     architecture = Architecture.get_current()",
          "",
          "[Removed Lines]",
          "683: @run_check(\"Checking remove flag only set for suspended providers\")",
          "684: def check_removed_flag_only_set_for_suspended_providers(yaml_files: dict[str, dict]):",
          "685:     num_errors = 0",
          "686:     num_providers = 0",
          "687:     for package_info in yaml_files.values():",
          "688:         num_providers += 1",
          "689:         package_name = package_info[\"package-name\"]",
          "690:         suspended = package_info[\"suspended\"]",
          "691:         removed = package_info.get(\"removed\", False)",
          "692:         if removed and not suspended:",
          "693:             errors.append(",
          "694:                 f\"The provider {package_name} has removed set to True in their provider.yaml file \"",
          "695:                 f\"but suspended flag is set to false. You should only set removed flag in order to \"",
          "696:                 f\"prepare last release for a provider that has been previously suspended. \"",
          "697:                 f\"[yellow]How to fix it[/]: Please suspend the provider first before removing it.\"",
          "698:             )",
          "699:             num_errors += 1",
          "700:     return num_providers, num_errors",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "726:     check_notification_classes(all_parsed_yaml_files)",
          "727:     check_unique_provider_name(all_parsed_yaml_files)",
          "728:     check_providers_have_all_documentation_files(all_parsed_yaml_files)",
          "731:     if all_files_loaded:",
          "732:         # Only check those if all provider files are loaded",
          "",
          "[Removed Lines]",
          "729:     check_removed_flag_only_set_for_suspended_providers(all_parsed_yaml_files)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "105:             dependencies = json.load(f)",
          "106:         provider_dict = {}",
          "107:         for key, value in dependencies.items():",
          "108:             if value.get(DEPS):",
          "109:                 apply_pypi_suffix_to_airflow_packages(value[DEPS])",
          "110:             if CURRENT_PYTHON_VERSION not in value[\"excluded-python-versions\"] or skip_python_version_check:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "108:             if value[\"state\"] in [\"suspended\", \"removed\"]:",
          "109:                 continue",
          "",
          "---------------"
        ],
        "tests/always/test_example_dags.py||tests/always/test_example_dags.py": [
          "File: tests/always/test_example_dags.py -> tests/always/test_example_dags.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "40:     suspended_providers = []",
          "41:     for provider_path in AIRFLOW_PROVIDERS_ROOT.rglob(\"provider.yaml\"):",
          "42:         provider_yaml = yaml.safe_load(provider_path.read_text())",
          "44:             suspended_providers.append(",
          "45:                 provider_path.parent.relative_to(AIRFLOW_SOURCES_ROOT)",
          "46:                 .as_posix()",
          "",
          "[Removed Lines]",
          "43:         if provider_yaml.get(\"suspended\"):",
          "",
          "[Added Lines]",
          "43:         if provider_yaml[\"state\"] == \"suspended\":",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1e56cfade7aaf28eb8e55f1173a0fd85d225a47a",
      "candidate_info": {
        "commit_hash": "1e56cfade7aaf28eb8e55f1173a0fd85d225a47a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/1e56cfade7aaf28eb8e55f1173a0fd85d225a47a",
        "files": [
          "tests/conftest.py"
        ],
        "message": "Disable `PytestDeprecationWarning` when create warnings_recorder (#36759)\n\n(cherry picked from commit 70cefebbd5e20989b48386742089ebf747d991c1)",
        "before_after_code_files": [
          "tests/conftest.py||tests/conftest.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/conftest.py||tests/conftest.py": [
          "File: tests/conftest.py -> tests/conftest.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: import pytest",
          "31: import time_machine",
          "34: # We should set these before loading _any_ of the rest of airflow so that the",
          "35: # unit test mode config is set as early as possible.",
          "",
          "[Removed Lines]",
          "32: from _pytest.recwarn import WarningsRecorder",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1149: captured_warnings: dict[tuple[str, int, type[Warning], str], warnings.WarningMessage] = {}",
          "1150: captured_warnings_count: dict[tuple[str, int, type[Warning], str], int] = {}",
          "1152: default_formatwarning = warnings_recorder._module.formatwarning  # type: ignore[attr-defined]",
          "1153: default_showwarning = warnings_recorder._module.showwarning  # type: ignore[attr-defined]",
          "",
          "[Removed Lines]",
          "1151: warnings_recorder = WarningsRecorder()",
          "",
          "[Added Lines]",
          "1150: # By set ``_ispytest=True`` in WarningsRecorder we suppress annoying warnings:",
          "1151: # PytestDeprecationWarning: A private pytest class or function was used.",
          "1152: warnings_recorder = pytest.WarningsRecorder(_ispytest=True)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5deebca1735e8a570e107b42b493299770e9a5fc",
      "candidate_info": {
        "commit_hash": "5deebca1735e8a570e107b42b493299770e9a5fc",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/5deebca1735e8a570e107b42b493299770e9a5fc",
        "files": [
          "docs/apache-airflow/security/secrets/mask-sensitive-values.rst"
        ],
        "message": "Update mask-sensitive-values.rst (#36699)\n\n(cherry picked from commit 4c6be5476c8bed637101391e74e80a11f93d6daf)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "998dd5c84c2f22a12e255557fa80774d8b8d1086",
      "candidate_info": {
        "commit_hash": "998dd5c84c2f22a12e255557fa80774d8b8d1086",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/998dd5c84c2f22a12e255557fa80774d8b8d1086",
        "files": [
          "airflow/www/yarn.lock"
        ],
        "message": "Bump follow-redirects from 1.15.3 to 1.15.4 in /airflow/www (#36700)\n\nBumps [follow-redirects](https://github.com/follow-redirects/follow-redirects) from 1.15.3 to 1.15.4.\n- [Release notes](https://github.com/follow-redirects/follow-redirects/releases)\n- [Commits](https://github.com/follow-redirects/follow-redirects/compare/v1.15.3...v1.15.4)\n\n---\nupdated-dependencies:\n- dependency-name: follow-redirects\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>\nCo-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>\n(cherry picked from commit 0d7bad20d4398b55bafcd6fa41db89fc2e509d69)",
        "before_after_code_files": [
          "airflow/www/yarn.lock||airflow/www/yarn.lock"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/yarn.lock||airflow/www/yarn.lock": [
          "File: airflow/www/yarn.lock -> airflow/www/yarn.lock",
          "--- Hunk 1 ---",
          "[Context before]",
          "6441:     tslib \"^2.0.3\"",
          "6443: follow-redirects@^1.15.0:",
          "6448: for-each@^0.3.3:",
          "6449:   version \"0.3.3\"",
          "",
          "[Removed Lines]",
          "6444:   version \"1.15.3\"",
          "6445:   resolved \"https://registry.yarnpkg.com/follow-redirects/-/follow-redirects-1.15.3.tgz#fe2f3ef2690afce7e82ed0b44db08165b207123a\"",
          "6446:   integrity sha512-1VzOtuEM8pC9SFU1E+8KfTjZyMztRsgEfwQl44z8A25uy13jSzTj6dyK2Df52iV0vgHCfBwLhDWevLn95w5v6Q==",
          "",
          "[Added Lines]",
          "6444:   version \"1.15.4\"",
          "6445:   resolved \"https://registry.yarnpkg.com/follow-redirects/-/follow-redirects-1.15.4.tgz#cdc7d308bf6493126b17ea2191ea0ccf3e535adf\"",
          "6446:   integrity sha512-Cr4D/5wlrb0z9dgERpUL3LrmPKVDsETIJhaCMeDfuFYcqa5bldGV6wBsAN6X/vxlXQtFBMrXdXxdL8CbDTGniw==",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "525e22907471d08dc23bbcdc82c5c5c9d7687b4e",
      "candidate_info": {
        "commit_hash": "525e22907471d08dc23bbcdc82c5c5c9d7687b4e",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/525e22907471d08dc23bbcdc82c5c5c9d7687b4e",
        "files": [
          "airflow/jobs/backfill_job_runner.py"
        ],
        "message": "Revert \"Refactor _manage_executor_state by refreshing TIs in batch (#36418)\" (#36500)\n\nThis reverts commit 9d45db9e2cca2ad04db72f7e0712c478e5a8e1f1.\n\nt#\n\n(cherry picked from commit 72f43fcc838afc1c95b85dcb27af6519483ef64b)",
        "before_after_code_files": [
          "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py": [
          "File: airflow/jobs/backfill_job_runner.py -> airflow/jobs/backfill_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import attr",
          "24: import pendulum",
          "26: from sqlalchemy.exc import OperationalError",
          "27: from sqlalchemy.orm.session import make_transient",
          "28: from tabulate import tabulate",
          "",
          "[Removed Lines]",
          "25: from sqlalchemy import select, tuple_, update",
          "",
          "[Added Lines]",
          "25: from sqlalchemy import select, update",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "264:         :return: An iterable of expanded TaskInstance per MappedTask",
          "265:         \"\"\"",
          "266:         executor = self.job.executor",
          "291:             state, info = value",
          "294:                 self.log.warning(\"%s state %s not in running=%s\", key, state, running.values())",
          "295:                 continue",
          "299:             self.log.debug(\"Executor state: %s task %s\", state, ti)",
          "",
          "[Removed Lines]",
          "267:         # list of tuples (dag_id, task_id, execution_date, map_index) of running tasks in executor",
          "268:         buffered_events = list(executor.get_event_buffer().items())",
          "269:         running_tis_ids = [",
          "270:             (key.dag_id, key.task_id, key.run_id, key.map_index)",
          "271:             for key, _ in buffered_events",
          "272:             if key in running",
          "273:         ]",
          "274:         # list of TaskInstance of running tasks in executor (refreshed from db in batch)",
          "275:         refreshed_running_tis = session.scalars(",
          "276:             select(TaskInstance).where(",
          "277:                 tuple_(",
          "278:                     TaskInstance.dag_id,",
          "279:                     TaskInstance.task_id,",
          "280:                     TaskInstance.run_id,",
          "281:                     TaskInstance.map_index,",
          "282:                 ).in_(running_tis_ids)",
          "283:             )",
          "284:         ).all()",
          "285:         # dict of refreshed TaskInstance by key to easily find them",
          "286:         refreshed_running_tis_dict = {",
          "287:             (ti.dag_id, ti.task_id, ti.run_id, ti.map_index): ti for ti in refreshed_running_tis",
          "288:         }",
          "290:         for key, value in buffered_events:",
          "292:             ti_key = (key.dag_id, key.task_id, key.run_id, key.map_index)",
          "293:             if ti_key not in refreshed_running_tis_dict:",
          "297:             ti = refreshed_running_tis_dict[ti_key]",
          "",
          "[Added Lines]",
          "268:         # TODO: query all instead of refresh from db",
          "269:         for key, value in list(executor.get_event_buffer().items()):",
          "271:             if key not in running:",
          "275:             ti = running[key]",
          "276:             ti.refresh_from_db()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "3922ea13b109d996e636f871692b9527ebf87310",
      "candidate_info": {
        "commit_hash": "3922ea13b109d996e636f871692b9527ebf87310",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/3922ea13b109d996e636f871692b9527ebf87310",
        "files": [
          "dev/README_RELEASE_PROVIDER_PACKAGES.md",
          "generated/provider_metadata.json"
        ],
        "message": "Update providers metadata 2023-12-31 (#36507)\n\n(cherry picked from commit 3aeb31b5132ecdb789299b887bdc869e698a4a85)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "119ba542541f754dc2247bf9de8ea5fa7aa3e8f9",
      "candidate_info": {
        "commit_hash": "119ba542541f754dc2247bf9de8ea5fa7aa3e8f9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/119ba542541f754dc2247bf9de8ea5fa7aa3e8f9",
        "files": [
          "airflow/models/dag.py",
          "tests/models/test_dag.py"
        ],
        "message": "Fix Callback exception when a removed task is the last one in the task instance list (#36693)\n\n* Fix Callback exception when a removed task is the last one in the task instance list\n\n* Add test_dag_handle_callback_with_removed_task\n\n* Remove extra break line\n\n* Merge TIs filters\n\n* Fix static check\n\n* Revert changes\n\n(cherry picked from commit 8c1c09bab34be8234d9e152ed7e4c9b925c08459)",
        "before_after_code_files": [
          "airflow/models/dag.py||airflow/models/dag.py",
          "tests/models/test_dag.py||tests/models/test_dag.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1462:             # context for the callback.",
          "1463:             if dag.partial:",
          "1464:                 tis = [ti for ti in tis if not ti.state == State.NONE]",
          "1465:             ti = tis[-1]  # get first TaskInstance of DagRun",
          "1466:             ti.task = dag.get_task(ti.task_id)",
          "1467:             context = ti.get_template_context(session=session)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1465:             # filter out removed tasks",
          "1466:             tis = [ti for ti in tis if ti.state != TaskInstanceState.REMOVED]",
          "",
          "---------------"
        ],
        "tests/models/test_dag.py||tests/models/test_dag.py": [
          "File: tests/models/test_dag.py -> tests/models/test_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1512:         dag.clear()",
          "1513:         self._clean_up(dag_id)",
          "1515:     def test_next_dagrun_after_fake_scheduled_previous(self):",
          "1516:         \"\"\"",
          "1517:         Test scheduling a dag where there is a prior DagRun",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1515:     def test_dag_handle_callback_with_removed_task(self, dag_maker, session):",
          "1516:         \"\"\"",
          "1517:         Tests avoid crashes when a removed task is the last one in the list of task instance",
          "1518:         \"\"\"",
          "1519:         dag_id = \"test_dag_callback_with_removed_task\"",
          "1520:         mock_callback = mock.MagicMock()",
          "1521:         with DAG(",
          "1522:             dag_id=dag_id,",
          "1523:             on_success_callback=mock_callback,",
          "1524:             on_failure_callback=mock_callback,",
          "1525:         ) as dag:",
          "1526:             EmptyOperator(task_id=\"faketastic\")",
          "1527:             task_removed = EmptyOperator(task_id=\"removed_task\")",
          "1529:         with create_session() as session:",
          "1530:             dag_run = dag.create_dagrun(State.RUNNING, TEST_DATE, run_type=DagRunType.MANUAL, session=session)",
          "1531:             dag._remove_task(task_removed.task_id)",
          "1532:             tis = dag_run.get_task_instances(session=session)",
          "1533:             tis[-1].state = TaskInstanceState.REMOVED",
          "1534:             assert dag_run.get_task_instance(task_removed.task_id).state == TaskInstanceState.REMOVED",
          "1536:             # should not raise any exception",
          "1537:             dag.handle_callback(dag_run, success=True)",
          "1538:             dag.handle_callback(dag_run, success=False)",
          "1540:         dag.clear()",
          "1541:         self._clean_up(dag_id)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e49c573d9ce03066c61cce0624576700de1de07a",
      "candidate_info": {
        "commit_hash": "e49c573d9ce03066c61cce0624576700de1de07a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e49c573d9ce03066c61cce0624576700de1de07a",
        "files": [
          "dev/README_RELEASE_PROVIDER_PACKAGES.md",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2"
        ],
        "message": "Update release process for providers to include mixed RC versions (#36385)\n\nThis PR updates released process for providers to enable releasing\nproviders in more regular batches. Sometimes when we exclude a\nprovider from previous voting, we want to release RCN (2,3 etc.)\ncandidate.\n\nHowever, especially when time between previous RC and the new one\nis long (for example because fixing took a long time) we might\nwant to release the RCN release for that cancelled providers and\nRC1 for all the providers that have been changed in the meantime.\n\nThis cchange makes it possible (and easy):\n\n1) release RC1 for all providers (the RCN provider should be skipped,\n   because tag for this provider already exists.\n\n2) release the RCN providers with `--version-suffix-for-pypi rcN`.\n\nThe release process and tools were updated to account for that - where\nrc candidate number is retrieved from packages prepared in `dist`.\n\nFixed a few small missing things in the process.\n\n(cherry picked from commit 4deed641322343cb18eabd9ad199f5ac83f29ff0)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2||dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1495:     )",
          "1498: def get_prs_for_package(provider_id: str) -> list[int]:",
          "1499:     pr_matcher = re.compile(r\".*\\(#([0-9]*)\\)``$\")",
          "1500:     prs = []",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1498: VERSION_MATCH = re.compile(r\"([0-9]+)\\.([0-9]+)\\.([0-9]+)(.*)\")",
          "1501: def get_suffix_from_package_in_dist(dist_files: list[str], package: str) -> str | None:",
          "1502:     \"\"\"Get suffix from package prepared in dist folder.\"\"\"",
          "1503:     for file in dist_files:",
          "1504:         if file.startswith(f'apache_airflow_providers_{package.replace(\".\", \"_\")}') and file.endswith(",
          "1505:             \".tar.gz\"",
          "1506:         ):",
          "1507:             file = file[: -len(\".tar.gz\")]",
          "1508:             version = file.split(\"-\")[-1]",
          "1509:             match = VERSION_MATCH.match(version)",
          "1510:             if match:",
          "1511:                 return match.group(4)",
          "1512:     return None",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1566:         pypi_package_name: str",
          "1567:         version: str",
          "1568:         pr_list: list[PullRequest.PullRequest | Issue.Issue]",
          "1570:     if not provider_packages:",
          "1571:         provider_packages = list(DEPENDENCIES.keys())",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1586:         suffix: str",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1622:                 ).read_text()",
          "1623:             )",
          "1624:             if pull_request_list:",
          "1625:                 providers[provider_id] = ProviderPRInfo(",
          "1626:                     version=provider_yaml_dict[\"versions\"][0],",
          "1627:                     provider_package_id=provider_id,",
          "1628:                     pypi_package_name=provider_yaml_dict[\"package-name\"],",
          "1629:                     pr_list=pull_request_list,",
          "1630:                 )",
          "1631:         template = jinja2.Template(",
          "1632:             (Path(__file__).parents[1] / \"provider_issue_TEMPLATE.md.jinja2\").read_text()",
          "1633:         )",
          "1635:         get_console().print()",
          "1636:         get_console().print(",
          "1637:             \"[green]Below you can find the issue content that you can use \"",
          "",
          "[Removed Lines]",
          "1634:         issue_content = template.render(providers=providers, date=datetime.now(), suffix=suffix)",
          "",
          "[Added Lines]",
          "1643:                 package_suffix = get_suffix_from_package_in_dist(files_in_dist, provider_id)",
          "1649:                     suffix=package_suffix if package_suffix else suffix,",
          "1654:         issue_content = template.render(providers=providers, date=datetime.now())",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2||dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2": [
          "File: dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2 -> dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2",
          "--- Hunk 1 ---",
          "[Context before]",
          "10: Those are providers that require testing as there were some substantial changes introduced:",
          "12: {% for provider_id, provider_info in providers.items()  %}",
          "14: {%- for pr in provider_info.pr_list %}",
          "15:    - [ ] [{{ pr.title }} (#{{ pr.number }})]({{ pr.html_url }}): @{{ pr.user.login }}",
          "16: {%- endfor %}",
          "",
          "[Removed Lines]",
          "13: ## Provider [{{ provider_id }}: {{ provider_info.version }}{{ suffix }}](https://pypi.org/project/{{ provider_info.pypi_package_name }}/{{ provider_info.version }}{{ suffix }})",
          "",
          "[Added Lines]",
          "13: ## Provider [{{ provider_id }}: {{ provider_info.version }}{{ provider_info.suffix }}](https://pypi.org/project/{{ provider_info.pypi_package_name }}/{{ provider_info.version }}{{ provider_info.suffix }})",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "154a78dac9a89a54ce9de483c8eaa196f98862aa",
      "candidate_info": {
        "commit_hash": "154a78dac9a89a54ce9de483c8eaa196f98862aa",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/154a78dac9a89a54ce9de483c8eaa196f98862aa",
        "files": [
          "airflow/timetables/events.py",
          "tests/timetables/test_events_timetable.py"
        ],
        "message": "Do not let EventsTimetable schedule past events if catchup=False (#36134)\n\n* Fix the EventsTimetable schedules past events bug\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit c01daf811925816e9ae09b78c37b9ff8d87ce691)",
        "before_after_code_files": [
          "airflow/timetables/events.py||airflow/timetables/events.py",
          "tests/timetables/test_events_timetable.py||tests/timetables/test_events_timetable.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/timetables/events.py||airflow/timetables/events.py": [
          "File: airflow/timetables/events.py -> airflow/timetables/events.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import pendulum",
          "24: from airflow.timetables.base import DagRunInfo, DataInterval, Timetable",
          "26: if TYPE_CHECKING:",
          "27:     from pendulum import DateTime",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: from airflow.utils import timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "58:             self.event_dates.sort()",
          "59:         self.restrict_to_events = restrict_to_events",
          "60:         if description is None:",
          "65:         else:",
          "66:             self._summary = description",
          "67:             self.description = description",
          "",
          "[Removed Lines]",
          "61:             self.description = (",
          "62:                 f\"{len(self.event_dates)} Events between {self.event_dates[0]} and {self.event_dates[-1]}\"",
          "63:             )",
          "64:             self._summary = f\"{len(self.event_dates)} Events\"",
          "",
          "[Added Lines]",
          "62:             if self.event_dates:",
          "63:                 self.description = (",
          "64:                     f\"{len(self.event_dates)} events between {self.event_dates[0]} and {self.event_dates[-1]}\"",
          "65:                 )",
          "66:             else:",
          "67:                 self.description = \"No events\"",
          "68:             self._summary = f\"{len(self.event_dates)} events\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "79:         last_automated_data_interval: DataInterval | None,",
          "80:         restriction: TimeRestriction,",
          "81:     ) -> DagRunInfo | None:",
          "84:         else:",
          "93:         return DagRunInfo.exact(next_event)",
          "95:     def infer_manual_data_interval(self, *, run_after: DateTime) -> DataInterval:",
          "96:         # If Timetable not restricted to events, run for the time specified",
          "98:             return DataInterval.exact(run_after)",
          "100:         # If restricted to events, run for the most recent past event",
          "",
          "[Removed Lines]",
          "82:         if last_automated_data_interval is None:",
          "83:             next_event = self.event_dates[0]",
          "85:             future_dates = itertools.dropwhile(",
          "86:                 lambda when: when <= last_automated_data_interval.end,  # type: ignore",
          "87:                 self.event_dates,",
          "88:             )",
          "89:             next_event = next(future_dates, None)  # type: ignore",
          "90:             if next_event is None:",
          "91:                 return None",
          "97:         if not self.restrict_to_events:",
          "",
          "[Added Lines]",
          "86:         earliest = restriction.earliest",
          "87:         if not restriction.catchup:",
          "88:             current_time = timezone.utcnow()",
          "89:             if earliest is None or current_time > earliest:",
          "90:                 earliest = pendulum.instance(current_time)",
          "92:         for next_event in self.event_dates:",
          "93:             if earliest and next_event < earliest:",
          "94:                 continue",
          "95:             if last_automated_data_interval and next_event <= last_automated_data_interval.end:",
          "96:                 continue",
          "97:             break",
          "99:             # We need to return None if self.event_dates is empty or,",
          "100:             # if not empty, when no suitable event can be found.",
          "101:             return None",
          "103:         if restriction.latest is not None and next_event > restriction.latest:",
          "104:             return None",
          "110:         if not self.restrict_to_events or not self.event_dates:",
          "",
          "---------------"
        ],
        "tests/timetables/test_events_timetable.py||tests/timetables/test_events_timetable.py": [
          "File: tests/timetables/test_events_timetable.py -> tests/timetables/test_events_timetable.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import pendulum",
          "21: import pytest",
          "23: from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction, Timetable",
          "24: from airflow.timetables.events import EventsTimetable",
          "25: from airflow.utils.timezone import utc",
          "29: EVENT_DATES = [",
          "30:     pendulum.DateTime(2021, 9, 6, tzinfo=utc),",
          "",
          "[Removed Lines]",
          "27: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=utc)  # Precedes all events",
          "",
          "[Added Lines]",
          "22: import time_machine",
          "28: BEFORE_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=utc)  # Precedes all events",
          "29: START_DATE = pendulum.DateTime(2021, 9, 7, tzinfo=utc)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "93:     Test that when using strict event dates, manual runs before the first event have the first event's date",
          "94:     as the start interval",
          "95:     \"\"\"",
          "97:     expected_data_interval = DataInterval.exact(EVENT_DATES[0])",
          "98:     assert expected_data_interval == manual_run_data_interval",
          "",
          "[Removed Lines]",
          "96:     manual_run_data_interval = restricted_timetable.infer_manual_data_interval(run_after=START_DATE)",
          "",
          "[Added Lines]",
          "98:     manual_run_data_interval = restricted_timetable.infer_manual_data_interval(run_after=BEFORE_DATE)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "101: @pytest.mark.parametrize(",
          "102:     \"last_automated_data_interval, expected_next_info\",",
          "103:     [",
          "104:         pytest.param(DataInterval(day1, day1), DagRunInfo.interval(day2, day2))",
          "106:     ]",
          "107:     + [pytest.param(DataInterval(EVENT_DATES_SORTED[-1], EVENT_DATES_SORTED[-1]), None)],",
          "108: )",
          "",
          "[Removed Lines]",
          "105:         for day1, day2 in zip(EVENT_DATES_SORTED, EVENT_DATES_SORTED[1:])",
          "",
          "[Added Lines]",
          "106:         pytest.param(None, DagRunInfo.interval(START_DATE, START_DATE)),",
          "107:         pytest.param(",
          "108:             DataInterval(EVENT_DATES_SORTED[0], EVENT_DATES_SORTED[0]),",
          "109:             DagRunInfo.interval(START_DATE, START_DATE),",
          "110:         ),",
          "111:     ]",
          "112:     + [",
          "114:         for day1, day2 in zip(EVENT_DATES_SORTED[1:], EVENT_DATES_SORTED[2:])",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "118:         restriction=restriction,",
          "119:     )",
          "120:     assert next_info == expected_next_info",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "132: @pytest.mark.parametrize(",
          "133:     \"current_date\",",
          "134:     [",
          "135:         pytest.param(pendulum.DateTime(2021, 9, 1, tzinfo=utc), id=\"when-current-date-is-before-first-event\"),",
          "136:         pytest.param(pendulum.DateTime(2021, 9, 8, tzinfo=utc), id=\"when-current-date-is-in-the-middle\"),",
          "137:         pytest.param(pendulum.DateTime(2021, 12, 9, tzinfo=utc), id=\"when-current-date-is-after-last-event\"),",
          "138:     ],",
          "139: )",
          "140: @pytest.mark.parametrize(",
          "141:     \"last_automated_data_interval\",",
          "142:     [",
          "143:         pytest.param(None, id=\"first-run\"),",
          "144:         pytest.param(DataInterval(start=BEFORE_DATE, end=BEFORE_DATE), id=\"subsequent-run\"),",
          "145:     ],",
          "146: )",
          "147: def test_no_catchup_first_starts(",
          "148:     last_automated_data_interval: DataInterval | None,",
          "149:     current_date,",
          "150:     unrestricted_timetable: Timetable,",
          "151: ) -> None:",
          "152:     # we don't use the last_automated_data_interval here because it's always less than the first event",
          "153:     expected_date = max(current_date, START_DATE, EVENT_DATES_SORTED[0])",
          "154:     expected_info = None",
          "155:     if expected_date <= EVENT_DATES_SORTED[-1]:",
          "156:         expected_info = DagRunInfo.interval(start=expected_date, end=expected_date)",
          "158:     with time_machine.travel(current_date):",
          "159:         next_info = unrestricted_timetable.next_dagrun_info(",
          "160:             last_automated_data_interval=last_automated_data_interval,",
          "161:             restriction=TimeRestriction(earliest=START_DATE, latest=None, catchup=False),",
          "162:         )",
          "163:     assert next_info == expected_info",
          "166: def test_empty_timetable() -> None:",
          "167:     empty_timetable = EventsTimetable(event_dates=[])",
          "168:     next_info = empty_timetable.next_dagrun_info(",
          "169:         last_automated_data_interval=None,",
          "170:         restriction=TimeRestriction(earliest=START_DATE, latest=None, catchup=False),",
          "171:     )",
          "172:     assert next_info is None",
          "175: def test_empty_timetable_manual_run() -> None:",
          "176:     empty_timetable = EventsTimetable(event_dates=[])",
          "177:     manual_run_data_interval = empty_timetable.infer_manual_data_interval(run_after=START_DATE)",
          "178:     assert manual_run_data_interval == DataInterval(start=START_DATE, end=START_DATE)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "6d47a857436257ef9a478ddc83b578ccba05c9d6",
      "candidate_info": {
        "commit_hash": "6d47a857436257ef9a478ddc83b578ccba05c9d6",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/6d47a857436257ef9a478ddc83b578ccba05c9d6",
        "files": [
          "CONTRIBUTING.rst",
          "Dockerfile",
          "INSTALL",
          "airflow/utils/dot_renderer.py",
          "dev/breeze/src/airflow_breeze/global_constants.py",
          "docs/apache-airflow/extra-packages-ref.rst",
          "docs/docker-stack/build-arg-ref.rst",
          "docs/spelling_wordlist.txt",
          "images/breeze/output_prod-image_build.txt",
          "newsfragments/36647.significant.rst",
          "setup.cfg",
          "setup.py"
        ],
        "message": "Make `graphviz` dependency optional (#36647)\n\nThe `graphviz` dependency has been problematic as Airflow required\ndependency - especially for ARM-based installations. Graphviz\npackages require binary graphviz libraries - which is already a\nlimitation, but they also require to install graphviz Python\nbindings to be build and installed. This does not work for older\nLinux installation but - more importantly - when you try\nto install Graphviz libraries for Python 3.8, 3.9 for ARM M1\nMacBooks, the packages fail to install because Python bindings\ncompilation for M1 can only work for Python 3.10+.\n\nThere is not an easy solution for that except commenting out\ngraphviz dependency from setup.py, when you want to install Airflow\nfor Python 3.8, 3.9 for MacBook M1.\n\nHowever Graphviz is really used in two places:\n\n* when you want to render DAGs wia airflow CLI - either to an image\n  or directly to terminal (for terminals/systems supporting imgcat)\n\n* when you want to render ER diagram after you modified Airflow\n  models\n\nThe latter is a development-only feature, the former is production\nfeature, however it is a very niche one.\n\nThis PR turns rendering of the images in Airflow in optional feature\n(only working when graphviz python bindings are installed) and\neffectively turns graphviz into an optional extra (and removes it\nfrom requirements).\n\nThis is not a breaking change technically - the CLIs to render the\nDAGs is still there and IF you already have graphviz installed, it\nwill continue working as it did before. The only problem when it\ndoes not work is where you do not have graphviz installed for\nfresh installation and it will raise an error and inform that you need it.\n\nGraphviz will remain to be installed for most users:\n\n* the Airflow Image will still contain graphviz library, because\n  it is added there as extra\n* when previous version of Airflow has been installed already, then\n  graphviz library is already installed there and Airflow will\n  continue working as it did\n\nThe only change will be a new installation of new version of Airflow\nfrom the scratch, where graphviz will need to be specified as extra\nor installed separately in order to enable DAG rendering option.\n\nTaking into account this behaviour (which only requires to install\na graphviz package), this should not be considered as a breaking\nchange.\n\nExtracted from: #36537\n\n(cherry picked from commit 89f1737afb27f6e708c2e83e3d8e751d9a36f91e)",
        "before_after_code_files": [
          "airflow/utils/dot_renderer.py||airflow/utils/dot_renderer.py",
          "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py",
          "setup.cfg||setup.cfg",
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/dot_renderer.py||airflow/utils/dot_renderer.py": [
          "File: airflow/utils/dot_renderer.py -> airflow/utils/dot_renderer.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: \"\"\"Renderer DAG (tasks and dependencies) to the graphviz object.\"\"\"",
          "20: from __future__ import annotations",
          "22: from typing import TYPE_CHECKING, Any",
          "26: from airflow.exceptions import AirflowException",
          "27: from airflow.models.baseoperator import BaseOperator",
          "",
          "[Removed Lines]",
          "24: import graphviz",
          "",
          "[Added Lines]",
          "22: import warnings",
          "25: try:",
          "26:     import graphviz",
          "27: except ImportError:",
          "28:     warnings.warn(\"Could not import graphviz. Rendering graph to the graphical format will not be possible.\")",
          "29:     graphviz = None",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "151:     :param deps: List of DAG dependencies",
          "152:     :return: Graphviz object",
          "153:     \"\"\"",
          "154:     dot = graphviz.Digraph(graph_attr={\"rankdir\": \"LR\"})",
          "156:     for dag, dependencies in deps.items():",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "159:     if not graphviz:",
          "160:         raise AirflowException(",
          "161:             \"Could not import graphviz. Install the graphviz python package to fix this error.\"",
          "162:         )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "179:     :param tis: List of task instances",
          "180:     :return: Graphviz object",
          "181:     \"\"\"",
          "182:     dot = graphviz.Digraph(",
          "183:         dag.dag_id,",
          "184:         graph_attr={",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "191:     if not graphviz:",
          "192:         raise AirflowException(",
          "193:             \"Could not import graphviz. Install the graphviz python package to fix this error.\"",
          "194:         )",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py": [
          "File: dev/breeze/src/airflow_breeze/global_constants.py -> dev/breeze/src/airflow_breeze/global_constants.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "437:     \"ftp\",",
          "438:     \"google\",",
          "439:     \"google_auth\",",
          "440:     \"grpc\",",
          "441:     \"hashicorp\",",
          "442:     \"http\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "440:     \"graphviz\",",
          "",
          "---------------"
        ],
        "setup.cfg||setup.cfg": [
          "File: setup.cfg -> setup.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "108:     flask-wtf>=0.15",
          "109:     fsspec>=2023.10.0",
          "110:     google-re2>=1.0",
          "112:     gunicorn>=20.1.0",
          "113:     httpx",
          "114:     importlib_metadata>=1.7;python_version<\"3.9\"",
          "",
          "[Removed Lines]",
          "111:     graphviz>=0.12",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "318: ]",
          "319: doc_gen = [",
          "320:     \"eralchemy2\",",
          "321: ]",
          "322: flask_appbuilder_oauth = [",
          "323:     \"authlib>=1.0.0\",",
          "324:     # The version here should be upgraded at the same time as flask-appbuilder in setup.cfg",
          "325:     \"flask-appbuilder[oauth]==4.3.10\",",
          "326: ]",
          "327: kerberos = [",
          "328:     \"pykerberos>=1.1.13\",",
          "329:     \"requests_kerberos>=0.10.0\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "321:     \"graphviz>=0.12\",",
          "328: graphviz = [\"graphviz>=0.12\"]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "589:     \"deprecated_api\": deprecated_api,",
          "590:     \"github_enterprise\": flask_appbuilder_oauth,",
          "591:     \"google_auth\": flask_appbuilder_oauth,",
          "592:     \"kerberos\": kerberos,",
          "593:     \"ldap\": ldap,",
          "594:     \"leveldb\": leveldb,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "594:     \"graphviz\": graphviz,",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5da2e0a4fd368e7cd520627e049d67b405c3b36f",
      "candidate_info": {
        "commit_hash": "5da2e0a4fd368e7cd520627e049d67b405c3b36f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/5da2e0a4fd368e7cd520627e049d67b405c3b36f",
        "files": [
          "Dockerfile",
          "docker_tests/test_prod_image.py",
          "docs/docker-stack/changelog.rst"
        ],
        "message": "Use `mariadb` by default when build final prod image (#36716)\n\n(cherry picked from commit 11ec4100b3ffb86e15b43a8dde5f53f07d404508)",
        "before_after_code_files": [
          "docker_tests/test_prod_image.py||docker_tests/test_prod_image.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "docker_tests/test_prod_image.py||docker_tests/test_prod_image.py": [
          "File: docker_tests/test_prod_image.py -> docker_tests/test_prod_image.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "158:         \"grpc\": [\"grpc\", \"google.auth\", \"google_auth_httplib2\"],",
          "159:         \"hashicorp\": [\"hvac\"],",
          "160:         \"ldap\": [\"ldap\"],",
          "161:         \"postgres\": [\"psycopg2\"],",
          "162:         \"pyodbc\": [\"pyodbc\"],",
          "163:         \"redis\": [\"redis\"],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "161:         \"mysql\": [\"MySQLdb\", *([\"mysql\"] if bool(find_spec(\"mysql\")) else [])],",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "167:         \"statsd\": [\"statsd\"],",
          "168:         \"virtualenv\": [\"virtualenv\"],",
          "169:     }",
          "173:     @pytest.mark.skipif(os.environ.get(\"TEST_SLIM_IMAGE\") == \"true\", reason=\"Skipped with slim image\")",
          "174:     @pytest.mark.parametrize(\"package_name,import_names\", PACKAGE_IMPORTS.items())",
          "",
          "[Removed Lines]",
          "170:     if bool(find_spec(\"mysql\")):",
          "171:         PACKAGE_IMPORTS[\"mysql\"] = [\"mysql\"]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "96e01491192e6fdae04cd61a215d05a049f5571f",
      "candidate_info": {
        "commit_hash": "96e01491192e6fdae04cd61a215d05a049f5571f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/96e01491192e6fdae04cd61a215d05a049f5571f",
        "files": [
          "docs/apache-airflow/core-concepts/dags.rst"
        ],
        "message": "Update dags.rst with information on DAG pausing (#36540)\n\nAdded information on DAG behavior when its paused.\n\n(cherry picked from commit ef9e8b0e7a5d6dd932f18cd1aa48e1595821338a)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "975ddce2ef214794befffadb430fdb1199aeabe0",
      "candidate_info": {
        "commit_hash": "975ddce2ef214794befffadb430fdb1199aeabe0",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/975ddce2ef214794befffadb430fdb1199aeabe0",
        "files": [
          "airflow/providers/apache/beam/provider.yaml",
          "generated/provider_dependencies.json",
          "setup.py"
        ],
        "message": "Get rid of pyarrow-hotfix for CVE-2023-47248 (#36697)\n\nThe #35650 introduced a hotfix for Pyarrow CVE-2023-47248. So far\nwe have been blocked from removing it by Apache Beam that limited\nAirflow from bumping pyarrow to a version that was not vulnerable.\n\nThis is now possible since Apache Beam relesed 2.53.0 version on\n4th of January 2023 that allows to use non-vulnerable pyarrow.\n\nWe are now bumping both Pyarrow and Beam minimum versions to\nreflect that and remove pyarrow hotfix.\n\n(cherry picked from commit d105c7115f56f88d48a2888484a0ed7d1c01576f)",
        "before_after_code_files": [
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "349: otel = [\"opentelemetry-exporter-prometheus\"]",
          "350: pandas = [",
          "351:     \"pandas>=0.17.1\",",
          "356: ]",
          "357: password = [",
          "358:     \"bcrypt>=2.0.0\",",
          "",
          "[Removed Lines]",
          "352:     # Use pyarrow-hotfix to fix https://nvd.nist.gov/vuln/detail/CVE-2023-47248.",
          "353:     # We should remove it once Apache Beam frees us to upgrade to pyarrow 14.0.1",
          "354:     \"pyarrow-hotfix\",",
          "355:     \"pyarrow>=9.0.0\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b7dab91346e6bf2fc4a01ec6d52db82dedb49dcf",
      "candidate_info": {
        "commit_hash": "b7dab91346e6bf2fc4a01ec6d52db82dedb49dcf",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b7dab91346e6bf2fc4a01ec6d52db82dedb49dcf",
        "files": [
          "docs/apache-airflow/core-concepts/overview.rst",
          "docs/apache-airflow/img/diagram_basic_airflow_architecture.md5sum",
          "docs/apache-airflow/img/diagram_basic_airflow_architecture.png",
          "docs/apache-airflow/img/diagram_basic_airflow_architecture.py",
          "docs/apache-airflow/img/diagram_dag_processor_airflow_architecture.md5sum",
          "docs/apache-airflow/img/diagram_dag_processor_airflow_architecture.png",
          "docs/apache-airflow/img/diagram_dag_processor_airflow_architecture.py",
          "docs/apache-airflow/img/diagram_distributed_airflow_architecture.md5sum",
          "docs/apache-airflow/img/diagram_distributed_airflow_architecture.png",
          "docs/apache-airflow/img/diagram_distributed_airflow_architecture.py",
          "docs/diagrams/database.png",
          "docs/diagrams/multiple_files.png",
          "docs/diagrams/packages.png"
        ],
        "message": "Improve and add more complete description in the architecture diagrams (#36513)\n\nWhen it comes to access and management plugins management follows\ndifferent pattern than DAG files. While DAG files can (and should) be\nmodified by DAG authors, the whole idea of Plugins was to make it only\npossible to modify plugins folder (and installl plugin-enabled packages)\nby the Deploymenet Managers, not DAG authors.\n\nThe difference is quite important because even in a simplest\ninstallation, airflow webserver never needs to access DAG files, while\nit should be able to access plugins.\n\nThis is even more profound in the environment (leading in the future to\nmulti-tenant deployments) plugins are not 'per-tenant\" - they must be\ninstalled and managed by deployment manager, because those plugins can\nbe used by Airflow Webservers.\n\nIn the future we might want to make distinction between these two\ndifferent types of plugins, because theorethically it would be possible\nto distingquish \"scheduler, worker & triggerer\" plugins from the\n\"webserver\" plugins - however we do not have such disctinction today and\nwhoever manages plugins folder is impacting both webserver and workers.\n\nThis change also re-adds the \"basic\" architecture which is targetted\nas single-user and single machine deployment and presents it as the\nfirst architecture that the user encounters - which might make it more\ndigestible, while it also explains tha this is a simplified architecture\nand is followed by more complete and complex deployment scenarios\ninvolving distributed architecture, different user roles and security\nboundaries.\n\n(cherry picked from commit c47dcc59e5a0766d6b390bc8ceafc8ab6b762c88)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "9cf8a9a27e70098036d40094b9e083f3309e0f4b",
      "candidate_info": {
        "commit_hash": "9cf8a9a27e70098036d40094b9e083f3309e0f4b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/9cf8a9a27e70098036d40094b9e083f3309e0f4b",
        "files": [
          "generated/provider_metadata.json"
        ],
        "message": "Update providers metadata 2024-01-10 (#36718)\n\n(cherry picked from commit c59f8dee7802c03634faf8412dfb2bf122d7abc6)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "a51887bcb777bb2cdde3146d734b2e6b00c82175",
      "candidate_info": {
        "commit_hash": "a51887bcb777bb2cdde3146d734b2e6b00c82175",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a51887bcb777bb2cdde3146d734b2e6b00c82175",
        "files": [
          "RELEASE_NOTES.rst"
        ],
        "message": "Update the release date of Airflow 2.8.0 (#36321)\n\n2.8.0 was later released on 18th\n\n(cherry picked from commit c09a64c9c77cba13507b99c4054bdef653282027)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "c836c4c6f7be5880700aa1df3a3439052c76dbb3",
      "candidate_info": {
        "commit_hash": "c836c4c6f7be5880700aa1df3a3439052c76dbb3",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c836c4c6f7be5880700aa1df3a3439052c76dbb3",
        "files": [
          ".pre-commit-config.yaml",
          "STATIC_CODE_CHECKS.rst",
          "TESTING.rst",
          "dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "images/breeze/output_static-checks.svg",
          "images/breeze/output_static-checks.txt",
          "scripts/ci/docker-compose/integration-cassandra.yml",
          "scripts/ci/docker-compose/integration-celery.yml",
          "scripts/ci/docker-compose/integration-kafka.yml",
          "scripts/ci/docker-compose/integration-kerberos.yml",
          "scripts/ci/docker-compose/integration-mongo.yml",
          "scripts/ci/docker-compose/integration-openlineage.yml",
          "scripts/ci/docker-compose/integration-otel.yml",
          "scripts/ci/docker-compose/integration-pinot.yml",
          "scripts/ci/docker-compose/integration-statsd.yml",
          "scripts/ci/docker-compose/integration-trino.yml",
          "scripts/ci/pre_commit/pre_commit_check_integrations_list.py"
        ],
        "message": "Add pre-commit check to check integrations table. (#36497)\n\n(cherry picked from commit 9d1eba087b488d473f8e3a3b12df63d83c7364e8)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/pre_commit_ids.py||dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "scripts/ci/pre_commit/pre_commit_check_integrations_list.py||scripts/ci/pre_commit/pre_commit_check_integrations_list.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/pre_commit_ids.py||dev/breeze/src/airflow_breeze/pre_commit_ids.py": [
          "File: dev/breeze/src/airflow_breeze/pre_commit_ids.py -> dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "54:     \"check-hooks-apply\",",
          "55:     \"check-incorrect-use-of-LoggingMixin\",",
          "56:     \"check-init-decorator-arguments\",",
          "57:     \"check-lazy-logging\",",
          "58:     \"check-links-to-example-dags-do-not-use-hardcoded-versions\",",
          "59:     \"check-merge-conflict\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "57:     \"check-integrations-list-consistent\",",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_check_integrations_list.py||scripts/ci/pre_commit/pre_commit_check_integrations_list.py": [
          "File: scripts/ci/pre_commit/pre_commit_check_integrations_list.py -> scripts/ci/pre_commit/pre_commit_check_integrations_list.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python",
          "2: #",
          "3: # Licensed to the Apache Software Foundation (ASF) under one",
          "4: # or more contributor license agreements.  See the NOTICE file",
          "5: # distributed with this work for additional information",
          "6: # regarding copyright ownership.  The ASF licenses this file",
          "7: # to you under the Apache License, Version 2.0 (the",
          "8: # \"License\"); you may not use this file except in compliance",
          "9: # with the License.  You may obtain a copy of the License at",
          "10: #",
          "11: #   http://www.apache.org/licenses/LICENSE-2.0",
          "12: #",
          "13: # Unless required by applicable law or agreed to in writing,",
          "14: # software distributed under the License is distributed on an",
          "15: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "16: # KIND, either express or implied.  See the License for the",
          "17: # specific language governing permissions and limitations",
          "18: # under the License.",
          "19: \"\"\"",
          "20: Module to check integration tests are listed in documentation.",
          "22: Compare the contents of the integrations table and the docker-compose",
          "23: integration files, if there is a mismatch, the table is generated.",
          "24: \"\"\"",
          "25: from __future__ import annotations",
          "27: import re",
          "28: import sys",
          "29: from pathlib import Path",
          "30: from typing import Any",
          "32: import yaml",
          "34: # make sure common_precommit_utils is imported",
          "35: sys.path.insert(0, str(Path(__file__).parent.resolve()))",
          "36: from common_precommit_utils import (",
          "37:     AIRFLOW_SOURCES_ROOT_PATH,",
          "38:     console,",
          "39:     insert_documentation,",
          "40: )",
          "41: from tabulate import tabulate",
          "43: DOCUMENTATION_PATH = AIRFLOW_SOURCES_ROOT_PATH / \"TESTING.rst\"",
          "44: INTEGRATION_TESTS_PATH = AIRFLOW_SOURCES_ROOT_PATH / \"scripts\" / \"ci\" / \"docker-compose\"",
          "45: INTEGRATION_TEST_PREFIX = \"integration-*.yml\"",
          "46: DOCS_MARKER_START = \".. BEGIN AUTO-GENERATED INTEGRATION LIST\"",
          "47: DOCS_MARKER_END = \".. END AUTO-GENERATED INTEGRATION LIST\"",
          "48: _LIST_MATCH = r\"\\|[^|\\n]+\"",
          "51: def get_ci_integrations(",
          "52:     tests_path: Path = INTEGRATION_TESTS_PATH,",
          "53:     integration_prefix: str = INTEGRATION_TEST_PREFIX,",
          "54: ) -> dict[str, Path]:",
          "55:     \"\"\"Get list of integrations from matching filenames.\"\"\"",
          "56:     if not tests_path.is_dir() and tests_path.exists():",
          "57:         console.print(f\"[red]Bad tests path: {tests_path}. [/]\")",
          "58:         sys.exit(1)",
          "60:     integrations_files = [_i for _i in tests_path.glob(integration_prefix)]",
          "62:     if len(integrations_files) == 0:",
          "63:         console.print(",
          "64:             f\"[red]No integrations found.\"",
          "65:             f\"Pattern '{integration_prefix}' did not match any files under {tests_path}. [/]\"",
          "66:         )",
          "67:         sys.exit(1)",
          "69:     # parse into list of ids",
          "70:     integrations = {}",
          "71:     for _i in integrations_files:",
          "72:         try:",
          "73:             _key = _i.stem.split(\"-\")[1]",
          "74:             integrations[_key] = _i",
          "75:         except IndexError:",
          "76:             console.print(f\"[red]Tried to parse {_i.stem}, but did not contain '-' separator. [/]\")",
          "77:             continue",
          "79:     return integrations",
          "82: def get_docs_integrations(docs_path: Path = DOCUMENTATION_PATH):",
          "83:     \"\"\"Get integrations listed in docs.\"\"\"",
          "84:     table_lines = []",
          "85:     _list_start_line = None",
          "86:     with open(docs_path, encoding=\"utf8\") as f:",
          "87:         for line_n, line in enumerate(f):",
          "88:             if DOCS_MARKER_END in line:",
          "89:                 break",
          "90:             if DOCS_MARKER_START in line:",
          "91:                 _list_start_line = line_n",
          "92:             if _list_start_line is None:",
          "93:                 continue",
          "94:             if line_n > _list_start_line:",
          "95:                 table_lines.append(line)",
          "97:     if len(table_lines) == 0:",
          "98:         console.print(\"[red]No integrations table in docs.[/]\")",
          "99:         sys.exit(1)",
          "101:     table_cells = []",
          "102:     for line in table_lines:",
          "103:         m = re.findall(_LIST_MATCH, line)",
          "104:         if len(m) == 0:",
          "105:             continue",
          "106:         table_cells.append(m[0].strip(\"|\").strip())",
          "108:     def _list_matcher(j):",
          "109:         \"\"\"Filter callable to exclude header and empty cells.\"\"\"",
          "110:         if len(j) == 0:",
          "111:             return False",
          "112:         elif j in [\"Description\", \"Identifier\"]:",
          "113:             return False",
          "114:         else:",
          "115:             return True",
          "117:     table_cells = list(filter(_list_matcher, table_cells))",
          "118:     return table_cells",
          "121: def update_integration_tests_array(contents: dict[str, list[str]]):",
          "122:     \"\"\"Generate docs table.\"\"\"",
          "123:     rows = []",
          "124:     sorted_contents = dict(sorted(contents.items()))",
          "125:     for integration, description in sorted_contents.items():",
          "126:         formatted_hook_description = (",
          "127:             description[0] if len(description) == 1 else \"* \" + \"\\n* \".join(description)",
          "128:         )",
          "129:         rows.append((integration, formatted_hook_description))",
          "130:     formatted_table = \"\\n\" + tabulate(rows, tablefmt=\"grid\", headers=(\"Identifier\", \"Description\")) + \"\\n\\n\"",
          "131:     insert_documentation(",
          "132:         file_path=AIRFLOW_SOURCES_ROOT_PATH / \"TESTING.rst\",",
          "133:         content=formatted_table.splitlines(keepends=True),",
          "134:         header=DOCS_MARKER_START,",
          "135:         footer=DOCS_MARKER_END,",
          "136:     )",
          "139: def print_diff(source, target, msg):",
          "140:     difference = source - target",
          "141:     if difference:",
          "142:         console.print(msg)",
          "143:         for i in difference:",
          "144:             console.print(f\"[red]\\t- {i}[/]\")",
          "145:     return list(difference)",
          "148: def _get_breeze_description(parsed_compose: dict[str, Any], label_key: str = \"breeze.description\"):",
          "149:     \"\"\"Extract all breeze.description labels per image.\"\"\"",
          "150:     image_label_map = {}",
          "151:     # possible key error handled outside",
          "152:     for _img_name, img in parsed_compose[\"services\"].items():",
          "153:         try:",
          "154:             for _label_name, label in img[\"labels\"].items():",
          "155:                 if _label_name == label_key:",
          "156:                     image_label_map[_img_name] = label",
          "157:         except KeyError:",
          "158:             # service has no 'lables' entry",
          "159:             continue",
          "160:     return image_label_map",
          "163: def get_integration_descriptions(integrations: dict[str, Path]) -> dict[str, list[Any]]:",
          "164:     \"\"\"Pull breeze description from docker-compose files.\"\"\"",
          "165:     table = {}",
          "166:     for integration, path in integrations.items():",
          "167:         with open(path) as f:",
          "168:             _compose = yaml.safe_load(f)",
          "170:         try:",
          "171:             _labels = _get_breeze_description(_compose)",
          "172:         except KeyError:",
          "173:             console.print(f\"[red]No 'services' entry in compose file {path}.[/]\")",
          "174:             sys.exit(1)",
          "175:         table[integration] = list(_labels.values())",
          "176:     return table",
          "179: def main():",
          "180:     docs_integrations = get_docs_integrations()",
          "181:     ci_integrations = get_ci_integrations()",
          "183:     if len(ci_integrations) == 0:",
          "184:         console.print(\"[red]No integrations found.[/]\")",
          "185:         sys.exit(1)",
          "187:     _ci_items = set(ci_integrations)",
          "188:     _docs_items = set(docs_integrations)",
          "189:     diff = []",
          "190:     diff.append(print_diff(_ci_items, _docs_items, \"[red]Found in ci files, but not in docs: [/]\"))",
          "191:     diff.append(print_diff(_docs_items, _ci_items, \"[red]Found in docs, but not in ci files: [/]\"))",
          "192:     if diff:",
          "193:         console.print(",
          "194:             \"[yellow]Regenerating documentation table. Don't forget to review and commit possible changes.[/]\"",
          "195:         )",
          "197:     table_contents = get_integration_descriptions(ci_integrations)",
          "198:     update_integration_tests_array(table_contents)",
          "201: if __name__ == \"__main__\":",
          "202:     main()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "56ecbffc40183834b941cd84b861f997e580a7d3",
      "candidate_info": {
        "commit_hash": "56ecbffc40183834b941cd84b861f997e580a7d3",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/56ecbffc40183834b941cd84b861f997e580a7d3",
        "files": [
          "airflow/serialization/serializers/datetime.py",
          "tests/serialization/serializers/test_serializers.py"
        ],
        "message": "Stop serializing timezone-naive datetime to timezone-aware dateime with UTC tz (#36379)\n\n(cherry picked from commit 69f556dd136598662db9e87478584a3c96362fc9)",
        "before_after_code_files": [
          "airflow/serialization/serializers/datetime.py||airflow/serialization/serializers/datetime.py",
          "tests/serialization/serializers/test_serializers.py||tests/serialization/serializers/test_serializers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/serialization/serializers/datetime.py||airflow/serialization/serializers/datetime.py": [
          "File: airflow/serialization/serializers/datetime.py -> airflow/serialization/serializers/datetime.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24:     serialize as serialize_timezone,",
          "25: )",
          "26: from airflow.utils.module_loading import qualname",
          "29: if TYPE_CHECKING:",
          "30:     import datetime",
          "",
          "[Removed Lines]",
          "27: from airflow.utils.timezone import convert_to_utc, is_naive",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "46:     if isinstance(o, datetime):",
          "47:         qn = qualname(o)",
          "53:         return {TIMESTAMP: o.timestamp(), TIMEZONE: tz}, qn, __version__, True",
          "",
          "[Removed Lines]",
          "48:         if is_naive(o):",
          "49:             o = convert_to_utc(o)",
          "51:         tz = serialize_timezone(o.tzinfo)",
          "",
          "[Added Lines]",
          "48:         tz = serialize_timezone(o.tzinfo) if o.tzinfo else None",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "83:             else:",
          "84:                 tz = timezone(data[TIMEZONE])",
          "85:         else:",
          "88:     if classname == qualname(datetime.datetime) and isinstance(data, dict):",
          "89:         return datetime.datetime.fromtimestamp(float(data[TIMESTAMP]), tz=tz)",
          "",
          "[Removed Lines]",
          "86:             tz = deserialize_timezone(data[TIMEZONE][1], data[TIMEZONE][2], data[TIMEZONE][0])",
          "",
          "[Added Lines]",
          "83:             tz = (",
          "84:                 deserialize_timezone(data[TIMEZONE][1], data[TIMEZONE][2], data[TIMEZONE][0])",
          "85:                 if data[TIMEZONE]",
          "86:                 else None",
          "87:             )",
          "",
          "---------------"
        ],
        "tests/serialization/serializers/test_serializers.py||tests/serialization/serializers/test_serializers.py": [
          "File: tests/serialization/serializers/test_serializers.py -> tests/serialization/serializers/test_serializers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "84:         d = deserialize(s)",
          "85:         assert i.timestamp() == d.timestamp()",
          "87:     def test_deserialize_datetime_v1(self):",
          "88:         s = {",
          "89:             \"__classname__\": \"pendulum.datetime.DateTime\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "87:         i = datetime.datetime.now()",
          "88:         s = serialize(i)",
          "89:         d = deserialize(s)",
          "90:         assert i.timestamp() == d.timestamp()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4b4fd21037640c10ef99e635ef6d8eec98839e74",
      "candidate_info": {
        "commit_hash": "4b4fd21037640c10ef99e635ef6d8eec98839e74",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4b4fd21037640c10ef99e635ef6d8eec98839e74",
        "files": [
          "airflow/utils/log/file_task_handler.py"
        ],
        "message": "Restore function scoped httpx import in file_task_handler for perf (#36753)\n\n(cherry picked from commit c792b259699550d06054984f1643bb62df5ea37d)",
        "before_after_code_files": [
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: from typing import TYPE_CHECKING, Any, Callable, Iterable",
          "30: from urllib.parse import urljoin",
          "33: import pendulum",
          "35: from airflow.configuration import conf",
          "",
          "[Removed Lines]",
          "32: import httpx",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "81: def _fetch_logs_from_service(url, log_relative_path):",
          "82:     from airflow.utils.jwt_signer import JWTSigner",
          "84:     timeout = conf.getint(\"webserver\", \"log_fetch_timeout_sec\", fallback=None)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "81:     # Import occurs in function scope for perf. Ref: https://github.com/apache/airflow/pull/21438",
          "82:     import httpx",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "557:                 messages.append(f\"Found logs served from host {url}\")",
          "558:                 logs.append(response.text)",
          "559:         except Exception as e:",
          "561:                 messages.append(self.inherits_from_empty_operator_log_message)",
          "562:             else:",
          "563:                 messages.append(f\"Could not read served logs: {e}\")",
          "",
          "[Removed Lines]",
          "560:             if isinstance(e, httpx.UnsupportedProtocol) and ti.task.inherits_from_empty_operator is True:",
          "",
          "[Added Lines]",
          "562:             from httpx import UnsupportedProtocol",
          "564:             if isinstance(e, UnsupportedProtocol) and ti.task.inherits_from_empty_operator is True:",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "be1b269a0e11a5aea51cb3f0ec9da09541b0955a",
      "candidate_info": {
        "commit_hash": "be1b269a0e11a5aea51cb3f0ec9da09541b0955a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/be1b269a0e11a5aea51cb3f0ec9da09541b0955a",
        "files": [
          "docs/apache-airflow/administration-and-deployment/logging-monitoring/metrics.rst"
        ],
        "message": "metrics tagging documentation (#36627)\n\n* metrics tagging documentation\n\n(cherry picked from commit 667b842632fbee984b940a3b8b5a1f0bb3749a0f)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "fcd235777ec8df4dd7d43ce56cfec3762c79dbaf",
      "candidate_info": {
        "commit_hash": "fcd235777ec8df4dd7d43ce56cfec3762c79dbaf",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/fcd235777ec8df4dd7d43ce56cfec3762c79dbaf",
        "files": [
          "airflow/utils/log/file_processor_handler.py",
          "tests/utils/log/test_file_processor_handler.py"
        ],
        "message": "Create latest log dir symlink as relative link (#36019)\n\nSo `logs/**/*` is self-contained and the symlink doesn't break for\nexporting, build artifact, etc\n\n(cherry picked from commit 17e91727b29448e46470a8dd4b5909a0bdf38eb2)",
        "before_after_code_files": [
          "airflow/utils/log/file_processor_handler.py||airflow/utils/log/file_processor_handler.py",
          "tests/utils/log/test_file_processor_handler.py||tests/utils/log/test_file_processor_handler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/log/file_processor_handler.py||airflow/utils/log/file_processor_handler.py": [
          "File: airflow/utils/log/file_processor_handler.py -> airflow/utils/log/file_processor_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "117:         log_directory = self._get_log_directory()",
          "118:         latest_log_directory_path = os.path.join(self.base_log_folder, \"latest\")",
          "119:         if os.path.isdir(log_directory):",
          "120:             try:",
          "121:                 # if symlink exists but is stale, update it",
          "122:                 if os.path.islink(latest_log_directory_path):",
          "124:                         os.unlink(latest_log_directory_path)",
          "126:                 elif os.path.isdir(latest_log_directory_path) or os.path.isfile(latest_log_directory_path):",
          "127:                     logging.warning(",
          "128:                         \"%s already exists as a dir/file. Skip creating symlink.\", latest_log_directory_path",
          "129:                     )",
          "130:                 else:",
          "132:             except OSError:",
          "133:                 logging.warning(\"OSError while attempting to symlink the latest log directory\")",
          "",
          "[Removed Lines]",
          "123:                     if os.readlink(latest_log_directory_path) != log_directory:",
          "125:                         os.symlink(log_directory, latest_log_directory_path)",
          "131:                     os.symlink(log_directory, latest_log_directory_path)",
          "",
          "[Added Lines]",
          "120:             rel_link_target = Path(log_directory).relative_to(Path(latest_log_directory_path).parent)",
          "124:                     if os.path.realpath(latest_log_directory_path) != log_directory:",
          "126:                         os.symlink(rel_link_target, latest_log_directory_path)",
          "132:                     os.symlink(rel_link_target, latest_log_directory_path)",
          "",
          "---------------"
        ],
        "tests/utils/log/test_file_processor_handler.py||tests/utils/log/test_file_processor_handler.py": [
          "File: tests/utils/log/test_file_processor_handler.py -> tests/utils/log/test_file_processor_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "80:         with time_machine.travel(date1, tick=False):",
          "81:             handler.set_context(filename=os.path.join(self.dag_dir, \"log1\"))",
          "82:             assert os.path.islink(link)",
          "84:             assert os.path.exists(os.path.join(link, \"log1\"))",
          "86:         with time_machine.travel(date2, tick=False):",
          "87:             handler.set_context(filename=os.path.join(self.dag_dir, \"log2\"))",
          "88:             assert os.path.islink(link)",
          "90:             assert os.path.exists(os.path.join(link, \"log2\"))",
          "92:     def test_symlink_latest_log_directory_exists(self):",
          "",
          "[Removed Lines]",
          "83:             assert os.path.basename(os.readlink(link)) == date1",
          "89:             assert os.path.basename(os.readlink(link)) == date2",
          "",
          "[Added Lines]",
          "83:             assert os.path.basename(os.path.realpath(link)) == date1",
          "89:             assert os.path.basename(os.path.realpath(link)) == date2",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1dc0b2ec8401533a87c655e97fe37f04a1319a7e",
      "candidate_info": {
        "commit_hash": "1dc0b2ec8401533a87c655e97fe37f04a1319a7e",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/1dc0b2ec8401533a87c655e97fe37f04a1319a7e",
        "files": [
          "airflow/config_templates/config.yml",
          "airflow/www/app.py"
        ],
        "message": "Add flask config: `MAX_CONTENT_LENGTH` (#36401)\n\n(cherry picked from commit 84063e74fb2b0dd3a8308ff4170cb3e7236cf51e)",
        "before_after_code_files": [
          "airflow/www/app.py||airflow/www/app.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/app.py||airflow/www/app.py": [
          "File: airflow/www/app.py -> airflow/www/app.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "76:     flask_app.config[\"PERMANENT_SESSION_LIFETIME\"] = timedelta(minutes=settings.get_session_lifetime_config())",
          "78:     webserver_config = conf.get_mandatory_value(\"webserver\", \"config_file\")",
          "79:     # Enable customizations in webserver_config.py to be applied via Flask.current_app.",
          "80:     with flask_app.app_context():",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "78:     flask_app.config[\"MAX_CONTENT_LENGTH\"] = conf.getfloat(\"webserver\", \"allowed_payload_size\") * 1024 * 1024",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "842fd3b7fae758b53a2331f2bce87dc055bd174b",
      "candidate_info": {
        "commit_hash": "842fd3b7fae758b53a2331f2bce87dc055bd174b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/842fd3b7fae758b53a2331f2bce87dc055bd174b",
        "files": [
          "airflow/jobs/backfill_job_runner.py"
        ],
        "message": "Refactor _manage_executor_state by refreshing TIs in batch (#36418)\n\n* Refactor _manage_executor_state by refreshing TIs in batch\n\n* Use a short key without retry number\n\n(cherry picked from commit 9d45db9e2cca2ad04db72f7e0712c478e5a8e1f1)",
        "before_after_code_files": [
          "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py": [
          "File: airflow/jobs/backfill_job_runner.py -> airflow/jobs/backfill_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import attr",
          "24: import pendulum",
          "26: from sqlalchemy.exc import OperationalError",
          "27: from sqlalchemy.orm.session import make_transient",
          "28: from tabulate import tabulate",
          "",
          "[Removed Lines]",
          "25: from sqlalchemy import select, update",
          "",
          "[Added Lines]",
          "25: from sqlalchemy import select, tuple_, update",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "264:         :return: An iterable of expanded TaskInstance per MappedTask",
          "265:         \"\"\"",
          "266:         executor = self.job.executor",
          "270:             state, info = value",
          "272:                 self.log.warning(\"%s state %s not in running=%s\", key, state, running.values())",
          "273:                 continue",
          "278:             self.log.debug(\"Executor state: %s task %s\", state, ti)",
          "",
          "[Removed Lines]",
          "268:         # TODO: query all instead of refresh from db",
          "269:         for key, value in list(executor.get_event_buffer().items()):",
          "271:             if key not in running:",
          "275:             ti = running[key]",
          "276:             ti.refresh_from_db()",
          "",
          "[Added Lines]",
          "267:         # list of tuples (dag_id, task_id, execution_date, map_index) of running tasks in executor",
          "268:         buffered_events = list(executor.get_event_buffer().items())",
          "269:         running_tis_ids = [",
          "270:             (key.dag_id, key.task_id, key.run_id, key.map_index)",
          "271:             for key, _ in buffered_events",
          "272:             if key in running",
          "273:         ]",
          "274:         # list of TaskInstance of running tasks in executor (refreshed from db in batch)",
          "275:         refreshed_running_tis = session.scalars(",
          "276:             select(TaskInstance).where(",
          "277:                 tuple_(",
          "278:                     TaskInstance.dag_id,",
          "279:                     TaskInstance.task_id,",
          "280:                     TaskInstance.run_id,",
          "281:                     TaskInstance.map_index,",
          "282:                 ).in_(running_tis_ids)",
          "283:             )",
          "284:         ).all()",
          "285:         # dict of refreshed TaskInstance by key to easily find them",
          "286:         refreshed_running_tis_dict = {",
          "287:             (ti.dag_id, ti.task_id, ti.run_id, ti.map_index): ti for ti in refreshed_running_tis",
          "288:         }",
          "290:         for key, value in buffered_events:",
          "292:             ti_key = (key.dag_id, key.task_id, key.run_id, key.map_index)",
          "293:             if ti_key not in refreshed_running_tis_dict:",
          "297:             ti = refreshed_running_tis_dict[ti_key]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "682c43d474209fe8dfea9e6974851f2970758735",
      "candidate_info": {
        "commit_hash": "682c43d474209fe8dfea9e6974851f2970758735",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/682c43d474209fe8dfea9e6974851f2970758735",
        "files": [
          "dev/airflow-github"
        ],
        "message": "Optimise getting changelogs in airflow-github (#36548)\n\n(cherry picked from commit 142c7376c6d6566938ced345e97a1e2442e2fd32)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "4c613f827aa992ae5aff2d074eff16338ff5de54",
      "candidate_info": {
        "commit_hash": "4c613f827aa992ae5aff2d074eff16338ff5de54",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4c613f827aa992ae5aff2d074eff16338ff5de54",
        "files": [
          "airflow/hooks/filesystem.py",
          "airflow/hooks/package_index.py",
          "airflow/hooks/subprocess.py"
        ],
        "message": "Provide the logger_name param to base hook in order to override the logger name (#36674)\n\n(cherry picked from commit 8e8c080050c374b47fd0e6ffb8cb0a27adbca055)",
        "before_after_code_files": [
          "airflow/hooks/filesystem.py||airflow/hooks/filesystem.py",
          "airflow/hooks/package_index.py||airflow/hooks/package_index.py",
          "airflow/hooks/subprocess.py||airflow/hooks/subprocess.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/hooks/filesystem.py||airflow/hooks/filesystem.py": [
          "File: airflow/hooks/filesystem.py -> airflow/hooks/filesystem.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "59:             \"placeholders\": {},",
          "60:         }",
          "64:         conn = self.get_connection(fs_conn_id)",
          "65:         self.basepath = conn.extra_dejson.get(\"path\", \"\")",
          "66:         self.conn = conn",
          "",
          "[Removed Lines]",
          "62:     def __init__(self, fs_conn_id: str = default_conn_name):",
          "63:         super().__init__()",
          "",
          "[Added Lines]",
          "62:     def __init__(self, fs_conn_id: str = default_conn_name, **kwargs):",
          "63:         super().__init__(**kwargs)",
          "",
          "---------------"
        ],
        "airflow/hooks/package_index.py||airflow/hooks/package_index.py": [
          "File: airflow/hooks/package_index.py -> airflow/hooks/package_index.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "33:     conn_type = \"package_index\"",
          "34:     hook_name = \"Package Index (Python)\"",
          "38:         self.pi_conn_id = pi_conn_id",
          "39:         self.conn = None",
          "",
          "[Removed Lines]",
          "36:     def __init__(self, pi_conn_id: str = default_conn_name) -> None:",
          "37:         super().__init__()",
          "",
          "[Added Lines]",
          "36:     def __init__(self, pi_conn_id: str = default_conn_name, **kwargs) -> None:",
          "37:         super().__init__(**kwargs)",
          "",
          "---------------"
        ],
        "airflow/hooks/subprocess.py||airflow/hooks/subprocess.py": [
          "File: airflow/hooks/subprocess.py -> airflow/hooks/subprocess.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: class SubprocessHook(BaseHook):",
          "32:     \"\"\"Hook for running processes with the ``subprocess`` module.\"\"\"",
          "35:         self.sub_process: Popen[bytes] | None = None",
          "38:     def run_command(",
          "39:         self,",
          "",
          "[Removed Lines]",
          "34:     def __init__(self) -> None:",
          "36:         super().__init__()",
          "",
          "[Added Lines]",
          "34:     def __init__(self, **kwargs) -> None:",
          "36:         super().__init__(**kwargs)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e104aeb83016498784bedf16ec6bdf0d358fdfd5",
      "candidate_info": {
        "commit_hash": "e104aeb83016498784bedf16ec6bdf0d358fdfd5",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e104aeb83016498784bedf16ec6bdf0d358fdfd5",
        "files": [
          "docs/apache-airflow/core-concepts/objectstorage.rst"
        ],
        "message": "Fix code typo Docs ObjectStorage Extrenal Integrations example (#36261)\n\n* Fix code typo ObjectStorage Extrenal Integrations example\n\n* add ; to make it prettier\n\n(cherry picked from commit 3b1aaf1f7247fbaf91e1d064596f0809421bd169)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "4452e301ebbfd2995c3751c2b0d4c8e2d3455756",
      "candidate_info": {
        "commit_hash": "4452e301ebbfd2995c3751c2b0d4c8e2d3455756",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4452e301ebbfd2995c3751c2b0d4c8e2d3455756",
        "files": [
          "airflow/decorators/base.py",
          "docs/apache-airflow/tutorial/taskflow.rst",
          "tests/decorators/test_python.py"
        ],
        "message": "Make sure `multiple_outputs` is inferred correctly even when using `TypedDict` (#36652)\n\n* Use `issubclass()` to check if return type is a dictionary\n\n* Compare type to `typing.Mapping` instead of `typing.Dict`\n\n* Add documentation\n\n(cherry picked from commit e11b91c8e01b38023f209983a81aee23439a34a3)",
        "before_after_code_files": [
          "airflow/decorators/base.py||airflow/decorators/base.py",
          "tests/decorators/test_python.py||tests/decorators/test_python.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/decorators/base.py||airflow/decorators/base.py": [
          "File: airflow/decorators/base.py -> airflow/decorators/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27:     Callable,",
          "28:     ClassVar,",
          "29:     Collection,",
          "31:     Generic,",
          "32:     Iterator,",
          "33:     Mapping,",
          "",
          "[Removed Lines]",
          "30:     Dict,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "351:         except TypeError:  # Can't evaluate return type.",
          "352:             return False",
          "353:         ttype = getattr(return_type, \"__origin__\", return_type)",
          "356:     def __attrs_post_init__(self):",
          "357:         if \"self\" in self.function_signature.parameters:",
          "",
          "[Removed Lines]",
          "354:         return ttype is dict or ttype is Dict",
          "",
          "[Added Lines]",
          "353:         return issubclass(ttype, Mapping)",
          "",
          "---------------"
        ],
        "tests/decorators/test_python.py||tests/decorators/test_python.py": [
          "File: tests/decorators/test_python.py -> tests/decorators/test_python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "98:         assert identity_dict_with_decorator_call(5, 5).operator.multiple_outputs is True",
          "100:     def test_infer_multiple_outputs_forward_annotation(self):",
          "101:         if TYPE_CHECKING:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "100:     @pytest.mark.skipif(sys.version_info < (3, 8), reason=\"PEP 589 is implemented in Python 3.8\")",
          "101:     def test_infer_multiple_outputs_typed_dict(self):",
          "102:         from typing import TypedDict",
          "104:         class TypeDictClass(TypedDict):",
          "105:             pass",
          "107:         @task_decorator",
          "108:         def t1() -> TypeDictClass:",
          "109:             return {}",
          "111:         assert t1().operator.multiple_outputs is True",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1718b2bc7c19b260071ce8988da3e86806b56792",
      "candidate_info": {
        "commit_hash": "1718b2bc7c19b260071ce8988da3e86806b56792",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/1718b2bc7c19b260071ce8988da3e86806b56792",
        "files": [
          "airflow/www/static/js/dag/details/index.tsx"
        ],
        "message": "Fix details tab not showing when using dynamic task mapping (#36522)\n\n* Fix details tab not showing when using dynamic task mapping\n\n* Remove unnecessary variable\n\n(cherry picked from commit 18b701358183762b0b3609627c198c263680eb6e)",
        "before_after_code_files": [
          "airflow/www/static/js/dag/details/index.tsx||airflow/www/static/js/dag/details/index.tsx"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/static/js/dag/details/index.tsx||airflow/www/static/js/dag/details/index.tsx": [
          "File: airflow/www/static/js/dag/details/index.tsx -> airflow/www/static/js/dag/details/index.tsx",
          "--- Hunk 1 ---",
          "[Context before]",
          "301:           <TabPanel height=\"100%\">",
          "302:             {isDag && <DagContent />}",
          "303:             {isDagRun && <DagRunContent runId={runId} />}",
          "305:               <>",
          "306:                 <BackToTaskSummary",
          "307:                   isMapIndexDefined={mapIndex !== undefined && mapIndex > -1}",
          "",
          "[Removed Lines]",
          "304:             {isTaskInstance && (",
          "",
          "[Added Lines]",
          "304:             {!!runId && !!taskId && (",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "eefc0f88d0454871fb22a1f6deff584dd1f9b83a",
      "candidate_info": {
        "commit_hash": "eefc0f88d0454871fb22a1f6deff584dd1f9b83a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/eefc0f88d0454871fb22a1f6deff584dd1f9b83a",
        "files": [
          "Dockerfile.ci",
          "airflow/providers/apache/spark/provider.yaml",
          "generated/provider_dependencies.json"
        ],
        "message": "Bump min version for grpcio-status in spark provider (#36662)\n\nPreviously we limited grpcio minimum version to stop backtracking\nof `pip` from happening and we could not do it in the limits of\nspark provider, becaue some google dependencies used it and\nconflicted with it. This problem is now gone as we have newer\nversions of google dependencies and we can not only safely move\nit to spark provider but also bump it slightly higher to limit\nthe amount of backtracking we need to do.\n\nExtracted from #36537\n\n(cherry picked from commit ded01a5aba337882fb19e03c24d7736c7154fdd8)",
        "before_after_code_files": [
          "Dockerfile.ci||Dockerfile.ci"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "Dockerfile.ci||Dockerfile.ci": [
          "File: Dockerfile.ci -> Dockerfile.ci",
          "--- Hunk 1 ---",
          "[Context before]",
          "1116: # force them on the main Airflow package. Currently we need no extra limits as PIP 23.1+ has much better",
          "1117: # dependency resolution and we do not need to limit the versions of the dependencies",
          "1118: #",
          "1122: # Aiobotocore is limited for eager upgrade because it either causes a long backtracking or",
          "1123: # conflict when we do not limit it. It seems that `pip` has a hard time figuring the right",
          "1124: # combination of dependencies for aiobotocore, botocore, boto3 and s3fs together",
          "1125: #",
          "1127: ARG UPGRADE_TO_NEWER_DEPENDENCIES=\"false\"",
          "1128: ARG VERSION_SUFFIX_FOR_PYPI=\"\"",
          "",
          "[Removed Lines]",
          "1119: # Without grpcio-status limit, pip gets into very long backtracking",
          "1120: # We should attempt to remove it in the future",
          "1121: #",
          "1126: ARG EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=\"grpcio-status>=1.55.0 aiobotocore>=2.5.4\"",
          "",
          "[Added Lines]",
          "1123: ARG EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=\"aiobotocore>=2.5.4\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "986f6d8ea2b6cc47613bbcbb226eeb2c0ba48e7b",
      "candidate_info": {
        "commit_hash": "986f6d8ea2b6cc47613bbcbb226eeb2c0ba48e7b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/986f6d8ea2b6cc47613bbcbb226eeb2c0ba48e7b",
        "files": [
          "tests/models/test_mappedoperator.py"
        ],
        "message": "Add few tests on the mapped task group. (#36149)\n\n(cherry picked from commit bed2789c246c71656bad1cf45374ebef4d28fb2d)",
        "before_after_code_files": [
          "tests/models/test_mappedoperator.py||tests/models/test_mappedoperator.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/models/test_mappedoperator.py||tests/models/test_mappedoperator.py": [
          "File: tests/models/test_mappedoperator.py -> tests/models/test_mappedoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1568:             \"tg_2.my_work\": \"skipped\",",
          "1569:         }",
          "1570:         assert states == expected",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1572:     def test_skip_one_mapped_task_from_task_group_with_generator(self, dag_maker):",
          "1573:         with dag_maker() as dag:",
          "1575:             @task",
          "1576:             def make_list():",
          "1577:                 return [1, 2, 3]",
          "1579:             @task",
          "1580:             def double(n):",
          "1581:                 if n == 2:",
          "1582:                     raise AirflowSkipException()",
          "1583:                 return n * 2",
          "1585:             @task",
          "1586:             def last(n):",
          "1587:                 ...",
          "1589:             @task_group",
          "1590:             def group(n: int) -> None:",
          "1591:                 last(double(n))",
          "1593:             group.expand(n=make_list())",
          "1595:         dr = dag.test()",
          "1596:         states = self.get_states(dr)",
          "1597:         expected = {",
          "1598:             \"group.double\": {0: \"success\", 1: \"skipped\", 2: \"success\"},",
          "1599:             \"group.last\": {0: \"success\", 1: \"skipped\", 2: \"success\"},",
          "1600:             \"make_list\": \"success\",",
          "1601:         }",
          "1602:         assert states == expected",
          "1604:     def test_skip_one_mapped_task_from_task_group(self, dag_maker):",
          "1605:         with dag_maker() as dag:",
          "1607:             @task",
          "1608:             def double(n):",
          "1609:                 if n == 2:",
          "1610:                     raise AirflowSkipException()",
          "1611:                 return n * 2",
          "1613:             @task",
          "1614:             def last(n):",
          "1615:                 ...",
          "1617:             @task_group",
          "1618:             def group(n: int) -> None:",
          "1619:                 last(double(n))",
          "1621:             group.expand(n=[1, 2, 3])",
          "1623:         dr = dag.test()",
          "1624:         states = self.get_states(dr)",
          "1625:         expected = {",
          "1626:             \"group.double\": {0: \"success\", 1: \"skipped\", 2: \"success\"},",
          "1627:             \"group.last\": {0: \"success\", 1: \"skipped\", 2: \"success\"},",
          "1628:         }",
          "1629:         assert states == expected",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0ac3a371d6e9c9f81e463e328efa1b122a7d36f1",
      "candidate_info": {
        "commit_hash": "0ac3a371d6e9c9f81e463e328efa1b122a7d36f1",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0ac3a371d6e9c9f81e463e328efa1b122a7d36f1",
        "files": [
          "airflow/providers/papermill/provider.yaml",
          "generated/provider_dependencies.json"
        ],
        "message": "Bump min provider version for papermill (#36669)\n\nYet another long backtracking avoidance,\n\nWe are bumping REALLY OLD papermill min version (2019) with just\nold from 2022.\n\n(cherry picked from commit b5c6aead4415b53a294e1671990cbba924ed9445)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "de1f0230921246f0a85055a4ea988914b11d7bf6",
      "candidate_info": {
        "commit_hash": "de1f0230921246f0a85055a4ea988914b11d7bf6",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/de1f0230921246f0a85055a4ea988914b11d7bf6",
        "files": [
          "airflow/www/fab_security/manager.py"
        ],
        "message": "Add back FAB constant in legacy security manager (#36719)\n\n(cherry picked from commit acdbd577ab305718fbc05377c03f16c113b5ca26)",
        "before_after_code_files": [
          "airflow/www/fab_security/manager.py||airflow/www/fab_security/manager.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/fab_security/manager.py||airflow/www/fab_security/manager.py": [
          "File: airflow/www/fab_security/manager.py -> airflow/www/fab_security/manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: log = logging.getLogger(__name__)",
          "28: __lazy_imports = {",
          "29:     \"AUTH_DB\": \"flask_appbuilder.const\",",
          "30:     \"AUTH_LDAP\": \"flask_appbuilder.const\",",
          "31:     \"LOGMSG_WAR_SEC_LOGIN_FAILED\": \"flask_appbuilder.const\",",
          "32: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "29:     \"AUTH_OID\": \"flask_appbuilder.const\",",
          "32:     \"AUTH_REMOTE_USER\": \"flask_appbuilder.const\",",
          "33:     \"AUTH_OAUTH\": \"flask_appbuilder.const\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1180255fccfef46db485adfd7afdb5a5ae63f00b",
      "candidate_info": {
        "commit_hash": "1180255fccfef46db485adfd7afdb5a5ae63f00b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/1180255fccfef46db485adfd7afdb5a5ae63f00b",
        "files": [
          "airflow/www/static/js/dag/details/taskInstance/taskActions/MarkInstanceAs.tsx"
        ],
        "message": "enable mark task as failed/success always (#36254)\n\n(cherry picked from commit 20d547ecd886087cd89bcdf0015ce71dd0a12cef)",
        "before_after_code_files": [
          "airflow/www/static/js/dag/details/taskInstance/taskActions/MarkInstanceAs.tsx||airflow/www/static/js/dag/details/taskInstance/taskActions/MarkInstanceAs.tsx"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/static/js/dag/details/taskInstance/taskActions/MarkInstanceAs.tsx||airflow/www/static/js/dag/details/taskInstance/taskActions/MarkInstanceAs.tsx": [
          "File: airflow/www/static/js/dag/details/taskInstance/taskActions/MarkInstanceAs.tsx -> airflow/www/static/js/dag/details/taskInstance/taskActions/MarkInstanceAs.tsx",
          "--- Hunk 1 ---",
          "[Context before]",
          "296:           </Flex>",
          "297:         </MenuButton>",
          "298:         <MenuList>",
          "303:             <SimpleStatus state=\"failed\" mr={2} />",
          "304:             failed",
          "305:           </MenuItem>",
          "310:             <SimpleStatus state=\"success\" mr={2} />",
          "311:             success",
          "312:           </MenuItem>",
          "",
          "[Removed Lines]",
          "299:           <MenuItem",
          "300:             onClick={markAsFailed}",
          "301:             isDisabled={!isMappedSummary && currentState === \"failed\"}",
          "302:           >",
          "306:           <MenuItem",
          "307:             onClick={markAsSuccess}",
          "308:             isDisabled={!isMappedSummary && currentState === \"success\"}",
          "309:           >",
          "",
          "[Added Lines]",
          "299:           <MenuItem onClick={markAsFailed}>",
          "303:           <MenuItem onClick={markAsSuccess}>",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a161e6eb6e7f7ba8bfa57c80a83b83800d72e1ee",
      "candidate_info": {
        "commit_hash": "a161e6eb6e7f7ba8bfa57c80a83b83800d72e1ee",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a161e6eb6e7f7ba8bfa57c80a83b83800d72e1ee",
        "files": [
          ".github/workflows/ci.yml",
          "Dockerfile",
          "Dockerfile.ci",
          "scripts/docker/install_os_dependencies.sh"
        ],
        "message": "Add zlib1g-dev package to Airflow images (#36493)\n\nSeems that when mysql repository is used to install mysql client,\nit induces libxml compilation for Python 3.8 and 3.9 and this\nlibrary requires devel version of zlib that is missing in the image.\n\nThis PR adds the devel version as dev apt dependency.\n\n(cherry picked from commit 2bc34ffcb5e830544e024e085f36481a33852f49)",
        "before_after_code_files": [
          "Dockerfile.ci||Dockerfile.ci",
          "scripts/docker/install_os_dependencies.sh||scripts/docker/install_os_dependencies.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "Dockerfile.ci||Dockerfile.ci": [
          "File: Dockerfile.ci -> Dockerfile.ci",
          "--- Hunk 1 ---",
          "[Context before]",
          "58: freetds-bin freetds-dev git gosu graphviz graphviz-dev krb5-user ldap-utils libffi-dev libgeos-dev \\",
          "59: libkrb5-dev libldap2-dev libleveldb1d libleveldb-dev libsasl2-2 libsasl2-dev libsasl2-modules \\",
          "60: libssl-dev libxmlsec1 libxmlsec1-dev locales lsb-release openssh-client pkgconf sasl2-bin \\",
          "62:         export DEV_APT_DEPS",
          "63:     fi",
          "64: }",
          "",
          "[Removed Lines]",
          "61: software-properties-common sqlite3 sudo unixodbc unixodbc-dev\"",
          "",
          "[Added Lines]",
          "61: software-properties-common sqlite3 sudo unixodbc unixodbc-dev zlib1g-dev\"",
          "",
          "---------------"
        ],
        "scripts/docker/install_os_dependencies.sh||scripts/docker/install_os_dependencies.sh": [
          "File: scripts/docker/install_os_dependencies.sh -> scripts/docker/install_os_dependencies.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "40: freetds-bin freetds-dev git gosu graphviz graphviz-dev krb5-user ldap-utils libffi-dev libgeos-dev \\",
          "41: libkrb5-dev libldap2-dev libleveldb1d libleveldb-dev libsasl2-2 libsasl2-dev libsasl2-modules \\",
          "42: libssl-dev libxmlsec1 libxmlsec1-dev locales lsb-release openssh-client pkgconf sasl2-bin \\",
          "44:         export DEV_APT_DEPS",
          "45:     fi",
          "46: }",
          "",
          "[Removed Lines]",
          "43: software-properties-common sqlite3 sudo unixodbc unixodbc-dev\"",
          "",
          "[Added Lines]",
          "43: software-properties-common sqlite3 sudo unixodbc unixodbc-dev zlib1g-dev\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8eb2caad8e4567cf90656c3189f80f7c56a217d5",
      "candidate_info": {
        "commit_hash": "8eb2caad8e4567cf90656c3189f80f7c56a217d5",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8eb2caad8e4567cf90656c3189f80f7c56a217d5",
        "files": [
          "tests/test_utils/www.py",
          "tests/www/views/conftest.py",
          "tests/www/views/test_anonymous_as_admin_role.py"
        ],
        "message": "Add unit test for AUTH_ROLE_PUBLIC=Admin (#36787)\n\nSigned-off-by: BobDu <i@bobdu.cc>\n(cherry picked from commit a87953e8382e2e5d0af98368e896fe78af04da27)",
        "before_after_code_files": [
          "tests/test_utils/www.py||tests/test_utils/www.py",
          "tests/www/views/conftest.py||tests/www/views/conftest.py",
          "tests/www/views/test_anonymous_as_admin_role.py||tests/www/views/test_anonymous_as_admin_role.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/test_utils/www.py||tests/test_utils/www.py": [
          "File: tests/test_utils/www.py -> tests/test_utils/www.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "39:     return client",
          "42: def check_content_in_response(text, resp, resp_code=200):",
          "43:     resp_html = resp.data.decode(\"utf-8\")",
          "44:     assert resp_code == resp.status_code",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42: def client_without_login_as_admin(app):",
          "43:     # Anonymous users as Admin if set AUTH_ROLE_PUBLIC=Admin",
          "44:     app.config[\"AUTH_ROLE_PUBLIC\"] = \"Admin\"",
          "45:     client = app.test_client()",
          "46:     return client",
          "",
          "---------------"
        ],
        "tests/www/views/conftest.py||tests/www/views/conftest.py": [
          "File: tests/www/views/conftest.py -> tests/www/views/conftest.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: from tests.test_utils.api_connexion_utils import delete_user",
          "31: from tests.test_utils.config import conf_vars",
          "32: from tests.test_utils.decorators import dont_initialize_flask_app_submodules",
          "36: @pytest.fixture(autouse=True, scope=\"module\")",
          "",
          "[Removed Lines]",
          "33: from tests.test_utils.www import client_with_login, client_without_login",
          "",
          "[Added Lines]",
          "33: from tests.test_utils.www import client_with_login, client_without_login, client_without_login_as_admin",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "130:     return client_without_login(app)",
          "133: class _TemplateWithContext(NamedTuple):",
          "134:     template: jinja2.environment.Template",
          "135:     context: dict[str, Any]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "133: @pytest.fixture()",
          "134: def anonymous_client_as_admin(app):",
          "135:     return client_without_login_as_admin(app)",
          "",
          "---------------"
        ],
        "tests/www/views/test_anonymous_as_admin_role.py||tests/www/views/test_anonymous_as_admin_role.py": [
          "File: tests/www/views/test_anonymous_as_admin_role.py -> tests/www/views/test_anonymous_as_admin_role.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: import pytest",
          "22: from airflow.models import Pool",
          "23: from airflow.utils.session import create_session",
          "24: from tests.test_utils.www import check_content_in_response",
          "26: pytestmark = pytest.mark.db_test",
          "28: POOL = {",
          "29:     \"pool\": \"test-pool\",",
          "30:     \"slots\": 777,",
          "31:     \"description\": \"test-pool-description\",",
          "32:     \"include_deferred\": False,",
          "33: }",
          "36: @pytest.fixture(autouse=True)",
          "37: def clear_pools():",
          "38:     with create_session() as session:",
          "39:         session.query(Pool).delete()",
          "42: @pytest.fixture()",
          "43: def pool_factory(session):",
          "44:     def factory(**values):",
          "45:         pool = Pool(**{**POOL, **values})  # Passed in values override defaults.",
          "46:         session.add(pool)",
          "47:         session.commit()",
          "48:         return pool",
          "50:     return factory",
          "53: def test_delete_pool_anonymous_user_no_role(anonymous_client, pool_factory):",
          "54:     pool = pool_factory()",
          "55:     resp = anonymous_client.post(f\"pool/delete/{pool.id}\")",
          "56:     assert 302 == resp.status_code",
          "57:     assert \"/login/\" == resp.headers[\"Location\"]",
          "60: def test_delete_pool_anonymous_user_as_admin(anonymous_client_as_admin, pool_factory):",
          "61:     pool = pool_factory()",
          "62:     resp = anonymous_client_as_admin.post(f\"pool/delete/{pool.id}\", follow_redirects=True)",
          "63:     check_content_in_response(\"Deleted Row\", resp)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a45e149bcb4036a74e4eb0ec04a4de7eee9c397f",
      "candidate_info": {
        "commit_hash": "a45e149bcb4036a74e4eb0ec04a4de7eee9c397f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a45e149bcb4036a74e4eb0ec04a4de7eee9c397f",
        "files": [
          "docs/apache-airflow/security/audit_logs.rst"
        ],
        "message": "Improve audit_logs.rst (#36213)\n\nAdd more detail and accuracy to audit logs documentation\n\n(cherry picked from commit cbe8245990a066af97a0e0981de00402c0bcd637)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "5dad650b526374365681d72613a73475cdecf127",
      "candidate_info": {
        "commit_hash": "5dad650b526374365681d72613a73475cdecf127",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/5dad650b526374365681d72613a73475cdecf127",
        "files": [
          "airflow/auth/managers/base_auth_manager.py",
          "tests/api_connexion/endpoints/test_dag_run_endpoint.py"
        ],
        "message": "Bugfix: Webserver returns 500 for POST requests to api/dag/*/dagrun from anonymous user (#36275)\n\n* airflow#36110 -  bugfix\n\n* return type fixed\n\n* airflow#36110 -  bugfix\n\n* airflow#36110 -  fixes\n\n* airflow#36110 -  fixes\n\n* airflow#36110 -  adding test\n\n* airflow#36110 -  adding test\n\n* Fix unit test\n\n* Don't call get_id twice\n\n* Update app configuration after initialization\n\n---------\n\nCo-authored-by: hussein-awala <hussein@awala.fr>\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 71bc871d35cd3b562a49ce8f209098e2e24c1ef8)",
        "before_after_code_files": [
          "airflow/auth/managers/base_auth_manager.py||airflow/auth/managers/base_auth_manager.py",
          "tests/api_connexion/endpoints/test_dag_run_endpoint.py||tests/api_connexion/endpoints/test_dag_run_endpoint.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/auth/managers/base_auth_manager.py||airflow/auth/managers/base_auth_manager.py": [
          "File: airflow/auth/managers/base_auth_manager.py -> airflow/auth/managers/base_auth_manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "99:     def get_user(self) -> BaseUser | None:",
          "100:         \"\"\"Return the user associated to the user in session.\"\"\"",
          "103:         \"\"\"Return the user ID associated to the user in session.\"\"\"",
          "104:         user = self.get_user()",
          "105:         if not user:",
          "106:             self.log.error(\"Calling 'get_user_id()' but the user is not signed in.\")",
          "107:             raise AirflowException(\"The user must be signed in.\")",
          "110:     def init(self) -> None:",
          "111:         \"\"\"",
          "",
          "[Removed Lines]",
          "102:     def get_user_id(self) -> str:",
          "108:         return str(user.get_id())",
          "",
          "[Added Lines]",
          "102:     def get_user_id(self) -> str | None:",
          "108:         if user_id := user.get_id():",
          "109:             return str(user_id)",
          "110:         return None",
          "",
          "---------------"
        ],
        "tests/api_connexion/endpoints/test_dag_run_endpoint.py||tests/api_connexion/endpoints/test_dag_run_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_dag_run_endpoint.py -> tests/api_connexion/endpoints/test_dag_run_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1861:             environ_overrides={\"REMOTE_USER\": \"test\"},",
          "1862:         )",
          "1863:         assert response.status_code == 404",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1865:     @conf_vars(",
          "1866:         {",
          "1867:             (\"api\", \"auth_backends\"): \"airflow.api.auth.backend.default\",",
          "1868:         }",
          "1869:     )",
          "1870:     def test_should_respond_200_with_anonymous_user(self, dag_maker, session):",
          "1871:         from airflow.www import app as application",
          "1873:         app = application.create_app(testing=True)",
          "1874:         app.config[\"AUTH_ROLE_PUBLIC\"] = \"Admin\"",
          "1875:         dag_runs = self._create_test_dag_run(DagRunState.SUCCESS)",
          "1876:         session.add_all(dag_runs)",
          "1877:         session.commit()",
          "1878:         created_dr = dag_runs[0]",
          "1879:         response = app.test_client().patch(",
          "1880:             f\"api/v1/dags/{created_dr.dag_id}/dagRuns/TEST_DAG_RUN_ID_1/setNote\",",
          "1881:             json={\"note\": \"I am setting a note with anonymous user\"},",
          "1882:         )",
          "1883:         assert response.status_code == 200",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8c41a8f14d381e7b0f1068d2d2503408b97fc707",
      "candidate_info": {
        "commit_hash": "8c41a8f14d381e7b0f1068d2d2503408b97fc707",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8c41a8f14d381e7b0f1068d2d2503408b97fc707",
        "files": [
          "airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py",
          "airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py",
          "airflow/models/taskinstance.py",
          "airflow/utils/db.py",
          "docs/apache-airflow/img/airflow_erd.sha256",
          "docs/apache-airflow/img/airflow_erd.svg",
          "docs/apache-airflow/migrations-ref.rst",
          "scripts/in_container/run_mypy.sh"
        ],
        "message": "Remove usused index on task instance (#36737)\n\nIndex is only helpful for a user's custom query -- not for airflow in general (see comment https://github.com/apache/airflow/pull/30762#issuecomment-1886658295).  Noticed that this query had zero scans over a period of months.  I also observed that it also takes up as much space as the table itself.  Since it's not generally useful, it doesn't belong in airflow OSS.\n\nReverts #30762\n\n(cherry picked from commit e20b400317ae4eb41181c5b0cee466eff768b521)",
        "before_after_code_files": [
          "airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py||airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py",
          "airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py||airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py",
          "airflow/models/taskinstance.py||airflow/models/taskinstance.py",
          "airflow/utils/db.py||airflow/utils/db.py",
          "scripts/in_container/run_mypy.sh||scripts/in_container/run_mypy.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py||airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py": [
          "File: airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py -> airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "38: def upgrade():",
          "39:     \"\"\"Apply Add index to task_instance table\"\"\"",
          "48: def downgrade():",
          "49:     \"\"\"Unapply Add index to task_instance table\"\"\"",
          "",
          "[Removed Lines]",
          "40:     op.create_index(",
          "41:         \"ti_state_incl_start_date\",",
          "42:         \"task_instance\",",
          "43:         [\"dag_id\", \"task_id\", \"state\"],",
          "44:         postgresql_include=[\"start_date\"],",
          "45:     )",
          "50:     op.drop_index(\"ti_state_incl_start_date\", table_name=\"task_instance\")",
          "",
          "[Added Lines]",
          "40:     # We don't add this index anymore because it's not useful.",
          "41:     pass",
          "46:     # At 2.8.1 we removed this index as it is not used, and changed this migration not to add it",
          "47:     # So we use drop if exists (cus it might not be there)",
          "48:     import sqlalchemy",
          "49:     from contextlib import suppress",
          "51:     with suppress(sqlalchemy.exc.DatabaseError):  # mysql does not support drop if exists index",
          "52:         op.drop_index(\"ti_state_incl_start_date\", table_name=\"task_instance\", if_exists=True)",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py||airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py": [
          "File: airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py -> airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "19: \"\"\"Drop unused TI index",
          "21: Revision ID: 88344c1d9134",
          "22: Revises: 10b52ebd31f7",
          "23: Create Date: 2024-01-11 11:54:48.232030",
          "25: \"\"\"",
          "27: import sqlalchemy as sa",
          "28: from alembic import op",
          "31: # revision identifiers, used by Alembic.",
          "32: revision = \"88344c1d9134\"",
          "33: down_revision = \"10b52ebd31f7\"",
          "34: branch_labels = None",
          "35: depends_on = None",
          "36: airflow_version = \"2.8.1\"",
          "39: def upgrade():",
          "40:     \"\"\"Apply refactor dag run indexes\"\"\"",
          "41:     # This index may have been created in 2.7 but we've since removed it from migrations",
          "42:     import sqlalchemy",
          "43:     from contextlib import suppress",
          "45:     with suppress(sqlalchemy.exc.DatabaseError):  # mysql does not support drop if exists index",
          "46:         op.drop_index(\"ti_state_incl_start_date\", table_name=\"task_instance\", if_exists=True)",
          "49: def downgrade():",
          "50:     \"\"\"Unapply refactor dag run indexes\"\"\"",
          "",
          "---------------"
        ],
        "airflow/models/taskinstance.py||airflow/models/taskinstance.py": [
          "File: airflow/models/taskinstance.py -> airflow/models/taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1256:         # Existing \"ti_state_lkp\" is not enough for such query when this table has millions of rows, since",
          "1257:         # rows have to be fetched in order to retrieve the start_date column. With this index, INDEX ONLY SCAN",
          "1258:         # is performed and that query runs within milliseconds.",
          "1260:         Index(\"ti_pool\", pool, state, priority_weight),",
          "1261:         Index(\"ti_job_id\", job_id),",
          "1262:         Index(\"ti_trigger_id\", trigger_id),",
          "",
          "[Removed Lines]",
          "1259:         Index(\"ti_state_incl_start_date\", dag_id, task_id, state, postgresql_include=[\"start_date\"]),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/utils/db.py||airflow/utils/db.py": [
          "File: airflow/utils/db.py -> airflow/utils/db.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "89:     \"2.6.2\": \"c804e5c76e3e\",",
          "90:     \"2.7.0\": \"405de8318b3a\",",
          "91:     \"2.8.0\": \"10b52ebd31f7\",",
          "92: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "92:     \"2.8.1\": \"88344c1d9134\",",
          "",
          "---------------"
        ],
        "scripts/in_container/run_mypy.sh||scripts/in_container/run_mypy.sh": [
          "File: scripts/in_container/run_mypy.sh -> scripts/in_container/run_mypy.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: ADDITIONAL_MYPY_OPTIONS=()",
          "25: if [[ ${SUSPENDED_PROVIDERS_FOLDERS=} != \"\" ]];",
          "26: then",
          "27:     for folder in ${SUSPENDED_PROVIDERS_FOLDERS=}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: export MYPY_FORCE_COLOR=true",
          "26: export TERM=ansi",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "373af529cc66402c332597606d75cfb616666271",
      "candidate_info": {
        "commit_hash": "373af529cc66402c332597606d75cfb616666271",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/373af529cc66402c332597606d75cfb616666271",
        "files": [
          "docs/apache-airflow/authoring-and-scheduling/deferring.rst"
        ],
        "message": "Deferrable Operators Docs Edits (#33620)\n\nCo-authored-by: Jed Cunningham <66968678+jedcunningham@users.noreply.github.com>\nCo-authored-by: Daniel Standish <15932138+dstandish@users.noreply.github.com>\n(cherry picked from commit 657223c0fd5460b1aa9fca8d96c2b4a17eb40ef9)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "ae0ceb382dc0dfb91adc5d22b480c321ffbe3fe0",
      "candidate_info": {
        "commit_hash": "ae0ceb382dc0dfb91adc5d22b480c321ffbe3fe0",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ae0ceb382dc0dfb91adc5d22b480c321ffbe3fe0",
        "files": [
          ".pre-commit-config.yaml",
          ".rat-excludes",
          "CONTRIBUTING.rst",
          "docs/apache-airflow-providers-fab/img/diagram_fab_auth_manager_airflow_architecture.png",
          "docs/apache-airflow/img/diagram_auth_manager_airflow_architecture.md5sum",
          "docs/apache-airflow/img/diagram_auth_manager_airflow_architecture.png",
          "docs/apache-airflow/img/diagram_auth_manager_airflow_architecture.py",
          "docs/apache-airflow/img/diagram_basic_airflow_architecture.md5sum",
          "docs/apache-airflow/img/diagram_basic_airflow_architecture.py",
          "docs/apache-airflow/img/diagram_dag_processor_airflow_architecture.md5sum",
          "docs/apache-airflow/img/diagram_dag_processor_airflow_architecture.py",
          "docs/apache-airflow/img/diagram_fab_auth_manager_airflow_architecture.md5sum",
          "docs/apache-airflow/img/diagram_fab_auth_manager_airflow_architecture.png",
          "docs/apache-airflow/img/diagram_fab_auth_manager_airflow_architecture.py",
          "docs/diagrams/python_multiprocess_logo.png",
          "images/diagrams/python_multiprocess_logo.png",
          "scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py"
        ],
        "message": "Improve pre-commit to generate Airflow diagrams as a code (#36333)\n\nSince we are getting more diagrams generated in Airflow using the\n\"diagram as a code\" approach, this PR improves the pre-commit to be\nmore suitable to support generation of more of the images coming\nfrom different sources, placed in different directories and generated\nindependently, so that the whole process is more distributed and easy\nfor whoever creates diagrams to add their own diagram.\n\nThe changes implemented in this PR:\n\n* the code to generate the diagrams is now next to the diagram they\n  generate. It has the same name as the diagram, but it has the .py\n  extension. This way it is immediately visible where is the source\n  of each diagram (right next to each diagram)\n\n* each of the .py diagram Python files is runnable on its own. This\n  way you can easily regenerate the diagrams by running corresponding\n  Python file or even automate it by running \"save\" action and generate\n  the diagrams automatically by running the Python code every time\n  the file is saved. That makes a very nice workflow on iterating on\n  each diagram, independently from each othere\n\n* the pre-commit script is given a set of folders which should be\n  scanned and it finds and run the diagrams on pre-commmit. It also\n  creates and verifies the md5sum hash of the source Python file\n  separately for each diagram and only runs diagram generation when\n  the source file changed vs. last time the hash was saved and\n  committed. The hash sum is stored next to the image and sources\n  with .md5sum extension\n\nAlso updated documentation in the CONTRIBUTING.rst explaining how\nto generate the diagrams and what is the mechanism of that\ngeneration.\n\n(cherry picked from commit b35b08ec41814b6fe5d7388296db83a726e6d6d0)",
        "before_after_code_files": [
          "scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py||scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py||scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py": [
          "File: scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py -> scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import hashlib",
          "22: from pathlib import Path",
          "29: from rich.console import Console",
          "31: console = Console(width=400, color_system=\"standard\")",
          "33: LOCAL_DIR = Path(__file__).parent",
          "34: AIRFLOW_SOURCES_ROOT = Path(__file__).parents[3]",
          "124: def main():",
          "138: if __name__ == \"__main__\":",
          "",
          "[Removed Lines]",
          "21: import os",
          "24: from diagrams import Cluster, Diagram, Edge",
          "25: from diagrams.custom import Custom",
          "26: from diagrams.onprem.client import User",
          "27: from diagrams.onprem.database import PostgreSQL",
          "28: from diagrams.programming.flowchart import MultipleDocuments",
          "35: DOCS_IMAGES_DIR = AIRFLOW_SOURCES_ROOT / \"docs\" / \"apache-airflow\" / \"img\"",
          "36: PYTHON_MULTIPROCESS_LOGO = AIRFLOW_SOURCES_ROOT / \"images\" / \"diagrams\" / \"python_multiprocess_logo.png\"",
          "38: BASIC_ARCHITECTURE_IMAGE_NAME = \"diagram_basic_airflow_architecture\"",
          "39: DAG_PROCESSOR_AIRFLOW_ARCHITECTURE_IMAGE_NAME = \"diagram_dag_processor_airflow_architecture\"",
          "40: DIAGRAM_HASH_FILE_NAME = \"diagram_hash.txt\"",
          "43: def generate_basic_airflow_diagram(filename: str):",
          "44:     basic_architecture_image_file = (DOCS_IMAGES_DIR / BASIC_ARCHITECTURE_IMAGE_NAME).with_suffix(\".png\")",
          "45:     console.print(f\"[bright_blue]Generating architecture image {basic_architecture_image_file}\")",
          "46:     with Diagram(name=\"\", show=False, direction=\"LR\", curvestyle=\"ortho\", filename=filename):",
          "47:         with Cluster(\"Parsing & Scheduling\"):",
          "48:             schedulers = Custom(\"Scheduler(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "50:         metadata_db = PostgreSQL(\"Metadata DB\")",
          "52:         dag_author = User(\"DAG Author\")",
          "53:         dag_files = MultipleDocuments(\"DAG files\")",
          "55:         dag_author >> Edge(color=\"black\", style=\"dashed\", reverse=False) >> dag_files",
          "57:         with Cluster(\"Execution\"):",
          "58:             workers = Custom(\"Worker(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "59:             triggerer = Custom(\"Triggerer(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "61:         schedulers - Edge(color=\"blue\", style=\"dashed\", taillabel=\"Executor\") - workers",
          "63:         schedulers >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> metadata_db",
          "64:         workers >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> metadata_db",
          "65:         triggerer >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> metadata_db",
          "67:         operations_user = User(\"Operations User\")",
          "68:         with Cluster(\"UI\"):",
          "69:             webservers = Custom(\"Webserver(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "71:         webservers >> Edge(color=\"black\", style=\"dashed\", reverse=True) >> operations_user",
          "73:         metadata_db >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> webservers",
          "75:         dag_files >> Edge(color=\"brown\", style=\"solid\") >> workers",
          "76:         dag_files >> Edge(color=\"brown\", style=\"solid\") >> schedulers",
          "77:         dag_files >> Edge(color=\"brown\", style=\"solid\") >> triggerer",
          "78:     console.print(f\"[green]Generating architecture image {basic_architecture_image_file}\")",
          "81: def generate_dag_processor_airflow_diagram(filename: str):",
          "82:     dag_processor_architecture_image_file = (",
          "83:         DOCS_IMAGES_DIR / DAG_PROCESSOR_AIRFLOW_ARCHITECTURE_IMAGE_NAME",
          "84:     ).with_suffix(\".png\")",
          "85:     console.print(f\"[bright_blue]Generating architecture image {dag_processor_architecture_image_file}\")",
          "86:     with Diagram(name=\"\", show=False, direction=\"LR\", curvestyle=\"ortho\", filename=filename):",
          "87:         operations_user = User(\"Operations User\")",
          "88:         with Cluster(\"No DAG Python Code Execution\", graph_attr={\"bgcolor\": \"lightgrey\"}):",
          "89:             with Cluster(\"Scheduling\"):",
          "90:                 schedulers = Custom(\"Scheduler(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "92:             with Cluster(\"UI\"):",
          "93:                 webservers = Custom(\"Webserver(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "95:         webservers >> Edge(color=\"black\", style=\"dashed\", reverse=True) >> operations_user",
          "97:         metadata_db = PostgreSQL(\"Metadata DB\")",
          "99:         dag_author = User(\"DAG Author\")",
          "100:         with Cluster(\"DAG Python Code Execution\"):",
          "101:             with Cluster(\"Execution\"):",
          "102:                 workers = Custom(\"Worker(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "103:                 triggerer = Custom(\"Triggerer(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "104:             with Cluster(\"Parsing\"):",
          "105:                 dag_processors = Custom(\"DAG\\nProcessor(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "106:             dag_files = MultipleDocuments(\"DAG files\")",
          "108:         dag_author >> Edge(color=\"black\", style=\"dashed\", reverse=False) >> dag_files",
          "110:         workers - Edge(color=\"blue\", style=\"dashed\", headlabel=\"Executor\") - schedulers",
          "112:         metadata_db >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> webservers",
          "113:         metadata_db >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> schedulers",
          "114:         dag_processors >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> metadata_db",
          "115:         workers >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> metadata_db",
          "116:         triggerer >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> metadata_db",
          "118:         dag_files >> Edge(color=\"brown\", style=\"solid\") >> workers",
          "119:         dag_files >> Edge(color=\"brown\", style=\"solid\") >> dag_processors",
          "120:         dag_files >> Edge(color=\"brown\", style=\"solid\") >> triggerer",
          "121:     console.print(f\"[green]Generating architecture image {dag_processor_architecture_image_file}\")",
          "125:     hash_md5 = hashlib.md5()",
          "126:     hash_md5.update(Path(__file__).resolve().read_bytes())",
          "127:     my_file_hash = hash_md5.hexdigest()",
          "128:     hash_file = LOCAL_DIR / DIAGRAM_HASH_FILE_NAME",
          "129:     if not hash_file.exists() or not hash_file.read_text().strip() == str(my_file_hash).strip():",
          "130:         os.chdir(DOCS_IMAGES_DIR)",
          "131:         generate_basic_airflow_diagram(BASIC_ARCHITECTURE_IMAGE_NAME)",
          "132:         generate_dag_processor_airflow_diagram(DAG_PROCESSOR_AIRFLOW_ARCHITECTURE_IMAGE_NAME)",
          "133:         hash_file.write_text(str(my_file_hash) + \"\\n\")",
          "134:     else:",
          "135:         console.print(\"[bright_blue]No changes to generation script. Not regenerating the images.\")",
          "",
          "[Added Lines]",
          "21: import subprocess",
          "22: import sys",
          "33: def _get_file_hash(file_to_check: Path) -> str:",
          "34:     hash_md5 = hashlib.md5()",
          "35:     hash_md5.update(Path(file_to_check).resolve().read_bytes())",
          "36:     return hash_md5.hexdigest()",
          "40:     # get all files as arguments",
          "41:     for arg in sys.argv[1:]:",
          "42:         source_file = Path(arg).resolve()",
          "43:         checksum = _get_file_hash(source_file)",
          "44:         hash_file = source_file.with_suffix(\".md5sum\")",
          "45:         if not hash_file.exists() or not hash_file.read_text().strip() == str(checksum).strip():",
          "46:             console.print(f\"[bright_blue]Changes in {source_file}. Regenerating the image.\")",
          "47:             subprocess.run(",
          "48:                 [sys.executable, source_file.resolve().as_posix()], check=True, cwd=source_file.parent",
          "49:             )",
          "50:             hash_file.write_text(str(checksum) + \"\\n\")",
          "51:         else:",
          "52:             console.print(f\"[bright_blue]No changes in {source_file}. Not regenerating the image.\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bfce9a4f20168001c1faec679e0baf9b051076dd",
      "candidate_info": {
        "commit_hash": "bfce9a4f20168001c1faec679e0baf9b051076dd",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/bfce9a4f20168001c1faec679e0baf9b051076dd",
        "files": [
          "Dockerfile",
          "dev/breeze/src/airflow_breeze/global_constants.py",
          "docs/docker-stack/build-arg-ref.rst",
          "images/breeze/output_prod-image_build.svg",
          "images/breeze/output_prod-image_build.txt"
        ],
        "message": "Remove common.io from chicken-egg providers. (#36284)\n\nNow that Airflow 2.8.0 is released, we can remove common.io from\nchicken-egg providers.\n\n(cherry picked from commit 34d500158769d1d197911a2cd4ac5818bcd117d2)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py": [
          "File: dev/breeze/src/airflow_breeze/global_constants.py -> dev/breeze/src/airflow_breeze/global_constants.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "430:     \"async\",",
          "431:     \"celery\",",
          "432:     \"cncf.kubernetes\",",
          "433:     \"docker\",",
          "434:     \"elasticsearch\",",
          "435:     \"ftp\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "433:     \"common.io\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "456:     # END OF EXTRAS LIST UPDATED BY PRE COMMIT",
          "457: ]",
          "466: def _exclusion(providers: Iterable[str]) -> str:",
          "",
          "[Removed Lines]",
          "459: CHICKEN_EGG_PROVIDERS = \" \".join(",
          "460:     [",
          "461:         \"common.io\",",
          "462:     ]",
          "463: )",
          "",
          "[Added Lines]",
          "460: CHICKEN_EGG_PROVIDERS = \" \".join([])",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cbab5e4df34df123460dbf805790602a65c2173b",
      "candidate_info": {
        "commit_hash": "cbab5e4df34df123460dbf805790602a65c2173b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cbab5e4df34df123460dbf805790602a65c2173b",
        "files": [
          "airflow/www/templates/airflow/dag_details.html"
        ],
        "message": "rename concurrency label to max active tasks (#36691)\n\n(cherry picked from commit 9cbfed474c10891c1429cd538a2bf6c8d014096d)",
        "before_after_code_files": [
          "airflow/www/templates/airflow/dag_details.html||airflow/www/templates/airflow/dag_details.html"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/templates/airflow/dag_details.html||airflow/www/templates/airflow/dag_details.html": [
          "File: airflow/www/templates/airflow/dag_details.html -> airflow/www/templates/airflow/dag_details.html",
          "--- Hunk 1 ---",
          "[Context before]",
          "72:       <td>{{ active_runs | length }} / {{ dag.max_active_runs }}</td>",
          "73:     </tr>",
          "74:     <tr>",
          "76:       <td>{{ dag.max_active_tasks }}</td>",
          "77:     </tr>",
          "78:     <tr>",
          "",
          "[Removed Lines]",
          "75:       <th>Concurrency</th>",
          "",
          "[Added Lines]",
          "75:       <th>Max Active Tasks</th>",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "536df66160d7600a34447906397238e6a9a1b0e6",
      "candidate_info": {
        "commit_hash": "536df66160d7600a34447906397238e6a9a1b0e6",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/536df66160d7600a34447906397238e6a9a1b0e6",
        "files": [
          "airflow/providers/cohere/provider.yaml",
          "generated/provider_dependencies.json"
        ],
        "message": "Bump Cohere min version to limit backtracking on upgrade (#36666)\n\nWhile testing #36537 I noticed cohere backracking quite a bit with\nolder version. Bumping Cohere to more recent minimum version\n(released in November) decreased it quite a bit.\n\nSince Cohere is mostly standalone package, and likely you want to\nbump people to later version, it's safe to assume we can bump the\nminimum version.\n\n(cherry picked from commit 9797f9278a3a0cbe725e84ba0c80f686cd16f1e0)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "ee12a6d7b67e6027d02217ae1530826311ca2545",
      "candidate_info": {
        "commit_hash": "ee12a6d7b67e6027d02217ae1530826311ca2545",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ee12a6d7b67e6027d02217ae1530826311ca2545",
        "files": [
          "airflow/cli/commands/scheduler_command.py",
          "tests/cli/commands/test_scheduler_command.py"
        ],
        "message": "Fix airflow-scheduler exiting with code 0 on exceptions (#36800)\n\n* Fix airflow-scheduler exiting with code 0 on exceptions\n\n* Fix static check\n\n(cherry picked from commit 1d5d5022b8fc92f23f9fdc3b61269e5c7acfaf39)",
        "before_after_code_files": [
          "airflow/cli/commands/scheduler_command.py||airflow/cli/commands/scheduler_command.py",
          "tests/cli/commands/test_scheduler_command.py||tests/cli/commands/test_scheduler_command.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/commands/scheduler_command.py||airflow/cli/commands/scheduler_command.py": [
          "File: airflow/cli/commands/scheduler_command.py -> airflow/cli/commands/scheduler_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import logging",
          "21: from argparse import Namespace",
          "23: from multiprocessing import Process",
          "25: from airflow import settings",
          "",
          "[Removed Lines]",
          "22: from contextlib import contextmanager",
          "",
          "[Added Lines]",
          "22: from contextlib import ExitStack, contextmanager",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "44:     ExecutorLoader.validate_database_executor_compatibility(job_runner.job.executor)",
          "45:     InternalApiConfig.force_database_direct_access()",
          "46:     enable_health_check = conf.getboolean(\"scheduler\", \"ENABLE_HEALTH_CHECK\")",
          "48:         try:",
          "49:             run_job(job=job_runner.job, execute_callable=job_runner._execute)",
          "50:         except Exception:",
          "51:             log.exception(\"Exception when running scheduler job\")",
          "54: @cli_utils.action_cli",
          "",
          "[Removed Lines]",
          "47:     with _serve_logs(args.skip_serve_logs), _serve_health_check(enable_health_check):",
          "",
          "[Added Lines]",
          "47:     with ExitStack() as stack:",
          "48:         stack.enter_context(_serve_logs(args.skip_serve_logs))",
          "49:         stack.enter_context(_serve_health_check(enable_health_check))",
          "55:             raise",
          "56:         finally:",
          "57:             # Ensure that the contexts are closed",
          "58:             stack.close()",
          "",
          "---------------"
        ],
        "tests/cli/commands/test_scheduler_command.py||tests/cli/commands/test_scheduler_command.py": [
          "File: tests/cli/commands/test_scheduler_command.py -> tests/cli/commands/test_scheduler_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "169:         mock_scheduler_job,",
          "170:     ):",
          "171:         args = self.parser.parse_args([\"scheduler\"])",
          "174:         # Make sure that run_job is called, that the exception has been logged, and that the serve_logs",
          "175:         # sub-process has been terminated",
          "",
          "[Removed Lines]",
          "172:         scheduler_command.scheduler(args)",
          "",
          "[Added Lines]",
          "172:         with pytest.raises(Exception, match=\"run_job failed\"):",
          "173:             scheduler_command.scheduler(args)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "36422edc3738784394f5faa2452d7bc3b1801a33",
      "candidate_info": {
        "commit_hash": "36422edc3738784394f5faa2452d7bc3b1801a33",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/36422edc3738784394f5faa2452d7bc3b1801a33",
        "files": [
          "README.md",
          "RELEASE_NOTES.rst",
          "airflow/__init__.py",
          "airflow/api_connexion/openapi/v1.yaml",
          "airflow/reproducible_build.yaml",
          "docs/apache-airflow/installation/supported-versions.rst",
          "docs/docker-stack/README.md",
          "docs/docker-stack/docker-examples/extending/add-airflow-configuration/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-apt-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-build-essential-extend/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-providers/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-pypi-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-requirement-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/custom-providers/Dockerfile",
          "docs/docker-stack/docker-examples/extending/embedding-dags/Dockerfile",
          "docs/docker-stack/docker-examples/extending/writable-directory/Dockerfile",
          "docs/docker-stack/entrypoint.rst",
          "generated/PYPI_README.md",
          "scripts/ci/pre_commit/pre_commit_supported_versions.py"
        ],
        "message": "Update version to 2.8.1",
        "before_after_code_files": [
          "airflow/__init__.py||airflow/__init__.py",
          "scripts/ci/pre_commit/pre_commit_supported_versions.py||scripts/ci/pre_commit/pre_commit_supported_versions.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/__init__.py||airflow/__init__.py": [
          "File: airflow/__init__.py -> airflow/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: \"\"\"",
          "27: from __future__ import annotations",
          "31: # flake8: noqa: F401",
          "",
          "[Removed Lines]",
          "29: __version__ = \"2.8.1.dev0\"",
          "",
          "[Added Lines]",
          "29: __version__ = \"2.8.1\"",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_supported_versions.py||scripts/ci/pre_commit/pre_commit_supported_versions.py": [
          "File: scripts/ci/pre_commit/pre_commit_supported_versions.py -> scripts/ci/pre_commit/pre_commit_supported_versions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: HEADERS = (\"Version\", \"Current Patch/Minor\", \"State\", \"First Release\", \"Limited Support\", \"EOL/Terminated\")",
          "29: SUPPORTED_VERSIONS = (",
          "31:     (\"1.10\", \"1.10.15\", \"EOL\", \"Aug 27, 2018\", \"Dec 17, 2020\", \"June 17, 2021\"),",
          "32:     (\"1.9\", \"1.9.0\", \"EOL\", \"Jan 03, 2018\", \"Aug 27, 2018\", \"Aug 27, 2018\"),",
          "33:     (\"1.8\", \"1.8.2\", \"EOL\", \"Mar 19, 2017\", \"Jan 03, 2018\", \"Jan 03, 2018\"),",
          "",
          "[Removed Lines]",
          "30:     (\"2\", \"2.8.0\", \"Supported\", \"Dec 17, 2020\", \"TBD\", \"TBD\"),",
          "",
          "[Added Lines]",
          "30:     (\"2\", \"2.8.1\", \"Supported\", \"Dec 17, 2020\", \"TBD\", \"TBD\"),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "21b58ecac36bcf81d24f9a79e02d88e4d04cc94d",
      "candidate_info": {
        "commit_hash": "21b58ecac36bcf81d24f9a79e02d88e4d04cc94d",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/21b58ecac36bcf81d24f9a79e02d88e4d04cc94d",
        "files": [
          "airflow/cli/commands/dag_command.py",
          "tests/cli/commands/test_dag_command.py"
        ],
        "message": "Raise error when ``DagRun`` fails while running ``dag test`` (#36517)\n\n**Motivation**:\n\nCurrently, when using `airflow dags test`, there is no easy way to know programmatically if a DagRun fails since the state is not stored in DB. The way to do know relies on log lines as below:\n\n```bash\nstate=$(airflow dags test exception_dag | grep \"DagRun Finished\" | awk -F, '{for(i=1;i<=NF;i++) if ($i ~ / state=/) print $i}' | awk -F= '{print $2}') if [[ $state == \"failed\" ]]; then exit 1 else exit 0 fi\n```\n\nThis PR adds will return an exit code 1 when `airflow dags test` command if DagRun fails and makes it easy to integrate in CI for testing.\n\n(cherry picked from commit 383ad31c76411fb0a9f7d4243729d7bb0640ff0c)",
        "before_after_code_files": [
          "airflow/cli/commands/dag_command.py||airflow/cli/commands/dag_command.py",
          "tests/cli/commands/test_dag_command.py||tests/cli/commands/test_dag_command.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/commands/dag_command.py||airflow/cli/commands/dag_command.py": [
          "File: airflow/cli/commands/dag_command.py -> airflow/cli/commands/dag_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "515:             raise SystemExit(f\"Configuration {args.conf!r} is not valid JSON. Error: {e}\")",
          "516:     execution_date = args.execution_date or timezone.utcnow()",
          "517:     dag = dag or get_dag(subdir=args.subdir, dag_id=args.dag_id)",
          "519:     show_dagrun = args.show_dagrun",
          "520:     imgcat = args.imgcat_dagrun",
          "521:     filename = args.save_dagrun",
          "",
          "[Removed Lines]",
          "518:     dag.test(execution_date=execution_date, run_conf=run_conf, session=session)",
          "",
          "[Added Lines]",
          "518:     dr: DagRun = dag.test(execution_date=execution_date, run_conf=run_conf, session=session)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "536:         if show_dagrun:",
          "537:             print(dot_graph.source)",
          "540: @cli_utils.action_cli",
          "541: @providers_configuration_loaded",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "539:     if dr and dr.state == DagRunState.FAILED:",
          "540:         raise SystemExit(\"DagRun failed\")",
          "",
          "---------------"
        ],
        "tests/cli/commands/test_dag_command.py||tests/cli/commands/test_dag_command.py": [
          "File: tests/cli/commands/test_dag_command.py -> tests/cli/commands/test_dag_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "43: from airflow.triggers.temporal import DateTimeTrigger, TimeDeltaTrigger",
          "44: from airflow.utils import timezone",
          "45: from airflow.utils.session import create_session",
          "46: from airflow.utils.types import DagRunType",
          "47: from tests.models import TEST_DAGS_FOLDER",
          "48: from tests.test_utils.config import conf_vars",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "46: from airflow.utils.state import DagRunState",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "747:             ]",
          "748:         )",
          "750:     @mock.patch(\"airflow.cli.commands.dag_command.get_dag\")",
          "751:     @mock.patch(\"airflow.utils.timezone.utcnow\")",
          "752:     def test_dag_test_no_execution_date(self, mock_utcnow, mock_get_dag):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "751:     @mock.patch(\"airflow.cli.commands.dag_command.get_dag\")",
          "752:     def test_dag_test_fail_raise_error(self, mock_get_dag):",
          "753:         execution_date_str = DEFAULT_DATE.isoformat()",
          "754:         mock_get_dag.return_value.test.return_value = DagRun(",
          "755:             dag_id=\"example_bash_operator\", execution_date=DEFAULT_DATE, state=DagRunState.FAILED",
          "756:         )",
          "757:         cli_args = self.parser.parse_args([\"dags\", \"test\", \"example_bash_operator\", execution_date_str])",
          "758:         with pytest.raises(SystemExit, match=r\"DagRun failed\"):",
          "759:             dag_command.dag_test(cli_args)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a6a305f0dc5f49b60ac2e0153262b9741b6b503c",
      "candidate_info": {
        "commit_hash": "a6a305f0dc5f49b60ac2e0153262b9741b6b503c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a6a305f0dc5f49b60ac2e0153262b9741b6b503c",
        "files": [
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py",
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py",
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py",
          "tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py"
        ],
        "message": "Kubernetes executor running slots leak fix (#36240)\n\n---------\n\nCo-authored-by: gopal <gopal_dirisala@apple.com>\n(cherry picked from commit 49108e15eb2eb30e2ccb95c9332db7b38d35f2de)",
        "before_after_code_files": [
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py",
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py",
          "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py",
          "tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py||tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py": [
          "File: airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py -> airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "74:     raise",
          "75: from airflow.configuration import conf",
          "76: from airflow.executors.base_executor import BaseExecutor",
          "78: from airflow.providers.cncf.kubernetes.kube_config import KubeConfig",
          "79: from airflow.providers.cncf.kubernetes.kubernetes_helper_functions import annotations_to_key",
          "80: from airflow.utils.event_scheduler import EventScheduler",
          "",
          "[Removed Lines]",
          "77: from airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types import POD_EXECUTOR_DONE_KEY",
          "",
          "[Added Lines]",
          "77: from airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types import (",
          "78:     ADOPTED,",
          "79:     POD_EXECUTOR_DONE_KEY,",
          "80: )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "450:     def _change_state(",
          "451:         self,",
          "452:         key: TaskInstanceKey,",
          "454:         pod_name: str,",
          "455:         namespace: str,",
          "456:         session: Session = NEW_SESSION,",
          "",
          "[Removed Lines]",
          "453:         state: TaskInstanceState | None,",
          "",
          "[Added Lines]",
          "456:         state: TaskInstanceState | str | None,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "458:         if TYPE_CHECKING:",
          "459:             assert self.kube_scheduler",
          "461:         if state == TaskInstanceState.RUNNING:",
          "462:             self.event_buffer[key] = state, None",
          "463:             return",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "464:         if state == ADOPTED:",
          "465:             # When the task pod is adopted by another executor,",
          "466:             # then remove the task from the current executor running queue.",
          "467:             try:",
          "468:                 self.running.remove(key)",
          "469:             except KeyError:",
          "470:                 self.log.debug(\"TI key not in running: %s\", key)",
          "471:             return",
          "",
          "---------------"
        ],
        "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py": [
          "File: airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py -> airflow/providers/cncf/kubernetes/executors/kubernetes_executor_types.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "21: if TYPE_CHECKING:",
          "22:     from airflow.executors.base_executor import CommandType",
          "23:     from airflow.models.taskinstance import TaskInstanceKey",
          "",
          "[Removed Lines]",
          "19: from typing import TYPE_CHECKING, Any, Dict, Optional, Tuple",
          "",
          "[Added Lines]",
          "19: from typing import TYPE_CHECKING, Any, Dict, Optional, Tuple, Union",
          "21: ADOPTED = \"adopted\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "27:     KubernetesJobType = Tuple[TaskInstanceKey, CommandType, Any, Optional[str]]",
          "29:     # key, pod state, pod_name, namespace, resource_version",
          "32:     # pod_name, namespace, pod state, annotations, resource_version",
          "35: ALL_NAMESPACES = \"ALL_NAMESPACES\"",
          "36: POD_EXECUTOR_DONE_KEY = \"airflow_executor_done\"",
          "",
          "[Removed Lines]",
          "30:     KubernetesResultsType = Tuple[TaskInstanceKey, Optional[TaskInstanceState], str, str, str]",
          "33:     KubernetesWatchType = Tuple[str, str, Optional[TaskInstanceState], Dict[str, str], str]",
          "",
          "[Added Lines]",
          "31:     KubernetesResultsType = Tuple[TaskInstanceKey, Optional[Union[TaskInstanceState, str]], str, str, str]",
          "34:     KubernetesWatchType = Tuple[str, str, Optional[Union[TaskInstanceState, str]], Dict[str, str], str]",
          "",
          "---------------"
        ],
        "airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py||airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py": [
          "File: airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py -> airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "41: try:",
          "42:     from airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types import (",
          "43:         ALL_NAMESPACES,",
          "44:         POD_EXECUTOR_DONE_KEY,",
          "45:     )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "43:         ADOPTED,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "220:         pod = event[\"object\"]",
          "221:         annotations_string = annotations_for_logging_task_metadata(annotations)",
          "222:         \"\"\"Process status response.\"\"\"",
          "224:             # deletion_timestamp is set by kube server when a graceful deletion is requested.",
          "225:             # since kube server have received request to delete pod set TI state failed",
          "226:             if event[\"type\"] == \"DELETED\" and pod.metadata.deletion_timestamp:",
          "",
          "[Removed Lines]",
          "223:         if status == \"Pending\":",
          "",
          "[Added Lines]",
          "224:         if event[\"type\"] == \"DELETED\" and not pod.metadata.deletion_timestamp:",
          "225:             # This will happen only when the task pods are adopted by another executor.",
          "226:             # So, there is no change in the pod state.",
          "227:             # However, need to free the executor slot from the current executor.",
          "228:             self.log.info(\"Event: pod %s adopted, annotations: %s\", pod_name, annotations_string)",
          "229:             self.watcher_queue.put((pod_name, namespace, ADOPTED, annotations, resource_version))",
          "230:         elif status == \"Pending\":",
          "",
          "---------------"
        ],
        "tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py||tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py": [
          "File: tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py -> tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:         KubernetesExecutor,",
          "45:         PodReconciliationError,",
          "46:     )",
          "48:     from airflow.providers.cncf.kubernetes.executors.kubernetes_executor_utils import (",
          "49:         AirflowKubernetesScheduler,",
          "50:         KubernetesJobWatcher,",
          "",
          "[Removed Lines]",
          "47:     from airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types import POD_EXECUTOR_DONE_KEY",
          "",
          "[Added Lines]",
          "47:     from airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types import (",
          "48:         ADOPTED,",
          "49:         POD_EXECUTOR_DONE_KEY,",
          "50:     )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "644:         finally:",
          "645:             executor.end()",
          "647:     @pytest.mark.db_test",
          "648:     @pytest.mark.parametrize(",
          "649:         \"multi_namespace_mode_namespace_list, watchers_keys\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "650:     @pytest.mark.db_test",
          "651:     @mock.patch(\"airflow.providers.cncf.kubernetes.executors.kubernetes_executor_utils.KubernetesJobWatcher\")",
          "652:     @mock.patch(\"airflow.providers.cncf.kubernetes.kube_client.get_kube_client\")",
          "653:     @mock.patch(",
          "654:         \"airflow.providers.cncf.kubernetes.executors.kubernetes_executor_utils.AirflowKubernetesScheduler.delete_pod\"",
          "655:     )",
          "656:     def test_change_state_adopted(self, mock_delete_pod, mock_get_kube_client, mock_kubernetes_job_watcher):",
          "657:         executor = self.kubernetes_executor",
          "658:         executor.start()",
          "659:         try:",
          "660:             key = (\"dag_id\", \"task_id\", \"run_id\", \"try_number2\")",
          "661:             executor.running = {key}",
          "662:             executor._change_state(key, ADOPTED, \"pod_name\", \"default\")",
          "663:             assert len(executor.event_buffer) == 0",
          "664:             assert len(executor.running) == 0",
          "665:             mock_delete_pod.assert_not_called()",
          "666:         finally:",
          "667:             executor.end()",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1372:         self._run()",
          "1373:         self.watcher.watcher_queue.put.assert_not_called()",
          "1377:         self.events.append({\"type\": \"DELETED\", \"object\": self.pod})",
          "1379:         self._run()",
          "1382:     def test_process_status_running_deleted(self):",
          "1383:         self.pod.status.phase = \"Running\"",
          "",
          "[Removed Lines]",
          "1375:     def test_process_status_succeeded_type_delete(self):",
          "1376:         self.pod.status.phase = \"Succeeded\"",
          "1380:         self.watcher.watcher_queue.put.assert_not_called()",
          "",
          "[Added Lines]",
          "1397:     @pytest.mark.parametrize(",
          "1398:         \"ti_state\",",
          "1399:         [",
          "1400:             TaskInstanceState.SUCCESS,",
          "1401:             TaskInstanceState.FAILED,",
          "1402:             TaskInstanceState.RUNNING,",
          "1403:             TaskInstanceState.QUEUED,",
          "1404:             TaskInstanceState.UP_FOR_RETRY,",
          "1405:         ],",
          "1406:     )",
          "1407:     def test_process_status_pod_adopted(self, ti_state):",
          "1408:         self.pod.status.phase = ti_state",
          "1410:         self.pod.metadata.deletion_timestamp = None",
          "1413:         self.watcher.watcher_queue.put.assert_called_once_with(",
          "1414:             (",
          "1415:                 self.pod.metadata.name,",
          "1416:                 self.watcher.namespace,",
          "1417:                 ADOPTED,",
          "1418:                 self.core_annotations,",
          "1419:                 self.pod.metadata.resource_version,",
          "1420:             )",
          "1421:         )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "808ed02476385c8c670ae3e64c70f19954496075",
      "candidate_info": {
        "commit_hash": "808ed02476385c8c670ae3e64c70f19954496075",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/808ed02476385c8c670ae3e64c70f19954496075",
        "files": [
          "airflow/utils/task_group.py",
          "tests/utils/test_task_group.py"
        ],
        "message": "Fix get_leaves calculation for teardown in nested group (#36456)\n\nWhen arrowing `group` >> `task`, the \"leaves\" of `group` are connected to `task`. When calculating leaves in the group, teardown tasks are ignored, and we recurse upstream to find non-teardowns.\n\nWhat was happening, and what this fixes, is you might recurse to a work task that already has another non-teardown downstream in the group.  In that case you should ignore the work task (because it already has a non-teardown descendent).\n\nResolves #36345\n\n(cherry picked from commit 949fc5788ec7a7a1e4f6bc850d2615ec0f79a57d)",
        "before_after_code_files": [
          "airflow/utils/task_group.py||airflow/utils/task_group.py",
          "tests/utils/test_task_group.py||tests/utils/test_task_group.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/task_group.py||airflow/utils/task_group.py": [
          "File: airflow/utils/task_group.py -> airflow/utils/task_group.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "371:         tasks = list(self)",
          "372:         ids = {x.task_id for x in tasks}",
          "374:         def recurse_for_first_non_teardown(task):",
          "375:             for upstream_task in task.upstream_list:",
          "376:                 if upstream_task.task_id not in ids:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "374:         def has_non_teardown_downstream(task, exclude: str):",
          "375:             for down_task in task.downstream_list:",
          "376:                 if down_task.task_id == exclude:",
          "377:                     continue",
          "378:                 elif down_task.task_id not in ids:",
          "379:                     continue",
          "380:                 elif not down_task.is_teardown:",
          "381:                     return True",
          "382:             return False",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "381:                 elif task.is_teardown and upstream_task.is_setup:",
          "382:                     # don't go through the teardown-to-setup path",
          "383:                     continue",
          "385:                     yield upstream_task",
          "387:         for task in tasks:",
          "",
          "[Removed Lines]",
          "384:                 else:",
          "",
          "[Added Lines]",
          "394:                 # return unless upstream task already has non-teardown downstream in group",
          "395:                 elif not has_non_teardown_downstream(upstream_task, exclude=task.task_id):",
          "",
          "---------------"
        ],
        "tests/utils/test_task_group.py||tests/utils/test_task_group.py": [
          "File: tests/utils/test_task_group.py -> tests/utils/test_task_group.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "584:     ]",
          "587: def test_duplicate_group_id():",
          "588:     from airflow.exceptions import DuplicateTaskIdFound",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "587: def test_dag_edges_setup_teardown_nested():",
          "588:     from airflow.decorators import task, task_group",
          "589:     from airflow.models.dag import DAG",
          "590:     from airflow.operators.empty import EmptyOperator",
          "592:     execution_date = pendulum.parse(\"20200101\")",
          "594:     with DAG(dag_id=\"s_t_dag\", start_date=execution_date) as dag:",
          "596:         @task",
          "597:         def test_task():",
          "598:             print(\"Hello world!\")",
          "600:         @task_group",
          "601:         def inner():",
          "602:             inner_start = EmptyOperator(task_id=\"start\")",
          "603:             inner_end = EmptyOperator(task_id=\"end\")",
          "605:             test_task_r = test_task.override(task_id=\"work\")()",
          "606:             inner_start >> test_task_r >> inner_end.as_teardown(setups=inner_start)",
          "608:         @task_group",
          "609:         def outer():",
          "610:             outer_work = EmptyOperator(task_id=\"work\")",
          "611:             inner_group = inner()",
          "612:             inner_group >> outer_work",
          "614:         dag_start = EmptyOperator(task_id=\"dag_start\")",
          "615:         dag_end = EmptyOperator(task_id=\"dag_end\")",
          "616:         dag_start >> outer() >> dag_end",
          "618:     edges = dag_edges(dag)",
          "620:     actual = sorted((e[\"source_id\"], e[\"target_id\"], e.get(\"is_setup_teardown\")) for e in edges)",
          "621:     assert actual == [",
          "622:         (\"dag_start\", \"outer.upstream_join_id\", None),",
          "623:         (\"outer.downstream_join_id\", \"dag_end\", None),",
          "624:         (\"outer.inner.downstream_join_id\", \"outer.work\", None),",
          "625:         (\"outer.inner.start\", \"outer.inner.end\", True),",
          "626:         (\"outer.inner.start\", \"outer.inner.work\", None),",
          "627:         (\"outer.inner.work\", \"outer.inner.downstream_join_id\", None),",
          "628:         (\"outer.inner.work\", \"outer.inner.end\", None),",
          "629:         (\"outer.upstream_join_id\", \"outer.inner.start\", None),",
          "630:         (\"outer.work\", \"outer.downstream_join_id\", None),",
          "631:     ]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0db3bc6b53cdb86720900418b63f515b76db46f8",
      "candidate_info": {
        "commit_hash": "0db3bc6b53cdb86720900418b63f515b76db46f8",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0db3bc6b53cdb86720900418b63f515b76db46f8",
        "files": [
          ".github/workflows/build-images.yml",
          ".github/workflows/ci.yml",
          "airflow/provider.yaml.schema.json",
          "dev/README_RELEASE_PROVIDER_PACKAGES.md",
          "dev/breeze/src/airflow_breeze/commands/common_options.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands_config.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py",
          "dev/breeze/src/airflow_breeze/utils/packages.py",
          "images/breeze/output_build-docs.svg",
          "images/breeze/output_build-docs.txt",
          "images/breeze/output_release-management_add-back-references.svg",
          "images/breeze/output_release-management_add-back-references.txt",
          "images/breeze/output_release-management_install-provider-packages.txt",
          "images/breeze/output_release-management_prepare-provider-documentation.svg",
          "images/breeze/output_release-management_prepare-provider-documentation.txt",
          "images/breeze/output_release-management_prepare-provider-packages.svg",
          "images/breeze/output_release-management_prepare-provider-packages.txt",
          "images/breeze/output_release-management_publish-docs.svg",
          "images/breeze/output_release-management_publish-docs.txt",
          "images/breeze/output_release-management_verify-provider-packages.txt",
          "images/breeze/output_shell.txt",
          "images/breeze/output_start-airflow.txt"
        ],
        "message": "Add feture of \"not-ready\" provider. (#36391)\n\nThis PR adds possibility of marking the provider as \"not ready\" in the\nprovider.yaml (by setting optional field as \"not-ready\" to `true\".\n\nSetting provider as \"not-ready\", removes it by default from all the\nrelease management commands - preparing documentation files preparing\nprovider packages, publishing docs.\n\nYou can include such providers via `--include-not-ready-providers`\nflag (or setting INCLUDE_NOT_READY_PROVIDERS environment variable to\ntrue).\n\nThis flag is set to True in our CI, so that we can make sure the\nproviders in-progress are also being tested and verified, but when\nrelease manager prepares packages, those providers are not prepared.\n\nThat will help in early stage of a lifecycle of a provider when we\nalready want to iterate and test it continuously, but - for example\nthe API of such provider is not yet stable or when we are in progress\nof moving functionality for such provider from core.\n\nThis PR also marks `fab` providers as \"not-ready\" as it is still\nearly days and we want to exclude it for now from any kind of release\nprocess.\n\n(cherry picked from commit 341d5b747db78b9be00d5d5dc491e37d413570da)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/common_options.py||dev/breeze/src/airflow_breeze/commands/common_options.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands_config.py||dev/breeze/src/airflow_breeze/commands/developer_commands_config.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py||dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py",
          "dev/breeze/src/airflow_breeze/utils/packages.py||dev/breeze/src/airflow_breeze/utils/packages.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/common_options.py||dev/breeze/src/airflow_breeze/commands/common_options.py": [
          "File: dev/breeze/src/airflow_breeze/commands/common_options.py -> dev/breeze/src/airflow_breeze/commands/common_options.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "74:     required=False,",
          "75:     type=NotVerifiedBetterChoice(",
          "76:         get_available_packages(",
          "78:         )",
          "79:     ),",
          "80: )",
          "",
          "[Removed Lines]",
          "77:             include_non_provider_doc_packages=True, include_all_providers=True, include_removed=True",
          "",
          "[Added Lines]",
          "77:             include_non_provider_doc_packages=True,",
          "78:             include_all_providers=True,",
          "79:             include_removed=True,",
          "80:             include_not_ready=True,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "189:     is_flag=True,",
          "190:     envvar=\"INCLUDE_REMOVED_PROVIDERS\",",
          "191: )",
          "192: option_include_success_outputs = click.option(",
          "193:     \"--include-success-outputs\",",
          "194:     help=\"Whether to include outputs of successful parallel runs (skipped by default).\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "195: option_include_not_ready_providers = click.option(",
          "196:     \"--include-not-ready-providers\",",
          "197:     help=\"Whether to include providers that are not yet ready to be released.\",",
          "198:     is_flag=True,",
          "199:     envvar=\"INCLUDE_NOT_READY_PROVIDERS\",",
          "200: )",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/developer_commands.py -> dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:     option_forward_credentials,",
          "45:     option_github_repository,",
          "46:     option_image_tag_for_running,",
          "47:     option_include_removed_providers,",
          "48:     option_installation_package_format,",
          "49:     option_integration,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "47:     option_include_not_ready_providers,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "590: @click.option(\"-d\", \"--docs-only\", help=\"Only build documentation.\", is_flag=True)",
          "591: @option_dry_run",
          "592: @option_github_repository",
          "593: @option_include_removed_providers",
          "594: @click.option(",
          "595:     \"--one-pass-only\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "594: @option_include_not_ready_providers",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "612:     clean_build: bool,",
          "613:     docs_only: bool,",
          "614:     github_repository: str,",
          "615:     include_removed_providers: bool,",
          "616:     one_pass_only: bool,",
          "617:     package_filter: tuple[str, ...],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "617:     include_not_ready_providers: bool,",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "640:         spellcheck_only=spellcheck_only,",
          "641:         one_pass_only=one_pass_only,",
          "642:         short_doc_packages=expand_all_provider_packages(",
          "644:         ),",
          "645:     )",
          "646:     cmd = \"/opt/airflow/scripts/in_container/run_docs_build.sh \" + \" \".join(",
          "",
          "[Removed Lines]",
          "643:             doc_packages, include_removed=include_removed_providers",
          "",
          "[Added Lines]",
          "646:             short_doc_packages=doc_packages,",
          "647:             include_removed=include_removed_providers,",
          "648:             include_not_ready=include_not_ready_providers,",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/developer_commands_config.py||dev/breeze/src/airflow_breeze/commands/developer_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/developer_commands_config.py -> dev/breeze/src/airflow_breeze/commands/developer_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "294:                 \"--clean-build\",",
          "295:                 \"--one-pass-only\",",
          "296:                 \"--package-filter\",",
          "297:                 \"--include-removed-providers\",",
          "298:                 \"--github-repository\",",
          "299:                 \"--builder\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "297:                 \"--include-not-ready-providers\",",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "46:     option_github_repository,",
          "47:     option_historical_python_version,",
          "48:     option_image_tag_for_running,",
          "49:     option_include_removed_providers,",
          "50:     option_include_success_outputs,",
          "51:     option_installation_package_format,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "49:     option_include_not_ready_providers,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "153:     \"provider_packages\",",
          "154:     nargs=-1,",
          "155:     required=False,",
          "157: )",
          "158: option_airflow_site_directory = click.option(",
          "159:     \"-a\",",
          "",
          "[Removed Lines]",
          "156:     type=NotVerifiedBetterChoice(get_available_packages(include_removed=False)),",
          "",
          "[Added Lines]",
          "157:     type=NotVerifiedBetterChoice(get_available_packages(include_removed=False, include_not_ready=False)),",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "346: @argument_provider_packages",
          "347: @option_answer",
          "348: @option_dry_run",
          "349: @option_include_removed_providers",
          "350: @click.option(",
          "351:     \"--non-interactive\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "350: @option_include_not_ready_providers",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "368: def prepare_provider_documentation(",
          "369:     base_branch: str,",
          "370:     github_repository: str,",
          "371:     include_removed_providers: bool,",
          "372:     non_interactive: bool,",
          "373:     only_min_version_update: bool,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "373:     include_not_ready_providers: bool,",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "390:     fix_ownership_using_docker()",
          "391:     cleanup_python_generated_files()",
          "392:     if not provider_packages:",
          "395:     if not skip_git_fetch:",
          "396:         run_command([\"git\", \"remote\", \"rm\", \"apache-https-for-providers\"], check=False, stderr=DEVNULL)",
          "",
          "[Removed Lines]",
          "393:         provider_packages = get_available_packages(include_removed=include_removed_providers)",
          "",
          "[Added Lines]",
          "396:         provider_packages = get_available_packages(",
          "397:             include_removed=include_removed_providers, include_not_ready=include_not_ready_providers",
          "398:         )",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "520: )",
          "521: @option_dry_run",
          "522: @option_github_repository",
          "523: @option_include_removed_providers",
          "524: @argument_provider_packages",
          "525: @option_verbose",
          "526: def prepare_provider_packages(",
          "527:     clean_dist: bool,",
          "528:     github_repository: str,",
          "529:     include_removed_providers: bool,",
          "530:     package_format: str,",
          "531:     package_list_file: IO | None,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "528: @option_include_not_ready_providers",
          "535:     include_not_ready_providers: bool,",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "538:     fix_ownership_using_docker()",
          "539:     cleanup_python_generated_files()",
          "540:     packages_list = get_packages_list_to_act_on(",
          "542:     )",
          "543:     if not skip_tag_check:",
          "544:         run_command([\"git\", \"remote\", \"rm\", \"apache-https-for-providers\"], check=False, stderr=DEVNULL)",
          "",
          "[Removed Lines]",
          "541:         package_list_file, provider_packages, include_removed_providers",
          "",
          "[Added Lines]",
          "548:         package_list_file=package_list_file,",
          "549:         provider_packages=provider_packages,",
          "550:         include_removed=include_removed_providers,",
          "551:         include_not_ready=include_not_ready_providers,",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "1145: @option_airflow_site_directory",
          "1146: @option_debug_resources",
          "1147: @option_dry_run",
          "1148: @option_include_removed_providers",
          "1149: @option_include_success_outputs",
          "1150: @click.option(\"-s\", \"--override-versioned\", help=\"Overrides versioned directories.\", is_flag=True)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1158: @option_include_not_ready_providers",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "1165:     debug_resources: bool,",
          "1166:     doc_packages: tuple[str, ...],",
          "1167:     include_success_outputs: bool,",
          "1168:     include_removed_providers: bool,",
          "1169:     override_versioned: bool,",
          "1170:     package_filter: tuple[str, ...],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1179:     include_not_ready_providers: bool,",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1180:         )",
          "1182:     current_packages = find_matching_long_package_names(",
          "1184:         filters=package_filter,",
          "1185:     )",
          "1186:     print(f\"Publishing docs for {len(current_packages)} package(s)\")",
          "",
          "[Removed Lines]",
          "1183:         short_packages=expand_all_provider_packages(doc_packages, include_removed=include_removed_providers),",
          "",
          "[Added Lines]",
          "1195:         short_packages=expand_all_provider_packages(",
          "1196:             short_doc_packages=doc_packages,",
          "1197:             include_removed=include_removed_providers,",
          "1198:             include_not_ready=include_not_ready_providers,",
          "1199:         ),",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "1209:     help=\"Command to add back references for documentation to make it backward compatible.\",",
          "1210: )",
          "1211: @option_airflow_site_directory",
          "1212: @option_include_removed_providers",
          "1213: @argument_doc_packages",
          "1214: @option_dry_run",
          "1215: @option_verbose",
          "1216: def add_back_references(",
          "1217:     airflow_site_directory: str,",
          "1218:     include_removed_providers: bool,",
          "1219:     doc_packages: tuple[str, ...],",
          "1220: ):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1228: @option_include_not_ready_providers",
          "1235:     include_not_ready_providers: bool,",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "1232:         )",
          "1233:         sys.exit(1)",
          "1234:     start_generating_back_references(",
          "1236:     )",
          "",
          "[Removed Lines]",
          "1235:         site_path, list(expand_all_provider_packages(doc_packages, include_removed=include_removed_providers))",
          "",
          "[Added Lines]",
          "1253:         site_path,",
          "1254:         list(",
          "1255:             expand_all_provider_packages(",
          "1256:                 short_doc_packages=doc_packages,",
          "1257:                 include_removed=include_removed_providers,",
          "1258:                 include_not_ready=include_not_ready_providers,",
          "1259:             )",
          "1260:         ),",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "132:             \"options\": [",
          "133:                 \"--clean-dist\",",
          "134:                 \"--github-repository\",",
          "135:                 \"--include-removed-providers\",",
          "136:                 \"--package-format\",",
          "137:                 \"--package-list-file\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "135:                 \"--include-not-ready-providers\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "147:             \"options\": [",
          "148:                 \"--base-branch\",",
          "149:                 \"--github-repository\",",
          "150:                 \"--include-removed-providers\",",
          "151:                 \"--non-interactive\",",
          "152:                 \"--only-min-version-update\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "151:                 \"--include-not-ready-providers\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "197:             \"name\": \"Publish Docs\",",
          "198:             \"options\": [",
          "199:                 \"--airflow-site-directory\",",
          "200:                 \"--include-removed-providers\",",
          "201:                 \"--override-versioned\",",
          "202:                 \"--package-filter\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "202:                 \"--include-not-ready-providers\",",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "218:             \"name\": \"Add Back References to Docs\",",
          "219:             \"options\": [",
          "220:                 \"--airflow-site-directory\",",
          "221:                 \"--include-removed-providers\",",
          "222:             ],",
          "223:         },",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "224:                 \"--include-not-ready-providers\",",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py||dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py": [
          "File: dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py -> dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import sys",
          "23: from pathlib import Path",
          "24: from shutil import copytree, rmtree",
          "27: from airflow_breeze.utils.console import get_console",
          "28: from airflow_breeze.utils.packages import (",
          "29:     get_available_packages,",
          "30:     get_latest_provider_tag,",
          "31:     get_provider_details,",
          "32:     get_provider_jinja_context,",
          "33:     get_removed_provider_ids,",
          "",
          "[Removed Lines]",
          "25: from typing import IO, Any",
          "",
          "[Added Lines]",
          "25: from typing import Any, TextIO",
          "31:     get_not_ready_provider_ids,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "230: def get_packages_list_to_act_on(",
          "232: ) -> list[str]:",
          "233:     if package_list_file and provider_packages:",
          "234:         get_console().print(",
          "",
          "[Removed Lines]",
          "231:     package_list_file: IO | None, provider_packages: tuple[str, ...], include_removed: bool = False",
          "",
          "[Added Lines]",
          "232:     package_list_file: TextIO | None,",
          "233:     provider_packages: tuple[str, ...],",
          "234:     include_not_ready: bool = False,",
          "235:     include_removed: bool = False,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "237:         sys.exit(1)",
          "238:     if package_list_file:",
          "239:         removed_provider_ids = get_removed_provider_ids()",
          "240:         return [",
          "241:             package.strip()",
          "242:             for package in package_list_file.readlines()",
          "244:         ]",
          "245:     elif provider_packages:",
          "246:         return list(provider_packages)",
          "",
          "[Removed Lines]",
          "243:             if package.strip() not in removed_provider_ids",
          "247:     return get_available_packages(include_removed=include_removed)",
          "",
          "[Added Lines]",
          "244:         not_ready_provider_ids = get_not_ready_provider_ids()",
          "248:             if (package.strip() not in removed_provider_ids or include_removed)",
          "249:             and (package.strip() not in not_ready_provider_ids or include_not_ready)",
          "253:     return get_available_packages(include_removed=include_removed, include_not_ready=include_not_ready)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/packages.py||dev/breeze/src/airflow_breeze/utils/packages.py": [
          "File: dev/breeze/src/airflow_breeze/utils/packages.py -> dev/breeze/src/airflow_breeze/utils/packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "228:     ]",
          "231: def get_provider_requirements(provider_id: str) -> list[str]:",
          "232:     package_metadata = get_provider_packages_metadata().get(provider_id)",
          "233:     return package_metadata[\"dependencies\"] if package_metadata else []",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "231: @lru_cache",
          "232: def get_not_ready_provider_ids() -> list[str]:",
          "233:     return [",
          "234:         provider_id",
          "235:         for provider_id, provider_metadata in get_provider_packages_metadata().items()",
          "236:         if provider_metadata.get(\"not-ready\", False)",
          "237:     ]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "239:     include_all_providers: bool = False,",
          "240:     include_suspended: bool = False,",
          "241:     include_removed: bool = False,",
          "242: ) -> list[str]:",
          "243:     \"\"\"",
          "244:     Return provider ids for all packages that are available currently (not suspended).",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "251:     include_not_ready: bool = False,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "246:     :rtype: object",
          "247:     :param include_suspended: whether the suspended packages should be included",
          "248:     :param include_removed: whether the removed packages should be included",
          "249:     :param include_non_provider_doc_packages: whether the non-provider doc packages should be included",
          "250:            (packages like apache-airflow, helm-chart, docker-stack)",
          "251:     :param include_all_providers: whether \"all-providers\" should be included ni the list.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "259:     :param include_not_ready: whether the not-ready ppackages should be included",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "253:     \"\"\"",
          "254:     provider_ids: list[str] = list(json.loads(PROVIDER_DEPENDENCIES_JSON_FILE_PATH.read_text()).keys())",
          "255:     available_packages = []",
          "256:     if include_non_provider_doc_packages:",
          "257:         available_packages.extend(REGULAR_DOC_PACKAGES)",
          "258:     if include_all_providers:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "267:     not_ready_provider_ids = get_not_ready_provider_ids()",
          "268:     if not include_not_ready:",
          "269:         provider_ids = [",
          "270:             provider_id for provider_id in provider_ids if provider_id not in not_ready_provider_ids",
          "271:         ]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "268: def expand_all_provider_packages(",
          "270: ) -> tuple[str, ...]:",
          "271:     \"\"\"In case there are \"all-providers\" in the list, expand the list with all providers.\"\"\"",
          "272:     if \"all-providers\" in short_doc_packages:",
          "273:         packages = [package for package in short_doc_packages if package != \"all-providers\"]",
          "275:         short_doc_packages = tuple(set(packages))",
          "276:     return short_doc_packages",
          "",
          "[Removed Lines]",
          "269:     short_doc_packages: tuple[str, ...], include_removed: bool = False",
          "274:         packages.extend(get_available_packages(include_removed=include_removed))",
          "",
          "[Added Lines]",
          "285:     short_doc_packages: tuple[str, ...],",
          "286:     include_removed: bool = False,",
          "287:     include_not_ready: bool = False,",
          "292:         packages.extend(",
          "293:             get_available_packages(include_removed=include_removed, include_not_ready=include_not_ready)",
          "294:         )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5576e1151dcabe66b18b350d296574631d4f493e",
      "candidate_info": {
        "commit_hash": "5576e1151dcabe66b18b350d296574631d4f493e",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/5576e1151dcabe66b18b350d296574631d4f493e",
        "files": [
          "dev/README_RELEASE_PROVIDER_PACKAGES.md",
          "generated/provider_metadata.json"
        ],
        "message": "Update providers metadata 2023-12-28 (#36459)\n\n(cherry picked from commit 8fea49f90a6b1597648d5b95982624eaff34fd68)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "4bad5e4f95990c918e7138c83e900200524ec61f",
      "candidate_info": {
        "commit_hash": "4bad5e4f95990c918e7138c83e900200524ec61f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4bad5e4f95990c918e7138c83e900200524ec61f",
        "files": [
          "airflow/providers/apache/hdfs/provider.yaml",
          "airflow/providers/apache/kylin/provider.yaml",
          "dev/breeze/src/airflow_breeze/breeze.py"
        ],
        "message": "Fixes small issues related to suspended/removed providers (#36501)\n\nAfter speeding up breeze in #36499 there are few small fixes needed\nfor suspended/removed providers.",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/breeze.py||dev/breeze/src/airflow_breeze/breeze.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/breeze.py||dev/breeze/src/airflow_breeze/breeze.py": [
          "File: dev/breeze/src/airflow_breeze/breeze.py -> dev/breeze/src/airflow_breeze/breeze.py"
        ]
      }
    },
    {
      "candidate_hash": "d80d6a1a0143055598c0b269c015b426d7191b4e",
      "candidate_info": {
        "commit_hash": "d80d6a1a0143055598c0b269c015b426d7191b4e",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/d80d6a1a0143055598c0b269c015b426d7191b4e",
        "files": [
          "airflow/operators/bash.py",
          "tests/operators/test_bash.py"
        ],
        "message": "Fix AirflowSkipException message raised by BashOperator (#36354)\n\n(cherry picked from commit 667a5b2d29c1ce46021d400fa591650855dcf26c)",
        "before_after_code_files": [
          "airflow/operators/bash.py||airflow/operators/bash.py",
          "tests/operators/test_bash.py||tests/operators/test_bash.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/operators/bash.py||airflow/operators/bash.py": [
          "File: airflow/operators/bash.py -> airflow/operators/bash.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "162:             skip_on_exit_code",
          "163:             if isinstance(skip_on_exit_code, Container)",
          "164:             else [skip_on_exit_code]",
          "166:             else []",
          "167:         )",
          "168:         self.cwd = cwd",
          "",
          "[Removed Lines]",
          "165:             if skip_on_exit_code",
          "",
          "[Added Lines]",
          "165:             if skip_on_exit_code is not None",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "206:             output_encoding=self.output_encoding,",
          "207:             cwd=self.cwd,",
          "208:         )",
          "211:         elif result.exit_code != 0:",
          "212:             raise AirflowException(",
          "213:                 f\"Bash command failed. The command returned a non-zero exit code {result.exit_code}.\"",
          "",
          "[Removed Lines]",
          "209:         if self.skip_on_exit_code is not None and result.exit_code in self.skip_on_exit_code:",
          "210:             raise AirflowSkipException(f\"Bash command returned exit code {self.skip_on_exit_code}. Skipping.\")",
          "",
          "[Added Lines]",
          "209:         if result.exit_code in self.skip_on_exit_code:",
          "210:             raise AirflowSkipException(f\"Bash command returned exit code {result.exit_code}. Skipping.\")",
          "",
          "---------------"
        ],
        "tests/operators/test_bash.py||tests/operators/test_bash.py": [
          "File: tests/operators/test_bash.py -> tests/operators/test_bash.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "169:     @pytest.mark.parametrize(",
          "170:         \"extra_kwargs,actual_exit_code,expected_exc\",",
          "171:         [",
          "175:             ({\"skip_on_exit_code\": None}, 99, AirflowException),",
          "176:             ({\"skip_on_exit_code\": [100]}, 100, AirflowSkipException),",
          "181:         ],",
          "182:     )",
          "183:     def test_skip(self, extra_kwargs, actual_exit_code, expected_exc):",
          "",
          "[Removed Lines]",
          "172:             (None, 99, AirflowSkipException),",
          "173:             ({\"skip_on_exit_code\": 100}, 100, AirflowSkipException),",
          "174:             ({\"skip_on_exit_code\": 100}, 101, AirflowException),",
          "177:             ({\"skip_on_exit_code\": (100, 101)}, 100, AirflowSkipException),",
          "178:             ({\"skip_on_exit_code\": 100}, 101, AirflowException),",
          "179:             ({\"skip_on_exit_code\": [100, 102]}, 101, AirflowException),",
          "180:             ({\"skip_on_exit_code\": None}, 0, None),",
          "",
          "[Added Lines]",
          "172:             ({}, 0, None),",
          "173:             ({}, 100, AirflowException),",
          "174:             ({}, 99, AirflowSkipException),",
          "175:             ({\"skip_on_exit_code\": None}, 0, None),",
          "176:             ({\"skip_on_exit_code\": None}, 100, AirflowException),",
          "178:             ({\"skip_on_exit_code\": 100}, 0, None),",
          "179:             ({\"skip_on_exit_code\": 100}, 100, AirflowSkipException),",
          "180:             ({\"skip_on_exit_code\": 100}, 99, AirflowException),",
          "181:             ({\"skip_on_exit_code\": 0}, 0, AirflowSkipException),",
          "182:             ({\"skip_on_exit_code\": [100]}, 0, None),",
          "184:             ({\"skip_on_exit_code\": [100]}, 99, AirflowException),",
          "185:             ({\"skip_on_exit_code\": [100, 102]}, 99, AirflowException),",
          "186:             ({\"skip_on_exit_code\": (100,)}, 0, None),",
          "187:             ({\"skip_on_exit_code\": (100,)}, 100, AirflowSkipException),",
          "188:             ({\"skip_on_exit_code\": (100,)}, 99, AirflowException),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b06b1fc8be66298bb8682dcb87b5413ee88d63af",
      "candidate_info": {
        "commit_hash": "b06b1fc8be66298bb8682dcb87b5413ee88d63af",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b06b1fc8be66298bb8682dcb87b5413ee88d63af",
        "files": [
          ".github/ISSUE_TEMPLATE/airflow_bug_report.yml",
          "Dockerfile"
        ],
        "message": "Airflow 2.8.0 has been released (#36298)\n\n* Update RELEASE_NOTES.rst\n\n(cherry picked from commit 26990e2ed640039e14458cb223e2d6801e315770)\n\n* Update RELEASE_NOTES.rst\n\n(cherry picked from commit d0c1c452ce06a89c0ea24ef21384efe26393a446)\n\n* Update RELEASE_NOTES.rst\n\n(cherry picked from commit db2b75c233e3e3c59ec9d0563b93ddbe733ad0bf)\n\n* Airflow 2.8.0 has been released\n\n* fixup! Airflow 2.8.0 has been released\n\n(cherry picked from commit 51d31147894c543c5231a94fedbc4ff1589a32eb)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "2c2c6a3129fd382c2ad899d597a8a1cbaf78530c",
      "candidate_info": {
        "commit_hash": "2c2c6a3129fd382c2ad899d597a8a1cbaf78530c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/2c2c6a3129fd382c2ad899d597a8a1cbaf78530c",
        "files": [
          "docs/apache-airflow/core-concepts/tasks.rst",
          "docs/apache-airflow/howto/set-up-database.rst"
        ],
        "message": "Add description on the ways how users should approach DB monitoring (#36483)\n\n* Add description on the ways how users should approach DB monitoring\n\nOften our users are not aware that they are responsible for setting\nup and monitoring the database they chose as the metaa-data backend.\n\nWhile details of the tables and database structure of the metadata\nDB used by Airflow is internal detail, the monitoring, tracking\nthe usage, fine-tuning and optimisation of the database configuration\nand detecting some cases where database becomes a bottle neck is\ngenerally a task that Deployment Manager should be aware of and\nit should be approached in a generic way - specific to the database\nchosen by the Deployment Manager and it also depends a lot on the\nchoice of managed database if managed database is chosen by the\nDeployment Manager.\n\nThis chapter makes it explicit and gives enough leads to the\nDeployment Manager to be able to follow after they chose the\ndatabase, it also explain the specific parameters tha the\nDeployment Manager should pay attention to when setting up\nsuch monitoring.\n\nWe also add an explanation of how Deployment Manager can setup\nclient-side logging of SQL queries generated by Airflow in case\ndatabase access is suspected for performance issues with Airflow,\nas a poor-man's version of complete, server-side monitoring and\nexplains caveats of such client side configuraiton.\n\n* Update docs/apache-airflow/howto/set-up-database.rst\n\nCo-authored-by: Jens Scheffler <95105677+jscheffl@users.noreply.github.com>\n\n* fixup! Update docs/apache-airflow/howto/set-up-database.rst\n\n---------\n\nCo-authored-by: Jens Scheffler <95105677+jscheffl@users.noreply.github.com>\n(cherry picked from commit dea715dd4e148c29c19b4100183785b9d91df44d)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "2c10e93f6745e9f86c3be901839b89eaac374ce2",
      "candidate_info": {
        "commit_hash": "2c10e93f6745e9f86c3be901839b89eaac374ce2",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/2c10e93f6745e9f86c3be901839b89eaac374ce2",
        "files": [
          "airflow/__init__.py",
          "docs/docker-stack/README.md",
          "docs/docker-stack/docker-examples/extending/add-airflow-configuration/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-apt-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-build-essential-extend/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-providers/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-pypi-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-requirement-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/custom-providers/Dockerfile",
          "docs/docker-stack/docker-examples/extending/embedding-dags/Dockerfile",
          "docs/docker-stack/docker-examples/extending/writable-directory/Dockerfile",
          "docs/docker-stack/entrypoint.rst"
        ],
        "message": "Update version of airflow to 2.8.1.dev0",
        "before_after_code_files": [
          "airflow/__init__.py||airflow/__init__.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/__init__.py||airflow/__init__.py": [
          "File: airflow/__init__.py -> airflow/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: \"\"\"",
          "27: from __future__ import annotations",
          "31: # flake8: noqa: F401",
          "",
          "[Removed Lines]",
          "29: __version__ = \"2.8.0\"",
          "",
          "[Added Lines]",
          "29: __version__ = \"2.8.1.dev0\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "054ee36ea19d0f2336ec8f50e1bda1fd4308d365",
      "candidate_info": {
        "commit_hash": "054ee36ea19d0f2336ec8f50e1bda1fd4308d365",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/054ee36ea19d0f2336ec8f50e1bda1fd4308d365",
        "files": [
          ".dockerignore",
          ".github/actions/build-prod-images/action.yml",
          ".github/workflows/ci.yml",
          ".gitignore",
          ".pre-commit-config.yaml",
          ".rat-excludes",
          "3rd-party-licenses/LICENSE-bootstrap.txt",
          "3rd-party-licenses/LICENSE-bootstrap3-typeahead.txt",
          "3rd-party-licenses/LICENSE-d3-shape.txt",
          "3rd-party-licenses/LICENSE-d3-tip.txt",
          "3rd-party-licenses/LICENSE-d3js.txt",
          "3rd-party-licenses/LICENSE-dagre-d3.txt",
          "3rd-party-licenses/LICENSE-datatables.txt",
          "3rd-party-licenses/LICENSE-elasticmock.txt",
          "3rd-party-licenses/LICENSE-eonasdan-bootstrap-datetimepicker.txt",
          "3rd-party-licenses/LICENSE-flask-kerberos.txt",
          "3rd-party-licenses/LICENSE-hue.txt",
          "3rd-party-licenses/LICENSE-jqclock.txt",
          "3rd-party-licenses/LICENSE-jquery.txt",
          "3rd-party-licenses/LICENSE-moment.txt",
          "3rd-party-licenses/LICENSE-normalize.txt",
          "3rd-party-licenses/LICENSE-pytest-capture-warnings.txt",
          "3rd-party-licenses/LICENSE-unicodecsv.txt",
          "BREEZE.rst",
          "CI.rst",
          "CONTRIBUTING.rst",
          "CONTRIBUTORS_QUICK_START.rst",
          "Dockerfile",
          "Dockerfile.ci",
          "IMAGES.rst",
          "INSTALL",
          "LOCAL_VIRTUALENV.rst",
          "MANIFEST.in",
          "README.md",
          "STATIC_CODE_CHECKS.rst",
          "airflow/_vendor/README.md",
          "airflow/cli/cli_parser.py",
          "airflow/configuration.py",
          "airflow/provider.yaml.schema.json",
          "airflow/providers/MANAGING_PROVIDERS_LIFECYCLE.rst",
          "airflow/providers/amazon/provider.yaml",
          "airflow/providers/apache/beam/provider.yaml",
          "airflow/providers/apache/hive/provider.yaml",
          "airflow/providers/apache/impala/provider.yaml",
          "airflow/providers/apache/spark/provider.yaml",
          "airflow/providers/celery/provider.yaml",
          "airflow/providers/databricks/provider.yaml",
          "airflow/providers/google/provider.yaml",
          "airflow/providers/microsoft/azure/provider.yaml",
          "airflow/providers/mongo/provider.yaml",
          "airflow/providers/mysql/provider.yaml",
          "airflow/providers/oracle/provider.yaml",
          "airflow/providers/postgres/provider.yaml",
          "airflow/providers/tabular/provider.yaml",
          "airflow/providers_manager.py",
          "airflow/www/webpack.config.js",
          "clients/gen/common.sh",
          "dev/MANUALLY_GENERATING_IMAGE_CACHE_AND_CONSTRAINTS.md",
          "dev/README.md",
          "dev/README_RELEASE_AIRFLOW.md",
          "dev/README_RELEASE_PROVIDER_PACKAGES.md",
          "dev/airflow_pre_installed_providers.txt",
          "dev/breeze/README.md",
          "dev/breeze/SELECTIVE_CHECKS.md",
          "dev/breeze/doc/adr/0003-bootstrapping-virtual-environment.md",
          "dev/breeze/pyproject.toml",
          "dev/breeze/src/airflow_breeze/commands/ci_image_commands.py",
          "dev/breeze/src/airflow_breeze/commands/ci_image_commands_config.py",
          "dev/breeze/src/airflow_breeze/commands/common_image_options.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands_config.py",
          "dev/breeze/src/airflow_breeze/commands/production_image_commands.py",
          "dev/breeze/src/airflow_breeze/commands/production_image_commands_config.py",
          "dev/breeze/src/airflow_breeze/commands/release_candidate_command.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/global_constants.py",
          "dev/breeze/src/airflow_breeze/params/build_ci_params.py",
          "dev/breeze/src/airflow_breeze/params/build_prod_params.py",
          "dev/breeze/src/airflow_breeze/params/common_build_params.py",
          "dev/breeze/src/airflow_breeze/params/shell_params.py",
          "dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py",
          "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/src/airflow_breeze/utils/kubernetes_utils.py",
          "dev/breeze/src/airflow_breeze/utils/packages.py",
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "dev/breeze/tests/test_selective_checks.py",
          "dev/hatch_build.py",
          "dev/prod_image_installed_providers.txt",
          "dev/refresh_images.sh",
          "dev/sign.sh",
          "docker_tests/docker_tests_utils.py",
          "docker_tests/test_prod_image.py",
          "docs/apache-airflow-providers-apache-hive/index.rst",
          "docs/apache-airflow-providers/howto/create-custom-providers.rst",
          "docs/apache-airflow/administration-and-deployment/modules_management.rst",
          "docs/apache-airflow/extra-packages-ref.rst",
          "docs/apache-airflow/installation/installing-from-pypi.rst",
          "docs/docker-stack/build-arg-ref.rst",
          "docs/spelling_wordlist.txt",
          "generated/PYPI_README.md",
          "generated/provider_dependencies.json",
          "images/breeze/output_ci-image_build.svg",
          "images/breeze/output_ci-image_build.txt",
          "images/breeze/output_prod-image_build.svg",
          "images/breeze/output_prod-image_build.txt",
          "images/breeze/output_release-management.svg",
          "images/breeze/output_release-management.txt",
          "images/breeze/output_release-management_generate-constraints.svg",
          "images/breeze/output_release-management_generate-constraints.txt",
          "images/breeze/output_release-management_prepare-airflow-package.svg",
          "images/breeze/output_release-management_prepare-airflow-package.txt",
          "images/breeze/output_start-airflow.svg",
          "images/breeze/output_start-airflow.txt",
          "images/breeze/output_static-checks.svg",
          "images/breeze/output_static-checks.txt",
          "newsfragments/36537.significant.rst",
          "pyproject.toml",
          "scripts/ci/docker-compose/devcontainer.env",
          "scripts/ci/docker-compose/local.yml",
          "scripts/ci/kubernetes/k8s_requirements.txt",
          "scripts/ci/pre_commit/pre_commit_check_extra_packages_ref.py",
          "scripts/ci/pre_commit/pre_commit_check_order_dockerfile_extras.py",
          "scripts/ci/pre_commit/pre_commit_check_order_pyproject_toml.py",
          "scripts/ci/pre_commit/pre_commit_check_order_setup.py",
          "scripts/ci/pre_commit/pre_commit_check_setup_extra_packages_ref.py",
          "scripts/ci/pre_commit/pre_commit_compile_www_assets.py",
          "scripts/ci/pre_commit/pre_commit_insert_extras.py",
          "scripts/ci/pre_commit/pre_commit_sort_installed_providers.py",
          "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py",
          "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py.md5sum",
          "scripts/docker/entrypoint_ci.sh",
          "scripts/docker/install_airflow.sh",
          "scripts/docker/install_airflow_dependencies_from_branch_tip.sh",
          "scripts/in_container/run_generate_constraints.py",
          "scripts/in_container/run_prepare_airflow_packages.py",
          "scripts/in_container/verify_providers.py",
          "scripts/tools/initialize_virtualenv.py",
          "setup.cfg",
          "setup.py",
          "tests/cli/commands/test_task_command.py",
          "tests/plugins/test_plugins_manager.py",
          "tests/providers/google/cloud/utils/base_gcp_mock.py"
        ],
        "message": "Standardize airflow build process and switch to Hatchling build backend (#36537)\n\nThis PR changes Airflow installation and build backend to use new\nstandard Python ways of building Python applications.\n\nWe've been trying to do it for quite a while. Airflow tranditionally\nhas been using complex and convoluted build process based on\nsetuptools and (extremely) custom setup.py file. It survived\nmigration to Airflow 2.0 and splitting Airlfow monorepo into\nAirflow and Providers, adding pre-installed providers and switching\nproviders to use flit (and follow build standards).\n\nSo far tooling in Python ecosystme had not been able to fuflill our\nneeds and we refrained to develop our own tooling, but finally with\nappearance of Hatch (managed by Python Packaging Authority) and\nfew recent advancements there we are finally able to swtich to\nPython standard ways of managing project dependnecy configuration\nand project build setup (with a few customizations).\n\nThis PR makes airflow build process follow those standard PEPs:\n\n* Airflow has all build configuration stored in pyproject.toml\n  following PEP 518 which allows any fronted (`pip`, `poetry`,\n  `hatch`, `flit`, or whatever other frontend is used to\n  install required build dependendencies to install Airflow\n  locally and to build distribution pacakges (sdist/wheel)\n\n* Hatchling backend follows PEP 517 for standard source tree and build\n  backend implementation that allows to execute the build in a\n  frontend-independent way\n\n* We store all project metadata in pyprooject.toml - following\n  PEP 621 where all necessary project metadata components were\n  defined.\n\n* We plug-in into Hatchling \"editable build\" hooks following\n  PEP 660. Hatchling internally builds editable wheel that\n  is used as ephemeral step and communication between backend\n  and frontend (and this ephemeral wheel is used to make\n  editable installation of the projeect - suitable for fast\n  iteration of code without reinstalling the package.\n\nWith Airflow having many provider packages in single source tree\nwhere we want to be able to install and develop airflow and\nproviders together, this is not a small feat to implement the\ncase wher editable installation has to behave quite a bit\ndifferently when it comes to packaging and dependencies for\neditable install (when you want to edit sources directly) and\ninstallable package (where you want to have separate Airflow\npackage and provider packages). Fortunately the standardisation\nefforts in the Python Packaging community and tooling implementing\nit had finally made it possible.\n\nSome of the important ways bow this has been achieved:\n\n* We continue using provider.yaml in providers as the single source\n  of trutgh for per-provider dependencies. We added a possibility\n  to specify \"devel-dependencies\" in provider.yaml so that all\n  per-provider dependencies in `generated/provider_dependencies.json`\n  and `pyproject.toml` are generated from those dependencies via\n  update-providers-dependencies pre-commit.\n\n* Pyproject.toml is generally managed manually, but the part where\n  provider dependencies and bundle dependencies are used is\n  automatically updated by a pre-commit whenever provider\n  dependencies change. Those generated provider dependencies contain\n  just dependencies of providers - not the provider packages, but\n  in the final \"standard\" wheel file they are replaced with\n  \"apache-airflow-providers-PROVIDER\" dependencies - so that the\n  wheel package will only install the provider and use the\n  dependencies of that version of provider it installs.\n\n* We are utilising custom hatchiling build hooks (PEP 660 standard)\n  that allow to modify 'standard' wheel package on-the-fly when\n  the wheel is being prepared by adding preinstalled package\n  dependencies (which are not needed in editable build) and by\n  removing all devel extras (that are not needed in the PyPI\n  distributed wheel package). This allows to solve the conundrum\n  of having different \"editable\" and \"standard\" behaviour while\n  keeping the same project specification in pyproject.toml.\n\n* We added description of how `Hatch` can be employed as build\n  frontend in order to manage local virtualenv and install Airflow\n  in editable way easily - while keeping all properties of the\n  installed application (including working airflow cli and\n  package metadata discovery) as well as how to use PEP-standard\n  ways of bulding wheel and sdist packages.\n\n* We have a custom step (following PEP-standards) to inject\n  airflow-specific build steps - compiling www assets and\n  generating git commit hash version to display it in the UI\n\n* We also show how all this makes it possible to make it easy to\n  manage local virtualenvs and editable installations for Airflow\n  contributors - without vendor lock-in of the build tools as\n  by following standard PEPs Airflow can be locally and editably\n  installed by anyone using any build front-end tools following\n  the standards - whether you use `pip`, `poetry`, `Hatch`, `flit`\n  or any other frontent build tools, Airflow local installation\n  and package building will work the same way for all of them,\n  where both \"editable\" and \"standard\" package prepration is\n  managed by `hatchling` backend in the same way.\n\n* Previously our extras contained a \".\" which is not normalized\n  name for extras - `pip` and other tools replaced it automatically\n  with `_'. This change updates the extra names to contain\n  '-' rather than '.' in the name, following PEP-685.  This should be\n  fully backwards compatible, users will still be able to use \".\" but it\n  will be normalized to \"-\" in Airflow packages. This is also future\n  proof as it is expected that all package managers and tools\n  will eventually use PEP-685 applied to extras, even if currently\n  some of the tools (pip + setuptools) might generate warnings.\n\n* Additionally, this change organizes the documentation around\n  the extras and dependencies, explaining the reasoning behind\n  all the different extras we have.\n\n* As a bonus (and this is what we used to test it all) we are\n  documenting how to use Hatch frontend to:\n\n  * manage multiple Python installations\n  * manage multiple Pythob virtualenv environments\n  * build Airflow packages for release management\n\n(cherry picked from commit c439ab87c421aaa6bd5d8074780e4f63606a1ef1)",
        "before_after_code_files": [
          "Dockerfile.ci||Dockerfile.ci",
          "MANIFEST.in||MANIFEST.in",
          "airflow/cli/cli_parser.py||airflow/cli/cli_parser.py",
          "airflow/configuration.py||airflow/configuration.py",
          "airflow/providers_manager.py||airflow/providers_manager.py",
          "airflow/www/webpack.config.js||airflow/www/webpack.config.js",
          "clients/gen/common.sh||clients/gen/common.sh",
          "dev/breeze/src/airflow_breeze/commands/ci_image_commands.py||dev/breeze/src/airflow_breeze/commands/ci_image_commands.py",
          "dev/breeze/src/airflow_breeze/commands/ci_image_commands_config.py||dev/breeze/src/airflow_breeze/commands/ci_image_commands_config.py",
          "dev/breeze/src/airflow_breeze/commands/common_image_options.py||dev/breeze/src/airflow_breeze/commands/common_image_options.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands_config.py||dev/breeze/src/airflow_breeze/commands/developer_commands_config.py",
          "dev/breeze/src/airflow_breeze/commands/production_image_commands.py||dev/breeze/src/airflow_breeze/commands/production_image_commands.py",
          "dev/breeze/src/airflow_breeze/commands/production_image_commands_config.py||dev/breeze/src/airflow_breeze/commands/production_image_commands_config.py",
          "dev/breeze/src/airflow_breeze/commands/release_candidate_command.py||dev/breeze/src/airflow_breeze/commands/release_candidate_command.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py",
          "dev/breeze/src/airflow_breeze/params/build_ci_params.py||dev/breeze/src/airflow_breeze/params/build_ci_params.py",
          "dev/breeze/src/airflow_breeze/params/build_prod_params.py||dev/breeze/src/airflow_breeze/params/build_prod_params.py",
          "dev/breeze/src/airflow_breeze/params/common_build_params.py||dev/breeze/src/airflow_breeze/params/common_build_params.py",
          "dev/breeze/src/airflow_breeze/params/shell_params.py||dev/breeze/src/airflow_breeze/params/shell_params.py",
          "dev/breeze/src/airflow_breeze/pre_commit_ids.py||dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py||dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py",
          "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py||dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/src/airflow_breeze/utils/kubernetes_utils.py||dev/breeze/src/airflow_breeze/utils/kubernetes_utils.py",
          "dev/breeze/src/airflow_breeze/utils/packages.py||dev/breeze/src/airflow_breeze/utils/packages.py",
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "dev/breeze/tests/test_selective_checks.py||dev/breeze/tests/test_selective_checks.py",
          "dev/hatch_build.py||dev/hatch_build.py",
          "dev/refresh_images.sh||dev/refresh_images.sh",
          "dev/sign.sh||dev/sign.sh",
          "docker_tests/docker_tests_utils.py||docker_tests/docker_tests_utils.py",
          "docker_tests/test_prod_image.py||docker_tests/test_prod_image.py",
          "scripts/ci/docker-compose/devcontainer.env||scripts/ci/docker-compose/devcontainer.env",
          "scripts/ci/pre_commit/pre_commit_check_extra_packages_ref.py||scripts/ci/pre_commit/pre_commit_check_extra_packages_ref.py",
          "scripts/ci/pre_commit/pre_commit_check_order_dockerfile_extras.py||scripts/ci/pre_commit/pre_commit_check_order_dockerfile_extras.py",
          "scripts/ci/pre_commit/pre_commit_check_order_pyproject_toml.py||scripts/ci/pre_commit/pre_commit_check_order_pyproject_toml.py",
          "scripts/ci/pre_commit/pre_commit_check_order_setup.py||scripts/ci/pre_commit/pre_commit_check_order_setup.py",
          "scripts/ci/pre_commit/pre_commit_check_setup_extra_packages_ref.py||scripts/ci/pre_commit/pre_commit_check_setup_extra_packages_ref.py",
          "scripts/ci/pre_commit/pre_commit_compile_www_assets.py||scripts/ci/pre_commit/pre_commit_compile_www_assets.py",
          "scripts/ci/pre_commit/pre_commit_insert_extras.py||scripts/ci/pre_commit/pre_commit_insert_extras.py",
          "scripts/ci/pre_commit/pre_commit_sort_installed_providers.py||scripts/ci/pre_commit/pre_commit_sort_installed_providers.py",
          "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py||scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py",
          "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py.md5sum||scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py.md5sum",
          "scripts/docker/entrypoint_ci.sh||scripts/docker/entrypoint_ci.sh",
          "scripts/docker/install_airflow.sh||scripts/docker/install_airflow.sh",
          "scripts/docker/install_airflow_dependencies_from_branch_tip.sh||scripts/docker/install_airflow_dependencies_from_branch_tip.sh",
          "scripts/in_container/run_generate_constraints.py||scripts/in_container/run_generate_constraints.py",
          "scripts/in_container/run_prepare_airflow_packages.py||scripts/in_container/run_prepare_airflow_packages.py",
          "scripts/in_container/verify_providers.py||scripts/in_container/verify_providers.py",
          "scripts/tools/initialize_virtualenv.py||scripts/tools/initialize_virtualenv.py",
          "setup.cfg||setup.cfg",
          "setup.py||setup.py",
          "tests/cli/commands/test_task_command.py||tests/cli/commands/test_task_command.py",
          "tests/plugins/test_plugins_manager.py||tests/plugins/test_plugins_manager.py",
          "tests/providers/google/cloud/utils/base_gcp_mock.py||tests/providers/google/cloud/utils/base_gcp_mock.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "Dockerfile.ci||Dockerfile.ci": [
          "File: Dockerfile.ci -> Dockerfile.ci",
          "--- Hunk 1 ---",
          "[Context before]",
          "598:             \"${AIRFLOW_INSTALLATION_METHOD}[${AIRFLOW_EXTRAS}]${AIRFLOW_VERSION_SPECIFICATION}\" \\",
          "599:             ${EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=}",
          "600:         if [[ -n \"${AIRFLOW_INSTALL_EDITABLE_FLAG}\" ]]; then",
          "602:             # We can only do it when we install airflow from sources",
          "603:             set -x",
          "605:             pip install --root-user-action ignore ${AIRFLOW_INSTALL_EDITABLE_FLAG} \\",
          "606:                 ${ADDITIONAL_PIP_INSTALL_FLAGS} \\",
          "607:                 \"${AIRFLOW_INSTALLATION_METHOD}[${AIRFLOW_EXTRAS}]${AIRFLOW_VERSION_SPECIFICATION}\"",
          "",
          "[Removed Lines]",
          "601:             # Remove airflow and reinstall it using editable flag",
          "604:             pip uninstall apache-airflow --yes",
          "",
          "[Added Lines]",
          "601:             # Remove airflow and all providers and reinstall it using editable flag",
          "604:             pip freeze | grep apache-airflow-providers | xargs pip uninstall --yes 2>/dev/null || true",
          "605:             pip uninstall apache-airflow --yes 2>/dev/null || true",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "623:             \"${AIRFLOW_INSTALLATION_METHOD}[${AIRFLOW_EXTRAS}]${AIRFLOW_VERSION_SPECIFICATION}\" \\",
          "624:             --constraint \"${AIRFLOW_CONSTRAINTS_LOCATION}\"",
          "625:         common::install_pip_version",
          "627:         pip install --root-user-action ignore --upgrade --upgrade-strategy only-if-needed \\",
          "628:             ${ADDITIONAL_PIP_INSTALL_FLAGS} \\",
          "629:             ${AIRFLOW_INSTALL_EDITABLE_FLAG} \\",
          "",
          "[Removed Lines]",
          "626:         # then upgrade if needed without using constraints to account for new limits in setup.py",
          "",
          "[Added Lines]",
          "627:         # then upgrade if needed without using constraints to account for new limits in pyproject.toml",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "899:     if [[ ${DOWNGRADE_SQLALCHEMY=} != \"true\" ]]; then",
          "900:         return",
          "901:     fi",
          "903:     echo",
          "904:     echo \"${COLOR_BLUE}Downgrading sqlalchemy to minimum supported version: ${min_sqlalchemy_version}${COLOR_RESET}\"",
          "905:     echo",
          "",
          "[Removed Lines]",
          "902:     min_sqlalchemy_version=$(grep \"sqlalchemy>=\" setup.cfg | sed \"s/.*>=\\([0-9\\.]*\\).*/\\1/\")",
          "",
          "[Added Lines]",
          "903:     min_sqlalchemy_version=$(grep \"\\\"sqlalchemy>=\" pyproject.toml | sed \"s/.*>=\\([0-9\\.]*\\).*/\\1/\" | xargs)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1057: # It can also be overwritten manually by setting the AIRFLOW_CI_BUILD_EPOCH environment variable.",
          "1058: ARG AIRFLOW_CI_BUILD_EPOCH=\"6\"",
          "1059: ARG AIRFLOW_PRE_CACHED_PIP_PACKAGES=\"true\"",
          "1062: ARG AIRFLOW_PIP_VERSION=23.3.2",
          "1063: # Setup PIP",
          "1064: # By default PIP install run without cache to make image smaller",
          "",
          "[Removed Lines]",
          "1060: # By default in the image, we are installing all providers when installing from sources",
          "1061: ARG INSTALL_PROVIDERS_FROM_SOURCES=\"true\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1088:     DEFAULT_CONSTRAINTS_BRANCH=${DEFAULT_CONSTRAINTS_BRANCH} \\",
          "1089:     AIRFLOW_CI_BUILD_EPOCH=${AIRFLOW_CI_BUILD_EPOCH} \\",
          "1090:     AIRFLOW_PRE_CACHED_PIP_PACKAGES=${AIRFLOW_PRE_CACHED_PIP_PACKAGES} \\",
          "1092:     AIRFLOW_VERSION=${AIRFLOW_VERSION} \\",
          "1093:     AIRFLOW_PIP_VERSION=${AIRFLOW_PIP_VERSION} \\",
          "1094: # In the CI image we always:",
          "",
          "[Removed Lines]",
          "1091:     INSTALL_PROVIDERS_FROM_SOURCES=${INSTALL_PROVIDERS_FROM_SOURCES} \\",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1116: # force them on the main Airflow package. Currently we need no extra limits as PIP 23.1+ has much better",
          "1117: # dependency resolution and we do not need to limit the versions of the dependencies",
          "1118: #",
          "1122: #",
          "1124: ARG UPGRADE_TO_NEWER_DEPENDENCIES=\"false\"",
          "1125: ARG VERSION_SUFFIX_FOR_PYPI=\"\"",
          "",
          "[Removed Lines]",
          "1119: # Aiobotocore is limited for eager upgrade because it either causes a long backtracking or",
          "1120: # conflict when we do not limit it. It seems that `pip` has a hard time figuring the right",
          "1121: # combination of dependencies for aiobotocore, botocore, boto3 and s3fs together",
          "1123: ARG EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=\"aiobotocore>=2.5.4\"",
          "",
          "[Added Lines]",
          "1117: # boto3 is limited to <1.34 because of aiobotocore that only works with 1.33 and we want to help",
          "1118: # `pip` to limit the versions it checks and limit backtracking, by explicitly specifying these limits",
          "1119: # when performing eager upgrade of dependencies - this way it won't even consider 1.34 versions of boto",
          "1120: # We should update it every time a new version of aiobotocore is released supporting 1.34",
          "1122: ARG EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=\"boto3>=1.33,<1.34\"",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "1138: # In case of CI builds we want to pre-install main version of airflow dependencies so that",
          "1139: # We do not have to always reinstall it from the scratch.",
          "1140: # And is automatically reinstalled from the scratch every time patch release of python gets released",
          "1143: # the cache is only used when \"upgrade to newer dependencies\" is not set to automatically",
          "1144: # account for removed dependencies (we do not install them in the first place)",
          "1145: RUN bash /scripts/docker/install_pip_version.sh; \\",
          "",
          "[Removed Lines]",
          "1141: # The Airflow (and providers in case INSTALL_PROVIDERS_FROM_SOURCES is \"false\")",
          "1142: # are uninstalled, only dependencies remain.",
          "",
          "[Added Lines]",
          "1140: # The Airflow and providers are uninstalled, only dependencies remain.",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "1158: RUN bash /scripts/docker/install_pipx_tools.sh",
          "1160: # Airflow sources change frequently but dependency configuration won't change that often",
          "1165: COPY airflow/__init__.py ${AIRFLOW_SOURCES}/airflow/",
          "1167: COPY constraints/* ${AIRFLOW_SOURCES}/constraints/",
          "1169: COPY --from=scripts install_airflow.sh /scripts/docker/",
          "1172: # This will be usually incremental small set of packages in CI optimized build, so it will be very fast",
          "1173: # In non-CI optimized build this will install all dependencies before installing sources.",
          "1176: # and push the constraints if everything is successful",
          "1177: RUN bash /scripts/docker/install_airflow.sh",
          "",
          "[Removed Lines]",
          "1161: # We copy setup.py and other files needed to perform setup of dependencies",
          "1162: # So in case setup.py changes we can install latest dependencies required.",
          "1163: COPY setup.py ${AIRFLOW_SOURCES}/setup.py",
          "1164: COPY setup.cfg ${AIRFLOW_SOURCES}/setup.cfg",
          "1166: COPY generated/provider_dependencies.json ${AIRFLOW_SOURCES}/generated/",
          "1171: # The goal of this line is to install the dependencies from the most current setup.py from sources",
          "1174: # Usually we will install versions based on the dependencies in setup.py and upgraded only if needed.",
          "1175: # But in cron job we will install latest versions matching setup.py to see if there is no breaking change",
          "",
          "[Added Lines]",
          "1159: # We copy pyproject.toml and other files needed to perform setup of dependencies",
          "1160: # So in case pyproject.toml changes we can install latest dependencies required.",
          "1161: COPY pyproject.toml ${AIRFLOW_SOURCES}/pyproject.toml",
          "1163: COPY generated/* ${AIRFLOW_SOURCES}/generated/",
          "1165: COPY LICENSE ${AIRFLOW_SOURCES}/LICENSE",
          "1166: COPY dev/airflow_pre_installed_providers.txt ${AIRFLOW_SOURCES}/dev/airflow_pre_installed_providers.txt",
          "1167: COPY dev/hatch_build.py ${AIRFLOW_SOURCES}/dev/hatch_build.py",
          "1170: # The goal of this line is to install the dependencies from the most current pyproject.toml from sources",
          "1173: # Usually we will install versions based on the dependencies in pyproject.toml and upgraded only if needed.",
          "1174: # But in cron job we will install latest versions matching pyproject.toml to see if there is no breaking change",
          "",
          "---------------"
        ],
        "MANIFEST.in||MANIFEST.in": [
          "File: MANIFEST.in -> MANIFEST.in",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/cli/cli_parser.py||airflow/cli/cli_parser.py": [
          "File: airflow/cli/cli_parser.py -> airflow/cli/cli_parser.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import argparse",
          "27: import logging",
          "28: from argparse import Action",
          "29: from collections import Counter",
          "30: from functools import lru_cache",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "28: import sys",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "72: try:",
          "73:     auth_mgr = get_auth_manager_cls()",
          "74:     airflow_commands.extend(auth_mgr.get_cli_commands())",
          "77:     # do not re-raise for the same reason as above",
          "80: ALL_COMMANDS_DICT: dict[str, CLICommand] = {sp.name: sp for sp in airflow_commands}",
          "",
          "[Removed Lines]",
          "75: except Exception:",
          "76:     log.exception(\"cannot load CLI commands from auth manager\")",
          "",
          "[Added Lines]",
          "76: except Exception as e:",
          "77:     log.warning(\"cannot load CLI commands from auth manager: %s\", e)",
          "78:     log.warning(\"Authentication manager is not configured and webserver will not be able to start.\")",
          "80:     if len(sys.argv) > 1 and sys.argv[1] == \"webserver\":",
          "81:         log.exception(e)",
          "82:         sys.exit(1)",
          "",
          "---------------"
        ],
        "airflow/configuration.py||airflow/configuration.py": [
          "File: airflow/configuration.py -> airflow/configuration.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1191:         try:",
          "1192:             return import_string(full_qualified_path)",
          "1193:         except ImportError as e:",
          "1195:             raise AirflowConfigException(",
          "1196:                 f'The object could not be loaded. Please check \"{key}\" key in \"{section}\" section. '",
          "1197:                 f'Current value: \"{full_qualified_path}\".'",
          "",
          "[Removed Lines]",
          "1194:             log.error(e)",
          "",
          "[Added Lines]",
          "1194:             log.warning(e)",
          "",
          "---------------"
        ],
        "airflow/providers_manager.py||airflow/providers_manager.py": [
          "File: airflow/providers_manager.py -> airflow/providers_manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "959:                 field_behaviours = hook_class.get_ui_field_behaviour()",
          "960:                 if field_behaviours:",
          "961:                     self._add_customized_fields(package_name, hook_class, field_behaviours)",
          "962:         except Exception as e:",
          "963:             log.warning(",
          "964:                 \"Exception when importing '%s' from '%s' package: %s\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "962:         except ImportError as e:",
          "963:             if \"No module named 'flask_appbuilder'\" in e.msg:",
          "964:                 log.warning(",
          "965:                     \"The hook_class '%s' is not fully initialized (UI widgets will be missing), because \"",
          "966:                     \"the 'flask_appbuilder' package is not installed, however it is not required for \"",
          "967:                     \"Airflow components to work\",",
          "968:                     hook_class_name,",
          "969:                 )",
          "",
          "---------------"
        ],
        "airflow/www/webpack.config.js||airflow/www/webpack.config.js": [
          "File: airflow/www/webpack.config.js -> airflow/www/webpack.config.js",
          "--- Hunk 1 ---",
          "[Context before]",
          "275:     }),",
          "276:     new LicensePlugin({",
          "277:       additionalFiles: {",
          "279:       },",
          "280:       unacceptableLicenseTest: (licenseIdentifier) =>",
          "281:         [",
          "",
          "[Removed Lines]",
          "278:         \"../../../../licenses/LICENSES-ui.txt\": formatLicenses,",
          "",
          "[Added Lines]",
          "278:         \"../../../../3rd-party-licenses/LICENSES-ui.txt\": formatLicenses,",
          "",
          "---------------"
        ],
        "clients/gen/common.sh||clients/gen/common.sh": [
          "File: clients/gen/common.sh -> clients/gen/common.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "59: git_push.sh",
          "60: .gitlab-ci.yml",
          "61: requirements.txt",
          "64: test-requirements.txt",
          "65: tox.ini",
          "66: EOF",
          "",
          "[Removed Lines]",
          "62: setup.cfg",
          "63: setup.py",
          "",
          "[Added Lines]",
          "62: pyproject.toml",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/ci_image_commands.py||dev/breeze/src/airflow_breeze/commands/ci_image_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/ci_image_commands.py -> dev/breeze/src/airflow_breeze/commands/ci_image_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "46:     option_image_tag_for_pulling,",
          "47:     option_image_tag_for_verifying,",
          "48:     option_install_mysql_client_type,",
          "50:     option_platform_multiple,",
          "51:     option_prepare_buildx_cache,",
          "52:     option_pull,",
          "",
          "[Removed Lines]",
          "49:     option_install_providers_from_sources,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "304: @option_install_mysql_client_type",
          "305: @option_image_tag_for_building",
          "306: @option_include_success_outputs",
          "308: @option_parallelism",
          "309: @option_platform_multiple",
          "310: @option_prepare_buildx_cache",
          "",
          "[Removed Lines]",
          "307: @option_install_providers_from_sources",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "345:     image_tag: str,",
          "346:     include_success_outputs,",
          "347:     install_mysql_client_type: str,",
          "349:     parallelism: int,",
          "350:     platform: str | None,",
          "351:     prepare_buildx_cache: bool,",
          "",
          "[Removed Lines]",
          "348:     install_providers_from_sources: bool,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "417:         github_token=github_token,",
          "418:         image_tag=image_tag,",
          "419:         install_mysql_client_type=install_mysql_client_type,",
          "421:         prepare_buildx_cache=prepare_buildx_cache,",
          "422:         push=push,",
          "423:         python=python,",
          "",
          "[Removed Lines]",
          "420:         install_providers_from_sources=install_providers_from_sources,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/ci_image_commands_config.py||dev/breeze/src/airflow_breeze/commands/ci_image_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/ci_image_commands_config.py -> dev/breeze/src/airflow_breeze/commands/ci_image_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "58:                 \"--commit-sha\",",
          "59:                 \"--debian-version\",",
          "60:                 \"--install-mysql-client-type\",",
          "62:                 \"--python-image\",",
          "63:             ],",
          "64:         },",
          "",
          "[Removed Lines]",
          "61:                 \"--install-providers-from-sources\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/common_image_options.py||dev/breeze/src/airflow_breeze/commands/common_image_options.py": [
          "File: dev/breeze/src/airflow_breeze/commands/common_image_options.py -> dev/breeze/src/airflow_breeze/commands/common_image_options.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "146:     default=ALLOWED_INSTALL_MYSQL_CLIENT_TYPES[0],",
          "147:     envvar=\"INSTALL_MYSQL_CLIENT_TYPE\",",
          "148: )",
          "155: option_platform_multiple = click.option(",
          "156:     \"--platform\",",
          "157:     help=\"Platform for Airflow image.\",",
          "",
          "[Removed Lines]",
          "149: option_install_providers_from_sources = click.option(",
          "150:     \"--install-providers-from-sources\",",
          "151:     help=\"Install providers from sources when installing.\",",
          "152:     is_flag=True,",
          "153:     envvar=\"INSTALL_PROVIDERS_FROM_SOURCES\",",
          "154: )",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/developer_commands.py -> dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "424: @main.command(name=\"start-airflow\")",
          "425: @click.option(",
          "427:     help=\"Skips compilation of assets when starting airflow even if the content of www changed \"",
          "428:     \"(mutually exclusive with --dev-mode).\",",
          "429:     is_flag=True,",
          "",
          "[Removed Lines]",
          "426:     \"--skip-asset-compilation\",",
          "",
          "[Added Lines]",
          "426:     \"--skip-assets-compilation\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "431: @click.option(",
          "432:     \"--dev-mode\",",
          "433:     help=\"Starts webserver in dev mode (assets are always recompiled in this case when starting) \"",
          "435:     is_flag=True,",
          "436: )",
          "437: @click.argument(\"extra-args\", nargs=-1, type=click.UNPROCESSED)",
          "",
          "[Removed Lines]",
          "434:     \"(mutually exclusive with --skip-asset-compilation).\",",
          "",
          "[Added Lines]",
          "434:     \"(mutually exclusive with --skip-assets-compilation).\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "512:     providers_skip_constraints: bool,",
          "513:     python: str,",
          "514:     restart: bool,",
          "516:     standalone_dag_processor: bool,",
          "517:     use_airflow_version: str | None,",
          "518:     use_packages_from_dist: bool,",
          "",
          "[Removed Lines]",
          "515:     skip_asset_compilation: bool,",
          "",
          "[Added Lines]",
          "515:     skip_assets_compilation: bool,",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "521:     Enter breeze environment and starts all Airflow components in the tmux session.",
          "522:     Compile assets if contents of www directory changed.",
          "523:     \"\"\"",
          "525:         get_console().print(",
          "526:             \"[warning]You cannot skip asset compilation in dev mode! Assets will be compiled!\"",
          "527:         )",
          "530:         run_compile_www_assets(dev=dev_mode, run_in_background=True, force_clean=False)",
          "531:     airflow_constraints_reference = _determine_constraint_branch_used(",
          "532:         airflow_constraints_reference, use_airflow_version",
          "",
          "[Removed Lines]",
          "524:     if dev_mode and skip_asset_compilation:",
          "528:         skip_asset_compilation = True",
          "529:     if use_airflow_version is None and not skip_asset_compilation:",
          "",
          "[Added Lines]",
          "524:     if dev_mode and skip_assets_compilation:",
          "528:         skip_assets_compilation = True",
          "529:     if use_airflow_version is None and not skip_assets_compilation:",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/developer_commands_config.py||dev/breeze/src/airflow_breeze/commands/developer_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/developer_commands_config.py -> dev/breeze/src/airflow_breeze/commands/developer_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "228:         {",
          "229:             \"name\": \"Asset compilation options\",",
          "230:             \"options\": [",
          "232:                 \"--dev-mode\",",
          "233:             ],",
          "234:         },",
          "",
          "[Removed Lines]",
          "231:                 \"--skip-asset-compilation\",",
          "",
          "[Added Lines]",
          "231:                 \"--skip-assets-compilation\",",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/production_image_commands.py||dev/breeze/src/airflow_breeze/commands/production_image_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/production_image_commands.py -> dev/breeze/src/airflow_breeze/commands/production_image_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "41:     option_image_tag_for_pulling,",
          "42:     option_image_tag_for_verifying,",
          "43:     option_install_mysql_client_type,",
          "45:     option_platform_multiple,",
          "46:     option_prepare_buildx_cache,",
          "47:     option_pull,",
          "",
          "[Removed Lines]",
          "44:     option_install_providers_from_sources,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "233: @option_image_tag_for_building",
          "234: @option_include_success_outputs",
          "235: @option_install_mysql_client_type",
          "237: @option_parallelism",
          "238: @option_platform_multiple",
          "239: @option_prepare_buildx_cache",
          "",
          "[Removed Lines]",
          "236: @option_install_providers_from_sources",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "284:     install_airflow_version: str | None,",
          "285:     install_mysql_client_type: str,",
          "286:     install_packages_from_context: bool,",
          "288:     installation_method: str,",
          "289:     parallelism: int,",
          "290:     platform: str | None,",
          "",
          "[Removed Lines]",
          "287:     install_providers_from_sources: bool,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "348:         install_airflow_version=install_airflow_version,",
          "349:         install_mysql_client_type=install_mysql_client_type,",
          "350:         install_packages_from_context=install_packages_from_context,",
          "352:         installation_method=installation_method,",
          "353:         prepare_buildx_cache=prepare_buildx_cache,",
          "354:         push=push,",
          "",
          "[Removed Lines]",
          "351:         install_providers_from_sources=install_providers_from_sources,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/production_image_commands_config.py||dev/breeze/src/airflow_breeze/commands/production_image_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/production_image_commands_config.py -> dev/breeze/src/airflow_breeze/commands/production_image_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "57:                 \"--python-image\",",
          "58:                 \"--commit-sha\",",
          "59:                 \"--additional-pip-install-flags\",",
          "61:             ],",
          "62:         },",
          "63:         {",
          "",
          "[Removed Lines]",
          "60:                 \"--install-providers-from-sources\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_candidate_command.py||dev/breeze/src/airflow_breeze/commands/release_candidate_command.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_candidate_command.py -> dev/breeze/src/airflow_breeze/commands/release_candidate_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "92: def create_artifacts_with_sdist():",
          "94:     console_print(\"Artifacts created\")",
          "",
          "[Removed Lines]",
          "93:     run_command([\"python3\", \"setup.py\", \"compile_assets\", \"sdist\", \"bdist_wheel\"], check=True)",
          "",
          "[Added Lines]",
          "93:     run_command([\"hatch\", \"build\", \"-t\", \"sdist\", \"-t\", \"wheel\"], check=True)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "100:             \"breeze\",",
          "101:             \"release-management\",",
          "102:             \"prepare-airflow-package\",",
          "104:             \"--package-format\",",
          "105:             \"both\",",
          "106:         ],",
          "",
          "[Removed Lines]",
          "103:             \"--use-container-for-assets-compilation\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "356:     # Create the artifacts",
          "357:     if confirm_action(\"Use breeze to create artifacts?\"):",
          "358:         create_artifacts_with_breeze()",
          "360:         create_artifacts_with_sdist()",
          "361:     # Sign the release",
          "362:     sign_the_release(airflow_repo_root)",
          "",
          "[Removed Lines]",
          "359:     elif confirm_action(\"Use setup.py to create artifacts?\"):",
          "",
          "[Added Lines]",
          "358:     elif confirm_action(\"Use hatch to create artifacts?\"):",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: import glob",
          "20: import operator",
          "21: import os",
          "22: import re",
          "23: import shutil",
          "24: import sys",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: import random",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "129: )",
          "130: from airflow_breeze.utils.path_utils import (",
          "131:     AIRFLOW_SOURCES_ROOT,",
          "133:     CONSTRAINTS_CACHE_DIR,",
          "134:     DIST_DIR,",
          "135:     GENERATED_PROVIDER_PACKAGES_DIR,",
          "",
          "[Removed Lines]",
          "132:     AIRFLOW_WWW_DIR,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "143: )",
          "144: from airflow_breeze.utils.python_versions import get_python_version_list",
          "145: from airflow_breeze.utils.run_utils import (",
          "147:     run_command,",
          "149: )",
          "150: from airflow_breeze.utils.shared_options import get_dry_run, get_verbose",
          "151: from airflow_breeze.utils.versions import is_pre_release",
          "",
          "[Removed Lines]",
          "146:     clean_www_assets,",
          "148:     run_compile_www_assets,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "209: WHEEL_VERSION = \"0.36.2\"",
          "210: GITPYTHON_VERSION = \"3.1.40\"",
          "211: RICH_VERSION = \"13.7.0\"",
          "213: AIRFLOW_BUILD_DOCKERFILE = f\"\"\"",
          "214: FROM python:{DEFAULT_PYTHON_MAJOR_MINOR_VERSION}-slim-{ALLOWED_DEBIAN_VERSIONS[0]}",
          "215: RUN apt-get update && apt-get install -y --no-install-recommends git",
          "218: \"\"\"",
          "258: @release_management.command(",
          "",
          "[Removed Lines]",
          "216: RUN pip install pip=={AIRFLOW_PIP_VERSION} wheel=={WHEEL_VERSION} \\\\",
          "217:    gitpython=={GITPYTHON_VERSION} rich=={RICH_VERSION}",
          "220: AIRFLOW_BUILD_IMAGE_TAG = \"apache/airflow:local-build-image\"",
          "221: NODE_BUILD_IMAGE_TAG = \"node:21.2.0-bookworm-slim\"",
          "224: def _compile_assets_in_docker():",
          "225:     clean_www_assets()",
          "226:     get_console().print(\"[info]Compiling assets in docker container\\n\")",
          "227:     result = run_command(",
          "228:         [",
          "229:             \"docker\",",
          "230:             \"run\",",
          "231:             \"-t\",",
          "232:             \"-v\",",
          "233:             f\"{AIRFLOW_WWW_DIR}:/opt/airflow/airflow/www/\",",
          "234:             \"-e\",",
          "235:             \"FORCE_COLOR=true\",",
          "236:             NODE_BUILD_IMAGE_TAG,",
          "237:             \"bash\",",
          "238:             \"-c\",",
          "239:             \"cd /opt/airflow/airflow/www && yarn install --frozen-lockfile && yarn run build\",",
          "240:         ],",
          "241:         text=True,",
          "242:         capture_output=not get_verbose(),",
          "243:         check=False,",
          "244:     )",
          "245:     if result.returncode != 0:",
          "246:         get_console().print(\"[error]Error compiling assets[/]\")",
          "247:         get_console().print(result.stdout)",
          "248:         get_console().print(result.stderr)",
          "249:         fix_ownership_using_docker()",
          "250:         sys.exit(result.returncode)",
          "252:     get_console().print(\"[success]compiled assets in docker container\\n\")",
          "253:     get_console().print(\"[info]Fixing ownership of compiled assets\\n\")",
          "254:     fix_ownership_using_docker()",
          "255:     get_console().print(\"[success]Fixing ownership of compiled assets\\n\")",
          "",
          "[Added Lines]",
          "210: NODE_VERSION = \"21.2.0\"",
          "211: PRE_COMMIT_VERSION = \"3.5.0\"",
          "216: RUN pip install pip=={AIRFLOW_PIP_VERSION} hatch==1.9.1 \\",
          "217:   gitpython=={GITPYTHON_VERSION} rich=={RICH_VERSION} pre-commit=={PRE_COMMIT_VERSION}",
          "218: COPY . /opt/airflow",
          "221: AIRFLOW_BUILD_DOCKERIGNORE = \"\"\"",
          "222: # Git version is dynamically generated",
          "223: airflow/git_version",
          "224: # Exclude mode_modules pulled by \"yarn\" for compilation of www files generated by NPM",
          "225: airflow/www/node_modules",
          "227: # Exclude link to docs",
          "228: airflow/www/static/docs",
          "230: # Exclude python generated files",
          "238: /dist/",
          "252: # Exclude temporary vi files",
          "255: # Exclude output files",
          "259: # Exclude auto-generated Finder files on Mac OS",
          "263: # Exclude docs generated files",
          "264: docs/_build/",
          "265: docs/_api/",
          "266: docs/_doctrees/",
          "268: # files generated by memray",
          "271: \"\"\"",
          "273: AIRFLOW_BUILD_IMAGE_TAG = \"apache/airflow:local-build-image\"",
          "274: NODE_BUILD_IMAGE_TAG = f\"node:{NODE_VERSION}-bookworm-slim\"",
          "276: AIRFLOW_BUILD_DOCKERFILE_PATH = AIRFLOW_SOURCES_ROOT / \"airflow-build-dockerfile\"",
          "277: AIRFLOW_BUILD_DOCKERFILE_IGNORE_PATH = AIRFLOW_SOURCES_ROOT / \"airflow-build-dockerfile.dockerignore\"",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "260:     help=\"Prepare sdist/whl package of Airflow.\",",
          "261: )",
          "262: @option_package_format",
          "271: @option_version_suffix_for_pypi",
          "272: @option_verbose",
          "273: @option_dry_run",
          "274: def prepare_airflow_packages(",
          "275:     package_format: str,",
          "276:     version_suffix_for_pypi: str,",
          "278: ):",
          "279:     perform_environment_checks()",
          "280:     fix_ownership_using_docker()",
          "281:     cleanup_python_generated_files()",
          "290:     run_command(",
          "293:         text=True,",
          "294:         check=True,",
          "295:         env={\"DOCKER_CLI_HINTS\": \"false\"},",
          "296:     )",
          "298:         cmd=[",
          "299:             \"docker\",",
          "300:             \"run\",",
          "301:             \"-t\",",
          "304:             \"-e\",",
          "305:             f\"VERSION_SUFFIX_FOR_PYPI={version_suffix_for_pypi}\",",
          "306:             \"-e\",",
          "307:             \"GITHUB_ACTIONS\",",
          "308:             \"-e\",",
          "309:             f\"PACKAGE_FORMAT={package_format}\",",
          "310:             AIRFLOW_BUILD_IMAGE_TAG,",
          "311:             \"python\",",
          "312:             \"/opt/airflow/scripts/in_container/run_prepare_airflow_packages.py\",",
          "313:         ],",
          "315:     )",
          "316:     get_console().print(\"[success]Successfully prepared Airflow package!\\n\\n\")",
          "322: def provider_action_summary(description: str, message_type: MessageType, packages: list[str]):",
          "",
          "[Removed Lines]",
          "263: @click.option(",
          "264:     \"--use-container-for-assets-compilation\",",
          "265:     is_flag=True,",
          "266:     help=\"If set, the assets are compiled in docker container. On MacOS, asset compilation in containers \"",
          "267:     \"is slower, due to slow mounted filesystem and number of node_module files so by default asset \"",
          "268:     \"compilation is done locally. This option is useful for officially building packages by release \"",
          "269:     \"manager on MacOS to make sure it is a reproducible build.\",",
          "270: )",
          "277:     use_container_for_assets_compilation: bool,",
          "282:     get_console().print(\"[info]Compiling assets\\n\")",
          "283:     from sys import platform",
          "285:     if platform == \"darwin\" and not use_container_for_assets_compilation:",
          "286:         run_compile_www_assets(dev=False, run_in_background=False, force_clean=True)",
          "287:     else:",
          "288:         _compile_assets_in_docker()",
          "289:     get_console().print(\"[success]Assets compiled successfully[/]\")",
          "291:         [\"docker\", \"build\", \"--tag\", AIRFLOW_BUILD_IMAGE_TAG, \"-\"],",
          "292:         input=AIRFLOW_BUILD_DOCKERFILE,",
          "297:     run_command(",
          "302:             \"-v\",",
          "303:             f\"{AIRFLOW_SOURCES_ROOT}:/opt/airflow:cached\",",
          "314:         check=True,",
          "317:     get_console().print(\"\\n[info]Cleaning ownership of generated files\\n\")",
          "318:     fix_ownership_using_docker()",
          "319:     get_console().print(\"\\n[success]Cleaned ownership of generated files\\n\")",
          "",
          "[Added Lines]",
          "295:     # This is security feature.",
          "296:     #",
          "297:     # Building the image needed to build airflow package including .git directory",
          "298:     # In isolated environment, to not allow the in-docker code to override local code",
          "299:     # The image used to build airflow package is built from scratch and contains",
          "300:     # Full Airflow code including Airflow repository is added to the image, but locally build node_modules",
          "301:     # are not added to the context of that image",
          "302:     AIRFLOW_BUILD_DOCKERFILE_PATH.write_text(AIRFLOW_BUILD_DOCKERFILE.strip())",
          "303:     AIRFLOW_BUILD_DOCKERFILE_IGNORE_PATH.write_text(AIRFLOW_BUILD_DOCKERIGNORE.strip())",
          "305:         [",
          "306:             \"docker\",",
          "307:             \"buildx\",",
          "308:             \"build\",",
          "309:             \".\",",
          "310:             \"-f\",",
          "311:             \"airflow-build-dockerfile\",",
          "312:             \"--tag\",",
          "313:             AIRFLOW_BUILD_IMAGE_TAG,",
          "314:         ],",
          "317:         cwd=AIRFLOW_SOURCES_ROOT,",
          "320:     container_id = f\"airflow-build-{random.getrandbits(64):08x}\"",
          "321:     result = run_command(",
          "325:             \"--name\",",
          "326:             container_id,",
          "331:             \"HOME=/opt/airflow/files/home\",",
          "332:             \"-e\",",
          "336:             \"-w\",",
          "337:             \"/opt/airflow\",",
          "342:         check=False,",
          "344:     if result.returncode != 0:",
          "345:         get_console().print(\"[error]Error preparing Airflow package[/]\")",
          "346:         fix_ownership_using_docker()",
          "347:         sys.exit(result.returncode)",
          "348:     DIST_DIR.mkdir(parents=True, exist_ok=True)",
          "349:     # Copy all files in the dist directory in container to the host dist directory (note '/.' in SRC)",
          "350:     run_command([\"docker\", \"cp\", f\"{container_id}:/opt/airflow/dist/.\", \"./dist\"], check=True)",
          "351:     run_command([\"docker\", \"rm\", \"--force\", container_id], check=True)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "700: @release_management.command(",
          "701:     name=\"generate-constraints\",",
          "703: )",
          "704: @option_python",
          "705: @option_run_in_parallel",
          "",
          "[Removed Lines]",
          "702:     help=\"Generates pinned constraint files with all extras from setup.py in parallel.\",",
          "",
          "[Added Lines]",
          "735:     help=\"Generates pinned constraint files with all extras from pyproject.toml in parallel.\",",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "55:             \"name\": \"Package flags\",",
          "56:             \"options\": [",
          "57:                 \"--package-format\",",
          "59:                 \"--version-suffix-for-pypi\",",
          "60:             ],",
          "61:         }",
          "",
          "[Removed Lines]",
          "58:                 \"--use-container-for-assets-compilation\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py": [
          "File: dev/breeze/src/airflow_breeze/global_constants.py -> dev/breeze/src/airflow_breeze/global_constants.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "388: # Initialize files for rebuild check",
          "389: FILES_FOR_REBUILD_CHECK = [",
          "392:     \"Dockerfile.ci\",",
          "393:     \".dockerignore\",",
          "394:     \"generated/provider_dependencies.json\",",
          "",
          "[Removed Lines]",
          "390:     \"setup.py\",",
          "391:     \"setup.cfg\",",
          "",
          "[Added Lines]",
          "390:     \"pyproject.toml\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "430:     \"amazon\",",
          "431:     \"async\",",
          "432:     \"celery\",",
          "435:     \"docker\",",
          "436:     \"elasticsearch\",",
          "437:     \"ftp\",",
          "438:     \"google\",",
          "440:     \"graphviz\",",
          "441:     \"grpc\",",
          "442:     \"hashicorp\",",
          "443:     \"http\",",
          "444:     \"ldap\",",
          "446:     \"mysql\",",
          "447:     \"odbc\",",
          "448:     \"openlineage\",",
          "",
          "[Removed Lines]",
          "433:     \"cncf.kubernetes\",",
          "434:     \"common.io\",",
          "439:     \"google_auth\",",
          "445:     \"microsoft.azure\",",
          "",
          "[Added Lines]",
          "432:     \"cncf-kubernetes\",",
          "433:     \"common-io\",",
          "438:     \"google-auth\",",
          "444:     \"microsoft-azure\",",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/params/build_ci_params.py||dev/breeze/src/airflow_breeze/params/build_ci_params.py": [
          "File: dev/breeze/src/airflow_breeze/params/build_ci_params.py -> dev/breeze/src/airflow_breeze/params/build_ci_params.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "34:     airflow_constraints_mode: str = \"constraints-source-providers\"",
          "35:     airflow_constraints_reference: str = DEFAULT_AIRFLOW_CONSTRAINTS_BRANCH",
          "37:     airflow_pre_cached_pip_packages: bool = True",
          "38:     force_build: bool = False",
          "39:     upgrade_to_newer_dependencies: bool = False",
          "",
          "[Removed Lines]",
          "36:     airflow_extras: str = \"devel_ci\"",
          "",
          "[Added Lines]",
          "36:     airflow_extras: str = \"devel-ci\"",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/params/build_prod_params.py||dev/breeze/src/airflow_breeze/params/build_prod_params.py": [
          "File: dev/breeze/src/airflow_breeze/params/build_prod_params.py -> dev/breeze/src/airflow_breeze/params/build_prod_params.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "212:         self._req_arg(\"DOCKER_CONTEXT_FILES\", self.docker_context_files)",
          "213:         self._req_arg(\"INSTALL_PACKAGES_FROM_CONTEXT\", self.install_packages_from_context)",
          "214:         self._req_arg(\"INSTALL_POSTGRES_CLIENT\", self.install_postgres_client)",
          "216:         self._req_arg(\"PYTHON_BASE_IMAGE\", self.python_base_image)",
          "217:         # optional build args",
          "218:         self._opt_arg(\"AIRFLOW_CONSTRAINTS_LOCATION\", self.airflow_constraints_location)",
          "",
          "[Removed Lines]",
          "215:         self._req_arg(\"INSTALL_PROVIDERS_FROM_SOURCES\", self.install_providers_from_sources)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/params/common_build_params.py||dev/breeze/src/airflow_breeze/params/common_build_params.py": [
          "File: dev/breeze/src/airflow_breeze/params/common_build_params.py -> dev/breeze/src/airflow_breeze/params/common_build_params.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "63:     github_repository: str = APACHE_AIRFLOW_GITHUB_REPOSITORY",
          "64:     github_token: str = os.environ.get(\"GITHUB_TOKEN\", \"\")",
          "65:     image_tag: str | None = None",
          "67:     install_mysql_client_type: str = ALLOWED_INSTALL_MYSQL_CLIENT_TYPES[0]",
          "68:     platform: str = DOCKER_DEFAULT_PLATFORM",
          "69:     prepare_buildx_cache: bool = False",
          "",
          "[Removed Lines]",
          "66:     install_providers_from_sources: bool = False",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/params/shell_params.py||dev/breeze/src/airflow_breeze/params/shell_params.py": [
          "File: dev/breeze/src/airflow_breeze/params/shell_params.py -> dev/breeze/src/airflow_breeze/params/shell_params.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "162:     image_tag: str | None = None",
          "163:     include_mypy_volume: bool = False",
          "164:     install_airflow_version: str = \"\"",
          "166:     install_selected_providers: str | None = None",
          "167:     integration: tuple[str, ...] = ()",
          "168:     issue_id: str = \"\"",
          "",
          "[Removed Lines]",
          "165:     install_providers_from_sources: bool = True",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "526:         _set_var(_env, \"HOST_USER_ID\", self.host_user_id)",
          "527:         _set_var(_env, \"INIT_SCRIPT_FILE\", None, \"init.sh\")",
          "528:         _set_var(_env, \"INSTALL_AIRFLOW_VERSION\", self.install_airflow_version)",
          "530:         _set_var(_env, \"INSTALL_SELECTED_PROVIDERS\", self.install_selected_providers)",
          "531:         _set_var(_env, \"ISSUE_ID\", self.issue_id)",
          "532:         _set_var(_env, \"LOAD_DEFAULT_CONNECTIONS\", self.load_default_connections)",
          "",
          "[Removed Lines]",
          "529:         _set_var(_env, \"INSTALL_PROVIDERS_FROM_SOURCES\", self.install_providers_from_sources)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/pre_commit_ids.py||dev/breeze/src/airflow_breeze/pre_commit_ids.py": [
          "File: dev/breeze/src/airflow_breeze/pre_commit_ids.py -> dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "71:     \"check-providers-init-file-missing\",",
          "72:     \"check-providers-subpackages-init-file-exist\",",
          "73:     \"check-pydevd-left-in-code\",",
          "74:     \"check-revision-heads-map\",",
          "75:     \"check-safe-filter-usage-in-html\",",
          "77:     \"check-start-date-not-used-in-defaults\",",
          "78:     \"check-system-tests-present\",",
          "79:     \"check-system-tests-tocs\",",
          "",
          "[Removed Lines]",
          "76:     \"check-setup-order\",",
          "",
          "[Added Lines]",
          "74:     \"check-pyproject-toml-order\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "133:     \"update-supported-versions\",",
          "134:     \"update-vendored-in-k8s-json-schema\",",
          "135:     \"update-version\",",
          "137:     \"yamllint\",",
          "138: ]",
          "",
          "[Removed Lines]",
          "136:     \"validate-pyproject\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py||dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py": [
          "File: dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py -> dev/breeze/src/airflow_breeze/prepare_providers/provider_documentation.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "656: ) -> tuple[bool, bool]:",
          "657:     \"\"\"Updates generated files.",
          "661:     :param provider_package_id: id of the package",
          "662:     :param reapply_templates_only: regenerate already released documentation only - without updating versions",
          "",
          "[Removed Lines]",
          "659:     This includes the readme, changes, and/or setup.cfg/setup.py/manifest.in/provider_info.",
          "",
          "[Added Lines]",
          "659:     This includes the readme, changes, and provider.yaml files.",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py||dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py": [
          "File: dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py -> dev/breeze/src/airflow_breeze/prepare_providers/provider_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "260:         return [",
          "261:             package.strip()",
          "262:             for package in package_list_file.readlines()",
          "264:             and (package.strip() not in not_ready_provider_ids or include_not_ready)",
          "265:         ]",
          "266:     elif provider_packages:",
          "",
          "[Removed Lines]",
          "263:             if (package.strip() not in removed_provider_ids or include_removed)",
          "",
          "[Added Lines]",
          "263:             if not package.strip().startswith(\"#\")",
          "264:             and (package.strip() not in removed_provider_ids or include_removed)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "66: # Those are volumes that are mounted when MOUNT_SELECTED is chosen (which is the default when",
          "67: # entering Breeze. MOUNT_SELECTED prevents to mount the files that you can have accidentally added",
          "69: # This is important to get a \"clean\" environment for different python versions and to avoid",
          "70: # unnecessary slow-downs when you are mounting files on MacOS (which has very slow filesystem)",
          "71: # Any time you add a top-level folder in airflow that should also be added to container you should",
          "",
          "[Removed Lines]",
          "68: # in your sources (or they were added automatically by setup.py etc.) to be mounted to container.",
          "",
          "[Added Lines]",
          "68: # in your sources (or they were added automatically by pyproject.toml) to be mounted to container.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "80:     (\".rat-excludes\", \"/opt/airflow/.rat-excludes\"),",
          "81:     (\"BREEZE.rst\", \"/opt/airflow/BREEZE.rst\"),",
          "82:     (\"LICENSE\", \"/opt/airflow/LICENSE\"),",
          "84:     (\"NOTICE\", \"/opt/airflow/NOTICE\"),",
          "85:     (\"RELEASE_NOTES.rst\", \"/opt/airflow/RELEASE_NOTES.rst\"),",
          "86:     (\"airflow\", \"/opt/airflow/airflow\"),",
          "",
          "[Removed Lines]",
          "83:     (\"MANIFEST.in\", \"/opt/airflow/MANIFEST.in\"),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "95:     (\"pyproject.toml\", \"/opt/airflow/pyproject.toml\"),",
          "96:     (\"scripts\", \"/opt/airflow/scripts\"),",
          "97:     (\"scripts/docker/entrypoint_ci.sh\", \"/entrypoint\"),",
          "100:     (\"tests\", \"/opt/airflow/tests\"),",
          "101:     (\"helm_tests\", \"/opt/airflow/helm_tests\"),",
          "102:     (\"kubernetes_tests\", \"/opt/airflow/kubernetes_tests\"),",
          "",
          "[Removed Lines]",
          "98:     (\"setup.cfg\", \"/opt/airflow/setup.cfg\"),",
          "99:     (\"setup.py\", \"/opt/airflow/setup.py\"),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/kubernetes_utils.py||dev/breeze/src/airflow_breeze/utils/kubernetes_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/kubernetes_utils.py -> dev/breeze/src/airflow_breeze/utils/kubernetes_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "299:         str(K8S_REQUIREMENTS.resolve()),",
          "300:     ]",
          "301:     env = os.environ.copy()",
          "303:     capture_output = True",
          "304:     if get_verbose():",
          "305:         capture_output = False",
          "",
          "[Removed Lines]",
          "302:     env[\"INSTALL_PROVIDERS_FROM_SOURCES\"] = \"true\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/packages.py||dev/breeze/src/airflow_breeze/utils/packages.py": [
          "File: dev/breeze/src/airflow_breeze/utils/packages.py -> dev/breeze/src/airflow_breeze/utils/packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "54: LONG_PROVIDERS_PREFIX = \"apache-airflow-providers-\"",
          "67: class EntityType(Enum):",
          "68:     Operators = \"Operators\"",
          "",
          "[Removed Lines]",
          "56: # TODO: use single source of truth for those",
          "57: # for now we need to keep them in sync with the ones in setup.py",
          "58: PREINSTALLED_PROVIDERS = [",
          "59:     \"common.sql\",",
          "60:     \"ftp\",",
          "61:     \"http\",",
          "62:     \"imap\",",
          "63:     \"sqlite\",",
          "64: ]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "259:     if include_not_ready:",
          "260:         valid_states.add(\"not-ready\")",
          "261:     if include_regular:",
          "263:     if include_suspended:",
          "264:         valid_states.add(\"suspended\")",
          "265:     if include_removed:",
          "",
          "[Removed Lines]",
          "262:         valid_states.add(\"ready\")",
          "",
          "[Added Lines]",
          "252:         valid_states.update({\"ready\", \"pre-release\"})",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py": [
          "File: dev/breeze/src/airflow_breeze/utils/selective_checks.py -> dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "113:             r\"^dev/.*\\.py$\",",
          "114:             r\"^Dockerfile\",",
          "115:             r\"^scripts\",",
          "118:             r\"^generated/provider_dependencies.json$\",",
          "119:         ],",
          "120:         FileGroupForCi.PYTHON_PRODUCTION_FILES: [",
          "121:             r\"^airflow/.*\\.py\",",
          "123:         ],",
          "124:         FileGroupForCi.JAVASCRIPT_PRODUCTION_FILES: [",
          "125:             r\"^airflow/.*\\.[jt]sx?\",",
          "",
          "[Removed Lines]",
          "116:             r\"^setup.py\",",
          "117:             r\"^setup.cfg\",",
          "122:             r\"^setup.py\",",
          "",
          "[Added Lines]",
          "116:             r\"^pyproject.toml\",",
          "121:             r\"^pyproject.toml\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "141:         ],",
          "142:         FileGroupForCi.SETUP_FILES: [",
          "143:             r\"^pyproject.toml\",",
          "146:             r\"^generated/provider_dependencies.json$\",",
          "147:         ],",
          "148:         FileGroupForCi.DOC_FILES: [",
          "",
          "[Removed Lines]",
          "144:             r\"^setup.cfg\",",
          "145:             r\"^setup.py\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_selective_checks.py||dev/breeze/tests/test_selective_checks.py": [
          "File: dev/breeze/tests/test_selective_checks.py -> dev/breeze/tests/test_selective_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "426:         ),",
          "427:         (",
          "428:             pytest.param(",
          "430:                 {",
          "431:                     \"affected-providers-list-as-string\": ALL_PROVIDERS_AFFECTED,",
          "432:                     \"all-python-versions\": \"['3.8', '3.9', '3.10', '3.11']\",",
          "",
          "[Removed Lines]",
          "429:                 (\"setup.py\",),",
          "",
          "[Added Lines]",
          "429:                 (\"pyproject.toml\",),",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "447:                     \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "448:                 },",
          "449:                 id=\"Everything should run - including all providers and upgrading to \"",
          "451:             )",
          "452:         ),",
          "453:         (",
          "",
          "[Removed Lines]",
          "450:                 \"newer requirements as setup.py changed and all Python versions\",",
          "",
          "[Added Lines]",
          "450:                 \"newer requirements as pyproject.toml changed and all Python versions\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1168:             id=\"Regular source changed\",",
          "1169:         ),",
          "1170:         pytest.param(",
          "1172:             {",
          "1173:                 \"upgrade-to-newer-dependencies\": \"true\",",
          "1174:             },",
          "1175:             (),",
          "1185:         ),",
          "1186:         pytest.param(",
          "1187:             (\"airflow/providers/microsoft/azure/provider.yaml\",),",
          "",
          "[Removed Lines]",
          "1171:             (\"setup.py\",),",
          "1176:             id=\"Setup.py changed\",",
          "1177:         ),",
          "1178:         pytest.param(",
          "1179:             (\"setup.cfg\",),",
          "1180:             {",
          "1181:                 \"upgrade-to-newer-dependencies\": \"true\",",
          "1182:             },",
          "1183:             (),",
          "1184:             id=\"Setup.cfg changed\",",
          "",
          "[Added Lines]",
          "1171:             (\"pyproject.toml\",),",
          "1176:             id=\"pyproject.toml changed\",",
          "",
          "---------------"
        ],
        "dev/hatch_build.py||dev/hatch_build.py": [
          "File: dev/hatch_build.py -> dev/hatch_build.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "19: import json",
          "20: import logging",
          "21: import os",
          "22: from pathlib import Path",
          "23: from subprocess import run",
          "24: from typing import Any, Callable, Iterable",
          "26: from hatchling.builders.config import BuilderConfig",
          "27: from hatchling.builders.hooks.plugin.interface import BuildHookInterface",
          "28: from hatchling.builders.plugin.interface import BuilderInterface",
          "29: from hatchling.plugin.manager import PluginManager",
          "31: log = logging.getLogger(__name__)",
          "32: log_level = logging.getLevelName(os.getenv(\"CUSTOM_AIRFLOW_BUILD_LOG_LEVEL\", \"INFO\"))",
          "33: log.setLevel(log_level)",
          "35: AIRFLOW_ROOT_PATH = Path(__file__).parent.parent.resolve()",
          "36: GENERATED_PROVIDERS_DEPENDENCIES_FILE = AIRFLOW_ROOT_PATH / \"generated\" / \"provider_dependencies.json\"",
          "37: DEV_DIR_PATH = AIRFLOW_ROOT_PATH / \"dev\"",
          "38: PREINSTALLED_PROVIDERS_FILE = DEV_DIR_PATH / \"airflow_pre_installed_providers.txt\"",
          "39: DEPENDENCIES = json.loads(GENERATED_PROVIDERS_DEPENDENCIES_FILE.read_text())",
          "40: PREINSTALLED_PROVIDER_IDS = [",
          "41:     package.strip()",
          "42:     for package in PREINSTALLED_PROVIDERS_FILE.read_text().splitlines()",
          "43:     if not package.strip().startswith(\"#\")",
          "44: ]",
          "46: # if providers are ready, we can preinstall them",
          "47: PREINSTALLED_PROVIDERS = [",
          "48:     f\"apache-airflow-providers-{provider_id.replace('.','-')}\"",
          "49:     for provider_id in PREINSTALLED_PROVIDER_IDS",
          "50:     if DEPENDENCIES[provider_id][\"state\"] == \"ready\"",
          "51: ]",
          "52: # if provider is in not-ready or pre-release, we need to install its dependencies",
          "53: # however we need to skip apache-airflow itself and potentially any providers that are",
          "54: PREINSTALLED_NOT_READY_DEPS = []",
          "55: for provider_id in PREINSTALLED_PROVIDER_IDS:",
          "56:     if DEPENDENCIES[provider_id][\"state\"] not in [\"ready\", \"suspended\", \"removed\"]:",
          "57:         for dependency in DEPENDENCIES[provider_id][\"deps\"]:",
          "58:             if dependency.startswith(\"apache-airflow-providers\"):",
          "59:                 raise Exception(",
          "60:                     f\"The provider {provider_id} is pre-installed and it has as dependency \"",
          "61:                     f\"to another provider {dependency}. This is not allowed. Pre-installed\"",
          "62:                     f\"providers should only have 'apache-airflow' and regular dependencies.\"",
          "63:                 )",
          "64:             if not dependency.startswith(\"apache-airflow\"):",
          "65:                 PREINSTALLED_NOT_READY_DEPS.append(dependency)",
          "68: class CustomBuild(BuilderInterface[BuilderConfig, PluginManager]):",
          "69:     \"\"\"Custom build class for Airflow assets and git version.\"\"\"",
          "71:     # Note that this name of the plugin MUST be `custom` - as long as we use it from custom",
          "72:     # hatch_build.py file and not from external plugin. See note in the:",
          "73:     # https://hatch.pypa.io/latest/plugins/build-hook/custom/#example",
          "74:     #",
          "75:     PLUGIN_NAME = \"custom\"",
          "77:     def clean(self, directory: str, versions: Iterable[str]) -> None:",
          "78:         work_dir = Path(self.root)",
          "79:         commands = [",
          "80:             [\"rm -rf airflow/www/static/dist\"],",
          "81:             [\"rm -rf airflow/www/node_modules\"],",
          "82:         ]",
          "83:         for cmd in commands:",
          "84:             run(cmd, cwd=work_dir.as_posix(), check=True, shell=True)",
          "86:     def get_version_api(self) -> dict[str, Callable[..., str]]:",
          "87:         \"\"\"Custom build target for standard package preparation.\"\"\"",
          "88:         return {\"standard\": self.build_standard}",
          "90:     def build_standard(self, directory: str, artifacts: Any, **build_data: Any) -> str:",
          "91:         self.write_git_version()",
          "92:         work_dir = Path(self.root)",
          "93:         commands = [",
          "94:             [\"pre-commit run --hook-stage manual compile-www-assets --all-files\"],",
          "95:         ]",
          "96:         for cmd in commands:",
          "97:             run(cmd, cwd=work_dir.as_posix(), check=True, shell=True)",
          "98:         dist_path = work_dir / \"airflow\" / \"www\" / \"static\" / \"dist\"",
          "99:         return dist_path.resolve().as_posix()",
          "101:     def get_git_version(self) -> str:",
          "102:         \"\"\"",
          "103:         Return a version to identify the state of the underlying git repo.",
          "105:         The version will indicate whether the head of the current git-backed working directory",
          "106:         is tied to a release tag or not. It will indicate the former with a 'release:{version}'",
          "107:         prefix and the latter with a '.dev0' suffix. Following the prefix will be a sha of the",
          "108:         current branch head. Finally, a \"dirty\" suffix is appended to indicate that uncommitted",
          "109:         changes are present.",
          "111:         Example pre-release version: \".dev0+2f635dc265e78db6708f59f68e8009abb92c1e65\".",
          "112:         Example release version: \".release+2f635dc265e78db6708f59f68e8009abb92c1e65\".",
          "113:         Example modified release version: \".release+2f635dc265e78db6708f59f68e8009abb92c1e65\".dirty",
          "115:         :return: Found Airflow version in Git repo.",
          "116:         \"\"\"",
          "117:         try:",
          "118:             import git",
          "120:             try:",
          "121:                 repo = git.Repo(str(Path(self.root) / \".git\"))",
          "122:             except git.NoSuchPathError:",
          "123:                 log.warning(\".git directory not found: Cannot compute the git version\")",
          "124:                 return \"\"",
          "125:             except git.InvalidGitRepositoryError:",
          "126:                 log.warning(\"Invalid .git directory not found: Cannot compute the git version\")",
          "127:                 return \"\"",
          "128:         except ImportError:",
          "129:             log.warning(\"gitpython not found: Cannot compute the git version.\")",
          "130:             return \"\"",
          "131:         if repo:",
          "132:             sha = repo.head.commit.hexsha",
          "133:             if repo.is_dirty():",
          "134:                 return f\".dev0+{sha}.dirty\"",
          "135:             # commit is clean",
          "136:             return f\".release:{sha}\"",
          "137:         return \"no_git_version\"",
          "139:     def write_git_version(self) -> None:",
          "140:         \"\"\"Write git version to git_version file.\"\"\"",
          "141:         version = self.get_git_version()",
          "142:         git_version_file = Path(self.root) / \"airflow\" / \"git_version\"",
          "143:         self.app.display(f\"Writing version {version} to {git_version_file}\")",
          "144:         git_version_file.write_text(version)",
          "147: class CustomBuildHook(BuildHookInterface[BuilderConfig]):",
          "148:     \"\"\"Custom build hook for Airflow - remove devel extras and adds preinstalled providers.\"\"\"",
          "150:     def initialize(self, version: str, build_data: dict[str, Any]) -> None:",
          "151:         \"\"\"",
          "152:         This occurs immediately before each build.",
          "154:         Any modifications to the build data will be seen by the build target.",
          "155:         \"\"\"",
          "156:         if version == \"standard\":",
          "157:             # remove devel dependencies from optional dependencies for standard packages",
          "158:             self.metadata.core._optional_dependencies = {",
          "159:                 key: value",
          "160:                 for (key, value) in self.metadata.core.optional_dependencies.items()",
          "161:                 if not key.startswith(\"devel\") and key not in [\"doc\", \"doc-gen\"]",
          "162:             }",
          "163:             # Replace editable dependencies with provider dependencies for provider packages",
          "164:             for dependency_id in DEPENDENCIES.keys():",
          "165:                 if DEPENDENCIES[dependency_id][\"state\"] != \"ready\":",
          "166:                     continue",
          "167:                 normalized_dependency_id = dependency_id.replace(\".\", \"-\")",
          "168:                 self.metadata.core._optional_dependencies[normalized_dependency_id] = [",
          "169:                     f\"apache-airflow-providers-{normalized_dependency_id}\"",
          "170:                 ]",
          "171:             # Inject preinstalled providers into the dependencies for standard packages",
          "172:             if self.metadata.core._dependencies:",
          "173:                 for provider in PREINSTALLED_PROVIDERS:",
          "174:                     self.metadata.core._dependencies.append(provider)",
          "175:                 for dependency in PREINSTALLED_NOT_READY_DEPS:",
          "176:                     self.metadata.core._dependencies.append(dependency)",
          "",
          "---------------"
        ],
        "dev/refresh_images.sh||dev/refresh_images.sh": [
          "File: dev/refresh_images.sh -> dev/refresh_images.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "36: rm -fv ./dist/* ./docker-context-files/*",
          "38: breeze release-management prepare-provider-packages \\",
          "40:     --package-format wheel \\",
          "41:     --version-suffix-for-pypi dev0",
          "",
          "[Removed Lines]",
          "39:     --package-list-file ./airflow/providers/installed_providers.txt \\",
          "",
          "[Added Lines]",
          "39:     --package-list-file ./dev/prod_image_installed_providers.txt \\",
          "",
          "---------------"
        ],
        "dev/sign.sh||dev/sign.sh": [
          "File: dev/sign.sh -> dev/sign.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: set -euo pipefail",
          "23: # you will still be required to type in your signing key password",
          "24: # or it needs to be available in your keychain",
          "",
          "[Removed Lines]",
          "20: # Use this to sign the tar balls generated from",
          "21: # python setup.py sdist --formats=gztar",
          "22: # ie. sign.sh <my_tar_ball>",
          "",
          "[Added Lines]",
          "20: # Use this to sign the tar balls generated via hatch",
          "",
          "---------------"
        ],
        "docker_tests/docker_tests_utils.py||docker_tests/docker_tests_utils.py": [
          "File: docker_tests/docker_tests_utils.py -> docker_tests/docker_tests_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "73: It can mean one of those:",
          "75: 1) The main is currently broken (other PRs will fail with the same error)",
          "",
          "[Removed Lines]",
          "76: 2) You changed some dependencies in setup.py or setup.cfg and they are conflicting.",
          "",
          "[Added Lines]",
          "76: 2) You changed some dependencies in pyproject.toml (either manually or automatically by pre-commit)",
          "77:    and they are conflicting.",
          "",
          "---------------"
        ],
        "docker_tests/test_prod_image.py||docker_tests/test_prod_image.py": [
          "File: docker_tests/test_prod_image.py -> docker_tests/test_prod_image.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import os",
          "21: import subprocess",
          "22: from importlib.util import find_spec",
          "24: import pytest",
          "27: from docker_tests.command_utils import run_command",
          "28: from docker_tests.constants import SOURCE_ROOT",
          "29: from docker_tests.docker_tests_utils import (",
          "",
          "[Removed Lines]",
          "26: # isort:off (needed to workaround isort bug)",
          "",
          "[Added Lines]",
          "23: from pathlib import Path",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "33:     run_python_in_docker,",
          "34: )",
          "42: class TestCommands:",
          "",
          "[Removed Lines]",
          "36: # isort:on (needed to workaround isort bug)",
          "37: from setup import PREINSTALLED_PROVIDERS",
          "39: INSTALLED_PROVIDER_PATH = SOURCE_ROOT / \"airflow\" / \"providers\" / \"installed_providers.txt\"",
          "",
          "[Added Lines]",
          "36: DEV_DIR_PATH = SOURCE_ROOT / \"dev\"",
          "37: AIRFLOW_PRE_INSTALLED_PROVIDERS_FILE_PATH = DEV_DIR_PATH / \"airflow_pre_installed_providers.txt\"",
          "38: PROD_IMAGE_PROVIDERS_FILE_PATH = DEV_DIR_PATH / \"prod_image_installed_providers.txt\"",
          "39: AIRFLOW_ROOT_PATH = Path(__file__).parents[2].resolve()",
          "40: SLIM_IMAGE_PROVIDERS = [",
          "41:     f\"apache-airflow-providers-{provider_id.replace('.','-')}\"",
          "42:     for provider_id in AIRFLOW_PRE_INSTALLED_PROVIDERS_FILE_PATH.read_text().splitlines()",
          "43:     if not provider_id.startswith(\"#\")",
          "44: ]",
          "45: REGULAR_IMAGE_PROVIDERS = [",
          "46:     f\"apache-airflow-providers-{provider_id.replace('.','-')}\"",
          "47:     for provider_id in PROD_IMAGE_PROVIDERS_FILE_PATH.read_text().splitlines()",
          "48:     if not provider_id.startswith(\"#\")",
          "49: ]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "80: class TestPythonPackages:",
          "81:     def test_required_providers_are_installed(self):",
          "82:         if os.environ.get(\"TEST_SLIM_IMAGE\"):",
          "84:         else:",
          "87:         assert len(packages_to_install) != 0",
          "89:         output = run_bash_in_docker(",
          "90:             \"airflow providers list --output json\", stderr=subprocess.DEVNULL, return_output=True",
          "91:         )",
          "92:         providers = json.loads(output)",
          "94:         assert len(packages_installed) != 0",
          "101:     def test_pip_dependencies_conflict(self):",
          "102:         try:",
          "",
          "[Removed Lines]",
          "83:             lines = PREINSTALLED_PROVIDERS",
          "85:             lines = (d.strip() for d in INSTALLED_PROVIDER_PATH.read_text().splitlines())",
          "86:         packages_to_install = {f\"apache-airflow-providers-{d.replace('.', '-')}\" for d in lines}",
          "93:         packages_installed = {d[\"package_name\"] for d in providers}",
          "96:         assert packages_to_install == packages_installed, (",
          "97:             f\"List of expected installed packages and image content mismatch. \"",
          "98:             f\"Check {INSTALLED_PROVIDER_PATH} file.\"",
          "99:         )",
          "",
          "[Added Lines]",
          "93:             packages_to_install = set(SLIM_IMAGE_PROVIDERS)",
          "94:             package_file = AIRFLOW_PRE_INSTALLED_PROVIDERS_FILE_PATH",
          "96:             packages_to_install = set(REGULAR_IMAGE_PROVIDERS)",
          "97:             package_file = PROD_IMAGE_PROVIDERS_FILE_PATH",
          "103:         packages_installed = set(d[\"package_name\"] for d in providers)",
          "106:         assert (",
          "107:             packages_to_install == packages_installed",
          "108:         ), f\"List of expected installed packages and image content mismatch. Check {package_file} file.\"",
          "",
          "---------------"
        ],
        "scripts/ci/docker-compose/devcontainer.env||scripts/ci/docker-compose/devcontainer.env": [
          "File: scripts/ci/docker-compose/devcontainer.env -> scripts/ci/docker-compose/devcontainer.env",
          "--- Hunk 1 ---",
          "[Context before]",
          "46: INIT_SCRIPT_FILE=\"init.sh\"",
          "47: INSTALL_AIRFLOW_VERSION=",
          "48: AIRFLOW_CONSTRAINTS_MODE=",
          "50: INSTALL_SELECTED_PROVIDERS=",
          "51: USE_AIRFLOW_VERSION=",
          "52: USE_PACKAGES_FROM_DIST=",
          "",
          "[Removed Lines]",
          "49: INSTALL_PROVIDERS_FROM_SOURCES=",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_check_extra_packages_ref.py||scripts/ci/pre_commit/pre_commit_check_extra_packages_ref.py": [
          "File: scripts/ci/pre_commit/pre_commit_check_extra_packages_ref.py -> scripts/ci/pre_commit/pre_commit_check_extra_packages_ref.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python",
          "2: #",
          "3: # Licensed to the Apache Software Foundation (ASF) under one",
          "4: # or more contributor license agreements.  See the NOTICE file",
          "5: # distributed with this work for additional information",
          "6: # regarding copyright ownership.  The ASF licenses this file",
          "7: # to you under the Apache License, Version 2.0 (the",
          "8: # \"License\"); you may not use this file except in compliance",
          "9: # with the License.  You may obtain a copy of the License at",
          "10: #",
          "11: #   http://www.apache.org/licenses/LICENSE-2.0",
          "12: #",
          "13: # Unless required by applicable law or agreed to in writing,",
          "14: # software distributed under the License is distributed on an",
          "15: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "16: # KIND, either express or implied.  See the License for the",
          "17: # specific language governing permissions and limitations",
          "18: # under the License.",
          "19: \"\"\"",
          "20: Checks if all the libraries in setup.py are listed in installation.rst file",
          "21: \"\"\"",
          "22: from __future__ import annotations",
          "24: import re",
          "25: import sys",
          "26: from pathlib import Path",
          "28: from tabulate import tabulate",
          "30: # tomllib is available in Python 3.11+ and before that tomli offers same interface for parsing TOML files",
          "31: try:",
          "32:     import tomllib",
          "33: except ImportError:",
          "34:     import tomli as tomllib",
          "37: AIRFLOW_ROOT_PATH = Path(__file__).parents[3].resolve()",
          "38: EXTRA_PACKAGES_REF_FILE = AIRFLOW_ROOT_PATH / \"docs\" / \"apache-airflow\" / \"extra-packages-ref.rst\"",
          "39: PYPROJECT_TOML_FILE_PATH = AIRFLOW_ROOT_PATH / \"pyproject.toml\"",
          "41: sys.path.insert(0, str(Path(__file__).parent.resolve()))  # make sure common_precommit_utils is imported",
          "43: from common_precommit_utils import console",
          "45: pyproject_toml_content = tomllib.loads(PYPROJECT_TOML_FILE_PATH.read_text())",
          "47: optional_dependencies: dict[str, list[str]] = pyproject_toml_content[\"project\"][\"optional-dependencies\"]",
          "48: doc_ref_content = EXTRA_PACKAGES_REF_FILE.read_text()",
          "50: errors: list[str] = []",
          "51: regular_suggestions: list[str] = []",
          "52: devel_suggestions: list[str] = []",
          "53: suggestions: list[tuple] = []",
          "54: suggestions_devel: list[tuple] = []",
          "55: suggestions_providers: list[tuple] = []",
          "57: for dependency in optional_dependencies:",
          "58:     console.print(f\"[bright_blue]Checking if {dependency} is mentioned in refs[/]\")",
          "59:     find_matching = re.search(rf\"^\\| {dependency} *\\|\", doc_ref_content, flags=re.MULTILINE)",
          "60:     if not find_matching:",
          "61:         errors.append(f\"[red]ERROR: {dependency} is not listed in {EXTRA_PACKAGES_REF_FILE}[/]\")",
          "62:         is_devel_dep = dependency.startswith(\"devel\") or dependency in [\"doc\", \"doc-gen\"]",
          "63:         short_dep = dependency.replace(\"devel-\", \"\")",
          "64:         if is_devel_dep:",
          "65:             suggestions_devel.append(",
          "66:                 (",
          "67:                     dependency,",
          "68:                     f\"pip install -e '.[{dependency}]'\",",
          "69:                     f\"Adds all test libraries needed to test {short_dep}\",",
          "70:                 )",
          "71:             )",
          "72:         else:",
          "73:             suggestions.append(",
          "74:                 (",
          "75:                     dependency,",
          "76:                     f\"pip install apache-airflow[{dependency}]\",",
          "77:                     f\"{dependency.capitalize()} hooks and operators\",",
          "78:                 )",
          "79:             )",
          "81: HEADERS = [\"extra\", \"install command\", \"enables\"]",
          "82: if errors:",
          "83:     console.print(\"\\n\".join(errors))",
          "84:     console.print()",
          "85:     console.print(\"[bright_blue]Suggested tables to add to references::[/]\")",
          "86:     if suggestions:",
          "87:         console.print(\"[bright_blue]Regular dependencies[/]\")",
          "88:         console.print(tabulate(suggestions, headers=HEADERS, tablefmt=\"grid\"), markup=False)",
          "89:     if suggestions_devel:",
          "90:         console.print(\"[bright_blue]Devel dependencies[/]\")",
          "91:         console.print(tabulate(suggestions_devel, headers=HEADERS, tablefmt=\"grid\"), markup=False)",
          "92:     if suggestions_providers:",
          "93:         console.print(\"[bright_blue]Devel dependencies[/]\")",
          "94:         console.print(tabulate(suggestions_providers, headers=HEADERS, tablefmt=\"grid\"), markup=False)",
          "95:     sys.exit(1)",
          "96: else:",
          "97:     console.print(f\"[green]Checked: {len(optional_dependencies)} dependencies are mentioned[/]\")",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_check_order_dockerfile_extras.py||scripts/ci/pre_commit/pre_commit_check_order_dockerfile_extras.py": [
          "File: scripts/ci/pre_commit/pre_commit_check_order_dockerfile_extras.py -> scripts/ci/pre_commit/pre_commit_check_order_dockerfile_extras.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # specific language governing permissions and limitations",
          "18: # under the License.",
          "19: \"\"\"",
          "21: \"\"\"",
          "22: from __future__ import annotations",
          "",
          "[Removed Lines]",
          "20: Test for an order of dependencies in setup.py",
          "",
          "[Added Lines]",
          "20: Check if extras in Dockerfile are reflected in docker build-arg-ref.rst and global constants.",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_check_order_pyproject_toml.py||scripts/ci/pre_commit/pre_commit_check_order_pyproject_toml.py": [
          "File: scripts/ci/pre_commit/pre_commit_check_order_pyproject_toml.py -> scripts/ci/pre_commit/pre_commit_check_order_pyproject_toml.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python",
          "2: #",
          "3: # Licensed to the Apache Software Foundation (ASF) under one",
          "4: # or more contributor license agreements.  See the NOTICE file",
          "5: # distributed with this work for additional information",
          "6: # regarding copyright ownership.  The ASF licenses this file",
          "7: # to you under the Apache License, Version 2.0 (the",
          "8: # \"License\"); you may not use this file except in compliance",
          "9: # with the License.  You may obtain a copy of the License at",
          "10: #",
          "11: #   http://www.apache.org/licenses/LICENSE-2.0",
          "12: #",
          "13: # Unless required by applicable law or agreed to in writing,",
          "14: # software distributed under the License is distributed on an",
          "15: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "16: # KIND, either express or implied.  See the License for the",
          "17: # specific language governing permissions and limitations",
          "18: # under the License.",
          "19: \"\"\"",
          "20: Test for an order of dependencies in setup.py",
          "21: \"\"\"",
          "22: from __future__ import annotations",
          "24: import re",
          "25: import sys",
          "26: from pathlib import Path",
          "28: from rich import print",
          "30: errors: list[str] = []",
          "32: AIRFLOW_ROOT_PATH = Path(__file__).parents[3].resolve()",
          "33: PYPROJECT_TOML_PATH = AIRFLOW_ROOT_PATH / \"pyproject.toml\"",
          "35: sys.path.insert(0, str(Path(__file__).parent.resolve()))  # make sure common_precommit_utils is imported",
          "36: from common_precommit_utils import check_list_sorted",
          "39: def check_extras(type: str, extra: str, extras: list[str]) -> None:",
          "40:     r\"\"\"",
          "41:     Test for an order of dependencies in extra defined",
          "42:     `^dependent_group_name = [.*?]\\n` in setup.py",
          "43:     \"\"\"",
          "44:     print(f\"[info]Checking {type}:{extra}[/]\")",
          "45:     extras = [extra.replace(\"[\", \"\\\\[\") for extra in extras]",
          "46:     check_list_sorted(extras, f\"Order of extra: {type}:{extra}\", errors)",
          "49: def extract_deps(content: str, extra: str) -> list[str]:",
          "50:     deps: list[str] = []",
          "51:     extracting = False",
          "52:     for line in content.splitlines():",
          "53:         line = line.strip()",
          "54:         if line.startswith(\"#\"):",
          "55:             continue",
          "56:         if not extracting and line == f\"{extra} = [\":",
          "57:             extracting = True",
          "58:         elif extracting and line == \"]\":",
          "59:             break",
          "60:         elif extracting:",
          "61:             deps.append(line.strip().strip(\",\").strip('\"'))",
          "62:     return deps",
          "65: def check_type(pyproject_toml_contents: str, type: str) -> None:",
          "66:     \"\"\"",
          "67:     Test for an order of dependencies groups between mark",
          "68:     '# Start dependencies group' and '# End dependencies group' in setup.py",
          "69:     \"\"\"",
          "70:     print(f\"[info]Checking {type}[/]\")",
          "71:     pattern_type = re.compile(f\"# START OF {type}\\n(.*)# END OF {type}\", re.DOTALL)",
          "72:     parsed_type_content = pattern_type.findall(pyproject_toml_contents)[0]",
          "73:     # strip comments",
          "74:     parsed_type_content = (",
          "75:         \"\\n\".join([line for line in parsed_type_content.splitlines() if not line.startswith(\"#\")]) + \"\\n\"",
          "76:     )",
          "77:     pattern_extra_name = re.compile(r\" = \\[.*?]\\n\", re.DOTALL)",
          "78:     type_content = pattern_extra_name.sub(\",\", parsed_type_content)",
          "80:     list_extra_names = type_content.strip(\",\").split(\",\")",
          "81:     check_list_sorted(list_extra_names, \"Order of dependencies\", errors)",
          "82:     for extra in list_extra_names:",
          "83:         deps_list = extract_deps(parsed_type_content, extra)",
          "84:         check_extras(type, extra, deps_list)",
          "87: if __name__ == \"__main__\":",
          "88:     file_contents = PYPROJECT_TOML_PATH.read_text()",
          "89:     check_type(file_contents, \"core extras\")",
          "90:     check_type(file_contents, \"Apache no provider extras\")",
          "91:     check_type(file_contents, \"devel extras\")",
          "92:     check_type(file_contents, \"doc extras\")",
          "93:     check_type(file_contents, \"bundle extras\")",
          "94:     check_type(file_contents, \"deprecated extras\")",
          "96:     print()",
          "97:     for error in errors:",
          "98:         print(error)",
          "100:     print()",
          "102:     if errors:",
          "103:         sys.exit(1)",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_check_order_setup.py||scripts/ci/pre_commit/pre_commit_check_order_setup.py": [
          "File: scripts/ci/pre_commit/pre_commit_check_order_setup.py -> scripts/ci/pre_commit/pre_commit_check_order_setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_check_setup_extra_packages_ref.py||scripts/ci/pre_commit/pre_commit_check_setup_extra_packages_ref.py": [
          "File: scripts/ci/pre_commit/pre_commit_check_setup_extra_packages_ref.py -> scripts/ci/pre_commit/pre_commit_check_setup_extra_packages_ref.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_compile_www_assets.py||scripts/ci/pre_commit/pre_commit_compile_www_assets.py": [
          "File: scripts/ci/pre_commit/pre_commit_compile_www_assets.py -> scripts/ci/pre_commit/pre_commit_compile_www_assets.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import hashlib",
          "21: import os",
          "22: import re",
          "23: import subprocess",
          "24: import sys",
          "25: from pathlib import Path",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "23: import shutil",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "54: if __name__ == \"__main__\":",
          "55:     www_directory = AIRFLOW_SOURCES_PATH / \"airflow\" / \"www\"",
          "56:     WWW_HASH_FILE.parent.mkdir(exist_ok=True)",
          "62:     env = os.environ.copy()",
          "63:     env[\"FORCE_COLOR\"] = \"true\"",
          "64:     subprocess.check_call([\"yarn\", \"install\", \"--frozen-lockfile\"], cwd=os.fspath(www_directory))",
          "65:     subprocess.check_call([\"yarn\", \"run\", \"build\"], cwd=os.fspath(www_directory), env=env)",
          "66:     WWW_HASH_FILE.write_text(new_hash)",
          "",
          "[Removed Lines]",
          "57:     old_hash = WWW_HASH_FILE.read_text() if WWW_HASH_FILE.exists() else \"\"",
          "58:     new_hash = get_directory_hash(www_directory, skip_path_regexp=r\".*node_modules.*\")",
          "59:     if new_hash == old_hash:",
          "60:         print(\"The WWW directory has not changed! Skip regeneration.\")",
          "61:         sys.exit(0)",
          "",
          "[Added Lines]",
          "57:     node_modules_directory = www_directory / \"node_modules\"",
          "58:     dist_directory = www_directory / \"static\" / \"dist\"",
          "60:     if node_modules_directory.exists() and dist_directory.exists():",
          "61:         old_hash = WWW_HASH_FILE.read_text() if WWW_HASH_FILE.exists() else \"\"",
          "62:         new_hash = get_directory_hash(www_directory, skip_path_regexp=r\".*node_modules.*\")",
          "63:         if new_hash == old_hash:",
          "64:             print(\"The WWW directory has not changed! Skip regeneration.\")",
          "65:             sys.exit(0)",
          "66:     else:",
          "67:         shutil.rmtree(node_modules_directory, ignore_errors=True)",
          "68:         shutil.rmtree(dist_directory, ignore_errors=True)",
          "73:     new_hash = get_directory_hash(www_directory, skip_path_regexp=r\".*node_modules.*\")",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_insert_extras.py||scripts/ci/pre_commit/pre_commit_insert_extras.py": [
          "File: scripts/ci/pre_commit/pre_commit_insert_extras.py -> scripts/ci/pre_commit/pre_commit_insert_extras.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "21: import sys",
          "22: import textwrap",
          "23: from pathlib import Path",
          "27: sys.path.insert(0, str(Path(__file__).parent.resolve()))  # make sure common_precommit_utils is imported",
          "48: DEFAULT_EXTRAS = (",
          "49:     \"amazon,async,celery,cncf.kubernetes,daskexecutor,docker,elasticsearch,ftp,google,\"",
          "",
          "[Removed Lines]",
          "20: import os",
          "25: AIRFLOW_SOURCES_DIR = Path(__file__).parents[3].resolve()",
          "28: sys.path.insert(0, str(AIRFLOW_SOURCES_DIR))  # make sure setup is imported from Airflow",
          "29: # flake8: noqa: F401",
          "31: os.environ[\"_SKIP_PYTHON_VERSION_CHECK\"] = \"true\"",
          "33: from common_precommit_utils import insert_documentation",
          "35: from setup import EXTRAS_DEPENDENCIES",
          "37: sys.path.append(str(AIRFLOW_SOURCES_DIR))",
          "39: RST_HEADER = \"  .. START EXTRAS HERE\"",
          "40: RST_FOOTER = \"  .. END EXTRAS HERE\"",
          "42: INSTALL_HEADER = \"# START EXTRAS HERE\"",
          "43: INSTALL_FOOTER = \"# END EXTRAS HERE\"",
          "45: CONSTANTS_HEADER = \"# START EXTRAS HERE\"",
          "46: CONSTANTS_FOOTER = \"# END EXTRAS HERE\"",
          "",
          "[Added Lines]",
          "22: from enum import Enum",
          "25: # tomllib is available in Python 3.11+ and before that tomli offers same interface for parsing TOML files",
          "26: try:",
          "27:     import tomllib",
          "28: except ImportError:",
          "29:     import tomli as tomllib",
          "31: AIRFLOW_ROOT_PATH = Path(__file__).parents[3].resolve()",
          "32: PYPROJECT_TOML_FILE_PATH = AIRFLOW_ROOT_PATH / \"pyproject.toml\"",
          "35: from common_precommit_utils import insert_documentation",
          "38: class ExtraType(Enum):",
          "39:     DEVEL = \"DEVEL\"",
          "40:     DOC = \"DOC\"",
          "41:     REGULAR = \"REGULAR\"",
          "44: def get_header_and_footer(extra_type: ExtraType, file_format: str) -> tuple[str, str]:",
          "45:     if file_format == \"rst\":",
          "46:         return f\"  .. START {extra_type.value} EXTRAS HERE\", f\"  .. END {extra_type.value} EXTRAS HERE\"",
          "47:     elif file_format == \"txt\":",
          "48:         return f\"# START {extra_type.value} EXTRAS HERE\", f\"# END {extra_type.value} EXTRAS HERE\"",
          "49:     else:",
          "50:         raise Exception(f\"Bad format {format} passed. Only rst and txt are supported\")",
          "53: def get_wrapped_list(extras_set: set[str]) -> list[str]:",
          "54:     return [line + \"\\n\" for line in textwrap.wrap(\", \".join(sorted(extras_set)), 100)]",
          "57: def get_extra_types_dict(extras: dict[str, list[str]]) -> dict[ExtraType, tuple[set[str], list[str]]]:",
          "58:     \"\"\"",
          "59:     Split extras into four types.",
          "61:     :return: dictionary of extra types with tuple of two set,list - set of extras and text-wrapped list",
          "62:     \"\"\"",
          "63:     extra_type_dict: dict[ExtraType, tuple[set[str], list[str]]] = {}",
          "65:     for extra_type in ExtraType:",
          "66:         extra_type_dict[extra_type] = (set(), [])",
          "68:     for key, value in extras.items():",
          "69:         if key.startswith(\"devel\"):",
          "70:             extra_type_dict[ExtraType.DEVEL][0].add(key)",
          "71:         elif key in [\"doc\", \"doc-gen\"]:",
          "72:             extra_type_dict[ExtraType.DOC][0].add(key)",
          "73:         else:",
          "74:             extra_type_dict[ExtraType.REGULAR][0].add(key)",
          "76:     for extra_type in ExtraType:",
          "77:         extra_type_dict[extra_type][1].extend(get_wrapped_list(extra_type_dict[extra_type][0]))",
          "79:     return extra_type_dict",
          "82: def get_extras_from_pyproject_toml() -> dict[str, list[str]]:",
          "83:     pyproject_toml_content = tomllib.loads(PYPROJECT_TOML_FILE_PATH.read_text())",
          "84:     return pyproject_toml_content[\"project\"][\"optional-dependencies\"]",
          "87: FILES_TO_UPDATE = [(AIRFLOW_ROOT_PATH / \"INSTALL\", \"txt\"), (AIRFLOW_ROOT_PATH / \"CONTRIBUTING.rst\", \"rst\")]",
          "90: def process_documentation_files():",
          "91:     extra_type_dict = get_extra_types_dict(get_extras_from_pyproject_toml())",
          "92:     for file, file_format in FILES_TO_UPDATE:",
          "93:         if not file.exists():",
          "94:             raise Exception(f\"File {file} does not exist\")",
          "95:         for extra_type in ExtraType:",
          "96:             header, footer = get_header_and_footer(extra_type, file_format)",
          "97:             insert_documentation(file, extra_type_dict[extra_type][1], header, footer)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "55: if __name__ == \"__main__\":",
          "",
          "[Removed Lines]",
          "56:     install_file_path = AIRFLOW_SOURCES_DIR / \"INSTALL\"",
          "57:     contributing_file_path = AIRFLOW_SOURCES_DIR / \"CONTRIBUTING.rst\"",
          "58:     global_constants_file_path = (",
          "59:         AIRFLOW_SOURCES_DIR / \"dev\" / \"breeze\" / \"src\" / \"airflow_breeze\" / \"global_constants.py\"",
          "60:     )",
          "61:     extras_list = textwrap.wrap(\", \".join(EXTRAS_DEPENDENCIES.keys()), 100)",
          "62:     extras_list = [line + \"\\n\" for line in extras_list]",
          "63:     extras_code = [f\"    {extra}\\n\" for extra in EXTRAS_DEPENDENCIES.keys()]",
          "64:     insert_documentation(install_file_path, extras_list, INSTALL_HEADER, INSTALL_FOOTER)",
          "65:     insert_documentation(contributing_file_path, extras_list, RST_HEADER, RST_FOOTER)",
          "",
          "[Added Lines]",
          "108:     process_documentation_files()",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_sort_installed_providers.py||scripts/ci/pre_commit/pre_commit_sort_installed_providers.py": [
          "File: scripts/ci/pre_commit/pre_commit_sort_installed_providers.py -> scripts/ci/pre_commit/pre_commit_sort_installed_providers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: AIRFLOW_SOURCES = Path(__file__).parents[3].resolve()",
          "32: def stable_sort(x):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "30: DEV_DIR_PATH = AIRFLOW_SOURCES / \"dev\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "37:     return sorted(set(sequence), key=stable_sort)",
          "43:     sorted_content = sort_uniq(content)",
          "",
          "[Removed Lines]",
          "40: if __name__ == \"__main__\":",
          "41:     installed_providers_path = Path(AIRFLOW_SOURCES) / \"airflow\" / \"providers\" / \"installed_providers.txt\"",
          "42:     content = installed_providers_path.read_text().splitlines(keepends=True)",
          "44:     installed_providers_path.write_text(\"\".join(sorted_content))",
          "",
          "[Added Lines]",
          "41: def sort_file(path: Path):",
          "42:     content = path.read_text().splitlines(keepends=True)",
          "44:     path.write_text(\"\".join(sorted_content))",
          "47: if __name__ == \"__main__\":",
          "48:     prod_image_installed_providers_path = DEV_DIR_PATH / \"prod_image_installed_providers.txt\"",
          "49:     airflow_pre_installed_providers_path = DEV_DIR_PATH / \"airflow_pre_installed_providers.txt\"",
          "50:     sort_file(prod_image_installed_providers_path)",
          "51:     sort_file(airflow_pre_installed_providers_path)",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py||scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py": [
          "File: scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py -> scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: import json",
          "21: import os",
          "22: import sys",
          "23: from ast import Import, ImportFrom, NodeVisitor, parse",
          "24: from collections import defaultdict",
          "25: from pathlib import Path",
          "26: from typing import Any, List",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import hashlib",
          "26: from enum import Enum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "41: DEPENDENCIES_JSON_FILE_PATH = AIRFLOW_SOURCES_ROOT / \"generated\" / \"provider_dependencies.json\"",
          "43: sys.path.insert(0, str(AIRFLOW_SOURCES_ROOT))  # make sure setup is imported from Airflow",
          "45: warnings: list[str] = []",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "45: PYPROJECT_TOML_FILE_PATH = AIRFLOW_SOURCES_ROOT / \"pyproject.toml\"",
          "47: MY_FILE = Path(__file__).resolve()",
          "48: MY_MD5SUM_FILE = MY_FILE.parent / MY_FILE.name.replace(\".py\", \".py.md5sum\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "109:     provider_candidate = relative_path_or_file.replace(os.sep, \".\").split(\".\")",
          "110:     while provider_candidate:",
          "111:         candidate_provider_id = \".\".join(provider_candidate)",
          "114:         if candidate_provider_id in ALL_PROVIDERS:",
          "115:             return candidate_provider_id",
          "116:         provider_candidate = provider_candidate[:-1]",
          "",
          "[Removed Lines]",
          "112:         if \"google_vendor\" in candidate_provider_id:",
          "113:             candidate_provider_id = candidate_provider_id.replace(\"google_vendor\", \"google\")",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "176: STATES: dict[str, str] = {}",
          "179: if __name__ == \"__main__\":",
          "180:     find_all_providers_and_provider_files()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "184: FOUND_EXTRAS: dict[str, list[str]] = defaultdict(list)",
          "187: class ParsedDependencyTypes(Enum):",
          "188:     CORE_EXTRAS = \"core extras\"",
          "189:     APACHE_NO_PROVIDER_EXTRAS = \"Apache no provider extras\"",
          "190:     DEVEL_EXTRAS = \"devel extras\"",
          "191:     DOC_EXTRAS = \"doc extras\"",
          "192:     BUNDLE_EXTRAS = \"bundle extras\"",
          "193:     DEPRECATED_EXTRAS = \"deprecated extras\"",
          "194:     MANUAL_EXTRAS = \"manual extras\"",
          "197: GENERATED_DEPENDENCIES_START = \"# START OF GENERATED DEPENDENCIES\"",
          "198: GENERATED_DEPENDENCIES_END = \"# END OF GENERATED DEPENDENCIES\"",
          "201: def normalize_extra(dependency: str) -> str:",
          "202:     return dependency.replace(\".\", \"-\").replace(\"_\", \"-\")",
          "205: def normalize_package_name(dependency: str) -> str:",
          "206:     return f\"apache-airflow-providers-{dependency.replace('.', '-').replace('_', '-')}\"",
          "209: def convert_to_extra_dependency(dependency: str) -> str:",
          "210:     # if there is version in dependency - remove it as we do not need it in extra specification",
          "211:     # for editable installation",
          "212:     if \">=\" in dependency:",
          "213:         dependency = dependency.split(\">=\")[0]",
          "214:     extra = dependency.replace(\"apache-airflow-providers-\", \"\").replace(\"-\", \"_\").replace(\".\", \"_\")",
          "215:     return f\"apache-airflow[{extra}]\"",
          "218: def generate_dependencies(",
          "219:     result_content: list[str],",
          "220:     dependencies: dict[str, dict[str, list[str] | str]],",
          "221: ):",
          "222:     def generate_parsed_extras(type: ParsedDependencyTypes):",
          "223:         result_content.append(f\"    # {type.value}\")",
          "224:         for extra in FOUND_EXTRAS[type.value]:",
          "225:             result_content.append(f'    \"apache-airflow[{extra}]\",')",
          "227:     def get_python_exclusion(dependency_info: dict[str, list[str] | str]):",
          "228:         excluded_python_versions = dependency_info.get(\"excluded-python-versions\")",
          "229:         exclusion = \"\"",
          "230:         if excluded_python_versions:",
          "231:             separator = \";\"",
          "232:             for version in excluded_python_versions:",
          "233:                 exclusion += f'{separator}python_version != \\\\\"{version}\\\\\"'",
          "234:                 separator = \" and \"",
          "235:         return exclusion",
          "237:     for dependency, dependency_info in dependencies.items():",
          "238:         if dependency_info[\"state\"] in [\"suspended\", \"removed\"]:",
          "239:             continue",
          "240:         result_content.append(f\"{normalize_extra(dependency)} = [\")",
          "241:         deps = dependency_info[\"deps\"]",
          "242:         if not isinstance(deps, list):",
          "243:             raise TypeError(f\"Wrong type of 'deps' {deps} for {dependency} in {DEPENDENCIES_JSON_FILE_PATH}\")",
          "244:         for dep in deps:",
          "245:             if dep.startswith(\"apache-airflow-providers-\"):",
          "246:                 dep = convert_to_extra_dependency(dep)",
          "247:             elif dep.startswith(\"apache-airflow>=\"):",
          "248:                 continue",
          "249:             result_content.append(f'  \"{dep}{get_python_exclusion(dependency_info)}\",')",
          "250:         devel_deps = dependency_info.get(\"devel-deps\")",
          "251:         if devel_deps:",
          "252:             result_content.append(f\"  # Devel dependencies for the {dependency} provider\")",
          "253:             for dep in devel_deps:",
          "254:                 result_content.append(f'  \"{dep}{get_python_exclusion(dependency_info)}\",')",
          "255:         result_content.append(\"]\")",
          "256:     result_content.append(\"all = [\")",
          "257:     generate_parsed_extras(ParsedDependencyTypes.CORE_EXTRAS)",
          "258:     generate_parsed_extras(ParsedDependencyTypes.APACHE_NO_PROVIDER_EXTRAS)",
          "259:     result_content.append(\"    # Provider extras\")",
          "260:     for dependency, dependency_info in dependencies.items():",
          "261:         result_content.append(f'    \"apache-airflow[{normalize_extra(dependency)}]\",')",
          "262:     result_content.append(\"]\")",
          "263:     result_content.append(\"devel-all = [\")",
          "264:     result_content.append('    \"apache-airflow[all]\",')",
          "265:     result_content.append('    \"apache-airflow[devel]\",')",
          "266:     result_content.append('    \"apache-airflow[doc]\",')",
          "267:     result_content.append('    \"apache-airflow[doc-gen]\",')",
          "268:     result_content.append('    \"apache-airflow[saml]\",')",
          "269:     generate_parsed_extras(ParsedDependencyTypes.APACHE_NO_PROVIDER_EXTRAS)",
          "270:     result_content.append(\"    # Include all provider deps\")",
          "271:     for dependency, dependency_info in dependencies.items():",
          "272:         result_content.append(f'    \"apache-airflow[{normalize_extra(dependency)}]\",')",
          "273:     result_content.append(\"]\")",
          "276: def get_dependency_type(dependency_type: str) -> ParsedDependencyTypes | None:",
          "277:     for dep_type in ParsedDependencyTypes:",
          "278:         if dep_type.value == dependency_type:",
          "279:             return dep_type",
          "280:     return None",
          "283: def update_pyproject_toml(dependencies: dict[str, dict[str, list[str] | str]]):",
          "284:     file_content = PYPROJECT_TOML_FILE_PATH.read_text()",
          "285:     result_content: list[str] = []",
          "286:     copying = True",
          "287:     current_type: str | None = None",
          "288:     line_count: int = 0",
          "289:     for line in file_content.splitlines():",
          "290:         if copying:",
          "291:             result_content.append(line)",
          "292:         if line.strip().startswith(GENERATED_DEPENDENCIES_START):",
          "293:             copying = False",
          "294:             generate_dependencies(result_content, dependencies)",
          "295:         elif line.strip().startswith(GENERATED_DEPENDENCIES_END):",
          "296:             copying = True",
          "297:             result_content.append(line)",
          "298:         elif line.strip().startswith(\"# START OF \"):",
          "299:             current_type = line.strip().replace(\"# START OF \", \"\")",
          "300:             type_enum = get_dependency_type(current_type)",
          "301:             if type_enum is None:",
          "302:                 console.print(",
          "303:                     f\"[red]Wrong start of section '{current_type}' in {PYPROJECT_TOML_FILE_PATH} \"",
          "304:                     f\"at line {line_count}: Unknown section type\"",
          "305:                 )",
          "306:                 sys.exit(1)",
          "307:         elif line.strip().startswith(\"# END OF \"):",
          "308:             end_type = line.strip().replace(\"# END OF \", \"\")",
          "309:             if end_type != current_type:",
          "310:                 console.print(",
          "311:                     f\"[red]Wrong end of section {end_type} in {PYPROJECT_TOML_FILE_PATH} at line {line_count}\"",
          "312:                 )",
          "313:                 sys.exit(1)",
          "314:         if current_type:",
          "315:             if line.strip().endswith(\" = [\"):",
          "316:                 FOUND_EXTRAS[current_type].append(line.split(\" = [\")[0].strip())",
          "317:         line_count += 1",
          "318:     PYPROJECT_TOML_FILE_PATH.write_text(\"\\n\".join(result_content) + \"\\n\")",
          "321: def calculate_my_hash():",
          "322:     my_file = MY_FILE.resolve()",
          "323:     hash_md5 = hashlib.md5()",
          "324:     hash_md5.update(my_file.read_bytes())",
          "325:     return hash_md5.hexdigest()",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "185:         check_if_different_provider_used(file)",
          "186:     for provider, provider_yaml_content in ALL_PROVIDERS.items():",
          "187:         ALL_DEPENDENCIES[provider][\"deps\"].extend(provider_yaml_content[\"dependencies\"])",
          "188:         STATES[provider] = provider_yaml_content[\"state\"]",
          "189:     if warnings:",
          "190:         console.print(\"[yellow]Warnings!\\n\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "337:         ALL_DEPENDENCIES[provider][\"devel-deps\"].extend(provider_yaml_content.get(\"devel-dependencies\") or [])",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "199:     unique_sorted_dependencies: dict[str, dict[str, list[str] | str]] = defaultdict(dict)",
          "200:     for key in sorted(ALL_DEPENDENCIES.keys()):",
          "201:         unique_sorted_dependencies[key][\"deps\"] = sorted(ALL_DEPENDENCIES[key][\"deps\"])",
          "202:         unique_sorted_dependencies[key][\"cross-providers-deps\"] = sorted(",
          "203:             set(ALL_DEPENDENCIES[key][\"cross-providers-deps\"])",
          "204:         )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "352:         unique_sorted_dependencies[key][\"devel-deps\"] = ALL_DEPENDENCIES[key].get(\"devel-deps\") or []",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "210:         console.print(\"[red]Errors found during verification. Exiting!\")",
          "211:         console.print()",
          "212:         sys.exit(1)",
          "214:     new_dependencies = json.dumps(unique_sorted_dependencies, indent=2) + \"\\n\"",
          "216:         DEPENDENCIES_JSON_FILE_PATH.write_text(json.dumps(unique_sorted_dependencies, indent=2) + \"\\n\")",
          "217:         if os.environ.get(\"CI\"):",
          "218:             console.print()",
          "220:             console.print(",
          "221:                 f\"[red]You need to run the following command locally and commit generated \"",
          "222:                 f\"{DEPENDENCIES_JSON_FILE_PATH.relative_to(AIRFLOW_SOURCES_ROOT)} file:\\n\"",
          "",
          "[Removed Lines]",
          "213:     old_dependencies = DEPENDENCIES_JSON_FILE_PATH.read_text()",
          "215:     if new_dependencies != old_dependencies:",
          "219:             console.print(f\"[info]There is a need to regenerate {DEPENDENCIES_JSON_FILE_PATH}\")",
          "",
          "[Added Lines]",
          "364:     old_dependencies = (",
          "365:         DEPENDENCIES_JSON_FILE_PATH.read_text() if DEPENDENCIES_JSON_FILE_PATH.exists() else \"{}\"",
          "366:     )",
          "368:     old_md5sum = MY_MD5SUM_FILE.read_text().strip() if MY_MD5SUM_FILE.exists() else \"\"",
          "369:     new_md5sum = calculate_my_hash()",
          "370:     if new_dependencies != old_dependencies or new_md5sum != old_md5sum:",
          "374:             console.print(f\"There is a need to regenerate {DEPENDENCIES_JSON_FILE_PATH}\")",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "229:                 f\"[yellow]Regenerated new dependencies. Please commit \"",
          "230:                 f\"{DEPENDENCIES_JSON_FILE_PATH.relative_to(AIRFLOW_SOURCES_ROOT)}!\\n\"",
          "231:             )",
          "233:             console.print()",
          "234:         sys.exit(1)",
          "235:     else:",
          "236:         console.print(",
          "",
          "[Removed Lines]",
          "232:             console.print(f\"[info]Written {DEPENDENCIES_JSON_FILE_PATH}\")",
          "",
          "[Added Lines]",
          "387:             console.print(f\"Written {DEPENDENCIES_JSON_FILE_PATH}\")",
          "388:             console.print()",
          "389:             update_pyproject_toml(unique_sorted_dependencies)",
          "390:             console.print(f\"Written {PYPROJECT_TOML_FILE_PATH}\")",
          "392:             MY_MD5SUM_FILE.write_text(new_md5sum + \"\\n\")",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py.md5sum||scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py.md5sum": [
          "File: scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py.md5sum -> scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py.md5sum",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: ed25c4f6b220c14b40bbf370fee9388e",
          "",
          "---------------"
        ],
        "scripts/docker/entrypoint_ci.sh||scripts/docker/entrypoint_ci.sh": [
          "File: scripts/docker/entrypoint_ci.sh -> scripts/docker/entrypoint_ci.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "231:     if [[ ${DOWNGRADE_SQLALCHEMY=} != \"true\" ]]; then",
          "232:         return",
          "233:     fi",
          "235:     echo",
          "236:     echo \"${COLOR_BLUE}Downgrading sqlalchemy to minimum supported version: ${min_sqlalchemy_version}${COLOR_RESET}\"",
          "237:     echo",
          "",
          "[Removed Lines]",
          "234:     min_sqlalchemy_version=$(grep \"sqlalchemy>=\" setup.cfg | sed \"s/.*>=\\([0-9\\.]*\\).*/\\1/\")",
          "",
          "[Added Lines]",
          "234:     min_sqlalchemy_version=$(grep \"\\\"sqlalchemy>=\" pyproject.toml | sed \"s/.*>=\\([0-9\\.]*\\).*/\\1/\" | xargs)",
          "",
          "---------------"
        ],
        "scripts/docker/install_airflow.sh||scripts/docker/install_airflow.sh": [
          "File: scripts/docker/install_airflow.sh -> scripts/docker/install_airflow.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "60:             \"${AIRFLOW_INSTALLATION_METHOD}[${AIRFLOW_EXTRAS}]${AIRFLOW_VERSION_SPECIFICATION}\" \\",
          "61:             ${EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=}",
          "62:         if [[ -n \"${AIRFLOW_INSTALL_EDITABLE_FLAG}\" ]]; then",
          "64:             # We can only do it when we install airflow from sources",
          "65:             set -x",
          "67:             pip install --root-user-action ignore ${AIRFLOW_INSTALL_EDITABLE_FLAG} \\",
          "68:                 ${ADDITIONAL_PIP_INSTALL_FLAGS} \\",
          "69:                 \"${AIRFLOW_INSTALLATION_METHOD}[${AIRFLOW_EXTRAS}]${AIRFLOW_VERSION_SPECIFICATION}\"",
          "",
          "[Removed Lines]",
          "63:             # Remove airflow and reinstall it using editable flag",
          "66:             pip uninstall apache-airflow --yes",
          "",
          "[Added Lines]",
          "63:             # Remove airflow and all providers and reinstall it using editable flag",
          "66:             pip freeze | grep apache-airflow-providers | xargs pip uninstall --yes 2>/dev/null || true",
          "67:             pip uninstall apache-airflow --yes 2>/dev/null || true",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "85:             \"${AIRFLOW_INSTALLATION_METHOD}[${AIRFLOW_EXTRAS}]${AIRFLOW_VERSION_SPECIFICATION}\" \\",
          "86:             --constraint \"${AIRFLOW_CONSTRAINTS_LOCATION}\"",
          "87:         common::install_pip_version",
          "89:         pip install --root-user-action ignore --upgrade --upgrade-strategy only-if-needed \\",
          "90:             ${ADDITIONAL_PIP_INSTALL_FLAGS} \\",
          "91:             ${AIRFLOW_INSTALL_EDITABLE_FLAG} \\",
          "",
          "[Removed Lines]",
          "88:         # then upgrade if needed without using constraints to account for new limits in setup.py",
          "",
          "[Added Lines]",
          "89:         # then upgrade if needed without using constraints to account for new limits in pyproject.toml",
          "",
          "---------------"
        ],
        "scripts/docker/install_airflow_dependencies_from_branch_tip.sh||scripts/docker/install_airflow_dependencies_from_branch_tip.sh": [
          "File: scripts/docker/install_airflow_dependencies_from_branch_tip.sh -> scripts/docker/install_airflow_dependencies_from_branch_tip.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: # shellcheck shell=bash disable=SC2086",
          "20: # Installs Airflow from $AIRFLOW_BRANCH tip. This is pure optimisation. It is done because we do not want",
          "22: # when a file is changed, when added to docker context, it invalidates the cache and it causes Docker",
          "23: # build to reinstall all dependencies from scratch. This can take a loooooot of time. Therefore we install",
          "24: # the dependencies first from main (and uninstall airflow right after) so that we can start installing",
          "26: #",
          "27: # If INSTALL_MYSQL_CLIENT is set to false, mysql extra is removed",
          "28: # If INSTALL_POSTGRES_CLIENT is set to false, postgres extra is removed",
          "",
          "[Removed Lines]",
          "21: # to reinstall all dependencies from scratch when setup.py changes. Problem with Docker caching is that",
          "25: # deps from those pre-installed dependencies. It saves few minutes of build time when setup.py changes.",
          "",
          "[Added Lines]",
          "21: # to reinstall all dependencies from scratch when pyproject.toml changes. Problem with Docker caching is that",
          "25: # deps from those pre-installed dependencies. It saves few minutes of build time when pyproject.toml changes.",
          "",
          "---------------"
        ],
        "scripts/in_container/run_generate_constraints.py||scripts/in_container/run_generate_constraints.py": [
          "File: scripts/in_container/run_generate_constraints.py -> scripts/in_container/run_generate_constraints.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: import os",
          "21: import sys",
          "22: from dataclasses import dataclass",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import json",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "32: AIRFLOW_SOURCES = Path(__file__).resolve().parents[2]",
          "33: DEFAULT_BRANCH = os.environ.get(\"DEFAULT_BRANCH\", \"main\")",
          "34: PYTHON_VERSION = os.environ.get(\"PYTHON_MAJOR_MINOR_VERSION\", \"3.8\")",
          "36: now = datetime.now().isoformat()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "36: GENERATED_PROVIDER_DEPENDENCIES_FILE = AIRFLOW_SOURCES / \"generated\" / \"provider_dependencies.json\"",
          "38: ALL_PROVIDER_DEPENDENCIES = json.loads(GENERATED_PROVIDER_DEPENDENCIES_FILE.read_text())",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "127: def install_local_airflow_with_eager_upgrade(",
          "129: ) -> None:",
          "130:     run_command(",
          "131:         [",
          "",
          "[Removed Lines]",
          "128:     config_params: ConfigParams, eager_upgrade_additional_requirements: str, extras: list[str]",
          "",
          "[Added Lines]",
          "132:     config_params: ConfigParams, eager_upgrade_additional_requirements: str",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "133:             \"install\",",
          "134:             \"--root-user-action\",",
          "135:             \"ignore\",",
          "138:             \"--upgrade\",",
          "139:             \"--upgrade-strategy\",",
          "",
          "[Removed Lines]",
          "136:             f\".[{','.join(extras)}]\",",
          "",
          "[Added Lines]",
          "140:             \"-e\",",
          "141:             \".[all-core]\",",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "254:     )",
          "269: def generate_constraints_source_providers(config_params: ConfigParams) -> None:",
          "",
          "[Removed Lines]",
          "257: def get_core_airflow_dependencies() -> list[str]:",
          "258:     import setup",
          "260:     return list(setup.CORE_EXTRAS_DEPENDENCIES.keys())",
          "263: def get_all_provider_packages() -> list[str]:",
          "264:     import setup",
          "266:     return setup.get_all_provider_packages().split(\" \")",
          "",
          "[Added Lines]",
          "262: def get_all_active_provider_packages() -> list[str]:",
          "263:     return [",
          "264:         f\"apache-airflow-providers-{provider.replace('.','-')}\"",
          "265:         for provider in ALL_PROVIDER_DEPENDENCIES.keys()",
          "266:         if ALL_PROVIDER_DEPENDENCIES[provider][\"state\"] == \"ready\"",
          "267:     ]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "288:     :return:",
          "289:     \"\"\"",
          "290:     dist_dir = Path(\"/dist\")",
          "293:     chicken_egg_prefixes = []",
          "294:     packages_to_install = []",
          "295:     console.print(\"[bright_blue]Installing Airflow with PyPI providers with eager upgrade\")",
          "",
          "[Removed Lines]",
          "291:     core_dependencies = get_core_airflow_dependencies()",
          "292:     all_provider_packages = get_all_provider_packages()",
          "",
          "[Added Lines]",
          "292:     all_provider_packages = get_all_active_provider_packages()",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "332:             \"install\",",
          "333:             \"--root-user-action\",",
          "334:             \"ignore\",",
          "338:             \"--upgrade\",",
          "",
          "[Removed Lines]",
          "335:             f\".[{','.join(core_dependencies)}]\",",
          "",
          "[Added Lines]",
          "335:             \".[all-core]\",",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "355:     Generates constraints without any provider dependencies. This is used mostly to generate SBOM",
          "356:     files - where we generate list of dependencies for Airflow without any provider installed.",
          "357:     \"\"\"",
          "359:     uninstall_all_packages(config_params)",
          "360:     console.print(",
          "362:     )",
          "363:     install_local_airflow_with_eager_upgrade(",
          "365:     )",
          "367:     with config_params.current_constraints_file.open(\"w\") as constraints_file:",
          "368:         constraints_file.write(NO_PROVIDERS_CONSTRAINTS_PREFIX)",
          "369:         freeze_packages_to_file(config_params, constraints_file)",
          "",
          "[Removed Lines]",
          "358:     core_dependencies = get_core_airflow_dependencies()",
          "361:         f\"[bright_blue]Installing airflow with [{core_dependencies}] extras only \" f\"with eager upgrade.\"",
          "364:         config_params, config_params.eager_upgrade_additional_requirements, core_dependencies",
          "366:     console.print(f\"[success]Installed airflow with [{core_dependencies}] extras only with eager upgrade.\")",
          "",
          "[Added Lines]",
          "360:         \"[bright_blue]Installing airflow with [all-core] extras only with eager upgrade in \"",
          "361:         \"installable mode.\"",
          "364:         config_params, config_params.eager_upgrade_additional_requirements",
          "366:     console.print(\"[success]Installed airflow with [all-core] extras only with eager upgrade.\")",
          "",
          "---------------"
        ],
        "scripts/in_container/run_prepare_airflow_packages.py||scripts/in_container/run_prepare_airflow_packages.py": [
          "File: scripts/in_container/run_prepare_airflow_packages.py -> scripts/in_container/run_prepare_airflow_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import re",
          "23: import subprocess",
          "24: import sys",
          "25: from pathlib import Path",
          "26: from shutil import rmtree",
          "44: AIRFLOW_SOURCES_ROOT = Path(__file__).parents[2].resolve()",
          "45: WWW_DIRECTORY = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"www\"",
          "114: for file in (AIRFLOW_SOURCES_ROOT / \"dist\").glob(\"apache*\"):",
          "",
          "[Removed Lines]",
          "28: import rich",
          "31: def process_summary(success_message: str, error_message: str, completed_process: subprocess.CompletedProcess):",
          "32:     if completed_process.returncode != 0:",
          "33:         if os.environ.get(\"GITHUB_ACTIONS\", \"\") != \"\":",
          "34:             print(\"::endgroup::\")",
          "35:             print(f\"::error::{error_message}\")",
          "36:         rich.print(f\"[red]{error_message}\")",
          "37:         rich.print(completed_process.stdout)",
          "38:         rich.print(completed_process.stderr)",
          "39:         sys.exit(completed_process.returncode)",
          "40:     else:",
          "41:         rich.print(f\"[green]{success_message}\")",
          "47: rich.print(\"[bright_blue]Cleaning build directories\\n\")",
          "49: for egg_info_file in AIRFLOW_SOURCES_ROOT.glob(\"*egg-info*\"):",
          "50:     rmtree(egg_info_file, ignore_errors=True)",
          "52: rmtree(AIRFLOW_SOURCES_ROOT / \"build\", ignore_errors=True)",
          "54: rich.print(\"[green]Cleaned build directories\\n\\n\")",
          "56: version_suffix = os.environ.get(\"VERSION_SUFFIX_FOR_PYPI\", \"\")",
          "57: package_format = os.environ.get(\"PACKAGE_FORMAT\", \"wheel\")",
          "59: rich.print(f\"[bright_blue]Marking {AIRFLOW_SOURCES_ROOT} as safe directory for git commands.\\n\")",
          "61: subprocess.run(",
          "62:     [\"git\", \"config\", \"--global\", \"--unset-all\", \"safe.directory\"],",
          "63:     cwd=AIRFLOW_SOURCES_ROOT,",
          "64:     stdout=subprocess.DEVNULL,",
          "65:     stderr=subprocess.DEVNULL,",
          "66:     check=False,",
          "67: )",
          "69: subprocess.run(",
          "70:     [\"git\", \"config\", \"--global\", \"--add\", \"safe.directory\", AIRFLOW_SOURCES_ROOT],",
          "71:     cwd=AIRFLOW_SOURCES_ROOT,",
          "72:     stdout=subprocess.DEVNULL,",
          "73:     stderr=subprocess.DEVNULL,",
          "74:     check=True,",
          "75: )",
          "77: rich.print(f\"[green]Marked {AIRFLOW_SOURCES_ROOT} as safe directory for git commands.\\n\")",
          "79: rich.print(\"[bright_blue]Checking airflow version\\n\")",
          "81: airflow_version = subprocess.check_output(",
          "82:     [sys.executable, \"setup.py\", \"--version\"], text=True, cwd=AIRFLOW_SOURCES_ROOT",
          "83: ).strip()",
          "85: rich.print(f\"[green]Airflow version: {airflow_version}\\n\")",
          "87: RELEASED_VERSION_MATCHER = re.compile(r\"^\\d+\\.\\d+\\.\\d+$\")",
          "89: command = [sys.executable, \"setup.py\"]",
          "91: if version_suffix:",
          "92:     if RELEASED_VERSION_MATCHER.match(airflow_version):",
          "93:         rich.print(f\"[warning]Adding {version_suffix} suffix to the {airflow_version}\")",
          "94:         command.extend([\"egg_info\", \"--tag-build\", version_suffix])",
          "95:     elif not airflow_version.endswith(version_suffix):",
          "96:         rich.print(f\"[red]Version {airflow_version} does not end with {version_suffix}. Using !\")",
          "97:         sys.exit(1)",
          "99: if package_format in [\"both\", \"wheel\"]:",
          "100:     command.append(\"bdist_wheel\")",
          "101: if package_format in [\"both\", \"sdist\"]:",
          "102:     command.append(\"sdist\")",
          "104: rich.print(f\"[bright_blue]Building packages: {package_format}\\n\")",
          "106: process = subprocess.run(command, capture_output=True, text=True, cwd=AIRFLOW_SOURCES_ROOT)",
          "108: process_summary(\"Airflow packages built successfully\", \"Error building Airflow packages\", process)",
          "110: if os.environ.get(\"GITHUB_ACTIONS\", \"\") != \"\":",
          "111:     print(\"::endgroup::\")",
          "113: rich.print(\"[green]Packages built successfully:\\n\")",
          "115:     rich.print(file.name)",
          "116: rich.print()",
          "",
          "[Added Lines]",
          "25: from contextlib import contextmanager",
          "29: from rich.console import Console",
          "31: console = Console(color_system=\"standard\", width=200)",
          "34: AIRFLOW_INIT_FILE = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"__init__.py\"",
          "36: VERSION_SUFFIX = os.environ.get(\"VERSION_SUFFIX_FOR_PYPI\", \"\")",
          "37: PACKAGE_FORMAT = os.environ.get(\"PACKAGE_FORMAT\", \"wheel\")",
          "40: def clean_build_directory():",
          "41:     console.print(\"[bright_blue]Cleaning build directories\\n\")",
          "42:     for egg_info_file in AIRFLOW_SOURCES_ROOT.glob(\"*egg-info*\"):",
          "43:         rmtree(egg_info_file, ignore_errors=True)",
          "44:     rmtree(AIRFLOW_SOURCES_ROOT / \"build\", ignore_errors=True)",
          "45:     console.print(\"[green]Cleaned build directories\\n\\n\")",
          "48: def mark_git_directory_as_safe():",
          "49:     console.print(f\"[bright_blue]Marking {AIRFLOW_SOURCES_ROOT} as safe directory for git commands.\\n\")",
          "50:     subprocess.run(",
          "51:         [\"git\", \"config\", \"--global\", \"--unset-all\", \"safe.directory\"],",
          "52:         cwd=AIRFLOW_SOURCES_ROOT,",
          "53:         stdout=subprocess.DEVNULL,",
          "54:         stderr=subprocess.DEVNULL,",
          "55:         check=False,",
          "56:     )",
          "57:     subprocess.run(",
          "58:         [\"git\", \"config\", \"--global\", \"--add\", \"safe.directory\", AIRFLOW_SOURCES_ROOT],",
          "59:         cwd=AIRFLOW_SOURCES_ROOT,",
          "60:         stdout=subprocess.DEVNULL,",
          "61:         stderr=subprocess.DEVNULL,",
          "62:         check=False,",
          "63:     )",
          "64:     console.print(f\"[green]Marked {AIRFLOW_SOURCES_ROOT} as safe directory for git commands.\\n\")",
          "67: def get_current_airflow_version() -> str:",
          "68:     console.print(\"[bright_blue]Checking airflow version\\n\")",
          "69:     airflow_version = subprocess.check_output(",
          "70:         [sys.executable, \"-m\", \"hatch\", \"version\"], text=True, cwd=AIRFLOW_SOURCES_ROOT",
          "71:     ).strip()",
          "72:     console.print(f\"[green]Airflow version: {airflow_version}\\n\")",
          "73:     return airflow_version",
          "76: def build_airflow_packages(package_format: str):",
          "77:     build_command = [sys.executable, \"-m\", \"hatch\", \"build\", \"-t\", \"custom\"]",
          "79:     if package_format in [\"both\", \"wheel\"]:",
          "80:         build_command.extend([\"-t\", \"wheel\"])",
          "81:     if package_format in [\"both\", \"sdist\"]:",
          "82:         build_command.extend([\"-t\", \"sdist\"])",
          "84:     console.print(f\"[bright_blue]Building packages: {package_format}\\n\")",
          "85:     build_process = subprocess.run(build_command, capture_output=False, cwd=AIRFLOW_SOURCES_ROOT)",
          "87:     if build_process.returncode != 0:",
          "88:         console.print(\"[red]Error building Airflow packages\")",
          "89:         sys.exit(build_process.returncode)",
          "90:     else:",
          "91:         console.print(\"[green]Airflow packages built successfully\")",
          "94: def set_package_version(version: str) -> None:",
          "95:     console.print(f\"\\n[yellow]Setting {version} for Airflow package\\n\")",
          "96:     # replace __version__ with the version passed as argument in python",
          "97:     init_content = AIRFLOW_INIT_FILE.read_text()",
          "98:     init_content = re.sub(r'__version__ = \"[^\"]+\"', f'__version__ = \"{version}\"', init_content)",
          "99:     AIRFLOW_INIT_FILE.write_text(init_content)",
          "102: @contextmanager",
          "103: def package_version(version_suffix: str):",
          "104:     release_version_matcher = re.compile(r\"^\\d+\\.\\d+\\.\\d+$\")",
          "105:     airflow_version = get_current_airflow_version()",
          "107:     update_version = False",
          "108:     if version_suffix:",
          "109:         if airflow_version.endswith(f\".{version_suffix}\"):",
          "110:             console.print(",
          "111:                 f\"[bright_blue]The {airflow_version} already has suffix {version_suffix}. Not updating it.\\n\"",
          "112:             )",
          "113:         elif not release_version_matcher.match(airflow_version):",
          "114:             console.print(",
          "115:                 f\"[red]You should only pass version suffix if {airflow_version}\"",
          "116:                 f\"does not have suffix in code. The version in code is: {airflow_version}.\\n\"",
          "117:             )",
          "118:             console.print(",
          "119:                 \"[yellow]Make sure that you remove the suffix before using `--version-suffix-for-pypi`!\"",
          "120:             )",
          "121:             sys.exit(1)",
          "122:         else:",
          "123:             update_version = True",
          "124:     if update_version:",
          "125:         set_package_version(f\"{airflow_version}.{version_suffix}\")",
          "126:     try:",
          "127:         yield",
          "128:     finally:",
          "129:         # Set the version back to the original version",
          "130:         if update_version:",
          "131:             set_package_version(airflow_version)",
          "134: clean_build_directory()",
          "135: mark_git_directory_as_safe()",
          "137: with package_version(VERSION_SUFFIX):",
          "138:     build_airflow_packages(PACKAGE_FORMAT)",
          "141:     console.print(file.name)",
          "142: console.print()",
          "",
          "---------------"
        ],
        "scripts/in_container/verify_providers.py||scripts/in_container/verify_providers.py": [
          "File: scripts/in_container/verify_providers.py -> scripts/in_container/verify_providers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import importlib",
          "21: import logging",
          "22: import os",
          "23: import pkgutil",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import json",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "41: AIRFLOW_SOURCES_ROOT = Path(__file__).parents[2].resolve()",
          "42: PROVIDERS_PATH = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"providers\"",
          "44: USE_AIRFLOW_VERSION = os.environ.get(\"USE_AIRFLOW_VERSION\") or \"\"",
          "45: IS_AIRFLOW_VERSION_PROVIDED = re.match(\"^(\\d+)\\.(\\d+)\\.(\\d+)\\S*$\", USE_AIRFLOW_VERSION)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "44: GENERATED_PROVIDERS_DEPENDENCIES_FILE = AIRFLOW_SOURCES_ROOT / \"generated\" / \"provider_dependencies.json\"",
          "45: ALL_DEPENDENCIES = json.loads(GENERATED_PROVIDERS_DEPENDENCIES_FILE.read_text())",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "131: def get_all_providers() -> list[str]:",
          "141: def import_all_classes(",
          "",
          "[Removed Lines]",
          "132:     \"\"\"Returns all providers for regular packages.",
          "134:     :return: list of providers that are considered for provider packages",
          "135:     \"\"\"",
          "136:     from setup import ALL_PROVIDERS",
          "138:     return list(ALL_PROVIDERS)",
          "",
          "[Added Lines]",
          "135:     return list(ALL_DEPENDENCIES.keys())",
          "",
          "---------------"
        ],
        "scripts/tools/initialize_virtualenv.py||scripts/tools/initialize_virtualenv.py": [
          "File: scripts/tools/initialize_virtualenv.py -> scripts/tools/initialize_virtualenv.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "75: You can specify extras as single coma-separated parameter to install. For example",
          "81: which might not be possible to install cleanly on your host because of lack of",
          "82: system packages. It's easier to install extras one-by-one as needed.",
          "",
          "[Removed Lines]",
          "80: Note that \"devel_all\" installs all possible dependencies and we have > 600 of them,",
          "",
          "[Added Lines]",
          "83: Note that \"devel-all\" installs all possible dependencies and we have > 600 of them,",
          "",
          "---------------"
        ],
        "setup.cfg||setup.cfg": [
          "File: setup.cfg -> setup.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/cli/commands/test_task_command.py||tests/cli/commands/test_task_command.py": [
          "File: tests/cli/commands/test_task_command.py -> tests/cli/commands/test_task_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "49: from airflow.utils.session import create_session",
          "50: from airflow.utils.state import State",
          "51: from airflow.utils.types import DagRunType",
          "53: from tests.test_utils.config import conf_vars",
          "54: from tests.test_utils.db import clear_db_pools, clear_db_runs",
          "",
          "[Removed Lines]",
          "52: from setup import AIRFLOW_SOURCES_ROOT",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "60:     from airflow.models.dag import DAG",
          "62: DEFAULT_DATE = timezone.datetime(2022, 1, 1)",
          "68: def reset(dag_id):",
          "",
          "[Removed Lines]",
          "63: ROOT_FOLDER = os.path.realpath(",
          "64:     os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir, os.pardir)",
          "65: )",
          "",
          "[Added Lines]",
          "62: ROOT_FOLDER = Path(__file__).parents[3].resolve()",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "755:             \"os.environ\",",
          "756:             AIRFLOW_IS_K8S_EXECUTOR_POD=is_k8s,",
          "757:             AIRFLOW_IS_EXECUTOR_CONTAINER=is_container_exec,",
          "759:         ):",
          "760:             with subprocess.Popen(",
          "761:                 args=[sys.executable, \"-m\", \"airflow\", *self.task_args, \"-S\", self.dag_path],",
          "",
          "[Removed Lines]",
          "758:             PYTHONPATH=os.fspath(AIRFLOW_SOURCES_ROOT),",
          "",
          "[Added Lines]",
          "755:             PYTHONPATH=os.fspath(ROOT_FOLDER),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "763:                 stderr=subprocess.PIPE,",
          "764:             ) as process:",
          "765:                 output, err = process.communicate()",
          "766:         lines = []",
          "767:         found_start = False",
          "768:         for line_ in output.splitlines():",
          "769:             line = line_.decode(\"utf-8\")",
          "770:             if \"Running <TaskInstance: test_logging_dag.test_task test_run\" in line:",
          "771:                 found_start = True",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "763:         if err:",
          "764:             print(err.decode(\"utf-8\"))",
          "768:             print(line_.decode(\"utf-8\"))",
          "",
          "---------------"
        ],
        "tests/plugins/test_plugins_manager.py||tests/plugins/test_plugins_manager.py": [
          "File: tests/plugins/test_plugins_manager.py -> tests/plugins/test_plugins_manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "32: from airflow.plugins_manager import AirflowPlugin",
          "33: from airflow.utils.module_loading import qualname",
          "34: from airflow.www import app as application",
          "36: from tests.test_utils.config import conf_vars",
          "37: from tests.test_utils.mock_plugins import mock_plugin_manager",
          "",
          "[Removed Lines]",
          "35: from setup import AIRFLOW_SOURCES_ROOT",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "41: importlib_metadata_string = \"importlib_metadata\"",
          "43: try:",
          "44:     import importlib_metadata",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42: AIRFLOW_SOURCES_ROOT = Path(__file__).parents[2].resolve()",
          "",
          "---------------"
        ],
        "tests/providers/google/cloud/utils/base_gcp_mock.py||tests/providers/google/cloud/utils/base_gcp_mock.py": [
          "File: tests/providers/google/cloud/utils/base_gcp_mock.py -> tests/providers/google/cloud/utils/base_gcp_mock.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "32:     impersonation_chain=None,",
          "33:     delegate_to=None,",
          "34: ):",
          "36:     self._conn = gcp_conn_id",
          "37:     self.impersonation_chain = impersonation_chain",
          "38:     self._client = None",
          "",
          "[Removed Lines]",
          "35:     self.extras_list = {\"project\": GCP_PROJECT_ID_HOOK_UNIT_TEST}",
          "",
          "[Added Lines]",
          "35:     self.standard_extras_list = {\"project\": GCP_PROJECT_ID_HOOK_UNIT_TEST}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "48:     impersonation_chain=None,",
          "49:     delegate_to=None,",
          "50: ):",
          "52:     self._conn = gcp_conn_id",
          "53:     self.impersonation_chain = impersonation_chain",
          "54:     self._client = None",
          "",
          "[Removed Lines]",
          "51:     self.extras_list = {}",
          "",
          "[Added Lines]",
          "51:     self.standard_extras_list = {}",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "80b19d7951d77f99661ace5ac2fb2abe24d18eca",
      "candidate_info": {
        "commit_hash": "80b19d7951d77f99661ace5ac2fb2abe24d18eca",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/80b19d7951d77f99661ace5ac2fb2abe24d18eca",
        "files": [
          "docs/apache-airflow/installation/dependencies.rst"
        ],
        "message": "Fix copy&paste victim for installation instructions (#36572)\n\nThe #36521 had a copy&paste mistake with missing `c`\n\n(cherry picked from commit 2c15dc9dd1b360bcb3b6d6c2e41e50ef0d97801e)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "6624adeb01106d73f7c933c4c99edb48d8795e43",
      "candidate_info": {
        "commit_hash": "6624adeb01106d73f7c933c4c99edb48d8795e43",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/6624adeb01106d73f7c933c4c99edb48d8795e43",
        "files": [
          "airflow/api_connexion/endpoints/variable_endpoint.py",
          "tests/api_connexion/endpoints/test_variable_endpoint.py"
        ],
        "message": "Fix the required access for get_variable endpoint (#36396)\n\n(cherry picked from commit 34132e37c691995ff94dd1b8518d1f5c3a2ec998)",
        "before_after_code_files": [
          "airflow/api_connexion/endpoints/variable_endpoint.py||airflow/api_connexion/endpoints/variable_endpoint.py",
          "tests/api_connexion/endpoints/test_variable_endpoint.py||tests/api_connexion/endpoints/test_variable_endpoint.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api_connexion/endpoints/variable_endpoint.py||airflow/api_connexion/endpoints/variable_endpoint.py": [
          "File: airflow/api_connexion/endpoints/variable_endpoint.py -> airflow/api_connexion/endpoints/variable_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "57:     return Response(status=HTTPStatus.NO_CONTENT)",
          "61: @provide_session",
          "62: def get_variable(*, variable_key: str, session: Session = NEW_SESSION) -> Response:",
          "63:     \"\"\"Get a variable by key.\"\"\"",
          "",
          "[Removed Lines]",
          "60: @security.requires_access_variable(\"DELETE\")",
          "",
          "[Added Lines]",
          "60: @security.requires_access_variable(\"GET\")",
          "",
          "---------------"
        ],
        "tests/api_connexion/endpoints/test_variable_endpoint.py||tests/api_connexion/endpoints/test_variable_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_variable_endpoint.py -> tests/api_connexion/endpoints/test_variable_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "46:             (permissions.ACTION_CAN_DELETE, permissions.RESOURCE_VARIABLE),",
          "47:         ],",
          "48:     )",
          "49:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
          "51:     yield app",
          "53:     delete_user(app, username=\"test\")  # type: ignore",
          "54:     delete_user(app, username=\"test_no_permissions\")  # type: ignore",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "49:     create_user(",
          "50:         app,  # type: ignore",
          "51:         username=\"test_read_only\",",
          "52:         role_name=\"TestReadOnly\",",
          "53:         permissions=[",
          "54:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_VARIABLE),",
          "55:         ],",
          "56:     )",
          "57:     create_user(",
          "58:         app,  # type: ignore",
          "59:         username=\"test_delete_only\",",
          "60:         role_name=\"TestDeleteOnly\",",
          "61:         permissions=[",
          "62:             (permissions.ACTION_CAN_DELETE, permissions.RESOURCE_VARIABLE),",
          "63:         ],",
          "64:     )",
          "70:     delete_user(app, username=\"test_read_only\")  # type: ignore",
          "71:     delete_user(app, username=\"test_delete_only\")  # type: ignore",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "111: class TestGetVariable(TestVariableEndpoint):",
          "113:         expected_value = '{\"foo\": 1}'",
          "114:         Variable.set(\"TEST_VARIABLE_KEY\", expected_value)",
          "115:         response = self.client.get(",
          "117:         )",
          "121:     def test_should_respond_404_if_not_found(self):",
          "122:         response = self.client.get(",
          "",
          "[Removed Lines]",
          "112:     def test_should_respond_200(self):",
          "116:             \"/api/v1/variables/TEST_VARIABLE_KEY\", environ_overrides={\"REMOTE_USER\": \"test\"}",
          "118:         assert response.status_code == 200",
          "119:         assert response.json == {\"key\": \"TEST_VARIABLE_KEY\", \"value\": expected_value, \"description\": None}",
          "",
          "[Added Lines]",
          "130:     @pytest.mark.parametrize(",
          "131:         \"user, expected_status_code\",",
          "132:         [",
          "133:             (\"test\", 200),",
          "134:             (\"test_read_only\", 200),",
          "135:             (\"test_delete_only\", 403),",
          "136:             (\"test_no_permissions\", 403),",
          "137:         ],",
          "138:     )",
          "139:     def test_read_variable(self, user, expected_status_code):",
          "143:             \"/api/v1/variables/TEST_VARIABLE_KEY\", environ_overrides={\"REMOTE_USER\": user}",
          "145:         assert response.status_code == expected_status_code",
          "146:         if expected_status_code == 200:",
          "147:             assert response.json == {\"key\": \"TEST_VARIABLE_KEY\", \"value\": expected_value, \"description\": None}",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "fef77f5a2c0a49ac5536c6f9c4e11ca4d001f3ae",
      "candidate_info": {
        "commit_hash": "fef77f5a2c0a49ac5536c6f9c4e11ca4d001f3ae",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/fef77f5a2c0a49ac5536c6f9c4e11ca4d001f3ae",
        "files": [
          "airflow/www/decorators.py",
          "tests/www/views/test_views_paused.py"
        ],
        "message": "Bugfix/logging for pausing (#36182)\n\n---------\n\nCo-authored-by: Aleph Melo <alephmelo@icloud.com>\n(cherry picked from commit c884f3ce3250bb9dd58cf3dd8dde7c2555e664a5)",
        "before_after_code_files": [
          "airflow/www/decorators.py||airflow/www/decorators.py",
          "tests/www/views/test_views_paused.py||tests/www/views/test_views_paused.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/decorators.py||airflow/www/decorators.py": [
          "File: airflow/www/decorators.py -> airflow/www/decorators.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "93:                     user = get_auth_manager().get_user_name()",
          "94:                     user_display = get_auth_manager().get_user_display_name()",
          "97:                 extra_fields = [",
          "98:                     (k, secrets_masker.redact(v, k))",
          "99:                     for k, v in itertools.chain(request.values.items(multi=True), request.view_args.items())",
          "",
          "[Removed Lines]",
          "96:                 fields_skip_logging = {\"csrf_token\", \"_csrf_token\"}",
          "",
          "[Added Lines]",
          "96:                 fields_skip_logging = {\"csrf_token\", \"_csrf_token\", \"is_paused\"}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "107:                 params = {**request.values, **request.view_args}",
          "109:                 log = Log(",
          "110:                     event=event or f.__name__,",
          "111:                     task_instance=None,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "109:                 if params and \"is_paused\" in params:",
          "110:                     extra_fields.append((\"is_paused\", params[\"is_paused\"] == \"false\"))",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_paused.py||tests/www/views/test_views_paused.py": [
          "File: tests/www/views/test_views_paused.py -> tests/www/views/test_views_paused.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "19: import pytest",
          "21: from airflow.models.log import Log",
          "22: from tests.test_utils.db import clear_db_dags",
          "24: pytestmark = pytest.mark.db_test",
          "27: @pytest.fixture(autouse=True)",
          "28: def dags(create_dummy_dag):",
          "29:     paused_dag, _ = create_dummy_dag(dag_id=\"paused_dag\", is_paused_upon_creation=True)",
          "30:     dag, _ = create_dummy_dag(dag_id=\"unpaused_dag\")",
          "32:     yield dag, paused_dag",
          "34:     clear_db_dags()",
          "37: def test_logging_pause_dag(admin_client, dags, session):",
          "38:     dag, _ = dags",
          "39:     # is_paused=false mean pause the dag",
          "40:     admin_client.post(f\"/paused?is_paused=false&dag_id={dag.dag_id}\", follow_redirects=True)",
          "41:     dag_query = session.query(Log).filter(Log.dag_id == dag.dag_id)",
          "42:     assert \"('is_paused', True)\" in dag_query.first().extra",
          "45: def test_logging_unpuase_dag(admin_client, dags, session):",
          "46:     _, paused_dag = dags",
          "47:     # is_paused=true mean unpause the dag",
          "48:     admin_client.post(f\"/paused?is_paused=true&dag_id={paused_dag.dag_id}\", follow_redirects=True)",
          "49:     dag_query = session.query(Log).filter(Log.dag_id == paused_dag.dag_id)",
          "50:     assert \"('is_paused', False)\" in dag_query.first().extra",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b9447b1e086a989ca3dffac599654e5ffcba0e44",
      "candidate_info": {
        "commit_hash": "b9447b1e086a989ca3dffac599654e5ffcba0e44",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b9447b1e086a989ca3dffac599654e5ffcba0e44",
        "files": [
          "airflow/www/templates/airflow/traceback.html"
        ],
        "message": "Improve the error message displayed when there is a webserver error (#36570)\n\nThe error message displayed when errors happen in the webserver did\nnot properly communicated, that the user MUST look at the logs and\nprovide more information to investigate the root cause (after looking\nat the logs themselves). We have a number of users just copy&pasting\nthe generic error message without understanding that this is not nearly\nenough to help them.\n\nThis PR proposes a bit more explicit call to look at the logs and\nexplains why details are not displayed (for security reasons).\n\n(cherry picked from commit 574f86ee1e323d2a1613284f4bb77b6c4a3d3d0a)",
        "before_after_code_files": [
          "airflow/www/templates/airflow/traceback.html||airflow/www/templates/airflow/traceback.html"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/templates/airflow/traceback.html||airflow/www/templates/airflow/traceback.html": [
          "File: airflow/www/templates/airflow/traceback.html -> airflow/www/templates/airflow/traceback.html",
          "--- Hunk 1 ---",
          "[Context before]",
          "27:       <h1> Ooops! </h1>",
          "28:       <div>",
          "29:           <pre>",
          "50: Python version: {{ python_version }}",
          "51: Airflow version: {{ airflow_version }}",
          "",
          "[Removed Lines]",
          "30: Something bad has happened.",
          "32: Airflow is used by many users, and it is very likely that others had similar problems and you can easily find",
          "33: a solution to your problem.",
          "35: Consider following these steps:",
          "48:     Make sure however, to include all relevant details and results of your investigation so far.",
          "",
          "[Added Lines]",
          "30: Something bad has happened. For security reasons detailed information about the error is not logged.",
          "41:     All those resources might help you to find a solution to your problem.",
          "46:     get the logs with errors, describe results of your investigation so far, and consider creating a",
          "47:     <b><a href=\"https://github.com/apache/airflow/issues/new/choose\">bug report</a></b> including this information.",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b9ef5a089837e4d911c7a80d4ab5449946fbb6d8",
      "candidate_info": {
        "commit_hash": "b9ef5a089837e4d911c7a80d4ab5449946fbb6d8",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b9ef5a089837e4d911c7a80d4ab5449946fbb6d8",
        "files": [
          "dev/README_RELEASE_PROVIDER_PACKAGES.md",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2",
          "images/breeze/output_release-management_generate-issue-content-providers.svg",
          "images/breeze/output_release-management_generate-issue-content-providers.txt"
        ],
        "message": "Improve generation of \"Status of testing\" issue (#36470)\n\nThis PR improves/simplifies the process of issue generation when\nprovider package rc candidates are prepared for voting.\n\nIt improves the commmand to generate the issue and makes it simpler\n(less copy&paste) to create such issue, the issue also does not use\nthe \"Meta\" template and gets the right labels assigned automatically.\n\nRecent changes that automatically derive the suffix from PyPI packages\nprepared, removed the need of passing `--suffix` as parameter. In all\ncases the right rc* suffix will be automatically added during issue\ngeneration based on the version of package being prepared. The process\nhas been updated and command simplified by removing the `--suffix` flag.\n\nWhen the issue is prepared, we display the issue in terminal and asked\nthe release manager to create the issue by copy&pasting the issue\ncontent and title to a new issue, but that required a few copy&pastes\nand opening new Issue via \"Meta\" task type. This PR simplifies it a\nbit by not only displaying the content but also generating a URL that\ncan be either copy&pasted to browser URL field or just Cmd+clicked\nif your terminal allows that. Issue created this way does not have\nthe \"Body\" field header and has the labels properly assigned including\na dedicated \"testing status\" label that is used to gether stats for\npast \"status\" issues.\n\nThe advice for release manager has been improved (the comment generated\nhad some missing end of sentence and it should be now clearer on how\nto iterate during issue generation if you want to remove some PRs from\nthe generated issue content.\n\n(cherry picked from commit 5d88f6f9a4c7140c1da9db47aab7caf2d4c5f453)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2||dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: from datetime import datetime",
          "30: from pathlib import Path",
          "31: from subprocess import DEVNULL",
          "34: import click",
          "35: from rich.progress import Progress",
          "",
          "[Removed Lines]",
          "32: from typing import IO, TYPE_CHECKING, Any, Generator, NamedTuple",
          "",
          "[Added Lines]",
          "32: from typing import IO, TYPE_CHECKING, Any, Generator, Iterable, NamedTuple",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1571:     return prs",
          "1574: @release_management.command(",
          "1575:     name=\"generate-issue-content-providers\", help=\"Generates content for issue to test the release.\"",
          "1576: )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1574: def create_github_issue_url(title: str, body: str, labels: Iterable[str]) -> str:",
          "1575:     \"\"\"",
          "1576:     Creates URL to create the issue with title, body and labels.",
          "1577:     :param title: issue title",
          "1578:     :param body: issue body",
          "1579:     :param labels: labels for the issue",
          "1580:     :return: URL to use to create the issue",
          "1581:     \"\"\"",
          "1582:     from urllib.parse import quote",
          "1584:     quoted_labels = quote(\",\".join(labels))",
          "1585:     quoted_title = quote(title)",
          "1586:     quoted_body = quote(body)",
          "1587:     return (",
          "1588:         f\"https://github.com/apache/airflow/issues/new?labels={quoted_labels}&\"",
          "1589:         f\"title={quoted_title}&body={quoted_body}\"",
          "1590:     )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1592:     is_flag=True,",
          "1593:     help=\"Only consider package ids with packages prepared in the dist folder\",",
          "1594: )",
          "1596: @argument_provider_packages",
          "1597: def generate_issue_content_providers(",
          "1598:     disable_progress: bool,",
          "1599:     excluded_pr_list: str,",
          "1600:     github_token: str,",
          "1601:     only_available_in_dist: bool,",
          "1603:     provider_packages: list[str],",
          "1604: ):",
          "1605:     import jinja2",
          "",
          "[Removed Lines]",
          "1595: @click.option(\"--suffix\", default=\"rc1\", help=\"Suffix to add to the version prepared\")",
          "1602:     suffix: str,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1674:                     provider_package_id=provider_id,",
          "1675:                     pypi_package_name=provider_yaml_dict[\"package-name\"],",
          "1676:                     pr_list=pull_request_list,",
          "1678:                 )",
          "1679:         template = jinja2.Template(",
          "1680:             (Path(__file__).parents[1] / \"provider_issue_TEMPLATE.md.jinja2\").read_text()",
          "",
          "[Removed Lines]",
          "1677:                     suffix=package_suffix if package_suffix else suffix,",
          "",
          "[Added Lines]",
          "1694:                     suffix=package_suffix if package_suffix else \"\",",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1688:         get_console().print()",
          "1689:         get_console().print()",
          "1690:         get_console().print(",
          "1692:             f\"prepared on {datetime.now():%B %d, %Y}[/]\"",
          "1693:         )",
          "1694:         get_console().print()",
          "1698:         users: set[str] = set()",
          "1699:         for provider_info in providers.values():",
          "1700:             for pr in provider_info.pr_list:",
          "1701:                 users.add(\"@\" + pr.user.login)",
          "1706: def get_all_constraint_files(refresh_constraints: bool, python_version: str) -> None:",
          "",
          "[Removed Lines]",
          "1691:             \"Issue title: [yellow]Status of testing Providers that were \"",
          "1695:         syntax = Syntax(issue_content, \"markdown\", theme=\"ansi_dark\")",
          "1696:         get_console().print(syntax)",
          "1697:         get_console().print()",
          "1702:         get_console().print(\"All users involved in the PRs:\")",
          "1703:         get_console().print(\" \".join(users))",
          "",
          "[Added Lines]",
          "1708:             \"Issue title: [warning]Status of testing Providers that were \"",
          "1712:         issue_content += \"\\n\"",
          "1717:         issue_content += f\"All users involved in the PRs:\\n{' '.join(users)}\"",
          "1718:         syntax = Syntax(issue_content, \"markdown\", theme=\"ansi_dark\")",
          "1719:         get_console().print(syntax)",
          "1720:         url_to_create_the_issue = create_github_issue_url(",
          "1721:             title=f\"Status of testing Providers that were prepared on {datetime.now():%B %d, %Y}\",",
          "1722:             body=issue_content,",
          "1723:             labels=[\"testing status\", \"kind:meta\"],",
          "1724:         )",
          "1725:         get_console().print()",
          "1726:         get_console().print(",
          "1727:             \"[info]You can prefill the issue by copy&pasting this link to browser \"",
          "1728:             \"(or Cmd+Click if your terminal supports it):\\n\"",
          "1729:         )",
          "1730:         print(url_to_create_the_issue)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "234:                 \"--excluded-pr-list\",",
          "235:                 \"--github-token\",",
          "236:                 \"--only-available-in-dist\",",
          "238:             ],",
          "239:         }",
          "240:     ],",
          "",
          "[Removed Lines]",
          "237:                 \"--suffix\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2||dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2": [
          "File: dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2 -> dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: NOTE TO RELEASE MANAGER:",
          "30: -->",
          "",
          "[Removed Lines]",
          "23: Please move here the providers that have doc-only changes or for which changes are trivial, and",
          "24: you could assess that they are OK. In case",
          "26: The providers are automatically installed on Airflow 2.3 and latest `main` during the CI, so we know they",
          "27: are installable. Also, all classes within the providers are imported during the CI run so we know all",
          "28: providers can be imported.",
          "",
          "[Added Lines]",
          "23: You can move here the providers that have doc-only changes or for which changes are trivial, and",
          "24: you could assess that they are OK.",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "879af06000f615f0cbf19a3b5e0dce9a4e31fb6f",
      "candidate_info": {
        "commit_hash": "879af06000f615f0cbf19a3b5e0dce9a4e31fb6f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/879af06000f615f0cbf19a3b5e0dce9a4e31fb6f",
        "files": [
          ".github/workflows/ci.yml"
        ],
        "message": "Use default Postgres version for Quarantined tests (#36390)\n\n(cherry picked from commit 8e70d560245e9a102834fc8ff1d3cb269254e5a5)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "e98fed3e892d16f09fabd42abc251325d98aaf91",
      "candidate_info": {
        "commit_hash": "e98fed3e892d16f09fabd42abc251325d98aaf91",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e98fed3e892d16f09fabd42abc251325d98aaf91",
        "files": [
          "airflow/www/views.py",
          "tests/www/views/test_views_tasks.py"
        ],
        "message": "Remove option ot set a task instance to running state in UI (#36518)\n\n* Remove option ot set a task instance to running state in UI\n* Uups, fix pytests\n\n(cherry picked from commit 60aa611f04559391621e7d0d9612cfffef6368a1)",
        "before_after_code_files": [
          "airflow/www/views.py||airflow/www/views.py",
          "tests/www/views/test_views_tasks.py||tests/www/views/test_views_tasks.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "5450:         \"action_clear\": \"edit\",",
          "5451:         \"action_clear_downstream\": \"edit\",",
          "5452:         \"action_muldelete\": \"delete\",",
          "5454:         \"action_set_failed\": \"edit\",",
          "5455:         \"action_set_success\": \"edit\",",
          "5456:         \"action_set_retry\": \"edit\",",
          "",
          "[Removed Lines]",
          "5453:         \"action_set_running\": \"edit\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "5727:         except Exception:",
          "5728:             flash(\"Failed to set state\", \"error\")",
          "5739:     @action(\"set_failed\", \"Set state to 'failed'\", \"\", single=False)",
          "5740:     @auth.has_access_dag_entities(\"PUT\", DagAccessEntity.TASK_INSTANCE)",
          "5741:     @action_logging",
          "",
          "[Removed Lines]",
          "5730:     @action(\"set_running\", \"Set state to 'running'\", \"\", single=False)",
          "5731:     @auth.has_access_dag_entities(\"PUT\", DagAccessEntity.TASK_INSTANCE)",
          "5732:     @action_logging",
          "5733:     def action_set_running(self, tis):",
          "5734:         \"\"\"Set state to 'running'.\"\"\"",
          "5735:         self.set_task_instance_state(tis, TaskInstanceState.RUNNING)",
          "5736:         self.update_redirect()",
          "5737:         return redirect(self.get_redirect())",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_tasks.py||tests/www/views/test_views_tasks.py": [
          "File: tests/www/views/test_views_tasks.py -> tests/www/views/test_views_tasks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "943: @pytest.mark.parametrize(",
          "944:     \"action, expected_state\",",
          "945:     [",
          "947:         (\"set_failed\", State.FAILED),",
          "948:         (\"set_success\", State.SUCCESS),",
          "949:         (\"set_retry\", State.UP_FOR_RETRY),",
          "950:         (\"set_skipped\", State.SKIPPED),",
          "951:     ],",
          "953: )",
          "954: def test_task_instance_set_state(session, admin_client, action, expected_state):",
          "955:     task_id = \"runme_0\"",
          "",
          "[Removed Lines]",
          "946:         (\"set_running\", State.RUNNING),",
          "952:     ids=[\"running\", \"failed\", \"success\", \"retry\", \"skipped\"],",
          "",
          "[Added Lines]",
          "951:     ids=[\"failed\", \"success\", \"retry\", \"skipped\"],",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "972: @pytest.mark.parametrize(",
          "973:     \"action\",",
          "974:     [",
          "976:         \"set_failed\",",
          "977:         \"set_success\",",
          "978:         \"set_retry\",",
          "",
          "[Removed Lines]",
          "975:         \"set_running\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d7861f894552a9d83dfbd28ba3bc615044328467",
      "candidate_info": {
        "commit_hash": "d7861f894552a9d83dfbd28ba3bc615044328467",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/d7861f894552a9d83dfbd28ba3bc615044328467",
        "files": [
          "README.md",
          "generated/PYPI_README.md"
        ],
        "message": "Add Airflow logo to the README (#36645)\n\nSimilar to Apache Superset, adding this repo to README so that Airflow repo shows up nicely with thumbnails on various Topics pages like https://github.com/topics/airflow\n\n(cherry picked from commit 9a20aa8f0fe55f6dc2b6f94b8b41d36108bdd0d7)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "07e30c9f9c1e8eda82360d66ba74970ebb64a65a",
      "candidate_info": {
        "commit_hash": "07e30c9f9c1e8eda82360d66ba74970ebb64a65a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/07e30c9f9c1e8eda82360d66ba74970ebb64a65a",
        "files": [
          "setup.cfg"
        ],
        "message": "Fix configuration of mypy plugins to point to paths not modules (#36563)\n\nThe configuration of our MyPy plugins was wrongly pointing to\nmodules rather than paths. This caused problems in the environment\nwhere you had no PYTHONPATH set pointing to the root of your\nAirflow sources. One of the side effects was that MyPy Plugin\nfor IntelliJ failed with \"invalid plugin\" error.\n\nThis PR changes the plugins to use relative paths instead - which\nshould work when mypy is invoked from the root of the\nproject (which in general is how our mypy gets invoked anyway and\nis the default settings for most IDE integrations.\n\n(cherry picked from commit 8fba23fc8450d13c3a241252b547b95c0e258782)",
        "before_after_code_files": [
          "setup.cfg||setup.cfg"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.cfg||setup.cfg": [
          "File: setup.cfg -> setup.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "205: warn_redundant_casts = True",
          "206: warn_unused_ignores = False",
          "207: plugins =",
          "210: pretty = True",
          "211: show_error_codes = True",
          "212: # Mypy since 0.991 warns about type annotations being present in an untyped",
          "",
          "[Removed Lines]",
          "208:   dev.mypy.plugin.decorators,",
          "209:   dev.mypy.plugin.outputs",
          "",
          "[Added Lines]",
          "208:   dev/mypy/plugin/decorators.py,",
          "209:   dev/mypy/plugin/outputs.py",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4535686a9801f77cdfad7221e95315a657fa5153",
      "candidate_info": {
        "commit_hash": "4535686a9801f77cdfad7221e95315a657fa5153",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4535686a9801f77cdfad7221e95315a657fa5153",
        "files": [
          "airflow/hooks/filesystem.py"
        ],
        "message": "Follow BaseHook connection fields method signature in FSHook (#36444)\n\n(cherry picked from commit f5e5027e1c342237758424d7af50787a33329509)",
        "before_after_code_files": [
          "airflow/hooks/filesystem.py||airflow/hooks/filesystem.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/hooks/filesystem.py||airflow/hooks/filesystem.py": [
          "File: airflow/hooks/filesystem.py -> airflow/hooks/filesystem.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "41:     conn_type = \"fs\"",
          "42:     hook_name = \"File (path)\"",
          "46:         \"\"\"Return connection widgets to add to connection form.\"\"\"",
          "47:         from flask_appbuilder.fieldwidgets import BS3TextFieldWidget",
          "48:         from flask_babel import lazy_gettext",
          "",
          "[Removed Lines]",
          "44:     @staticmethod",
          "45:     def get_connection_form_widgets() -> dict[str, Any]:",
          "",
          "[Added Lines]",
          "44:     @classmethod",
          "45:     def get_connection_form_widgets(cls) -> dict[str, Any]:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "51:         return {\"path\": StringField(lazy_gettext(\"Path\"), widget=BS3TextFieldWidget())}",
          "55:         \"\"\"Return custom field behaviour.\"\"\"",
          "56:         return {",
          "57:             \"hidden_fields\": [\"host\", \"schema\", \"port\", \"login\", \"password\", \"extra\"],",
          "",
          "[Removed Lines]",
          "53:     @staticmethod",
          "54:     def get_ui_field_behaviour() -> dict[str, Any]:",
          "",
          "[Added Lines]",
          "53:     @classmethod",
          "54:     def get_ui_field_behaviour(cls) -> dict[str, Any]:",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d79f7a4027fd9b252005fa98f4a31d171f65c45b",
      "candidate_info": {
        "commit_hash": "d79f7a4027fd9b252005fa98f4a31d171f65c45b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/d79f7a4027fd9b252005fa98f4a31d171f65c45b",
        "files": [
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "dev/breeze/tests/test_selective_checks.py"
        ],
        "message": "Fix problems with missing selective checks on new types of unit tests (#36372)\n\nWhen the DB/NonDB tests were introduced (#35160) new test types have\nbeen added (separating various Python test types from generic\nOperator test type). However we have not added matching of the python\noperator and test files into the right selective unit test type. This\ncaused that when only `operators/python.py` and `tests/test_python` were\nchanged, then `Operators` test type was run but the specific Python *\ntest types were not run.\n\nThis PR fixes it for current test type (including also separated\nSerialization test type) and for the future - instead of matching\nselected test type we match all of them except the few that we\nnow are \"special\" (\"Always, Core, Other, PlainAsserts\").\n\n(cherry picked from commit b0db1f94ede7d316b3f63a924176e5e1eefa89c1)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "dev/breeze/tests/test_selective_checks.py||dev/breeze/tests/test_selective_checks.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py": [
          "File: dev/breeze/src/airflow_breeze/utils/selective_checks.py -> dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "223:     }",
          "224: )",
          "226: TEST_TYPE_MATCHES = HashableDict(",
          "227:     {",
          "228:         SelectiveUnitTestTypes.API: [",
          "233:         ],",
          "234:         SelectiveUnitTestTypes.CLI: [",
          "237:         ],",
          "238:         SelectiveUnitTestTypes.OPERATORS: [",
          "241:         ],",
          "242:         SelectiveUnitTestTypes.PROVIDERS: [",
          "243:             r\"^airflow/providers/\",",
          "244:             r\"^tests/system/providers/\",",
          "245:             r\"^tests/providers/\",",
          "246:         ],",
          "252:         ],",
          "253:         SelectiveUnitTestTypes.WWW: [r\"^airflow/www\", r\"^tests/www\"],",
          "254:     }",
          "255: )",
          "",
          "[Removed Lines]",
          "229:             r\"^airflow/api\",",
          "230:             r\"^airflow/api_connexion\",",
          "231:             r\"^tests/api\",",
          "232:             r\"^tests/api_connexion\",",
          "235:             r\"^airflow/cli\",",
          "236:             r\"^tests/cli\",",
          "239:             r\"^airflow/operators\",",
          "240:             r\"^tests/operators\",",
          "247:         SelectiveUnitTestTypes.PYTHON_VENV: [",
          "248:             r\"^tests/operators/test_python.py\",",
          "249:         ],",
          "250:         SelectiveUnitTestTypes.BRANCH_PYTHON_VENV: [",
          "251:             r\"^tests/operators/test_python.py\",",
          "",
          "[Added Lines]",
          "226: PYTHON_OPERATOR_FILES = [",
          "227:     r\"^airflow/operators/python.py\",",
          "228:     r\"^tests/operators/test_python.py\",",
          "229: ]",
          "234:             r\"^airflow/api/\",",
          "235:             r\"^airflow/api_connexion/\",",
          "236:             r\"^airflow/api_internal/\",",
          "237:             r\"^tests/api/\",",
          "238:             r\"^tests/api_connexion/\",",
          "239:             r\"^tests/api_internal/\",",
          "242:             r\"^airflow/cli/\",",
          "243:             r\"^tests/cli/\",",
          "246:             r\"^airflow/operators/\",",
          "247:             r\"^tests/operators/\",",
          "254:         SelectiveUnitTestTypes.SERIALIZATION: [",
          "255:             r\"^airflow/serialization/\",",
          "256:             r\"^tests/serialization/\",",
          "258:         SelectiveUnitTestTypes.PYTHON_VENV: PYTHON_OPERATOR_FILES,",
          "259:         SelectiveUnitTestTypes.BRANCH_PYTHON_VENV: PYTHON_OPERATOR_FILES,",
          "260:         SelectiveUnitTestTypes.EXTERNAL_PYTHON: PYTHON_OPERATOR_FILES,",
          "261:         SelectiveUnitTestTypes.EXTERNAL_BRANCH_PYTHON: PYTHON_OPERATOR_FILES,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "653:         candidate_test_types: set[str] = {\"Always\"}",
          "654:         matched_files: set[str] = set()",
          "671:         kubernetes_files = self._matching_files(",
          "672:             FileGroupForCi.KUBERNETES_FILES, CI_FILE_GROUP_MATCHES, CI_FILE_GROUP_EXCLUDES",
          "",
          "[Removed Lines]",
          "655:         matched_files.update(",
          "656:             self._select_test_type_if_matching(candidate_test_types, SelectiveUnitTestTypes.WWW)",
          "657:         )",
          "658:         matched_files.update(",
          "659:             self._select_test_type_if_matching(candidate_test_types, SelectiveUnitTestTypes.PROVIDERS)",
          "660:         )",
          "661:         matched_files.update(",
          "662:             self._select_test_type_if_matching(candidate_test_types, SelectiveUnitTestTypes.CLI)",
          "663:         )",
          "664:         matched_files.update(",
          "665:             self._select_test_type_if_matching(candidate_test_types, SelectiveUnitTestTypes.OPERATORS)",
          "666:         )",
          "667:         matched_files.update(",
          "668:             self._select_test_type_if_matching(candidate_test_types, SelectiveUnitTestTypes.API)",
          "669:         )",
          "",
          "[Added Lines]",
          "664:         for test_type in SelectiveUnitTestTypes:",
          "665:             if test_type not in [",
          "666:                 SelectiveUnitTestTypes.ALWAYS,",
          "667:                 SelectiveUnitTestTypes.CORE,",
          "668:                 SelectiveUnitTestTypes.OTHER,",
          "669:                 SelectiveUnitTestTypes.PLAIN_ASSERTS,",
          "670:             ]:",
          "671:                 matched_files.update(self._select_test_type_if_matching(candidate_test_types, test_type))",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_selective_checks.py||dev/breeze/tests/test_selective_checks.py": [
          "File: dev/breeze/tests/test_selective_checks.py -> dev/breeze/tests/test_selective_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "161:                 id=\"Only Operator tests and DOCS should run\",",
          "162:             )",
          "163:         ),",
          "164:         (",
          "165:             pytest.param(",
          "166:                 (",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "164:         (",
          "165:             pytest.param(",
          "166:                 (\"airflow/operators/python.py\",),",
          "167:                 {",
          "168:                     \"affected-providers-list-as-string\": None,",
          "169:                     \"all-python-versions\": \"['3.8']\",",
          "170:                     \"all-python-versions-list-as-string\": \"3.8\",",
          "171:                     \"python-versions\": \"['3.8']\",",
          "172:                     \"python-versions-list-as-string\": \"3.8\",",
          "173:                     \"ci-image-build\": \"true\",",
          "174:                     \"prod-image-build\": \"false\",",
          "175:                     \"needs-helm-tests\": \"false\",",
          "176:                     \"run-tests\": \"true\",",
          "177:                     \"run-amazon-tests\": \"false\",",
          "178:                     \"docs-build\": \"true\",",
          "179:                     \"skip-pre-commits\": \"check-provider-yaml-valid,identity,lint-helm-chart,mypy-dev,\"",
          "180:                     \"mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "181:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "182:                     \"parallel-test-types-list-as-string\": \"Always BranchExternalPython BranchPythonVenv \"",
          "183:                     \"ExternalPython Operators PythonVenv\",",
          "184:                 },",
          "185:                 id=\"Only Python tests\",",
          "186:             )",
          "187:         ),",
          "188:         (",
          "189:             pytest.param(",
          "190:                 (\"airflow/serialization/python.py\",),",
          "191:                 {",
          "192:                     \"affected-providers-list-as-string\": None,",
          "193:                     \"all-python-versions\": \"['3.8']\",",
          "194:                     \"all-python-versions-list-as-string\": \"3.8\",",
          "195:                     \"python-versions\": \"['3.8']\",",
          "196:                     \"python-versions-list-as-string\": \"3.8\",",
          "197:                     \"ci-image-build\": \"true\",",
          "198:                     \"prod-image-build\": \"false\",",
          "199:                     \"needs-helm-tests\": \"false\",",
          "200:                     \"run-tests\": \"true\",",
          "201:                     \"run-amazon-tests\": \"false\",",
          "202:                     \"docs-build\": \"true\",",
          "203:                     \"skip-pre-commits\": \"check-provider-yaml-valid,identity,lint-helm-chart,mypy-dev,\"",
          "204:                     \"mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "205:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "206:                     \"parallel-test-types-list-as-string\": \"Always Serialization\",",
          "207:                 },",
          "208:                 id=\"Only Serialization tests\",",
          "209:             )",
          "210:         ),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cff4394256da40028adbe9fa0300288b582e8245",
      "candidate_info": {
        "commit_hash": "cff4394256da40028adbe9fa0300288b582e8245",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cff4394256da40028adbe9fa0300288b582e8245",
        "files": [
          ".github/workflows/recheck-old-bug-report.yml",
          ".github/workflows/stale.yml",
          "airflow/auth/managers/fab/security_manager/override.py",
          "airflow/providers/google/cloud/hooks/dataflow.py",
          "generated/provider_dependencies.json",
          "scripts/in_container/run_provider_yaml_files_check.py",
          "scripts/in_container/verify_providers.py"
        ],
        "message": "Bump stalebot to version 9 (#36494)\n\n(cherry picked from commit 13e4905a60011e162f34e86a77acfcb4af874685)",
        "before_after_code_files": [
          "airflow/auth/managers/fab/security_manager/override.py||airflow/auth/managers/fasecurity_manager/override.py",
          "airflow/providers/google/cloud/hooks/dataflow.py||airflow/providers/google/cloud/hooks/dataflow.py",
          "scripts/in_container/run_provider_yaml_files_check.py||scripts/in_container/run_provider_yaml_files_check.py",
          "scripts/in_container/verify_providers.py||scripts/in_container/verify_providers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/auth/managers/fab/security_manager/override.py||airflow/auth/managers/fasecurity_manager/override.py": [
          "File: airflow/auth/managers/fab/security_manager/override.py -> airflow/auth/managers/fasecurity_manager/override.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2088:             @appbuilder.sm.oauth_user_info_getter",
          "2089:             def my_oauth_user_info(sm, provider, response=None):",
          "2093:                 return {}",
          "2094:         \"\"\"",
          "",
          "[Removed Lines]",
          "2090:                 if provider == 'github':",
          "2091:                     me = sm.oauth_remotes[provider].get('user')",
          "2092:                     return {'username': me.data.get('login')}",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/hooks/dataflow.py||airflow/providers/google/cloud/hooks/dataflow.py": [
          "File: airflow/providers/google/cloud/hooks/dataflow.py -> airflow/providers/google/cloud/hooks/dataflow.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "57: def process_line_and_extract_dataflow_job_id_callback(",
          "59: ) -> Callable[[str], None]:",
          "60:     \"\"\"Build callback that triggers the specified function.",
          "",
          "[Removed Lines]",
          "58:     on_new_job_id_callback: Callable[[str], None] | None",
          "",
          "[Added Lines]",
          "58:     on_new_job_id_callback: Callable[[str], None] | None,",
          "",
          "---------------"
        ],
        "scripts/in_container/run_provider_yaml_files_check.py||scripts/in_container/run_provider_yaml_files_check.py": [
          "File: scripts/in_container/run_provider_yaml_files_check.py -> scripts/in_container/run_provider_yaml_files_check.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "314: @run_check(\"Checking completeness of list of {sensors, hooks, operators, triggers}\")",
          "315: def check_correctness_of_list_of_sensors_operators_hook_trigger_modules(",
          "317: ) -> tuple[int, int]:",
          "318:     num_errors = 0",
          "319:     num_modules = 0",
          "",
          "[Removed Lines]",
          "316:     yaml_files: dict[str, dict]",
          "",
          "[Added Lines]",
          "316:     yaml_files: dict[str, dict],",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "355: @run_check(\"Checking for duplicates in list of {sensors, hooks, operators, triggers}\")",
          "356: def check_duplicates_in_integrations_names_of_hooks_sensors_operators(",
          "358: ) -> tuple[int, int]:",
          "359:     num_errors = 0",
          "360:     num_integrations = 0",
          "",
          "[Removed Lines]",
          "357:     yaml_files: dict[str, dict]",
          "",
          "[Added Lines]",
          "357:     yaml_files: dict[str, dict],",
          "",
          "---------------"
        ],
        "scripts/in_container/verify_providers.py||scripts/in_container/verify_providers.py": [
          "File: scripts/in_container/verify_providers.py -> scripts/in_container/verify_providers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "580: def check_if_classes_are_properly_named(",
          "582: ) -> tuple[int, int]:",
          "583:     \"\"\"Check if all entities in the dictionary are named properly.",
          "",
          "[Removed Lines]",
          "581:     entity_summary: dict[EntityType, EntityTypeSummary]",
          "",
          "[Added Lines]",
          "581:     entity_summary: dict[EntityType, EntityTypeSummary],",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cdca4cae474cdf57fd0f567b26c1c91ac97d0876",
      "candidate_info": {
        "commit_hash": "cdca4cae474cdf57fd0f567b26c1c91ac97d0876",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cdca4cae474cdf57fd0f567b26c1c91ac97d0876",
        "files": [
          "airflow/utils/log/file_task_handler.py"
        ],
        "message": "Add log lookup exception for empty op subtypes (#35536)\n\n* Add log lookup exception for empty op subtypes\n\n* Use exception catching approach instead to preserve tests\n\n(cherry picked from commit ddcaef45593a5411859327ab2d16ed648073b986)",
        "before_after_code_files": [
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: from typing import TYPE_CHECKING, Any, Callable, Iterable",
          "30: from urllib.parse import urljoin",
          "32: import pendulum",
          "34: from airflow.configuration import conf",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32: import httpx",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "80: def _fetch_logs_from_service(url, log_relative_path):",
          "83:     from airflow.utils.jwt_signer import JWTSigner",
          "85:     timeout = conf.getint(\"webserver\", \"log_fetch_timeout_sec\", fallback=None)",
          "",
          "[Removed Lines]",
          "81:     import httpx",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "170:     \"\"\"",
          "172:     trigger_should_wrap = True",
          "174:     def __init__(self, base_log_folder: str, filename_template: str | None = None):",
          "175:         super().__init__()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "172:     inherits_from_empty_operator_log_message = (",
          "173:         \"Operator inherits from empty operator and thus does not have logs\"",
          "174:     )",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "555:                 messages.append(f\"Found logs served from host {url}\")",
          "556:                 logs.append(response.text)",
          "557:         except Exception as e:",
          "560:         return messages, logs",
          "562:     def _read_remote_logs(self, ti, try_number, metadata=None) -> tuple[list[str], list[str]]:",
          "",
          "[Removed Lines]",
          "558:             messages.append(f\"Could not read served logs: {e}\")",
          "559:             logger.exception(\"Could not read served logs\")",
          "",
          "[Added Lines]",
          "560:             if isinstance(e, httpx.UnsupportedProtocol) and ti.task.inherits_from_empty_operator is True:",
          "561:                 messages.append(self.inherits_from_empty_operator_log_message)",
          "562:             else:",
          "563:                 messages.append(f\"Could not read served logs: {e}\")",
          "564:                 logger.exception(\"Could not read served logs\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bf935545adbfeb24fe9449b21316c9c4024767bc",
      "candidate_info": {
        "commit_hash": "bf935545adbfeb24fe9449b21316c9c4024767bc",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/bf935545adbfeb24fe9449b21316c9c4024767bc",
        "files": [
          "airflow/decorators/base.py",
          "tests/decorators/test_python.py"
        ],
        "message": "Fix check on subclass for `typing.Union` in `_infer_multiple_outputs` for Python 3.10+ (#36728)\n\n* Fix check on subclass for `typing.Union` in `_infer_multiple_outputs` for Python 3.10+\n\n* Limit PEP 604 test by Python 3.10\n\n(cherry picked from commit f1d82971053287c27c83e1b945b774a6a37a8552)",
        "before_after_code_files": [
          "airflow/decorators/base.py||airflow/decorators/base.py",
          "tests/decorators/test_python.py||tests/decorators/test_python.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/decorators/base.py||airflow/decorators/base.py": [
          "File: airflow/decorators/base.py -> airflow/decorators/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "350:         except TypeError:  # Can't evaluate return type.",
          "351:             return False",
          "352:         ttype = getattr(return_type, \"__origin__\", return_type)",
          "355:     def __attrs_post_init__(self):",
          "356:         if \"self\" in self.function_signature.parameters:",
          "",
          "[Removed Lines]",
          "353:         return issubclass(ttype, Mapping)",
          "",
          "[Added Lines]",
          "353:         return isinstance(ttype, type) and issubclass(ttype, Mapping)",
          "",
          "---------------"
        ],
        "tests/decorators/test_python.py||tests/decorators/test_python.py": [
          "File: tests/decorators/test_python.py -> tests/decorators/test_python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "98:         assert identity_dict_with_decorator_call(5, 5).operator.multiple_outputs is True",
          "101:     def test_infer_multiple_outputs_typed_dict(self):",
          "102:         from typing import TypedDict",
          "",
          "[Removed Lines]",
          "100:     @pytest.mark.skipif(sys.version_info < (3, 8), reason=\"PEP 589 is implemented in Python 3.8\")",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "111:         assert t1().operator.multiple_outputs is True",
          "113:     def test_infer_multiple_outputs_forward_annotation(self):",
          "114:         if TYPE_CHECKING:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "112:     # We do not enable `from __future__ import annotations` for particular this test module,",
          "113:     # that mean `str | None` annotation would raise TypeError in Python 3.9 and below",
          "114:     @pytest.mark.skipif(sys.version_info < (3, 10), reason=\"PEP 604 is implemented in Python 3.10\")",
          "115:     def test_infer_multiple_outputs_pep_604_union_type(self):",
          "116:         @task_decorator",
          "117:         def t1() -> str | None:",
          "118:             # Before PEP 604 which are implemented in Python 3.10 `str | None`",
          "119:             # returns `types.UnionType` which are class and could be check in `issubclass()`.",
          "120:             # However in Python 3.10+ this construction returns object `typing.Union`",
          "121:             # which can not be used in `issubclass()`",
          "122:             return \"foo\"",
          "124:         assert t1().operator.multiple_outputs is False",
          "126:     def test_infer_multiple_outputs_union_type(self):",
          "127:         @task_decorator",
          "128:         def t1() -> Union[str, None]:",
          "129:             return \"foo\"",
          "131:         assert t1().operator.multiple_outputs is False",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0dba06d66064edab2144bfcb70cd253a31ce6bc7",
      "candidate_info": {
        "commit_hash": "0dba06d66064edab2144bfcb70cd253a31ce6bc7",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0dba06d66064edab2144bfcb70cd253a31ce6bc7",
        "files": [
          "airflow/models/dag.py"
        ],
        "message": "fix datetime reference in DAG.is_fixed_time_schedule (#36370)\n\n(cherry picked from commit 547ddf6317b3fc93e766b61daf11308b552e6d6b)",
        "before_after_code_files": [
          "airflow/models/dag.py||airflow/models/dag.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "874:         from croniter import croniter",
          "876:         cron = croniter(self.timetable._expression)",
          "879:         return next_b.minute == next_a.minute and next_b.hour == next_a.hour",
          "881:     def following_schedule(self, dttm):",
          "",
          "[Removed Lines]",
          "877:         next_a = cron.get_next(datetime.datetime)",
          "878:         next_b = cron.get_next(datetime.datetime)",
          "",
          "[Added Lines]",
          "877:         next_a = cron.get_next(datetime)",
          "878:         next_b = cron.get_next(datetime)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ed1e49a1196d8bc68fc9f48f9ad5cc5b30cf4921",
      "candidate_info": {
        "commit_hash": "ed1e49a1196d8bc68fc9f48f9ad5cc5b30cf4921",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ed1e49a1196d8bc68fc9f48f9ad5cc5b30cf4921",
        "files": [
          ".github/workflows/ci.yml",
          ".pre-commit-config.yaml",
          "STATIC_CODE_CHECKS.rst",
          "dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "dev/breeze/tests/test_selective_checks.py",
          "images/breeze/output_static-checks.svg",
          "images/breeze/output_static-checks.txt",
          "scripts/ci/pre_commit/pre_commit_mypy.py",
          "scripts/ci/pre_commit/pre_commit_mypy_folder.py"
        ],
        "message": "Simplify how mypy \"folder\" checks are run (#36760)\n\nThe #36638 change introduced \"full package\" checks - where in\ncase of CI we run mypy checks separately from regular static checks,\nfor the whole folders.\n\nHowever it's been a little convoluted on how the checks were run,\nwith a separate env variable. Instead we can actually have multiple\nmypy-* checks (same as we have for local pre-commit runs) as mypy\nallows to have multiple checks with the same name in various stages.\n\nThis change simplifies the setup a bit:\n\n* we name the checks \"folder\" checks because this is what they are\n* we name the check names consistent (\"airflow\", \"providers\", \"docs\",\n  \"dev\") with mypy-folders output\n* we have separate small script to run the folder checks\n* we map \"providers\" into \"airflow/providers\" in the pre-commit\n\n(cherry picked from commit a912948b51cc50ae6c92496e11abebbba0c647e5)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/pre_commit_ids.py||dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "dev/breeze/tests/test_selective_checks.py||dev/breeze/tests/test_selective_checks.py",
          "scripts/ci/pre_commit/pre_commit_mypy.py||scripts/ci/pre_commit/pre_commit_mypy.py",
          "scripts/ci/pre_commit/pre_commit_mypy_folder.py||scripts/ci/pre_commit/pre_commit_mypy_folder.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/pre_commit_ids.py||dev/breeze/src/airflow_breeze/pre_commit_ids.py": [
          "File: dev/breeze/src/airflow_breeze/pre_commit_ids.py -> dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "103:     \"lint-markdown\",",
          "104:     \"lint-openapi\",",
          "105:     \"mixed-line-ending\",",
          "108:     \"mypy-dev\",",
          "109:     \"mypy-docs\",",
          "110:     \"mypy-providers\",",
          "",
          "[Removed Lines]",
          "106:     \"mypy\",",
          "107:     \"mypy-core\",",
          "",
          "[Added Lines]",
          "106:     \"mypy-airflow\",",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py": [
          "File: dev/breeze/src/airflow_breeze/utils/selective_checks.py -> dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "582:             return False",
          "584:     @cached_property",
          "587:         if (",
          "588:             self._matching_files(",
          "589:                 FileGroupForCi.ALL_AIRFLOW_PYTHON_FILES, CI_FILE_GROUP_MATCHES, CI_FILE_GROUP_EXCLUDES",
          "590:             )",
          "591:             or self.full_tests_needed",
          "592:         ):",
          "594:         if (",
          "595:             self._matching_files(",
          "596:                 FileGroupForCi.ALL_PROVIDERS_PYTHON_FILES, CI_FILE_GROUP_MATCHES, CI_FILE_GROUP_EXCLUDES",
          "597:             )",
          "598:             or self._are_all_providers_affected()",
          "599:         ) and self._default_branch == \"main\":",
          "601:         if (",
          "602:             self._matching_files(",
          "603:                 FileGroupForCi.ALL_DOCS_PYTHON_FILES, CI_FILE_GROUP_MATCHES, CI_FILE_GROUP_EXCLUDES",
          "604:             )",
          "605:             or self.full_tests_needed",
          "606:         ):",
          "608:         if (",
          "609:             self._matching_files(",
          "610:                 FileGroupForCi.ALL_DEV_PYTHON_FILES, CI_FILE_GROUP_MATCHES, CI_FILE_GROUP_EXCLUDES",
          "611:             )",
          "612:             or self.full_tests_needed",
          "613:         ):",
          "617:     @cached_property",
          "618:     def needs_mypy(self) -> bool:",
          "621:     @cached_property",
          "622:     def needs_python_scans(self) -> bool:",
          "",
          "[Removed Lines]",
          "585:     def mypy_packages(self) -> list[str]:",
          "586:         packages_to_run: list[str] = []",
          "593:             packages_to_run.append(\"airflow\")",
          "600:             packages_to_run.append(\"airflow/providers\")",
          "607:             packages_to_run.append(\"docs\")",
          "614:             packages_to_run.append(\"dev\")",
          "615:         return packages_to_run",
          "619:         return self.mypy_packages != []",
          "",
          "[Added Lines]",
          "585:     def mypy_folders(self) -> list[str]:",
          "586:         folders_to_check: list[str] = []",
          "593:             folders_to_check.append(\"airflow\")",
          "600:             folders_to_check.append(\"providers\")",
          "607:             folders_to_check.append(\"docs\")",
          "614:             folders_to_check.append(\"dev\")",
          "615:         return folders_to_check",
          "619:         return self.mypy_folders != []",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_selective_checks.py||dev/breeze/tests/test_selective_checks.py": [
          "File: dev/breeze/tests/test_selective_checks.py -> dev/breeze/tests/test_selective_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "112:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "113:                     \"parallel-test-types-list-as-string\": None,",
          "114:                     \"needs-mypy\": \"false\",",
          "116:                 },",
          "117:                 id=\"No tests on simple change\",",
          "118:             )",
          "",
          "[Removed Lines]",
          "115:                     \"mypy-packages\": \"[]\",",
          "",
          "[Added Lines]",
          "115:                     \"mypy-folders\": \"[]\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "137:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "138:                     \"parallel-test-types-list-as-string\": \"API Always\",",
          "139:                     \"needs-mypy\": \"true\",",
          "141:                 },",
          "142:                 id=\"Only API tests and DOCS should run\",",
          "143:             )",
          "",
          "[Removed Lines]",
          "140:                     \"mypy-packages\": \"['airflow']\",",
          "",
          "[Added Lines]",
          "140:                     \"mypy-folders\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "162:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "163:                     \"parallel-test-types-list-as-string\": \"Always Operators\",",
          "164:                     \"needs-mypy\": \"true\",",
          "166:                 },",
          "167:                 id=\"Only Operator tests and DOCS should run\",",
          "168:             )",
          "",
          "[Removed Lines]",
          "165:                     \"mypy-packages\": \"['airflow']\",",
          "",
          "[Added Lines]",
          "165:                     \"mypy-folders\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "188:                     \"parallel-test-types-list-as-string\": \"Always BranchExternalPython BranchPythonVenv \"",
          "189:                     \"ExternalPython Operators PythonVenv\",",
          "190:                     \"needs-mypy\": \"true\",",
          "192:                 },",
          "193:                 id=\"Only Python tests\",",
          "194:             )",
          "",
          "[Removed Lines]",
          "191:                     \"mypy-packages\": \"['airflow']\",",
          "",
          "[Added Lines]",
          "191:                     \"mypy-folders\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "213:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "214:                     \"parallel-test-types-list-as-string\": \"Always Serialization\",",
          "215:                     \"needs-mypy\": \"true\",",
          "217:                 },",
          "218:                 id=\"Only Serialization tests\",",
          "219:             )",
          "",
          "[Removed Lines]",
          "216:                     \"mypy-packages\": \"['airflow']\",",
          "",
          "[Added Lines]",
          "216:                     \"mypy-folders\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "243:                     \"parallel-test-types-list-as-string\": \"API Always Providers[amazon] \"",
          "244:                     \"Providers[common.sql,openlineage,pgvector,postgres] Providers[google]\",",
          "245:                     \"needs-mypy\": \"true\",",
          "247:                 },",
          "248:                 id=\"API and providers tests and docs should run\",",
          "249:             )",
          "",
          "[Removed Lines]",
          "246:                     \"mypy-packages\": \"['airflow', 'airflow/providers']\",",
          "",
          "[Added Lines]",
          "246:                     \"mypy-folders\": \"['airflow', 'providers']\",",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "269:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "270:                     \"parallel-test-types-list-as-string\": \"Always Providers[apache.beam] Providers[google]\",",
          "271:                     \"needs-mypy\": \"true\",",
          "273:                 },",
          "274:                 id=\"Selected Providers and docs should run\",",
          "275:             )",
          "",
          "[Removed Lines]",
          "272:                     \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "[Added Lines]",
          "272:                     \"mypy-folders\": \"['providers']\",",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "295:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "296:                     \"parallel-test-types-list-as-string\": None,",
          "297:                     \"needs-mypy\": \"false\",",
          "299:                 },",
          "300:                 id=\"Only docs builds should run - no tests needed\",",
          "301:             )",
          "",
          "[Removed Lines]",
          "298:                     \"mypy-packages\": \"[]\",",
          "",
          "[Added Lines]",
          "298:                     \"mypy-folders\": \"[]\",",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "325:                     \"parallel-test-types-list-as-string\": \"Always Providers[amazon] \"",
          "326:                     \"Providers[common.sql,openlineage,pgvector,postgres] Providers[google]\",",
          "327:                     \"needs-mypy\": \"true\",",
          "329:                 },",
          "330:                 id=\"Helm tests, providers (both upstream and downstream),\"",
          "331:                 \"kubernetes tests and docs should run\",",
          "",
          "[Removed Lines]",
          "328:                     \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "[Added Lines]",
          "328:                     \"mypy-folders\": \"['providers']\",",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "357:                     \"parallel-test-types-list-as-string\": \"Always \"",
          "358:                     \"Providers[airbyte,apache.livy,dbt.cloud,dingding,discord,http] Providers[amazon]\",",
          "359:                     \"needs-mypy\": \"true\",",
          "361:                 },",
          "362:                 id=\"Helm tests, http and all relevant providers, kubernetes tests and \"",
          "363:                 \"docs should run even if unimportant files were added\",",
          "",
          "[Removed Lines]",
          "360:                     \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "[Added Lines]",
          "360:                     \"mypy-folders\": \"['providers']\",",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "387:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "388:                     \"parallel-test-types-list-as-string\": \"Always Providers[airbyte,http]\",",
          "389:                     \"needs-mypy\": \"true\",",
          "391:                 },",
          "392:                 id=\"Helm tests, airbyte/http providers, kubernetes tests and \"",
          "393:                 \"docs should run even if unimportant files were added\",",
          "",
          "[Removed Lines]",
          "390:                     \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "[Added Lines]",
          "390:                     \"mypy-folders\": \"['providers']\",",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "418:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "419:                     \"parallel-test-types-list-as-string\": \"Always\",",
          "420:                     \"needs-mypy\": \"true\",",
          "422:                 },",
          "423:                 id=\"Docs should run even if unimportant files were added and prod image \"",
          "424:                 \"should be build for chart changes\",",
          "",
          "[Removed Lines]",
          "421:                     \"mypy-packages\": \"['airflow']\",",
          "",
          "[Added Lines]",
          "421:                     \"mypy-folders\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "444:                     \"upgrade-to-newer-dependencies\": \"true\",",
          "445:                     \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "446:                     \"needs-mypy\": \"true\",",
          "448:                 },",
          "449:                 id=\"Everything should run - including all providers and upgrading to \"",
          "450:                 \"newer requirements as pyproject.toml changed and all Python versions\",",
          "",
          "[Removed Lines]",
          "447:                     \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "447:                     \"mypy-folders\": \"['airflow', 'providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "470:                     \"upgrade-to-newer-dependencies\": \"true\",",
          "471:                     \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "472:                     \"needs-mypy\": \"true\",",
          "474:                 },",
          "475:                 id=\"Everything should run and upgrading to newer requirements as dependencies change\",",
          "476:             )",
          "",
          "[Removed Lines]",
          "473:                     \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "473:                     \"mypy-folders\": \"['airflow', 'providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "498:                 \"Providers[apache.hive,cncf.kubernetes,common.sql,exasol,ftp,http,\"",
          "499:                 \"imap,microsoft.azure,mongo,mysql,openlineage,postgres,salesforce,ssh] Providers[google]\",",
          "500:                 \"needs-mypy\": \"true\",",
          "502:             },",
          "503:             id=\"Providers tests run including amazon tests if amazon provider files changed\",",
          "504:         ),",
          "",
          "[Removed Lines]",
          "501:                 \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "[Added Lines]",
          "501:                 \"mypy-folders\": \"['providers']\",",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "521:                 \"upgrade-to-newer-dependencies\": \"false\",",
          "522:                 \"parallel-test-types-list-as-string\": \"Always Providers[airbyte,http]\",",
          "523:                 \"needs-mypy\": \"true\",",
          "525:             },",
          "526:             id=\"Providers tests run without amazon tests if no amazon file changed\",",
          "527:         ),",
          "",
          "[Removed Lines]",
          "524:                 \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "[Added Lines]",
          "524:                 \"mypy-folders\": \"['providers']\",",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "548:                 \"Providers[apache.hive,cncf.kubernetes,common.sql,exasol,ftp,http,\"",
          "549:                 \"imap,microsoft.azure,mongo,mysql,openlineage,postgres,salesforce,ssh] Providers[google]\",",
          "550:                 \"needs-mypy\": \"true\",",
          "552:             },",
          "553:             id=\"Providers tests run including amazon tests if amazon provider files changed\",",
          "554:         ),",
          "",
          "[Removed Lines]",
          "551:                 \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "[Added Lines]",
          "551:                 \"mypy-folders\": \"['providers']\",",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "575:                 \"upgrade-to-newer-dependencies\": \"false\",",
          "576:                 \"parallel-test-types-list-as-string\": \"Always Providers[common.io]\",",
          "577:                 \"needs-mypy\": \"true\",",
          "579:             },",
          "580:             id=\"Only Always and Common.IO tests should run when only common.io and tests/always changed\",",
          "581:         ),",
          "",
          "[Removed Lines]",
          "578:                 \"mypy-packages\": \"['airflow', 'airflow/providers']\",",
          "",
          "[Added Lines]",
          "578:                 \"mypy-folders\": \"['airflow', 'providers']\",",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "619:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "620:                     \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "621:                     \"needs-mypy\": \"true\",",
          "623:                 },",
          "624:                 id=\"Everything should run including all providers when full tests are needed\",",
          "625:             )",
          "",
          "[Removed Lines]",
          "622:                     \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "622:                     \"mypy-folders\": \"['airflow', 'providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "648:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "649:                     \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "650:                     \"needs-mypy\": \"true\",",
          "652:                 },",
          "653:                 id=\"Everything should run including full providers when full \"",
          "654:                 \"tests are needed even with different label set as well\",",
          "",
          "[Removed Lines]",
          "651:                     \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "651:                     \"mypy-folders\": \"['airflow', 'providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "675:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "676:                     \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "677:                     \"needs-mypy\": \"true\",",
          "679:                 },",
          "680:                 id=\"Everything should run including full providers when\"",
          "681:                 \"full tests are needed even if no files are changed\",",
          "",
          "[Removed Lines]",
          "678:                     \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "678:                     \"mypy-folders\": \"['airflow', 'providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "705:                     \"BranchPythonVenv CLI Core ExternalPython Operators Other PlainAsserts \"",
          "706:                     \"PythonVenv Serialization WWW\",",
          "707:                     \"needs-mypy\": \"true\",",
          "709:                 },",
          "710:                 id=\"Everything should run except Providers and lint pre-commit \"",
          "711:                 \"when full tests are needed for non-main branch\",",
          "",
          "[Removed Lines]",
          "708:                     \"mypy-packages\": \"['airflow', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "708:                     \"mypy-folders\": \"['airflow', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "751:                 \"skip-provider-tests\": \"true\",",
          "752:                 \"parallel-test-types-list-as-string\": None,",
          "753:                 \"needs-mypy\": \"false\",",
          "755:             },",
          "756:             id=\"Nothing should run if only non-important files changed\",",
          "757:         ),",
          "",
          "[Removed Lines]",
          "754:                 \"mypy-packages\": \"[]\",",
          "",
          "[Added Lines]",
          "754:                 \"mypy-folders\": \"[]\",",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "781:                 \"skip-provider-tests\": \"true\",",
          "782:                 \"parallel-test-types-list-as-string\": \"Always\",",
          "783:                 \"needs-mypy\": \"false\",",
          "785:             },",
          "786:             id=\"No Helm tests, No providers no lint charts, should run if \"",
          "787:             \"only chart/providers changed in non-main but PROD image should be built\",",
          "",
          "[Removed Lines]",
          "784:                 \"mypy-packages\": \"[]\",",
          "",
          "[Added Lines]",
          "784:                 \"mypy-folders\": \"[]\",",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "812:                 \"skip-provider-tests\": \"true\",",
          "813:                 \"parallel-test-types-list-as-string\": \"Always CLI\",",
          "814:                 \"needs-mypy\": \"true\",",
          "816:             },",
          "817:             id=\"Only CLI tests and Kubernetes tests should run if cli/chart files changed in non-main branch\",",
          "818:         ),",
          "",
          "[Removed Lines]",
          "815:                 \"mypy-packages\": \"['airflow']\",",
          "",
          "[Added Lines]",
          "815:                 \"mypy-folders\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 26 ---",
          "[Context before]",
          "839:                 \"parallel-test-types-list-as-string\": \"API Always BranchExternalPython BranchPythonVenv \"",
          "840:                 \"CLI Core ExternalPython Operators Other PlainAsserts PythonVenv Serialization WWW\",",
          "841:                 \"needs-mypy\": \"true\",",
          "843:             },",
          "844:             id=\"All tests except Providers and helm lint pre-commit \"",
          "845:             \"should run if core file changed in non-main branch\",",
          "",
          "[Removed Lines]",
          "842:                 \"mypy-packages\": \"['airflow']\",",
          "",
          "[Added Lines]",
          "842:                 \"mypy-folders\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 27 ---",
          "[Context before]",
          "880:                 \"skip-provider-tests\": \"true\",",
          "881:                 \"parallel-test-types-list-as-string\": None,",
          "882:                 \"needs-mypy\": \"false\",",
          "884:             },",
          "885:             id=\"Nothing should run if only non-important files changed\",",
          "886:         ),",
          "",
          "[Removed Lines]",
          "883:                 \"mypy-packages\": \"[]\",",
          "",
          "[Added Lines]",
          "883:                 \"mypy-folders\": \"[]\",",
          "",
          "---------------",
          "--- Hunk 28 ---",
          "[Context before]",
          "901:                 \"skip-provider-tests\": \"true\",",
          "902:                 \"parallel-test-types-list-as-string\": \"Always\",",
          "903:                 \"needs-mypy\": \"true\",",
          "905:             },",
          "906:             id=\"Only Always and docs build should run if only system tests changed\",",
          "907:         ),",
          "",
          "[Removed Lines]",
          "904:                 \"mypy-packages\": \"['airflow']\",",
          "",
          "[Added Lines]",
          "904:                 \"mypy-folders\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 29 ---",
          "[Context before]",
          "936:                 \"microsoft.azure,microsoft.mssql,mysql,openlineage,oracle,postgres,presto,salesforce,\"",
          "937:                 \"samba,sftp,ssh,trino] Providers[google]\",",
          "938:                 \"needs-mypy\": \"true\",",
          "940:             },",
          "941:             id=\"CLI tests and Google-related provider tests should run if cli/chart files changed but \"",
          "942:             \"prod image should be build too and k8s tests too\",",
          "",
          "[Removed Lines]",
          "939:                 \"mypy-packages\": \"['airflow', 'airflow/providers']\",",
          "",
          "[Added Lines]",
          "939:                 \"mypy-folders\": \"['airflow', 'providers']\",",
          "",
          "---------------",
          "--- Hunk 30 ---",
          "[Context before]",
          "964:                 \"skip-provider-tests\": \"true\",",
          "965:                 \"parallel-test-types-list-as-string\": \"API Always CLI Operators WWW\",",
          "966:                 \"needs-mypy\": \"true\",",
          "968:             },",
          "969:             id=\"No providers tests should run if only CLI/API/Operators/WWW file changed\",",
          "970:         ),",
          "",
          "[Removed Lines]",
          "967:                 \"mypy-packages\": \"['airflow']\",",
          "",
          "[Added Lines]",
          "967:                 \"mypy-folders\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 31 ---",
          "[Context before]",
          "986:                 \"skip-provider-tests\": \"false\",",
          "987:                 \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "988:                 \"needs-mypy\": \"true\",",
          "990:             },",
          "991:             id=\"Tests for all providers should run if model file changed\",",
          "992:         ),",
          "",
          "[Removed Lines]",
          "989:                 \"mypy-packages\": \"['airflow', 'airflow/providers']\",",
          "",
          "[Added Lines]",
          "989:                 \"mypy-folders\": \"['airflow', 'providers']\",",
          "",
          "---------------",
          "--- Hunk 32 ---",
          "[Context before]",
          "1008:                 \"skip-provider-tests\": \"false\",",
          "1009:                 \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "1010:                 \"needs-mypy\": \"true\",",
          "1012:             },",
          "1013:             id=\"Tests for all providers should run if any other than API/WWW/CLI/Operators file changed.\",",
          "1014:         ),",
          "",
          "[Removed Lines]",
          "1011:                 \"mypy-packages\": \"['airflow', 'airflow/providers']\",",
          "",
          "[Added Lines]",
          "1011:                 \"mypy-folders\": \"['airflow', 'providers']\",",
          "",
          "---------------",
          "--- Hunk 33 ---",
          "[Context before]",
          "1049:                 \"upgrade-to-newer-dependencies\": \"true\",",
          "1050:                 \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "1051:                 \"needs-mypy\": \"true\",",
          "1053:             },",
          "1054:             id=\"All tests run on push even if unimportant file changed\",",
          "1055:         ),",
          "",
          "[Removed Lines]",
          "1052:                 \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "1052:                 \"mypy-folders\": \"['airflow', 'providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 34 ---",
          "[Context before]",
          "1072:                 \"parallel-test-types-list-as-string\": \"API Always BranchExternalPython BranchPythonVenv \"",
          "1073:                 \"CLI Core ExternalPython Operators Other PlainAsserts PythonVenv Serialization WWW\",",
          "1074:                 \"needs-mypy\": \"true\",",
          "1076:             },",
          "1077:             id=\"All tests except Providers and Helm run on push\"",
          "1078:             \" even if unimportant file changed in non-main branch\",",
          "",
          "[Removed Lines]",
          "1075:                 \"mypy-packages\": \"['airflow', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "1075:                 \"mypy-folders\": \"['airflow', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 35 ---",
          "[Context before]",
          "1095:                 \"upgrade-to-newer-dependencies\": \"true\",",
          "1096:                 \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "1097:                 \"needs-mypy\": \"true\",",
          "1099:             },",
          "1100:             id=\"All tests run on push if core file changed\",",
          "1101:         ),",
          "",
          "[Removed Lines]",
          "1098:                 \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "1098:                 \"mypy-folders\": \"['airflow', 'providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 36 ---",
          "[Context before]",
          "1150:             else \"false\",",
          "1151:             \"parallel-test-types-list-as-string\": ALL_CI_SELECTIVE_TEST_TYPES,",
          "1152:             \"needs-mypy\": \"true\",",
          "1154:         },",
          "1155:         str(stderr),",
          "1156:     )",
          "",
          "[Removed Lines]",
          "1153:             \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "1153:             \"mypy-folders\": \"['airflow', 'providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 37 ---",
          "[Context before]",
          "1640:             (\"README.md\",),",
          "1641:             {",
          "1642:                 \"needs-mypy\": \"false\",",
          "1644:             },",
          "1645:             \"main\",",
          "1646:             (),",
          "",
          "[Removed Lines]",
          "1643:                 \"mypy-packages\": \"[]\",",
          "",
          "[Added Lines]",
          "1643:                 \"mypy-folders\": \"[]\",",
          "",
          "---------------",
          "--- Hunk 38 ---",
          "[Context before]",
          "1650:             (\"airflow/cli/file.py\",),",
          "1651:             {",
          "1652:                 \"needs-mypy\": \"true\",",
          "1654:             },",
          "1655:             \"main\",",
          "1656:             (),",
          "",
          "[Removed Lines]",
          "1653:                 \"mypy-packages\": \"['airflow']\",",
          "",
          "[Added Lines]",
          "1653:                 \"mypy-folders\": \"['airflow']\",",
          "",
          "---------------",
          "--- Hunk 39 ---",
          "[Context before]",
          "1660:             (\"airflow/models/file.py\",),",
          "1661:             {",
          "1662:                 \"needs-mypy\": \"true\",",
          "1664:             },",
          "1665:             \"main\",",
          "1666:             (),",
          "",
          "[Removed Lines]",
          "1663:                 \"mypy-packages\": \"['airflow', 'airflow/providers']\",",
          "",
          "[Added Lines]",
          "1663:                 \"mypy-folders\": \"['airflow', 'providers']\",",
          "",
          "---------------",
          "--- Hunk 40 ---",
          "[Context before]",
          "1670:             (\"airflow/providers/a_file.py\",),",
          "1671:             {",
          "1672:                 \"needs-mypy\": \"true\",",
          "1674:             },",
          "1675:             \"main\",",
          "1676:             (),",
          "",
          "[Removed Lines]",
          "1673:                 \"mypy-packages\": \"['airflow/providers']\",",
          "",
          "[Added Lines]",
          "1673:                 \"mypy-folders\": \"['providers']\",",
          "",
          "---------------",
          "--- Hunk 41 ---",
          "[Context before]",
          "1680:             (\"docs/a_file.py\",),",
          "1681:             {",
          "1682:                 \"needs-mypy\": \"true\",",
          "1684:             },",
          "1685:             \"main\",",
          "1686:             (),",
          "",
          "[Removed Lines]",
          "1683:                 \"mypy-packages\": \"['docs']\",",
          "",
          "[Added Lines]",
          "1683:                 \"mypy-folders\": \"['docs']\",",
          "",
          "---------------",
          "--- Hunk 42 ---",
          "[Context before]",
          "1690:             (\"dev/a_package/a_file.py\",),",
          "1691:             {",
          "1692:                 \"needs-mypy\": \"true\",",
          "1694:             },",
          "1695:             \"main\",",
          "1696:             (),",
          "",
          "[Removed Lines]",
          "1693:                 \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "1693:                 \"mypy-folders\": \"['airflow', 'providers', 'docs', 'dev']\",",
          "",
          "---------------",
          "--- Hunk 43 ---",
          "[Context before]",
          "1700:             (\"readme.md\",),",
          "1701:             {",
          "1702:                 \"needs-mypy\": \"true\",",
          "1704:             },",
          "1705:             \"main\",",
          "1706:             (\"full tests needed\",),",
          "",
          "[Removed Lines]",
          "1703:                 \"mypy-packages\": \"['airflow', 'airflow/providers', 'docs', 'dev']\",",
          "",
          "[Added Lines]",
          "1703:                 \"mypy-folders\": \"['airflow', 'providers', 'docs', 'dev']\",",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_mypy.py||scripts/ci/pre_commit/pre_commit_mypy.py": [
          "File: scripts/ci/pre_commit/pre_commit_mypy.py -> scripts/ci/pre_commit/pre_commit_mypy.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import os",
          "22: import sys",
          "23: from pathlib import Path",
          "",
          "[Removed Lines]",
          "21: import shlex",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "34: initialize_breeze_precommit(__name__, __file__)",
          "36: files_to_test = pre_process_files(sys.argv[1:])",
          "40: if files_to_test == [\"--namespace-packages\"] or files_to_test == []:",
          "41:     print(\"No files to tests. Quitting\")",
          "42:     sys.exit(0)",
          "",
          "[Removed Lines]",
          "37: mypy_packages = os.environ.get(\"MYPY_PACKAGES\")",
          "38: if mypy_packages:",
          "39:     files_to_test += shlex.split(mypy_packages)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "57: )",
          "58: ci_environment = os.environ.get(\"CI\")",
          "59: if res.returncode != 0:",
          "68:     upgrading = os.environ.get(\"UPGRADE_TO_NEWER_DEPENDENCIES\", \"false\") != \"false\"",
          "69:     if upgrading:",
          "70:         console.print(",
          "",
          "[Removed Lines]",
          "60:     if mypy_packages and ci_environment:",
          "61:         console.print(",
          "62:             \"[yellow]You are running mypy with the packages selected. If you want to\"",
          "63:             \"reproduce it locally, you need to run the following command:\\n\"",
          "64:         )",
          "65:         console.print(",
          "66:             f'MYPY_PACKAGES=\"{mypy_packages}\" pre-commit run --hook-stage manual mypy --all-files\\n'",
          "67:         )",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_mypy_folder.py||scripts/ci/pre_commit/pre_commit_mypy_folder.py": [
          "File: scripts/ci/pre_commit/pre_commit_mypy_folder.py -> scripts/ci/pre_commit/pre_commit_mypy_folder.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: import os",
          "21: import sys",
          "22: from pathlib import Path",
          "24: sys.path.insert(0, str(Path(__file__).parent.resolve()))",
          "26: from common_precommit_utils import (",
          "27:     console,",
          "28:     initialize_breeze_precommit,",
          "29:     run_command_via_breeze_shell,",
          "30: )",
          "32: initialize_breeze_precommit(__name__, __file__)",
          "34: ALLOWED_FOLDERS = [\"airflow\", \"airflow/providers\", \"dev\", \"docs\"]",
          "36: if len(sys.argv) < 2:",
          "37:     console.print(f\"[yellow]You need to specify the folder to test as parameter: {ALLOWED_FOLDERS}\\n\")",
          "38:     sys.exit(1)",
          "40: mypy_folder = sys.argv[1]",
          "41: if mypy_folder not in ALLOWED_FOLDERS:",
          "42:     console.print(f\"[yellow]Wrong folder {mypy_folder}. It should be one of those: {ALLOWED_FOLDERS}\\n\")",
          "43:     sys.exit(1)",
          "45: arguments = [mypy_folder]",
          "46: if mypy_folder == \"airflow/providers\":",
          "47:     arguments.append(\"--namespace-packages\")",
          "49: res = run_command_via_breeze_shell(",
          "50:     [",
          "51:         \"/opt/airflow/scripts/in_container/run_mypy.sh\",",
          "53:     ],",
          "54:     warn_image_upgrade_needed=True,",
          "55:     extra_env={",
          "56:         \"INCLUDE_MYPY_VOLUME\": \"true\",",
          "57:         # Need to mount local sources when running it - to not have to rebuild the image",
          "58:         # and to let CI work on it when running on PRs from forks - because mypy-dev uses files",
          "59:         # that are not available at the time when image is built in CI",
          "60:         \"MOUNT_SOURCES\": \"selected\",",
          "61:     },",
          "62: )",
          "63: ci_environment = os.environ.get(\"CI\")",
          "64: if res.returncode != 0:",
          "65:     if ci_environment:",
          "66:         console.print(",
          "67:             \"[yellow]You are running mypy with the folders selected. If you want to\"",
          "68:             \"reproduce it locally, you need to run the following command:\\n\"",
          "69:         )",
          "70:         console.print(\"pre-commit run --hook-stage manual mypy-<folder> --all-files\\n\")",
          "71:     upgrading = os.environ.get(\"UPGRADE_TO_NEWER_DEPENDENCIES\", \"false\") != \"false\"",
          "72:     if upgrading:",
          "73:         console.print(",
          "74:             \"[yellow]You are running mypy with the image that has dependencies upgraded automatically.\\n\"",
          "75:         )",
          "76:     flag = \" --upgrade-to-newer-dependencies\" if upgrading else \"\"",
          "77:     console.print(",
          "78:         \"[yellow]If you see strange stacktraces above, and can't reproduce it, please run\"",
          "79:         \" this command and try again:\\n\"",
          "80:     )",
          "81:     console.print(f\"breeze ci-image build --python 3.8{flag}\\n\")",
          "82:     console.print(\"[yellow]You can also run `breeze down --cleanup-mypy-cache` to clean up the cache used.\\n\")",
          "83: sys.exit(res.returncode)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c93209bf28995fd8155d28ba774d3fe1da33179f",
      "candidate_info": {
        "commit_hash": "c93209bf28995fd8155d28ba774d3fe1da33179f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c93209bf28995fd8155d28ba774d3fe1da33179f",
        "files": [
          "docs/apache-airflow/installation/dependencies.rst",
          "docs/apache-airflow/installation/prerequisites.rst"
        ],
        "message": "Update installation prerequisites after upgrading to Debian Bookworm (#36521)\n\nWe've migrated to Debian Bookworm for our images in #31941 but\nour installation prerequisites and example system level dependencies\nmention Bullseye. Since for 2.8 we run all tests on Bookworm but we\nalso suppport custom Bullseye imeage (to be removed likely in 2.9)\nwe should put Bookworm front and center but also explain differences\nwith Bullseye.\n\n(cherry picked from commit 9f90a655e884be0a99ee80564c41190260fa9ba1)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "c0ffa9c5d96625c68ded9562632674ed366b5eb3",
      "candidate_info": {
        "commit_hash": "c0ffa9c5d96625c68ded9562632674ed366b5eb3",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c0ffa9c5d96625c68ded9562632674ed366b5eb3",
        "files": [
          "RELEASE_NOTES.rst",
          "airflow/reproducible_build.yaml",
          "newsfragments/36281.significant.rst",
          "newsfragments/36537.significant.rst",
          "newsfragments/36647.significant.rst",
          "newsfragments/config.toml"
        ],
        "message": "Update RELEASE_NOTES.rst",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "cf83862ccb502f22b4294a813b792cb8b068ddbd",
      "candidate_info": {
        "commit_hash": "cf83862ccb502f22b4294a813b792cb8b068ddbd",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cf83862ccb502f22b4294a813b792cb8b068ddbd",
        "files": [
          ".rat-excludes",
          "3rd-party-licenses/LICENSE-reproducible.txt",
          "BREEZE.rst",
          "dev/README_RELEASE_AIRFLOW.md",
          "dev/breeze/src/airflow_breeze/commands/release_candidate_command.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/utils/reproducible.py",
          "images/breeze/output_release-management.svg",
          "images/breeze/output_release-management.txt",
          "images/breeze/output_release-management_prepare-airflow-package.svg",
          "images/breeze/output_release-management_prepare-airflow-package.txt",
          "images/breeze/output_release-management_prepare-airflow-tarball.svg",
          "images/breeze/output_release-management_prepare-airflow-tarball.txt",
          "images/breeze/output_setup_check-all-params-in-groups.svg",
          "images/breeze/output_setup_check-all-params-in-groups.txt",
          "images/breeze/output_setup_regenerate-command-images.svg",
          "images/breeze/output_setup_regenerate-command-images.txt",
          "scripts/in_container/run_prepare_airflow_packages.py"
        ],
        "message": "Update Airflow release process to include reproducible tarballs (#36744)\n\nSource tarball is the main artifact produced by the release\nprocess - one that is the \"official\" release and named like that\nby the Apache Software Foundation.\n\nThis PR makes the source tarball generation reproducible - following\nreproducibility of the `.whl` and `sdist` packages.\n\nThis change adds:\n\n* vendors-in reproducible.py script that repacks .tar.gz package\n  in reproducible way using source-date-epoch as timestamps\n* breeze release-management prepare-airflow-tarball command\n* adds verification of the tarballs to PMC verification process\n* adds --use-local-hatch for package building command to allow for\n  faster / non-docker build of packages for PMC verification\n* improves diagnostic output of the release and build commands\n\n(cherry picked from commit 72a571dc6d21d90f92d5ce683a5d40c6a527fcb0)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/release_candidate_command.py||dev/breeze/src/airflow_breeze/commands/release_candidate_command.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/utils/reproducible.py||dev/breeze/src/airflow_breeze/utils/reproducible.py",
          "scripts/in_container/run_prepare_airflow_packages.py||scripts/in_container/run_prepare_airflow_packages.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/release_candidate_command.py||dev/breeze/src/airflow_breeze/commands/release_candidate_command.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_candidate_command.py -> dev/breeze/src/airflow_breeze/commands/release_candidate_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: from __future__ import annotations",
          "19: import os",
          "21: import click",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import shutil",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "25: from airflow_breeze.utils.confirm import confirm_action",
          "26: from airflow_breeze.utils.console import console_print",
          "27: from airflow_breeze.utils.path_utils import AIRFLOW_SOURCES_ROOT",
          "28: from airflow_breeze.utils.run_utils import run_command",
          "30: CI = os.environ.get(\"CI\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "29: from airflow_breeze.utils.reproducible import archive_deterministically, get_source_date_epoch",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "59: def git_tag(version):",
          "60:     if confirm_action(f\"Tag {version}?\"):",
          "61:         run_command([\"git\", \"tag\", \"-s\", f\"{version}\", \"-m\", f\"Apache Airflow {version}\"], check=True)",
          "65: def git_clean():",
          "66:     if confirm_action(\"Clean git repo?\"):",
          "67:         run_command([\"breeze\", \"ci\", \"fix-ownership\"], dry_run_override=DRY_RUN, check=True)",
          "68:         run_command([\"git\", \"clean\", \"-fxd\"], dry_run_override=DRY_RUN, check=True)",
          "77:         run_command(",
          "78:             [",
          "79:                 \"git\",",
          "",
          "[Removed Lines]",
          "62:         console_print(\"Tagged\")",
          "69:         console_print(\"Git repo cleaned\")",
          "72: def tarball_release(version, version_without_rc):",
          "73:     if confirm_action(\"Create tarball?\"):",
          "74:         run_command([\"rm\", \"-rf\", \"dist\"], check=True)",
          "76:         run_command([\"mkdir\", \"dist\"], check=True)",
          "",
          "[Added Lines]",
          "64:         console_print(\"[success]Tagged\")",
          "71:         console_print(\"[success]Git repo cleaned\")",
          "74: DIST_DIR = AIRFLOW_SOURCES_ROOT / \"dist\"",
          "75: OUT_DIR = AIRFLOW_SOURCES_ROOT / \"out\"",
          "76: REPRODUCIBLE_DIR = OUT_DIR / \"reproducible\"",
          "79: def tarball_release(version: str, version_without_rc: str, source_date_epoch: int):",
          "80:     if confirm_action(\"Create tarball?\"):",
          "81:         console_print(f\"[info]Creating tarball for Airflow {version}\")",
          "82:         shutil.rmtree(OUT_DIR, ignore_errors=True)",
          "83:         DIST_DIR.mkdir(exist_ok=True)",
          "84:         OUT_DIR.mkdir(exist_ok=True)",
          "85:         REPRODUCIBLE_DIR.mkdir(exist_ok=True)",
          "86:         archive_name = f\"apache-airflow-{version_without_rc}-source.tar.gz\"",
          "87:         temporary_archive = OUT_DIR / archive_name",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "82:                 f\"{version}\",",
          "83:                 f\"--prefix=apache-airflow-{version_without_rc}/\",",
          "84:                 \"-o\",",
          "86:             ],",
          "87:             check=True,",
          "88:         )",
          "98:     run_command(",
          "99:         [",
          "100:             \"breeze\",",
          "",
          "[Removed Lines]",
          "85:                 f\"dist/apache-airflow-{version_without_rc}-source.tar.gz\",",
          "89:         console_print(\"Tarball created\")",
          "92: def create_artifacts_with_sdist():",
          "93:     run_command([\"hatch\", \"build\", \"-t\", \"sdist\", \"-t\", \"wheel\"], check=True)",
          "94:     console_print(\"Artifacts created\")",
          "97: def create_artifacts_with_breeze():",
          "",
          "[Added Lines]",
          "96:                 temporary_archive.as_posix(),",
          "100:         run_command(",
          "101:             [",
          "102:                 \"tar\",",
          "103:                 \"-xf\",",
          "104:                 temporary_archive.as_posix(),",
          "105:                 \"-C\",",
          "106:                 REPRODUCIBLE_DIR.as_posix(),",
          "107:                 \"--strip\",",
          "108:                 \"1\",",
          "109:             ]",
          "110:         )",
          "111:         final_archive = DIST_DIR / archive_name",
          "112:         archive_deterministically(",
          "113:             dir_to_archive=REPRODUCIBLE_DIR.as_posix(),",
          "114:             dest_archive=final_archive.as_posix(),",
          "115:             prepend_path=None,",
          "116:             timestamp=source_date_epoch,",
          "117:         )",
          "118:         console_print(f\"[success]Tarball created in {final_archive}\")",
          "121: def create_artifacts_with_hatch(source_date_epoch: int):",
          "122:     console_print(\"[info]Creating artifacts with hatch\")",
          "123:     shutil.rmtree(DIST_DIR, ignore_errors=True)",
          "124:     DIST_DIR.mkdir(exist_ok=True)",
          "125:     env_copy = os.environ.copy()",
          "126:     env_copy[\"SOURCE_DATE_EPOCH\"] = str(source_date_epoch)",
          "127:     run_command(",
          "128:         [\"hatch\", \"build\", \"-c\", \"-t\", \"custom\", \"-t\", \"sdist\", \"-t\", \"wheel\"], check=True, env=env_copy",
          "129:     )",
          "130:     console_print(\"[success]Successfully prepared Airflow packages:\")",
          "131:     for file in sorted(DIST_DIR.glob(\"apache_airflow*\")):",
          "132:         console_print(print(file.name))",
          "133:     console_print()",
          "136: def create_artifacts_with_docker():",
          "137:     console_print(\"[info]Creating artifacts with docker\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "105:         ],",
          "106:         check=True,",
          "107:     )",
          "111: def sign_the_release(repo_root):",
          "112:     if confirm_action(\"Do you want to sign the release?\"):",
          "113:         os.chdir(f\"{repo_root}/dist\")",
          "114:         run_command(\"./../dev/sign.sh *\", dry_run_override=DRY_RUN, check=True, shell=True)",
          "118: def tag_and_push_constraints(version, version_branch):",
          "",
          "[Removed Lines]",
          "108:     console_print(\"Artifacts created\")",
          "115:         console_print(\"Release signed\")",
          "",
          "[Added Lines]",
          "148:     console_print(\"[success]Artifacts created\")",
          "155:         console_print(\"[success]Release signed\")",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "135:         run_command(",
          "136:             [\"git\", \"push\", \"origin\", \"tag\", f\"constraints-{version}\"], dry_run_override=DRY_RUN, check=True",
          "137:         )",
          "141: def clone_asf_repo(version, repo_root):",
          "",
          "[Removed Lines]",
          "138:         console_print(\"Constraints tagged and pushed\")",
          "",
          "[Added Lines]",
          "178:         console_print(\"[success]Constraints tagged and pushed\")",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "146:             check=True,",
          "147:         )",
          "148:         run_command([\"svn\", \"update\", \"--set-depth=infinity\", \"asf-dist/dev/airflow\"], check=True)",
          "152: def move_artifacts_to_svn(version, repo_root):",
          "",
          "[Removed Lines]",
          "149:         console_print(\"Cloned ASF repo successfully\")",
          "",
          "[Added Lines]",
          "189:         console_print(\"[success]Cloned ASF repo successfully\")",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "154:         os.chdir(f\"{repo_root}/asf-dist/dev/airflow\")",
          "155:         run_command([\"svn\", \"mkdir\", f\"{version}\"], dry_run_override=DRY_RUN, check=True)",
          "156:         run_command(f\"mv {repo_root}/dist/* {version}/\", dry_run_override=DRY_RUN, check=True, shell=True)",
          "158:         run_command([\"ls\"], dry_run_override=DRY_RUN)",
          "",
          "[Removed Lines]",
          "157:         console_print(\"Moved artifacts to SVN:\")",
          "",
          "[Added Lines]",
          "197:         console_print(\"[success]Moved artifacts to SVN:\")",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "171:             dry_run_override=DRY_RUN,",
          "172:             check=True,",
          "173:         )",
          "177: def delete_asf_repo(repo_root):",
          "",
          "[Removed Lines]",
          "174:         console_print(\"Files pushed to svn\")",
          "",
          "[Added Lines]",
          "214:         console_print(\"[success]Files pushed to svn\")",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "183: def prepare_pypi_packages(version, version_suffix, repo_root):",
          "184:     if confirm_action(\"Prepare pypi packages?\"):",
          "186:         os.chdir(repo_root)",
          "187:         run_command([\"git\", \"checkout\", f\"{version}\"], dry_run_override=DRY_RUN, check=True)",
          "188:         run_command(",
          "",
          "[Removed Lines]",
          "185:         console_print(\"Preparing PyPI packages\")",
          "",
          "[Added Lines]",
          "225:         console_print(\"[info]Preparing PyPI packages\")",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "198:             check=True,",
          "199:         )",
          "200:         run_command([\"twine\", \"check\", \"dist/*\"], check=True)",
          "204: def push_packages_to_pypi(version):",
          "205:     if confirm_action(\"Do you want to push packages to production PyPI?\"):",
          "206:         run_command([\"twine\", \"upload\", \"-r\", \"pypi\", \"dist/*\"], dry_run_override=DRY_RUN, check=True)",
          "208:         console_print(",
          "209:             \"Again, confirm that the package is available here: https://pypi.python.org/pypi/apache-airflow\"",
          "210:         )",
          "",
          "[Removed Lines]",
          "201:         console_print(\"PyPI packages prepared\")",
          "207:         console_print(\"Packages pushed to production PyPI\")",
          "",
          "[Added Lines]",
          "241:         console_print(\"[success]PyPI packages prepared\")",
          "247:         console_print(\"[success]Packages pushed to production PyPI\")",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "240:         )",
          "241:         confirm_action(f\"Confirm that {version} is pushed to PyPI(not PyPI test). Is it pushed?\", abort=True)",
          "242:         run_command([\"git\", \"push\", \"origin\", \"tag\", f\"{version}\"], dry_run_override=DRY_RUN, check=True)",
          "246: def create_issue_for_testing(version, previous_version, github_token):",
          "",
          "[Removed Lines]",
          "243:         console_print(\"Release candidate tag pushed to GitHub\")",
          "",
          "[Added Lines]",
          "283:         console_print(\"[success]Release candidate tag pushed to GitHub\")",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "293:                 dry_run_override=DRY_RUN,",
          "294:                 check=True,",
          "295:             )",
          "297:     os.chdir(repo_root)",
          "300: @release_management.command(",
          "301:     name=\"start-rc-process\",",
          "302:     short_help=\"Start RC process\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "336:     console_print(\"[success]Old releases removed\")",
          "340: @release_management.command(",
          "341:     name=\"prepare-airflow-tarball\",",
          "342:     help=\"Prepare airflow's source tarball.\",",
          "343: )",
          "344: @click.option(",
          "345:     \"--version\", required=True, help=\"The release candidate version e.g. 2.4.3rc1\", envvar=\"VERSION\"",
          "346: )",
          "347: def prepare_airflow_tarball(version: str):",
          "348:     from packaging.version import Version",
          "350:     airflow_version = Version(version)",
          "351:     if not airflow_version.is_prerelease:",
          "352:         exit(\"--version value must be a pre-release\")",
          "353:     source_date_epoch = get_source_date_epoch()",
          "354:     version_without_rc = airflow_version.base_version",
          "355:     # Create the tarball",
          "356:     tarball_release(",
          "357:         version=version, version_without_rc=version_without_rc, source_date_epoch=source_date_epoch",
          "358:     )",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "311: def publish_release_candidate(version, previous_version, github_token):",
          "312:     from packaging.version import Version",
          "315:         exit(\"--version value must be a pre-release\")",
          "316:     if Version(previous_version).is_prerelease:",
          "317:         exit(\"--previous-version value must be a release not a pre-release\")",
          "",
          "[Removed Lines]",
          "314:     if not Version(version).is_prerelease:",
          "",
          "[Added Lines]",
          "375:     airflow_version = Version(version)",
          "376:     if not airflow_version.is_prerelease:",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "320:         if not github_token:",
          "321:             console_print(\"GITHUB_TOKEN is not set! Issue generation will fail.\")",
          "322:             confirm_action(\"Do you want to continue?\", abort=True)",
          "326:     os.chdir(AIRFLOW_SOURCES_ROOT)",
          "327:     airflow_repo_root = os.getcwd()",
          "",
          "[Removed Lines]",
          "323:     version_suffix = version[5:]",
          "324:     version_branch = version[:3].replace(\".\", \"-\")",
          "325:     version_without_rc = version[:5]",
          "",
          "[Added Lines]",
          "386:     version_suffix = airflow_version.pre[0] + str(airflow_version.pre[1])",
          "387:     version_branch = str(airflow_version.release[0]) + \"-\" + str(airflow_version.release[1])",
          "388:     version_without_rc = airflow_version.base_version",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "343:     confirm_action(\"Pushes will be made to origin. Do you want to continue?\", abort=True)",
          "344:     # Merge the sync PR",
          "345:     merge_pr(version_branch)",
          "348:     git_tag(version)",
          "349:     git_clean()",
          "353:     # Create the tarball",
          "355:     # Create the artifacts",
          "358:     elif confirm_action(\"Use hatch to create artifacts?\"):",
          "360:     # Sign the release",
          "361:     sign_the_release(airflow_repo_root)",
          "362:     # Tag and push constraints",
          "",
          "[Removed Lines]",
          "347:     # Tag & clean the repo",
          "350:     # Build the latest image",
          "351:     if confirm_action(\"Build latest breeze image?\"):",
          "352:         run_command([\"breeze\", \"ci-image\", \"build\", \"--python\", \"3.8\"], dry_run_override=DRY_RUN, check=True)",
          "354:     tarball_release(version, version_without_rc)",
          "356:     if confirm_action(\"Use breeze to create artifacts?\"):",
          "357:         create_artifacts_with_breeze()",
          "359:         create_artifacts_with_sdist()",
          "",
          "[Added Lines]",
          "409:     #",
          "410:     # # Tag & clean the repo",
          "413:     source_date_epoch = get_source_date_epoch()",
          "414:     shutil.rmtree(DIST_DIR, ignore_errors=True)",
          "416:     tarball_release(",
          "417:         version=version, version_without_rc=version_without_rc, source_date_epoch=source_date_epoch",
          "418:     )",
          "420:     if confirm_action(\"Use docker to create artifacts?\"):",
          "421:         create_artifacts_with_docker()",
          "423:         create_artifacts_with_hatch()",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "142:     get_related_providers,",
          "143: )",
          "144: from airflow_breeze.utils.python_versions import get_python_version_list",
          "145: from airflow_breeze.utils.run_utils import (",
          "146:     run_command,",
          "147: )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "145: from airflow_breeze.utils.reproducible import get_source_date_epoch",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "282:     name=\"prepare-airflow-package\",",
          "283:     help=\"Prepare sdist/whl package of Airflow.\",",
          "284: )",
          "285: @option_package_format",
          "286: @option_version_suffix_for_pypi",
          "287: @option_verbose",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "286: @click.option(",
          "287:     \"--use-local-hatch\",",
          "288:     is_flag=True,",
          "289:     help=\"Use local hatch instead of docker to build the package. You need to have hatch installed.\",",
          "290: )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "289: def prepare_airflow_packages(",
          "290:     package_format: str,",
          "291:     version_suffix_for_pypi: str,",
          "292: ):",
          "293:     perform_environment_checks()",
          "294:     fix_ownership_using_docker()",
          "295:     cleanup_python_generated_files()",
          "296:     # This is security feature.",
          "297:     #",
          "298:     # Building the image needed to build airflow package including .git directory",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "298:     use_local_hatch: bool,",
          "303:     source_date_epoch = get_source_date_epoch()",
          "304:     if use_local_hatch:",
          "305:         hatch_build_command = [\"hatch\", \"build\", \"-c\", \"-t\", \"custom\"]",
          "306:         if package_format in [\"sdist\", \"both\"]:",
          "307:             hatch_build_command.extend([\"-t\", \"sdist\"])",
          "308:         if package_format in [\"wheel\", \"both\"]:",
          "309:             hatch_build_command.extend([\"-t\", \"wheel\"])",
          "310:         env_copy = os.environ.copy()",
          "311:         env_copy[\"SOURCE_DATE_EPOCH\"] = str(source_date_epoch)",
          "312:         run_command(",
          "313:             hatch_build_command,",
          "314:             check=True,",
          "315:             env=env_copy,",
          "316:         )",
          "317:         get_console().print(\"[success]Successfully prepared Airflow packages:\")",
          "318:         for file in sorted(DIST_DIR.glob(\"apache_airflow*\")):",
          "319:             get_console().print(file.name)",
          "320:         get_console().print()",
          "321:         return",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "350:     # Copy all files in the dist directory in container to the host dist directory (note '/.' in SRC)",
          "351:     run_command([\"docker\", \"cp\", f\"{container_id}:/opt/airflow/dist/.\", \"./dist\"], check=True)",
          "352:     run_command([\"docker\", \"rm\", \"--force\", container_id], check=True)",
          "356: def provider_action_summary(description: str, message_type: MessageType, packages: list[str]):",
          "",
          "[Removed Lines]",
          "353:     get_console().print(\"[success]Successfully prepared Airflow package!\\n\\n\")",
          "",
          "[Added Lines]",
          "379:     get_console().print(\"[success]Successfully prepared Airflow packages:\")",
          "380:     for file in sorted(DIST_DIR.glob(\"apache_airflow*\")):",
          "381:         get_console().print(file.name)",
          "382:     get_console().print()",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: RELEASE_AIRFLOW_COMMANDS: dict[str, str | list[str]] = {",
          "20:     \"name\": \"Airflow release commands\",",
          "21:     \"commands\": [",
          "23:         \"create-minor-branch\",",
          "24:         \"start-rc-process\",",
          "25:         \"start-release\",",
          "26:         \"release-prod-images\",",
          "",
          "[Removed Lines]",
          "22:         \"prepare-airflow-package\",",
          "",
          "[Added Lines]",
          "23:         \"prepare-airflow-package\",",
          "24:         \"prepare-airflow-tarball\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "43: RELEASE_OTHER_COMMANDS: dict[str, str | list[str]] = {",
          "44:     \"name\": \"Other release commands\",",
          "45:     \"commands\": [",
          "46:         \"publish-docs\",",
          "47:         \"generate-constraints\",",
          "49:     ],",
          "50: }",
          "",
          "[Removed Lines]",
          "48:         \"add-back-references\",",
          "",
          "[Added Lines]",
          "47:         \"add-back-references\",",
          "50:         \"update-constraints\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "55:             \"name\": \"Package flags\",",
          "56:             \"options\": [",
          "57:                 \"--package-format\",",
          "58:                 \"--version-suffix-for-pypi\",",
          "59:             ],",
          "60:         }",
          "61:     ],",
          "62:     \"breeze release-management verify-provider-packages\": [",
          "63:         {",
          "64:             \"name\": \"Provider verification flags\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "60:                 \"--use-local-hatch\",",
          "65:     \"breeze release-management prepare-airflow-tarball\": [",
          "66:         {",
          "67:             \"name\": \"Package flags\",",
          "68:             \"options\": [",
          "69:                 \"--version\",",
          "70:             ],",
          "71:         }",
          "72:     ],",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/reproducible.py||dev/breeze/src/airflow_breeze/utils/reproducible.py": [
          "File: dev/breeze/src/airflow_breeze/utils/reproducible.py -> dev/breeze/src/airflow_breeze/utils/reproducible.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python3",
          "4: # Licensed to the Apache Software Foundation (ASF) under one",
          "5: # or more contributor license agreements.  See the NOTICE file",
          "6: # distributed with this work for additional information",
          "7: # regarding copyright ownership.  The ASF licenses this file",
          "8: # to you under the Apache License, Version 2.0 (the",
          "9: # \"License\"); you may not use this file except in compliance",
          "10: # with the License.  You may obtain a copy of the License at",
          "11: #",
          "12: #   http://www.apache.org/licenses/LICENSE-2.0",
          "13: #",
          "14: # Unless required by applicable law or agreed to in writing,",
          "15: # software distributed under the License is distributed on an",
          "16: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "17: # KIND, either express or implied.  See the License for the",
          "18: # specific language governing permissions and limitations",
          "19: # under the License.",
          "21: # Copyright 2013 The Servo Project Developers.",
          "22: # Copyright 2017 zerolib Developers.",
          "23: #",
          "24: # Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or",
          "25: # http://www.apache.org/licenses/LICENSE-2.0> or the MIT license",
          "26: # <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your",
          "27: # option. This file may not be copied, modified, or distributed",
          "28: # except according to those terms.",
          "30: # This command is a largely vendored-in script from",
          "31: # https://github.com/MuxZeroNet/reproducible/blob/master/reproducible.py",
          "32: from __future__ import annotations",
          "34: import contextlib",
          "35: import gzip",
          "36: import itertools",
          "37: import locale",
          "38: import os",
          "39: import tarfile",
          "40: from argparse import ArgumentParser",
          "42: from airflow_breeze.utils.path_utils import AIRFLOW_SOURCES_ROOT",
          "45: def get_source_date_epoch():",
          "46:     import yaml",
          "48:     reproducible_build_yaml = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"reproducible_build.yaml\"",
          "49:     reproducible_build_dict = yaml.safe_load(reproducible_build_yaml.read_text())",
          "50:     source_date_epoch: int = reproducible_build_dict[\"source-date-epoch\"]",
          "51:     return source_date_epoch",
          "54: @contextlib.contextmanager",
          "55: def cd(new_path):",
          "56:     \"\"\"Context manager for changing the current working directory\"\"\"",
          "57:     previous_path = os.getcwd()",
          "58:     try:",
          "59:         os.chdir(new_path)",
          "60:         yield",
          "61:     finally:",
          "62:         os.chdir(previous_path)",
          "65: @contextlib.contextmanager",
          "66: def setlocale(name):",
          "67:     \"\"\"Context manager for changing the current locale\"\"\"",
          "68:     saved_locale = locale.setlocale(locale.LC_ALL)",
          "69:     try:",
          "70:         yield locale.setlocale(locale.LC_ALL, name)",
          "71:     finally:",
          "72:         locale.setlocale(locale.LC_ALL, saved_locale)",
          "75: def archive_deterministically(dir_to_archive, dest_archive, prepend_path=None, timestamp=0):",
          "76:     \"\"\"Create a .tar.gz archive in a deterministic (reproducible) manner.",
          "78:     See https://reproducible-builds.org/docs/archives/ for more details.\"\"\"",
          "80:     def reset(tarinfo):",
          "81:         \"\"\"Helper to reset owner/group and modification time for tar entries\"\"\"",
          "82:         tarinfo.uid = tarinfo.gid = 0",
          "83:         tarinfo.uname = tarinfo.gname = \"root\"",
          "84:         tarinfo.mtime = timestamp",
          "85:         return tarinfo",
          "87:     dest_archive = os.path.abspath(dest_archive)",
          "88:     with cd(dir_to_archive):",
          "89:         current_dir = \".\"",
          "90:         file_list = [current_dir]",
          "91:         for root, dirs, files in os.walk(current_dir):",
          "92:             for name in itertools.chain(dirs, files):",
          "93:                 file_list.append(os.path.join(root, name))",
          "95:         # Sort file entries with the fixed locale",
          "96:         with setlocale(\"C\"):",
          "97:             file_list.sort(key=locale.strxfrm)",
          "99:         # Use a temporary file and atomic rename to avoid partially-formed",
          "100:         # packaging (in case of exceptional situations like running out of disk space).",
          "101:         temp_file = f\"{dest_archive}.temp~\"",
          "102:         with os.fdopen(os.open(temp_file, os.O_WRONLY | os.O_CREAT, 0o644), \"wb\") as out_file:",
          "103:             with gzip.GzipFile(\"wb\", fileobj=out_file, mtime=0) as gzip_file:",
          "104:                 with tarfile.open(fileobj=gzip_file, mode=\"w:\") as tar_file:",
          "105:                     for entry in file_list:",
          "106:                         arcname = entry",
          "107:                         if prepend_path is not None:",
          "108:                             arcname = os.path.normpath(os.path.join(prepend_path, arcname))",
          "109:                         tar_file.add(entry, filter=reset, recursive=False, arcname=arcname)",
          "110:         os.rename(temp_file, dest_archive)",
          "113: def main():",
          "114:     parser = ArgumentParser()",
          "115:     parser.add_argument(\"-d\", \"--dir\", help=\"directory to archive\")",
          "116:     parser.add_argument(\"-o\", \"--out\", help=\"archive destination\")",
          "117:     parser.add_argument(\"-p\", \"--prepend\", help=\"prepend path\")",
          "118:     parser.add_argument(",
          "119:         \"-t\", \"--timestamp\", help=\"timestamp of files\", type=int, default=get_source_date_epoch()",
          "120:     )",
          "122:     args = parser.parse_args()",
          "124:     if not args.dir or not args.out:",
          "125:         error = (",
          "126:             \"You should provide a directory to archive, and the \"",
          "127:             f\"archive file name, not {repr((args.dir, args.out))}\"",
          "128:         )",
          "129:         raise ValueError(error)",
          "131:     archive_deterministically(args.dir, args.out, args.prepend, args.timestamp)",
          "134: if __name__ == \"__main__\":",
          "135:     main()",
          "",
          "---------------"
        ],
        "scripts/in_container/run_prepare_airflow_packages.py||scripts/in_container/run_prepare_airflow_packages.py": [
          "File: scripts/in_container/run_prepare_airflow_packages.py -> scripts/in_container/run_prepare_airflow_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "78: def build_airflow_packages(package_format: str):",
          "79:     build_command = [sys.executable, \"-m\", \"hatch\", \"build\", \"-t\", \"custom\"]",
          "83:     if package_format in [\"both\", \"sdist\"]:",
          "84:         build_command.extend([\"-t\", \"sdist\"])",
          "86:     reproducible_date = yaml.safe_load(REPRODUCIBLE_BUILD_FILE.read_text())[\"source-date-epoch\"]",
          "",
          "[Removed Lines]",
          "81:     if package_format in [\"both\", \"wheel\"]:",
          "82:         build_command.extend([\"-t\", \"wheel\"])",
          "",
          "[Added Lines]",
          "82:     if package_format in [\"both\", \"wheel\"]:",
          "83:         build_command.extend([\"-t\", \"wheel\"])",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "67d8636cfc8f43f07303905f08a4d5561e998330",
      "candidate_info": {
        "commit_hash": "67d8636cfc8f43f07303905f08a4d5561e998330",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/67d8636cfc8f43f07303905f08a4d5561e998330",
        "files": [
          "airflow/providers/telegram/provider.yaml",
          "generated/provider_dependencies.json"
        ],
        "message": "Bump min telegram-bot version (#36670)\n\nThis is the last one from the long-backtracking series.\n\nTelegram 20.2 has been released in March 2023 and for all practical\npurposes using recent version is a good idea to interact with such\nservices. Bumping it cuts down on a number of backtracking loops\npip has to do when backtracking.\n\n(cherry picked from commit 1f6d764334925e0efb417eebc872e129d03fbf1a)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "81b23317dde8ddf68295725131e4956c10146d5d",
      "candidate_info": {
        "commit_hash": "81b23317dde8ddf68295725131e4956c10146d5d",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/81b23317dde8ddf68295725131e4956c10146d5d",
        "files": [
          "airflow/jobs/backfill_job_runner.py"
        ],
        "message": "Refactor _manage_executor_state by refreshing TIs in batch (#36502)\n\nRefactor _manage_executor_state by refreshing TIs in batch (#36418)\" (#36500)\"\n\nHandle Microsoft SQL Server\n\n(cherry picked from commit 9cf5f6f08483ff141df51c07daa91a0aa34906ec)",
        "before_after_code_files": [
          "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py": [
          "File: airflow/jobs/backfill_job_runner.py -> airflow/jobs/backfill_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import attr",
          "24: import pendulum",
          "26: from sqlalchemy.exc import OperationalError",
          "27: from sqlalchemy.orm.session import make_transient",
          "28: from tabulate import tabulate",
          "",
          "[Removed Lines]",
          "25: from sqlalchemy import select, update",
          "",
          "[Added Lines]",
          "25: from sqlalchemy import select, tuple_, update",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "264:         :return: An iterable of expanded TaskInstance per MappedTask",
          "265:         \"\"\"",
          "266:         executor = self.job.executor",
          "270:             state, info = value",
          "272:                 self.log.warning(\"%s state %s not in running=%s\", key, state, running.values())",
          "273:                 continue",
          "278:             self.log.debug(\"Executor state: %s task %s\", state, ti)",
          "",
          "[Removed Lines]",
          "268:         # TODO: query all instead of refresh from db",
          "269:         for key, value in list(executor.get_event_buffer().items()):",
          "271:             if key not in running:",
          "275:             ti = running[key]",
          "276:             ti.refresh_from_db()",
          "",
          "[Added Lines]",
          "267:         # list of tuples (dag_id, task_id, execution_date, map_index) of running tasks in executor",
          "268:         buffered_events = list(executor.get_event_buffer().items())",
          "269:         if session.get_bind().dialect.name == \"mssql\":",
          "270:             # SQL Server doesn't support multiple column subqueries",
          "271:             # TODO: Remove this once we drop support for SQL Server (#35868)",
          "272:             need_refresh = True",
          "273:             running_dict = {(ti.dag_id, ti.task_id, ti.run_id, ti.map_index): ti for ti in running.values()}",
          "274:         else:",
          "275:             running_tis_ids = [",
          "276:                 (key.dag_id, key.task_id, key.run_id, key.map_index)",
          "277:                 for key, _ in buffered_events",
          "278:                 if key in running",
          "279:             ]",
          "280:             # list of TaskInstance of running tasks in executor (refreshed from db in batch)",
          "281:             refreshed_running_tis = session.scalars(",
          "282:                 select(TaskInstance).where(",
          "283:                     tuple_(",
          "284:                         TaskInstance.dag_id,",
          "285:                         TaskInstance.task_id,",
          "286:                         TaskInstance.run_id,",
          "287:                         TaskInstance.map_index,",
          "288:                     ).in_(running_tis_ids)",
          "289:                 )",
          "290:             ).all()",
          "291:             # dict of refreshed TaskInstance by key to easily find them",
          "292:             running_dict = {",
          "293:                 (ti.dag_id, ti.task_id, ti.run_id, ti.map_index): ti for ti in refreshed_running_tis",
          "294:             }",
          "295:             need_refresh = False",
          "297:         for key, value in buffered_events:",
          "299:             ti_key = (key.dag_id, key.task_id, key.run_id, key.map_index)",
          "300:             if ti_key not in running_dict:",
          "304:             ti = running_dict[ti_key]",
          "305:             if need_refresh:",
          "306:                 ti.refresh_from_db(session=session)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "48afea83e6c692258a266930cc76df8950e80f4e",
      "candidate_info": {
        "commit_hash": "48afea83e6c692258a266930cc76df8950e80f4e",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/48afea83e6c692258a266930cc76df8950e80f4e",
        "files": [
          "airflow/example_dags/plugins/workday.py",
          "tests/plugins/workday.py"
        ],
        "message": "Straighten typing in workday timetable (#36296)\n\n(cherry picked from commit 954bb60e876b7cbb491ec7542ecdbb6bb9b8ab03)",
        "before_after_code_files": [
          "airflow/example_dags/plugins/workday.py||airflow/example_dags/plugins/workday.py",
          "tests/plugins/workday.py||tests/plugins/workday.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/example_dags/plugins/workday.py||airflow/example_dags/plugins/workday.py": [
          "File: airflow/example_dags/plugins/workday.py -> airflow/example_dags/plugins/workday.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "73:     ) -> DagRunInfo | None:",
          "74:         if last_automated_data_interval is not None:  # There was a previous run on the regular schedule.",
          "75:             last_start = last_automated_data_interval.start",
          "91:         # Skip weekends and holidays",
          "94:         if restriction.latest is not None and next_start > restriction.latest:",
          "95:             return None  # Over the DAG's scheduled end; don't schedule.",
          "",
          "[Removed Lines]",
          "76:             next_start = DateTime.combine((last_start + timedelta(days=1)).date(), Time.min).replace(",
          "77:                 tzinfo=UTC",
          "78:             )",
          "79:         else:  # This is the first ever run on the regular schedule.",
          "80:             next_start = restriction.earliest",
          "81:             if next_start is None:  # No start_date. Don't schedule.",
          "82:                 return None",
          "83:             if not restriction.catchup:",
          "84:                 # If the DAG has catchup=False, today is the earliest to consider.",
          "85:                 next_start = max(next_start, DateTime.combine(Date.today(), Time.min).replace(tzinfo=UTC))",
          "86:             elif next_start.time() != Time.min:",
          "87:                 # If earliest does not fall on midnight, skip to the next day.",
          "88:                 next_start = DateTime.combine(next_start.date() + timedelta(days=1), Time.min).replace(",
          "89:                     tzinfo=UTC",
          "90:                 )",
          "92:         next_start = self.get_next_workday(next_start)",
          "",
          "[Added Lines]",
          "76:             next_start = DateTime.combine((last_start + timedelta(days=1)).date(), Time.min)",
          "77:         # Otherwise this is the first ever run on the regular schedule...",
          "78:         elif (earliest := restriction.earliest) is None:",
          "79:             return None  # No start_date. Don't schedule.",
          "80:         elif not restriction.catchup:",
          "81:             # If the DAG has catchup=False, today is the earliest to consider.",
          "82:             next_start = max(earliest, DateTime.combine(Date.today(), Time.min))",
          "83:         elif earliest.time() != Time.min:",
          "84:             # If earliest does not fall on midnight, skip to the next day.",
          "85:             next_start = DateTime.combine(earliest.date() + timedelta(days=1), Time.min)",
          "86:         else:",
          "87:             next_start = earliest",
          "89:         next_start = self.get_next_workday(next_start.replace(tzinfo=UTC))",
          "",
          "---------------"
        ],
        "tests/plugins/workday.py||tests/plugins/workday.py": [
          "File: tests/plugins/workday.py -> tests/plugins/workday.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "",
          "[Removed Lines]",
          "18: \"\"\"Plugin to demonstrate timetable registration and accommodate example DAGs.\"\"\"",
          "19: from __future__ import annotations",
          "21: import logging",
          "22: from datetime import timedelta",
          "24: # [START howto_timetable]",
          "25: from pendulum import UTC, Date, DateTime, Time",
          "27: from airflow.plugins_manager import AirflowPlugin",
          "28: from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction, Timetable",
          "30: log = logging.getLogger(__name__)",
          "31: try:",
          "32:     from pandas.tseries.holiday import USFederalHolidayCalendar",
          "34:     holiday_calendar = USFederalHolidayCalendar()",
          "35: except ImportError:",
          "36:     log.warning(\"Could not import pandas. Holidays will not be considered.\")",
          "37:     holiday_calendar = None  # type: ignore[assignment]",
          "40: class AfterWorkdayTimetable(Timetable):",
          "41:     def get_next_workday(self, d: DateTime, incr=1) -> DateTime:",
          "42:         next_start = d",
          "43:         while True:",
          "44:             if next_start.weekday() in (5, 6):  # If next start is in the weekend go to next day",
          "45:                 next_start = next_start + incr * timedelta(days=1)",
          "46:                 continue",
          "47:             if holiday_calendar is not None:",
          "48:                 holidays = holiday_calendar.holidays(start=next_start, end=next_start).to_pydatetime()",
          "49:                 if next_start in holidays:  # If next start is a holiday go to next day",
          "50:                     next_start = next_start + incr * timedelta(days=1)",
          "51:                     continue",
          "52:             break",
          "53:         return next_start",
          "55:     # [START howto_timetable_infer_manual_data_interval]",
          "56:     def infer_manual_data_interval(self, run_after: DateTime) -> DataInterval:",
          "57:         start = DateTime.combine((run_after - timedelta(days=1)).date(), Time.min).replace(tzinfo=UTC)",
          "58:         # Skip backwards over weekends and holidays to find last run",
          "59:         start = self.get_next_workday(start, incr=-1)",
          "60:         return DataInterval(start=start, end=(start + timedelta(days=1)))",
          "62:     # [END howto_timetable_infer_manual_data_interval]",
          "64:     # [START howto_timetable_next_dagrun_info]",
          "65:     def next_dagrun_info(",
          "66:         self,",
          "68:         last_automated_data_interval: DataInterval | None,",
          "69:         restriction: TimeRestriction,",
          "70:     ) -> DagRunInfo | None:",
          "71:         if last_automated_data_interval is not None:  # There was a previous run on the regular schedule.",
          "72:             last_start = last_automated_data_interval.start",
          "73:             next_start = DateTime.combine((last_start + timedelta(days=1)).date(), Time.min).replace(",
          "74:                 tzinfo=UTC",
          "75:             )",
          "76:         else:  # This is the first ever run on the regular schedule.",
          "77:             next_start = restriction.earliest",
          "78:             if next_start is None:  # No start_date. Don't schedule.",
          "79:                 return None",
          "80:             if not restriction.catchup:",
          "81:                 # If the DAG has catchup=False, today is the earliest to consider.",
          "82:                 next_start = max(next_start, DateTime.combine(Date.today(), Time.min).replace(tzinfo=UTC))",
          "83:             elif next_start.time() != Time.min:",
          "84:                 # If earliest does not fall on midnight, skip to the next day.",
          "85:                 next_start = DateTime.combine(next_start.date() + timedelta(days=1), Time.min).replace(",
          "86:                     tzinfo=UTC",
          "87:                 )",
          "88:         # Skip weekends and holidays",
          "89:         next_start = self.get_next_workday(next_start)",
          "91:         if restriction.latest is not None and next_start > restriction.latest:",
          "92:             return None  # Over the DAG's scheduled end; don't schedule.",
          "93:         return DagRunInfo.interval(start=next_start, end=(next_start + timedelta(days=1)))",
          "95:     # [END howto_timetable_next_dagrun_info]",
          "98: class WorkdayTimetablePlugin(AirflowPlugin):",
          "99:     name = \"workday_timetable_plugin\"",
          "100:     timetables = [AfterWorkdayTimetable]",
          "103: # [END howto_timetable]",
          "",
          "[Added Lines]",
          "18: \"\"\"Plugin to demonstrate timetable registration and accommodate example DAGs.",
          "20: This simply forwards the timetable from ``airflow.example_dags``, so we can make",
          "21: it discoverable to unit tests without exposing the entire subpackage.",
          "22: \"\"\"",
          "23: from __future__ import annotations",
          "25: from airflow.example_dags.plugins.workday import AfterWorkdayTimetable, WorkdayTimetablePlugin",
          "27: __all__ = [\"AfterWorkdayTimetable\", \"WorkdayTimetablePlugin\"]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5422a3612b03fe66b606e9cbc66480c3c18543a7",
      "candidate_info": {
        "commit_hash": "5422a3612b03fe66b606e9cbc66480c3c18543a7",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/5422a3612b03fe66b606e9cbc66480c3c18543a7",
        "files": [
          "airflow/api_connexion/endpoints/dag_warning_endpoint.py",
          "airflow/api_connexion/security.py"
        ],
        "message": "Replace deprecated get_accessible_dag_ids and use get_readable_dags in get_dag_warnings (#36256)\n\n(cherry picked from commit 9406f00c0cab795375973e84702824e685d53e04)",
        "before_after_code_files": [
          "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py",
          "airflow/api_connexion/security.py||airflow/api_connexion/security.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py": [
          "File: airflow/api_connexion/endpoints/dag_warning_endpoint.py -> airflow/api_connexion/endpoints/dag_warning_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: from typing import TYPE_CHECKING",
          "22: from sqlalchemy import select",
          "24: from airflow.api_connexion import security",
          "",
          "[Removed Lines]",
          "21: from flask import g",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "27:     DagWarningCollection,",
          "28:     dag_warning_collection_schema,",
          "29: )",
          "30: from airflow.auth.managers.models.resource_details import DagAccessEntity",
          "31: from airflow.models.dagwarning import DagWarning as DagWarningModel",
          "33: from airflow.utils.db import get_query_count",
          "34: from airflow.utils.session import NEW_SESSION, provide_session",
          "",
          "[Removed Lines]",
          "32: from airflow.utils.airflow_flask_app import get_airflow_app",
          "",
          "[Added Lines]",
          "29: from airflow.api_connexion.security import get_readable_dags",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "61:     if dag_id:",
          "62:         query = query.where(DagWarningModel.dag_id == dag_id)",
          "63:     else:",
          "65:         query = query.where(DagWarningModel.dag_id.in_(readable_dags))",
          "66:     if warning_type:",
          "67:         query = query.where(DagWarningModel.warning_type == warning_type)",
          "",
          "[Removed Lines]",
          "64:         readable_dags = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
          "",
          "[Added Lines]",
          "63:         readable_dags = get_readable_dags()",
          "",
          "---------------"
        ],
        "airflow/api_connexion/security.py||airflow/api_connexion/security.py": [
          "File: airflow/api_connexion/security.py -> airflow/api_connexion/security.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "267:     return requires_access_decorator",
          "",
          "[Removed Lines]",
          "270: def get_readable_dags() -> list[str]:",
          "271:     return get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
          "",
          "[Added Lines]",
          "270: def get_readable_dags() -> set[str]:",
          "271:     return get_auth_manager().get_permitted_dag_ids(user=g.user)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "79e54c9948179f1d91caefb1e1dc049e34cf7b7e",
      "candidate_info": {
        "commit_hash": "79e54c9948179f1d91caefb1e1dc049e34cf7b7e",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/79e54c9948179f1d91caefb1e1dc049e34cf7b7e",
        "files": [
          ".asf.yaml",
          "docs/apache-airflow/img/logos/github_repository_social_image.png"
        ],
        "message": "Add Github social media preview image (#36653)\n\n(cherry picked from commit a039cfb36536f5390977bd10254527cc7a8a60b6)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "2c2d71c43b2d6f1de9efe011223c5fefceb68621",
      "candidate_info": {
        "commit_hash": "2c2d71c43b2d6f1de9efe011223c5fefceb68621",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/2c2d71c43b2d6f1de9efe011223c5fefceb68621",
        "files": [
          "tests/models/test_mappedoperator.py"
        ],
        "message": "Fix tests to adopt changes in Jinja 3.1.3 (#36731)\n\n(cherry picked from commit 8b33e25e502c18f42dd3f76c95fefd78fb3a04a3)",
        "before_after_code_files": [
          "tests/models/test_mappedoperator.py||tests/models/test_mappedoperator.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/models/test_mappedoperator.py||tests/models/test_mappedoperator.py": [
          "File: tests/models/test_mappedoperator.py -> tests/models/test_mappedoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: from collections import defaultdict",
          "22: from datetime import timedelta",
          "23: from typing import TYPE_CHECKING",
          "25: from unittest.mock import patch",
          "27: import pendulum",
          "",
          "[Removed Lines]",
          "24: from unittest import mock",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "405:     assert t.expand_input.value == {\"params\": [{\"c\": \"x\"}, {\"d\": 1}]}",
          "409:     with set_current_task_instance_session(session=session):",
          "411:         class MyOperator(BaseOperator):",
          "",
          "[Removed Lines]",
          "408: def test_mapped_render_template_fields_validating_operator(dag_maker, session):",
          "",
          "[Added Lines]",
          "407: def test_mapped_render_template_fields_validating_operator(dag_maker, session, tmp_path):",
          "408:     file_template_dir = tmp_path / \"path\" / \"to\"",
          "409:     file_template_dir.mkdir(parents=True, exist_ok=True)",
          "410:     file_template = file_template_dir / \"file.ext\"",
          "411:     file_template.write_text(\"loaded data\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "427:         def execute(self, context):",
          "428:             pass",
          "431:             task1 = BaseOperator(task_id=\"op1\")",
          "432:             output1 = task1.output",
          "433:             mapped = MyOperator.partial(",
          "",
          "[Removed Lines]",
          "430:         with dag_maker(session=session):",
          "",
          "[Added Lines]",
          "434:         with dag_maker(session=session, template_searchpath=tmp_path.__fspath__()):",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "455:         mapped_ti.map_index = 0",
          "457:         assert isinstance(mapped_ti.task, MappedOperator)",
          "462:         assert isinstance(mapped_ti.task, MyOperator)",
          "464:         assert mapped_ti.task.partial_template == \"a\", \"Should be templated!\"",
          "",
          "[Removed Lines]",
          "458:         with patch(\"builtins.open\", mock.mock_open(read_data=b\"loaded data\")), patch(",
          "459:             \"os.path.isfile\", return_value=True",
          "460:         ), patch(\"os.path.getmtime\", return_value=0):",
          "461:             mapped.render_template_fields(context=mapped_ti.get_template_context(session=session))",
          "",
          "[Added Lines]",
          "462:         mapped.render_template_fields(context=mapped_ti.get_template_context(session=session))",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "468:         assert mapped_ti.task.file_template == \"loaded data\", \"Should be templated!\"",
          "472:     with set_current_task_instance_session(session=session):",
          "474:         class MyOperator(BaseOperator):",
          "",
          "[Removed Lines]",
          "471: def test_mapped_expand_kwargs_render_template_fields_validating_operator(dag_maker, session):",
          "",
          "[Added Lines]",
          "472: def test_mapped_expand_kwargs_render_template_fields_validating_operator(dag_maker, session, tmp_path):",
          "473:     file_template_dir = tmp_path / \"path\" / \"to\"",
          "474:     file_template_dir.mkdir(parents=True, exist_ok=True)",
          "475:     file_template = file_template_dir / \"file.ext\"",
          "476:     file_template.write_text(\"loaded data\")",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "490:             def execute(self, context):",
          "491:                 pass",
          "494:             mapped = MyOperator.partial(",
          "495:                 task_id=\"a\", partial_template=\"{{ ti.task_id }}\", partial_static=\"{{ ti.task_id }}\"",
          "496:             ).expand_kwargs(",
          "",
          "[Removed Lines]",
          "493:         with dag_maker(session=session):",
          "",
          "[Added Lines]",
          "499:         with dag_maker(session=session, template_searchpath=tmp_path.__fspath__()):",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "502:         mapped_ti: TaskInstance = dr.get_task_instance(mapped.task_id, session=session, map_index=0)",
          "504:         assert isinstance(mapped_ti.task, MappedOperator)",
          "509:         assert isinstance(mapped_ti.task, MyOperator)",
          "511:         assert mapped_ti.task.partial_template == \"a\", \"Should be templated!\"",
          "",
          "[Removed Lines]",
          "505:         with patch(\"builtins.open\", mock.mock_open(read_data=b\"loaded data\")), patch(",
          "506:             \"os.path.isfile\", return_value=True",
          "507:         ), patch(\"os.path.getmtime\", return_value=0):",
          "508:             mapped.render_template_fields(context=mapped_ti.get_template_context(session=session))",
          "",
          "[Added Lines]",
          "511:         mapped.render_template_fields(context=mapped_ti.get_template_context(session=session))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "dd344fd961bdc81efb7698652a3c148fb918eafb",
      "candidate_info": {
        "commit_hash": "dd344fd961bdc81efb7698652a3c148fb918eafb",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/dd344fd961bdc81efb7698652a3c148fb918eafb",
        "files": [
          "README.md",
          "dev/breeze/src/airflow_breeze/global_constants.py",
          "generated/PYPI_README.md",
          "images/breeze/output_k8s_configure-cluster.svg",
          "images/breeze/output_k8s_configure-cluster.txt",
          "images/breeze/output_k8s_create-cluster.svg",
          "images/breeze/output_k8s_create-cluster.txt",
          "images/breeze/output_k8s_delete-cluster.svg",
          "images/breeze/output_k8s_delete-cluster.txt",
          "images/breeze/output_k8s_deploy-airflow.svg",
          "images/breeze/output_k8s_deploy-airflow.txt",
          "images/breeze/output_k8s_k9s.svg",
          "images/breeze/output_k8s_k9s.txt",
          "images/breeze/output_k8s_logs.svg",
          "images/breeze/output_k8s_logs.txt",
          "images/breeze/output_k8s_run-complete-tests.svg",
          "images/breeze/output_k8s_run-complete-tests.txt",
          "images/breeze/output_k8s_shell.svg",
          "images/breeze/output_k8s_shell.txt",
          "images/breeze/output_k8s_status.svg",
          "images/breeze/output_k8s_status.txt",
          "images/breeze/output_k8s_tests.svg",
          "images/breeze/output_k8s_tests.txt",
          "images/breeze/output_k8s_upload-k8s-image.svg",
          "images/breeze/output_k8s_upload-k8s-image.txt"
        ],
        "message": "feat: K8S 1.29 support (#36527)\n\n(cherry picked from commit 7e26f79d4b9f0dccf0d39db3d40efbf08aa8d083)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py": [
          "File: dev/breeze/src/airflow_breeze/global_constants.py -> dev/breeze/src/airflow_breeze/global_constants.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "74: #   - https://endoflife.date/amazon-eks",
          "75: #   - https://endoflife.date/azure-kubernetes-service",
          "76: #   - https://endoflife.date/google-kubernetes-engine",
          "78: ALLOWED_EXECUTORS = [",
          "79:     \"LocalExecutor\",",
          "80:     \"KubernetesExecutor\",",
          "",
          "[Removed Lines]",
          "77: ALLOWED_KUBERNETES_VERSIONS = [\"v1.25.11\", \"v1.26.6\", \"v1.27.3\", \"v1.28.0\"]",
          "",
          "[Added Lines]",
          "77: ALLOWED_KUBERNETES_VERSIONS = [\"v1.25.11\", \"v1.26.6\", \"v1.27.3\", \"v1.28.0\", \"v1.29.0\"]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "00ee94bb64139d66ffde4b0fea71effcd36b120f",
      "candidate_info": {
        "commit_hash": "00ee94bb64139d66ffde4b0fea71effcd36b120f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/00ee94bb64139d66ffde4b0fea71effcd36b120f",
        "files": [
          ".pre-commit-config.yaml",
          "airflow/io/path.py",
          "airflow/macros/__init__.py",
          "airflow/models/baseoperator.py",
          "airflow/models/dag.py",
          "airflow/models/taskinstance.py",
          "airflow/models/xcom_arg.py",
          "airflow/providers/amazon/aws/operators/s3.py",
          "airflow/providers/amazon/aws/operators/sagemaker.py",
          "airflow/providers/apache/cassandra/hooks/cassandra.py",
          "airflow/providers/apache/cassandra/sensors/record.py",
          "airflow/providers/apache/cassandra/sensors/table.py",
          "airflow/providers/apache/hive/hooks/hive.py",
          "airflow/providers/apache/hive/macros/hive.py",
          "airflow/providers/databricks/operators/databricks.py",
          "airflow/providers/ftp/operators/ftp.py",
          "airflow/providers/google/cloud/operators/bigquery.py",
          "airflow/providers/google/cloud/operators/gcs.py",
          "airflow/providers/google/cloud/operators/kubernetes_engine.py",
          "airflow/providers/google/cloud/operators/pubsub.py",
          "airflow/providers/google/cloud/transfers/adls_to_gcs.py",
          "airflow/providers/google/cloud/transfers/gcs_to_gcs.py",
          "airflow/providers/google/cloud/transfers/mssql_to_gcs.py",
          "airflow/providers/microsoft/azure/operators/adls.py",
          "airflow/providers/microsoft/azure/operators/container_instances.py",
          "airflow/providers/sftp/operators/sftp.py",
          "airflow/sensors/weekday.py",
          "airflow/utils/email.py",
          "airflow/utils/helpers.py",
          "airflow/utils/types.py",
          "airflow/www/extensions/init_appbuilder.py",
          "dev/mypy/plugin/outputs.py",
          "pyproject.toml",
          "tests/conftest.py",
          "tests/test_utils/providers.py",
          "tests/www/views/test_views_tasks.py"
        ],
        "message": "Add code snippet formatting in docstrings via Ruff (#36262)\n\nThis was made available [as part of v0.1.8 of the Ruff Formatter](https://astral.sh/blog/ruff-v0.1.8#formatting-code-snippets-in-docstrings). Adding this config option to the `ruff-format` pre-commit hook.\n\n(cherry picked from commit e9ba37bb58da0e3d6739ec063f7160f50487d3b8)",
        "before_after_code_files": [
          "airflow/io/path.py||airflow/io/path.py",
          "airflow/macros/__init__.py||airflow/macros/__init__.py",
          "airflow/models/baseoperator.py||airflow/models/baseoperator.py",
          "airflow/models/dag.py||airflow/models/dag.py",
          "airflow/models/taskinstance.py||airflow/models/taskinstance.py",
          "airflow/models/xcom_arg.py||airflow/models/xcom_arg.py",
          "airflow/providers/amazon/aws/operators/s3.py||airflow/providers/amazon/aws/operators/s3.py",
          "airflow/providers/amazon/aws/operators/sagemaker.py||airflow/providers/amazon/aws/operators/sagemaker.py",
          "airflow/providers/apache/cassandrhooks/cassandra.py||airflow/providers/apache/cassandra/hooks/cassandra.py",
          "airflow/providers/apache/cassandrsensors/record.py||airflow/providers/apache/cassandra/sensors/record.py",
          "airflow/providers/apache/cassandrsensors/table.py||airflow/providers/apache/cassandra/sensors/table.py",
          "airflow/providers/apache/hive/hooks/hive.py||airflow/providers/apache/hive/hooks/hive.py",
          "airflow/providers/apache/hive/macros/hive.py||airflow/providers/apache/hive/macros/hive.py",
          "airflow/providers/databricks/operators/databricks.py||airflow/providers/databricks/operators/databricks.py",
          "airflow/providers/ftp/operators/ftp.py||airflow/providers/ftp/operators/ftp.py",
          "airflow/providers/google/cloud/operators/bigquery.py||airflow/providers/google/cloud/operators/bigquery.py",
          "airflow/providers/google/cloud/operators/gcs.py||airflow/providers/google/cloud/operators/gcs.py",
          "airflow/providers/google/cloud/operators/kubernetes_engine.py||airflow/providers/google/cloud/operators/kubernetes_engine.py",
          "airflow/providers/google/cloud/operators/pubsub.py||airflow/providers/google/cloud/operators/pubsub.py",
          "airflow/providers/google/cloud/transfers/adls_to_gcs.py||airflow/providers/google/cloud/transfers/adls_to_gcs.py",
          "airflow/providers/google/cloud/transfers/gcs_to_gcs.py||airflow/providers/google/cloud/transfers/gcs_to_gcs.py",
          "airflow/providers/google/cloud/transfers/mssql_to_gcs.py||airflow/providers/google/cloud/transfers/mssql_to_gcs.py",
          "airflow/providers/microsoft/azure/operators/adls.py||airflow/providers/microsoft/azure/operators/adls.py",
          "airflow/providers/microsoft/azure/operators/container_instances.py||airflow/providers/microsoft/azure/operators/container_instances.py",
          "airflow/providers/sftp/operators/sftp.py||airflow/providers/sftp/operators/sftp.py",
          "airflow/sensors/weekday.py||airflow/sensors/weekday.py",
          "airflow/utils/email.py||airflow/utils/email.py",
          "airflow/utils/helpers.py||airflow/utils/helpers.py",
          "airflow/utils/types.py||airflow/utils/types.py",
          "airflow/www/extensions/init_appbuilder.py||airflow/www/extensions/init_appbuilder.py",
          "dev/mypy/plugin/outputs.py||dev/mypy/plugin/outputs.py",
          "tests/conftest.py||tests/conftest.py",
          "tests/test_utils/providers.py||tests/test_utils/providers.py",
          "tests/www/views/test_views_tasks.py||tests/www/views/test_views_tasks.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/io/path.py||airflow/io/path.py": [
          "File: airflow/io/path.py -> airflow/io/path.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "263:         --------",
          "264:         >>> read_block(0, 13)",
          "265:         b'Alice, 100\\\\nBo'",
          "267:         b'Alice, 100\\\\nBob, 200\\\\n'",
          "269:         Use ``length=None`` to read to the end of the file.",
          "271:         b'Alice, 100\\\\nBob, 200\\\\nCharlie, 300'",
          "273:         See Also",
          "",
          "[Removed Lines]",
          "266:         >>> read_block(0, 13, delimiter=b'\\\\n')",
          "270:         >>> read_block(0, None, delimiter=b'\\\\n')",
          "",
          "[Added Lines]",
          "266:         >>> read_block(0, 13, delimiter=b\"\\\\n\")",
          "270:         >>> read_block(0, None, delimiter=b\"\\\\n\")",
          "",
          "---------------"
        ],
        "airflow/macros/__init__.py||airflow/macros/__init__.py": [
          "File: airflow/macros/__init__.py -> airflow/macros/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "49:     :param ds: anchor date in ``YYYY-MM-DD`` format to add to",
          "50:     :param days: number of days to add to the ds, you can use negative values",
          "53:     '2015-01-06'",
          "55:     '2015-01-01'",
          "56:     \"\"\"",
          "57:     if not days:",
          "",
          "[Removed Lines]",
          "52:     >>> ds_add('2015-01-01', 5)",
          "54:     >>> ds_add('2015-01-06', -5)",
          "",
          "[Added Lines]",
          "52:     >>> ds_add(\"2015-01-01\", 5)",
          "54:     >>> ds_add(\"2015-01-06\", -5)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "68:     :param input_format: input string format. E.g. %Y-%m-%d",
          "69:     :param output_format: output string format  E.g. %Y-%m-%d",
          "72:     '01-01-15'",
          "74:     '2015-01-05'",
          "75:     \"\"\"",
          "76:     return datetime.strptime(str(ds), input_format).strftime(output_format)",
          "",
          "[Removed Lines]",
          "71:     >>> ds_format('2015-01-01', \"%Y-%m-%d\", \"%m-%d-%y\")",
          "73:     >>> ds_format('1/5/2015', \"%m/%d/%Y\",  \"%Y-%m-%d\")",
          "",
          "[Added Lines]",
          "71:     >>> ds_format(\"2015-01-01\", \"%Y-%m-%d\", \"%m-%d-%y\")",
          "73:     >>> ds_format(\"1/5/2015\", \"%m/%d/%Y\", \"%Y-%m-%d\")",
          "",
          "---------------"
        ],
        "airflow/models/baseoperator.py||airflow/models/baseoperator.py": [
          "File: airflow/models/baseoperator.py -> airflow/models/baseoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "629:         the KubernetesExecutor ::",
          "638:     :param do_xcom_push: if True, an XCom is pushed containing the Operator's",
          "639:         result",
          "",
          "[Removed Lines]",
          "631:             MyOperator(...,",
          "632:                 executor_config={",
          "633:                     \"KubernetesExecutor\":",
          "634:                         {\"image\": \"myCustomDockerImage\"}",
          "635:                 }",
          "636:             )",
          "",
          "[Added Lines]",
          "631:             MyOperator(..., executor_config={\"KubernetesExecutor\": {\"image\": \"myCustomDockerImage\"}})",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1152:             # This is equivalent to",
          "1153:             with DAG(...):",
          "1154:                 generate_content = GenerateContentOperator(task_id=\"generate_content\")",
          "1158:                 generate_content >> send_email",
          "1160:         \"\"\"",
          "",
          "[Removed Lines]",
          "1155:                 send_email = EmailOperator(",
          "1156:                     ..., html_content=\"{{ task_instance.xcom_pull('generate_content') }}\"",
          "1157:                 )",
          "",
          "[Added Lines]",
          "1150:                 send_email = EmailOperator(..., html_content=\"{{ task_instance.xcom_pull('generate_content') }}\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1867:     Then you can accomplish like so::",
          "1876:     :param elements: a list of operators / lists of operators",
          "1877:     \"\"\"",
          "",
          "[Removed Lines]",
          "1869:         chain_linear(",
          "1870:             op1,",
          "1871:             [op2, op3],",
          "1872:             [op4, op5, op6],",
          "1873:             op7",
          "1874:         )",
          "",
          "[Added Lines]",
          "1862:         chain_linear(op1, [op2, op3], [op4, op5, op6], op7)",
          "",
          "---------------"
        ],
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "385:                 jinja_environment_kwargs={",
          "387:                     # some other jinja2 Environment options here",
          "389:             )",
          "",
          "[Removed Lines]",
          "384:             DAG(dag_id='my-dag',",
          "386:                     'keep_trailing_newline': True,",
          "388:                 }",
          "",
          "[Added Lines]",
          "384:             DAG(",
          "385:                 dag_id=\"my-dag\",",
          "387:                     \"keep_trailing_newline\": True,",
          "389:                 },",
          "",
          "---------------"
        ],
        "airflow/models/taskinstance.py||airflow/models/taskinstance.py": [
          "File: airflow/models/taskinstance.py -> airflow/models/taskinstance.py"
        ],
        "airflow/models/xcom_arg.py||airflow/models/xcom_arg.py": [
          "File: airflow/models/xcom_arg.py -> airflow/models/xcom_arg.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "60:         xcomarg >> op",
          "61:         xcomarg << op",
          "",
          "[Removed Lines]",
          "62:         op >> xcomarg   # By BaseOperator code",
          "63:         op << xcomarg   # By BaseOperator code",
          "",
          "[Added Lines]",
          "62:         op >> xcomarg  # By BaseOperator code",
          "63:         op << xcomarg  # By BaseOperator code",
          "",
          "---------------"
        ],
        "airflow/providers/amazon/aws/operators/s3.py||airflow/providers/amazon/aws/operators/s3.py": [
          "File: airflow/providers/amazon/aws/operators/s3.py -> airflow/providers/amazon/aws/operators/s3.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "762:         ``customers/2018/04/`` key in the ``data`` bucket. ::",
          "764:             s3_file = S3ListOperator(",
          "770:             )",
          "771:     \"\"\"",
          "",
          "[Removed Lines]",
          "765:                 task_id='list_3s_files',",
          "766:                 bucket='data',",
          "767:                 prefix='customers/2018/04/',",
          "768:                 delimiter='/',",
          "769:                 aws_conn_id='aws_customers_conn'",
          "",
          "[Added Lines]",
          "765:                 task_id=\"list_3s_files\",",
          "766:                 bucket=\"data\",",
          "767:                 prefix=\"customers/2018/04/\",",
          "768:                 delimiter=\"/\",",
          "769:                 aws_conn_id=\"aws_customers_conn\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "843:         from the S3 ``customers/2018/04/`` prefix in the ``data`` bucket. ::",
          "845:             s3_file = S3ListPrefixesOperator(",
          "851:             )",
          "852:     \"\"\"",
          "",
          "[Removed Lines]",
          "846:                 task_id='list_s3_prefixes',",
          "847:                 bucket='data',",
          "848:                 prefix='customers/2018/04/',",
          "849:                 delimiter='/',",
          "850:                 aws_conn_id='aws_customers_conn'",
          "",
          "[Added Lines]",
          "846:                 task_id=\"list_s3_prefixes\",",
          "847:                 bucket=\"data\",",
          "848:                 prefix=\"customers/2018/04/\",",
          "849:                 delimiter=\"/\",",
          "850:                 aws_conn_id=\"aws_customers_conn\",",
          "",
          "---------------"
        ],
        "airflow/providers/amazon/aws/operators/sagemaker.py||airflow/providers/amazon/aws/operators/sagemaker.py": [
          "File: airflow/providers/amazon/aws/operators/sagemaker.py -> airflow/providers/amazon/aws/operators/sagemaker.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "404:         If you need to create a SageMaker endpoint based on an existed",
          "405:         SageMaker model and an existed SageMaker endpoint config::",
          "409:         If you need to create all of SageMaker model, SageMaker endpoint-config and SageMaker endpoint::",
          "411:             config = {",
          "415:             }",
          "417:         For details of the configuration parameter of model_configuration see",
          "",
          "[Removed Lines]",
          "407:             config = endpoint_configuration;",
          "412:                 'Model': model_configuration,",
          "413:                 'EndpointConfig': endpoint_config_configuration,",
          "414:                 'Endpoint': endpoint_configuration",
          "",
          "[Added Lines]",
          "407:             config = endpoint_configuration",
          "412:                 \"Model\": model_configuration,",
          "413:                 \"EndpointConfig\": endpoint_config_configuration,",
          "414:                 \"Endpoint\": endpoint_configuration,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "580:         If you need to create both SageMaker model and SageMaker Transform job::",
          "587:         For details of the configuration parameter of transform_config see",
          "588:         :py:meth:`SageMaker.Client.create_transform_job`",
          "",
          "[Removed Lines]",
          "582:             config = {",
          "583:                 'Model': model_config,",
          "584:                 'Transform': transform_config",
          "585:             }",
          "",
          "[Added Lines]",
          "582:             config = {\"Model\": model_config, \"Transform\": transform_config}",
          "",
          "---------------"
        ],
        "airflow/providers/apache/cassandrhooks/cassandra.py||airflow/providers/apache/cassandra/hooks/cassandra.py": [
          "File: airflow/providers/apache/cassandrhooks/cassandra.py -> airflow/providers/apache/cassandra/hooks/cassandra.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "50:         {\"ssl_options\": {\"ca_certs\": PATH_TO_CA_CERTS}}",
          "",
          "---------------"
        ],
        "airflow/providers/apache/cassandrsensors/record.py||airflow/providers/apache/cassandra/sensors/record.py": [
          "File: airflow/providers/apache/cassandrsensors/record.py -> airflow/providers/apache/cassandra/sensors/record.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "41:     >>> cassandra_sensor = CassandraRecordSensor(",
          "42:     ...     table=\"k.t\",",
          "43:     ...     keys={\"p1\": \"v1\", \"p2\": \"v2\"},",
          "44:     ...     cassandra_conn_id=\"cassandra_default\",",
          "45:     ...     task_id=\"cassandra_sensor\",",
          "46:     ... )",
          "",
          "---------------"
        ],
        "airflow/providers/apache/cassandrsensors/table.py||airflow/providers/apache/cassandra/sensors/table.py": [
          "File: airflow/providers/apache/cassandrsensors/table.py -> airflow/providers/apache/cassandra/sensors/table.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "41:     >>> cassandra_sensor = CassandraTableSensor(",
          "42:     ...     table=\"k.t\", cassandra_conn_id=\"cassandra_default\", task_id=\"cassandra_sensor\"",
          "43:     ... )",
          "",
          "---------------"
        ],
        "airflow/providers/apache/hive/hooks/hive.py||airflow/providers/apache/hive/hooks/hive.py": [
          "File: airflow/providers/apache/hive/hooks/hive.py -> airflow/providers/apache/hive/hooks/hive.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "583:         :param schema: Name of hive schema (database) @table belongs to",
          "584:         :param table: Name of hive table @partition belongs to",
          "588:         >>> hh = HiveMetastoreHook()",
          "591:         True",
          "592:         \"\"\"",
          "593:         with self.metastore as client:",
          "",
          "[Removed Lines]",
          "585:         :param partition: Expression that matches the partitions to check for",
          "586:             (eg `a = 'b' AND c = 'd'`)",
          "589:         >>> t = 'static_babynames_partitioned'",
          "590:         >>> hh.check_for_partition('airflow', t, \"ds='2015-01-01'\")",
          "",
          "[Added Lines]",
          "585:         :param partition: Expression that matches the partitions to check for (e.g. `a = 'b' AND c = 'd'`)",
          "588:         >>> t = \"static_babynames_partitioned\"",
          "589:         >>> hh.check_for_partition(\"airflow\", t, \"ds='2015-01-01'\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "606:         :param partition_name: Name of the partitions to check for (eg `a=b/c=d`)",
          "608:         >>> hh = HiveMetastoreHook()",
          "611:         True",
          "613:         False",
          "614:         \"\"\"",
          "615:         with self.metastore as client:",
          "",
          "[Removed Lines]",
          "609:         >>> t = 'static_babynames_partitioned'",
          "610:         >>> hh.check_for_named_partition('airflow', t, \"ds=2015-01-01\")",
          "612:         >>> hh.check_for_named_partition('airflow', t, \"ds=xxx\")",
          "",
          "[Added Lines]",
          "608:         >>> t = \"static_babynames_partitioned\"",
          "609:         >>> hh.check_for_named_partition(\"airflow\", t, \"ds=2015-01-01\")",
          "611:         >>> hh.check_for_named_partition(\"airflow\", t, \"ds=xxx\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "619:         \"\"\"Get a metastore table object.",
          "621:         >>> hh = HiveMetastoreHook()",
          "623:         >>> t.tableName",
          "624:         'static_babynames'",
          "625:         >>> [col.name for col in t.sd.cols]",
          "",
          "[Removed Lines]",
          "622:         >>> t = hh.get_table(db='airflow', table_name='static_babynames')",
          "",
          "[Added Lines]",
          "621:         >>> t = hh.get_table(db=\"airflow\", table_name=\"static_babynames\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "649:         For subpartitioned table, the number might easily exceed this.",
          "651:         >>> hh = HiveMetastoreHook()",
          "654:         >>> len(parts)",
          "655:         1",
          "656:         >>> parts",
          "",
          "[Removed Lines]",
          "652:         >>> t = 'static_babynames_partitioned'",
          "653:         >>> parts = hh.get_partitions(schema='airflow', table_name=t)",
          "",
          "[Added Lines]",
          "651:         >>> t = \"static_babynames_partitioned\"",
          "652:         >>> parts = hh.get_partitions(schema=\"airflow\", table_name=t)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "765:         Check if table exists.",
          "767:         >>> hh = HiveMetastoreHook()",
          "769:         True",
          "771:         False",
          "772:         \"\"\"",
          "773:         try:",
          "",
          "[Removed Lines]",
          "768:         >>> hh.table_exists(db='airflow', table_name='static_babynames')",
          "770:         >>> hh.table_exists(db='airflow', table_name='does_not_exist')",
          "",
          "[Added Lines]",
          "767:         >>> hh.table_exists(db=\"airflow\", table_name=\"static_babynames\")",
          "769:         >>> hh.table_exists(db=\"airflow\", table_name=\"does_not_exist\")",
          "",
          "---------------"
        ],
        "airflow/providers/apache/hive/macros/hive.py||airflow/providers/apache/hive/macros/hive.py": [
          "File: airflow/providers/apache/hive/macros/hive.py -> airflow/providers/apache/hive/macros/hive.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "39:     :param field: the field to get the max value from. If there's only",
          "40:         one partition field, this will be inferred",
          "43:     '2015-01-01'",
          "44:     \"\"\"",
          "45:     from airflow.providers.apache.hive.hooks.hive import HiveMetastoreHook",
          "",
          "[Removed Lines]",
          "42:     >>> max_partition('airflow.static_babynames_partitioned')",
          "",
          "[Added Lines]",
          "42:     >>> max_partition(\"airflow.static_babynames_partitioned\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "94:     :param metastore_conn_id: which metastore connection to use",
          "95:     :returns: The closest date",
          "99:     '2015-01-01'",
          "100:     \"\"\"",
          "101:     from airflow.providers.apache.hive.hooks.hive import HiveMetastoreHook",
          "",
          "[Removed Lines]",
          "97:     >>> tbl = 'airflow.static_babynames_partitioned'",
          "98:     >>> closest_ds_partition(tbl, '2015-01-02')",
          "",
          "[Added Lines]",
          "97:     >>> tbl = \"airflow.static_babynames_partitioned\"",
          "98:     >>> closest_ds_partition(tbl, \"2015-01-02\")",
          "",
          "---------------"
        ],
        "airflow/providers/databricks/operators/databricks.py||airflow/providers/databricks/operators/databricks.py": [
          "File: airflow/providers/databricks/operators/databricks.py -> airflow/providers/databricks/operators/databricks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "580:     For example ::",
          "582:         json = {",
          "588:         }",
          "592:     Another way to accomplish the same thing is to use the named parameters",
          "593:     of the ``DatabricksRunNowOperator`` directly. Note that there is exactly",
          "594:     one named parameter for each top level parameter in the ``run-now``",
          "595:     endpoint. In this method, your code would look like this: ::",
          "604:         python_params = [\"douglas adams\", \"42\"]",
          "",
          "[Removed Lines]",
          "583:           \"job_id\": 42,",
          "584:           \"notebook_params\": {",
          "585:             \"dry-run\": \"true\",",
          "586:             \"oldest-time-to-consider\": \"1457570074236\"",
          "587:           }",
          "590:         notebook_run = DatabricksRunNowOperator(task_id='notebook_run', json=json)",
          "597:         job_id=42",
          "599:         notebook_params = {",
          "600:             \"dry-run\": \"true\",",
          "601:             \"oldest-time-to-consider\": \"1457570074236\"",
          "602:         }",
          "",
          "[Added Lines]",
          "583:             \"job_id\": 42,",
          "584:             \"notebook_params\": {\"dry-run\": \"true\", \"oldest-time-to-consider\": \"1457570074236\"},",
          "587:         notebook_run = DatabricksRunNowOperator(task_id=\"notebook_run\", json=json)",
          "594:         job_id = 42",
          "596:         notebook_params = {\"dry-run\": \"true\", \"oldest-time-to-consider\": \"1457570074236\"}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "612:             notebook_params=notebook_params,",
          "613:             python_params=python_params,",
          "614:             jar_params=jar_params,",
          "616:         )",
          "618:     In the case where both the json parameter **AND** the named parameters",
          "",
          "[Removed Lines]",
          "615:             spark_submit_params=spark_submit_params",
          "",
          "[Added Lines]",
          "609:             spark_submit_params=spark_submit_params,",
          "",
          "---------------"
        ],
        "airflow/providers/ftp/operators/ftp.py||airflow/providers/ftp/operators/ftp.py": [
          "File: airflow/providers/ftp/operators/ftp.py -> airflow/providers/ftp/operators/ftp.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "66:                 remote_filepath=\"/tmp/tmp1/tmp2/file.txt\",",
          "67:                 operation=\"put\",",
          "68:                 create_intermediate_dirs=True,",
          "70:             )",
          "71:     \"\"\"",
          "",
          "[Removed Lines]",
          "69:                 dag=dag",
          "",
          "[Added Lines]",
          "69:                 dag=dag,",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/operators/bigquery.py||airflow/providers/google/cloud/operators/bigquery.py": [
          "File: airflow/providers/google/cloud/operators/bigquery.py -> airflow/providers/google/cloud/operators/bigquery.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "449:             # job.result() returns a RowIterator. Mypy expects an instance of SupportsNext[Any] for",
          "450:             # the next() call which the RowIterator does not resemble to. Hence, ignore the arg-type error.",
          "451:             records = next(job.result())  # type: ignore[arg-type]",
          "453:             self.log.info(\"Current state of job %s is %s\", job.job_id, job.state)",
          "455:     @staticmethod",
          "",
          "[Removed Lines]",
          "452:             self.check_value(records)",
          "",
          "[Added Lines]",
          "452:             self.check_value(records)  # type: ignore[attr-defined]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "886:         get_data = BigQueryGetDataOperator(",
          "891:             max_results=100,",
          "894:         )",
          "896:     :param dataset_id: The dataset ID of the requested table. (templated)",
          "",
          "[Removed Lines]",
          "887:             task_id='get_data_from_bq',",
          "888:             dataset_id='test_dataset',",
          "889:             table_id='Transaction_partitions',",
          "890:             project_id='internal-gcp-project',",
          "892:             selected_fields='DATE',",
          "893:             gcp_conn_id='airflow-conn-id'",
          "",
          "[Added Lines]",
          "887:             task_id=\"get_data_from_bq\",",
          "888:             dataset_id=\"test_dataset\",",
          "889:             table_id=\"Transaction_partitions\",",
          "890:             project_id=\"internal-gcp-project\",",
          "892:             selected_fields=\"DATE\",",
          "893:             gcp_conn_id=\"airflow-conn-id\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1337:     :param gcs_schema_object: Full path to the JSON file containing",
          "1338:         schema (templated). For",
          "",
          "[Removed Lines]",
          "1334:             schema_fields=[{\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},",
          "1335:                            {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"}]",
          "",
          "[Added Lines]",
          "1334:             schema_fields = [",
          "1335:                 {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},",
          "1336:                 {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},",
          "1337:             ]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1353:         CreateTable = BigQueryCreateEmptyTableOperator(",
          "1361:         )",
          "1365:         [",
          "1376:         ]",
          "1380:         CreateTable = BigQueryCreateEmptyTableOperator(",
          "1389:         )",
          "1391:     :param view: [Optional] A dictionary containing definition for the view.",
          "",
          "[Removed Lines]",
          "1354:             task_id='BigQueryCreateEmptyTableOperator_task',",
          "1355:             dataset_id='ODS',",
          "1356:             table_id='Employees',",
          "1357:             project_id='internal-gcp-project',",
          "1358:             gcs_schema_object='gs://schema-bucket/employee_schema.json',",
          "1359:             gcp_conn_id='airflow-conn-id',",
          "1360:             google_cloud_storage_conn_id='airflow-conn-id'",
          "1366:             {",
          "1367:             \"mode\": \"NULLABLE\",",
          "1368:             \"name\": \"emp_name\",",
          "1369:             \"type\": \"STRING\"",
          "1370:             },",
          "1371:             {",
          "1372:             \"mode\": \"REQUIRED\",",
          "1373:             \"name\": \"salary\",",
          "1374:             \"type\": \"INTEGER\"",
          "1375:             }",
          "1381:             task_id='BigQueryCreateEmptyTableOperator_task',",
          "1382:             dataset_id='ODS',",
          "1383:             table_id='Employees',",
          "1384:             project_id='internal-gcp-project',",
          "1385:             schema_fields=[{\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},",
          "1386:                             {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"}],",
          "1387:             gcp_conn_id='airflow-conn-id-account',",
          "1388:             google_cloud_storage_conn_id='airflow-conn-id'",
          "",
          "[Added Lines]",
          "1356:             task_id=\"BigQueryCreateEmptyTableOperator_task\",",
          "1357:             dataset_id=\"ODS\",",
          "1358:             table_id=\"Employees\",",
          "1359:             project_id=\"internal-gcp-project\",",
          "1360:             gcs_schema_object=\"gs://schema-bucket/employee_schema.json\",",
          "1361:             gcp_conn_id=\"airflow-conn-id\",",
          "1362:             google_cloud_storage_conn_id=\"airflow-conn-id\",",
          "1368:             {\"mode\": \"NULLABLE\", \"name\": \"emp_name\", \"type\": \"STRING\"},",
          "1369:             {\"mode\": \"REQUIRED\", \"name\": \"salary\", \"type\": \"INTEGER\"},",
          "1375:             task_id=\"BigQueryCreateEmptyTableOperator_task\",",
          "1376:             dataset_id=\"ODS\",",
          "1377:             table_id=\"Employees\",",
          "1378:             project_id=\"internal-gcp-project\",",
          "1379:             schema_fields=[",
          "1380:                 {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},",
          "1381:                 {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},",
          "1382:             ],",
          "1383:             gcp_conn_id=\"airflow-conn-id-account\",",
          "1384:             google_cloud_storage_conn_id=\"airflow-conn-id\",",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1588:         Should not be set when source_format is 'DATASTORE_BACKUP'.",
          "1589:     :param table_resource: Table resource as described in documentation:",
          "",
          "[Removed Lines]",
          "1585:             schema_fields=[{\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},",
          "1586:                            {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"}]",
          "",
          "[Added Lines]",
          "1581:             schema_fields = [",
          "1582:                 {\"name\": \"emp_name\", \"type\": \"STRING\", \"mode\": \"REQUIRED\"},",
          "1583:                 {\"name\": \"salary\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\"},",
          "1584:             ]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1880:         delete_temp_data = BigQueryDeleteDatasetOperator(",
          "1887:     \"\"\"",
          "1889:     template_fields: Sequence[str] = (",
          "",
          "[Removed Lines]",
          "1881:             dataset_id='temp-dataset',",
          "1882:             project_id='temp-project',",
          "1883:             delete_contents=True, # Force the deletion of the dataset as well as its tables (if any).",
          "1884:             gcp_conn_id='_my_gcp_conn_',",
          "1885:             task_id='Deletetemp',",
          "1886:             dag=dag)",
          "",
          "[Added Lines]",
          "1879:             dataset_id=\"temp-dataset\",",
          "1880:             project_id=\"temp-project\",",
          "1881:             delete_contents=True,  # Force the deletion of the dataset as well as its tables (if any).",
          "1882:             gcp_conn_id=\"_my_gcp_conn_\",",
          "1883:             task_id=\"Deletetemp\",",
          "1884:             dag=dag,",
          "1885:         )",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/operators/gcs.py||airflow/providers/google/cloud/operators/gcs.py": [
          "File: airflow/providers/google/cloud/operators/gcs.py -> airflow/providers/google/cloud/operators/gcs.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "188:         folder in ``data`` bucket. ::",
          "190:             GCS_Files = GCSListOperator(",
          "196:             )",
          "197:     \"\"\"",
          "",
          "[Removed Lines]",
          "191:                 task_id='GCS_Files',",
          "192:                 bucket='data',",
          "193:                 prefix='sales/sales-2017/',",
          "194:                 match_glob='**/*/.avro',",
          "195:                 gcp_conn_id=google_cloud_conn_id",
          "",
          "[Added Lines]",
          "191:                 task_id=\"GCS_Files\",",
          "192:                 bucket=\"data\",",
          "193:                 prefix=\"sales/sales-2017/\",",
          "194:                 match_glob=\"**/*/.avro\",",
          "195:                 gcp_conn_id=google_cloud_conn_id,",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/operators/kubernetes_engine.py||airflow/providers/google/cloud/operators/kubernetes_engine.py": [
          "File: airflow/providers/google/cloud/operators/kubernetes_engine.py -> airflow/providers/google/cloud/operators/kubernetes_engine.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "192:     The **minimum** required to define a cluster to create is:",
          "194:     ``dict()`` ::",
          "198:     or",
          "200:     ``Cluster`` proto ::",
          "201:         from google.cloud.container_v1.types import Cluster",
          "",
          "[Removed Lines]",
          "195:         cluster_def = {'name': 'my-cluster-name',",
          "196:                        'initial_node_count': 1}",
          "203:         cluster_def = Cluster(name='my-cluster-name', initial_node_count=1)",
          "",
          "[Added Lines]",
          "195:         cluster_def = {\"name\": \"my-cluster-name\", \"initial_node_count\": 1}",
          "202:         cluster_def = Cluster(name=\"my-cluster-name\", initial_node_count=1)",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/operators/pubsub.py||airflow/providers/google/cloud/operators/pubsub.py": [
          "File: airflow/providers/google/cloud/operators/pubsub.py -> airflow/providers/google/cloud/operators/pubsub.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "58:     By default, if the topic already exists, this operator will",
          "59:     not cause the DAG to fail. ::",
          "67:     The operator can be configured to fail if the topic already exists. ::",
          "77:             )",
          "79:     Both ``project_id`` and ``topic`` are templated so you can use Jinja templating in their values.",
          "81:     :param project_id: Optional, the Google Cloud project ID where the topic will be created.",
          "",
          "[Removed Lines]",
          "61:         with DAG('successful DAG') as dag:",
          "62:             (",
          "63:                 PubSubCreateTopicOperator(project_id='my-project', topic='my_new_topic')",
          "64:                 >> PubSubCreateTopicOperator(project_id='my-project', topic='my_new_topic')",
          "65:             )",
          "69:         with DAG('failing DAG') as dag:",
          "70:             (",
          "71:                 PubSubCreateTopicOperator(project_id='my-project', topic='my_new_topic')",
          "72:                 >> PubSubCreateTopicOperator(",
          "73:                     project_id='my-project',",
          "74:                     topic='my_new_topic',",
          "75:                     fail_if_exists=True,",
          "76:                 )",
          "",
          "[Added Lines]",
          "61:         with DAG(\"successful DAG\") as dag:",
          "62:             create_topic = PubSubCreateTopicOperator(project_id=\"my-project\", topic=\"my_new_topic\")",
          "63:             create_topic_again = PubSubCreateTopicOperator(project_id=\"my-project\", topic=\"my_new_topic\")",
          "65:             create_topic >> create_topic_again",
          "69:         with DAG(\"failing DAG\") as dag:",
          "70:             create_topic = PubSubCreateTopicOperator(project_id=\"my-project\", topic=\"my_new_topic\")",
          "71:             create_topic_again = PubSubCreateTopicOperator(",
          "72:                 project_id=\"my-project\", topic=\"my_new_topic\", fail_if_exists=True",
          "75:             create_topic >> create_topic_again",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "197:     By default, if the subscription already exists, this operator will",
          "198:     not cause the DAG to fail. However, the topic must exist in the project. ::",
          "212:             )",
          "214:     The operator can be configured to fail if the subscription already exists.",
          "215:     ::",
          "230:             )",
          "232:     Finally, subscription is not required. If not passed, the operator will",
          "233:     generated a universally unique identifier for the subscription's name. ::",
          "238:     ``project_id``, ``topic``, ``subscription``, ``subscription_project_id`` and",
          "239:     ``impersonation_chain`` are templated so you can use Jinja templating in their values.",
          "",
          "[Removed Lines]",
          "200:         with DAG('successful DAG') as dag:",
          "201:             (",
          "202:                 PubSubCreateSubscriptionOperator(",
          "203:                     project_id='my-project',",
          "204:                     topic='my-topic',",
          "205:                     subscription='my-subscription'",
          "206:                 )",
          "207:                 >> PubSubCreateSubscriptionOperator(",
          "208:                     project_id='my-project',",
          "209:                     topic='my-topic',",
          "210:                     subscription='my-subscription',",
          "211:                 )",
          "217:         with DAG('failing DAG') as dag:",
          "218:             (",
          "219:                 PubSubCreateSubscriptionOperator(",
          "220:                     project_id='my-project',",
          "221:                     topic='my-topic',",
          "222:                     subscription='my-subscription',",
          "223:                 )",
          "224:                 >> PubSubCreateSubscriptionOperator(",
          "225:                     project_id='my-project',",
          "226:                     topic='my-topic',",
          "227:                     subscription='my-subscription',",
          "228:                     fail_if_exists=True,",
          "229:                 )",
          "235:         with DAG('DAG') as dag:",
          "236:             PubSubCreateSubscriptionOperator(project_id='my-project', topic='my-topic')",
          "",
          "[Added Lines]",
          "198:         with DAG(\"successful DAG\") as dag:",
          "199:             create_subscription = PubSubCreateSubscriptionOperator(",
          "200:                 project_id=\"my-project\", topic=\"my-topic\", subscription=\"my-subscription\"",
          "201:             )",
          "202:             create_subscription_again = PubSubCreateSubscriptionOperator(",
          "203:                 project_id=\"my-project\", topic=\"my-topic\", subscription=\"my-subscription\"",
          "206:             create_subscription >> create_subscription_again",
          "212:         with DAG(\"failing DAG\") as dag:",
          "213:             create_subscription = PubSubCreateSubscriptionOperator(",
          "214:                 project_id=\"my-project\", topic=\"my-topic\", subscription=\"my-subscription\"",
          "216:             create_subscription_again = PubSubCreateSubscriptionOperator(",
          "217:                 project_id=\"my-project\", topic=\"my-topic\", subscription=\"my-subscription\", fail_if_exists=True",
          "218:             )",
          "220:             create_subscription >> create_subscription_again",
          "225:         with DAG(\"DAG\") as dag:",
          "226:             PubSubCreateSubscriptionOperator(project_id=\"my-project\", topic=\"my-topic\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "410:     By default, if the topic does not exist, this operator will",
          "411:     not cause the DAG to fail. ::",
          "416:     The operator can be configured to fail if the topic does not exist. ::",
          "419:             PubSubDeleteTopicOperator(",
          "421:             )",
          "423:     Both ``project_id`` and ``topic`` are templated so you can use Jinja templating in their values.",
          "",
          "[Removed Lines]",
          "413:         with DAG('successful DAG') as dag:",
          "414:             PubSubDeleteTopicOperator(project_id='my-project', topic='non_existing_topic')",
          "418:         with DAG('failing DAG') as dag:",
          "420:                 project_id='my-project', topic='non_existing_topic', fail_if_not_exists=True,",
          "",
          "[Added Lines]",
          "403:         with DAG(\"successful DAG\") as dag:",
          "404:             PubSubDeleteTopicOperator(project_id=\"my-project\", topic=\"non_existing_topic\")",
          "408:         with DAG(\"failing DAG\") as dag:",
          "410:                 project_id=\"my-project\",",
          "411:                 topic=\"non_existing_topic\",",
          "412:                 fail_if_not_exists=True,",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "506:     By default, if the subscription does not exist, this operator will",
          "507:     not cause the DAG to fail. ::",
          "512:     The operator can be configured to fail if the subscription already exists.",
          "514:     ::",
          "517:             PubSubDeleteSubscriptionOperator(",
          "519:             )",
          "521:     ``project_id``, and ``subscription`` are templated so you can use Jinja templating in their values.",
          "",
          "[Removed Lines]",
          "509:         with DAG('successful DAG') as dag:",
          "510:             PubSubDeleteSubscriptionOperator(project_id='my-project', subscription='non-existing')",
          "516:         with DAG('failing DAG') as dag:",
          "518:                 project_id='my-project', subscription='non-existing', fail_if_not_exists=True,",
          "",
          "[Added Lines]",
          "501:         with DAG(\"successful DAG\") as dag:",
          "502:             PubSubDeleteSubscriptionOperator(project_id=\"my-project\", subscription=\"non-existing\")",
          "508:         with DAG(\"failing DAG\") as dag:",
          "510:                 project_id=\"my-project\",",
          "511:                 subscription=\"non-existing\",",
          "512:                 fail_if_not_exists=True,",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "605:     in a single Google Cloud project. If the topic does not exist, this",
          "606:     task will fail. ::",
          "614:         t1 = PubSubPublishMessageOperator(",
          "617:             messages=[m1, m2, m3],",
          "618:             create_topic=True,",
          "619:             dag=dag,",
          "",
          "[Removed Lines]",
          "608:         m1 = {'data': b'Hello, World!',",
          "609:               'attributes': {'type': 'greeting'}",
          "610:              }",
          "611:         m2 = {'data': b'Knock, knock'}",
          "612:         m3 = {'attributes': {'foo': ''}}",
          "615:             project_id='my-project',",
          "616:             topic='my_topic',",
          "",
          "[Added Lines]",
          "602:         m1 = {\"data\": b\"Hello, World!\", \"attributes\": {\"type\": \"greeting\"}}",
          "603:         m2 = {\"data\": b\"Knock, knock\"}",
          "604:         m3 = {\"attributes\": {\"foo\": \"\"}}",
          "607:             project_id=\"my-project\",",
          "608:             topic=\"my_topic\",",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/transfers/adls_to_gcs.py||airflow/providers/google/cloud/transfers/adls_to_gcs.py": [
          "File: airflow/providers/google/cloud/transfers/adls_to_gcs.py -> airflow/providers/google/cloud/transfers/adls_to_gcs.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "58:         resulting gcs path will be ``gs://mybucket/hello/world.avro`` ::",
          "60:             copy_single_file = AdlsToGoogleCloudStorageOperator(",
          "64:                 replace=False,",
          "67:             )",
          "69:         The following Operator would copy all parquet files from ADLS",
          "",
          "[Removed Lines]",
          "61:                 task_id='copy_single_file',",
          "62:                 src_adls='hello/world.avro',",
          "63:                 dest_gcs='gs://mybucket',",
          "65:                 azure_data_lake_conn_id='azure_data_lake_default',",
          "66:                 gcp_conn_id='google_cloud_default'",
          "",
          "[Added Lines]",
          "61:                 task_id=\"copy_single_file\",",
          "62:                 src_adls=\"hello/world.avro\",",
          "63:                 dest_gcs=\"gs://mybucket\",",
          "65:                 azure_data_lake_conn_id=\"azure_data_lake_default\",",
          "66:                 gcp_conn_id=\"google_cloud_default\",",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/transfers/gcs_to_gcs.py||airflow/providers/google/cloud/transfers/gcs_to_gcs.py": [
          "File: airflow/providers/google/cloud/transfers/gcs_to_gcs.py -> airflow/providers/google/cloud/transfers/gcs_to_gcs.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "101:     ``copied_sales/2017/january-backup.avro`` in the ``data_backup`` bucket ::",
          "103:         copy_single_file = GCSToGCSOperator(",
          "109:             exact_match=True,",
          "111:         )",
          "113:     The following Operator would copy all the Avro files from ``sales/sales-2017``",
          "",
          "[Removed Lines]",
          "104:             task_id='copy_single_file',",
          "105:             source_bucket='data',",
          "106:             source_objects=['sales/sales-2017/january.avro'],",
          "107:             destination_bucket='data_backup',",
          "108:             destination_object='copied_sales/2017/january-backup.avro',",
          "110:             gcp_conn_id=google_cloud_conn_id",
          "",
          "[Added Lines]",
          "104:             task_id=\"copy_single_file\",",
          "105:             source_bucket=\"data\",",
          "106:             source_objects=[\"sales/sales-2017/january.avro\"],",
          "107:             destination_bucket=\"data_backup\",",
          "108:             destination_object=\"copied_sales/2017/january-backup.avro\",",
          "110:             gcp_conn_id=google_cloud_conn_id,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "141:     process. ::",
          "143:         move_files = GCSToGCSOperator(",
          "148:             move_object=True,",
          "150:         )",
          "152:     The following Operator would move all the Avro files from ``sales/sales-2019``",
          "",
          "[Removed Lines]",
          "144:             task_id='move_files',",
          "145:             source_bucket='data',",
          "146:             source_object='sales/sales-2017/*.avro',",
          "147:             destination_bucket='data_backup',",
          "149:             gcp_conn_id=google_cloud_conn_id",
          "",
          "[Added Lines]",
          "144:             task_id=\"move_files\",",
          "145:             source_bucket=\"data\",",
          "146:             source_object=\"sales/sales-2017/*.avro\",",
          "147:             destination_bucket=\"data_backup\",",
          "149:             gcp_conn_id=google_cloud_conn_id,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "154:      ``data_backup`` bucket, deleting the original files in the process. ::",
          "156:         move_files = GCSToGCSOperator(",
          "162:             move_object=True,",
          "164:         )",
          "166:     \"\"\"",
          "",
          "[Removed Lines]",
          "157:             task_id='move_files',",
          "158:             source_bucket='data',",
          "159:             source_objects=['sales/sales-2019/*.avro', 'sales/sales-2020'],",
          "160:             destination_bucket='data_backup',",
          "161:             delimiter='.avro',",
          "163:             gcp_conn_id=google_cloud_conn_id",
          "",
          "[Added Lines]",
          "157:             task_id=\"move_files\",",
          "158:             source_bucket=\"data\",",
          "159:             source_objects=[\"sales/sales-2019/*.avro\", \"sales/sales-2020\"],",
          "160:             destination_bucket=\"data_backup\",",
          "161:             delimiter=\".avro\",",
          "163:             gcp_conn_id=google_cloud_conn_id,",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "329:         the ``data_backup`` bucket (b/a.csv, b/b.csv, b/c.csv) ::",
          "331:             copy_files = GCSToGCSOperator(",
          "338:             )",
          "340:         Example 2:",
          "",
          "[Removed Lines]",
          "332:                 task_id='copy_files_without_wildcard',",
          "333:                 source_bucket='data',",
          "334:                 source_objects=['a/'],",
          "335:                 destination_bucket='data_backup',",
          "336:                 destination_object='b/',",
          "337:                 gcp_conn_id=google_cloud_conn_id",
          "",
          "[Added Lines]",
          "332:                 task_id=\"copy_files_without_wildcard\",",
          "333:                 source_bucket=\"data\",",
          "334:                 source_objects=[\"a/\"],",
          "335:                 destination_bucket=\"data_backup\",",
          "336:                 destination_object=\"b/\",",
          "337:                 gcp_conn_id=google_cloud_conn_id,",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "345:         the ``data_backup`` bucket (b/a.avro, b/b.avro, b/c.avro) ::",
          "347:             copy_files = GCSToGCSOperator(",
          "355:             )",
          "357:         Example 3:",
          "",
          "[Removed Lines]",
          "348:                 task_id='copy_files_without_wildcard',",
          "349:                 source_bucket='data',",
          "350:                 source_objects=['a/'],",
          "351:                 destination_bucket='data_backup',",
          "352:                 destination_object='b/',",
          "353:                 delimiter='.avro',",
          "354:                 gcp_conn_id=google_cloud_conn_id",
          "",
          "[Added Lines]",
          "348:                 task_id=\"copy_files_without_wildcard\",",
          "349:                 source_bucket=\"data\",",
          "350:                 source_objects=[\"a/\"],",
          "351:                 destination_bucket=\"data_backup\",",
          "352:                 destination_object=\"b/\",",
          "353:                 delimiter=\".avro\",",
          "354:                 gcp_conn_id=google_cloud_conn_id,",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "362:         the ``data_backup`` bucket (b/file_1.txt, b/file_2.csv, b/file_3.avro) ::",
          "364:             copy_files = GCSToGCSOperator(",
          "371:             )",
          "373:         Example 4:",
          "",
          "[Removed Lines]",
          "365:                 task_id='copy_files_without_wildcard',",
          "366:                 source_bucket='data',",
          "367:                 source_objects=['a/file_1.txt', 'a/file_2.csv', 'a/file_3.avro'],",
          "368:                 destination_bucket='data_backup',",
          "369:                 destination_object='b/',",
          "370:                 gcp_conn_id=google_cloud_conn_id",
          "",
          "[Added Lines]",
          "365:                 task_id=\"copy_files_without_wildcard\",",
          "366:                 source_bucket=\"data\",",
          "367:                 source_objects=[\"a/file_1.txt\", \"a/file_2.csv\", \"a/file_3.avro\"],",
          "368:                 destination_bucket=\"data_backup\",",
          "369:                 destination_object=\"b/\",",
          "370:                 gcp_conn_id=google_cloud_conn_id,",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "378:         (b/foo.txt, b/foo.txt.abc, b/foo.txt/subfolder/file.txt) ::",
          "380:             copy_files = GCSToGCSOperator(",
          "387:             )",
          "388:         \"\"\"",
          "389:         objects = hook.list(",
          "",
          "[Removed Lines]",
          "381:                 task_id='copy_files_without_wildcard',",
          "382:                 source_bucket='data',",
          "383:                 source_object='a/foo.txt',",
          "384:                 destination_bucket='data_backup',",
          "385:                 destination_object='b/',",
          "386:                 gcp_conn_id=google_cloud_conn_id",
          "",
          "[Added Lines]",
          "381:                 task_id=\"copy_files_without_wildcard\",",
          "382:                 source_bucket=\"data\",",
          "383:                 source_object=\"a/foo.txt\",",
          "384:                 destination_bucket=\"data_backup\",",
          "385:                 destination_object=\"b/\",",
          "386:                 gcp_conn_id=google_cloud_conn_id,",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/transfers/mssql_to_gcs.py||airflow/providers/google/cloud/transfers/mssql_to_gcs.py": [
          "File: airflow/providers/google/cloud/transfers/mssql_to_gcs.py -> airflow/providers/google/cloud/transfers/mssql_to_gcs.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "42:         'mssql-export' GCS bucket (along with a schema file). ::",
          "44:             export_customers = MSSQLToGCSOperator(",
          "54:             )",
          "56:     .. seealso::",
          "",
          "[Removed Lines]",
          "45:                 task_id='export_customers',",
          "46:                 sql='SELECT * FROM dbo.Customers;',",
          "47:                 bit_fields=['some_bit_field', 'another_bit_field'],",
          "48:                 bucket='mssql-export',",
          "49:                 filename='data/customers/export.json',",
          "50:                 schema_filename='schemas/export.json',",
          "51:                 mssql_conn_id='mssql_default',",
          "52:                 gcp_conn_id='google_cloud_default',",
          "53:                 dag=dag",
          "",
          "[Added Lines]",
          "45:                 task_id=\"export_customers\",",
          "46:                 sql=\"SELECT * FROM dbo.Customers;\",",
          "47:                 bit_fields=[\"some_bit_field\", \"another_bit_field\"],",
          "48:                 bucket=\"mssql-export\",",
          "49:                 filename=\"data/customers/export.json\",",
          "50:                 schema_filename=\"schemas/export.json\",",
          "51:                 mssql_conn_id=\"mssql_default\",",
          "52:                 gcp_conn_id=\"google_cloud_default\",",
          "53:                 dag=dag,",
          "",
          "---------------"
        ],
        "airflow/providers/microsoft/azure/operators/adls.py||airflow/providers/microsoft/azure/operators/adls.py": [
          "File: airflow/providers/microsoft/azure/operators/adls.py -> airflow/providers/microsoft/azure/operators/adls.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "78:         folder in the specified ADLS account ::",
          "80:             adls_files = ADLSListOperator(",
          "84:             )",
          "85:     \"\"\"",
          "",
          "[Removed Lines]",
          "81:                 task_id='adls_files',",
          "82:                 path='folder/output/*.parquet',",
          "83:                 azure_data_lake_conn_id='azure_data_lake_default'",
          "",
          "[Added Lines]",
          "81:                 task_id=\"adls_files\",",
          "82:                 path=\"folder/output/*.parquet\",",
          "83:                 azure_data_lake_conn_id=\"azure_data_lake_default\",",
          "",
          "---------------"
        ],
        "airflow/providers/microsoft/azure/operators/container_instances.py||airflow/providers/microsoft/azure/operators/container_instances.py": [
          "File: airflow/providers/microsoft/azure/operators/container_instances.py -> airflow/providers/microsoft/azure/operators/container_instances.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "119:     \"\"\"",
          "121:     template_fields: Sequence[str] = (\"name\", \"image\", \"command\", \"environment_variables\", \"volumes\")",
          "",
          "[Removed Lines]",
          "96:                 AzureContainerInstancesOperator(",
          "97:                     ci_conn_id = \"azure_service_principal\",",
          "98:                     registry_conn_id = \"azure_registry_user\",",
          "99:                     resource_group = \"my-resource-group\",",
          "100:                     name = \"my-container-name-{{ ds }}\",",
          "101:                     image = \"myprivateregistry.azurecr.io/my_container:latest\",",
          "102:                     region = \"westeurope\",",
          "103:                     environment_variables = {\"MODEL_PATH\":  \"my_value\",",
          "104:                      \"POSTGRES_LOGIN\": \"{{ macros.connection('postgres_default').login }}\",",
          "105:                      \"POSTGRES_PASSWORD\": \"{{ macros.connection('postgres_default').password }}\",",
          "106:                      \"JOB_GUID\": \"{{ ti.xcom_pull(task_ids='task1', key='guid') }}\" },",
          "107:                     secured_variables = ['POSTGRES_PASSWORD'],",
          "108:                     volumes = [(\"azure_container_instance_conn_id\",",
          "109:                             \"my_storage_container\",",
          "110:                             \"my_fileshare\",",
          "111:                             \"/input-data\",",
          "112:                         True),],",
          "113:                     memory_in_gb=14.0,",
          "114:                     cpu=4.0,",
          "115:                     gpu=GpuResource(count=1, sku='K80'),",
          "116:                     command=[\"/bin/echo\", \"world\"],",
          "117:                     task_id=\"start_container\"",
          "118:                 )",
          "",
          "[Added Lines]",
          "96:         AzureContainerInstancesOperator(",
          "97:             ci_conn_id=\"azure_service_principal\",",
          "98:             registry_conn_id=\"azure_registry_user\",",
          "99:             resource_group=\"my-resource-group\",",
          "100:             name=\"my-container-name-{{ ds }}\",",
          "101:             image=\"myprivateregistry.azurecr.io/my_container:latest\",",
          "102:             region=\"westeurope\",",
          "103:             environment_variables={",
          "104:                 \"MODEL_PATH\": \"my_value\",",
          "105:                 \"POSTGRES_LOGIN\": \"{{ macros.connection('postgres_default').login }}\",",
          "106:                 \"POSTGRES_PASSWORD\": \"{{ macros.connection('postgres_default').password }}\",",
          "107:                 \"JOB_GUID\": \"{{ ti.xcom_pull(task_ids='task1', key='guid') }}\",",
          "108:             },",
          "109:             secured_variables=[\"POSTGRES_PASSWORD\"],",
          "110:             volumes=[",
          "111:                 (\"azure_container_instance_conn_id\", \"my_storage_container\", \"my_fileshare\", \"/input-data\", True),",
          "112:             ],",
          "113:             memory_in_gb=14.0,",
          "114:             cpu=4.0,",
          "115:             gpu=GpuResource(count=1, sku=\"K80\"),",
          "116:             command=[\"/bin/echo\", \"world\"],",
          "117:             task_id=\"start_container\",",
          "118:         )",
          "",
          "---------------"
        ],
        "airflow/providers/sftp/operators/sftp.py||airflow/providers/sftp/operators/sftp.py": [
          "File: airflow/providers/sftp/operators/sftp.py -> airflow/providers/sftp/operators/sftp.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "74:                 remote_filepath=\"/tmp/tmp1/tmp2/file.txt\",",
          "75:                 operation=\"put\",",
          "76:                 create_intermediate_dirs=True,",
          "78:             )",
          "80:     \"\"\"",
          "",
          "[Removed Lines]",
          "77:                 dag=dag",
          "",
          "[Added Lines]",
          "77:                 dag=dag,",
          "",
          "---------------"
        ],
        "airflow/sensors/weekday.py||airflow/sensors/weekday.py": [
          "File: airflow/sensors/weekday.py -> airflow/sensors/weekday.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "41:         weekend_check = DayOfWeekSensor(",
          "49:         weekend_check = DayOfWeekSensor(",
          "",
          "[Removed Lines]",
          "42:             task_id='weekend_check',",
          "43:             week_day='Saturday',",
          "44:             use_task_logical_date=True,",
          "45:             dag=dag)",
          "50:             task_id='weekend_check',",
          "51:             week_day={'Saturday', 'Sunday'},",
          "52:             use_task_logical_date=True,",
          "53:             dag=dag)",
          "",
          "[Added Lines]",
          "42:             task_id=\"weekend_check\", week_day=\"Saturday\", use_task_logical_date=True, dag=dag",
          "43:         )",
          "48:             task_id=\"weekend_check\", week_day={\"Saturday\", \"Sunday\"}, use_task_logical_date=True, dag=dag",
          "49:         )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "58:         from airflow.utils.weekday import WeekDay",
          "60:         weekend_check = DayOfWeekSensor(",
          "62:             week_day={WeekDay.SATURDAY, WeekDay.SUNDAY},",
          "63:             use_task_logical_date=True,",
          "66:     :param week_day: Day of the week to check (full name). Optionally, a set",
          "67:         of days can also be provided using a set.",
          "",
          "[Removed Lines]",
          "61:             task_id='weekend_check',",
          "64:             dag=dag)",
          "",
          "[Added Lines]",
          "57:             task_id=\"weekend_check\",",
          "60:             dag=dag,",
          "61:         )",
          "",
          "---------------"
        ],
        "airflow/utils/email.py||airflow/utils/email.py": [
          "File: airflow/utils/email.py -> airflow/utils/email.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "125:     :param custom_headers: Dictionary of custom headers to include in the email.",
          "126:     :param kwargs: Additional keyword arguments.",
          "129:     \"\"\"",
          "130:     smtp_mail_from = conf.get(\"smtp\", \"SMTP_MAIL_FROM\")",
          "",
          "[Removed Lines]",
          "128:     >>> send_email('test@example.com', 'foo', '<b>Foo</b> bar', ['/dev/null'], dryrun=True)",
          "",
          "[Added Lines]",
          "128:     >>> send_email(\"test@example.com\", \"foo\", \"<b>Foo</b> bar\", [\"/dev/null\"], dryrun=True)",
          "",
          "---------------"
        ],
        "airflow/utils/helpers.py||airflow/utils/helpers.py": [
          "File: airflow/utils/helpers.py -> airflow/utils/helpers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "155:     \"\"\"",
          "156:     Return an iterable with one level flattened.",
          "159:     ['blue', 'red', 'green', 'yellow', 'pink']",
          "160:     \"\"\"",
          "161:     return [e for i in iterable for e in i]",
          "",
          "[Removed Lines]",
          "158:     >>> as_flattened_list((('blue', 'red'), ('green', 'yellow', 'pink')))",
          "",
          "[Added Lines]",
          "158:     >>> as_flattened_list(((\"blue\", \"red\"), (\"green\", \"yellow\", \"pink\")))",
          "",
          "---------------"
        ],
        "airflow/utils/types.py||airflow/utils/types.py": [
          "File: airflow/utils/types.py -> airflow/utils/types.py"
        ],
        "airflow/www/extensions/init_appbuilder.py||airflow/www/extensions/init_appbuilder.py": [
          "File: airflow/www/extensions/init_appbuilder.py -> airflow/www/extensions/init_appbuilder.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "411:             # or not instantiated",
          "412:             appbuilder.add_view(MyModelView, \"My View\")",
          "413:             # Register a view, a submenu \"Other View\" from \"Other\" with a phone icon.",
          "420:             # Register a view, with category icon and translation.",
          "421:             appbuilder.add_view(",
          "422:                 YetOtherModelView,",
          "423:                 \"Other View\",",
          "426:                 category=\"Others\",",
          "429:             )",
          "430:             # Register a view whose menu item will be conditionally displayed",
          "431:             appbuilder.add_view(",
          "432:                 YourFeatureView,",
          "433:                 \"Your Feature\",",
          "436:                 menu_cond=lambda: is_feature_enabled(\"your-feature\"),",
          "437:             )",
          "438:             # Add a link",
          "440:         \"\"\"",
          "441:         baseview = self._check_and_init(baseview)",
          "442:         log.info(LOGMSG_INF_FAB_ADD_VIEW, baseview.__class__.__name__, name)",
          "",
          "[Removed Lines]",
          "414:             appbuilder.add_view(",
          "415:                 MyOtherModelView,",
          "416:                 \"Other View\",",
          "417:                 icon='fa-phone',",
          "418:                 category=\"Others\"",
          "419:             )",
          "424:                 icon='fa-phone',",
          "425:                 label=_('Other View'),",
          "427:                 category_icon='fa-envelop',",
          "428:                 category_label=_('Other View')",
          "434:                 icon='fa-feature',",
          "435:                 label=_('Your Feature'),",
          "439:             appbuilder.add_link(\"google\", href=\"www.google.com\", icon = \"fa-google-plus\")",
          "",
          "[Added Lines]",
          "414:             appbuilder.add_view(MyOtherModelView, \"Other View\", icon=\"fa-phone\", category=\"Others\")",
          "419:                 icon=\"fa-phone\",",
          "420:                 label=_(\"Other View\"),",
          "422:                 category_icon=\"fa-envelop\",",
          "423:                 category_label=_(\"Other View\"),",
          "429:                 icon=\"fa-feature\",",
          "430:                 label=_(\"Your Feature\"),",
          "434:             appbuilder.add_link(\"google\", href=\"www.google.com\", icon=\"fa-google-plus\")",
          "",
          "---------------"
        ],
        "dev/mypy/plugin/outputs.py||dev/mypy/plugin/outputs.py": [
          "File: dev/mypy/plugin/outputs.py -> dev/mypy/plugin/outputs.py"
        ],
        "tests/conftest.py||tests/conftest.py": [
          "File: tests/conftest.py -> tests/conftest.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "616:     the same argument as DAG::",
          "618:         with dag_maker(dag_id=\"mydag\") as dag:",
          "622:     If the DagModel you want to use needs different parameters than the one",
          "623:     automatically created by the dag_maker, you have to update the DagModel as below::",
          "",
          "[Removed Lines]",
          "619:             task1 = EmptyOperator(task_id='mytask')",
          "620:             task2 = EmptyOperator(task_id='mytask2')",
          "",
          "[Added Lines]",
          "619:             task1 = EmptyOperator(task_id=\"mytask\")",
          "620:             task2 = EmptyOperator(task_id=\"mytask2\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "854:     is not here, please use `default_args` so that the DAG will pass it to the",
          "855:     Task::",
          "859:     You cannot be able to alter the created DagRun or DagModel, use `dag_maker` fixture instead.",
          "860:     \"\"\"",
          "",
          "[Removed Lines]",
          "857:         dag, task = create_dummy_dag(default_args={'start_date':timezone.datetime(2016, 1, 1)})",
          "",
          "[Added Lines]",
          "857:         dag, task = create_dummy_dag(default_args={\"start_date\": timezone.datetime(2016, 1, 1)})",
          "",
          "---------------"
        ],
        "tests/test_utils/providers.py||tests/test_utils/providers.py": [
          "File: tests/test_utils/providers.py -> tests/test_utils/providers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "36:     Returns provider version given provider package name.",
          "38:     Example::",
          "40:             raise Exception(",
          "41:                 \"You must now remove `get_kube_client` from PodManager \"",
          "42:                 \"and make kube_client a required argument.\"",
          "",
          "[Removed Lines]",
          "39:         if provider_version('apache-airflow-providers-cncf-kubernetes') >= (6, 0):",
          "",
          "[Added Lines]",
          "39:         if provider_version(\"apache-airflow-providers-cncf-kubernetes\") >= (6, 0):",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_tasks.py||tests/www/views/test_views_tasks.py": [
          "File: tests/www/views/test_views_tasks.py -> tests/www/views/test_views_tasks.py"
        ]
      }
    },
    {
      "candidate_hash": "b63a63f988eb92095e5b7974b886b65debd57efb",
      "candidate_info": {
        "commit_hash": "b63a63f988eb92095e5b7974b886b65debd57efb",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b63a63f988eb92095e5b7974b886b65debd57efb",
        "files": [
          "README.md",
          "docs/apache-airflow/howto/set-up-database.rst",
          "docs/apache-airflow/installation/prerequisites.rst",
          "generated/PYPI_README.md"
        ],
        "message": "Announce MSSQL support end in Airflow 2.9.0, add migration script hints (#36509)\n\n* Announce MSSQL support end in Airflow 2.9.0, add migration script hints\n* Fix Sphinx formatting\n* Update README as well\n\n(cherry picked from commit eecb4793ab9081e5235d0740e05844e634b69419)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "874227c2cf8b61dfc3d7d4a91d5441411404266c",
      "candidate_info": {
        "commit_hash": "874227c2cf8b61dfc3d7d4a91d5441411404266c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/874227c2cf8b61dfc3d7d4a91d5441411404266c",
        "files": [
          "airflow/providers/amazon/provider.yaml",
          "airflow/providers/apache/hive/provider.yaml",
          "airflow/providers/common/sql/provider.yaml",
          "airflow/providers/exasol/provider.yaml",
          "airflow/providers/google/provider.yaml",
          "airflow/providers/presto/provider.yaml",
          "airflow/providers/salesforce/provider.yaml",
          "airflow/providers/trino/provider.yaml",
          "airflow/providers/weaviate/provider.yaml",
          "generated/provider_dependencies.json",
          "setup.py"
        ],
        "message": "Set min pandas dependency to 1.2.5 for all providers and airflow (#36698)\n\nWe had some REALLY old minimum version of Pandas set for all our\npandas dependency - Pandas 0.17.1 has been released in 2015 (!)\n\nLooking at the dependency tree - most of our dependencies had\n> 1.2.5 set - which is more than reasonable limit as Pandas 1.2.5\nhad been released in June 2021 - so more than 2.5 years ago.\n\nThis limit bump further helps us to limit the pip backtracking\nthat starts happening in certain situations.\n\nExtracted from: #36537\n\n(cherry picked from commit ecb2c9f24d1364642604c14f0deb681ab4894135)",
        "before_after_code_files": [
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "348: leveldb = [\"plyvel\"]",
          "349: otel = [\"opentelemetry-exporter-prometheus\"]",
          "350: pandas = [",
          "352: ]",
          "353: password = [",
          "354:     \"bcrypt>=2.0.0\",",
          "",
          "[Removed Lines]",
          "351:     \"pandas>=0.17.1\",",
          "",
          "[Added Lines]",
          "351:     \"pandas>=1.2.5\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f0b18881a6917b292617a66c1efcfdc7119fe447",
      "candidate_info": {
        "commit_hash": "f0b18881a6917b292617a66c1efcfdc7119fe447",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f0b18881a6917b292617a66c1efcfdc7119fe447",
        "files": [
          ".pre-commit-config.yaml",
          ".rat-excludes",
          "STATIC_CODE_CHECKS.rst",
          "airflow/reproducible_build.yaml",
          "dev/README_RELEASE_AIRFLOW.md",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "images/breeze/output_static-checks.svg",
          "images/breeze/output_static-checks.txt",
          "scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py",
          "scripts/in_container/run_prepare_airflow_packages.py"
        ],
        "message": "Add support for reproducible build date epoch for Airflow releases (#36726)\n\nHatch has built-in support for reproducible builds, however it\nuses a hard-coded 2020 date to generate the reproducible binaries,\nwhich produces whl, tar.gz files that contain file dates that are\npretty old. This might be confusing for anyone who is looking at\nthe file contents and timestamp inside.\n\nThis PR adds support (similar to provider approach) to store current\nreproducible date in the repository - so that it can be committed\nand tagged together with Airflow sources. It is updated fully\nautomaticallly by pre-commit whenever release notes change, which\nbasically means that whenever release notes are update just\nbefore release, the reproducible date is updated to current date.\n\nFor now we only check if the packages produced by hatchling\nbuild are reproducible.\n\n(cherry picked from commit a2d6c389f69034c526554b3291874dc4d66c4529)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/pre_commit_ids.py||dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py||scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py",
          "scripts/in_container/run_prepare_airflow_packages.py||scripts/in_container/run_prepare_airflow_packages.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "209: RICH_VERSION = \"13.7.0\"",
          "210: NODE_VERSION = \"21.2.0\"",
          "211: PRE_COMMIT_VERSION = \"3.5.0\"",
          "213: AIRFLOW_BUILD_DOCKERFILE = f\"\"\"",
          "214: FROM python:{DEFAULT_PYTHON_MAJOR_MINOR_VERSION}-slim-{ALLOWED_DEBIAN_VERSIONS[0]}",
          "215: RUN apt-get update && apt-get install -y --no-install-recommends git",
          "218: COPY . /opt/airflow",
          "219: \"\"\"",
          "",
          "[Removed Lines]",
          "216: RUN pip install pip=={AIRFLOW_PIP_VERSION} hatch==1.9.1 \\",
          "217:   gitpython=={GITPYTHON_VERSION} rich=={RICH_VERSION} pre-commit=={PRE_COMMIT_VERSION}",
          "",
          "[Added Lines]",
          "212: PYYAML_VERSION = \"6.0.1\"",
          "217: RUN pip install pip=={AIRFLOW_PIP_VERSION} hatch==1.9.1 pyyaml=={PYYAML_VERSION}\\",
          "218:  gitpython=={GITPYTHON_VERSION} rich=={RICH_VERSION} pre-commit=={PRE_COMMIT_VERSION}",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/pre_commit_ids.py||dev/breeze/src/airflow_breeze/pre_commit_ids.py": [
          "File: dev/breeze/src/airflow_breeze/pre_commit_ids.py -> dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "129:     \"update-local-yml-file\",",
          "130:     \"update-migration-references\",",
          "131:     \"update-providers-dependencies\",",
          "132:     \"update-spelling-wordlist-to-be-sorted\",",
          "133:     \"update-supported-versions\",",
          "134:     \"update-vendored-in-k8s-json-schema\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "132:     \"update-reproducible-source-date-epoch\",",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py||scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py": [
          "File: scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py -> scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: import sys",
          "21: from hashlib import md5",
          "22: from pathlib import Path",
          "23: from time import time",
          "25: import yaml",
          "27: sys.path.insert(0, str(Path(__file__).parent.resolve()))  # make sure common_precommit_utils is importable",
          "29: from common_precommit_utils import AIRFLOW_SOURCES_ROOT_PATH",
          "31: RELEASE_NOTES_FILE_PATH = AIRFLOW_SOURCES_ROOT_PATH / \"RELEASE_NOTES.rst\"",
          "32: REPRODUCIBLE_BUILD_FILE = AIRFLOW_SOURCES_ROOT_PATH / \"airflow\" / \"reproducible_build.yaml\"",
          "34: if __name__ == \"__main__\":",
          "35:     hash_md5 = md5()",
          "36:     hash_md5.update(RELEASE_NOTES_FILE_PATH.read_bytes())",
          "37:     release_notes_hash = hash_md5.hexdigest()",
          "38:     reproducible_build_text = REPRODUCIBLE_BUILD_FILE.read_text()",
          "39:     reproducible_build = yaml.safe_load(reproducible_build_text)",
          "40:     old_hash = reproducible_build[\"release-notes-hash\"]",
          "41:     if release_notes_hash != old_hash:",
          "42:         # Replace the hash in the file",
          "43:         reproducible_build[\"release-notes-hash\"] = release_notes_hash",
          "44:         reproducible_build[\"source-date-epoch\"] = int(time())",
          "45:     REPRODUCIBLE_BUILD_FILE.write_text(yaml.dump(reproducible_build))",
          "",
          "---------------"
        ],
        "scripts/in_container/run_prepare_airflow_packages.py||scripts/in_container/run_prepare_airflow_packages.py": [
          "File: scripts/in_container/run_prepare_airflow_packages.py -> scripts/in_container/run_prepare_airflow_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: from pathlib import Path",
          "27: from shutil import rmtree",
          "29: from rich.console import Console",
          "31: console = Console(color_system=\"standard\", width=200)",
          "33: AIRFLOW_SOURCES_ROOT = Path(__file__).parents[2].resolve()",
          "34: AIRFLOW_INIT_FILE = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"__init__.py\"",
          "35: WWW_DIRECTORY = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"www\"",
          "36: VERSION_SUFFIX = os.environ.get(\"VERSION_SUFFIX_FOR_PYPI\", \"\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "29: import yaml",
          "35: REPRODUCIBLE_BUILD_FILE = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"reproducible_build.yaml\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "81:     if package_format in [\"both\", \"sdist\"]:",
          "82:         build_command.extend([\"-t\", \"sdist\"])",
          "84:     console.print(f\"[bright_blue]Building packages: {package_format}\\n\")",
          "87:     if build_process.returncode != 0:",
          "88:         console.print(\"[red]Error building Airflow packages\")",
          "",
          "[Removed Lines]",
          "85:     build_process = subprocess.run(build_command, capture_output=False, cwd=AIRFLOW_SOURCES_ROOT)",
          "",
          "[Added Lines]",
          "86:     reproducible_date = yaml.safe_load(REPRODUCIBLE_BUILD_FILE.read_text())[\"source-date-epoch\"]",
          "88:     envcopy = os.environ.copy()",
          "89:     envcopy[\"SOURCE_DATE_EPOCH\"] = str(reproducible_date)",
          "91:     build_process = subprocess.run(",
          "92:         build_command,",
          "93:         capture_output=False,",
          "94:         cwd=AIRFLOW_SOURCES_ROOT,",
          "95:         env=envcopy,",
          "96:     )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c730881000f479f6b5f2844e3e2beebf75980bf3",
      "candidate_info": {
        "commit_hash": "c730881000f479f6b5f2844e3e2beebf75980bf3",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c730881000f479f6b5f2844e3e2beebf75980bf3",
        "files": [
          "BREEZE.rst",
          "dev/breeze/README.md",
          "dev/breeze/pyproject.toml",
          "scripts/ci/install_breeze.sh"
        ],
        "message": "Upgrade to latest versions of `pip` and `pipx` in CI runners (#36646)\n\nThe CI runners did not have latest version of `pip` and `pipx`. This\nchange updates the installation scripts to fix `pip` to the same\nversion as in the CI image and down-binds pipx to 1.4.1 which is\nrecently released bugfix version with better logging and installation\ninstructions.\n\n(cherry picked from commit 75bc05ce1f53de112f7eee7be524a26f2a3f6845)",
        "before_after_code_files": [
          "scripts/ci/install_breeze.sh||scripts/ci/install_breeze.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/install_breeze.sh||scripts/ci/install_breeze.sh": [
          "File: scripts/ci/install_breeze.sh -> scripts/ci/install_breeze.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: cd \"$( dirname \"${BASH_SOURCE[0]}\" )/../../\"",
          "23: python -m pipx install --editable ./dev/breeze/ --force",
          "24: echo '/home/runner/.local/bin' >> \"${GITHUB_PATH}\"",
          "",
          "[Removed Lines]",
          "22: python -m pip install \"pipx>=1.2.1\"",
          "",
          "[Added Lines]",
          "22: python -m pip install --upgrade pip==23.3.2",
          "23: python -m pip install \"pipx>=1.4.1\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "29dbafde9984c87474127ffdc0dc6523f1cd050b",
      "candidate_info": {
        "commit_hash": "29dbafde9984c87474127ffdc0dc6523f1cd050b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/29dbafde9984c87474127ffdc0dc6523f1cd050b",
        "files": [
          "docs/apache-airflow/templates-ref.rst"
        ],
        "message": "Add further details to replacement documentation (#36485)\n\n(cherry picked from commit 718efc0c6ca08833e9b536b1c48b9e771d7f8bdc)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "02b33f3cd39041800d93535a0a3d878c125b2579",
      "candidate_info": {
        "commit_hash": "02b33f3cd39041800d93535a0a3d878c125b2579",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/02b33f3cd39041800d93535a0a3d878c125b2579",
        "files": [
          "dev/README_RELEASE_PROVIDER_PACKAGES.md"
        ],
        "message": "Update description of release process to include options for next RCs (#36506)\n\n* Update description of release process to include options for next RCs\n\nOften we excluded individual providers from release and we have two\noptions how to handle them - either release them on their own with\nan accelerated vote, or include the providers in the next wave of\nproviders.\n\nThis PR improves description of our process and email template to\nexplain it better and guide the release manager on what to do.\n\nAlso improved notification message\n\nCo-authored-by: Hussein Awala <hussein@awala.fr>\n\n---------\n\nCo-authored-by: Hussein Awala <hussein@awala.fr>\n(cherry picked from commit 52a16bf332c6b3d39207a5467d504cb70403f820)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "ba46e6065edf62fb5d674492db714073e46e59ff",
      "candidate_info": {
        "commit_hash": "ba46e6065edf62fb5d674492db714073e46e59ff",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ba46e6065edf62fb5d674492db714073e46e59ff",
        "files": [
          ".github/actions/breeze/action.yml",
          ".github/workflows/build-images.yml"
        ],
        "message": "Avoid auto-detection of Airflow Sources during breeze installation (#36792)\n\nIn some circumstances, when breeze is installed in CI (when we\nupdate to newer breeze version in \"build-info\" workflow in old\nbranches) breeze is not able to auto-detect sources it was installed\nfrom.\n\nThis PR changes it by passing the sources via environment variable.\n\n(cherry picked from commit e8080b82f1e2bd2f976b8a47058efb7459bbc9dd)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "7985fa898a7670903eea4426db12dfc4d2ab57a8",
      "candidate_info": {
        "commit_hash": "7985fa898a7670903eea4426db12dfc4d2ab57a8",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7985fa898a7670903eea4426db12dfc4d2ab57a8",
        "files": [
          "dev/README_RELEASE_PROVIDER_PACKAGES.md"
        ],
        "message": "Add note for release manager about updating the states of providers (#36392)\n\n(cherry picked from commit d172ca933a7718e2c2d89761b5a38515621ffc0c)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "99ec731487e57cf3245953fda4d9a5746677a4c7",
      "candidate_info": {
        "commit_hash": "99ec731487e57cf3245953fda4d9a5746677a4c7",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/99ec731487e57cf3245953fda4d9a5746677a4c7",
        "files": [
          "dev/breeze/src/airflow_breeze/commands/common_package_installation_options.py",
          "images/breeze/output_release-management_install-provider-packages.svg",
          "images/breeze/output_release-management_install-provider-packages.txt",
          "images/breeze/output_release-management_verify-provider-packages.svg",
          "images/breeze/output_release-management_verify-provider-packages.txt",
          "images/breeze/output_shell.svg",
          "images/breeze/output_shell.txt",
          "images/breeze/output_start-airflow.svg",
          "images/breeze/output_start-airflow.txt",
          "scripts/in_container/install_airflow_and_providers.py"
        ],
        "message": "Fix --use-airflow-version constraints (#36378)\n\nWhen `--use-airflow-version` is a numeric or rc version, the constraints\nshould be specific for that version when installing airflow. For example\nwhen we install 2.7.3rc1, `constraints-2.7.3rc1` should be used.\n\nThis has been lost when fixing version in CI.\n\nThis PR introduces these fixes:\n\n* default varlue for airflow constraints is DEFAULT_AIRFLOW_CONSTRAINTS_BRANCH\n\n* when --use-airflow-version is numeric version and default value is\n  used for constraints (DEFAULT_AIRFLOW_CONSTRAINTS_BRANCH) then it\n  is replaced with `constraints-VERSION`\n\n* when we print out constraints used, we print which are the\n  constraints used by Airflow and which by providers.\n\n(cherry picked from commit 5ddd67a9a670caf210fbcd15e033561ebe4404d8)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/common_package_installation_options.py||dev/breeze/src/airflow_breeze/commands/common_package_installation_options.py",
          "scripts/in_container/install_airflow_and_providers.py||scripts/in_container/install_airflow_and_providers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/common_package_installation_options.py||dev/breeze/src/airflow_breeze/commands/common_package_installation_options.py": [
          "File: dev/breeze/src/airflow_breeze/commands/common_package_installation_options.py -> dev/breeze/src/airflow_breeze/commands/common_package_installation_options.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import click",
          "22: from airflow_breeze.global_constants import ALLOWED_CONSTRAINTS_MODES_CI, ALLOWED_CONSTRAINTS_MODES_PROD",
          "23: from airflow_breeze.utils.custom_param_types import BetterChoice",
          "25: option_airflow_constraints_reference = click.option(",
          "26:     \"--airflow-constraints-reference\",",
          "29:     envvar=\"AIRFLOW_CONSTRAINTS_REFERENCE\",",
          "30: )",
          "31: option_airflow_constraints_location = click.option(",
          "",
          "[Removed Lines]",
          "27:     help=\"Constraint reference to use for airflow installation (used in calculated constraints URL). \"",
          "28:     \"Can be 'default' in which case the default constraints-reference is used.\",",
          "",
          "[Added Lines]",
          "22: from airflow_breeze.branch_defaults import DEFAULT_AIRFLOW_CONSTRAINTS_BRANCH",
          "28:     default=DEFAULT_AIRFLOW_CONSTRAINTS_BRANCH,",
          "29:     help=\"Constraint reference to use for airflow installation (used in calculated constraints URL).\",",
          "",
          "---------------"
        ],
        "scripts/in_container/install_airflow_and_providers.py||scripts/in_container/install_airflow_and_providers.py": [
          "File: scripts/in_container/install_airflow_and_providers.py -> scripts/in_container/install_airflow_and_providers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "76:     constraints_reference: str | None,",
          "77:     github_repository: str,",
          "78:     python_version: str,",
          "79: ):",
          "80:     constraints_base = f\"https://raw.githubusercontent.com/{github_repository}/{constraints_reference}\"",
          "81:     location = f\"{constraints_base}/{constraints_mode}-{python_version}.txt\"",
          "83:     return location",
          "",
          "[Removed Lines]",
          "82:     console.print(f\"[info]Determined constraints as: {location}\")",
          "",
          "[Added Lines]",
          "79:     providers: bool,",
          "83:     console.print(f\"[info]Determined {'providers' if providers else 'airflow'} constraints as: {location}\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "126:         constraints_reference=airflow_constraints_reference,",
          "127:         github_repository=github_repository,",
          "128:         python_version=python_version,",
          "129:     )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "130:         providers=False,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "157:         constraints_reference=providers_constraints_reference,",
          "158:         python_version=python_version,",
          "159:         github_repository=github_repository,",
          "160:     )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "162:         providers=True,",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ba3213bb9378185d99fb2141b8423851cbd191e8",
      "candidate_info": {
        "commit_hash": "ba3213bb9378185d99fb2141b8423851cbd191e8",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ba3213bb9378185d99fb2141b8423851cbd191e8",
        "files": [
          ".github/workflows/ci.yml",
          "Dockerfile",
          "Dockerfile.ci",
          "IMAGES.rst",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/global_constants.py",
          "docs/docker-stack/build-arg-ref.rst",
          "scripts/docker/common.sh"
        ],
        "message": "Upgrade to just released `pip` 23.3.2 (#36271)\n\n(cherry picked from commit 41096e0c266e3adb0ac3985d2609701f53aded00)",
        "before_after_code_files": [
          "Dockerfile.ci||Dockerfile.ci",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py",
          "scripts/docker/common.sh||scripts/docker/common.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "Dockerfile.ci||Dockerfile.ci": [
          "File: Dockerfile.ci -> Dockerfile.ci",
          "--- Hunk 1 ---",
          "[Context before]",
          "491: function common::override_pip_version_if_needed() {",
          "492:     if [[ -n ${AIRFLOW_VERSION} ]]; then",
          "493:         if [[ ${AIRFLOW_VERSION} =~ ^2\\.0.* || ${AIRFLOW_VERSION} =~ ^1\\.* ]]; then",
          "495:         fi",
          "496:     fi",
          "497: }",
          "",
          "[Removed Lines]",
          "494:             export AIRFLOW_PIP_VERSION=\"23.3.1\"",
          "",
          "[Added Lines]",
          "494:             export AIRFLOW_PIP_VERSION=\"23.3.2\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "995: # THE 3 LINES ARE ONLY NEEDED IN ORDER TO MAKE PYMSSQL BUILD WORK WITH LATEST CYTHON",
          "996: # AND SHOULD BE REMOVED WHEN WORKAROUND IN install_mssql.sh IS REMOVED",
          "998: ENV AIRFLOW_PIP_VERSION=${AIRFLOW_PIP_VERSION}",
          "999: COPY --from=scripts common.sh /scripts/docker/",
          "",
          "[Removed Lines]",
          "997: ARG AIRFLOW_PIP_VERSION=23.3.1",
          "",
          "[Added Lines]",
          "997: ARG AIRFLOW_PIP_VERSION=23.3.2",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1059: ARG AIRFLOW_PRE_CACHED_PIP_PACKAGES=\"true\"",
          "1060: # By default in the image, we are installing all providers when installing from sources",
          "1061: ARG INSTALL_PROVIDERS_FROM_SOURCES=\"true\"",
          "1063: # Setup PIP",
          "1064: # By default PIP install run without cache to make image smaller",
          "1065: ARG PIP_NO_CACHE_DIR=\"true\"",
          "",
          "[Removed Lines]",
          "1062: ARG AIRFLOW_PIP_VERSION=23.3.1",
          "",
          "[Added Lines]",
          "1062: ARG AIRFLOW_PIP_VERSION=23.3.2",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "205:     comparable_version: Version",
          "209: WHEEL_VERSION = \"0.36.2\"",
          "210: GITPYTHON_VERSION = \"3.1.40\"",
          "211: RICH_VERSION = \"13.7.0\"",
          "",
          "[Removed Lines]",
          "208: AIRFLOW_PIP_VERSION = \"23.3.1\"",
          "",
          "[Added Lines]",
          "208: AIRFLOW_PIP_VERSION = \"23.3.2\"",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py": [
          "File: dev/breeze/src/airflow_breeze/global_constants.py -> dev/breeze/src/airflow_breeze/global_constants.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "121: ALLOWED_MSSQL_VERSIONS = [\"2017-latest\", \"2019-latest\"]",
          "125: # packages that  providers docs",
          "126: REGULAR_DOC_PACKAGES = [",
          "",
          "[Removed Lines]",
          "123: PIP_VERSION = \"23.3.1\"",
          "",
          "[Added Lines]",
          "123: PIP_VERSION = \"23.3.2\"",
          "",
          "---------------"
        ],
        "scripts/docker/common.sh||scripts/docker/common.sh": [
          "File: scripts/docker/common.sh -> scripts/docker/common.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "43: function common::override_pip_version_if_needed() {",
          "44:     if [[ -n ${AIRFLOW_VERSION} ]]; then",
          "45:         if [[ ${AIRFLOW_VERSION} =~ ^2\\.0.* || ${AIRFLOW_VERSION} =~ ^1\\.* ]]; then",
          "47:         fi",
          "48:     fi",
          "49: }",
          "",
          "[Removed Lines]",
          "46:             export AIRFLOW_PIP_VERSION=\"23.3.1\"",
          "",
          "[Added Lines]",
          "46:             export AIRFLOW_PIP_VERSION=\"23.3.2\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7237331ee162abaf023e67ebf2b414833d138deb",
      "candidate_info": {
        "commit_hash": "7237331ee162abaf023e67ebf2b414833d138deb",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7237331ee162abaf023e67ebf2b414833d138deb",
        "files": [
          "airflow/www/auth.py",
          "airflow/www/views.py"
        ],
        "message": "Allow anoymous user edit/show resource when set `AUTH_ROLE_PUBLIC` (#36750)\n\n(cherry picked from commit 512461c74523f8e015b5ccc1cc01184e4fd3960f)",
        "before_after_code_files": [
          "airflow/www/auth.py||airflow/www/auth.py",
          "airflow/www/views.py||airflow/www/views.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/auth.py||airflow/www/auth.py": [
          "File: airflow/www/auth.py -> airflow/www/auth.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "107:             _permission_name = self.method_permission_name.get(f.__name__)",
          "108:             if _permission_name:",
          "109:                 permission_str = f\"{PERMISSION_PREFIX}{_permission_name}\"",
          "118:         ):",
          "119:             return f(self, *args, **kwargs)",
          "120:         else:",
          "",
          "[Removed Lines]",
          "110:         if (",
          "111:             get_auth_manager().is_logged_in()",
          "112:             and permission_str in self.base_permissions",
          "113:             and self.appbuilder.sm.has_access(",
          "114:                 action_name=permission_str,",
          "115:                 resource_name=self.class_permission_name,",
          "116:                 resource_pk=kwargs.get(\"pk\"),",
          "117:             )",
          "",
          "[Added Lines]",
          "110:         if permission_str in self.base_permissions and self.appbuilder.sm.has_access(",
          "111:             action_name=permission_str,",
          "112:             resource_name=self.class_permission_name,",
          "113:             resource_pk=kwargs.get(\"pk\"),",
          "",
          "---------------"
        ],
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "133: from airflow.utils.timezone import td_format, utcnow",
          "134: from airflow.version import version",
          "135: from airflow.www import auth, utils as wwwutils",
          "137: from airflow.www.decorators import action_logging, gzipped",
          "138: from airflow.www.extensions.init_auth_manager import get_auth_manager",
          "139: from airflow.www.forms import (",
          "",
          "[Removed Lines]",
          "136: from airflow.www.auth import has_access_with_pk",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "4002:         return attribute",
          "4004:     @expose(\"/show/<pk>\", methods=[\"GET\"])",
          "4006:     def show(self, pk):",
          "4007:         \"\"\"",
          "4008:         Show view.",
          "",
          "[Removed Lines]",
          "4005:     @has_access_with_pk",
          "",
          "[Added Lines]",
          "4004:     @auth.has_access_with_pk",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "4024:         )",
          "4026:     @expose(\"/edit/<pk>\", methods=[\"GET\", \"POST\"])",
          "4028:     def edit(self, pk):",
          "4029:         \"\"\"",
          "4030:         Edit view.",
          "",
          "[Removed Lines]",
          "4027:     @has_access_with_pk",
          "",
          "[Added Lines]",
          "4026:     @auth.has_access_with_pk",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "4048:             )",
          "4050:     @expose(\"/delete/<pk>\", methods=[\"GET\", \"POST\"])",
          "4052:     def delete(self, pk):",
          "4053:         \"\"\"",
          "4054:         Delete view.",
          "",
          "[Removed Lines]",
          "4051:     @has_access_with_pk",
          "",
          "[Added Lines]",
          "4050:     @auth.has_access_with_pk",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "4746:         return redirect(self.get_redirect())",
          "4748:     @expose(\"/delete/<pk>\", methods=[\"GET\", \"POST\"])",
          "4750:     def delete(self, pk):",
          "4751:         \"\"\"Single delete.\"\"\"",
          "4752:         if models.Pool.is_default_pool(pk):",
          "",
          "[Removed Lines]",
          "4749:     @has_access_with_pk",
          "",
          "[Added Lines]",
          "4748:     @auth.has_access_with_pk",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ca1aa4a5dce0ebe8d4aa5b378524b698bb105b8d",
      "candidate_info": {
        "commit_hash": "ca1aa4a5dce0ebe8d4aa5b378524b698bb105b8d",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ca1aa4a5dce0ebe8d4aa5b378524b698bb105b8d",
        "files": [
          "docs/apache-airflow/howto/operator/python.rst"
        ],
        "message": "Update admonitions in Python operator doc to reflect sentiment (#36340)\n\nWithin the Python operator how-to guide, there are doc admonitions that don't necessarily reflect the sentiment of the call-out. For example, there are several places where there is a recommendation to use the TaskFlow version of an operator but the admonition is a warning; this is more of a tip when authoring DAGs.\n\nThis PR hopes to match the sentiment of the call-out with an \"appropriate\" admonition in the doc for better context and visbility when reading the doc.\n\n(cherry picked from commit f76060c45a67771442162c65cf5cc7f7882b6f28)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "4f05fe6583d32efab6141be516c71b55d769c1ef",
      "candidate_info": {
        "commit_hash": "4f05fe6583d32efab6141be516c71b55d769c1ef",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4f05fe6583d32efab6141be516c71b55d769c1ef",
        "files": [
          "airflow/decorators/branch_external_python.py",
          "airflow/decorators/branch_python.py",
          "airflow/decorators/branch_virtualenv.py",
          "airflow/decorators/external_python.py",
          "airflow/decorators/python_virtualenv.py",
          "airflow/decorators/short_circuit.py",
          "airflow/models/abstractoperator.py",
          "tests/decorators/test_branch_virtualenv.py",
          "tests/decorators/test_external_python.py",
          "tests/decorators/test_python_virtualenv.py"
        ],
        "message": "Fix Python-based decorators templating (#36103)\n\nTemplating of Python-based decorators has been broken since\nimplementation. The decorators used template_fields definition\nas defined originally in PythonOperator rather than the ones from\nvirtualenv because template fields were redefined in\n_PythonDecoratedOperator class and they took precedence (MRU).\n\nThis PR add explicit copying of template_fields from the operators\nthat they are decorating.\n\nFixes: #36102\n(cherry picked from commit 3904206b69428525db31ff7813daa0322f7b83e8)",
        "before_after_code_files": [
          "airflow/decorators/branch_external_python.py||airflow/decorators/branch_external_python.py",
          "airflow/decorators/branch_python.py||airflow/decorators/branch_python.py",
          "airflow/decorators/branch_virtualenv.py||airflow/decorators/branch_virtualenv.py",
          "airflow/decorators/external_python.py||airflow/decorators/external_python.py",
          "airflow/decorators/python_virtualenv.py||airflow/decorators/python_virtualenv.py",
          "airflow/decorators/short_circuit.py||airflow/decorators/short_circuit.py",
          "airflow/models/abstractoperator.py||airflow/models/abstractoperator.py",
          "tests/decorators/test_branch_virtualenv.py||tests/decorators/test_branch_virtualenv.py",
          "tests/decorators/test_external_python.py||tests/decorators/test_external_python.py",
          "tests/decorators/test_python_virtualenv.py||tests/decorators/test_python_virtualenv.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/decorators/branch_external_python.py||airflow/decorators/branch_external_python.py": [
          "File: airflow/decorators/branch_external_python.py -> airflow/decorators/branch_external_python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: class _BranchExternalPythonDecoratedOperator(_PythonDecoratedOperator, BranchExternalPythonOperator):",
          "30:     \"\"\"Wraps a Python callable and captures args/kwargs when called for execution.\"\"\"",
          "32:     custom_operator_name: str = \"@task.branch_external_python\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32:     template_fields = BranchExternalPythonOperator.template_fields",
          "",
          "---------------"
        ],
        "airflow/decorators/branch_python.py||airflow/decorators/branch_python.py": [
          "File: airflow/decorators/branch_python.py -> airflow/decorators/branch_python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: class _BranchPythonDecoratedOperator(_PythonDecoratedOperator, BranchPythonOperator):",
          "30:     \"\"\"Wraps a Python callable and captures args/kwargs when called for execution.\"\"\"",
          "32:     custom_operator_name: str = \"@task.branch\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32:     template_fields = BranchPythonOperator.template_fields",
          "",
          "---------------"
        ],
        "airflow/decorators/branch_virtualenv.py||airflow/decorators/branch_virtualenv.py": [
          "File: airflow/decorators/branch_virtualenv.py -> airflow/decorators/branch_virtualenv.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: class _BranchPythonVirtualenvDecoratedOperator(_PythonDecoratedOperator, BranchPythonVirtualenvOperator):",
          "30:     \"\"\"Wraps a Python callable and captures args/kwargs when called for execution.\"\"\"",
          "32:     custom_operator_name: str = \"@task.branch_virtualenv\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32:     template_fields = BranchPythonVirtualenvOperator.template_fields",
          "",
          "---------------"
        ],
        "airflow/decorators/external_python.py||airflow/decorators/external_python.py": [
          "File: airflow/decorators/external_python.py -> airflow/decorators/external_python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: class _PythonExternalDecoratedOperator(_PythonDecoratedOperator, ExternalPythonOperator):",
          "30:     \"\"\"Wraps a Python callable and captures args/kwargs when called for execution.\"\"\"",
          "32:     custom_operator_name: str = \"@task.external_python\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32:     template_fields = ExternalPythonOperator.template_fields",
          "",
          "---------------"
        ],
        "airflow/decorators/python_virtualenv.py||airflow/decorators/python_virtualenv.py": [
          "File: airflow/decorators/python_virtualenv.py -> airflow/decorators/python_virtualenv.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: class _PythonVirtualenvDecoratedOperator(_PythonDecoratedOperator, PythonVirtualenvOperator):",
          "30:     \"\"\"Wraps a Python callable and captures args/kwargs when called for execution.\"\"\"",
          "32:     custom_operator_name: str = \"@task.virtualenv\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32:     template_fields = PythonVirtualenvOperator.template_fields",
          "",
          "---------------"
        ],
        "airflow/decorators/short_circuit.py||airflow/decorators/short_circuit.py": [
          "File: airflow/decorators/short_circuit.py -> airflow/decorators/short_circuit.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: class _ShortCircuitDecoratedOperator(_PythonDecoratedOperator, ShortCircuitOperator):",
          "30:     \"\"\"Wraps a Python callable and captures args/kwargs when called for execution.\"\"\"",
          "32:     custom_operator_name: str = \"@task.short_circuit\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32:     template_fields = ShortCircuitOperator.template_fields",
          "",
          "---------------"
        ],
        "airflow/models/abstractoperator.py||airflow/models/abstractoperator.py": [
          "File: airflow/models/abstractoperator.py -> airflow/models/abstractoperator.py"
        ],
        "tests/decorators/test_branch_virtualenv.py||tests/decorators/test_branch_virtualenv.py": [
          "File: tests/decorators/test_branch_virtualenv.py -> tests/decorators/test_branch_virtualenv.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31:     # possibilities. So we are increasing the timeout for this test to 3x of the default timeout",
          "32:     @pytest.mark.execution_timeout(180)",
          "33:     @pytest.mark.parametrize(\"branch_task_name\", [\"task_1\", \"task_2\"])",
          "35:         @task",
          "36:         def dummy_f():",
          "37:             pass",
          "",
          "[Removed Lines]",
          "34:     def test_branch_one(self, dag_maker, branch_task_name):",
          "",
          "[Added Lines]",
          "34:     def test_branch_one(self, dag_maker, branch_task_name, tmp_path):",
          "35:         requirements_file = tmp_path / \"requirements.txt\"",
          "36:         requirements_file.write_text(\"funcsigs==0.4\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "58:         else:",
          "61:             def branch_operator():",
          "62:                 import funcsigs",
          "64:                 print(f\"We successfully imported funcsigs version {funcsigs.__version__}\")",
          "65:                 return \"task_2\"",
          "68:             branchoperator = branch_operator()",
          "69:             df = dummy_f()",
          "70:             task_1 = task_1()",
          "",
          "[Removed Lines]",
          "60:             @task.branch_virtualenv(task_id=\"branching\", requirements=[\"funcsigs\"])",
          "67:         with dag_maker():",
          "",
          "[Added Lines]",
          "63:             @task.branch_virtualenv(task_id=\"branching\", requirements=\"requirements.txt\")",
          "70:         with dag_maker(template_searchpath=tmp_path.as_posix()):",
          "",
          "---------------"
        ],
        "tests/decorators/test_external_python.py||tests/decorators/test_external_python.py": [
          "File: tests/decorators/test_external_python.py -> tests/decorators/test_external_python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "75:         ret.operator.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE)",
          "77:     def test_no_dill_installed_raises_exception_when_use_dill(self, dag_maker, venv_python):",
          "78:         @task.external_python(python=venv_python, use_dill=True)",
          "79:         def f():",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "77:     def test_with_templated_python(self, dag_maker, venv_python_with_dill):",
          "78:         # add template that produces empty string when rendered",
          "79:         templated_python_with_dill = venv_python_with_dill.as_posix() + \"{{ '' }}\"",
          "81:         @task.external_python(python=templated_python_with_dill, use_dill=True)",
          "82:         def f():",
          "83:             \"\"\"Import dill to double-check it is installed .\"\"\"",
          "84:             import dill  # noqa: F401",
          "86:         with dag_maker():",
          "87:             ret = f()",
          "89:         ret.operator.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE)",
          "",
          "---------------"
        ],
        "tests/decorators/test_python_virtualenv.py||tests/decorators/test_python_virtualenv.py": [
          "File: tests/decorators/test_python_virtualenv.py -> tests/decorators/test_python_virtualenv.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "104:         ret.operator.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE)",
          "106:     def test_unpinned_requirements(self, dag_maker):",
          "107:         @task.virtualenv(",
          "108:             system_site_packages=False,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "106:     def test_with_requirements_file(self, dag_maker, tmp_path):",
          "107:         requirements_file = tmp_path / \"requirements.txt\"",
          "108:         requirements_file.write_text(\"funcsigs==0.4\\nattrs==23.1.0\")",
          "110:         @task.virtualenv(",
          "111:             system_site_packages=False,",
          "112:             requirements=\"requirements.txt\",",
          "113:             python_version=PYTHON_VERSION,",
          "114:             use_dill=True,",
          "115:         )",
          "116:         def f():",
          "117:             import funcsigs",
          "119:             if funcsigs.__version__ != \"0.4\":",
          "120:                 raise Exception",
          "122:             import attrs",
          "124:             if attrs.__version__ != \"23.1.0\":",
          "125:                 raise Exception",
          "127:         with dag_maker(template_searchpath=tmp_path.as_posix()):",
          "128:             ret = f()",
          "130:         ret.operator.run(start_date=DEFAULT_DATE, end_date=DEFAULT_DATE)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "171981fce86d8448a880914040680475b2e5edaa",
      "candidate_info": {
        "commit_hash": "171981fce86d8448a880914040680475b2e5edaa",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/171981fce86d8448a880914040680475b2e5edaa",
        "files": [
          "airflow/jobs/scheduler_job_runner.py"
        ],
        "message": "Fix the type hint for tis_query in _process_executor_events (#36655)\n\n(cherry picked from commit 98f5ce269acf7165b4c620f7a018d5ba34a7606e)",
        "before_after_code_files": [
          "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py": [
          "File: airflow/jobs/scheduler_job_runner.py -> airflow/jobs/scheduler_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "706:         query = select(TI).where(filter_for_tis).options(selectinload(TI.dag_model))",
          "707:         # row lock this entire set of taskinstances to make sure the scheduler doesn't fail when we have",
          "708:         # multi-schedulers",
          "710:             query,",
          "711:             of=TI,",
          "712:             session=session,",
          "714:         )",
          "716:         for ti in tis:",
          "717:             try_number = ti_primary_key_to_try_number_map[ti.key.primary]",
          "718:             buffer_key = ti.key.with_try_number(try_number)",
          "",
          "[Removed Lines]",
          "709:         tis: Iterator[TI] = with_row_locks(",
          "715:         tis = session.scalars(tis)",
          "",
          "[Added Lines]",
          "709:         tis_query: Query = with_row_locks(",
          "715:         tis: Iterator[TI] = session.scalars(tis_query)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e1d2e7f3b52c40e34a5b947fc64cc6e641841c81",
      "candidate_info": {
        "commit_hash": "e1d2e7f3b52c40e34a5b947fc64cc6e641841c81",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e1d2e7f3b52c40e34a5b947fc64cc6e641841c81",
        "files": [
          "BREEZE.rst",
          "TESTING.rst",
          "scripts/ci/pre_commit/common_precommit_utils.py"
        ],
        "message": "Update BREEZE.rst with different test example (#36234)\n\nUpdating the breeze docs with different pytest example as the function mentioned in the example is removed from the test_core.py\n\n(cherry picked from commit 71c726d52d5a8a30f59268cc175560a4244c8016)",
        "before_after_code_files": [
          "scripts/ci/pre_commit/common_precommit_utils.py||scripts/ci/pre_commit/common_precommit_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/pre_commit/common_precommit_utils.py||scripts/ci/pre_commit/common_precommit_utils.py": [
          "File: scripts/ci/pre_commit/common_precommit_utils.py -> scripts/ci/pre_commit/common_precommit_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "80:     if os.environ.get(\"SKIP_BREEZE_PRE_COMMITS\"):",
          "81:         console.print(\"[yellow]Skipping breeze pre-commit as SKIP_BREEZE_PRE_COMMIT is set\")",
          "83:     if shutil.which(\"breeze\") is None:",
          "84:         console.print(",
          "85:             \"[red]The `breeze` command is not on path.[/]\\n\\n\"",
          "",
          "[Removed Lines]",
          "82:         sys.exit(1)",
          "",
          "[Added Lines]",
          "82:         sys.exit(0)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cde72b6e0e6f7a03e3840a931f55feb0760b5134",
      "candidate_info": {
        "commit_hash": "cde72b6e0e6f7a03e3840a931f55feb0760b5134",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cde72b6e0e6f7a03e3840a931f55feb0760b5134",
        "files": [
          "airflow/providers/hashicorp/_internal_client/vault_client.py",
          "airflow/providers/hashicorp/provider.yaml",
          "generated/provider_dependencies.json",
          "tests/providers/hashicorp/_internal_client/test_vault_client.py",
          "tests/providers/hashicorp/hooks/test_vault.py",
          "tests/providers/hashicorp/secrets/test_vault.py"
        ],
        "message": "Explicitly passing `raise_on_deleted_version=True` to `read_secret_version` in Hashicorp operator (#36532)\n\n* explicitly passing raise_on_deleted_version=True to read_secret_version\n\n* fix tests\n\n* update hvac version\n\n(cherry picked from commit cd5ab08d95aaf4c65e56a91f1843d04c09f27cb1)",
        "before_after_code_files": [
          "airflow/providers/hashicorp/_internal_client/vault_client.py||airflow/providers/hashicorp/_internal_client/vault_client.py",
          "tests/providers/hashicorp/_internal_client/test_vault_client.py||tests/providers/hashicorp/_internal_client/test_vault_client.py",
          "tests/providers/hashicorp/hooks/test_vault.py||tests/providers/hashicorp/hooks/test_vault.py",
          "tests/providers/hashicorp/secrets/test_vault.py||tests/providers/hashicorp/secrets/test_vault.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/hashicorp/_internal_client/vault_client.py||airflow/providers/hashicorp/_internal_client/vault_client.py": [
          "File: airflow/providers/hashicorp/_internal_client/vault_client.py -> airflow/providers/hashicorp/_internal_client/vault_client.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "373:                 response = self.client.secrets.kv.v1.read_secret(path=secret_path, mount_point=mount_point)",
          "374:             else:",
          "375:                 response = self.client.secrets.kv.v2.read_secret_version(",
          "377:                 )",
          "378:         except InvalidPath:",
          "379:             self.log.debug(\"Secret not found %s with mount point %s\", secret_path, mount_point)",
          "",
          "[Removed Lines]",
          "376:                     path=secret_path, mount_point=mount_point, version=secret_version",
          "",
          "[Added Lines]",
          "376:                     path=secret_path,",
          "377:                     mount_point=mount_point,",
          "378:                     version=secret_version,",
          "379:                     raise_on_deleted_version=True,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "422:         try:",
          "423:             mount_point, secret_path = self._parse_secret_path(secret_path)",
          "424:             return self.client.secrets.kv.v2.read_secret_version(",
          "426:             )",
          "427:         except InvalidPath:",
          "428:             self.log.debug(",
          "",
          "[Removed Lines]",
          "425:                 path=secret_path, mount_point=mount_point, version=secret_version",
          "",
          "[Added Lines]",
          "428:                 path=secret_path,",
          "429:                 mount_point=mount_point,",
          "430:                 version=secret_version,",
          "431:                 raise_on_deleted_version=True,",
          "",
          "---------------"
        ],
        "tests/providers/hashicorp/_internal_client/test_vault_client.py||tests/providers/hashicorp/_internal_client/test_vault_client.py": [
          "File: tests/providers/hashicorp/_internal_client/test_vault_client.py -> tests/providers/hashicorp/_internal_client/test_vault_client.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "641:         secret = vault_client.get_secret(secret_path=\"missing\")",
          "642:         assert secret is None",
          "643:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "645:         )",
          "647:     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")",
          "",
          "[Removed Lines]",
          "644:             mount_point=\"secret\", path=\"missing\", version=None",
          "",
          "[Added Lines]",
          "644:             mount_point=\"secret\", path=\"missing\", version=None, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "661:         assert secret is None",
          "662:         assert \"secret\" == vault_client.mount_point",
          "663:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "665:         )",
          "667:     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")",
          "",
          "[Removed Lines]",
          "664:             mount_point=\"secret\", path=\"missing\", version=None",
          "",
          "[Added Lines]",
          "664:             mount_point=\"secret\", path=\"missing\", version=None, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "716:         secret = vault_client.get_secret(secret_path=\"path/to/secret\")",
          "717:         assert {\"secret_key\": \"secret_value\"} == secret",
          "718:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "720:         )",
          "722:     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")",
          "",
          "[Removed Lines]",
          "719:             mount_point=\"secret\", path=\"path/to/secret\", version=None",
          "",
          "[Added Lines]",
          "719:             mount_point=\"secret\", path=\"path/to/secret\", version=None, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "754:         secret = vault_client.get_secret(secret_path=\"mount_point/path/to/secret\")",
          "755:         assert {\"secret_key\": \"secret_value\"} == secret",
          "756:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "758:         )",
          "760:     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")",
          "",
          "[Removed Lines]",
          "757:             mount_point=\"mount_point\", path=\"path/to/secret\", version=None",
          "",
          "[Added Lines]",
          "757:             mount_point=\"mount_point\", path=\"path/to/secret\", version=None, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "791:         secret = vault_client.get_secret(secret_path=\"missing\", secret_version=1)",
          "792:         assert {\"secret_key\": \"secret_value\"} == secret",
          "793:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "795:         )",
          "797:     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")",
          "",
          "[Removed Lines]",
          "794:             mount_point=\"secret\", path=\"missing\", version=1",
          "",
          "[Added Lines]",
          "794:             mount_point=\"secret\", path=\"missing\", version=1, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1015:             \"auth\": None,",
          "1016:         } == metadata",
          "1017:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "1019:         )",
          "1021:     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")",
          "",
          "[Removed Lines]",
          "1018:             mount_point=\"secret\", path=\"missing\", version=None",
          "",
          "[Added Lines]",
          "1018:             mount_point=\"secret\", path=\"missing\", version=None, raise_on_deleted_version=True",
          "",
          "---------------"
        ],
        "tests/providers/hashicorp/hooks/test_vault.py||tests/providers/hashicorp/hooks/test_vault.py": [
          "File: tests/providers/hashicorp/hooks/test_vault.py -> tests/providers/hashicorp/hooks/test_vault.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1005:         secret = test_hook.get_secret(secret_path=\"missing\")",
          "1006:         assert {\"secret_key\": \"secret_value\"} == secret",
          "1007:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "1009:         )",
          "1011:     @mock.patch(\"airflow.providers.hashicorp.hooks.vault.VaultHook.get_connection\")",
          "",
          "[Removed Lines]",
          "1008:             mount_point=\"secret\", path=\"missing\", version=None",
          "",
          "[Added Lines]",
          "1008:             mount_point=\"secret\", path=\"missing\", version=None, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1044:         secret = test_hook.get_secret(secret_path=\"missing\", secret_version=1)",
          "1045:         assert {\"secret_key\": \"secret_value\"} == secret",
          "1046:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "1048:         )",
          "1050:     @mock.patch(\"airflow.providers.hashicorp.hooks.vault.VaultHook.get_connection\")",
          "",
          "[Removed Lines]",
          "1047:             mount_point=\"secret\", path=\"missing\", version=1",
          "",
          "[Added Lines]",
          "1047:             mount_point=\"secret\", path=\"missing\", version=1, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1189:             \"auth\": None,",
          "1190:         } == metadata",
          "1191:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "1193:         )",
          "1195:     @mock.patch(\"airflow.providers.hashicorp.hooks.vault.VaultHook.get_connection\")",
          "",
          "[Removed Lines]",
          "1192:             mount_point=\"secret\", path=\"missing\", version=None",
          "",
          "[Added Lines]",
          "1192:             mount_point=\"secret\", path=\"missing\", version=None, raise_on_deleted_version=True",
          "",
          "---------------"
        ],
        "tests/providers/hashicorp/secrets/test_vault.py||tests/providers/hashicorp/secrets/test_vault.py": [
          "File: tests/providers/hashicorp/secrets/test_vault.py -> tests/providers/hashicorp/secrets/test_vault.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "302:         test_client = VaultBackend(**kwargs)",
          "303:         assert test_client.get_conn_uri(conn_id=\"test_mysql\") is None",
          "304:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "306:         )",
          "307:         assert test_client.get_connection(conn_id=\"test_mysql\") is None",
          "",
          "[Removed Lines]",
          "305:             mount_point=\"airflow\", path=\"connections/test_mysql\", version=None",
          "",
          "[Added Lines]",
          "305:             mount_point=\"airflow\", path=\"connections/test_mysql\", version=None, raise_on_deleted_version=True",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "454:         test_client = VaultBackend(**kwargs)",
          "455:         assert test_client.get_variable(\"hello\") is None",
          "456:         mock_client.secrets.kv.v2.read_secret_version.assert_called_once_with(",
          "458:         )",
          "459:         assert test_client.get_variable(\"hello\") is None",
          "",
          "[Removed Lines]",
          "457:             mount_point=\"airflow\", path=\"variables/hello\", version=None",
          "",
          "[Added Lines]",
          "457:             mount_point=\"airflow\", path=\"variables/hello\", version=None, raise_on_deleted_version=True",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bbf6ebc703f83963345acdbd69da089749fc1c4d",
      "candidate_info": {
        "commit_hash": "bbf6ebc703f83963345acdbd69da089749fc1c4d",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/bbf6ebc703f83963345acdbd69da089749fc1c4d",
        "files": [
          "airflow/decorators/__init__.pyi"
        ],
        "message": "Remove redundant `docker` decorator type annotations (#36406)\n\n(cherry picked from commit e3fd0d1a985fc99e4af8edaccda01f97cb9693d9)",
        "before_after_code_files": [
          "airflow/decorators/__init__.pyi||airflow/decorators/__init__.pyi"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/decorators/__init__.pyi||airflow/decorators/__init__.pyi": [
          "File: airflow/decorators/__init__.pyi -> airflow/decorators/__init__.pyi",
          "--- Hunk 1 ---",
          "[Context before]",
          "354:         privileged: bool = False,",
          "355:         cap_add: str | None = None,",
          "356:         extra_hosts: dict[str, str] | None = None,",
          "359:         timeout: int = 60,",
          "360:         device_requests: list[dict] | None = None,",
          "361:         log_opts_max_size: str | None = None,",
          "",
          "[Removed Lines]",
          "357:         retrieve_output: bool = False,",
          "358:         retrieve_output_path: str | None = None,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "441:         :param cap_add: Include container capabilities",
          "442:         :param extra_hosts: Additional hostnames to resolve inside the container,",
          "443:             as a mapping of hostname to IP address.",
          "448:         :param device_requests: Expose host resources such as GPUs to the container.",
          "449:         :param log_opts_max_size: The maximum size of the log before it is rolled.",
          "450:             A positive integer plus a modifier representing the unit of measure (k, m, or g).",
          "",
          "[Removed Lines]",
          "444:         :param retrieve_output: Should this docker image consistently attempt to pull from and output",
          "445:             file before manually shutting down the image. Useful for cases where users want a pickle serialized",
          "446:             output that is not posted to logs",
          "447:         :param retrieve_output_path: path for output file that will be retrieved and passed to xcom",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "830fcb02b1dd2759553a4a2e4a555f4c22f3598e",
      "candidate_info": {
        "commit_hash": "830fcb02b1dd2759553a4a2e4a555f4c22f3598e",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/830fcb02b1dd2759553a4a2e4a555f4c22f3598e",
        "files": [
          "docs/apache-airflow/administration-and-deployment/priority-weight.rst"
        ],
        "message": "Use cards when describing priority weighting methods (#36411)\n\n(cherry picked from commit 22654fa2f618d68d394fb837446ad2d4e3b81743)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "6e31c0f2d75e6fc7f5360ca125890e4f97723b11",
      "candidate_info": {
        "commit_hash": "6e31c0f2d75e6fc7f5360ca125890e4f97723b11",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/6e31c0f2d75e6fc7f5360ca125890e4f97723b11",
        "files": [
          "airflow/timetables/_cron.py",
          "kubernetes_tests/test_kubernetes_pod_operator.py",
          "tests/api_connexion/test_parameters.py",
          "tests/providers/cncf/kubernetes/utils/test_pod_manager.py",
          "tests/providers/openlineage/plugins/test_utils.py",
          "tests/serialization/test_serialized_objects.py",
          "tests/timetables/test_events_timetable.py",
          "tests/timetables/test_interval_timetable.py",
          "tests/timetables/test_trigger_timetable.py",
          "tests/timetables/test_workday_timetable.py"
        ],
        "message": "Use UTC explicitly in timetable tests (#36082)\n\n(cherry picked from commit c1d28b36e4ecfad6df2e5c0d412c8b7f8d38c11d)",
        "before_after_code_files": [
          "airflow/timetables/_cron.py||airflow/timetables/_cron.py",
          "kubernetes_tests/test_kubernetes_pod_operator.py||kubernetes_tests/test_kubernetes_pod_operator.py",
          "tests/api_connexion/test_parameters.py||tests/api_connexion/test_parameters.py",
          "tests/providers/cncf/kubernetes/utils/test_pod_manager.py||tests/providers/cncf/kubernetes/utils/test_pod_manager.py",
          "tests/providers/openlineage/plugins/test_utils.py||tests/providers/openlineage/plugins/test_utils.py",
          "tests/serialization/test_serialized_objects.py||tests/serialization/test_serialized_objects.py",
          "tests/timetables/test_events_timetable.py||tests/timetables/test_events_timetable.py",
          "tests/timetables/test_interval_timetable.py||tests/timetables/test_interval_timetable.py",
          "tests/timetables/test_trigger_timetable.py||tests/timetables/test_trigger_timetable.py",
          "tests/timetables/test_workday_timetable.py||tests/timetables/test_workday_timetable.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/timetables/_cron.py||airflow/timetables/_cron.py": [
          "File: airflow/timetables/_cron.py -> airflow/timetables/_cron.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: import datetime",
          "20: from typing import TYPE_CHECKING, Any",
          "22: from cron_descriptor import CasingTypeEnum, ExpressionDescriptor, FormatException, MissingFieldException",
          "23: from croniter import CroniterBadCronError, CroniterBadDateError, croniter",
          "26: from airflow.exceptions import AirflowTimetableInvalid",
          "27: from airflow.utils.dates import cron_presets",
          "",
          "[Removed Lines]",
          "24: from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "22: import pendulum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "30: if TYPE_CHECKING:",
          "31:     from pendulum import DateTime",
          "34: def _covers_every_hour(cron: croniter) -> bool:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32:     from pendulum.tz.timezone import Timezone",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "66:         self._expression = cron_presets.get(cron, cron)",
          "68:         if isinstance(timezone, str):",
          "70:         self._timezone = timezone",
          "72:         try:",
          "",
          "[Removed Lines]",
          "69:             timezone = Timezone(timezone)",
          "",
          "[Added Lines]",
          "70:             timezone = pendulum.tz.timezone(timezone)",
          "",
          "---------------"
        ],
        "kubernetes_tests/test_kubernetes_pod_operator.py||kubernetes_tests/test_kubernetes_pod_operator.py": [
          "File: kubernetes_tests/test_kubernetes_pod_operator.py -> kubernetes_tests/test_kubernetes_pod_operator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: from unittest.mock import ANY, MagicMock",
          "27: from uuid import uuid4",
          "29: import pytest",
          "30: from kubernetes import client",
          "31: from kubernetes.client import V1EnvVar, V1PodSecurityContext, V1SecurityContext, models as k8s",
          "32: from kubernetes.client.api_client import ApiClient",
          "33: from kubernetes.client.rest import ApiException",
          "36: from airflow.exceptions import AirflowException, AirflowSkipException",
          "37: from airflow.models.connection import Connection",
          "",
          "[Removed Lines]",
          "34: from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "29: import pendulum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "54: def create_context(task) -> Context:",
          "55:     dag = DAG(dag_id=\"dag\")",
          "57:     dag_run = DagRun(",
          "58:         dag_id=dag.dag_id,",
          "59:         execution_date=execution_date,",
          "",
          "[Removed Lines]",
          "56:     execution_date = timezone.datetime(2016, 1, 1, 1, 0, 0, tzinfo=Timezone(\"Europe/Amsterdam\"))",
          "",
          "[Added Lines]",
          "56:     execution_date = timezone.datetime(2016, 1, 1, 1, 0, 0, tzinfo=pendulum.tz.timezone(\"Europe/Amsterdam\"))",
          "",
          "---------------"
        ],
        "tests/api_connexion/test_parameters.py||tests/api_connexion/test_parameters.py": [
          "File: tests/api_connexion/test_parameters.py -> tests/api_connexion/test_parameters.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: from unittest import mock",
          "21: import pytest",
          "25: from airflow.api_connexion.exceptions import BadRequest",
          "26: from airflow.api_connexion.parameters import (",
          "",
          "[Removed Lines]",
          "22: from pendulum import DateTime",
          "23: from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "21: import pendulum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "107:         decorated_endpoint(param_a=\"2020-01-01T0:0:00+00:00\")",
          "111:     def test_should_propagate_exceptions(self):",
          "112:         decorator = format_parameters({\"param_a\": format_datetime})",
          "",
          "[Removed Lines]",
          "109:         endpoint.assert_called_once_with(param_a=DateTime(2020, 1, 1, 0, tzinfo=Timezone(\"UTC\")))",
          "",
          "[Added Lines]",
          "108:         endpoint.assert_called_once_with(param_a=pendulum.datetime(2020, 1, 1, 0, tz=\"UTC\"))",
          "",
          "---------------"
        ],
        "tests/providers/cncf/kubernetes/utils/test_pod_manager.py||tests/providers/cncf/kubernetes/utils/test_pod_manager.py": [
          "File: tests/providers/cncf/kubernetes/utils/test_pod_manager.py -> tests/providers/cncf/kubernetes/utils/test_pod_manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: from datetime import datetime",
          "21: from json.decoder import JSONDecodeError",
          "22: from types import SimpleNamespace",
          "24: from unittest import mock",
          "25: from unittest.mock import MagicMock",
          "",
          "[Removed Lines]",
          "23: from typing import cast",
          "",
          "[Added Lines]",
          "23: from typing import TYPE_CHECKING, cast",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "28: import pytest",
          "29: import time_machine",
          "30: from kubernetes.client.rest import ApiException",
          "33: from urllib3.exceptions import HTTPError as BaseHTTPError",
          "35: from airflow.exceptions import AirflowException",
          "",
          "[Removed Lines]",
          "31: from pendulum import DateTime",
          "32: from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "43: )",
          "44: from airflow.utils.timezone import utc",
          "47: class TestPodManager:",
          "48:     def setup_method(self):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "44: if TYPE_CHECKING:",
          "45:     from pendulum import DateTime",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "270:         status = self.pod_manager.fetch_container_logs(mock.MagicMock(), mock.MagicMock(), follow=True)",
          "274:     @mock.patch(\"airflow.providers.cncf.kubernetes.utils.pod_manager.PodManager.container_is_running\")",
          "275:     @mock.patch(\"airflow.providers.cncf.kubernetes.utils.pod_manager.PodManager.read_pod_logs\")",
          "",
          "[Removed Lines]",
          "272:         assert status.last_log_time == cast(DateTime, pendulum.parse(timestamp_string))",
          "",
          "[Added Lines]",
          "273:         assert status.last_log_time == cast(\"DateTime\", pendulum.parse(timestamp_string))",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "306:             mock_consumer_iter.side_effect = consumer_iter",
          "307:             mock_container_is_running.side_effect = [True, True, False]",
          "308:             status = self.pod_manager.fetch_container_logs(mock.MagicMock(), mock.MagicMock(), follow=True)",
          "310:         assert self.mock_progress_callback.call_count == expected_call_count",
          "312:     @mock.patch(\"airflow.providers.cncf.kubernetes.utils.pod_manager.PodManager.container_is_running\")",
          "",
          "[Removed Lines]",
          "309:         assert status.last_log_time == cast(DateTime, pendulum.parse(last_timestamp_string))",
          "",
          "[Added Lines]",
          "310:         assert status.last_log_time == cast(\"DateTime\", pendulum.parse(last_timestamp_string))",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "461:     def test_fetch_container_since_time(self, logs_available, container_running, mock_now):",
          "462:         \"\"\"If given since_time, should be used.\"\"\"",
          "463:         mock_pod = MagicMock()",
          "465:         logs_available.return_value = True",
          "466:         container_running.return_value = False",
          "467:         self.mock_kube_client.read_namespaced_pod_log.return_value = mock.MagicMock(",
          "468:             stream=mock.MagicMock(return_value=[b\"2021-01-01 hi\"])",
          "469:         )",
          "471:         self.pod_manager.fetch_container_logs(pod=mock_pod, container_name=\"base\", since_time=since_time)",
          "472:         args, kwargs = self.mock_kube_client.read_namespaced_pod_log.call_args_list[0]",
          "473:         assert kwargs[\"since_seconds\"] == 5",
          "",
          "[Removed Lines]",
          "464:         mock_now.return_value = DateTime(2020, 1, 1, 0, 0, 5, tzinfo=Timezone(\"UTC\"))",
          "470:         since_time = DateTime(2020, 1, 1, tzinfo=Timezone(\"UTC\"))",
          "",
          "[Added Lines]",
          "465:         mock_now.return_value = pendulum.datetime(2020, 1, 1, 0, 0, 5, tz=\"UTC\")",
          "471:         since_time = pendulum.datetime(2020, 1, 1, tz=\"UTC\")",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "488:         )",
          "489:         ret = self.pod_manager.fetch_container_logs(pod=mock_pod, container_name=\"base\", follow=follow)",
          "490:         assert len(container_running_mock.call_args_list) == is_running_calls",
          "492:         assert ret.running is exp_running",
          "494:     @pytest.mark.parametrize(",
          "",
          "[Removed Lines]",
          "491:         assert ret.last_log_time == DateTime(2021, 1, 1, tzinfo=Timezone(\"UTC\"))",
          "",
          "[Added Lines]",
          "492:         assert ret.last_log_time == pendulum.datetime(2021, 1, 1, tz=\"UTC\")",
          "",
          "---------------"
        ],
        "tests/providers/openlineage/plugins/test_utils.py||tests/providers/openlineage/plugins/test_utils.py": [
          "File: tests/providers/openlineage/plugins/test_utils.py -> tests/providers/openlineage/plugins/test_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: from json import JSONEncoder",
          "24: from typing import Any",
          "26: import pytest",
          "27: from attrs import define",
          "28: from openlineage.client.utils import RedactMixin",
          "30: from pkg_resources import parse_version",
          "32: from airflow.models import DAG as AIRFLOW_DAG, DagModel",
          "",
          "[Removed Lines]",
          "29: from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "26: import pendulum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "86:         state=State.NONE, run_id=run_id, data_interval=dag.get_next_data_interval(dag_model)",
          "87:     )",
          "88:     assert dagrun.data_interval_start is not None",
          "91:     assert dagrun.data_interval_start, dagrun.data_interval_end == (start_date_tz, end_date_tz)",
          "",
          "[Removed Lines]",
          "89:     start_date_tz = datetime.datetime(2022, 1, 1, tzinfo=Timezone(\"UTC\"))",
          "90:     end_date_tz = datetime.datetime(2022, 1, 1, hour=2, tzinfo=Timezone(\"UTC\"))",
          "",
          "[Added Lines]",
          "89:     start_date_tz = datetime.datetime(2022, 1, 1, tzinfo=pendulum.tz.timezone(\"UTC\"))",
          "90:     end_date_tz = datetime.datetime(2022, 1, 1, hour=2, tzinfo=pendulum.tz.timezone(\"UTC\"))",
          "",
          "---------------"
        ],
        "tests/serialization/test_serialized_objects.py||tests/serialization/test_serialized_objects.py": [
          "File: tests/serialization/test_serialized_objects.py -> tests/serialization/test_serialized_objects.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import json",
          "21: from datetime import datetime, timedelta",
          "23: import pytest",
          "24: from dateutil import relativedelta",
          "25: from kubernetes.client import models as k8s",
          "28: from airflow.datasets import Dataset",
          "29: from airflow.exceptions import SerializationError",
          "",
          "[Removed Lines]",
          "26: from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "23: import pendulum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "142:         (1, None, equals),",
          "143:         (datetime.utcnow(), DAT.DATETIME, equal_time),",
          "144:         (timedelta(minutes=2), DAT.TIMEDELTA, equals),",
          "146:         (relativedelta.relativedelta(hours=+1), DAT.RELATIVEDELTA, lambda a, b: a.hours == b.hours),",
          "147:         ({\"test\": \"dict\", \"test-1\": 1}, None, equals),",
          "148:         ([\"array_item\", 2], None, equals),",
          "",
          "[Removed Lines]",
          "145:         (Timezone(\"UTC\"), DAT.TIMEZONE, lambda a, b: a.name == b.name),",
          "",
          "[Added Lines]",
          "145:         (pendulum.tz.timezone(\"UTC\"), DAT.TIMEZONE, lambda a, b: a.name == b.name),",
          "",
          "---------------"
        ],
        "tests/timetables/test_events_timetable.py||tests/timetables/test_events_timetable.py": [
          "File: tests/timetables/test_events_timetable.py -> tests/timetables/test_events_timetable.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import pendulum",
          "21: import pytest",
          "24: from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction, Timetable",
          "25: from airflow.timetables.events import EventsTimetable",
          "29: EVENT_DATES = [",
          "36: ]",
          "38: EVENT_DATES_SORTED = [",
          "44: ]",
          "50: @pytest.fixture()",
          "",
          "[Removed Lines]",
          "23: from airflow.settings import TIMEZONE",
          "27: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=TIMEZONE)  # Precedes all events",
          "30:     pendulum.DateTime(2021, 9, 6, tzinfo=TIMEZONE),",
          "31:     pendulum.DateTime(2021, 9, 7, tzinfo=TIMEZONE),",
          "32:     pendulum.DateTime(2021, 9, 8, tzinfo=TIMEZONE),",
          "33:     pendulum.DateTime(2021, 9, 8, tzinfo=TIMEZONE),  # deliberate duplicate, should be ignored",
          "34:     pendulum.DateTime(2021, 10, 9, tzinfo=TIMEZONE),  # deliberately out of order",
          "35:     pendulum.DateTime(2021, 9, 10, tzinfo=TIMEZONE),",
          "39:     pendulum.DateTime(2021, 9, 6, tzinfo=TIMEZONE),",
          "40:     pendulum.DateTime(2021, 9, 7, tzinfo=TIMEZONE),",
          "41:     pendulum.DateTime(2021, 9, 8, tzinfo=TIMEZONE),",
          "42:     pendulum.DateTime(2021, 9, 10, tzinfo=TIMEZONE),",
          "43:     pendulum.DateTime(2021, 10, 9, tzinfo=TIMEZONE),",
          "46: NON_EVENT_DATE = pendulum.DateTime(2021, 10, 1, tzinfo=TIMEZONE)",
          "47: MOST_RECENT_EVENT = pendulum.DateTime(2021, 9, 10, tzinfo=TIMEZONE)",
          "",
          "[Added Lines]",
          "25: from airflow.utils.timezone import utc",
          "27: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=utc)  # Precedes all events",
          "30:     pendulum.DateTime(2021, 9, 6, tzinfo=utc),",
          "31:     pendulum.DateTime(2021, 9, 7, tzinfo=utc),",
          "32:     pendulum.DateTime(2021, 9, 8, tzinfo=utc),",
          "33:     pendulum.DateTime(2021, 9, 8, tzinfo=utc),  # deliberate duplicate, should be ignored",
          "34:     pendulum.DateTime(2021, 10, 9, tzinfo=utc),  # deliberately out of order",
          "35:     pendulum.DateTime(2021, 9, 10, tzinfo=utc),",
          "39:     pendulum.DateTime(2021, 9, 6, tzinfo=utc),",
          "40:     pendulum.DateTime(2021, 9, 7, tzinfo=utc),",
          "41:     pendulum.DateTime(2021, 9, 8, tzinfo=utc),",
          "42:     pendulum.DateTime(2021, 9, 10, tzinfo=utc),",
          "43:     pendulum.DateTime(2021, 10, 9, tzinfo=utc),",
          "46: NON_EVENT_DATE = pendulum.DateTime(2021, 10, 1, tzinfo=utc)",
          "47: MOST_RECENT_EVENT = pendulum.DateTime(2021, 9, 10, tzinfo=utc)",
          "",
          "---------------"
        ],
        "tests/timetables/test_interval_timetable.py||tests/timetables/test_interval_timetable.py": [
          "File: tests/timetables/test_interval_timetable.py -> tests/timetables/test_interval_timetable.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: import time_machine",
          "27: from airflow.exceptions import AirflowTimetableInvalid",
          "29: from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction, Timetable",
          "30: from airflow.timetables.interval import CronDataIntervalTimetable, DeltaDataIntervalTimetable",
          "34: PREV_DATA_INTERVAL_START = START_DATE",
          "35: PREV_DATA_INTERVAL_END = START_DATE + datetime.timedelta(days=1)",
          "36: PREV_DATA_INTERVAL = DataInterval(start=PREV_DATA_INTERVAL_START, end=PREV_DATA_INTERVAL_END)",
          "37: PREV_DATA_INTERVAL_EXACT = DataInterval.exact(PREV_DATA_INTERVAL_END)",
          "40: YESTERDAY = CURRENT_TIME - datetime.timedelta(days=1)",
          "41: OLD_INTERVAL = DataInterval(start=YESTERDAY, end=CURRENT_TIME)",
          "44: HOURLY_TIMEDELTA_TIMETABLE = DeltaDataIntervalTimetable(datetime.timedelta(hours=1))",
          "45: HOURLY_RELATIVEDELTA_TIMETABLE = DeltaDataIntervalTimetable(dateutil.relativedelta.relativedelta(hours=1))",
          "48: DELTA_FROM_MIDNIGHT = datetime.timedelta(minutes=30, hours=16)",
          "",
          "[Removed Lines]",
          "28: from airflow.settings import TIMEZONE",
          "32: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=TIMEZONE)",
          "39: CURRENT_TIME = pendulum.DateTime(2021, 9, 7, tzinfo=TIMEZONE)",
          "43: HOURLY_CRON_TIMETABLE = CronDataIntervalTimetable(\"@hourly\", TIMEZONE)",
          "47: CRON_TIMETABLE = CronDataIntervalTimetable(\"30 16 * * *\", TIMEZONE)",
          "",
          "[Added Lines]",
          "30: from airflow.utils.timezone import utc",
          "32: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=utc)",
          "39: CURRENT_TIME = pendulum.DateTime(2021, 9, 7, tzinfo=utc)",
          "43: HOURLY_CRON_TIMETABLE = CronDataIntervalTimetable(\"@hourly\", utc)",
          "47: CRON_TIMETABLE = CronDataIntervalTimetable(\"30 16 * * *\", utc)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "148:     \"timetable, error_message\",",
          "149:     [",
          "150:         pytest.param(",
          "152:             \"[0 0 1 13 0] is not acceptable, out of range\",",
          "153:             id=\"invalid-cron\",",
          "154:         ),",
          "",
          "[Removed Lines]",
          "151:             CronDataIntervalTimetable(\"0 0 1 13 0\", TIMEZONE),",
          "",
          "[Added Lines]",
          "151:             CronDataIntervalTimetable(\"0 0 1 13 0\", utc),",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "191:     [",
          "192:         # Arbitrary trigger time.",
          "193:         pytest.param(",
          "195:             DataInterval(",
          "198:             ),",
          "199:             id=\"adhoc\",",
          "200:         ),",
          "201:         # Trigger time falls exactly on interval boundary.",
          "202:         pytest.param(",
          "204:             DataInterval(",
          "207:             ),",
          "208:             id=\"exact\",",
          "209:         ),",
          "",
          "[Removed Lines]",
          "194:             pendulum.DateTime(2022, 8, 8, 1, tzinfo=TIMEZONE),",
          "196:                 pendulum.DateTime(2022, 8, 7, tzinfo=TIMEZONE),",
          "197:                 pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "203:             pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "205:                 pendulum.DateTime(2022, 8, 7, tzinfo=TIMEZONE),",
          "206:                 pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "",
          "[Added Lines]",
          "194:             pendulum.DateTime(2022, 8, 8, 1, tzinfo=utc),",
          "196:                 pendulum.DateTime(2022, 8, 7, tzinfo=utc),",
          "197:                 pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "203:             pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "205:                 pendulum.DateTime(2022, 8, 7, tzinfo=utc),",
          "206:                 pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "213:     trigger_at: pendulum.DateTime,",
          "214:     expected_interval: DataInterval,",
          "215: ) -> None:",
          "217:     assert timetable.infer_manual_data_interval(run_after=trigger_at) == expected_interval",
          "",
          "[Removed Lines]",
          "216:     timetable = CronDataIntervalTimetable(\"@daily\", TIMEZONE)",
          "",
          "[Added Lines]",
          "216:     timetable = CronDataIntervalTimetable(\"@daily\", utc)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "222:     [",
          "223:         pytest.param(",
          "224:             DataInterval(",
          "227:             ),",
          "228:             DagRunInfo.interval(",
          "231:             ),",
          "232:             id=\"exact\",",
          "233:         ),",
          "",
          "[Removed Lines]",
          "225:                 pendulum.DateTime(2022, 8, 7, tzinfo=TIMEZONE),",
          "226:                 pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "229:                 pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "230:                 pendulum.DateTime(2022, 8, 9, tzinfo=TIMEZONE),",
          "",
          "[Added Lines]",
          "225:                 pendulum.DateTime(2022, 8, 7, tzinfo=utc),",
          "226:                 pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "229:                 pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "230:                 pendulum.DateTime(2022, 8, 9, tzinfo=utc),",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "235:             # Previous data interval does not align with the current timetable.",
          "236:             # This is possible if the user edits a DAG with existing runs.",
          "237:             DataInterval(",
          "240:             ),",
          "241:             DagRunInfo.interval(",
          "244:             ),",
          "245:             id=\"changed\",",
          "246:         ),",
          "247:     ],",
          "248: )",
          "249: def test_cron_next_dagrun_info_alignment(last_data_interval: DataInterval, expected_info: DagRunInfo):",
          "251:     info = timetable.next_dagrun_info(",
          "252:         last_automated_data_interval=last_data_interval,",
          "253:         restriction=TimeRestriction(None, None, True),",
          "",
          "[Removed Lines]",
          "238:                 pendulum.DateTime(2022, 8, 7, 1, tzinfo=TIMEZONE),",
          "239:                 pendulum.DateTime(2022, 8, 8, 1, tzinfo=TIMEZONE),",
          "242:                 pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "243:                 pendulum.DateTime(2022, 8, 9, tzinfo=TIMEZONE),",
          "250:     timetable = CronDataIntervalTimetable(\"@daily\", TIMEZONE)",
          "",
          "[Added Lines]",
          "238:                 pendulum.DateTime(2022, 8, 7, 1, tzinfo=utc),",
          "239:                 pendulum.DateTime(2022, 8, 8, 1, tzinfo=utc),",
          "242:                 pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "243:                 pendulum.DateTime(2022, 8, 9, tzinfo=utc),",
          "250:     timetable = CronDataIntervalTimetable(\"@daily\", utc)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "269:     def test_entering_exact(self) -> None:",
          "270:         timetable = CronDataIntervalTimetable(\"0 3 * * *\", timezone=\"Europe/Zurich\")",
          "271:         restriction = TimeRestriction(",
          "273:             latest=None,",
          "274:             catchup=True,",
          "275:         )",
          "",
          "[Removed Lines]",
          "272:             earliest=pendulum.datetime(2023, 3, 24, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "272:             earliest=pendulum.datetime(2023, 3, 24, tz=utc),",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "277:         # Last run before DST. Interval starts and ends on 2am UTC (local time is +1).",
          "278:         next_info = timetable.next_dagrun_info(last_automated_data_interval=None, restriction=restriction)",
          "279:         assert next_info and next_info.data_interval == DataInterval(",
          "282:         )",
          "284:         # Crossing the DST switch. Interval starts on 2am UTC (local time +1)",
          "",
          "[Removed Lines]",
          "280:             pendulum.datetime(2023, 3, 24, 2, tz=TIMEZONE),",
          "281:             pendulum.datetime(2023, 3, 25, 2, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "280:             pendulum.datetime(2023, 3, 24, 2, tz=utc),",
          "281:             pendulum.datetime(2023, 3, 25, 2, tz=utc),",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "288:             restriction=restriction,",
          "289:         )",
          "290:         assert next_info and next_info.data_interval == DataInterval(",
          "293:         )",
          "295:         # In DST. Interval starts and ends on 1am UTC (local time is +2).",
          "",
          "[Removed Lines]",
          "291:             pendulum.datetime(2023, 3, 25, 2, tz=TIMEZONE),",
          "292:             pendulum.datetime(2023, 3, 26, 1, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "291:             pendulum.datetime(2023, 3, 25, 2, tz=utc),",
          "292:             pendulum.datetime(2023, 3, 26, 1, tz=utc),",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "298:             restriction=restriction,",
          "299:         )",
          "300:         assert next_info and next_info.data_interval == DataInterval(",
          "303:         )",
          "305:     def test_entering_skip(self) -> None:",
          "306:         timetable = CronDataIntervalTimetable(\"0 2 * * *\", timezone=\"Europe/Zurich\")",
          "307:         restriction = TimeRestriction(",
          "309:             latest=None,",
          "310:             catchup=True,",
          "311:         )",
          "",
          "[Removed Lines]",
          "301:             pendulum.datetime(2023, 3, 26, 1, tz=TIMEZONE),",
          "302:             pendulum.datetime(2023, 3, 27, 1, tz=TIMEZONE),",
          "308:             earliest=pendulum.datetime(2023, 3, 24, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "301:             pendulum.datetime(2023, 3, 26, 1, tz=utc),",
          "302:             pendulum.datetime(2023, 3, 27, 1, tz=utc),",
          "308:             earliest=pendulum.datetime(2023, 3, 24, tz=utc),",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "313:         # Last run before DST. Interval starts and ends on 1am UTC (local time is +1).",
          "314:         next_info = timetable.next_dagrun_info(last_automated_data_interval=None, restriction=restriction)",
          "315:         assert next_info and next_info.data_interval == DataInterval(",
          "318:         )",
          "320:         # Crossing the DST switch. Interval starts on 1am UTC (local time +1)",
          "",
          "[Removed Lines]",
          "316:             pendulum.datetime(2023, 3, 24, 1, tz=TIMEZONE),",
          "317:             pendulum.datetime(2023, 3, 25, 1, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "316:             pendulum.datetime(2023, 3, 24, 1, tz=utc),",
          "317:             pendulum.datetime(2023, 3, 25, 1, tz=utc),",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "325:             restriction=restriction,",
          "326:         )",
          "327:         assert next_info and next_info.data_interval == DataInterval(",
          "330:         )",
          "332:         # In DST. Interval starts on 1am UTC (local time is +2 but 2am local",
          "",
          "[Removed Lines]",
          "328:             pendulum.datetime(2023, 3, 25, 1, tz=TIMEZONE),",
          "329:             pendulum.datetime(2023, 3, 26, 1, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "328:             pendulum.datetime(2023, 3, 25, 1, tz=utc),",
          "329:             pendulum.datetime(2023, 3, 26, 1, tz=utc),",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "336:             restriction=restriction,",
          "337:         )",
          "338:         assert next_info and next_info.data_interval == DataInterval(",
          "341:         )",
          "343:     def test_exiting_exact(self) -> None:",
          "344:         timetable = CronDataIntervalTimetable(\"0 3 * * *\", timezone=\"Europe/Zurich\")",
          "345:         restriction = TimeRestriction(",
          "347:             latest=None,",
          "348:             catchup=True,",
          "349:         )",
          "",
          "[Removed Lines]",
          "339:             pendulum.datetime(2023, 3, 26, 1, tz=TIMEZONE),",
          "340:             pendulum.datetime(2023, 3, 27, 0, tz=TIMEZONE),",
          "346:             earliest=pendulum.datetime(2023, 10, 27, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "339:             pendulum.datetime(2023, 3, 26, 1, tz=utc),",
          "340:             pendulum.datetime(2023, 3, 27, 0, tz=utc),",
          "346:             earliest=pendulum.datetime(2023, 10, 27, tz=utc),",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "351:         # Last run in DST. Interval starts and ends on 1am UTC (local time is +2).",
          "352:         next_info = timetable.next_dagrun_info(last_automated_data_interval=None, restriction=restriction)",
          "353:         assert next_info and next_info.data_interval == DataInterval(",
          "356:         )",
          "358:         # Crossing the DST switch. Interval starts on 1am UTC (local time +2)",
          "",
          "[Removed Lines]",
          "354:             pendulum.datetime(2023, 10, 27, 1, tz=TIMEZONE),",
          "355:             pendulum.datetime(2023, 10, 28, 1, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "354:             pendulum.datetime(2023, 10, 27, 1, tz=utc),",
          "355:             pendulum.datetime(2023, 10, 28, 1, tz=utc),",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "362:             restriction=restriction,",
          "363:         )",
          "364:         assert next_info and next_info.data_interval == DataInterval(",
          "367:         )",
          "369:         # Out of DST. Interval starts and ends on 2am UTC (local time is +1).",
          "",
          "[Removed Lines]",
          "365:             pendulum.datetime(2023, 10, 28, 1, tz=TIMEZONE),",
          "366:             pendulum.datetime(2023, 10, 29, 2, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "365:             pendulum.datetime(2023, 10, 28, 1, tz=utc),",
          "366:             pendulum.datetime(2023, 10, 29, 2, tz=utc),",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "372:             restriction=restriction,",
          "373:         )",
          "374:         assert next_info and next_info.data_interval == DataInterval(",
          "377:         )",
          "379:     def test_exiting_fold(self) -> None:",
          "380:         timetable = CronDataIntervalTimetable(\"0 2 * * *\", timezone=\"Europe/Zurich\")",
          "381:         restriction = TimeRestriction(",
          "383:             latest=None,",
          "384:             catchup=True,",
          "385:         )",
          "",
          "[Removed Lines]",
          "375:             pendulum.datetime(2023, 10, 29, 2, tz=TIMEZONE),",
          "376:             pendulum.datetime(2023, 10, 30, 2, tz=TIMEZONE),",
          "382:             earliest=pendulum.datetime(2023, 10, 27, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "375:             pendulum.datetime(2023, 10, 29, 2, tz=utc),",
          "376:             pendulum.datetime(2023, 10, 30, 2, tz=utc),",
          "382:             earliest=pendulum.datetime(2023, 10, 27, tz=utc),",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "388:         # time is +2).",
          "389:         next_info = timetable.next_dagrun_info(last_automated_data_interval=None, restriction=restriction)",
          "390:         assert next_info and next_info.data_interval == DataInterval(",
          "393:         )",
          "395:         # Account for folding. Interval starts on 0am UTC (local time +2) and",
          "",
          "[Removed Lines]",
          "391:             pendulum.datetime(2023, 10, 27, 0, tz=TIMEZONE),",
          "392:             pendulum.datetime(2023, 10, 28, 0, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "391:             pendulum.datetime(2023, 10, 27, 0, tz=utc),",
          "392:             pendulum.datetime(2023, 10, 28, 0, tz=utc),",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "402:             restriction=restriction,",
          "403:         )",
          "404:         assert next_info and next_info.data_interval == DataInterval(",
          "407:         )",
          "409:         # Stepping out of DST. Interval starts from the folded 2am local time",
          "",
          "[Removed Lines]",
          "405:             pendulum.datetime(2023, 10, 28, 0, tz=TIMEZONE),",
          "406:             pendulum.datetime(2023, 10, 29, 1, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "405:             pendulum.datetime(2023, 10, 28, 0, tz=utc),",
          "406:             pendulum.datetime(2023, 10, 29, 1, tz=utc),",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "414:             restriction=restriction,",
          "415:         )",
          "416:         assert next_info and next_info.data_interval == DataInterval(",
          "419:         )",
          "",
          "[Removed Lines]",
          "417:             pendulum.datetime(2023, 10, 29, 1, tz=TIMEZONE),",
          "418:             pendulum.datetime(2023, 10, 30, 1, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "417:             pendulum.datetime(2023, 10, 29, 1, tz=utc),",
          "418:             pendulum.datetime(2023, 10, 30, 1, tz=utc),",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "429:     def test_7_to_8_entering(self):",
          "430:         timetable = CronDataIntervalTimetable(\"0 7-8 * * *\", timezone=\"America/Los_Angeles\")",
          "431:         restriction = TimeRestriction(",
          "433:             latest=None,",
          "434:             catchup=True,",
          "435:         )",
          "",
          "[Removed Lines]",
          "432:             earliest=pendulum.datetime(2020, 3, 7, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "432:             earliest=pendulum.datetime(2020, 3, 7, tz=utc),",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "440:             restriction=restriction,",
          "441:         )",
          "442:         assert next_info and next_info.data_interval == DataInterval(",
          "445:         )",
          "447:         # This interval ends an hour early since it includes the DST switch!",
          "",
          "[Removed Lines]",
          "443:             pendulum.datetime(2020, 3, 7, 7 + 8, tz=TIMEZONE),",
          "444:             pendulum.datetime(2020, 3, 7, 8 + 8, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "443:             pendulum.datetime(2020, 3, 7, 7 + 8, tz=utc),",
          "444:             pendulum.datetime(2020, 3, 7, 8 + 8, tz=utc),",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "450:             restriction=restriction,",
          "451:         )",
          "452:         assert next_info and next_info.data_interval == DataInterval(",
          "455:         )",
          "457:         # We're fully into DST so the interval is as expected.",
          "",
          "[Removed Lines]",
          "453:             pendulum.datetime(2020, 3, 7, 8 + 8, tz=TIMEZONE),",
          "454:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "453:             pendulum.datetime(2020, 3, 7, 8 + 8, tz=utc),",
          "454:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=utc),",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "460:             restriction=restriction,",
          "461:         )",
          "462:         assert next_info and next_info.data_interval == DataInterval(",
          "465:         )",
          "467:     def test_7_and_9_entering(self):",
          "468:         timetable = CronDataIntervalTimetable(\"0 7,9 * * *\", timezone=\"America/Los_Angeles\")",
          "469:         restriction = TimeRestriction(",
          "471:             latest=None,",
          "472:             catchup=True,",
          "473:         )",
          "",
          "[Removed Lines]",
          "463:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=TIMEZONE),",
          "464:             pendulum.datetime(2020, 3, 8, 8 + 7, tz=TIMEZONE),",
          "470:             earliest=pendulum.datetime(2020, 3, 7, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "463:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=utc),",
          "464:             pendulum.datetime(2020, 3, 8, 8 + 7, tz=utc),",
          "470:             earliest=pendulum.datetime(2020, 3, 7, tz=utc),",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "478:             restriction=restriction,",
          "479:         )",
          "480:         assert next_info and next_info.data_interval == DataInterval(",
          "483:         )",
          "485:         # This interval ends an hour early since it includes the DST switch!",
          "",
          "[Removed Lines]",
          "481:             pendulum.datetime(2020, 3, 7, 7 + 8, tz=TIMEZONE),",
          "482:             pendulum.datetime(2020, 3, 7, 9 + 8, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "481:             pendulum.datetime(2020, 3, 7, 7 + 8, tz=utc),",
          "482:             pendulum.datetime(2020, 3, 7, 9 + 8, tz=utc),",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "488:             restriction=restriction,",
          "489:         )",
          "490:         assert next_info and next_info.data_interval == DataInterval(",
          "493:         )",
          "495:         # We're fully into DST so the interval is as expected.",
          "",
          "[Removed Lines]",
          "491:             pendulum.datetime(2020, 3, 7, 9 + 8, tz=TIMEZONE),",
          "492:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=TIMEZONE),",
          "",
          "[Added Lines]",
          "491:             pendulum.datetime(2020, 3, 7, 9 + 8, tz=utc),",
          "492:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=utc),",
          "",
          "---------------",
          "--- Hunk 26 ---",
          "[Context before]",
          "498:             restriction=restriction,",
          "499:         )",
          "500:         assert next_info and next_info.data_interval == DataInterval(",
          "503:         )",
          "506: def test_fold_scheduling():",
          "507:     timetable = CronDataIntervalTimetable(\"*/30 * * * *\", timezone=\"Europe/Zurich\")",
          "508:     restriction = TimeRestriction(",
          "510:         latest=None,",
          "511:         catchup=True,",
          "512:     )",
          "",
          "[Removed Lines]",
          "501:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=TIMEZONE),",
          "502:             pendulum.datetime(2020, 3, 8, 9 + 7, tz=TIMEZONE),",
          "509:         earliest=pendulum.datetime(2023, 10, 28, 23, 30, tz=TIMEZONE),  # Locally 1:30 (DST).",
          "",
          "[Added Lines]",
          "501:             pendulum.datetime(2020, 3, 8, 7 + 7, tz=utc),",
          "502:             pendulum.datetime(2020, 3, 8, 9 + 7, tz=utc),",
          "509:         earliest=pendulum.datetime(2023, 10, 28, 23, 30, tz=utc),  # Locally 1:30 (DST).",
          "",
          "---------------",
          "--- Hunk 27 ---",
          "[Context before]",
          "517:         restriction=restriction,",
          "518:     )",
          "519:     assert next_info and next_info.data_interval == DataInterval(",
          "522:     )",
          "523:     next_info = timetable.next_dagrun_info(",
          "524:         last_automated_data_interval=next_info.data_interval,",
          "525:         restriction=restriction,",
          "526:     )",
          "527:     assert next_info and next_info.data_interval == DataInterval(",
          "530:     )",
          "532:     # Crossing into fold.",
          "",
          "[Removed Lines]",
          "520:         pendulum.datetime(2023, 10, 28, 23, 30, tz=TIMEZONE),",
          "521:         pendulum.datetime(2023, 10, 29, 0, 0, tz=TIMEZONE),  # Locally 2am (DST).",
          "528:         pendulum.datetime(2023, 10, 29, 0, 0, tz=TIMEZONE),",
          "529:         pendulum.datetime(2023, 10, 29, 0, 30, tz=TIMEZONE),  # Locally 2:30 (DST).",
          "",
          "[Added Lines]",
          "520:         pendulum.datetime(2023, 10, 28, 23, 30, tz=utc),",
          "521:         pendulum.datetime(2023, 10, 29, 0, 0, tz=utc),  # Locally 2am (DST).",
          "528:         pendulum.datetime(2023, 10, 29, 0, 0, tz=utc),",
          "529:         pendulum.datetime(2023, 10, 29, 0, 30, tz=utc),  # Locally 2:30 (DST).",
          "",
          "---------------",
          "--- Hunk 28 ---",
          "[Context before]",
          "535:         restriction=restriction,",
          "536:     )",
          "537:     assert next_info and next_info.data_interval == DataInterval(",
          "540:     )",
          "542:     # In the \"fold zone\".",
          "",
          "[Removed Lines]",
          "538:         pendulum.datetime(2023, 10, 29, 0, 30, tz=TIMEZONE),",
          "539:         pendulum.datetime(2023, 10, 29, 1, 0, tz=TIMEZONE),  # Locally 2am (fold, not DST).",
          "",
          "[Added Lines]",
          "538:         pendulum.datetime(2023, 10, 29, 0, 30, tz=utc),",
          "539:         pendulum.datetime(2023, 10, 29, 1, 0, tz=utc),  # Locally 2am (fold, not DST).",
          "",
          "---------------",
          "--- Hunk 29 ---",
          "[Context before]",
          "545:         restriction=restriction,",
          "546:     )",
          "547:     assert next_info and next_info.data_interval == DataInterval(",
          "550:     )",
          "552:     # Stepping out of fold.",
          "",
          "[Removed Lines]",
          "548:         pendulum.datetime(2023, 10, 29, 1, 0, tz=TIMEZONE),",
          "549:         pendulum.datetime(2023, 10, 29, 1, 30, tz=TIMEZONE),  # Locally 2am (fold, not DST).",
          "",
          "[Added Lines]",
          "548:         pendulum.datetime(2023, 10, 29, 1, 0, tz=utc),",
          "549:         pendulum.datetime(2023, 10, 29, 1, 30, tz=utc),  # Locally 2am (fold, not DST).",
          "",
          "---------------",
          "--- Hunk 30 ---",
          "[Context before]",
          "555:         restriction=restriction,",
          "556:     )",
          "557:     assert next_info and next_info.data_interval == DataInterval(",
          "560:     )",
          "",
          "[Removed Lines]",
          "558:         pendulum.datetime(2023, 10, 29, 1, 30, tz=TIMEZONE),",
          "559:         pendulum.datetime(2023, 10, 29, 2, 0, tz=TIMEZONE),  # Locally 3am (not DST).",
          "",
          "[Added Lines]",
          "558:         pendulum.datetime(2023, 10, 29, 1, 30, tz=utc),",
          "559:         pendulum.datetime(2023, 10, 29, 2, 0, tz=utc),  # Locally 3am (not DST).",
          "",
          "---------------"
        ],
        "tests/timetables/test_trigger_timetable.py||tests/timetables/test_trigger_timetable.py": [
          "File: tests/timetables/test_trigger_timetable.py -> tests/timetables/test_trigger_timetable.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import dateutil.relativedelta",
          "23: import pendulum",
          "25: import pytest",
          "26: import time_machine",
          "28: from airflow.exceptions import AirflowTimetableInvalid",
          "29: from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction",
          "30: from airflow.timetables.trigger import CronTriggerTimetable",
          "35: PREV_DATA_INTERVAL_END = START_DATE + datetime.timedelta(days=1)",
          "36: PREV_DATA_INTERVAL_EXACT = DataInterval.exact(PREV_DATA_INTERVAL_END)",
          "39: YESTERDAY = CURRENT_TIME - datetime.timedelta(days=1)",
          "43: DELTA_FROM_MIDNIGHT = datetime.timedelta(minutes=30, hours=16)",
          "",
          "[Removed Lines]",
          "24: import pendulum.tz",
          "32: TIMEZONE = pendulum.tz.timezone(\"UTC\")",
          "33: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=TIMEZONE)",
          "38: CURRENT_TIME = pendulum.DateTime(2021, 9, 7, tzinfo=TIMEZONE)",
          "41: HOURLY_CRON_TRIGGER_TIMETABLE = CronTriggerTimetable(\"@hourly\", timezone=TIMEZONE)",
          "",
          "[Added Lines]",
          "30: from airflow.utils.timezone import utc",
          "32: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=utc)",
          "37: CURRENT_TIME = pendulum.DateTime(2021, 9, 7, tzinfo=utc)",
          "40: HOURLY_CRON_TRIGGER_TIMETABLE = CronTriggerTimetable(\"@hourly\", timezone=utc)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "69:     next_start_time: pendulum.DateTime,",
          "70: ) -> None:",
          "71:     \"\"\"If ``catchup=False`` and start_date is a day before\"\"\"",
          "73:     next_info = timetable.next_dagrun_info(",
          "74:         last_automated_data_interval=last_automated_data_interval,",
          "75:         restriction=TimeRestriction(earliest=YESTERDAY, latest=None, catchup=False),",
          "",
          "[Removed Lines]",
          "72:     timetable = CronTriggerTimetable(\"30 16 * * *\", timezone=TIMEZONE)",
          "",
          "[Added Lines]",
          "71:     timetable = CronTriggerTimetable(\"30 16 * * *\", timezone=utc)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "81:     \"current_time, earliest, expected\",",
          "82:     [",
          "83:         pytest.param(",
          "85:             START_DATE,",
          "87:             id=\"current_time_on_boundary\",",
          "88:         ),",
          "89:         pytest.param(",
          "91:             START_DATE,",
          "93:             id=\"current_time_not_on_boundary\",",
          "94:         ),",
          "95:         pytest.param(",
          "97:             START_DATE,",
          "99:             id=\"current_time_miss_one_interval_on_boundary\",",
          "100:         ),",
          "101:         pytest.param(",
          "103:             START_DATE,",
          "105:             id=\"current_time_miss_one_interval_not_on_boundary\",",
          "106:         ),",
          "107:         pytest.param(",
          "111:             id=\"future_start_date\",",
          "112:         ),",
          "113:     ],",
          "",
          "[Removed Lines]",
          "84:             pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=TIMEZONE),",
          "86:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=TIMEZONE)),",
          "90:             pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=TIMEZONE),",
          "92:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=TIMEZONE)),",
          "96:             pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=TIMEZONE),",
          "98:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=TIMEZONE)),",
          "102:             pendulum.DateTime(2022, 7, 27, 1, 30, 0, tzinfo=TIMEZONE),",
          "104:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=TIMEZONE)),",
          "108:             pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=TIMEZONE),",
          "109:             pendulum.DateTime(2199, 12, 31, 22, 30, 0, tzinfo=TIMEZONE),",
          "110:             DagRunInfo.exact(pendulum.DateTime(2199, 12, 31, 23, 0, 0, tzinfo=TIMEZONE)),",
          "",
          "[Added Lines]",
          "83:             pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=utc),",
          "85:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=utc)),",
          "89:             pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=utc),",
          "91:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=utc)),",
          "95:             pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=utc),",
          "97:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=utc)),",
          "101:             pendulum.DateTime(2022, 7, 27, 1, 30, 0, tzinfo=utc),",
          "103:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=utc)),",
          "107:             pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=utc),",
          "108:             pendulum.DateTime(2199, 12, 31, 22, 30, 0, tzinfo=utc),",
          "109:             DagRunInfo.exact(pendulum.DateTime(2199, 12, 31, 23, 0, 0, tzinfo=utc)),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "129:     \"last_automated_data_interval, earliest, expected\",",
          "130:     [",
          "131:         pytest.param(",
          "133:             START_DATE,",
          "135:             id=\"last_automated_on_boundary\",",
          "136:         ),",
          "137:         pytest.param(",
          "139:             START_DATE,",
          "141:             id=\"last_automated_not_on_boundary\",",
          "142:         ),",
          "143:         pytest.param(",
          "144:             None,",
          "147:             id=\"no_last_automated_with_earliest_on_boundary\",",
          "148:         ),",
          "149:         pytest.param(",
          "150:             None,",
          "153:             id=\"no_last_automated_with_earliest_not_on_boundary\",",
          "154:         ),",
          "155:         pytest.param(",
          "",
          "[Removed Lines]",
          "132:             DataInterval.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=TIMEZONE)),",
          "134:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=TIMEZONE)),",
          "138:             DataInterval.exact(pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=TIMEZONE)),",
          "140:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=TIMEZONE)),",
          "145:             pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=TIMEZONE),",
          "146:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=TIMEZONE)),",
          "151:             pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=TIMEZONE),",
          "152:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=TIMEZONE)),",
          "",
          "[Added Lines]",
          "131:             DataInterval.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=utc)),",
          "133:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=utc)),",
          "137:             DataInterval.exact(pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=utc)),",
          "139:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=utc)),",
          "144:             pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=utc),",
          "145:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 0, 0, 0, tzinfo=utc)),",
          "150:             pendulum.DateTime(2022, 7, 27, 0, 30, 0, tzinfo=utc),",
          "151:             DagRunInfo.exact(pendulum.DateTime(2022, 7, 27, 1, 0, 0, tzinfo=utc)),",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "176:     # Runs every Monday on 16:30, covering the day before the run.",
          "177:     timetable = CronTriggerTimetable(",
          "178:         \"30 16 * * MON\",",
          "180:         interval=datetime.timedelta(hours=16, minutes=30),",
          "181:     )",
          "183:     next_info = timetable.next_dagrun_info(",
          "184:         last_automated_data_interval=DataInterval(",
          "187:         ),",
          "188:         restriction=TimeRestriction(earliest=START_DATE, latest=None, catchup=True),",
          "189:     )",
          "190:     assert next_info == DagRunInfo.interval(",
          "193:     )",
          "",
          "[Removed Lines]",
          "179:         timezone=TIMEZONE,",
          "185:             pendulum.DateTime(2022, 8, 1, tzinfo=TIMEZONE),",
          "186:             pendulum.DateTime(2022, 8, 1, 16, 30, tzinfo=TIMEZONE),",
          "191:         pendulum.DateTime(2022, 8, 8, tzinfo=TIMEZONE),",
          "192:         pendulum.DateTime(2022, 8, 8, 16, 30, tzinfo=TIMEZONE),",
          "",
          "[Added Lines]",
          "178:         timezone=utc,",
          "184:             pendulum.DateTime(2022, 8, 1, tzinfo=utc),",
          "185:             pendulum.DateTime(2022, 8, 1, 16, 30, tzinfo=utc),",
          "190:         pendulum.DateTime(2022, 8, 8, tzinfo=utc),",
          "191:         pendulum.DateTime(2022, 8, 8, 16, 30, tzinfo=utc),",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "200: def test_validate_failure() -> None:",
          "203:     with pytest.raises(AirflowTimetableInvalid) as ctx:",
          "204:         timetable.validate()",
          "",
          "[Removed Lines]",
          "201:     timetable = CronTriggerTimetable(\"0 0 1 13 0\", timezone=TIMEZONE)",
          "",
          "[Added Lines]",
          "200:     timetable = CronTriggerTimetable(\"0 0 1 13 0\", timezone=utc)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "210:     [",
          "211:         (HOURLY_CRON_TRIGGER_TIMETABLE, {\"expression\": \"0 * * * *\", \"timezone\": \"UTC\", \"interval\": 0}),",
          "212:         (",
          "214:             {\"expression\": \"0 0 1 12 *\", \"timezone\": \"UTC\", \"interval\": 7200.0},",
          "215:         ),",
          "216:         (",
          "217:             CronTriggerTimetable(",
          "218:                 \"0 0 1 12 0\",",
          "220:                 interval=dateutil.relativedelta.relativedelta(weekday=dateutil.relativedelta.MO),",
          "221:             ),",
          "222:             {\"expression\": \"0 0 1 12 0\", \"timezone\": \"Asia/Taipei\", \"interval\": {\"weekday\": [0]}},",
          "",
          "[Removed Lines]",
          "213:             CronTriggerTimetable(\"0 0 1 12 *\", timezone=TIMEZONE, interval=datetime.timedelta(hours=2)),",
          "219:                 timezone=pendulum.tz.timezone(\"Asia/Taipei\"),",
          "",
          "[Added Lines]",
          "212:             CronTriggerTimetable(\"0 0 1 12 *\", timezone=utc, interval=datetime.timedelta(hours=2)),",
          "218:                 timezone=\"Asia/Taipei\",",
          "",
          "---------------"
        ],
        "tests/timetables/test_workday_timetable.py||tests/timetables/test_workday_timetable.py": [
          "File: tests/timetables/test_workday_timetable.py -> tests/timetables/test_workday_timetable.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import pytest",
          "25: from airflow.example_dags.plugins.workday import AfterWorkdayTimetable",
          "27: from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction, Timetable",
          "31: WEEK_1_WEEKDAYS = [",
          "37: ]",
          "45: @pytest.fixture()",
          "",
          "[Removed Lines]",
          "26: from airflow.settings import TIMEZONE",
          "29: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=TIMEZONE)  # This is a Saturday.",
          "32:     pendulum.DateTime(2021, 9, 6, tzinfo=TIMEZONE),  # This is a US holiday",
          "33:     pendulum.DateTime(2021, 9, 7, tzinfo=TIMEZONE),",
          "34:     pendulum.DateTime(2021, 9, 8, tzinfo=TIMEZONE),",
          "35:     pendulum.DateTime(2021, 9, 9, tzinfo=TIMEZONE),",
          "36:     pendulum.DateTime(2021, 9, 10, tzinfo=TIMEZONE),",
          "39: WEEK_1_SATURDAY = pendulum.DateTime(2021, 9, 11, tzinfo=TIMEZONE)",
          "41: WEEK_2_MONDAY = pendulum.DateTime(2021, 9, 13, tzinfo=TIMEZONE)",
          "42: WEEK_2_TUESDAY = pendulum.DateTime(2021, 9, 14, tzinfo=TIMEZONE)",
          "",
          "[Added Lines]",
          "27: from airflow.utils.timezone import utc",
          "29: START_DATE = pendulum.DateTime(2021, 9, 4, tzinfo=utc)  # This is a Saturday.",
          "32:     pendulum.DateTime(2021, 9, 6, tzinfo=utc),  # This is a US holiday",
          "33:     pendulum.DateTime(2021, 9, 7, tzinfo=utc),",
          "34:     pendulum.DateTime(2021, 9, 8, tzinfo=utc),",
          "35:     pendulum.DateTime(2021, 9, 9, tzinfo=utc),",
          "36:     pendulum.DateTime(2021, 9, 10, tzinfo=utc),",
          "39: WEEK_1_SATURDAY = pendulum.DateTime(2021, 9, 11, tzinfo=utc)",
          "41: WEEK_2_MONDAY = pendulum.DateTime(2021, 9, 13, tzinfo=utc)",
          "42: WEEK_2_TUESDAY = pendulum.DateTime(2021, 9, 14, tzinfo=utc)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e2357c26c11160c52689ef75b10bd0932cea7d27",
      "candidate_info": {
        "commit_hash": "e2357c26c11160c52689ef75b10bd0932cea7d27",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e2357c26c11160c52689ef75b10bd0932cea7d27",
        "files": [
          "docs/apache-airflow/administration-and-deployment/logging-monitoring/metrics.rst"
        ],
        "message": "Update metrics.rst for param dagrun.schedule_delay (#36404)\n\nThe timer in statsd client if passed the timedelta, converts into milliseconds rather than seconds.\n\n(cherry picked from commit 6199f1ffc807c55cbcc36f834e06b9791ce222ca)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "8bb9e79b4d9157b87d467b000b89f0bf69c1d9db",
      "candidate_info": {
        "commit_hash": "8bb9e79b4d9157b87d467b000b89f0bf69c1d9db",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8bb9e79b4d9157b87d467b000b89f0bf69c1d9db",
        "files": [
          "setup.py"
        ],
        "message": "Get rid of pyarrow-hotfix for CVE-2023-47248 (#36697)\n\nThe #35650 introduced a hotfix for Pyarrow CVE-2023-47248. So far\nwe have been blocked from removing it by Apache Beam that limited\nAirflow from bumping pyarrow to a version that was not vulnerable.\n\nThis is now possible since Apache Beam relesed 2.53.0 version on\n4th of January 2023 that allows to use non-vulnerable pyarrow.\n\nWe are now bumping both Pyarrow and Beam minimum versions to\nreflect that and remove pyarrow hotfix.\n\n(cherry picked from commit d105c7115f56f88d48a2888484a0ed7d1c01576f)",
        "before_after_code_files": [
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "903:                 self.package_data[\"airflow\"].append(provider_relative_path)",
          "904:             # Add python_kubernetes_script.jinja2 to package data",
          "905:             self.package_data[\"airflow\"].append(\"providers/cncf/kubernetes/python_kubernetes_script.jinja2\")",
          "906:         else:",
          "907:             self.install_requires.extend(",
          "908:                 [",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "906:             # Add default email template to package data",
          "907:             self.package_data[\"airflow\"].append(\"providers/smtp/notifications/templates/email.html\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "185c9ff1b8db124e0b644b85da6499519a3a06d5",
      "candidate_info": {
        "commit_hash": "185c9ff1b8db124e0b644b85da6499519a3a06d5",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/185c9ff1b8db124e0b644b85da6499519a3a06d5",
        "files": [
          "airflow/www/static/css/main.css"
        ],
        "message": "Increase width of execution_date input in trigger.html (#36278)\n\n(cherry picked from commit aed3c922402121c64264654f8dd77dbfc0168cbb)",
        "before_after_code_files": [
          "airflow/www/static/css/main.css||airflow/www/static/css/main.css"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/static/css/main.css||airflow/www/static/css/main.css": [
          "File: airflow/www/static/css/main.css -> airflow/www/static/css/main.css",
          "--- Hunk 1 ---",
          "[Context before]",
          "146: }",
          "148: input#execution_date {",
          "149:   margin-bottom: 0;",
          "150: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "149:   width: 220px;",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "812794a15a902790ca8700a0ad2027f51caa748f",
      "candidate_info": {
        "commit_hash": "812794a15a902790ca8700a0ad2027f51caa748f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/812794a15a902790ca8700a0ad2027f51caa748f",
        "files": [
          ".github/workflows/ci.yml",
          "airflow/providers/google/marketing_platform/hooks/analytics.py",
          "airflow/providers/google/marketing_platform/hooks/analytics_admin.py",
          "airflow/providers/google/marketing_platform/links/__init__.py",
          "airflow/providers/google/marketing_platform/links/analytics_admin.py",
          "airflow/providers/google/marketing_platform/operators/analytics.py",
          "airflow/providers/google/marketing_platform/operators/analytics_admin.py",
          "airflow/providers/google/provider.yaml",
          "docs/apache-airflow-providers-google/operators/marketing_platform/analytics.rst",
          "docs/apache-airflow-providers-google/operators/marketing_platform/analytics_admin.rst",
          "generated/provider_dependencies.json",
          "tests/providers/google/marketing_platform/hooks/test_analytics_admin.py",
          "tests/providers/google/marketing_platform/links/__init__.py",
          "tests/providers/google/marketing_platform/links/test_analytics_admin.py",
          "tests/providers/google/marketing_platform/operators/test_analytics_admin.py",
          "tests/system/providers/google/marketing_platform/example_analytics_admin.py"
        ],
        "message": "Implement Google Analytics Admin (GA4) operators (#36276)\n\n(cherry picked from commit f28643b7bdc90a61ec5bd12f8505772cd8c3bf7f)",
        "before_after_code_files": [
          "airflow/providers/google/marketing_platform/hooks/analytics.py||airflow/providers/google/marketing_platform/hooks/analytics.py",
          "airflow/providers/google/marketing_platform/hooks/analytics_admin.py||airflow/providers/google/marketing_platform/hooks/analytics_admin.py",
          "airflow/providers/google/marketing_platform/links/__init__.py||airflow/providers/google/marketing_platform/links/__init__.py",
          "airflow/providers/google/marketing_platform/links/analytics_admin.py||airflow/providers/google/marketing_platform/links/analytics_admin.py",
          "airflow/providers/google/marketing_platform/operators/analytics.py||airflow/providers/google/marketing_platform/operators/analytics.py",
          "airflow/providers/google/marketing_platform/operators/analytics_admin.py||airflow/providers/google/marketing_platform/operators/analytics_admin.py",
          "tests/providers/google/marketing_platform/hooks/test_analytics_admin.py||tests/providers/google/marketing_platform/hooks/test_analytics_admin.py",
          "tests/providers/google/marketing_platform/links/__init__.py||tests/providers/google/marketing_platform/links/__init__.py",
          "tests/providers/google/marketing_platform/links/test_analytics_admin.py||tests/providers/google/marketing_platform/links/test_analytics_admin.py",
          "tests/providers/google/marketing_platform/operators/test_analytics_admin.py||tests/providers/google/marketing_platform/operators/test_analytics_admin.py",
          "tests/system/providers/google/marketing_platform/example_analytics_admin.py||tests/system/providers/google/marketing_platform/example_analytics_admin.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/google/marketing_platform/hooks/analytics.py||airflow/providers/google/marketing_platform/hooks/analytics.py": [
          "File: airflow/providers/google/marketing_platform/hooks/analytics.py -> airflow/providers/google/marketing_platform/hooks/analytics.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: from typing import Any",
          "22: from googleapiclient.discovery import Resource, build",
          "23: from googleapiclient.http import MediaFileUpload",
          "25: from airflow.providers.google.common.hooks.base_google import GoogleBaseHook",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import warnings",
          "26: from airflow.exceptions import AirflowProviderDeprecationWarning",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "31:     def __init__(self, api_version: str = \"v3\", *args, **kwargs):",
          "32:         super().__init__(*args, **kwargs)",
          "33:         self.api_version = api_version",
          "34:         self._conn = None",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "35:         warnings.warn(",
          "36:             f\"The `{type(self).__name__}` class is deprecated, please use \"",
          "37:             f\"`GoogleAnalyticsAdminHook` instead.\",",
          "38:             AirflowProviderDeprecationWarning,",
          "39:             stacklevel=1,",
          "40:         )",
          "",
          "---------------"
        ],
        "airflow/providers/google/marketing_platform/hooks/analytics_admin.py||airflow/providers/google/marketing_platform/hooks/analytics_admin.py": [
          "File: airflow/providers/google/marketing_platform/hooks/analytics_admin.py -> airflow/providers/google/marketing_platform/hooks/analytics_admin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: \"\"\"",
          "19: Hooks for Google Analytics (GA4) Admin service.",
          "21: .. spelling:word-list::",
          "23:     DataStream",
          "24:     ListAccountsPager",
          "25:     ListGoogleAdsLinksPager",
          "26: \"\"\"",
          "27: from __future__ import annotations",
          "29: from typing import TYPE_CHECKING, Sequence",
          "31: from google.analytics.admin_v1beta import (",
          "32:     AnalyticsAdminServiceClient,",
          "33:     DataStream,",
          "34:     Property,",
          "35: )",
          "36: from google.api_core.gapic_v1.method import DEFAULT, _MethodDefault",
          "38: from airflow.providers.google.common.consts import CLIENT_INFO",
          "39: from airflow.providers.google.common.hooks.base_google import GoogleBaseHook",
          "41: if TYPE_CHECKING:",
          "42:     from google.analytics.admin_v1beta.services.analytics_admin_service.pagers import (",
          "43:         ListAccountsPager,",
          "44:         ListGoogleAdsLinksPager,",
          "45:     )",
          "46:     from google.api_core.retry import Retry",
          "49: class GoogleAnalyticsAdminHook(GoogleBaseHook):",
          "50:     \"\"\"Hook for Google Analytics 4 (GA4) Admin API.\"\"\"",
          "52:     def __init__(self, *args, **kwargs) -> None:",
          "53:         super().__init__(*args, **kwargs)",
          "54:         self._conn: AnalyticsAdminServiceClient | None = None",
          "56:     def get_conn(self) -> AnalyticsAdminServiceClient:",
          "57:         if not self._conn:",
          "58:             self._conn = AnalyticsAdminServiceClient(",
          "59:                 credentials=self.get_credentials(), client_info=CLIENT_INFO",
          "60:             )",
          "61:         return self._conn",
          "63:     def list_accounts(",
          "64:         self,",
          "65:         page_size: int | None = None,",
          "66:         page_token: str | None = None,",
          "67:         show_deleted: bool | None = None,",
          "68:         retry: Retry | _MethodDefault = DEFAULT,",
          "69:         timeout: float | None = None,",
          "70:         metadata: Sequence[tuple[str, str]] = (),",
          "71:     ) -> ListAccountsPager:",
          "72:         \"\"\"Get list of accounts in Google Analytics.",
          "74:         .. seealso::",
          "75:             For more details please check the client library documentation:",
          "76:             https://developers.google.com/analytics/devguides/config/admin/v1/rest/v1beta/accounts/list",
          "78:         :param page_size: Optional, number of results to return in the list.",
          "79:         :param page_token: Optional. The next_page_token value returned from a previous List request, if any.",
          "80:         :param show_deleted: Optional. Whether to include soft-deleted (ie: \"trashed\") Accounts in the results.",
          "81:         :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "82:             will not be retried.",
          "83:         :param timeout: Optional. The timeout for this request.",
          "84:         :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "86:         :returns: List of Google Analytics accounts.",
          "87:         \"\"\"",
          "88:         request = {\"page_size\": page_size, \"page_token\": page_token, \"show_deleted\": show_deleted}",
          "89:         client = self.get_conn()",
          "90:         return client.list_accounts(request=request, retry=retry, timeout=timeout, metadata=metadata)",
          "92:     def create_property(",
          "93:         self,",
          "94:         analytics_property: Property | dict,",
          "95:         retry: Retry | _MethodDefault = DEFAULT,",
          "96:         timeout: float | None = None,",
          "97:         metadata: Sequence[tuple[str, str]] = (),",
          "98:     ) -> Property:",
          "99:         \"\"\"Create Google Analytics property.",
          "101:         .. seealso::",
          "102:             For more details please check the client library documentation:",
          "103:             https://developers.google.com/analytics/devguides/config/admin/v1/rest/v1beta/properties/create",
          "105:         :param analytics_property: The property to create. Note: the supplied property must specify its",
          "106:             parent.",
          "107:         :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "108:             will not be retried.",
          "109:         :param timeout: Optional. The timeout for this request.",
          "110:         :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "112:         :returns: Created Google Analytics property.",
          "113:         \"\"\"",
          "114:         client = self.get_conn()",
          "115:         return client.create_property(",
          "116:             request={\"property\": analytics_property},",
          "117:             retry=retry,",
          "118:             timeout=timeout,",
          "119:             metadata=metadata,",
          "120:         )",
          "122:     def delete_property(",
          "123:         self,",
          "124:         property_id: str,",
          "125:         retry: Retry | _MethodDefault = DEFAULT,",
          "126:         timeout: float | None = None,",
          "127:         metadata: Sequence[tuple[str, str]] = (),",
          "128:     ) -> Property:",
          "129:         \"\"\"Soft delete Google Analytics property.",
          "131:         .. seealso::",
          "132:             For more details please check the client library documentation:",
          "133:             https://developers.google.com/analytics/devguides/config/admin/v1/rest/v1beta/properties/delete",
          "135:         :param property_id: ID of the Property to soft-delete. Format: properties/{property_id}.",
          "136:         :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "137:             will not be retried.",
          "138:         :param timeout: Optional. The timeout for this request.",
          "139:         :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "141:         :returns: Resource message representing Google Analytics property.",
          "142:         \"\"\"",
          "143:         client = self.get_conn()",
          "144:         request = {\"name\": f\"properties/{property_id}\"}",
          "145:         return client.delete_property(request=request, retry=retry, timeout=timeout, metadata=metadata)",
          "147:     def create_data_stream(",
          "148:         self,",
          "149:         property_id: str,",
          "150:         data_stream: DataStream | dict,",
          "151:         retry: Retry | _MethodDefault = DEFAULT,",
          "152:         timeout: float | None = None,",
          "153:         metadata: Sequence[tuple[str, str]] = (),",
          "154:     ) -> DataStream:",
          "155:         \"\"\"Create Google Analytics data stream.",
          "157:         .. seealso::",
          "158:             For more details please check the client library documentation:",
          "159:             https://developers.google.com/analytics/devguides/config/admin/v1/rest/v1beta/properties.dataStreams/create",
          "161:         :param property_id: ID of the parent property for the data stream.",
          "162:         :param data_stream: The data stream to create.",
          "163:         :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "164:             will not be retried.",
          "165:         :param timeout: Optional. The timeout for this request.",
          "166:         :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "168:         :returns: Created Google Analytics data stream.",
          "169:         \"\"\"",
          "170:         client = self.get_conn()",
          "171:         return client.create_data_stream(",
          "172:             request={\"parent\": f\"properties/{property_id}\", \"data_stream\": data_stream},",
          "173:             retry=retry,",
          "174:             timeout=timeout,",
          "175:             metadata=metadata,",
          "176:         )",
          "178:     def delete_data_stream(",
          "179:         self,",
          "180:         property_id: str,",
          "181:         data_stream_id: str,",
          "182:         retry: Retry | _MethodDefault = DEFAULT,",
          "183:         timeout: float | None = None,",
          "184:         metadata: Sequence[tuple[str, str]] = (),",
          "185:     ) -> None:",
          "186:         \"\"\"Delete Google Analytics data stream.",
          "188:         .. seealso::",
          "189:             For more details please check the client library documentation:",
          "190:             https://developers.google.com/analytics/devguides/config/admin/v1/rest/v1beta/properties.dataStreams/delete",
          "192:         :param property_id: ID of the parent property for the data stream.",
          "193:         :param data_stream_id: The data stream id to delete.",
          "194:         :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "195:             will not be retried.",
          "196:         :param timeout: Optional. The timeout for this request.",
          "197:         :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "198:         \"\"\"",
          "199:         client = self.get_conn()",
          "200:         return client.delete_data_stream(",
          "201:             request={\"name\": f\"properties/{property_id}/dataStreams/{data_stream_id}\"},",
          "202:             retry=retry,",
          "203:             timeout=timeout,",
          "204:             metadata=metadata,",
          "205:         )",
          "207:     def list_google_ads_links(",
          "208:         self,",
          "209:         property_id: str,",
          "210:         page_size: int | None = None,",
          "211:         page_token: str | None = None,",
          "212:         retry: Retry | _MethodDefault = DEFAULT,",
          "213:         timeout: float | None = None,",
          "214:         metadata: Sequence[tuple[str, str]] = (),",
          "215:     ) -> ListGoogleAdsLinksPager:",
          "216:         \"\"\"Get list of Google Ads links.",
          "218:         .. seealso::",
          "219:             For more details please check the client library documentation:",
          "220:             https://googleapis.dev/python/analyticsadmin/latest/admin_v1beta/analytics_admin_service.html#google.analytics.admin_v1beta.services.analytics_admin_service.AnalyticsAdminServiceAsyncClient.list_google_ads_links",
          "222:         :param property_id: ID of the parent property.",
          "223:         :param page_size: Optional, number of results to return in the list.",
          "224:         :param page_token: Optional. The next_page_token value returned from a previous List request, if any.",
          "225:         :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "226:             will not be retried.",
          "227:         :param timeout: Optional. The timeout for this request.",
          "228:         :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "230:         :returns: List of Google Analytics accounts.",
          "231:         \"\"\"",
          "232:         client = self.get_conn()",
          "233:         request = {\"parent\": f\"properties/{property_id}\", \"page_size\": page_size, \"page_token\": page_token}",
          "234:         return client.list_google_ads_links(request=request, retry=retry, timeout=timeout, metadata=metadata)",
          "",
          "---------------"
        ],
        "airflow/providers/google/marketing_platform/links/__init__.py||airflow/providers/google/marketing_platform/links/__init__.py": [
          "File: airflow/providers/google/marketing_platform/links/__init__.py -> airflow/providers/google/marketing_platform/links/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "",
          "---------------"
        ],
        "airflow/providers/google/marketing_platform/links/analytics_admin.py||airflow/providers/google/marketing_platform/links/analytics_admin.py": [
          "File: airflow/providers/google/marketing_platform/links/analytics_admin.py -> airflow/providers/google/marketing_platform/links/analytics_admin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "19: from typing import TYPE_CHECKING, ClassVar",
          "21: from airflow.models import BaseOperator, BaseOperatorLink, XCom",
          "23: if TYPE_CHECKING:",
          "24:     from airflow.models.taskinstancekey import TaskInstanceKey",
          "25:     from airflow.utils.context import Context",
          "28: BASE_LINK = \"https://analytics.google.com/analytics/web/\"",
          "31: class GoogleAnalyticsBaseLink(BaseOperatorLink):",
          "32:     \"\"\"Base class for Google Analytics links.",
          "34:     :meta private:",
          "35:     \"\"\"",
          "37:     name: ClassVar[str]",
          "38:     key: ClassVar[str]",
          "39:     format_str: ClassVar[str]",
          "41:     def get_link(self, operator: BaseOperator, *, ti_key: TaskInstanceKey) -> str:",
          "42:         if conf := XCom.get_value(key=self.key, ti_key=ti_key):",
          "43:             res = BASE_LINK + \"#/\" + self.format_str.format(**conf)",
          "44:             return res",
          "45:         return \"\"",
          "48: class GoogleAnalyticsPropertyLink(GoogleAnalyticsBaseLink):",
          "49:     \"\"\"Helper class for constructing Google Analytics Property Link.\"\"\"",
          "51:     name = \"Data Analytics Property\"",
          "52:     key = \"data_analytics_property\"",
          "53:     format_str = \"p{property_id}/\"",
          "55:     @staticmethod",
          "56:     def persist(",
          "57:         context: Context,",
          "58:         task_instance: BaseOperator,",
          "59:         property_id: str,",
          "60:     ):",
          "61:         task_instance.xcom_push(",
          "62:             context,",
          "63:             key=GoogleAnalyticsPropertyLink.key,",
          "64:             value={\"property_id\": property_id},",
          "65:         )",
          "",
          "---------------"
        ],
        "airflow/providers/google/marketing_platform/operators/analytics.py||airflow/providers/google/marketing_platform/operators/analytics.py": [
          "File: airflow/providers/google/marketing_platform/operators/analytics.py -> airflow/providers/google/marketing_platform/operators/analytics.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: from __future__ import annotations",
          "21: import csv",
          "22: from tempfile import NamedTemporaryFile",
          "23: from typing import TYPE_CHECKING, Any, Sequence",
          "25: from airflow.models import BaseOperator",
          "26: from airflow.providers.google.cloud.hooks.gcs import GCSHook",
          "27: from airflow.providers.google.marketing_platform.hooks.analytics import GoogleAnalyticsHook",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: import warnings",
          "26: from airflow.exceptions import AirflowProviderDeprecationWarning",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "34:     \"\"\"",
          "35:     Lists all accounts to which the user has access.",
          "37:     .. seealso::",
          "38:         Check official API docs:",
          "39:         https://developers.google.com/analytics/devguides/config/mgmt/v3/mgmtReference/management/accounts/list",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "39:     .. seealso::",
          "40:         This operator is deprecated, please use",
          "41:         :class:`~airflow.providers.google.marketing_platform.operators.analytics_admin.GoogleAnalyticsAdminListAccountsOperator`:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "70:         impersonation_chain: str | Sequence[str] | None = None,",
          "72:     ) -> None:",
          "73:         super().__init__(**kwargs)",
          "75:         self.api_version = api_version",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "79:         warnings.warn(",
          "80:             f\"The `{type(self).__name__}` operator is deprecated, please use \"",
          "81:             f\"`GoogleAnalyticsAdminListAccountsOperator` instead.\",",
          "82:             AirflowProviderDeprecationWarning,",
          "83:             stacklevel=1,",
          "84:         )",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "90:     \"\"\"",
          "91:     Returns a web property-Google Ads link to which the user has access.",
          "93:     .. seealso::",
          "94:         Check official API docs:",
          "95:         https://developers.google.com/analytics/devguides/config/mgmt/v3/mgmtReference/management/webPropertyAdWordsLinks/get",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "106:     .. seealso::",
          "107:         This operator is deprecated, please use",
          "108:         :class:`~airflow.providers.google.marketing_platform.operators.analytics_admin.GoogleAnalyticsAdminGetGoogleAdsLinkOperator`:",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "133:     ):",
          "134:         super().__init__(**kwargs)",
          "136:         self.account_id = account_id",
          "137:         self.web_property_ad_words_link_id = web_property_ad_words_link_id",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "152:         warnings.warn(",
          "153:             f\"The `{type(self).__name__}` operator is deprecated, please use \"",
          "154:             f\"`GoogleAnalyticsAdminGetGoogleAdsLinkOperator` instead.\",",
          "155:             AirflowProviderDeprecationWarning,",
          "156:             stacklevel=1,",
          "157:         )",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "158:     \"\"\"",
          "159:     Lists webProperty-Google Ads links for a given web property.",
          "161:     .. seealso::",
          "162:         Check official API docs:",
          "163:         https://developers.google.com/analytics/devguides/config/mgmt/v3/mgmtReference/management/webPropertyAdWordsLinks/list#http-request",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "184:     .. seealso::",
          "185:         This operator is deprecated, please use",
          "186:         :class:`~airflow.providers.google.marketing_platform.operators.analytics_admin.GoogleAnalyticsAdminListGoogleAdsLinksOperator`:",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "198:     ) -> None:",
          "199:         super().__init__(**kwargs)",
          "201:         self.account_id = account_id",
          "202:         self.web_property_id = web_property_id",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "227:         warnings.warn(",
          "228:             f\"The `{type(self).__name__}` operator is deprecated, please use \"",
          "229:             f\"`GoogleAnalyticsAdminListGoogleAdsLinksOperator` instead.\",",
          "230:             AirflowProviderDeprecationWarning,",
          "231:             stacklevel=1,",
          "232:         )",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "221:     \"\"\"",
          "222:     Take a file from Cloud Storage and uploads it to GA via data import API.",
          "224:     :param storage_bucket: The Google cloud storage bucket where the file is stored.",
          "225:     :param storage_name_object: The name of the object in the desired Google cloud",
          "226:           storage bucket. (templated) If the destination points to an existing",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "257:     .. seealso::",
          "258:         This operator is deprecated, please use",
          "259:         :class:`~airflow.providers.google.marketing_platform.operators.analytics_admin.GoogleAnalyticsAdminCreateDataStreamOperator`:",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "266:         impersonation_chain: str | Sequence[str] | None = None,",
          "268:     ) -> None:",
          "269:         super().__init__(**kwargs)",
          "270:         self.storage_bucket = storage_bucket",
          "271:         self.storage_name_object = storage_name_object",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "306:         warnings.warn(",
          "307:             f\"The `{type(self).__name__}` operator is deprecated, please use \"",
          "308:             f\"`GoogleAnalyticsAdminCreateDataStreamOperator` instead.\",",
          "309:             AirflowProviderDeprecationWarning,",
          "310:             stacklevel=1,",
          "311:         )",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "317:     \"\"\"",
          "318:     Deletes previous GA uploads to leave the latest file to control the size of the Data Set Quota.",
          "320:     :param account_id: The GA account Id (long) to which the data upload belongs.",
          "321:     :param web_property_id: The web property UA-string associated with the upload.",
          "322:     :param custom_data_source_id: The id to which the data import belongs.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "363:     .. seealso::",
          "364:         This operator is deprecated, please use",
          "365:         :class:`~airflow.providers.google.marketing_platform.operators.analytics_admin.GoogleAnalyticsAdminDeleteDataStreamOperator`:",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "348:         impersonation_chain: str | Sequence[str] | None = None,",
          "350:     ) -> None:",
          "351:         super().__init__(**kwargs)",
          "353:         self.account_id = account_id",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "398:         warnings.warn(",
          "399:             f\"The `{type(self).__name__}` operator is deprecated, please use \"",
          "400:             f\"`GoogleAnalyticsAdminDeleteDataStreamOperator` instead.\",",
          "401:             AirflowProviderDeprecationWarning,",
          "402:             stacklevel=1,",
          "403:         )",
          "",
          "---------------"
        ],
        "airflow/providers/google/marketing_platform/operators/analytics_admin.py||airflow/providers/google/marketing_platform/operators/analytics_admin.py": [
          "File: airflow/providers/google/marketing_platform/operators/analytics_admin.py -> airflow/providers/google/marketing_platform/operators/analytics_admin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: \"\"\"This module contains Google Analytics 4 (GA4) operators.\"\"\"",
          "19: from __future__ import annotations",
          "21: from typing import TYPE_CHECKING, Any, Sequence",
          "23: from google.analytics.admin_v1beta import (",
          "24:     Account,",
          "25:     DataStream,",
          "26:     GoogleAdsLink,",
          "27:     Property,",
          "28: )",
          "29: from google.api_core.gapic_v1.method import DEFAULT, _MethodDefault",
          "31: from airflow.exceptions import AirflowNotFoundException",
          "32: from airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
          "33: from airflow.providers.google.marketing_platform.hooks.analytics_admin import GoogleAnalyticsAdminHook",
          "34: from airflow.providers.google.marketing_platform.links.analytics_admin import GoogleAnalyticsPropertyLink",
          "36: if TYPE_CHECKING:",
          "37:     from google.api_core.retry import Retry",
          "38:     from google.protobuf.message import Message",
          "40:     from airflow.utils.context import Context",
          "43: class GoogleAnalyticsAdminListAccountsOperator(GoogleCloudBaseOperator):",
          "44:     \"\"\"",
          "45:     Lists all accounts to which the user has access.",
          "47:     .. seealso::",
          "48:         For more information on how to use this operator, take a look at the guide:",
          "49:         :ref:`howto/operator:GoogleAnalyticsAdminListAccountsOperator`",
          "51:     :param page_size: Optional, number of results to return in the list.",
          "52:     :param page_token: Optional. The next_page_token value returned from a previous List request, if any.",
          "53:     :param show_deleted: Optional. Whether to include soft-deleted (ie: \"trashed\") Accounts in the results.",
          "54:     :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "55:             will not be retried.",
          "56:     :param timeout: Optional. The timeout for this request.",
          "57:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "58:     :param gcp_conn_id: The connection ID to use when fetching connection info.",
          "59:     :param impersonation_chain: Optional. Service account to impersonate using short-term",
          "60:         credentials, or chained list of accounts required to get the access_token",
          "61:         of the last account in the list, which will be impersonated in the request.",
          "62:         If set as a string, the account must grant the originating account",
          "63:         the Service Account Token Creator IAM role.",
          "64:         If set as a sequence, the identities from the list must grant",
          "65:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "66:         account from the list granting this role to the originating account (templated).",
          "67:     \"\"\"",
          "69:     template_fields: Sequence[str] = (",
          "70:         \"gcp_conn_id\",",
          "71:         \"impersonation_chain\",",
          "72:         \"page_size\",",
          "73:         \"page_token\",",
          "74:     )",
          "76:     def __init__(",
          "77:         self,",
          "79:         page_size: int | None = None,",
          "80:         page_token: str | None = None,",
          "81:         show_deleted: bool | None = None,",
          "82:         retry: Retry | _MethodDefault = DEFAULT,",
          "83:         timeout: float | None = None,",
          "84:         metadata: Sequence[tuple[str, str]] = (),",
          "85:         gcp_conn_id: str = \"google_cloud_default\",",
          "86:         impersonation_chain: str | Sequence[str] | None = None,",
          "88:     ) -> None:",
          "89:         super().__init__(**kwargs)",
          "90:         self.page_size = page_size",
          "91:         self.page_token = page_token",
          "92:         self.show_deleted = show_deleted",
          "93:         self.retry = retry",
          "94:         self.timeout = timeout",
          "95:         self.metadata = metadata",
          "96:         self.gcp_conn_id = gcp_conn_id",
          "97:         self.impersonation_chain = impersonation_chain",
          "99:     def execute(",
          "100:         self,",
          "101:         context: Context,",
          "102:     ) -> Sequence[Message]:",
          "103:         hook = GoogleAnalyticsAdminHook(",
          "104:             gcp_conn_id=self.gcp_conn_id,",
          "105:             impersonation_chain=self.impersonation_chain,",
          "106:         )",
          "107:         self.log.info(",
          "108:             \"Requesting list of Google Analytics accounts. \"",
          "109:             f\"Page size: {self.page_size}, page token: {self.page_token}\"",
          "110:         )",
          "111:         accounts = hook.list_accounts(",
          "112:             page_size=self.page_size,",
          "113:             page_token=self.page_token,",
          "114:             show_deleted=self.show_deleted,",
          "115:             retry=self.retry,",
          "116:             timeout=self.timeout,",
          "117:             metadata=self.metadata,",
          "118:         )",
          "119:         accounts_list: Sequence[Message] = [Account.to_dict(item) for item in accounts]",
          "120:         n = len(accounts_list)",
          "121:         self.log.info(\"Successful request. Retrieved %s item%s.\", n, \"s\" if n > 1 else \"\")",
          "122:         return accounts_list",
          "125: class GoogleAnalyticsAdminCreatePropertyOperator(GoogleCloudBaseOperator):",
          "126:     \"\"\"",
          "127:     Creates property.",
          "129:     .. seealso::",
          "130:         For more information on how to use this operator, take a look at the guide:",
          "131:         :ref:`howto/operator:GoogleAnalyticsAdminCreatePropertyOperator`",
          "133:     :param analytics_property: The property to create. Note: the supplied property must specify its parent.",
          "134:         For more details see: https://developers.google.com/analytics/devguides/config/admin/v1/rest/v1beta/properties#Property",
          "135:     :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "136:         will not be retried.",
          "137:     :param timeout: Optional. The timeout for this request.",
          "138:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "139:     :param gcp_conn_id: The connection ID to use when fetching connection info.",
          "140:     :param impersonation_chain: Optional. Service account to impersonate using short-term",
          "141:         credentials, or chained list of accounts required to get the access_token",
          "142:         of the last account in the list, which will be impersonated in the request.",
          "143:         If set as a string, the account must grant the originating account",
          "144:         the Service Account Token Creator IAM role.",
          "145:         If set as a sequence, the identities from the list must grant",
          "146:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "147:         account from the list granting this role to the originating account (templated).",
          "148:     \"\"\"",
          "150:     template_fields: Sequence[str] = (",
          "151:         \"gcp_conn_id\",",
          "152:         \"impersonation_chain\",",
          "153:         \"analytics_property\",",
          "154:     )",
          "155:     operator_extra_links = (GoogleAnalyticsPropertyLink(),)",
          "157:     def __init__(",
          "158:         self,",
          "160:         analytics_property: Property | dict[str, Any],",
          "161:         retry: Retry | _MethodDefault = DEFAULT,",
          "162:         timeout: float | None = None,",
          "163:         metadata: Sequence[tuple[str, str]] = (),",
          "164:         gcp_conn_id: str = \"google_cloud_default\",",
          "165:         impersonation_chain: str | Sequence[str] | None = None,",
          "167:     ) -> None:",
          "168:         super().__init__(**kwargs)",
          "169:         self.analytics_property = analytics_property",
          "170:         self.retry = retry",
          "171:         self.timeout = timeout",
          "172:         self.metadata = metadata",
          "173:         self.gcp_conn_id = gcp_conn_id",
          "174:         self.impersonation_chain = impersonation_chain",
          "176:     def execute(",
          "177:         self,",
          "178:         context: Context,",
          "179:     ) -> Message:",
          "180:         hook = GoogleAnalyticsAdminHook(",
          "181:             gcp_conn_id=self.gcp_conn_id,",
          "182:             impersonation_chain=self.impersonation_chain,",
          "183:         )",
          "184:         self.log.info(\"Creating a Google Analytics property.\")",
          "185:         prop = hook.create_property(",
          "186:             analytics_property=self.analytics_property,",
          "187:             retry=self.retry,",
          "188:             timeout=self.timeout,",
          "189:             metadata=self.metadata,",
          "190:         )",
          "191:         self.log.info(\"The Google Analytics property %s was created successfully.\", prop.name)",
          "192:         GoogleAnalyticsPropertyLink.persist(",
          "193:             context=context,",
          "194:             task_instance=self,",
          "195:             property_id=prop.name.lstrip(\"properties/\"),",
          "196:         )",
          "198:         return Property.to_dict(prop)",
          "201: class GoogleAnalyticsAdminDeletePropertyOperator(GoogleCloudBaseOperator):",
          "202:     \"\"\"",
          "203:     Soft-delete property.",
          "205:     .. seealso::",
          "206:         For more information on how to use this operator, take a look at the guide:",
          "207:         :ref:`howto/operator:GoogleAnalyticsAdminDeletePropertyOperator`",
          "209:     :param property_id: The id of the Property to soft-delete.",
          "210:     :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "211:         will not be retried.",
          "212:     :param timeout: Optional. The timeout for this request.",
          "213:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "214:     :param gcp_conn_id: The connection ID to use when fetching connection info.",
          "215:     :param impersonation_chain: Optional. Service account to impersonate using short-term",
          "216:         credentials, or chained list of accounts required to get the access_token",
          "217:         of the last account in the list, which will be impersonated in the request.",
          "218:         If set as a string, the account must grant the originating account",
          "219:         the Service Account Token Creator IAM role.",
          "220:         If set as a sequence, the identities from the list must grant",
          "221:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "222:         account from the list granting this role to the originating account (templated).",
          "223:     \"\"\"",
          "225:     template_fields: Sequence[str] = (",
          "226:         \"gcp_conn_id\",",
          "227:         \"impersonation_chain\",",
          "228:         \"property_id\",",
          "229:     )",
          "231:     def __init__(",
          "232:         self,",
          "234:         property_id: str,",
          "235:         retry: Retry | _MethodDefault = DEFAULT,",
          "236:         timeout: float | None = None,",
          "237:         metadata: Sequence[tuple[str, str]] = (),",
          "238:         gcp_conn_id: str = \"google_cloud_default\",",
          "239:         impersonation_chain: str | Sequence[str] | None = None,",
          "241:     ) -> None:",
          "242:         super().__init__(**kwargs)",
          "243:         self.property_id = property_id",
          "244:         self.retry = retry",
          "245:         self.timeout = timeout",
          "246:         self.metadata = metadata",
          "247:         self.gcp_conn_id = gcp_conn_id",
          "248:         self.impersonation_chain = impersonation_chain",
          "250:     def execute(",
          "251:         self,",
          "252:         context: Context,",
          "253:     ) -> Message:",
          "254:         hook = GoogleAnalyticsAdminHook(",
          "255:             gcp_conn_id=self.gcp_conn_id,",
          "256:             impersonation_chain=self.impersonation_chain,",
          "257:         )",
          "258:         self.log.info(\"Deleting a Google Analytics property.\")",
          "259:         prop = hook.delete_property(",
          "260:             property_id=self.property_id,",
          "261:             retry=self.retry,",
          "262:             timeout=self.timeout,",
          "263:             metadata=self.metadata,",
          "264:         )",
          "265:         self.log.info(\"The Google Analytics property %s was soft-deleted successfully.\", prop.name)",
          "266:         return Property.to_dict(prop)",
          "269: class GoogleAnalyticsAdminCreateDataStreamOperator(GoogleCloudBaseOperator):",
          "270:     \"\"\"",
          "271:     Creates Data stream.",
          "273:     .. seealso::",
          "274:         For more information on how to use this operator, take a look at the guide:",
          "275:         :ref:`howto/operator:GoogleAnalyticsAdminCreateDataStreamOperator`",
          "277:     :param property_id: ID of the parent property for the data stream.",
          "278:     :param data_stream: The data stream to create.",
          "279:         For more details see: https://developers.google.com/analytics/devguides/config/admin/v1/rest/v1beta/properties.dataStreams#DataStream",
          "280:     :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "281:         will not be retried.",
          "282:     :param timeout: Optional. The timeout for this request.",
          "283:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "284:     :param gcp_conn_id: The connection ID to use when fetching connection info.",
          "285:     :param impersonation_chain: Optional. Service account to impersonate using short-term",
          "286:         credentials, or chained list of accounts required to get the access_token",
          "287:         of the last account in the list, which will be impersonated in the request.",
          "288:         If set as a string, the account must grant the originating account",
          "289:         the Service Account Token Creator IAM role.",
          "290:         If set as a sequence, the identities from the list must grant",
          "291:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "292:         account from the list granting this role to the originating account (templated).",
          "293:     \"\"\"",
          "295:     template_fields: Sequence[str] = (",
          "296:         \"gcp_conn_id\",",
          "297:         \"impersonation_chain\",",
          "298:         \"property_id\",",
          "299:         \"data_stream\",",
          "300:     )",
          "302:     def __init__(",
          "303:         self,",
          "305:         property_id: str,",
          "306:         data_stream: DataStream | dict,",
          "307:         retry: Retry | _MethodDefault = DEFAULT,",
          "308:         timeout: float | None = None,",
          "309:         metadata: Sequence[tuple[str, str]] = (),",
          "310:         gcp_conn_id: str = \"google_cloud_default\",",
          "311:         impersonation_chain: str | Sequence[str] | None = None,",
          "313:     ) -> None:",
          "314:         super().__init__(**kwargs)",
          "315:         self.property_id = property_id",
          "316:         self.data_stream = data_stream",
          "317:         self.retry = retry",
          "318:         self.timeout = timeout",
          "319:         self.metadata = metadata",
          "320:         self.gcp_conn_id = gcp_conn_id",
          "321:         self.impersonation_chain = impersonation_chain",
          "323:     def execute(",
          "324:         self,",
          "325:         context: Context,",
          "326:     ) -> Message:",
          "327:         hook = GoogleAnalyticsAdminHook(",
          "328:             gcp_conn_id=self.gcp_conn_id,",
          "329:             impersonation_chain=self.impersonation_chain,",
          "330:         )",
          "331:         self.log.info(\"Creating a Google Analytics data stream.\")",
          "332:         data_stream = hook.create_data_stream(",
          "333:             property_id=self.property_id,",
          "334:             data_stream=self.data_stream,",
          "335:             retry=self.retry,",
          "336:             timeout=self.timeout,",
          "337:             metadata=self.metadata,",
          "338:         )",
          "339:         self.log.info(\"The Google Analytics data stream %s was created successfully.\", data_stream.name)",
          "340:         return DataStream.to_dict(data_stream)",
          "343: class GoogleAnalyticsAdminDeleteDataStreamOperator(GoogleCloudBaseOperator):",
          "344:     \"\"\"",
          "345:     Deletes Data stream.",
          "347:     .. seealso::",
          "348:         For more information on how to use this operator, take a look at the guide:",
          "349:         :ref:`howto/operator:GoogleAnalyticsAdminDeleteDataStreamOperator`",
          "351:     :param property_id: ID of the property which is parent for the data stream.",
          "352:     :param data_stream_id: ID of the data stream to delete.",
          "353:     :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "354:         will not be retried.",
          "355:     :param timeout: Optional. The timeout for this request.",
          "356:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "357:     :param gcp_conn_id: The connection ID to use when fetching connection info.",
          "358:     :param impersonation_chain: Optional. Service account to impersonate using short-term",
          "359:         credentials, or chained list of accounts required to get the access_token",
          "360:         of the last account in the list, which will be impersonated in the request.",
          "361:         If set as a string, the account must grant the originating account",
          "362:         the Service Account Token Creator IAM role.",
          "363:         If set as a sequence, the identities from the list must grant",
          "364:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "365:         account from the list granting this role to the originating account (templated).",
          "366:     \"\"\"",
          "368:     template_fields: Sequence[str] = (",
          "369:         \"gcp_conn_id\",",
          "370:         \"impersonation_chain\",",
          "371:         \"property_id\",",
          "372:         \"data_stream_id\",",
          "373:     )",
          "375:     def __init__(",
          "376:         self,",
          "378:         property_id: str,",
          "379:         data_stream_id: str,",
          "380:         retry: Retry | _MethodDefault = DEFAULT,",
          "381:         timeout: float | None = None,",
          "382:         metadata: Sequence[tuple[str, str]] = (),",
          "383:         gcp_conn_id: str = \"google_cloud_default\",",
          "384:         impersonation_chain: str | Sequence[str] | None = None,",
          "386:     ) -> None:",
          "387:         super().__init__(**kwargs)",
          "388:         self.property_id = property_id",
          "389:         self.data_stream_id = data_stream_id",
          "390:         self.retry = retry",
          "391:         self.timeout = timeout",
          "392:         self.metadata = metadata",
          "393:         self.gcp_conn_id = gcp_conn_id",
          "394:         self.impersonation_chain = impersonation_chain",
          "396:     def execute(",
          "397:         self,",
          "398:         context: Context,",
          "399:     ) -> None:",
          "400:         hook = GoogleAnalyticsAdminHook(",
          "401:             gcp_conn_id=self.gcp_conn_id,",
          "402:             impersonation_chain=self.impersonation_chain,",
          "403:         )",
          "404:         self.log.info(\"Deleting a Google Analytics data stream (id %s).\", self.data_stream_id)",
          "405:         hook.delete_data_stream(",
          "406:             property_id=self.property_id,",
          "407:             data_stream_id=self.data_stream_id,",
          "408:             retry=self.retry,",
          "409:             timeout=self.timeout,",
          "410:             metadata=self.metadata,",
          "411:         )",
          "412:         self.log.info(\"The Google Analytics data stream was deleted successfully.\")",
          "413:         return None",
          "416: class GoogleAnalyticsAdminListGoogleAdsLinksOperator(GoogleCloudBaseOperator):",
          "417:     \"\"\"",
          "418:     Lists all Google Ads links associated with a given property.",
          "420:     .. seealso::",
          "421:         For more information on how to use this operator, take a look at the guide:",
          "422:         :ref:`howto/operator:GoogleAnalyticsAdminListGoogleAdsLinksOperator`",
          "424:     :param property_id: ID of the parent property.",
          "425:     :param page_size: Optional, number of results to return in the list.",
          "426:     :param page_token: Optional. The next_page_token value returned from a previous List request, if any.",
          "427:     :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "428:         will not be retried.",
          "429:     :param timeout: Optional. The timeout for this request.",
          "430:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "431:     :param gcp_conn_id: The connection ID to use when fetching connection info.",
          "432:     :param impersonation_chain: Optional. Service account to impersonate using short-term",
          "433:         credentials, or chained list of accounts required to get the access_token",
          "434:         of the last account in the list, which will be impersonated in the request.",
          "435:         If set as a string, the account must grant the originating account",
          "436:         the Service Account Token Creator IAM role.",
          "437:         If set as a sequence, the identities from the list must grant",
          "438:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "439:         account from the list granting this role to the originating account (templated).",
          "440:     \"\"\"",
          "442:     template_fields: Sequence[str] = (",
          "443:         \"gcp_conn_id\",",
          "444:         \"impersonation_chain\",",
          "445:         \"property_id\",",
          "446:         \"page_size\",",
          "447:         \"page_token\",",
          "448:     )",
          "450:     def __init__(",
          "451:         self,",
          "453:         property_id: str,",
          "454:         page_size: int | None = None,",
          "455:         page_token: str | None = None,",
          "456:         retry: Retry | _MethodDefault = DEFAULT,",
          "457:         timeout: float | None = None,",
          "458:         metadata: Sequence[tuple[str, str]] = (),",
          "459:         gcp_conn_id: str = \"google_cloud_default\",",
          "460:         impersonation_chain: str | Sequence[str] | None = None,",
          "462:     ) -> None:",
          "463:         super().__init__(**kwargs)",
          "464:         self.property_id = property_id",
          "465:         self.page_size = page_size",
          "466:         self.page_token = page_token",
          "467:         self.retry = retry",
          "468:         self.timeout = timeout",
          "469:         self.metadata = metadata",
          "470:         self.gcp_conn_id = gcp_conn_id",
          "471:         self.impersonation_chain = impersonation_chain",
          "473:     def execute(",
          "474:         self,",
          "475:         context: Context,",
          "476:     ) -> Sequence[Message]:",
          "477:         hook = GoogleAnalyticsAdminHook(",
          "478:             gcp_conn_id=self.gcp_conn_id,",
          "479:             impersonation_chain=self.impersonation_chain,",
          "480:         )",
          "481:         self.log.info(",
          "482:             \"Requesting list of Google Ads links accounts for the property_id %s, \"",
          "483:             \"page size %s, page token %s\",",
          "484:             self.property_id,",
          "485:             self.page_size,",
          "486:             self.page_token,",
          "487:         )",
          "488:         google_ads_links = hook.list_google_ads_links(",
          "489:             property_id=self.property_id,",
          "490:             page_size=self.page_size,",
          "491:             page_token=self.page_token,",
          "492:             retry=self.retry,",
          "493:             timeout=self.timeout,",
          "494:             metadata=self.metadata,",
          "495:         )",
          "496:         ads_links_list: Sequence[Message] = [GoogleAdsLink.to_dict(item) for item in google_ads_links]",
          "497:         n = len(ads_links_list)",
          "498:         self.log.info(\"Successful request. Retrieved %s item%s.\", n, \"s\" if n > 1 else \"\")",
          "499:         return ads_links_list",
          "502: class GoogleAnalyticsAdminGetGoogleAdsLinkOperator(GoogleCloudBaseOperator):",
          "503:     \"\"\"",
          "504:     Gets a Google Ads link associated with a given property.",
          "506:     .. seealso::",
          "507:         For more information on how to use this operator, take a look at the guide:",
          "508:         :ref:`howto/operator:GoogleAnalyticsAdminGetGoogleAdsLinkOperator`",
          "510:     :param property_id: Parent property id.",
          "511:     :param google_ads_link_id: Google Ads link id.",
          "512:     :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "513:         will not be retried.",
          "514:     :param timeout: Optional. The timeout for this request.",
          "515:     :param metadata: Optional. Strings which should be sent along with the request as metadata.",
          "516:     :param gcp_conn_id: The connection ID to use when fetching connection info.",
          "517:     :param impersonation_chain: Optional. Service account to impersonate using short-term",
          "518:         credentials, or chained list of accounts required to get the access_token",
          "519:         of the last account in the list, which will be impersonated in the request.",
          "520:         If set as a string, the account must grant the originating account",
          "521:         the Service Account Token Creator IAM role.",
          "522:         If set as a sequence, the identities from the list must grant",
          "523:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "524:         account from the list granting this role to the originating account (templated).",
          "525:     \"\"\"",
          "527:     template_fields: Sequence[str] = (",
          "528:         \"gcp_conn_id\",",
          "529:         \"impersonation_chain\",",
          "530:         \"google_ads_link_id\",",
          "531:     )",
          "533:     def __init__(",
          "534:         self,",
          "536:         property_id: str,",
          "537:         google_ads_link_id: str,",
          "538:         retry: Retry | _MethodDefault = DEFAULT,",
          "539:         timeout: float | None = None,",
          "540:         metadata: Sequence[tuple[str, str]] = (),",
          "541:         gcp_conn_id: str = \"google_cloud_default\",",
          "542:         impersonation_chain: str | Sequence[str] | None = None,",
          "544:     ) -> None:",
          "545:         super().__init__(**kwargs)",
          "546:         self.property_id = property_id",
          "547:         self.google_ads_link_id = google_ads_link_id",
          "548:         self.retry = retry",
          "549:         self.timeout = timeout",
          "550:         self.metadata = metadata",
          "551:         self.gcp_conn_id = gcp_conn_id",
          "552:         self.impersonation_chain = impersonation_chain",
          "554:     def execute(",
          "555:         self,",
          "556:         context: Context,",
          "557:     ) -> Message:",
          "558:         hook = GoogleAnalyticsAdminHook(",
          "559:             gcp_conn_id=self.gcp_conn_id,",
          "560:             impersonation_chain=self.impersonation_chain,",
          "561:         )",
          "562:         self.log.info(",
          "563:             \"Requesting the Google Ads link with id %s for the property_id %s\",",
          "564:             self.google_ads_link_id,",
          "565:             self.property_id,",
          "566:         )",
          "567:         ads_links = hook.list_google_ads_links(",
          "568:             property_id=self.property_id,",
          "569:             retry=self.retry,",
          "570:             timeout=self.timeout,",
          "571:             metadata=self.metadata,",
          "572:         )",
          "573:         find_link = (item for item in ads_links if item.name.split(\"/\")[-1] == self.google_ads_link_id)",
          "574:         if ads_link := next(find_link, None):",
          "575:             self.log.info(\"Successful request.\")",
          "576:             return GoogleAdsLink.to_dict(ads_link)",
          "577:         raise AirflowNotFoundException(",
          "578:             f\"Google Ads Link with id {self.google_ads_link_id} and property id {self.property_id} not found\"",
          "579:         )",
          "",
          "---------------"
        ],
        "tests/providers/google/marketing_platform/hooks/test_analytics_admin.py||tests/providers/google/marketing_platform/hooks/test_analytics_admin.py": [
          "File: tests/providers/google/marketing_platform/hooks/test_analytics_admin.py -> tests/providers/google/marketing_platform/hooks/test_analytics_admin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: from unittest import mock",
          "22: from airflow.providers.google.marketing_platform.hooks.analytics_admin import GoogleAnalyticsAdminHook",
          "23: from tests.providers.google.cloud.utils.base_gcp_mock import mock_base_gcp_hook_default_project_id",
          "25: GCP_CONN_ID = \"test_gcp_conn_id\"",
          "26: IMPERSONATION_CHAIN = [\"ACCOUNT_1\", \"ACCOUNT_2\", \"ACCOUNT_3\"]",
          "27: TEST_PROPERTY_ID = \"123456789\"",
          "28: TEST_PROPERTY_NAME = f\"properties/{TEST_PROPERTY_ID}\"",
          "29: TEST_DATASTREAM_ID = \"987654321\"",
          "30: TEST_DATASTREAM_NAME = f\"properties/{TEST_PROPERTY_ID}/dataStreams/{TEST_DATASTREAM_ID}\"",
          "31: ANALYTICS_HOOK_PATH = \"airflow.providers.google.marketing_platform.hooks.analytics_admin\"",
          "34: class TestGoogleAnalyticsAdminHook:",
          "35:     def setup_method(self):",
          "36:         with mock.patch(",
          "37:             \"airflow.providers.google.common.hooks.base_google.GoogleBaseHook.__init__\",",
          "38:             new=mock_base_gcp_hook_default_project_id,",
          "39:         ):",
          "40:             self.hook = GoogleAnalyticsAdminHook(GCP_CONN_ID)",
          "42:     @mock.patch(\"airflow.providers.google.common.hooks.base_google.GoogleBaseHook.__init__\")",
          "43:     def test_init(self, mock_base_init):",
          "44:         GoogleAnalyticsAdminHook(",
          "45:             GCP_CONN_ID,",
          "46:             impersonation_chain=IMPERSONATION_CHAIN,",
          "47:         )",
          "48:         mock_base_init.assert_called_once_with(",
          "49:             GCP_CONN_ID,",
          "50:             impersonation_chain=IMPERSONATION_CHAIN,",
          "51:         )",
          "53:     @mock.patch(f\"{ANALYTICS_HOOK_PATH}.CLIENT_INFO\")",
          "54:     @mock.patch(f\"{ANALYTICS_HOOK_PATH}.GoogleAnalyticsAdminHook.get_credentials\")",
          "55:     @mock.patch(f\"{ANALYTICS_HOOK_PATH}.AnalyticsAdminServiceClient\")",
          "56:     def test_get_conn(self, mock_client, get_credentials, mock_client_info):",
          "57:         mock_credentials = mock.MagicMock()",
          "58:         get_credentials.return_value = mock_credentials",
          "60:         result = self.hook.get_conn()",
          "62:         mock_client.assert_called_once_with(credentials=mock_credentials, client_info=mock_client_info)",
          "63:         assert self.hook._conn == result",
          "65:     @mock.patch(f\"{ANALYTICS_HOOK_PATH}.GoogleAnalyticsAdminHook.get_conn\")",
          "66:     def test_list_accounts(self, mock_get_conn):",
          "67:         list_accounts_expected = mock.MagicMock()",
          "68:         mock_list_accounts = mock_get_conn.return_value.list_accounts",
          "69:         mock_list_accounts.return_value = list_accounts_expected",
          "70:         mock_page_size, mock_page_token, mock_show_deleted, mock_retry, mock_timeout, mock_metadata = (",
          "71:             mock.MagicMock() for _ in range(6)",
          "72:         )",
          "74:         request = {",
          "75:             \"page_size\": mock_page_size,",
          "76:             \"page_token\": mock_page_token,",
          "77:             \"show_deleted\": mock_show_deleted,",
          "78:         }",
          "80:         list_accounts_received = self.hook.list_accounts(",
          "81:             page_size=mock_page_size,",
          "82:             page_token=mock_page_token,",
          "83:             show_deleted=mock_show_deleted,",
          "84:             retry=mock_retry,",
          "85:             timeout=mock_timeout,",
          "86:             metadata=mock_metadata,",
          "87:         )",
          "88:         mock_list_accounts.assert_called_once_with(",
          "89:             request=request, retry=mock_retry, timeout=mock_timeout, metadata=mock_metadata",
          "90:         )",
          "91:         assert list_accounts_received == list_accounts_expected",
          "93:     @mock.patch(f\"{ANALYTICS_HOOK_PATH}.GoogleAnalyticsAdminHook.get_conn\")",
          "94:     def test_create_property(self, mock_get_conn):",
          "95:         property_expected = mock.MagicMock()",
          "97:         mock_create_property = mock_get_conn.return_value.create_property",
          "98:         mock_create_property.return_value = property_expected",
          "99:         mock_property, mock_retry, mock_timeout, mock_metadata = (mock.MagicMock() for _ in range(4))",
          "101:         property_created = self.hook.create_property(",
          "102:             analytics_property=mock_property, retry=mock_retry, timeout=mock_timeout, metadata=mock_metadata",
          "103:         )",
          "105:         request = {\"property\": mock_property}",
          "106:         mock_create_property.assert_called_once_with(",
          "107:             request=request, retry=mock_retry, timeout=mock_timeout, metadata=mock_metadata",
          "108:         )",
          "109:         assert property_created == property_expected",
          "111:     @mock.patch(f\"{ANALYTICS_HOOK_PATH}.GoogleAnalyticsAdminHook.get_conn\")",
          "112:     def test_delete_property(self, mock_get_conn):",
          "113:         property_expected = mock.MagicMock()",
          "114:         mock_delete_property = mock_get_conn.return_value.delete_property",
          "115:         mock_delete_property.return_value = property_expected",
          "116:         mock_retry, mock_timeout, mock_metadata = (mock.MagicMock() for _ in range(3))",
          "118:         property_deleted = self.hook.delete_property(",
          "119:             property_id=TEST_PROPERTY_ID,",
          "120:             retry=mock_retry,",
          "121:             timeout=mock_timeout,",
          "122:             metadata=mock_metadata,",
          "123:         )",
          "124:         request = {\"name\": TEST_PROPERTY_NAME}",
          "125:         mock_delete_property.assert_called_once_with(",
          "126:             request=request,",
          "127:             retry=mock_retry,",
          "128:             timeout=mock_timeout,",
          "129:             metadata=mock_metadata,",
          "130:         )",
          "131:         assert property_deleted == property_expected",
          "133:     @mock.patch(f\"{ANALYTICS_HOOK_PATH}.GoogleAnalyticsAdminHook.get_conn\")",
          "134:     def test_create_data_stream(self, mock_get_conn):",
          "135:         data_stream_expected = mock.MagicMock()",
          "136:         mock_create_data_stream = mock_get_conn.return_value.create_data_stream",
          "137:         mock_create_data_stream.return_value = data_stream_expected",
          "138:         mock_data_stream, mock_retry, mock_timeout, mock_metadata = (mock.MagicMock() for _ in range(4))",
          "140:         data_stream_created = self.hook.create_data_stream(",
          "141:             property_id=TEST_PROPERTY_ID,",
          "142:             data_stream=mock_data_stream,",
          "143:             retry=mock_retry,",
          "144:             timeout=mock_timeout,",
          "145:             metadata=mock_metadata,",
          "146:         )",
          "148:         request = {\"parent\": TEST_PROPERTY_NAME, \"data_stream\": mock_data_stream}",
          "149:         mock_create_data_stream.assert_called_once_with(",
          "150:             request=request, retry=mock_retry, timeout=mock_timeout, metadata=mock_metadata",
          "151:         )",
          "152:         assert data_stream_created == data_stream_expected",
          "154:     @mock.patch(f\"{ANALYTICS_HOOK_PATH}.GoogleAnalyticsAdminHook.get_conn\")",
          "155:     def test_delete_data_stream(self, mock_get_conn):",
          "156:         mock_retry, mock_timeout, mock_metadata = (mock.MagicMock() for _ in range(3))",
          "158:         self.hook.delete_data_stream(",
          "159:             property_id=TEST_PROPERTY_ID,",
          "160:             data_stream_id=TEST_DATASTREAM_ID,",
          "161:             retry=mock_retry,",
          "162:             timeout=mock_timeout,",
          "163:             metadata=mock_metadata,",
          "164:         )",
          "166:         request = {\"name\": TEST_DATASTREAM_NAME}",
          "167:         mock_get_conn.return_value.delete_data_stream.assert_called_once_with(",
          "168:             request=request,",
          "169:             retry=mock_retry,",
          "170:             timeout=mock_timeout,",
          "171:             metadata=mock_metadata,",
          "172:         )",
          "174:     @mock.patch(f\"{ANALYTICS_HOOK_PATH}.GoogleAnalyticsAdminHook.get_conn\")",
          "175:     def test_list_ads_links(self, mock_get_conn):",
          "176:         mock_page_size, mock_page_token, mock_retry, mock_timeout, mock_metadata = (",
          "177:             mock.MagicMock() for _ in range(5)",
          "178:         )",
          "180:         self.hook.list_google_ads_links(",
          "181:             property_id=TEST_PROPERTY_ID,",
          "182:             page_size=mock_page_size,",
          "183:             page_token=mock_page_token,",
          "184:             retry=mock_retry,",
          "185:             timeout=mock_timeout,",
          "186:             metadata=mock_metadata,",
          "187:         )",
          "189:         request = {\"parent\": TEST_PROPERTY_NAME, \"page_size\": mock_page_size, \"page_token\": mock_page_token}",
          "190:         mock_get_conn.return_value.list_google_ads_links.assert_called_once_with(",
          "191:             request=request, retry=mock_retry, timeout=mock_timeout, metadata=mock_metadata",
          "192:         )",
          "",
          "---------------"
        ],
        "tests/providers/google/marketing_platform/links/__init__.py||tests/providers/google/marketing_platform/links/__init__.py": [
          "File: tests/providers/google/marketing_platform/links/__init__.py -> tests/providers/google/marketing_platform/links/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "",
          "---------------"
        ],
        "tests/providers/google/marketing_platform/links/test_analytics_admin.py||tests/providers/google/marketing_platform/links/test_analytics_admin.py": [
          "File: tests/providers/google/marketing_platform/links/test_analytics_admin.py -> tests/providers/google/marketing_platform/links/test_analytics_admin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: from unittest import mock",
          "22: from airflow.providers.google.marketing_platform.links.analytics_admin import (",
          "23:     BASE_LINK,",
          "24:     GoogleAnalyticsPropertyLink,",
          "25: )",
          "27: TEST_PROPERTY_ID = \"123456789\"",
          "28: TEST_PROJECT_ID = \"test_project\"",
          "29: TEST_CONF_GOOGLE_ADS_LINK = {\"property_id\": TEST_PROJECT_ID}",
          "30: ANALYTICS_LINKS_PATH = \"airflow.providers.google.marketing_platform.links.analytics_admin\"",
          "33: class TestGoogleAnalyticsPropertyLink:",
          "34:     @mock.patch(f\"{ANALYTICS_LINKS_PATH}.XCom\")",
          "35:     def test_get_link(self, mock_xcom):",
          "36:         mock_ti_key = mock.MagicMock()",
          "37:         mock_xcom.get_value.return_value = TEST_CONF_GOOGLE_ADS_LINK",
          "38:         url_expected = f\"{BASE_LINK}#/p{TEST_PROJECT_ID}/\"",
          "40:         link = GoogleAnalyticsPropertyLink()",
          "41:         url = link.get_link(operator=mock.MagicMock(), ti_key=mock_ti_key)",
          "43:         mock_xcom.get_value.assert_called_once_with(key=link.key, ti_key=mock_ti_key)",
          "44:         assert url == url_expected",
          "46:     @mock.patch(f\"{ANALYTICS_LINKS_PATH}.XCom\")",
          "47:     def test_get_link_not_found(self, mock_xcom):",
          "48:         mock_ti_key = mock.MagicMock()",
          "49:         mock_xcom.get_value.return_value = None",
          "51:         link = GoogleAnalyticsPropertyLink()",
          "52:         url = link.get_link(operator=mock.MagicMock(), ti_key=mock_ti_key)",
          "54:         mock_xcom.get_value.assert_called_once_with(key=link.key, ti_key=mock_ti_key)",
          "55:         assert url == \"\"",
          "57:     def test_persist(self):",
          "58:         mock_context = mock.MagicMock()",
          "59:         mock_task_instance = mock.MagicMock()",
          "61:         GoogleAnalyticsPropertyLink.persist(",
          "62:             context=mock_context,",
          "63:             task_instance=mock_task_instance,",
          "64:             property_id=TEST_PROPERTY_ID,",
          "65:         )",
          "67:         mock_task_instance.xcom_push.assert_called_once_with(",
          "68:             mock_context,",
          "69:             key=GoogleAnalyticsPropertyLink.key,",
          "70:             value={\"property_id\": TEST_PROPERTY_ID},",
          "71:         )",
          "",
          "---------------"
        ],
        "tests/providers/google/marketing_platform/operators/test_analytics_admin.py||tests/providers/google/marketing_platform/operators/test_analytics_admin.py": [
          "File: tests/providers/google/marketing_platform/operators/test_analytics_admin.py -> tests/providers/google/marketing_platform/operators/test_analytics_admin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "19: from unittest import mock",
          "21: import pytest",
          "23: from airflow.exceptions import AirflowNotFoundException",
          "24: from airflow.providers.google.marketing_platform.operators.analytics_admin import (",
          "25:     GoogleAnalyticsAdminCreateDataStreamOperator,",
          "26:     GoogleAnalyticsAdminCreatePropertyOperator,",
          "27:     GoogleAnalyticsAdminDeleteDataStreamOperator,",
          "28:     GoogleAnalyticsAdminDeletePropertyOperator,",
          "29:     GoogleAnalyticsAdminGetGoogleAdsLinkOperator,",
          "30:     GoogleAnalyticsAdminListAccountsOperator,",
          "31:     GoogleAnalyticsAdminListGoogleAdsLinksOperator,",
          "32: )",
          "34: GCP_CONN_ID = \"google_cloud_default\"",
          "35: IMPERSONATION_CHAIN = [\"ACCOUNT_1\", \"ACCOUNT_2\", \"ACCOUNT_3\"]",
          "36: TEST_GA_GOOGLE_ADS_PROPERTY_ID = \"123456789\"",
          "37: TEST_GA_GOOGLE_ADS_LINK_ID = \"987654321\"",
          "38: TEST_GA_GOOGLE_ADS_LINK_NAME = (",
          "39:     f\"properties/{TEST_GA_GOOGLE_ADS_PROPERTY_ID}/googleAdsLinks/{TEST_GA_GOOGLE_ADS_LINK_ID}\"",
          "40: )",
          "41: TEST_PROPERTY_ID = \"123456789\"",
          "42: TEST_PROPERTY_NAME = f\"properties/{TEST_PROPERTY_ID}\"",
          "43: TEST_DATASTREAM_ID = \"987654321\"",
          "44: TEST_DATASTREAM_NAME = f\"properties/{TEST_PROPERTY_ID}/dataStreams/{TEST_DATASTREAM_ID}\"",
          "45: ANALYTICS_PATH = \"airflow.providers.google.marketing_platform.operators.analytics_admin\"",
          "48: class TestGoogleAnalyticsAdminListAccountsOperator:",
          "49:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAnalyticsAdminHook\")",
          "50:     @mock.patch(f\"{ANALYTICS_PATH}.Account.to_dict\")",
          "51:     def test_execute(self, account_to_dict_mock, hook_mock):",
          "52:         list_accounts_returned = (mock.MagicMock(), mock.MagicMock(), mock.MagicMock())",
          "53:         hook_mock.return_value.list_accounts.return_value = list_accounts_returned",
          "55:         list_accounts_serialized = [mock.MagicMock(), mock.MagicMock(), mock.MagicMock()]",
          "56:         account_to_dict_mock.side_effect = list_accounts_serialized",
          "58:         mock_page_size, mock_page_token, mock_show_deleted, mock_retry, mock_timeout, mock_metadata = (",
          "59:             mock.MagicMock() for _ in range(6)",
          "60:         )",
          "62:         retrieved_accounts_list = GoogleAnalyticsAdminListAccountsOperator(",
          "63:             task_id=\"test_task\",",
          "64:             page_size=mock_page_size,",
          "65:             page_token=mock_page_token,",
          "66:             show_deleted=mock_show_deleted,",
          "67:             retry=mock_retry,",
          "68:             timeout=mock_timeout,",
          "69:             metadata=mock_metadata,",
          "70:             gcp_conn_id=GCP_CONN_ID,",
          "71:             impersonation_chain=IMPERSONATION_CHAIN,",
          "72:         ).execute(context=None)",
          "74:         hook_mock.assert_called_once()",
          "75:         hook_mock.return_value.list_accounts.assert_called_once_with(",
          "76:             page_size=mock_page_size,",
          "77:             page_token=mock_page_token,",
          "78:             show_deleted=mock_show_deleted,",
          "79:             retry=mock_retry,",
          "80:             timeout=mock_timeout,",
          "81:             metadata=mock_metadata,",
          "82:         )",
          "83:         account_to_dict_mock.assert_has_calls([mock.call(item) for item in list_accounts_returned])",
          "84:         assert retrieved_accounts_list == list_accounts_serialized",
          "87: class TestGoogleAnalyticsAdminCreatePropertyOperator:",
          "88:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAnalyticsPropertyLink\")",
          "89:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAnalyticsAdminHook\")",
          "90:     @mock.patch(f\"{ANALYTICS_PATH}.Property.to_dict\")",
          "91:     def test_execute(self, property_to_dict_mock, hook_mock, _):",
          "92:         property_returned = mock.MagicMock()",
          "93:         hook_mock.return_value.create_property.return_value = property_returned",
          "95:         property_serialized = mock.MagicMock()",
          "96:         property_to_dict_mock.return_value = property_serialized",
          "98:         mock_property, mock_retry, mock_timeout, mock_metadata = (mock.MagicMock() for _ in range(4))",
          "100:         property_created = GoogleAnalyticsAdminCreatePropertyOperator(",
          "101:             task_id=\"test_task\",",
          "102:             analytics_property=mock_property,",
          "103:             retry=mock_retry,",
          "104:             timeout=mock_timeout,",
          "105:             metadata=mock_metadata,",
          "106:             gcp_conn_id=GCP_CONN_ID,",
          "107:             impersonation_chain=IMPERSONATION_CHAIN,",
          "108:         ).execute(context=None)",
          "110:         hook_mock.assert_called_once()",
          "111:         hook_mock.return_value.create_property.assert_called_once_with(",
          "112:             analytics_property=mock_property,",
          "113:             retry=mock_retry,",
          "114:             timeout=mock_timeout,",
          "115:             metadata=mock_metadata,",
          "116:         )",
          "117:         property_to_dict_mock.assert_called_once_with(property_returned)",
          "118:         assert property_created == property_serialized",
          "121: class TestGoogleAnalyticsAdminDeletePropertyOperator:",
          "122:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAnalyticsAdminHook\")",
          "123:     @mock.patch(f\"{ANALYTICS_PATH}.Property.to_dict\")",
          "124:     def test_execute(self, property_to_dict_mock, hook_mock):",
          "125:         property_returned = mock.MagicMock()",
          "126:         hook_mock.return_value.delete_property.return_value = property_returned",
          "128:         property_serialized = mock.MagicMock()",
          "129:         property_to_dict_mock.return_value = property_serialized",
          "131:         mock_retry, mock_timeout, mock_metadata = (mock.MagicMock() for _ in range(3))",
          "133:         property_deleted = GoogleAnalyticsAdminDeletePropertyOperator(",
          "134:             task_id=\"test_task\",",
          "135:             gcp_conn_id=GCP_CONN_ID,",
          "136:             impersonation_chain=IMPERSONATION_CHAIN,",
          "137:             property_id=TEST_PROPERTY_ID,",
          "138:             retry=mock_retry,",
          "139:             timeout=mock_timeout,",
          "140:             metadata=mock_metadata,",
          "141:         ).execute(context=None)",
          "143:         hook_mock.assert_called_once()",
          "144:         hook_mock.return_value.delete_property.assert_called_once_with(",
          "145:             property_id=TEST_PROPERTY_ID,",
          "146:             retry=mock_retry,",
          "147:             timeout=mock_timeout,",
          "148:             metadata=mock_metadata,",
          "149:         )",
          "150:         property_to_dict_mock.assert_called_once_with(property_returned)",
          "151:         assert property_deleted == property_serialized",
          "154: class TestGoogleAnalyticsAdminCreateDataStreamOperator:",
          "155:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAnalyticsAdminHook\")",
          "156:     @mock.patch(f\"{ANALYTICS_PATH}.DataStream.to_dict\")",
          "157:     def test_execute(self, data_stream_to_dict_mock, hook_mock):",
          "158:         data_stream_returned = mock.MagicMock()",
          "159:         hook_mock.return_value.create_data_stream.return_value = data_stream_returned",
          "161:         data_stream_serialized = mock.MagicMock()",
          "162:         data_stream_to_dict_mock.return_value = data_stream_serialized",
          "164:         mock_parent, mock_data_stream, mock_retry, mock_timeout, mock_metadata = (",
          "165:             mock.MagicMock() for _ in range(5)",
          "166:         )",
          "168:         data_stream_created = GoogleAnalyticsAdminCreateDataStreamOperator(",
          "169:             task_id=\"test_task\",",
          "170:             property_id=TEST_PROPERTY_ID,",
          "171:             data_stream=mock_data_stream,",
          "172:             retry=mock_retry,",
          "173:             timeout=mock_timeout,",
          "174:             metadata=mock_metadata,",
          "175:             gcp_conn_id=GCP_CONN_ID,",
          "176:             impersonation_chain=IMPERSONATION_CHAIN,",
          "177:         ).execute(context=None)",
          "179:         hook_mock.assert_called_once()",
          "180:         hook_mock.return_value.create_data_stream.assert_called_once_with(",
          "181:             property_id=TEST_PROPERTY_ID,",
          "182:             data_stream=mock_data_stream,",
          "183:             retry=mock_retry,",
          "184:             timeout=mock_timeout,",
          "185:             metadata=mock_metadata,",
          "186:         )",
          "187:         data_stream_to_dict_mock.assert_called_once_with(data_stream_returned)",
          "188:         assert data_stream_created == data_stream_serialized",
          "191: class TestGoogleAnalyticsAdminDeleteDataStreamOperator:",
          "192:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAnalyticsAdminHook\")",
          "193:     def test_execute(self, hook_mock):",
          "194:         mock_retry, mock_timeout, mock_metadata = (mock.MagicMock() for _ in range(3))",
          "196:         GoogleAnalyticsAdminDeleteDataStreamOperator(",
          "197:             task_id=\"test_task\",",
          "198:             gcp_conn_id=GCP_CONN_ID,",
          "199:             impersonation_chain=IMPERSONATION_CHAIN,",
          "200:             property_id=TEST_PROPERTY_ID,",
          "201:             data_stream_id=TEST_DATASTREAM_ID,",
          "202:             retry=mock_retry,",
          "203:             timeout=mock_timeout,",
          "204:             metadata=mock_metadata,",
          "205:         ).execute(context=None)",
          "207:         hook_mock.assert_called_once()",
          "208:         hook_mock.return_value.delete_data_stream.assert_called_once_with(",
          "209:             property_id=TEST_PROPERTY_ID,",
          "210:             data_stream_id=TEST_DATASTREAM_ID,",
          "211:             retry=mock_retry,",
          "212:             timeout=mock_timeout,",
          "213:             metadata=mock_metadata,",
          "214:         )",
          "217: class TestGoogleAnalyticsAdminListGoogleAdsLinksOperator:",
          "218:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAnalyticsAdminHook\")",
          "219:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAdsLink.to_dict\")",
          "220:     def test_execute(self, ads_link_to_dict_mock, hook_mock):",
          "221:         list_ads_links_returned = (mock.MagicMock(), mock.MagicMock(), mock.MagicMock())",
          "222:         hook_mock.return_value.list_google_ads_links.return_value = list_ads_links_returned",
          "224:         list_ads_links_serialized = [mock.MagicMock(), mock.MagicMock(), mock.MagicMock()]",
          "225:         ads_link_to_dict_mock.side_effect = list_ads_links_serialized",
          "227:         mock_page_size, mock_page_token, mock_show_deleted, mock_retry, mock_timeout, mock_metadata = (",
          "228:             mock.MagicMock() for _ in range(6)",
          "229:         )",
          "231:         retrieved_ads_links = GoogleAnalyticsAdminListGoogleAdsLinksOperator(",
          "232:             task_id=\"test_task\",",
          "233:             property_id=TEST_PROPERTY_ID,",
          "234:             page_size=mock_page_size,",
          "235:             page_token=mock_page_token,",
          "236:             retry=mock_retry,",
          "237:             timeout=mock_timeout,",
          "238:             metadata=mock_metadata,",
          "239:             gcp_conn_id=GCP_CONN_ID,",
          "240:             impersonation_chain=IMPERSONATION_CHAIN,",
          "241:         ).execute(context=None)",
          "243:         hook_mock.assert_called_once()",
          "244:         hook_mock.return_value.list_google_ads_links.assert_called_once_with(",
          "245:             property_id=TEST_PROPERTY_ID,",
          "246:             page_size=mock_page_size,",
          "247:             page_token=mock_page_token,",
          "248:             retry=mock_retry,",
          "249:             timeout=mock_timeout,",
          "250:             metadata=mock_metadata,",
          "251:         )",
          "252:         ads_link_to_dict_mock.assert_has_calls([mock.call(item) for item in list_ads_links_returned])",
          "253:         assert retrieved_ads_links == list_ads_links_serialized",
          "256: class TestGoogleAnalyticsAdminGetGoogleAdsLinkOperator:",
          "257:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAnalyticsAdminHook\")",
          "258:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAdsLink\")",
          "259:     def test_execute(self, mock_google_ads_link, hook_mock):",
          "260:         mock_ad_link = mock.MagicMock()",
          "261:         mock_ad_link.name = TEST_GA_GOOGLE_ADS_LINK_NAME",
          "262:         list_ads_links = hook_mock.return_value.list_google_ads_links",
          "263:         list_ads_links.return_value = [mock_ad_link]",
          "264:         mock_retry, mock_timeout, mock_metadata = (mock.MagicMock() for _ in range(3))",
          "266:         GoogleAnalyticsAdminGetGoogleAdsLinkOperator(",
          "267:             task_id=\"test_task\",",
          "268:             property_id=TEST_GA_GOOGLE_ADS_PROPERTY_ID,",
          "269:             google_ads_link_id=TEST_GA_GOOGLE_ADS_LINK_ID,",
          "270:             gcp_conn_id=GCP_CONN_ID,",
          "271:             impersonation_chain=IMPERSONATION_CHAIN,",
          "272:             retry=mock_retry,",
          "273:             timeout=mock_timeout,",
          "274:             metadata=mock_metadata,",
          "275:         ).execute(context=None)",
          "277:         hook_mock.assert_called_once()",
          "278:         hook_mock.return_value.list_google_ads_links.assert_called_once_with(",
          "279:             property_id=TEST_PROPERTY_ID,",
          "280:             retry=mock_retry,",
          "281:             timeout=mock_timeout,",
          "282:             metadata=mock_metadata,",
          "283:         )",
          "284:         mock_google_ads_link.to_dict.assert_called_once_with(mock_ad_link)",
          "286:     @mock.patch(f\"{ANALYTICS_PATH}.GoogleAnalyticsAdminHook\")",
          "287:     def test_execute_not_found(self, hook_mock):",
          "288:         list_ads_links = hook_mock.return_value.list_google_ads_links",
          "289:         list_ads_links.return_value = []",
          "290:         mock_retry, mock_timeout, mock_metadata = (mock.MagicMock() for _ in range(3))",
          "292:         with pytest.raises(AirflowNotFoundException):",
          "293:             GoogleAnalyticsAdminGetGoogleAdsLinkOperator(",
          "294:                 task_id=\"test_task\",",
          "295:                 gcp_conn_id=GCP_CONN_ID,",
          "296:                 impersonation_chain=IMPERSONATION_CHAIN,",
          "297:                 property_id=TEST_GA_GOOGLE_ADS_PROPERTY_ID,",
          "298:                 google_ads_link_id=TEST_GA_GOOGLE_ADS_LINK_ID,",
          "299:                 retry=mock_retry,",
          "300:                 timeout=mock_timeout,",
          "301:                 metadata=mock_metadata,",
          "302:             ).execute(context=None)",
          "304:         hook_mock.assert_called_once()",
          "305:         hook_mock.return_value.list_google_ads_links.assert_called_once_with(",
          "306:             property_id=TEST_PROPERTY_ID,",
          "307:             retry=mock_retry,",
          "308:             timeout=mock_timeout,",
          "309:             metadata=mock_metadata,",
          "310:         )",
          "",
          "---------------"
        ],
        "tests/system/providers/google/marketing_platform/example_analytics_admin.py||tests/system/providers/google/marketing_platform/example_analytics_admin.py": [
          "File: tests/system/providers/google/marketing_platform/example_analytics_admin.py -> tests/system/providers/google/marketing_platform/example_analytics_admin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "17: \"\"\"",
          "18: Example Airflow DAG that shows how to use Google Analytics (GA4) Admin Operators.",
          "20: This DAG relies on the following OS environment variables",
          "25: In order to run this test, make sure you followed steps:",
          "26: 1. Login to https://analytics.google.com",
          "27: 2. In the settings section create an account and save its ID in the variable GA_ACCOUNT_ID.",
          "28: 3. In the settings section go to the Property access management page and add your service account email with",
          "29: Editor permissions. This service account should be created on behalf of the account from the step 1.",
          "30: 4. Make sure Google Analytics Admin API is enabled in your GCP project.",
          "31: 5. Create Google Ads account and link it to your Google Analytics account in the GA admin panel.",
          "32: 6. Associate the Google Ads account with a property, and save this property's id in the variable",
          "33: GA_GOOGLE_ADS_PROPERTY_ID.",
          "34: \"\"\"",
          "35: from __future__ import annotations",
          "37: import json",
          "38: import logging",
          "39: import os",
          "40: from datetime import datetime",
          "42: from google.analytics import admin_v1beta as google_analytics",
          "44: from airflow.decorators import task",
          "45: from airflow.models import Connection",
          "46: from airflow.models.dag import DAG",
          "47: from airflow.operators.bash import BashOperator",
          "48: from airflow.providers.google.marketing_platform.operators.analytics_admin import (",
          "49:     GoogleAnalyticsAdminCreateDataStreamOperator,",
          "50:     GoogleAnalyticsAdminCreatePropertyOperator,",
          "51:     GoogleAnalyticsAdminDeleteDataStreamOperator,",
          "52:     GoogleAnalyticsAdminDeletePropertyOperator,",
          "53:     GoogleAnalyticsAdminGetGoogleAdsLinkOperator,",
          "54:     GoogleAnalyticsAdminListAccountsOperator,",
          "55:     GoogleAnalyticsAdminListGoogleAdsLinksOperator,",
          "56: )",
          "57: from airflow.settings import Session",
          "58: from airflow.utils.trigger_rule import TriggerRule",
          "60: ENV_ID = os.environ.get(\"SYSTEM_TESTS_ENV_ID\")",
          "61: DAG_ID = \"example_google_analytics_admin\"",
          "63: CONNECTION_ID = f\"connection_{DAG_ID}_{ENV_ID}\"",
          "64: ACCOUNT_ID = os.environ.get(\"GA_ACCOUNT_ID\", \"123456789\")",
          "65: PROPERTY_ID = \"{{ task_instance.xcom_pull('create_property')['name'].split('/')[-1] }}\"",
          "66: DATA_STREAM_ID = \"{{ task_instance.xcom_pull('create_data_stream')['name'].split('/')[-1] }}\"",
          "67: GA_GOOGLE_ADS_PROPERTY_ID = os.environ.get(\"GA_GOOGLE_ADS_PROPERTY_ID\", \"123456789\")",
          "68: GA_ADS_LINK_ID = \"{{ task_instance.xcom_pull('list_google_ads_links')[0]['name'].split('/')[-1] }}\"",
          "70: log = logging.getLogger(__name__)",
          "72: with DAG(",
          "73:     DAG_ID,",
          "74:     schedule=\"@once\",  # Override to match your needs,",
          "75:     start_date=datetime(2021, 1, 1),",
          "76:     catchup=False,",
          "77:     tags=[\"example\", \"analytics\"],",
          "78: ) as dag:",
          "80:     @task",
          "81:     def setup_connection(**kwargs) -> None:",
          "82:         connection = Connection(",
          "83:             conn_id=CONNECTION_ID,",
          "84:             conn_type=\"google_cloud_platform\",",
          "85:         )",
          "86:         conn_extra_json = json.dumps(",
          "87:             {",
          "88:                 \"scope\": \"https://www.googleapis.com/auth/analytics.edit,\"",
          "89:                 \"https://www.googleapis.com/auth/analytics.readonly\",",
          "90:             }",
          "91:         )",
          "92:         connection.set_extra(conn_extra_json)",
          "94:         session = Session()",
          "95:         if session.query(Connection).filter(Connection.conn_id == CONNECTION_ID).first():",
          "96:             log.warning(\"Connection %s already exists\", CONNECTION_ID)",
          "97:             return None",
          "99:         session.add(connection)",
          "100:         session.commit()",
          "102:     setup_connection_task = setup_connection()",
          "104:     # [START howto_marketing_platform_list_accounts_operator]",
          "105:     list_accounts = GoogleAnalyticsAdminListAccountsOperator(",
          "106:         task_id=\"list_account\",",
          "107:         gcp_conn_id=CONNECTION_ID,",
          "108:         show_deleted=True,",
          "109:     )",
          "110:     # [END howto_marketing_platform_list_accounts_operator]",
          "112:     # [START howto_marketing_platform_create_property_operator]",
          "113:     create_property = GoogleAnalyticsAdminCreatePropertyOperator(",
          "114:         task_id=\"create_property\",",
          "115:         analytics_property={",
          "116:             \"parent\": f\"accounts/{ACCOUNT_ID}\",",
          "117:             \"display_name\": \"Test display name\",",
          "118:             \"time_zone\": \"America/Los_Angeles\",",
          "119:         },",
          "120:         gcp_conn_id=CONNECTION_ID,",
          "121:     )",
          "122:     # [END howto_marketing_platform_create_property_operator]",
          "124:     # [START howto_marketing_platform_create_data_stream_operator]",
          "125:     create_data_stream = GoogleAnalyticsAdminCreateDataStreamOperator(",
          "126:         task_id=\"create_data_stream\",",
          "127:         property_id=PROPERTY_ID,",
          "128:         data_stream={",
          "129:             \"display_name\": \"Test data stream\",",
          "130:             \"web_stream_data\": {",
          "131:                 \"default_uri\": \"www.example.com\",",
          "132:             },",
          "133:             \"type_\": google_analytics.DataStream.DataStreamType.WEB_DATA_STREAM,",
          "134:         },",
          "135:         gcp_conn_id=CONNECTION_ID,",
          "136:     )",
          "137:     # [END howto_marketing_platform_create_data_stream_operator]",
          "139:     # [START howto_marketing_platform_delete_data_stream_operator]",
          "140:     delete_data_stream = GoogleAnalyticsAdminDeleteDataStreamOperator(",
          "141:         task_id=\"delete_datastream\",",
          "142:         property_id=PROPERTY_ID,",
          "143:         data_stream_id=DATA_STREAM_ID,",
          "144:         gcp_conn_id=CONNECTION_ID,",
          "145:     )",
          "146:     # [END howto_marketing_platform_delete_data_stream_operator]",
          "148:     # [START howto_marketing_platform_delete_property_operator]",
          "149:     delete_property = GoogleAnalyticsAdminDeletePropertyOperator(",
          "150:         task_id=\"delete_property\",",
          "151:         property_id=PROPERTY_ID,",
          "152:         gcp_conn_id=CONNECTION_ID,",
          "153:     )",
          "154:     # [END howto_marketing_platform_delete_property_operator]",
          "155:     delete_property.trigger_rule = TriggerRule.ALL_DONE",
          "157:     # [START howto_marketing_platform_list_google_ads_links]",
          "158:     list_google_ads_links = GoogleAnalyticsAdminListGoogleAdsLinksOperator(",
          "159:         task_id=\"list_google_ads_links\",",
          "160:         property_id=GA_GOOGLE_ADS_PROPERTY_ID,",
          "161:         gcp_conn_id=CONNECTION_ID,",
          "162:     )",
          "163:     # [END howto_marketing_platform_list_google_ads_links]",
          "165:     # [START howto_marketing_platform_get_google_ad_link]",
          "166:     get_ad_link = GoogleAnalyticsAdminGetGoogleAdsLinkOperator(",
          "167:         task_id=\"get_ad_link\",",
          "168:         property_id=GA_GOOGLE_ADS_PROPERTY_ID,",
          "169:         google_ads_link_id=GA_ADS_LINK_ID,",
          "170:         gcp_conn_id=CONNECTION_ID,",
          "171:     )",
          "172:     # [END howto_marketing_platform_get_google_ad_link]",
          "174:     delete_connection = BashOperator(",
          "175:         task_id=\"delete_connection\",",
          "176:         bash_command=f\"airflow connections delete {CONNECTION_ID}\",",
          "177:         trigger_rule=TriggerRule.ALL_DONE,",
          "178:     )",
          "180:     (",
          "181:         # TEST SETUP",
          "182:         setup_connection_task",
          "183:         # TEST BODY",
          "184:         >> list_accounts",
          "185:         >> create_property",
          "186:         >> create_data_stream",
          "187:         >> delete_data_stream",
          "188:         >> delete_property",
          "189:         >> list_google_ads_links",
          "190:         >> get_ad_link",
          "191:         # TEST TEARDOWN",
          "192:         >> delete_connection",
          "193:     )",
          "194:     from tests.system.utils.watcher import watcher",
          "196:     # This test needs watcher in order to properly mark success/failure",
          "197:     # when \"tearDown\" task with trigger rule is part of the DAG",
          "198:     list(dag.tasks) >> watcher()",
          "200: from tests.system.utils import get_test_run  # noqa: E402",
          "202: # Needed to run the example DAG with pytest (see: tests/system/README.md#run_via_pytest)",
          "203: test_run = get_test_run(dag)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "408e2d2fe7578030bb3ca9df7f2ace4b654d67f8",
      "candidate_info": {
        "commit_hash": "408e2d2fe7578030bb3ca9df7f2ace4b654d67f8",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/408e2d2fe7578030bb3ca9df7f2ace4b654d67f8",
        "files": [
          "setup.cfg"
        ],
        "message": "Ignore MyPy error introduced by the new Apache Beam (#36607)\n\nThe new Apache Beam 2.53.0 introduced a MyPy error that breaks\nour MyPy checks.\n\nThis is captured in https://github.com/apache/beam/issues/29927\nbut until it is addressed we need to ignore it.\n\n(cherry picked from commit 0c10ddb3c6e9d8cbc1592d1e0bf5532c5ed4dfa9)",
        "before_after_code_files": [
          "setup.cfg||setup.cfg"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.cfg||setup.cfg": [
          "File: setup.cfg -> setup.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "236: [mypy-referencing.*]",
          "237: # Referencing has some old type annotations that are not compatible with new versions of mypy",
          "238: ignore_errors = True",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "241: [mypy-apache_beam.*]",
          "242: # Beam has some old type annotations and they introduced an error recently with bad signature of",
          "243: # a function. This is captured in https://github.com/apache/beam/issues/29927",
          "244: # and we should remove this exclusion when it is fixed.",
          "245: ignore_errors = True",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b71d3882e944eecd64b5c4bf5fb42a3707496387",
      "candidate_info": {
        "commit_hash": "b71d3882e944eecd64b5c4bf5fb42a3707496387",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b71d3882e944eecd64b5c4bf5fb42a3707496387",
        "files": [
          "airflow/utils/event_scheduler.py"
        ],
        "message": "Avoid using dict as default value in call_regular_interval (#36608)\n\n(cherry picked from commit 9f275cf8900baac4e66eab3c5e8a1462a22f5ccc)",
        "before_after_code_files": [
          "airflow/utils/event_scheduler.py||airflow/utils/event_scheduler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/event_scheduler.py||airflow/utils/event_scheduler.py": [
          "File: airflow/utils/event_scheduler.py -> airflow/utils/event_scheduler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31:         delay: float,",
          "32:         action: Callable,",
          "33:         arguments=(),",
          "35:     ):",
          "36:         \"\"\"Call a function at (roughly) a given interval.\"\"\"",
          "",
          "[Removed Lines]",
          "34:         kwargs={},",
          "",
          "[Added Lines]",
          "34:         kwargs=None,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "43:             # Good enough for now",
          "44:             self.enter(delay, 1, repeat, args, kwargs)",
          "",
          "[Removed Lines]",
          "46:         self.enter(delay, 1, repeat, arguments, kwargs)",
          "",
          "[Added Lines]",
          "46:         self.enter(delay, 1, repeat, arguments, kwargs or {})",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a51278d585ae7558651d444ec5c7e36a76204c7c",
      "candidate_info": {
        "commit_hash": "a51278d585ae7558651d444ec5c7e36a76204c7c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a51278d585ae7558651d444ec5c7e36a76204c7c",
        "files": [
          "docs/apache-airflow/privacy_notice.rst"
        ],
        "message": "Link to Apache Software Foundation Privacy Policy. (#36394)\n\nSince we are following the privacy Policy of the ASF, we can link\nto the policyu (and remove out-dated information about Google\nAnalytics).\n\nFixes: https://github.com/apache/airflow-site/issues/917\n(cherry picked from commit b30a1b854c4d682c7fdaf156d7f08faeadf7afd9)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "bbf1fbdc79985ae41421edaaf7e564ad4b181eb5",
      "candidate_info": {
        "commit_hash": "bbf1fbdc79985ae41421edaaf7e564ad4b181eb5",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/bbf1fbdc79985ae41421edaaf7e564ad4b181eb5",
        "files": [
          "docs/apache-airflow/best-practices.rst"
        ],
        "message": "Replace numpy example with practical exercise demonstrating top-level code (#35097)\n\n* Replace numpy example with a practical exercise demonstrating top-level code\n\n(cherry picked from commit ba20baeafd5e28c164c37a837337b501bf8cde3f)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "4afcb7fda79c03eb66ed3e7188c9646a77ce7f2b",
      "candidate_info": {
        "commit_hash": "4afcb7fda79c03eb66ed3e7188c9646a77ce7f2b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4afcb7fda79c03eb66ed3e7188c9646a77ce7f2b",
        "files": [
          "airflow/models/dagrun.py"
        ],
        "message": "Remove dot value (#36712)\n\nThe update_state function in the DagRun class fails as ever since an update was made to Airflow, the state of the DagRun is returned as a string instead of an actual DagRunState. However, should this be fixed in the future, this change would still not affect anything since the state is called as a string here, which would return the value without needing to specify \".value\" anyway.\n\nCo-authored-by: fuat.cakici <fuat.cakici@knab.nl>\n(cherry picked from commit dfa695a48ec53f97b9443205d00d06ce0c67271d)",
        "before_after_code_files": [
          "airflow/models/dagrun.py||airflow/models/dagrun.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dagrun.py||airflow/models/dagrun.py": [
          "File: airflow/models/dagrun.py -> airflow/models/dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1044:         duration = self.end_date - self.start_date",
          "1045:         timer_params = {\"dt\": duration, \"tags\": self.stats_tags}",
          "1049:     @provide_session",
          "1050:     def verify_integrity(self, *, session: Session = NEW_SESSION) -> None:",
          "",
          "[Removed Lines]",
          "1046:         Stats.timing(f\"dagrun.duration.{self.state.value}.{self.dag_id}\", **timer_params)",
          "1047:         Stats.timing(f\"dagrun.duration.{self.state.value}\", **timer_params)",
          "",
          "[Added Lines]",
          "1046:         Stats.timing(f\"dagrun.duration.{self.state}.{self.dag_id}\", **timer_params)",
          "1047:         Stats.timing(f\"dagrun.duration.{self.state}\", **timer_params)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "dbc4b1fd9b6f53d6c8bc9abbde591e7df3c90536",
      "candidate_info": {
        "commit_hash": "dbc4b1fd9b6f53d6c8bc9abbde591e7df3c90536",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/dbc4b1fd9b6f53d6c8bc9abbde591e7df3c90536",
        "files": [
          ".github/workflows/ci.yml",
          "Dockerfile.ci",
          "airflow/models/dag.py",
          "airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py",
          "airflow/serialization/serialized_objects.py",
          "airflow/serialization/serializers/datetime.py",
          "airflow/serialization/serializers/timezone.py",
          "airflow/settings.py",
          "airflow/timetables/_cron.py",
          "airflow/timetables/trigger.py",
          "airflow/utils/sqlalchemy.py",
          "airflow/utils/timezone.py",
          "dev/breeze/src/airflow_breeze/commands/common_options.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands_config.py",
          "dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "dev/breeze/src/airflow_breeze/commands/testing_commands_config.py",
          "dev/breeze/src/airflow_breeze/params/shell_params.py",
          "images/breeze/output_shell.svg",
          "images/breeze/output_shell.txt",
          "images/breeze/output_testing_db-tests.svg",
          "images/breeze/output_testing_db-tests.txt",
          "images/breeze/output_testing_non-db-tests.svg",
          "images/breeze/output_testing_non-db-tests.txt",
          "images/breeze/output_testing_tests.svg",
          "images/breeze/output_testing_tests.txt",
          "kubernetes_tests/test_kubernetes_pod_operator.py",
          "newsfragments/36281.significant.rst",
          "pyproject.toml",
          "scripts/ci/docker-compose/devcontainer.env",
          "scripts/docker/entrypoint_ci.sh",
          "tests/api_connexion/endpoints/test_dag_endpoint.py",
          "tests/api_connexion/schemas/test_dag_schema.py",
          "tests/cli/commands/test_dag_command.py",
          "tests/models/test_dag.py",
          "tests/providers/openlineage/plugins/test_utils.py",
          "tests/sensors/test_time_sensor.py",
          "tests/serialization/serializers/test_serializers.py",
          "tests/serialization/test_serialized_objects.py",
          "tests/triggers/test_temporal.py",
          "tests/utils/test_timezone.py"
        ],
        "message": "Add support of Pendulum 3 (#36281)\n\n* Add support of Pendulum 3\n\n* Add backcompat to pendulum 2\n\n* Update airflow/serialization/serialized_objects.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n* Add newsfragments\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 2ffa6e4c4c9dc129daa54491d5af8f535cd0d479)",
        "before_after_code_files": [
          "Dockerfile.ci||Dockerfile.ci",
          "airflow/models/dag.py||airflow/models/dag.py",
          "airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py||airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py",
          "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py",
          "airflow/serialization/serializers/datetime.py||airflow/serialization/serializers/datetime.py",
          "airflow/serialization/serializers/timezone.py||airflow/serialization/serializers/timezone.py",
          "airflow/settings.py||airflow/settings.py",
          "airflow/timetables/_cron.py||airflow/timetables/_cron.py",
          "airflow/timetables/trigger.py||airflow/timetables/trigger.py",
          "airflow/utils/sqlalchemy.py||airflow/utils/sqlalchemy.py",
          "airflow/utils/timezone.py||airflow/utils/timezone.py",
          "dev/breeze/src/airflow_breeze/commands/common_options.py||dev/breeze/src/airflow_breeze/commands/common_options.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "dev/breeze/src/airflow_breeze/commands/developer_commands_config.py||dev/breeze/src/airflow_breeze/commands/developer_commands_config.py",
          "dev/breeze/src/airflow_breeze/commands/testing_commands.py||dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "dev/breeze/src/airflow_breeze/commands/testing_commands_config.py||dev/breeze/src/airflow_breeze/commands/testing_commands_config.py",
          "dev/breeze/src/airflow_breeze/params/shell_params.py||dev/breeze/src/airflow_breeze/params/shell_params.py",
          "kubernetes_tests/test_kubernetes_pod_operator.py||kubernetes_tests/test_kubernetes_pod_operator.py",
          "scripts/ci/docker-compose/devcontainer.env||scripts/ci/docker-compose/devcontainer.env",
          "scripts/docker/entrypoint_ci.sh||scripts/docker/entrypoint_ci.sh",
          "tests/api_connexion/endpoints/test_dag_endpoint.py||tests/api_connexion/endpoints/test_dag_endpoint.py",
          "tests/api_connexion/schemas/test_dag_schema.py||tests/api_connexion/schemas/test_dag_schema.py",
          "tests/cli/commands/test_dag_command.py||tests/cli/commands/test_dag_command.py",
          "tests/models/test_dag.py||tests/models/test_dag.py",
          "tests/providers/openlineage/plugins/test_utils.py||tests/providers/openlineage/plugins/test_utils.py",
          "tests/sensors/test_time_sensor.py||tests/sensors/test_time_sensor.py",
          "tests/serialization/serializers/test_serializers.py||tests/serialization/serializers/test_serializers.py",
          "tests/serialization/test_serialized_objects.py||tests/serialization/test_serialized_objects.py",
          "tests/triggers/test_temporal.py||tests/triggers/test_temporal.py",
          "tests/utils/test_timezone.py||tests/utils/test_timezone.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "Dockerfile.ci||Dockerfile.ci": [
          "File: Dockerfile.ci -> Dockerfile.ci",
          "--- Hunk 1 ---",
          "[Context before]",
          "908:     pip check",
          "909: }",
          "911: function check_run_tests() {",
          "912:     if [[ ${RUN_TESTS=} != \"true\" ]]; then",
          "913:         return",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "911: function check_download_pendulum() {",
          "912:     if [[ ${DOWNGRADE_PENDULUM=} != \"true\" ]]; then",
          "913:         return",
          "914:     fi",
          "915:     min_pendulum_version=$(grep \"\\\"pendulum>=\" pyproject.toml | sed \"s/.*>=\\([0-9\\.]*\\).*/\\1/\" | xargs)",
          "916:     echo",
          "917:     echo \"${COLOR_BLUE}Downgrading pendulum to minimum supported version: ${min_pendulum_version}${COLOR_RESET}\"",
          "918:     echo",
          "919:     pip install --root-user-action ignore \"pendulum==${min_pendulum_version}\"",
          "920:     pip check",
          "921: }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "937: environment_initialization",
          "938: check_boto_upgrade",
          "939: check_download_sqlalchemy",
          "940: check_run_tests \"${@}\"",
          "942: exec /bin/bash \"${@}\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "952: check_download_pendulum",
          "",
          "---------------"
        ],
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "138: if TYPE_CHECKING:",
          "139:     from types import ModuleType",
          "142:     from sqlalchemy.orm.query import Query",
          "143:     from sqlalchemy.orm.session import Session",
          "",
          "[Removed Lines]",
          "141:     from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "141:     from pendulum.tz.timezone import FixedTimezone, Timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "213:     return DataInterval(start, end)",
          "217:     \"\"\"Create a Timetable instance from a ``schedule_interval`` argument.\"\"\"",
          "218:     if interval is NOTSET:",
          "219:         return DeltaDataIntervalTimetable(DEFAULT_SCHEDULE_INTERVAL)",
          "",
          "[Removed Lines]",
          "216: def create_timetable(interval: ScheduleIntervalArg, timezone: Timezone) -> Timetable:",
          "",
          "[Added Lines]",
          "216: def create_timetable(interval: ScheduleIntervalArg, timezone: Timezone | FixedTimezone) -> Timetable:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "530:             tzinfo = None if date.tzinfo else settings.TIMEZONE",
          "531:             tz = pendulum.instance(date, tz=tzinfo).timezone",
          "534:         # Apply the timezone we settled on to end_date if it wasn't supplied",
          "535:         if \"end_date\" in self.default_args and self.default_args[\"end_date\"]:",
          "",
          "[Removed Lines]",
          "532:         self.timezone: Timezone = tz or settings.TIMEZONE",
          "",
          "[Added Lines]",
          "532:         self.timezone: Timezone | FixedTimezone = tz or settings.TIMEZONE",
          "",
          "---------------"
        ],
        "airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py||airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py": [
          "File: airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py -> airflow/providers/cncf/kubernetes/pod_launcher_deprecated.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import math",
          "22: import time",
          "23: import warnings",
          "26: import pendulum",
          "27: import tenacity",
          "",
          "[Removed Lines]",
          "24: from typing import TYPE_CHECKING",
          "",
          "[Added Lines]",
          "24: from typing import TYPE_CHECKING, cast",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "148:         \"\"\"",
          "149:         if get_logs:",
          "150:             read_logs_since_sec = None",
          "152:             while True:",
          "153:                 logs = self.read_pod_logs(pod, timestamps=True, since_seconds=read_logs_since_sec)",
          "154:                 for line in logs:",
          "155:                     timestamp, message = self.parse_log_line(line.decode(\"utf-8\"))",
          "156:                     if timestamp:",
          "158:                     self.log.info(message)",
          "159:                 time.sleep(1)",
          "",
          "[Removed Lines]",
          "151:             last_log_time = None",
          "157:                         last_log_time = pendulum.parse(timestamp)",
          "",
          "[Added Lines]",
          "151:             last_log_time: pendulum.DateTime | None = None",
          "157:                         last_log_time = cast(pendulum.DateTime, pendulum.parse(timestamp))",
          "",
          "---------------"
        ],
        "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py": [
          "File: airflow/serialization/serialized_objects.py -> airflow/serialization/serialized_objects.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "65: from airflow.utils.module_loading import import_string, qualname",
          "66: from airflow.utils.operator_resources import Resources",
          "67: from airflow.utils.task_group import MappedTaskGroup, TaskGroup",
          "68: from airflow.utils.types import NOTSET, ArgNotSet",
          "70: if TYPE_CHECKING:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "68: from airflow.utils.timezone import parse_timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "144:     return relativedelta.relativedelta(**var)",
          "148:     \"\"\"",
          "149:     Encode a Pendulum Timezone for serialization.",
          "",
          "[Removed Lines]",
          "147: def encode_timezone(var: Timezone) -> str | int:",
          "",
          "[Added Lines]",
          "148: def encode_timezone(var: Timezone | FixedTimezone) -> str | int:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "167:     )",
          "171:     \"\"\"Decode a previously serialized Pendulum Timezone.\"\"\"",
          "175: def _get_registered_timetable(importable_string: str) -> type[Timetable] | None:",
          "",
          "[Removed Lines]",
          "170: def decode_timezone(var: str | int) -> Timezone:",
          "172:     return pendulum.tz.timezone(var)",
          "",
          "[Added Lines]",
          "171: def decode_timezone(var: str | int) -> Timezone | FixedTimezone:",
          "173:     return parse_timezone(var)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "607:             raise TypeError(f\"Invalid type {type_!s} in deserialization.\")",
          "609:     _deserialize_datetime = pendulum.from_timestamp",
          "612:     @classmethod",
          "613:     def _deserialize_timedelta(cls, seconds: int) -> datetime.timedelta:",
          "",
          "[Removed Lines]",
          "610:     _deserialize_timezone = pendulum.tz.timezone",
          "",
          "[Added Lines]",
          "611:     _deserialize_timezone = parse_timezone",
          "",
          "---------------"
        ],
        "airflow/serialization/serializers/datetime.py||airflow/serialization/serializers/datetime.py": [
          "File: airflow/serialization/serializers/datetime.py -> airflow/serialization/serializers/datetime.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24:     serialize as serialize_timezone,",
          "25: )",
          "26: from airflow.utils.module_loading import qualname",
          "28: if TYPE_CHECKING:",
          "29:     import datetime",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: from airflow.utils.timezone import parse_timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "62:     import datetime",
          "64:     from pendulum import DateTime",
          "67:     tz: datetime.tzinfo | None = None",
          "68:     if isinstance(data, dict) and TIMEZONE in data:",
          "69:         if version == 1:",
          "70:             # try to deserialize unsupported timezones",
          "71:             timezone_mapping = {",
          "77:             }",
          "78:             if data[TIMEZONE] in timezone_mapping:",
          "79:                 tz = timezone_mapping[data[TIMEZONE]]",
          "80:             else:",
          "82:         else:",
          "83:             tz = (",
          "84:                 deserialize_timezone(data[TIMEZONE][1], data[TIMEZONE][2], data[TIMEZONE][0])",
          "",
          "[Removed Lines]",
          "65:     from pendulum.tz import fixed_timezone, timezone",
          "72:                 \"EDT\": fixed_timezone(-4 * 3600),",
          "73:                 \"CDT\": fixed_timezone(-5 * 3600),",
          "74:                 \"MDT\": fixed_timezone(-6 * 3600),",
          "75:                 \"PDT\": fixed_timezone(-7 * 3600),",
          "76:                 \"CEST\": timezone(\"CET\"),",
          "81:                 tz = timezone(data[TIMEZONE])",
          "",
          "[Added Lines]",
          "72:                 \"EDT\": parse_timezone(-4 * 3600),",
          "73:                 \"CDT\": parse_timezone(-5 * 3600),",
          "74:                 \"MDT\": parse_timezone(-6 * 3600),",
          "75:                 \"PDT\": parse_timezone(-7 * 3600),",
          "76:                 \"CEST\": parse_timezone(\"CET\"),",
          "81:                 tz = parse_timezone(data[TIMEZONE])",
          "",
          "---------------"
        ],
        "airflow/serialization/serializers/timezone.py||airflow/serialization/serializers/timezone.py": [
          "File: airflow/serialization/serializers/timezone.py -> airflow/serialization/serializers/timezone.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "76: def deserialize(classname: str, version: int, data: object) -> Any:",
          "79:     if not isinstance(data, (str, int)):",
          "80:         raise TypeError(f\"{data} is not of type int or str but of {type(data)}\")",
          "",
          "[Removed Lines]",
          "77:     from pendulum.tz import fixed_timezone, timezone",
          "",
          "[Added Lines]",
          "77:     from airflow.utils.timezone import parse_timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "82:     if version > __version__:",
          "83:         raise TypeError(f\"serialized {version} of {classname} > {__version__}\")",
          "88:     if \"zoneinfo.ZoneInfo\" in classname:",
          "89:         try:",
          "90:             from zoneinfo import ZoneInfo",
          "",
          "[Removed Lines]",
          "85:     if isinstance(data, int):",
          "86:         return fixed_timezone(data)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "94:         return ZoneInfo(data)",
          "99: # ported from pendulum.tz.timezone._get_tzinfo_name",
          "",
          "[Removed Lines]",
          "96:     return timezone(data)",
          "",
          "[Added Lines]",
          "93:     return parse_timezone(data)",
          "",
          "---------------"
        ],
        "airflow/settings.py||airflow/settings.py": [
          "File: airflow/settings.py -> airflow/settings.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import warnings",
          "27: from typing import TYPE_CHECKING, Any, Callable",
          "30: import pluggy",
          "31: import sqlalchemy",
          "32: from sqlalchemy import create_engine, exc, text",
          "",
          "[Removed Lines]",
          "29: import pendulum",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "40: from airflow.logging_config import configure_logging",
          "41: from airflow.utils.orm_event_handlers import setup_event_handlers",
          "42: from airflow.utils.state import State",
          "44: if TYPE_CHECKING:",
          "45:     from sqlalchemy.engine import Engine",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42: from airflow.utils.timezone import local_timezone, parse_timezone, utc",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "50: log = logging.getLogger(__name__)",
          "52: try:",
          "56:     else:",
          "58: except Exception:",
          "61: log.info(\"Configured default timezone %s\", TIMEZONE)",
          "",
          "[Removed Lines]",
          "53:     tz = conf.get_mandatory_value(\"core\", \"default_timezone\")",
          "54:     if tz == \"system\":",
          "55:         TIMEZONE = pendulum.tz.local_timezone()",
          "57:         TIMEZONE = pendulum.tz.timezone(tz)",
          "59:     TIMEZONE = pendulum.tz.timezone(\"UTC\")",
          "",
          "[Added Lines]",
          "53:     if (tz := conf.get_mandatory_value(\"core\", \"default_timezone\")) != \"system\":",
          "54:         TIMEZONE = parse_timezone(tz)",
          "56:         TIMEZONE = local_timezone()",
          "58:     TIMEZONE = utc",
          "",
          "---------------"
        ],
        "airflow/timetables/_cron.py||airflow/timetables/_cron.py": [
          "File: airflow/timetables/_cron.py -> airflow/timetables/_cron.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: import datetime",
          "20: from typing import TYPE_CHECKING, Any",
          "23: from cron_descriptor import CasingTypeEnum, ExpressionDescriptor, FormatException, MissingFieldException",
          "24: from croniter import CroniterBadCronError, CroniterBadDateError, croniter",
          "26: from airflow.exceptions import AirflowTimetableInvalid",
          "27: from airflow.utils.dates import cron_presets",
          "30: if TYPE_CHECKING:",
          "31:     from pendulum import DateTime",
          "35: def _covers_every_hour(cron: croniter) -> bool:",
          "",
          "[Removed Lines]",
          "22: import pendulum",
          "28: from airflow.utils.timezone import convert_to_utc, make_aware, make_naive",
          "32:     from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "27: from airflow.utils.timezone import convert_to_utc, make_aware, make_naive, parse_timezone",
          "31:     from pendulum.tz.timezone import FixedTimezone, Timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "63: class CronMixin:",
          "64:     \"\"\"Mixin to provide interface to work with croniter.\"\"\"",
          "67:         self._expression = cron_presets.get(cron, cron)",
          "69:         if isinstance(timezone, str):",
          "71:         self._timezone = timezone",
          "73:         try:",
          "",
          "[Removed Lines]",
          "66:     def __init__(self, cron: str, timezone: str | Timezone) -> None:",
          "70:             timezone = pendulum.tz.timezone(timezone)",
          "",
          "[Added Lines]",
          "65:     def __init__(self, cron: str, timezone: str | Timezone | FixedTimezone) -> None:",
          "69:             timezone = parse_timezone(timezone)",
          "",
          "---------------"
        ],
        "airflow/timetables/trigger.py||airflow/timetables/trigger.py": [
          "File: airflow/timetables/trigger.py -> airflow/timetables/trigger.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: if TYPE_CHECKING:",
          "28:     from dateutil.relativedelta import relativedelta",
          "31:     from airflow.timetables.base import TimeRestriction",
          "",
          "[Removed Lines]",
          "29:     from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "29:     from pendulum.tz.timezone import FixedTimezone, Timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "48:         self,",
          "49:         cron: str,",
          "52:         interval: datetime.timedelta | relativedelta = datetime.timedelta(),",
          "53:     ) -> None:",
          "54:         super().__init__(cron, timezone)",
          "",
          "[Removed Lines]",
          "51:         timezone: str | Timezone,",
          "",
          "[Added Lines]",
          "51:         timezone: str | Timezone | FixedTimezone,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "77:         return {\"expression\": self._expression, \"timezone\": timezone, \"interval\": interval}",
          "79:     def infer_manual_data_interval(self, *, run_after: DateTime) -> DataInterval:",
          "82:     def next_dagrun_info(",
          "83:         self,",
          "",
          "[Removed Lines]",
          "80:         return DataInterval(run_after - self._interval, run_after)",
          "",
          "[Added Lines]",
          "80:         return DataInterval(",
          "81:             # pendulum.Datetime \u00b1 timedelta should return pendulum.Datetime",
          "82:             # however mypy decide that output would be datetime.datetime",
          "83:             run_after - self._interval,  # type: ignore[arg-type]",
          "84:             run_after,",
          "85:         )",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "101:             next_start_time = max(start_time_candidates)",
          "102:         if restriction.latest is not None and restriction.latest < next_start_time:",
          "103:             return None",
          "",
          "[Removed Lines]",
          "104:         return DagRunInfo.interval(next_start_time - self._interval, next_start_time)",
          "",
          "[Added Lines]",
          "109:         return DagRunInfo.interval(",
          "110:             # pendulum.Datetime \u00b1 timedelta should return pendulum.Datetime",
          "111:             # however mypy decide that output would be datetime.datetime",
          "112:             next_start_time - self._interval,  # type: ignore[arg-type]",
          "113:             next_start_time,",
          "114:         )",
          "",
          "---------------"
        ],
        "airflow/utils/sqlalchemy.py||airflow/utils/sqlalchemy.py": [
          "File: airflow/utils/sqlalchemy.py -> airflow/utils/sqlalchemy.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import logging",
          "25: from typing import TYPE_CHECKING, Any, Generator, Iterable, overload",
          "28: from dateutil import relativedelta",
          "29: from sqlalchemy import TIMESTAMP, PickleType, and_, event, false, nullsfirst, or_, true, tuple_",
          "30: from sqlalchemy.dialects import mssql, mysql",
          "",
          "[Removed Lines]",
          "27: import pendulum",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "34: from airflow import settings",
          "35: from airflow.configuration import conf",
          "36: from airflow.serialization.enums import Encoding",
          "39: if TYPE_CHECKING:",
          "40:     from kubernetes.client.models.v1_pod import V1Pod",
          "",
          "[Removed Lines]",
          "37: from airflow.utils.timezone import make_naive",
          "",
          "[Added Lines]",
          "36: from airflow.utils.timezone import make_naive, utc",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "47: log = logging.getLogger(__name__)",
          "52: class UtcDateTime(TypeDecorator):",
          "53:     \"\"\"",
          "",
          "[Removed Lines]",
          "49: utc = pendulum.tz.timezone(\"UTC\")",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/utils/timezone.py||airflow/utils/timezone.py": [
          "File: airflow/utils/timezone.py -> airflow/utils/timezone.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import datetime as dt",
          "23: import pendulum",
          "24: from dateutil.relativedelta import relativedelta",
          "25: from pendulum.datetime import DateTime",
          "31: def is_localized(value):",
          "",
          "[Removed Lines]",
          "21: from typing import overload",
          "27: # UTC time zone as a tzinfo instance.",
          "28: utc = pendulum.tz.timezone(\"UTC\")",
          "",
          "[Added Lines]",
          "21: from typing import TYPE_CHECKING, overload",
          "27: if TYPE_CHECKING:",
          "28:     from pendulum.tz.timezone import FixedTimezone, Timezone",
          "30: _PENDULUM3 = pendulum.__version__.startswith(\"3\")",
          "31: # UTC Timezone as a tzinfo instance. Actual value depends on pendulum version:",
          "32: # - Timezone(\"UTC\") in pendulum 3",
          "33: # - FixedTimezone(0, \"UTC\") in pendulum 2",
          "34: utc = pendulum.UTC",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "135:     # Check that we won't overwrite the timezone of an aware datetime.",
          "136:     if is_localized(value):",
          "137:         raise ValueError(f\"make_aware expects a naive datetime, got {value}\")",
          "144:     localized = getattr(timezone, \"localize\", None)",
          "145:     if localized is not None:",
          "146:         # This method is available for pytz time zones",
          "",
          "[Removed Lines]",
          "138:     if hasattr(value, \"fold\"):",
          "139:         # In case of python 3.6 we want to do the same that pendulum does for python3.5",
          "140:         # i.e in case we move clock back we want to schedule the run at the time of the second",
          "141:         # instance of the same clock time rather than the first one.",
          "142:         # Fold parameter has no impact in other cases so we can safely set it to 1 here",
          "143:         value = value.replace(fold=1)",
          "",
          "[Added Lines]",
          "144:     # In case we move clock back we want to schedule the run at the time of the second",
          "145:     # instance of the same clock time rather than the first one.",
          "146:     # Fold parameter has no impact in other cases, so we can safely set it to 1 here",
          "147:     value = value.replace(fold=1)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "273:     if not joined:",
          "274:         return \"<1s\"",
          "275:     return joined",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "282: def parse_timezone(name: str | int) -> FixedTimezone | Timezone:",
          "283:     \"\"\"",
          "284:     Parse timezone and return one of the pendulum Timezone.",
          "286:     Provide the same interface as ``pendulum.timezone(name)``",
          "288:     :param name: Either IANA timezone or offset to UTC in seconds.",
          "290:     :meta private:",
          "291:     \"\"\"",
          "292:     if _PENDULUM3:",
          "293:         # This only presented in pendulum 3 and code do not reached into the pendulum 2",
          "294:         return pendulum.timezone(name)  # type: ignore[operator]",
          "295:     # In pendulum 2 this refers to the function, in pendulum 3 refers to the module",
          "296:     return pendulum.tz.timezone(name)  # type: ignore[operator]",
          "299: def local_timezone() -> FixedTimezone | Timezone:",
          "300:     \"\"\"",
          "301:     Return local timezone.",
          "303:     Provide the same interface as ``pendulum.tz.local_timezone()``",
          "305:     :meta private:",
          "306:     \"\"\"",
          "307:     return pendulum.tz.local_timezone()",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/common_options.py||dev/breeze/src/airflow_breeze/commands/common_options.py": [
          "File: dev/breeze/src/airflow_breeze/commands/common_options.py -> dev/breeze/src/airflow_breeze/commands/common_options.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "151:     is_flag=True,",
          "152:     envvar=\"DOWNGRADE_SQLALCHEMY\",",
          "153: )",
          "154: option_dry_run = click.option(",
          "155:     \"-D\",",
          "156:     \"--dry-run\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "154: option_downgrade_pendulum = click.option(",
          "155:     \"--downgrade-pendulum\",",
          "156:     help=\"Downgrade Pendulum to minimum supported version.\",",
          "157:     is_flag=True,",
          "158:     envvar=\"DOWNGRADE_PENDULUM\",",
          "159: )",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/developer_commands.py -> dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "39:     option_database_isolation,",
          "40:     option_db_reset,",
          "41:     option_docker_host,",
          "42:     option_downgrade_sqlalchemy,",
          "43:     option_dry_run,",
          "44:     option_forward_credentials,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42:     option_downgrade_pendulum,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "248: @option_db_reset",
          "249: @option_docker_host",
          "250: @option_downgrade_sqlalchemy",
          "251: @option_dry_run",
          "252: @option_executor_shell",
          "253: @option_force_build",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "252: @option_downgrade_pendulum",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "294:     database_isolation: bool,",
          "295:     db_reset: bool,",
          "296:     downgrade_sqlalchemy: bool,",
          "297:     docker_host: str | None,",
          "298:     executor: str,",
          "299:     extra_args: tuple,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "299:     downgrade_pendulum: bool,",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "354:         database_isolation=database_isolation,",
          "355:         db_reset=db_reset,",
          "356:         downgrade_sqlalchemy=downgrade_sqlalchemy,",
          "357:         docker_host=docker_host,",
          "358:         executor=executor,",
          "359:         extra_args=extra_args if not max_time else [\"exit\"],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "360:         downgrade_pendulum=downgrade_pendulum,",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/developer_commands_config.py||dev/breeze/src/airflow_breeze/commands/developer_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/developer_commands_config.py -> dev/breeze/src/airflow_breeze/commands/developer_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "159:             \"options\": [",
          "160:                 \"--upgrade-boto\",",
          "161:                 \"--downgrade-sqlalchemy\",",
          "162:             ],",
          "163:         },",
          "164:         {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "162:                 \"--downgrade-pendulum\",",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/testing_commands.py||dev/breeze/src/airflow_breeze/commands/testing_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/testing_commands.py -> dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29:     option_backend,",
          "30:     option_db_reset,",
          "31:     option_debug_resources,",
          "32:     option_downgrade_sqlalchemy,",
          "33:     option_dry_run,",
          "34:     option_forward_credentials,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32:     option_downgrade_pendulum,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "471: @option_excluded_parallel_test_types",
          "472: @option_upgrade_boto",
          "473: @option_downgrade_sqlalchemy",
          "474: @option_collect_only",
          "475: @option_remove_arm_packages",
          "476: @option_skip_docker_compose_down",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "475: @option_downgrade_pendulum",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "513: @option_excluded_parallel_test_types",
          "514: @option_upgrade_boto",
          "515: @option_downgrade_sqlalchemy",
          "516: @option_collect_only",
          "517: @option_remove_arm_packages",
          "518: @option_skip_docker_compose_down",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "518: @option_downgrade_pendulum",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "548: @option_collect_only",
          "549: @option_debug_resources",
          "550: @option_downgrade_sqlalchemy",
          "551: @option_dry_run",
          "552: @option_enable_coverage",
          "553: @option_excluded_parallel_test_types",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "554: @option_downgrade_pendulum",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "589:     db_reset: bool,",
          "590:     debug_resources: bool,",
          "591:     downgrade_sqlalchemy: bool,",
          "592:     enable_coverage: bool,",
          "593:     excluded_parallel_test_types: str,",
          "594:     extra_pytest_args: tuple,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "596:     downgrade_pendulum: bool,",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "632:         backend=backend,",
          "633:         collect_only=collect_only,",
          "634:         downgrade_sqlalchemy=downgrade_sqlalchemy,",
          "635:         enable_coverage=enable_coverage,",
          "636:         forward_credentials=forward_credentials,",
          "637:         forward_ports=False,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "640:         downgrade_pendulum=downgrade_pendulum,",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/testing_commands_config.py||dev/breeze/src/airflow_breeze/commands/testing_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/testing_commands_config.py -> dev/breeze/src/airflow_breeze/commands/testing_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "79:                 \"--mount-sources\",",
          "80:                 \"--upgrade-boto\",",
          "81:                 \"--downgrade-sqlalchemy\",",
          "82:                 \"--remove-arm-packages\",",
          "83:                 \"--skip-docker-compose-down\",",
          "84:             ],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "82:                 \"--downgrade-pendulum\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "126:                 \"--mount-sources\",",
          "127:                 \"--upgrade-boto\",",
          "128:                 \"--downgrade-sqlalchemy\",",
          "129:                 \"--remove-arm-packages\",",
          "130:                 \"--skip-docker-compose-down\",",
          "131:             ],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "130:                 \"--downgrade-pendulum\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "177:                 \"--mount-sources\",",
          "178:                 \"--upgrade-boto\",",
          "179:                 \"--downgrade-sqlalchemy\",",
          "180:                 \"--remove-arm-packages\",",
          "181:                 \"--skip-docker-compose-down\",",
          "182:             ],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "182:                 \"--downgrade-pendulum\",",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/params/shell_params.py||dev/breeze/src/airflow_breeze/params/shell_params.py": [
          "File: dev/breeze/src/airflow_breeze/params/shell_params.py -> dev/breeze/src/airflow_breeze/params/shell_params.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "148:     dev_mode: bool = False",
          "149:     docker_host: str | None = os.environ.get(\"DOCKER_HOST\")",
          "150:     downgrade_sqlalchemy: bool = False",
          "151:     dry_run: bool = False",
          "152:     enable_coverage: bool = False",
          "153:     executor: str = START_AIRFLOW_DEFAULT_ALLOWED_EXECUTOR",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "151:     downgrade_pendulum: bool = False",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "516:         _set_var(_env, \"DEV_MODE\", self.dev_mode)",
          "517:         _set_var(_env, \"DOCKER_IS_ROOTLESS\", self.rootless_docker)",
          "518:         _set_var(_env, \"DOWNGRADE_SQLALCHEMY\", self.downgrade_sqlalchemy)",
          "519:         _set_var(_env, \"ENABLED_SYSTEMS\", None, \"\")",
          "520:         _set_var(_env, \"FLOWER_HOST_PORT\", None, FLOWER_HOST_PORT)",
          "521:         _set_var(_env, \"GITHUB_ACTIONS\", self.github_actions)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "520:         _set_var(_env, \"DOWNGRADE_PENDULUM\", self.downgrade_pendulum)",
          "",
          "---------------"
        ],
        "kubernetes_tests/test_kubernetes_pod_operator.py||kubernetes_tests/test_kubernetes_pod_operator.py": [
          "File: kubernetes_tests/test_kubernetes_pod_operator.py -> kubernetes_tests/test_kubernetes_pod_operator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: from unittest.mock import ANY, MagicMock",
          "27: from uuid import uuid4",
          "30: import pytest",
          "31: from kubernetes import client",
          "32: from kubernetes.client import V1EnvVar, V1PodSecurityContext, V1SecurityContext, models as k8s",
          "",
          "[Removed Lines]",
          "29: import pendulum",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "54: def create_context(task) -> Context:",
          "55:     dag = DAG(dag_id=\"dag\")",
          "57:     dag_run = DagRun(",
          "58:         dag_id=dag.dag_id,",
          "59:         execution_date=execution_date,",
          "",
          "[Removed Lines]",
          "56:     execution_date = timezone.datetime(2016, 1, 1, 1, 0, 0, tzinfo=pendulum.tz.timezone(\"Europe/Amsterdam\"))",
          "",
          "[Added Lines]",
          "55:     execution_date = timezone.datetime(",
          "56:         2016, 1, 1, 1, 0, 0, tzinfo=timezone.parse_timezone(\"Europe/Amsterdam\")",
          "57:     )",
          "",
          "---------------"
        ],
        "scripts/ci/docker-compose/devcontainer.env||scripts/ci/docker-compose/devcontainer.env": [
          "File: scripts/ci/docker-compose/devcontainer.env -> scripts/ci/docker-compose/devcontainer.env",
          "--- Hunk 1 ---",
          "[Context before]",
          "68: SUSPENDED_PROVIDERS_FOLDERS=\"\"",
          "69: TEST_TYPE=",
          "70: UPGRADE_BOTO=\"false\"",
          "71: DOWNGRADE_SQLALCHEMY=\"false\"",
          "72: UPGRADE_TO_NEWER_DEPENDENCIES=\"false\"",
          "73: VERBOSE=\"false\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "71: DOWNGRADE_PENDULUM=\"false\"",
          "",
          "---------------"
        ],
        "scripts/docker/entrypoint_ci.sh||scripts/docker/entrypoint_ci.sh": [
          "File: scripts/docker/entrypoint_ci.sh -> scripts/docker/entrypoint_ci.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "239:     pip check",
          "240: }",
          "242: # Check if we should run tests and run them if needed",
          "243: function check_run_tests() {",
          "244:     if [[ ${RUN_TESTS=} != \"true\" ]]; then",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "242: # Download minimum supported version of pendulum to run tests with it",
          "243: function check_download_pendulum() {",
          "244:     if [[ ${DOWNGRADE_PENDULUM=} != \"true\" ]]; then",
          "245:         return",
          "246:     fi",
          "247:     min_pendulum_version=$(grep \"\\\"pendulum>=\" pyproject.toml | sed \"s/.*>=\\([0-9\\.]*\\).*/\\1/\" | xargs)",
          "248:     echo",
          "249:     echo \"${COLOR_BLUE}Downgrading pendulum to minimum supported version: ${min_pendulum_version}${COLOR_RESET}\"",
          "250:     echo",
          "251:     pip install --root-user-action ignore \"pendulum==${min_pendulum_version}\"",
          "252:     pip check",
          "253: }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "269: environment_initialization",
          "270: check_boto_upgrade",
          "271: check_download_sqlalchemy",
          "272: check_run_tests \"${@}\"",
          "274: # If we are not running tests - just exec to bash shell",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "285: check_download_pendulum",
          "",
          "---------------"
        ],
        "tests/api_connexion/endpoints/test_dag_endpoint.py||tests/api_connexion/endpoints/test_dag_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_dag_endpoint.py -> tests/api_connexion/endpoints/test_dag_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import unittest.mock",
          "21: from datetime import datetime",
          "23: import pytest",
          "25: from airflow.api_connexion.exceptions import EXCEPTIONS_LINK_MAP",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "23: import pendulum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "46: TASK_ID = \"op1\"",
          "47: DAG2_ID = \"test_dag2\"",
          "48: DAG3_ID = \"test_dag3\"",
          "51: @pytest.fixture(scope=\"module\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "50: UTC_JSON_REPR = \"UTC\" if pendulum.__version__.startswith(\"3\") else \"Timezone('UTC')\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "316:             \"tags\": [],",
          "317:             \"template_searchpath\": None,",
          "318:             \"timetable_description\": None,",
          "320:         }",
          "321:         assert response.json == expected",
          "",
          "[Removed Lines]",
          "319:             \"timezone\": \"Timezone('UTC')\",",
          "",
          "[Added Lines]",
          "321:             \"timezone\": UTC_JSON_REPR,",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "367:             \"tags\": [],",
          "368:             \"template_searchpath\": None,",
          "369:             \"timetable_description\": None,",
          "371:         }",
          "372:         assert response.json == expected",
          "",
          "[Removed Lines]",
          "370:             \"timezone\": \"Timezone('UTC')\",",
          "",
          "[Added Lines]",
          "372:             \"timezone\": UTC_JSON_REPR,",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "418:             \"tags\": [],",
          "419:             \"template_searchpath\": None,",
          "420:             \"timetable_description\": None,",
          "422:         }",
          "423:         assert response.json == expected",
          "",
          "[Removed Lines]",
          "421:             \"timezone\": \"Timezone('UTC')\",",
          "",
          "[Added Lines]",
          "423:             \"timezone\": UTC_JSON_REPR,",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "478:             \"tags\": [],",
          "479:             \"template_searchpath\": None,",
          "480:             \"timetable_description\": None,",
          "482:         }",
          "483:         response = self.client.get(",
          "484:             f\"/api/v1/dags/{self.dag_id}/details\", environ_overrides={\"REMOTE_USER\": \"test\"}",
          "",
          "[Removed Lines]",
          "481:             \"timezone\": \"Timezone('UTC')\",",
          "",
          "[Added Lines]",
          "483:             \"timezone\": UTC_JSON_REPR,",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "539:             \"tags\": [],",
          "540:             \"template_searchpath\": None,",
          "541:             \"timetable_description\": None,",
          "543:         }",
          "544:         expected.update({\"last_parsed\": response.json[\"last_parsed\"]})",
          "545:         assert response.json == expected",
          "",
          "[Removed Lines]",
          "542:             \"timezone\": \"Timezone('UTC')\",",
          "",
          "[Added Lines]",
          "544:             \"timezone\": UTC_JSON_REPR,",
          "",
          "---------------"
        ],
        "tests/api_connexion/schemas/test_dag_schema.py||tests/api_connexion/schemas/test_dag_schema.py": [
          "File: tests/api_connexion/schemas/test_dag_schema.py -> tests/api_connexion/schemas/test_dag_schema.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: from datetime import datetime",
          "21: import pytest",
          "23: from airflow.api_connexion.schemas.dag_schema import (",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import pendulum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "29: from airflow.models import DagModel, DagTag",
          "30: from airflow.models.dag import DAG",
          "33: def test_serialize_test_dag_schema(url_safe_serializer):",
          "34:     dag_model = DagModel(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33: UTC_JSON_REPR = \"UTC\" if pendulum.__version__.startswith(\"3\") else \"Timezone('UTC')\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "184:         \"start_date\": \"2020-06-19T00:00:00+00:00\",",
          "185:         \"tags\": [{\"name\": \"example1\"}, {\"name\": \"example2\"}],",
          "186:         \"template_searchpath\": None,",
          "188:         \"max_active_runs\": 16,",
          "189:         \"pickle_id\": None,",
          "190:         \"end_date\": None,",
          "",
          "[Removed Lines]",
          "187:         \"timezone\": \"Timezone('UTC')\",",
          "",
          "[Added Lines]",
          "190:         \"timezone\": UTC_JSON_REPR,",
          "",
          "---------------"
        ],
        "tests/cli/commands/test_dag_command.py||tests/cli/commands/test_dag_command.py": [
          "File: tests/cli/commands/test_dag_command.py -> tests/cli/commands/test_dag_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "50: from tests.test_utils.db import clear_db_dags, clear_db_runs",
          "52: DEFAULT_DATE = timezone.make_aware(datetime(2015, 1, 1), timezone=timezone.utc)",
          "54: # TODO: Check if tests needs side effects - locally there's missing DAG",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "53: if pendulum.__version__.startswith(\"3\"):",
          "54:     DEFAULT_DATE_REPR = DEFAULT_DATE.isoformat(sep=\" \")",
          "55: else:",
          "56:     DEFAULT_DATE_REPR = DEFAULT_DATE.isoformat()",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "162:             )",
          "164:         output = stdout.getvalue()",
          "166:         assert \"Task runme_0 located in DAG example_bash_operator\\n\" in output",
          "168:         mock_run.assert_not_called()  # Dry run shouldn't run the backfill",
          "",
          "[Removed Lines]",
          "165:         assert f\"Dry run of DAG example_bash_operator on {DEFAULT_DATE.isoformat()}\\n\" in output",
          "",
          "[Added Lines]",
          "169:         assert f\"Dry run of DAG example_bash_operator on {DEFAULT_DATE_REPR}\\n\" in output",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "236:         output = stdout.getvalue()",
          "242:         assert \"Task run_this_first located in DAG example_branch_python_operator_decorator\\n\" in output",
          "244:         assert \"Task run_this_first located in DAG example_branch_operator\\n\" in output",
          "246:     @mock.patch(\"airflow.cli.commands.dag_command.get_dag\")",
          "",
          "[Removed Lines]",
          "238:         assert (",
          "239:             f\"Dry run of DAG example_branch_python_operator_decorator on \"",
          "240:             f\"{DEFAULT_DATE.isoformat()}\\n\" in output",
          "241:         )",
          "243:         assert f\"Dry run of DAG example_branch_operator on {DEFAULT_DATE.isoformat()}\\n\" in output",
          "",
          "[Added Lines]",
          "242:         assert f\"Dry run of DAG example_branch_python_operator_decorator on {DEFAULT_DATE_REPR}\\n\" in output",
          "244:         assert f\"Dry run of DAG example_branch_operator on {DEFAULT_DATE_REPR}\\n\" in output",
          "",
          "---------------"
        ],
        "tests/models/test_dag.py||tests/models/test_dag.py": [
          "File: tests/models/test_dag.py -> tests/models/test_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "37: import pytest",
          "38: import time_machine",
          "39: from dateutil.relativedelta import relativedelta",
          "40: from sqlalchemy import inspect",
          "42: from airflow import settings",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "40: from pendulum.tz.timezone import Timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "676:         \"\"\"",
          "677:         Make sure DST transitions are properly observed",
          "678:         \"\"\"",
          "681:         assert start.isoformat() == \"2018-10-28T02:55:00+02:00\", \"Pre-condition: start date is in DST\"",
          "683:         utc = timezone.convert_to_utc(start)",
          "",
          "[Removed Lines]",
          "679:         local_tz = pendulum.timezone(\"Europe/Zurich\")",
          "680:         start = local_tz.convert(datetime.datetime(2018, 10, 28, 2, 55), dst_rule=pendulum.PRE_TRANSITION)",
          "",
          "[Added Lines]",
          "680:         local_tz = Timezone(\"Europe/Zurich\")",
          "681:         start = local_tz.convert(datetime.datetime(2018, 10, 28, 2, 55, fold=0))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "706:         Make sure DST transitions are properly observed",
          "707:         \"\"\"",
          "708:         local_tz = pendulum.timezone(\"Europe/Zurich\")",
          "711:         utc = timezone.convert_to_utc(start)",
          "",
          "[Removed Lines]",
          "709:         start = local_tz.convert(datetime.datetime(2018, 10, 27, 3), dst_rule=pendulum.PRE_TRANSITION)",
          "",
          "[Added Lines]",
          "710:         start = local_tz.convert(datetime.datetime(2018, 10, 27, 3, fold=0))",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "735:         Make sure DST transitions are properly observed",
          "736:         \"\"\"",
          "737:         local_tz = pendulum.timezone(\"Europe/Zurich\")",
          "740:         utc = timezone.convert_to_utc(start)",
          "",
          "[Removed Lines]",
          "738:         start = local_tz.convert(datetime.datetime(2018, 3, 25, 2), dst_rule=pendulum.PRE_TRANSITION)",
          "",
          "[Added Lines]",
          "739:         start = local_tz.convert(datetime.datetime(2018, 3, 25, 2, fold=0))",
          "",
          "---------------"
        ],
        "tests/providers/openlineage/plugins/test_utils.py||tests/providers/openlineage/plugins/test_utils.py": [
          "File: tests/providers/openlineage/plugins/test_utils.py -> tests/providers/openlineage/plugins/test_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: from json import JSONEncoder",
          "24: from typing import Any",
          "27: import pytest",
          "28: from attrs import define",
          "29: from openlineage.client.utils import RedactMixin",
          "",
          "[Removed Lines]",
          "26: import pendulum",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39:     to_json_encodable,",
          "40:     url_to_https,",
          "41: )",
          "42: from airflow.utils.log.secrets_masker import _secrets_masker",
          "43: from airflow.utils.state import State",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "41: from airflow.utils import timezone",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "86:         state=State.NONE, run_id=run_id, data_interval=dag.get_next_data_interval(dag_model)",
          "87:     )",
          "88:     assert dagrun.data_interval_start is not None",
          "91:     assert dagrun.data_interval_start, dagrun.data_interval_end == (start_date_tz, end_date_tz)",
          "",
          "[Removed Lines]",
          "89:     start_date_tz = datetime.datetime(2022, 1, 1, tzinfo=pendulum.tz.timezone(\"UTC\"))",
          "90:     end_date_tz = datetime.datetime(2022, 1, 1, hour=2, tzinfo=pendulum.tz.timezone(\"UTC\"))",
          "",
          "[Added Lines]",
          "89:     start_date_tz = datetime.datetime(2022, 1, 1, tzinfo=timezone.utc)",
          "90:     end_date_tz = datetime.datetime(2022, 1, 1, hour=2, tzinfo=timezone.utc)",
          "",
          "---------------"
        ],
        "tests/sensors/test_time_sensor.py||tests/sensors/test_time_sensor.py": [
          "File: tests/sensors/test_time_sensor.py -> tests/sensors/test_time_sensor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: from datetime import datetime, time",
          "23: import pendulum",
          "24: import pytest",
          "25: import time_machine",
          "28: from airflow.exceptions import TaskDeferred",
          "29: from airflow.models.dag import DAG",
          "",
          "[Removed Lines]",
          "21: from unittest.mock import patch",
          "26: from pendulum.tz.timezone import UTC",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "34: DEFAULT_TIMEZONE = \"Asia/Singapore\"  # UTC+08:00",
          "35: DEFAULT_DATE_WO_TZ = datetime(2015, 1, 1)",
          "39: class TestTimeSensor:",
          "",
          "[Removed Lines]",
          "36: DEFAULT_DATE_WITH_TZ = datetime(2015, 1, 1, tzinfo=pendulum.tz.timezone(DEFAULT_TIMEZONE))",
          "",
          "[Added Lines]",
          "34: DEFAULT_DATE_WITH_TZ = datetime(2015, 1, 1, tzinfo=timezone.parse_timezone(DEFAULT_TIMEZONE))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "46:         ],",
          "47:     )",
          "48:     @time_machine.travel(timezone.datetime(2020, 1, 1, 23, 0).replace(tzinfo=timezone.utc))",
          "56: class TestTimeSensorAsync:",
          "",
          "[Removed Lines]",
          "49:     def test_timezone(self, default_timezone, start_date, expected):",
          "50:         with patch(\"airflow.settings.TIMEZONE\", pendulum.timezone(default_timezone)):",
          "51:             dag = DAG(\"test\", default_args={\"start_date\": start_date})",
          "52:             op = TimeSensor(task_id=\"test\", target_time=time(10, 0), dag=dag)",
          "53:             assert op.poke(None) == expected",
          "",
          "[Added Lines]",
          "47:     def test_timezone(self, default_timezone, start_date, expected, monkeypatch):",
          "48:         monkeypatch.setattr(\"airflow.settings.TIMEZONE\", timezone.parse_timezone(default_timezone))",
          "49:         dag = DAG(\"test\", default_args={\"start_date\": start_date})",
          "50:         op = TimeSensor(task_id=\"test\", target_time=time(10, 0), dag=dag)",
          "51:         assert op.poke(None) == expected",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "72:         with DAG(\"test_target_time_aware\", start_date=timezone.datetime(2020, 1, 1, 23, 0)):",
          "73:             aware_time = time(0, 1).replace(tzinfo=pendulum.local_timezone())",
          "74:             op = TimeSensorAsync(task_id=\"test\", target_time=aware_time)",
          "78:     def test_target_time_naive_dag_timezone(self):",
          "79:         \"\"\"",
          "",
          "[Removed Lines]",
          "75:             assert hasattr(op.target_datetime.tzinfo, \"offset\")",
          "76:             assert op.target_datetime.tzinfo.offset == 0",
          "",
          "[Added Lines]",
          "73:             assert op.target_datetime.tzinfo == timezone.utc",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "85:         ):",
          "86:             op = TimeSensorAsync(task_id=\"test\", target_time=pendulum.time(9, 0))",
          "87:             assert op.target_datetime.time() == pendulum.time(1, 0)",
          "",
          "[Removed Lines]",
          "88:             assert op.target_datetime.tzinfo == UTC",
          "",
          "[Added Lines]",
          "85:             assert op.target_datetime.tzinfo == timezone.utc",
          "",
          "---------------"
        ],
        "tests/serialization/serializers/test_serializers.py||tests/serialization/serializers/test_serializers.py": [
          "File: tests/serialization/serializers/test_serializers.py -> tests/serialization/serializers/test_serializers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: from unittest.mock import patch",
          "23: import numpy as np",
          "24: import pendulum.tz",
          "25: import pytest",
          "26: from dateutil.tz import tzutc",
          "27: from deltalake import DeltaTable",
          "28: from pendulum import DateTime",
          "29: from pyiceberg.catalog import Catalog",
          "30: from pyiceberg.io import FileIO",
          "31: from pyiceberg.table import Table",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24: import pendulum",
          "30: from pendulum.tz.timezone import FixedTimezone, Timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39: else:",
          "40:     from backports.zoneinfo import ZoneInfo",
          "43: class TestSerializers:",
          "44:     def test_datetime(self):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "44: PENDULUM3 = pendulum.__version__.startswith(\"3\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "227:         assert i.version() == d.version()",
          "228:         assert i._storage_options == d._storage_options",
          "229:         assert d._storage_options is None",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "235:     @pytest.mark.skipif(not PENDULUM3, reason=\"Test case for pendulum~=3\")",
          "236:     @pytest.mark.parametrize(",
          "237:         \"ser_value, expected\",",
          "238:         [",
          "239:             pytest.param(",
          "240:                 {",
          "241:                     \"__classname__\": \"pendulum.datetime.DateTime\",",
          "242:                     \"__version__\": 2,",
          "243:                     \"__data__\": {",
          "244:                         \"timestamp\": 1680307200.0,",
          "245:                         \"tz\": {",
          "246:                             \"__classname__\": \"builtins.tuple\",",
          "247:                             \"__version__\": 1,",
          "248:                             \"__data__\": [\"UTC\", \"pendulum.tz.timezone.FixedTimezone\", 1, True],",
          "249:                         },",
          "250:                     },",
          "251:                 },",
          "252:                 pendulum.datetime(2023, 4, 1, tz=Timezone(\"UTC\")),",
          "253:                 id=\"in-utc-timezone\",",
          "254:             ),",
          "255:             pytest.param(",
          "256:                 {",
          "257:                     \"__classname__\": \"pendulum.datetime.DateTime\",",
          "258:                     \"__version__\": 2,",
          "259:                     \"__data__\": {",
          "260:                         \"timestamp\": 1680292800.0,",
          "261:                         \"tz\": {",
          "262:                             \"__classname__\": \"builtins.tuple\",",
          "263:                             \"__version__\": 1,",
          "264:                             \"__data__\": [\"Asia/Tbilisi\", \"pendulum.tz.timezone.Timezone\", 1, True],",
          "265:                         },",
          "266:                     },",
          "267:                 },",
          "268:                 pendulum.datetime(2023, 4, 1, tz=Timezone(\"Asia/Tbilisi\")),",
          "269:                 id=\"non-dts-timezone\",",
          "270:             ),",
          "271:             pytest.param(",
          "272:                 {",
          "273:                     \"__classname__\": \"pendulum.datetime.DateTime\",",
          "274:                     \"__version__\": 2,",
          "275:                     \"__data__\": {",
          "276:                         \"timestamp\": 1680303600.0,",
          "277:                         \"tz\": {",
          "278:                             \"__classname__\": \"builtins.tuple\",",
          "279:                             \"__version__\": 1,",
          "280:                             \"__data__\": [\"Europe/London\", \"pendulum.tz.timezone.Timezone\", 1, True],",
          "281:                         },",
          "282:                     },",
          "283:                 },",
          "284:                 pendulum.datetime(2023, 4, 1, tz=Timezone(\"Europe/London\")),",
          "285:                 id=\"dts-timezone\",",
          "286:             ),",
          "287:             pytest.param(",
          "288:                 {",
          "289:                     \"__classname__\": \"pendulum.datetime.DateTime\",",
          "290:                     \"__version__\": 2,",
          "291:                     \"__data__\": {",
          "292:                         \"timestamp\": 1680310800.0,",
          "293:                         \"tz\": {",
          "294:                             \"__classname__\": \"builtins.tuple\",",
          "295:                             \"__version__\": 1,",
          "296:                             \"__data__\": [-3600, \"pendulum.tz.timezone.FixedTimezone\", 1, True],",
          "297:                         },",
          "298:                     },",
          "299:                 },",
          "300:                 pendulum.datetime(2023, 4, 1, tz=FixedTimezone(-3600)),",
          "301:                 id=\"offset-timezone\",",
          "302:             ),",
          "303:         ],",
          "304:     )",
          "305:     def test_pendulum_2_to_3(self, ser_value, expected):",
          "306:         \"\"\"Test deserialize objects in pendulum 3 which serialised in pendulum 2.\"\"\"",
          "307:         assert deserialize(ser_value) == expected",
          "309:     @pytest.mark.skipif(PENDULUM3, reason=\"Test case for pendulum~=2\")",
          "310:     @pytest.mark.parametrize(",
          "311:         \"ser_value, expected\",",
          "312:         [",
          "313:             pytest.param(",
          "314:                 {",
          "315:                     \"__classname__\": \"pendulum.datetime.DateTime\",",
          "316:                     \"__version__\": 2,",
          "317:                     \"__data__\": {",
          "318:                         \"timestamp\": 1680307200.0,",
          "319:                         \"tz\": {",
          "320:                             \"__classname__\": \"builtins.tuple\",",
          "321:                             \"__version__\": 1,",
          "322:                             \"__data__\": [\"UTC\", \"pendulum.tz.timezone.Timezone\", 1, True],",
          "323:                         },",
          "324:                     },",
          "325:                 },",
          "326:                 pendulum.datetime(2023, 4, 1, tz=Timezone(\"UTC\")),",
          "327:                 id=\"in-utc-timezone\",",
          "328:             ),",
          "329:             pytest.param(",
          "330:                 {",
          "331:                     \"__classname__\": \"pendulum.datetime.DateTime\",",
          "332:                     \"__version__\": 2,",
          "333:                     \"__data__\": {",
          "334:                         \"timestamp\": 1680292800.0,",
          "335:                         \"tz\": {",
          "336:                             \"__classname__\": \"builtins.tuple\",",
          "337:                             \"__version__\": 1,",
          "338:                             \"__data__\": [\"Asia/Tbilisi\", \"pendulum.tz.timezone.Timezone\", 1, True],",
          "339:                         },",
          "340:                     },",
          "341:                 },",
          "342:                 pendulum.datetime(2023, 4, 1, tz=Timezone(\"Asia/Tbilisi\")),",
          "343:                 id=\"non-dts-timezone\",",
          "344:             ),",
          "345:             pytest.param(",
          "346:                 {",
          "347:                     \"__classname__\": \"pendulum.datetime.DateTime\",",
          "348:                     \"__version__\": 2,",
          "349:                     \"__data__\": {",
          "350:                         \"timestamp\": 1680303600.0,",
          "351:                         \"tz\": {",
          "352:                             \"__classname__\": \"builtins.tuple\",",
          "353:                             \"__version__\": 1,",
          "354:                             \"__data__\": [\"Europe/London\", \"pendulum.tz.timezone.Timezone\", 1, True],",
          "355:                         },",
          "356:                     },",
          "357:                 },",
          "358:                 pendulum.datetime(2023, 4, 1, tz=Timezone(\"Europe/London\")),",
          "359:                 id=\"dts-timezone\",",
          "360:             ),",
          "361:             pytest.param(",
          "362:                 {",
          "363:                     \"__classname__\": \"pendulum.datetime.DateTime\",",
          "364:                     \"__version__\": 2,",
          "365:                     \"__data__\": {",
          "366:                         \"timestamp\": 1680310800.0,",
          "367:                         \"tz\": {",
          "368:                             \"__classname__\": \"builtins.tuple\",",
          "369:                             \"__version__\": 1,",
          "370:                             \"__data__\": [-3600, \"pendulum.tz.timezone.FixedTimezone\", 1, True],",
          "371:                         },",
          "372:                     },",
          "373:                 },",
          "374:                 pendulum.datetime(2023, 4, 1, tz=FixedTimezone(-3600)),",
          "375:                 id=\"offset-timezone\",",
          "376:             ),",
          "377:         ],",
          "378:     )",
          "379:     def test_pendulum_3_to_2(self, ser_value, expected):",
          "380:         \"\"\"Test deserialize objects in pendulum 2 which serialised in pendulum 3.\"\"\"",
          "381:         assert deserialize(ser_value) == expected",
          "",
          "---------------"
        ],
        "tests/serialization/test_serialized_objects.py||tests/serialization/test_serialized_objects.py": [
          "File: tests/serialization/test_serialized_objects.py -> tests/serialization/test_serialized_objects.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import json",
          "21: from datetime import datetime, timedelta",
          "24: import pytest",
          "25: from dateutil import relativedelta",
          "26: from kubernetes.client import models as k8s",
          "28: from airflow.datasets import Dataset",
          "29: from airflow.exceptions import SerializationError",
          "",
          "[Removed Lines]",
          "23: import pendulum",
          "",
          "[Added Lines]",
          "26: from pendulum.tz.timezone import Timezone",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "142:         (1, None, equals),",
          "143:         (datetime.utcnow(), DAT.DATETIME, equal_time),",
          "144:         (timedelta(minutes=2), DAT.TIMEDELTA, equals),",
          "146:         (relativedelta.relativedelta(hours=+1), DAT.RELATIVEDELTA, lambda a, b: a.hours == b.hours),",
          "147:         ({\"test\": \"dict\", \"test-1\": 1}, None, equals),",
          "148:         ([\"array_item\", 2], None, equals),",
          "",
          "[Removed Lines]",
          "145:         (pendulum.tz.timezone(\"UTC\"), DAT.TIMEZONE, lambda a, b: a.name == b.name),",
          "",
          "[Added Lines]",
          "145:         (Timezone(\"UTC\"), DAT.TIMEZONE, lambda a, b: a.name == b.name),",
          "",
          "---------------"
        ],
        "tests/triggers/test_temporal.py||tests/triggers/test_temporal.py": [
          "File: tests/triggers/test_temporal.py -> tests/triggers/test_temporal.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "64: @pytest.mark.parametrize(",
          "65:     \"tz\",",
          "66:     [",
          "70:     ],",
          "71: )",
          "72: @pytest.mark.asyncio",
          "",
          "[Removed Lines]",
          "67:         pendulum.tz.timezone(\"UTC\"),",
          "68:         pendulum.tz.timezone(\"Europe/Paris\"),",
          "69:         pendulum.tz.timezone(\"America/Toronto\"),",
          "",
          "[Added Lines]",
          "67:         timezone.parse_timezone(\"UTC\"),",
          "68:         timezone.parse_timezone(\"Europe/Paris\"),",
          "69:         timezone.parse_timezone(\"America/Toronto\"),",
          "",
          "---------------"
        ],
        "tests/utils/test_timezone.py||tests/utils/test_timezone.py": [
          "File: tests/utils/test_timezone.py -> tests/utils/test_timezone.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import pendulum",
          "23: import pytest",
          "25: from airflow.utils import timezone",
          "31: UTC = timezone.utc",
          "",
          "[Removed Lines]",
          "26: from airflow.utils.timezone import coerce_datetime",
          "28: CET = pendulum.tz.timezone(\"Europe/Paris\")",
          "29: EAT = pendulum.tz.timezone(\"Africa/Nairobi\")  # Africa/Nairobi",
          "30: ICT = pendulum.tz.timezone(\"Asia/Bangkok\")  # Asia/Bangkok",
          "",
          "[Added Lines]",
          "24: from pendulum.tz.timezone import Timezone",
          "27: from airflow.utils.timezone import coerce_datetime, parse_timezone",
          "29: CET = Timezone(\"Europe/Paris\")",
          "30: EAT = Timezone(\"Africa/Nairobi\")  # Africa/Nairobi",
          "31: ICT = Timezone(\"Asia/Bangkok\")  # Asia/Bangkok",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "117: )",
          "118: def test_coerce_datetime(input_datetime, output_datetime):",
          "119:     assert output_datetime == coerce_datetime(input_datetime)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "123: @pytest.mark.parametrize(",
          "124:     \"tz_name\",",
          "125:     [",
          "126:         pytest.param(\"Europe/Paris\", id=\"CET\"),",
          "127:         pytest.param(\"Africa/Nairobi\", id=\"EAT\"),",
          "128:         pytest.param(\"Asia/Bangkok\", id=\"ICT\"),",
          "129:     ],",
          "130: )",
          "131: def test_parse_timezone_iana(tz_name: str):",
          "132:     tz = parse_timezone(tz_name)",
          "133:     assert tz.name == tz_name",
          "134:     assert parse_timezone(tz_name) is tz",
          "137: @pytest.mark.parametrize(\"tz_name\", [\"utc\", \"UTC\", \"uTc\"])",
          "138: def test_parse_timezone_utc(tz_name):",
          "139:     tz = parse_timezone(tz_name)",
          "140:     assert tz.name == \"UTC\"",
          "141:     assert parse_timezone(tz_name) is tz",
          "142:     assert tz is timezone.utc, \"Expected that UTC timezone is same object as `airflow.utils.timezone.utc`\"",
          "145: @pytest.mark.parametrize(",
          "146:     \"tz_offset, expected_offset, expected_name\",",
          "147:     [",
          "148:         pytest.param(0, 0, \"+00:00\", id=\"zero-offset\"),",
          "149:         pytest.param(-3600, -3600, \"-01:00\", id=\"1-hour-behind\"),",
          "150:         pytest.param(19800, 19800, \"+05:30\", id=\"5.5-hours-ahead\"),",
          "151:     ],",
          "152: )",
          "153: def test_parse_timezone_offset(tz_offset: int, expected_offset, expected_name):",
          "154:     tz = parse_timezone(tz_offset)",
          "155:     assert hasattr(tz, \"offset\")",
          "156:     assert tz.offset == expected_offset",
          "157:     assert tz.name == expected_name",
          "158:     assert parse_timezone(tz_offset) is tz",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c2e6b9a92dcd603c05f3008a7139eff52fe3e1b7",
      "candidate_info": {
        "commit_hash": "c2e6b9a92dcd603c05f3008a7139eff52fe3e1b7",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c2e6b9a92dcd603c05f3008a7139eff52fe3e1b7",
        "files": [
          "dev/breeze/src/airflow_breeze/utils/kubernetes_utils.py"
        ],
        "message": "Add version check for k8s setup venv command (#36673)\n\nThis command install airflow in k8s venv and in case version of\nPython is not yet supported by Airflow, it might fail.\n\nWe do not have check it lower-bound because breeze supports the\nsame minimum version of Airflow as Airflow itself.\n\nThe command prints instructions on how to reinstall breeze with\ndifferent Python version in such case.\n\n(cherry picked from commit 9264a4b4e21702a2bc71bb77ee3cc4ada9dfd5e7)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/utils/kubernetes_utils.py||dev/breeze/src/airflow_breeze/utils/kubernetes_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/utils/kubernetes_utils.py||dev/breeze/src/airflow_breeze/utils/kubernetes_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/kubernetes_utils.py -> dev/breeze/src/airflow_breeze/utils/kubernetes_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "32: from typing import Any, NamedTuple",
          "33: from urllib import request",
          "36: from airflow_breeze.utils.console import Output, get_console",
          "37: from airflow_breeze.utils.host_info_utils import Architecture, get_host_architecture, get_host_os",
          "38: from airflow_breeze.utils.path_utils import AIRFLOW_SOURCES_ROOT, BUILD_CACHE_DIR",
          "",
          "[Removed Lines]",
          "35: from airflow_breeze.global_constants import ALLOWED_ARCHITECTURES, HELM_VERSION, KIND_VERSION, PIP_VERSION",
          "",
          "[Added Lines]",
          "35: from airflow_breeze.global_constants import (",
          "36:     ALLOWED_ARCHITECTURES,",
          "37:     ALLOWED_PYTHON_MAJOR_MINOR_VERSIONS,",
          "38:     HELM_VERSION,",
          "39:     KIND_VERSION,",
          "40:     PIP_VERSION,",
          "41: )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "330:         get_console().print(f\"[info]Dry run - would be removing {K8S_ENV_PATH}\")",
          "331:     else:",
          "332:         shutil.rmtree(K8S_ENV_PATH, ignore_errors=True)",
          "333:     venv_command_result = run_command(",
          "334:         [sys.executable, \"-m\", \"venv\", str(K8S_ENV_PATH)],",
          "335:         check=False,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "339:     max_python_version = ALLOWED_PYTHON_MAJOR_MINOR_VERSIONS[-1]",
          "340:     max_python_version_tuple = tuple(int(x) for x in max_python_version.split(\".\"))",
          "341:     higher_python_version_tuple = max_python_version_tuple[0], max_python_version_tuple[1] + 1",
          "342:     if sys.version_info >= higher_python_version_tuple:",
          "343:         get_console().print(",
          "344:             f\"[red]This is not supported in Python {higher_python_version_tuple} and above[/]\\n\"",
          "345:         )",
          "346:         get_console().print(f\"[warning]Please use Python version before {higher_python_version_tuple}[/]\\n\")",
          "347:         get_console().print(",
          "348:             \"[info]You can uninstall breeze and install it again with earlier Python \"",
          "349:             \"version. For example:[/]\\n\"",
          "350:         )",
          "351:         get_console().print(\"pipx uninstall apache-airflow-breeze\")",
          "352:         get_console().print(\"pipx install --python PYTHON_PATH -e ./dev/breeze\\n\")",
          "353:         get_console().print(",
          "354:             f\"[info]PYTHON_PATH - path to your Python binary(< {higher_python_version_tuple})[/]\\n\"",
          "355:         )",
          "356:         get_console().print(\"[info]Then recreate your k8s virtualenv with:[/]\\n\")",
          "357:         get_console().print(\"breeze k8s setup-env --force-venv-setup\\n\")",
          "358:         sys.exit(1)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c284ece1454bc64afdb2686464e91e42a4519b55",
      "candidate_info": {
        "commit_hash": "c284ece1454bc64afdb2686464e91e42a4519b55",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c284ece1454bc64afdb2686464e91e42a4519b55",
        "files": [
          "dev/breeze/src/airflow_breeze/utils/md5_build_check.py"
        ],
        "message": "Less verbose information about changed provider.yaml files (#36307)\n\nWhen we attempt to see if provider.yaml files make changes in\ndependencies, we print verbose information on what provider.yaml\nfiles changeed, but this is not necessary or needed. This change\nmakes the output less verbose by detail - just a number of changed\nfiles rather than full list of them - the full list is only printed\nwhen `--verbose` flag is used.\n\n(cherry picked from commit 7212301b2200cb968cd38cdaddb30d7ed7360bda)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/utils/md5_build_check.py||dev/breeze/src/airflow_breeze/utils/md5_build_check.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/utils/md5_build_check.py||dev/breeze/src/airflow_breeze/utils/md5_build_check.py": [
          "File: dev/breeze/src/airflow_breeze/utils/md5_build_check.py -> dev/breeze/src/airflow_breeze/utils/md5_build_check.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: from airflow_breeze.utils.console import get_console",
          "30: from airflow_breeze.utils.path_utils import AIRFLOW_SOURCES_ROOT",
          "31: from airflow_breeze.utils.run_utils import run_command",
          "33: if TYPE_CHECKING:",
          "34:     from airflow_breeze.params.build_ci_params import BuildCiParams",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32: from airflow_breeze.utils.shared_options import get_verbose",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "102:         if modified_provider_yaml_files:",
          "103:             get_console().print(",
          "104:                 \"[info]Attempting to generate provider dependencies. \"",
          "109:             )",
          "110:             # Regenerate provider_dependencies.json",
          "111:             run_command(",
          "112:                 [",
          "",
          "[Removed Lines]",
          "105:                 \"Provider yaml files changed since last check:[/]\"",
          "106:             )",
          "107:             get_console().print(",
          "108:                 [os.fspath(file.relative_to(AIRFLOW_SOURCES_ROOT)) for file in modified_provider_yaml_files]",
          "",
          "[Added Lines]",
          "106:                 f\"{len(modified_provider_yaml_files)} provider.yaml file(s) changed since last check.\"",
          "108:             if get_verbose():",
          "109:                 get_console().print(",
          "110:                     [",
          "111:                         os.fspath(file.relative_to(AIRFLOW_SOURCES_ROOT))",
          "112:                         for file in modified_provider_yaml_files",
          "113:                     ]",
          "114:                 )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "21a411d58ffb08638817acf958c3119198860c0f",
      "candidate_info": {
        "commit_hash": "21a411d58ffb08638817acf958c3119198860c0f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/21a411d58ffb08638817acf958c3119198860c0f",
        "files": [
          "dev/breeze/src/airflow_breeze/global_constants.py"
        ],
        "message": "Add utkarsharma2 to committers list (#36474)\n\n(cherry picked from commit e3fb20d358646d276d4e275fa67d34b4fc13b73a)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py": [
          "File: dev/breeze/src/airflow_breeze/global_constants.py -> dev/breeze/src/airflow_breeze/global_constants.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "338:     \"sekikn\",",
          "339:     \"turbaszek\",",
          "340:     \"uranusjr\",",
          "341:     \"vikramkoka\",",
          "342:     \"vincbeck\",",
          "343:     \"xinbinhuang\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "341:     \"utkarsharma2\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "48331ec903238b2981c98f4f80f8d84c074e7aac",
      "candidate_info": {
        "commit_hash": "48331ec903238b2981c98f4f80f8d84c074e7aac",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/48331ec903238b2981c98f4f80f8d84c074e7aac",
        "files": [
          "docs/docker-stack/changelog.rst"
        ],
        "message": "Remove wrong changelog about default Python version in the image (#36777)\n\nIn #36003 we **thought** we changed default \"version\" image to\npoint to \"newest\" python version not to the \"oldest\" supported\none - as agreed in https://lists.apache.org/thread/0oxnvct24xlqsj76z42w2ttw2d043oy3\n\nHowever as observed and tracked in #36740 the change was not effective.\nWe only changed the moment at which latest image is pointing to\n2.8.0 but not whether 2.8.0 points to `python-3.8` or `python-3.11'.\n\nThis means that we should only do that change for Python 3.9 qnd\nrevert the changelog (and cherry-pick it to 2.8.1)\n\n(cherry picked from commit 270b112b8aa8f2759e99d4263de55027aea8c01c)",
        "before_after_code_files": []
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {}
    },
    {
      "candidate_hash": "d72131f952836a3134c90805ef7c3bcf82ea93e9",
      "candidate_info": {
        "commit_hash": "d72131f952836a3134c90805ef7c3bcf82ea93e9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/d72131f952836a3134c90805ef7c3bcf82ea93e9",
        "files": [
          "airflow/api_connexion/endpoints/config_endpoint.py",
          "airflow/api_connexion/endpoints/connection_endpoint.py",
          "airflow/api_connexion/endpoints/dag_endpoint.py",
          "airflow/api_connexion/endpoints/dag_run_endpoint.py",
          "airflow/api_connexion/endpoints/dag_source_endpoint.py",
          "airflow/api_connexion/endpoints/dag_warning_endpoint.py",
          "airflow/api_connexion/endpoints/dataset_endpoint.py",
          "airflow/api_connexion/endpoints/event_log_endpoint.py",
          "airflow/api_connexion/endpoints/extra_link_endpoint.py",
          "airflow/api_connexion/endpoints/import_error_endpoint.py",
          "airflow/api_connexion/endpoints/log_endpoint.py",
          "airflow/api_connexion/endpoints/plugin_endpoint.py",
          "airflow/api_connexion/endpoints/pool_endpoint.py",
          "airflow/api_connexion/endpoints/provider_endpoint.py",
          "airflow/api_connexion/endpoints/task_endpoint.py",
          "airflow/api_connexion/endpoints/task_instance_endpoint.py",
          "airflow/api_connexion/endpoints/variable_endpoint.py",
          "airflow/api_connexion/endpoints/xcom_endpoint.py",
          "airflow/api_connexion/security.py",
          "airflow/auth/managers/base_auth_manager.py",
          "airflow/auth/managers/fab/decorators/auth.py",
          "airflow/auth/managers/fab/fab_auth_manager.py",
          "airflow/auth/managers/fab/security_manager/override.py",
          "airflow/auth/managers/models/resource_details.py",
          "airflow/www/auth.py",
          "airflow/www/extensions/init_jinja_globals.py",
          "airflow/www/security_manager.py",
          "airflow/www/templates/airflow/dag.html",
          "airflow/www/views.py",
          "tests/api_connexion/endpoints/test_event_log_endpoint.py",
          "tests/api_connexion/endpoints/test_log_endpoint.py",
          "tests/api_connexion/endpoints/test_xcom_endpoint.py",
          "tests/auth/managers/fab/test_fab_auth_manager.py",
          "tests/auth/managers/test_base_auth_manager.py",
          "tests/www/test_security.py",
          "tests/www/views/test_views_acl.py",
          "tests/www/views/test_views_decorators.py",
          "tests/www/views/test_views_tasks.py"
        ],
        "message": "Use auth manager `is_authorized_` APIs to check user permissions in Rest API (#34317)",
        "before_after_code_files": [
          "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py",
          "airflow/api_connexion/endpoints/connection_endpoint.py||airflow/api_connexion/endpoints/connection_endpoint.py",
          "airflow/api_connexion/endpoints/dag_endpoint.py||airflow/api_connexion/endpoints/dag_endpoint.py",
          "airflow/api_connexion/endpoints/dag_run_endpoint.py||airflow/api_connexion/endpoints/dag_run_endpoint.py",
          "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
          "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py",
          "airflow/api_connexion/endpoints/dataset_endpoint.py||airflow/api_connexion/endpoints/dataset_endpoint.py",
          "airflow/api_connexion/endpoints/event_log_endpoint.py||airflow/api_connexion/endpoints/event_log_endpoint.py",
          "airflow/api_connexion/endpoints/extra_link_endpoint.py||airflow/api_connexion/endpoints/extra_link_endpoint.py",
          "airflow/api_connexion/endpoints/import_error_endpoint.py||airflow/api_connexion/endpoints/import_error_endpoint.py",
          "airflow/api_connexion/endpoints/log_endpoint.py||airflow/api_connexion/endpoints/log_endpoint.py",
          "airflow/api_connexion/endpoints/plugin_endpoint.py||airflow/api_connexion/endpoints/plugin_endpoint.py",
          "airflow/api_connexion/endpoints/pool_endpoint.py||airflow/api_connexion/endpoints/pool_endpoint.py",
          "airflow/api_connexion/endpoints/provider_endpoint.py||airflow/api_connexion/endpoints/provider_endpoint.py",
          "airflow/api_connexion/endpoints/task_endpoint.py||airflow/api_connexion/endpoints/task_endpoint.py",
          "airflow/api_connexion/endpoints/task_instance_endpoint.py||airflow/api_connexion/endpoints/task_instance_endpoint.py",
          "airflow/api_connexion/endpoints/variable_endpoint.py||airflow/api_connexion/endpoints/variable_endpoint.py",
          "airflow/api_connexion/endpoints/xcom_endpoint.py||airflow/api_connexion/endpoints/xcom_endpoint.py",
          "airflow/api_connexion/security.py||airflow/api_connexion/security.py",
          "airflow/auth/managers/base_auth_manager.py||airflow/auth/managers/base_auth_manager.py",
          "airflow/auth/managers/fab/decorators/auth.py||airflow/auth/managers/fab/decorators/auth.py",
          "airflow/auth/managers/fab/fab_auth_manager.py||airflow/auth/managers/fab/fab_auth_manager.py",
          "airflow/auth/managers/fab/security_manager/override.py||airflow/auth/managers/fab/security_manager/override.py",
          "airflow/auth/managers/models/resource_details.py||airflow/auth/managers/models/resource_details.py",
          "airflow/www/auth.py||airflow/www/auth.py",
          "airflow/www/extensions/init_jinja_globals.py||airflow/www/extensions/init_jinja_globals.py",
          "airflow/www/security_manager.py||airflow/www/security_manager.py",
          "airflow/www/templates/airflow/dag.html||airflow/www/templates/airflow/dag.html",
          "airflow/www/views.py||airflow/www/views.py",
          "tests/api_connexion/endpoints/test_event_log_endpoint.py||tests/api_connexion/endpoints/test_event_log_endpoint.py",
          "tests/api_connexion/endpoints/test_log_endpoint.py||tests/api_connexion/endpoints/test_log_endpoint.py",
          "tests/api_connexion/endpoints/test_xcom_endpoint.py||tests/api_connexion/endpoints/test_xcom_endpoint.py",
          "tests/auth/managers/fab/test_fab_auth_manager.py||tests/auth/managers/fab/test_fab_auth_manager.py",
          "tests/auth/managers/test_base_auth_manager.py||tests/auth/managers/test_base_auth_manager.py",
          "tests/www/test_security.py||tests/www/test_security.py",
          "tests/www/views/test_views_acl.py||tests/www/views/test_views_acl.py",
          "tests/www/views/test_views_decorators.py||tests/www/views/test_views_decorators.py",
          "tests/www/views/test_views_tasks.py||tests/www/views/test_views_tasks.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py"
          ],
          "candidate": [
            "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py": [
          "File: airflow/api_connexion/endpoints/config_endpoint.py -> airflow/api_connexion/endpoints/config_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: from airflow.api_connexion.exceptions import NotFound, PermissionDenied",
          "25: from airflow.api_connexion.schemas.config_schema import Config, ConfigOption, ConfigSection, config_schema",
          "26: from airflow.configuration import conf",
          "28: from airflow.settings import json",
          "30: LINE_SEP = \"\\n\"  # `\\n` cannot appear in f-strings",
          "",
          "[Removed Lines]",
          "27: from airflow.security import permissions",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "66:     return json.dumps(config_schema.dump(config), indent=4)",
          "70: def get_config(*, section: str | None = None) -> Response:",
          "71:     \"\"\"Get current configuration.\"\"\"",
          "72:     serializer = {",
          "",
          "[Removed Lines]",
          "69: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_CONFIG)])",
          "",
          "[Added Lines]",
          "68: @security.requires_access_configuration(\"GET\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "103:         )",
          "108:     serializer = {",
          "109:         \"text/plain\": _config_to_text,",
          "110:         \"application/json\": _config_to_json,",
          "",
          "[Removed Lines]",
          "106: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_CONFIG)])",
          "107: def get_value(section: str, option: str) -> Response:",
          "",
          "[Added Lines]",
          "105: @security.requires_access_configuration(\"GET\")",
          "106: def get_value(*, section: str, option: str) -> Response:",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/connection_endpoint.py||airflow/api_connexion/endpoints/connection_endpoint.py": [
          "File: airflow/api_connexion/endpoints/connection_endpoint.py -> airflow/api_connexion/endpoints/connection_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "53: RESOURCE_EVENT_PREFIX = \"connection\"",
          "57: @provide_session",
          "58: @action_logging(",
          "59:     event=action_event_from_permission(",
          "",
          "[Removed Lines]",
          "56: @security.requires_access([(permissions.ACTION_CAN_DELETE, permissions.RESOURCE_CONNECTION)])",
          "",
          "[Added Lines]",
          "56: @security.requires_access_connection(\"DELETE\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "73:     return NoContent, HTTPStatus.NO_CONTENT",
          "77: @provide_session",
          "78: def get_connection(*, connection_id: str, session: Session = NEW_SESSION) -> APIResponse:",
          "79:     \"\"\"Get a connection entry.\"\"\"",
          "",
          "[Removed Lines]",
          "76: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_CONNECTION)])",
          "",
          "[Added Lines]",
          "76: @security.requires_access_connection(\"GET\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "86:     return connection_schema.dump(connection)",
          "90: @format_parameters({\"limit\": check_limit})",
          "91: @provide_session",
          "92: def get_connections(",
          "",
          "[Removed Lines]",
          "89: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_CONNECTION)])",
          "",
          "[Added Lines]",
          "89: @security.requires_access_connection(\"GET\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "109:     )",
          "113: @provide_session",
          "114: @action_logging(",
          "115:     event=action_event_from_permission(",
          "",
          "[Removed Lines]",
          "112: @security.requires_access([(permissions.ACTION_CAN_EDIT, permissions.RESOURCE_CONNECTION)])",
          "",
          "[Added Lines]",
          "112: @security.requires_access_connection(\"PUT\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "147:     return connection_schema.dump(connection)",
          "151: @provide_session",
          "152: @action_logging(",
          "153:     event=action_event_from_permission(",
          "",
          "[Removed Lines]",
          "150: @security.requires_access([(permissions.ACTION_CAN_CREATE, permissions.RESOURCE_CONNECTION)])",
          "",
          "[Added Lines]",
          "150: @security.requires_access_connection(\"POST\")",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "176:     raise AlreadyExists(detail=f\"Connection already exist. ID: {conn_id}\")",
          "180: def test_connection() -> APIResponse:",
          "181:     \"\"\"",
          "182:     Test an API connection.",
          "",
          "[Removed Lines]",
          "179: @security.requires_access([(permissions.ACTION_CAN_CREATE, permissions.RESOURCE_CONNECTION)])",
          "",
          "[Added Lines]",
          "179: @security.requires_access_connection(\"POST\")",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/dag_endpoint.py||airflow/api_connexion/endpoints/dag_endpoint.py": [
          "File: airflow/api_connexion/endpoints/dag_endpoint.py -> airflow/api_connexion/endpoints/dag_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "36: )",
          "37: from airflow.exceptions import AirflowException, DagNotFound",
          "38: from airflow.models.dag import DagModel, DagTag",
          "40: from airflow.utils.airflow_flask_app import get_airflow_app",
          "41: from airflow.utils.db import get_query_count",
          "42: from airflow.utils.session import NEW_SESSION, provide_session",
          "44: if TYPE_CHECKING:",
          "45:     from sqlalchemy.orm import Session",
          "",
          "[Removed Lines]",
          "39: from airflow.security import permissions",
          "",
          "[Added Lines]",
          "42: from airflow.www.extensions.init_auth_manager import get_auth_manager",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "48:     from airflow.api_connexion.types import APIResponse, UpdateMask",
          "52: @provide_session",
          "53: def get_dag(*, dag_id: str, session: Session = NEW_SESSION) -> APIResponse:",
          "54:     \"\"\"Get basic information about a DAG.\"\"\"",
          "",
          "[Removed Lines]",
          "51: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG)])",
          "",
          "[Added Lines]",
          "51: @security.requires_access_dag(\"GET\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "60:     return dag_schema.dump(dag)",
          "64: def get_dag_details(*, dag_id: str) -> APIResponse:",
          "65:     \"\"\"Get details of DAG.\"\"\"",
          "66:     dag: DAG = get_airflow_app().dag_bag.get_dag(dag_id)",
          "",
          "[Removed Lines]",
          "63: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG)])",
          "",
          "[Added Lines]",
          "63: @security.requires_access_dag(\"GET\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "69:     return dag_detail_schema.dump(dag)",
          "73: @format_parameters({\"limit\": check_limit})",
          "74: @provide_session",
          "75: def get_dags(",
          "",
          "[Removed Lines]",
          "72: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG)])",
          "",
          "[Added Lines]",
          "72: @security.requires_access_dag(\"GET\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "96:     if dag_id_pattern:",
          "97:         dags_query = dags_query.where(DagModel.dag_id.ilike(f\"%{dag_id_pattern}%\"))",
          "101:     dags_query = dags_query.where(DagModel.dag_id.in_(readable_dags))",
          "102:     if tags:",
          "",
          "[Removed Lines]",
          "99:     readable_dags = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
          "",
          "[Added Lines]",
          "99:     readable_dags = get_auth_manager().get_permitted_dag_ids(user=g.user)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "110:     return dags_collection_schema.dump(DAGCollection(dags=dags, total_entries=total_entries))",
          "114: @provide_session",
          "115: def patch_dag(*, dag_id: str, update_mask: UpdateMask = None, session: Session = NEW_SESSION) -> APIResponse:",
          "116:     \"\"\"Update the specific DAG.\"\"\"",
          "",
          "[Removed Lines]",
          "113: @security.requires_access([(permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG)])",
          "",
          "[Added Lines]",
          "113: @security.requires_access_dag(\"PUT\")",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "132:     return dag_schema.dump(dag)",
          "136: @format_parameters({\"limit\": check_limit})",
          "137: @provide_session",
          "138: def patch_dags(limit, session, offset=0, only_active=True, tags=None, dag_id_pattern=None, update_mask=None):",
          "",
          "[Removed Lines]",
          "135: @security.requires_access([(permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG)])",
          "",
          "[Added Lines]",
          "135: @security.requires_access_dag(\"PUT\")",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "156:     if dag_id_pattern == \"~\":",
          "157:         dag_id_pattern = \"%\"",
          "158:     dags_query = dags_query.where(DagModel.dag_id.ilike(f\"%{dag_id_pattern}%\"))",
          "161:     dags_query = dags_query.where(DagModel.dag_id.in_(editable_dags))",
          "162:     if tags:",
          "",
          "[Removed Lines]",
          "159:     editable_dags = get_airflow_app().appbuilder.sm.get_editable_dag_ids(g.user)",
          "",
          "[Added Lines]",
          "159:     editable_dags = get_auth_manager().get_permitted_dag_ids(methods=[\"PUT\"], user=g.user)",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "180:     return dags_collection_schema.dump(DAGCollection(dags=dags, total_entries=total_entries))",
          "184: @provide_session",
          "185: def delete_dag(dag_id: str, session: Session = NEW_SESSION) -> APIResponse:",
          "186:     \"\"\"Delete the specific DAG.\"\"\"",
          "",
          "[Removed Lines]",
          "183: @security.requires_access([(permissions.ACTION_CAN_DELETE, permissions.RESOURCE_DAG)])",
          "",
          "[Added Lines]",
          "183: @security.requires_access_dag(\"DELETE\")",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/dag_run_endpoint.py||airflow/api_connexion/endpoints/dag_run_endpoint.py": [
          "File: airflow/api_connexion/endpoints/dag_run_endpoint.py -> airflow/api_connexion/endpoints/dag_run_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "56:     TaskInstanceReferenceCollection,",
          "57:     task_instance_reference_collection_schema,",
          "58: )",
          "59: from airflow.models import DagModel, DagRun",
          "60: from airflow.security import permissions",
          "61: from airflow.utils.airflow_flask_app import get_airflow_app",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "59: from airflow.auth.managers.models.resource_details import DagAccessEntity",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "76: RESOURCE_EVENT_PREFIX = \"dag_run\"",
          "85: @provide_session",
          "86: def delete_dag_run(*, dag_id: str, dag_run_id: str, session: Session = NEW_SESSION) -> APIResponse:",
          "87:     \"\"\"Delete a DAG Run.\"\"\"",
          "",
          "[Removed Lines]",
          "79: @security.requires_access(",
          "80:     [",
          "81:         (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),",
          "82:         (permissions.ACTION_CAN_DELETE, permissions.RESOURCE_DAG_RUN),",
          "83:     ],",
          "84: )",
          "",
          "[Added Lines]",
          "80: @security.requires_access_dag(\"DELETE\", DagAccessEntity.RUN)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "93:     return NoContent, HTTPStatus.NO_CONTENT",
          "102: @provide_session",
          "103: def get_dag_run(*, dag_id: str, dag_run_id: str, session: Session = NEW_SESSION) -> APIResponse:",
          "104:     \"\"\"Get a DAG Run.\"\"\"",
          "",
          "[Removed Lines]",
          "96: @security.requires_access(",
          "97:     [",
          "98:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "99:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "100:     ],",
          "101: )",
          "",
          "[Added Lines]",
          "92: @security.requires_access_dag(\"GET\", DagAccessEntity.RUN)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "111:     return dagrun_schema.dump(dag_run)",
          "121: @provide_session",
          "122: def get_upstream_dataset_events(",
          "",
          "[Removed Lines]",
          "114: @security.requires_access(",
          "115:     [",
          "116:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "117:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "118:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DATASET),",
          "119:     ],",
          "120: )",
          "",
          "[Added Lines]",
          "105: @security.requires_access_dag(\"GET\", DagAccessEntity.RUN)",
          "106: @security.requires_access_dataset(\"GET\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "194:     return session.scalars(query.offset(offset).limit(limit)).all(), total_entries",
          "203: @format_parameters(",
          "204:     {",
          "205:         \"start_date_gte\": format_datetime,",
          "",
          "[Removed Lines]",
          "197: @security.requires_access(",
          "198:     [",
          "199:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "200:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "201:     ],",
          "202: )",
          "",
          "[Added Lines]",
          "183: @security.requires_access_dag(\"GET\", DagAccessEntity.RUN)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "237:     #  This endpoint allows specifying ~ as the dag_id to retrieve DAG Runs for all DAGs.",
          "238:     if dag_id == \"~\":",
          "241:     else:",
          "242:         query = query.where(DagRun.dag_id == dag_id)",
          "",
          "[Removed Lines]",
          "239:         appbuilder = get_airflow_app().appbuilder",
          "240:         query = query.where(DagRun.dag_id.in_(appbuilder.sm.get_readable_dag_ids(g.user)))",
          "",
          "[Added Lines]",
          "220:         query = query.where(",
          "221:             DagRun.dag_id.in_(get_auth_manager().get_permitted_dag_ids(methods=[\"GET\"], user=g.user))",
          "222:         )",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "262:     return dagrun_collection_schema.dump(DAGRunCollection(dag_runs=dag_run, total_entries=total_entries))",
          "271: @provide_session",
          "272: def get_dag_runs_batch(*, session: Session = NEW_SESSION) -> APIResponse:",
          "273:     \"\"\"Get list of DAG Runs.\"\"\"",
          "",
          "[Removed Lines]",
          "265: @security.requires_access(",
          "266:     [",
          "267:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "268:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "269:     ],",
          "270: )",
          "",
          "[Added Lines]",
          "247: @security.requires_access_dag(\"GET\", DagAccessEntity.RUN)",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "277:     except ValidationError as err:",
          "278:         raise BadRequest(detail=str(err.messages))",
          "282:     query = select(DagRun)",
          "283:     if data.get(\"dag_ids\"):",
          "284:         dag_ids = set(data[\"dag_ids\"]) & set(readable_dag_ids)",
          "",
          "[Removed Lines]",
          "280:     appbuilder = get_airflow_app().appbuilder",
          "281:     readable_dag_ids = appbuilder.sm.get_readable_dag_ids(g.user)",
          "",
          "[Added Lines]",
          "257:     readable_dag_ids = get_auth_manager().get_permitted_dag_ids(methods=[\"GET\"], user=g.user)",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "307:     return dagrun_collection_schema.dump(DAGRunCollection(dag_runs=dag_runs, total_entries=total_entries))",
          "316: @provide_session",
          "317: @action_logging(",
          "318:     event=action_event_from_permission(",
          "",
          "[Removed Lines]",
          "310: @security.requires_access(",
          "311:     [",
          "312:         (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),",
          "313:         (permissions.ACTION_CAN_CREATE, permissions.RESOURCE_DAG_RUN),",
          "314:     ],",
          "315: )",
          "",
          "[Added Lines]",
          "286: @security.requires_access_dag(\"POST\", DagAccessEntity.RUN)",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "378:     raise AlreadyExists(detail=f\"DAGRun with DAG ID: '{dag_id}' and DAGRun ID: '{run_id}' already exists\")",
          "387: @provide_session",
          "388: def update_dag_run_state(*, dag_id: str, dag_run_id: str, session: Session = NEW_SESSION) -> APIResponse:",
          "389:     \"\"\"Set a state of a dag run.\"\"\"",
          "",
          "[Removed Lines]",
          "381: @security.requires_access(",
          "382:     [",
          "383:         (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),",
          "384:         (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG_RUN),",
          "385:     ],",
          "386: )",
          "",
          "[Added Lines]",
          "352: @security.requires_access_dag(\"PUT\", DagAccessEntity.RUN)",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "410:     return dagrun_schema.dump(dag_run)",
          "419: @provide_session",
          "420: def clear_dag_run(*, dag_id: str, dag_run_id: str, session: Session = NEW_SESSION) -> APIResponse:",
          "421:     \"\"\"Clear a dag run.\"\"\"",
          "",
          "[Removed Lines]",
          "413: @security.requires_access(",
          "414:     [",
          "415:         (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),",
          "416:         (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG_RUN),",
          "417:     ],",
          "418: )",
          "",
          "[Added Lines]",
          "379: @security.requires_access_dag(\"PUT\", DagAccessEntity.RUN)",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "461:         return dagrun_schema.dump(dag_run)",
          "470: @provide_session",
          "471: def set_dag_run_note(*, dag_id: str, dag_run_id: str, session: Session = NEW_SESSION) -> APIResponse:",
          "472:     \"\"\"Set the note for a dag run.\"\"\"",
          "",
          "[Removed Lines]",
          "464: @security.requires_access(",
          "465:     [",
          "466:         (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),",
          "467:         (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG_RUN),",
          "468:     ],",
          "469: )",
          "",
          "[Added Lines]",
          "425: @security.requires_access_dag(\"PUT\", DagAccessEntity.RUN)",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py": [
          "File: airflow/api_connexion/endpoints/dag_source_endpoint.py -> airflow/api_connexion/endpoints/dag_source_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: from airflow.api_connexion import security",
          "25: from airflow.api_connexion.exceptions import NotFound",
          "26: from airflow.api_connexion.schemas.dag_source_schema import dag_source_schema",
          "27: from airflow.models.dagcode import DagCode",
          "32: def get_dag_source(*, file_token: str) -> Response:",
          "33:     \"\"\"Get source code using file token.\"\"\"",
          "34:     secret_key = current_app.config[\"SECRET_KEY\"]",
          "",
          "[Removed Lines]",
          "28: from airflow.security import permissions",
          "31: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_CODE)])",
          "",
          "[Added Lines]",
          "27: from airflow.auth.managers.models.resource_details import DagAccessEntity",
          "31: @security.requires_access_dag(\"GET\", DagAccessEntity.CODE)",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py": [
          "File: airflow/api_connexion/endpoints/dag_warning_endpoint.py -> airflow/api_connexion/endpoints/dag_warning_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27:     DagWarningCollection,",
          "28:     dag_warning_collection_schema,",
          "29: )",
          "30: from airflow.models.dagwarning import DagWarning as DagWarningModel",
          "32: from airflow.utils.airflow_flask_app import get_airflow_app",
          "33: from airflow.utils.db import get_query_count",
          "34: from airflow.utils.session import NEW_SESSION, provide_session",
          "",
          "[Removed Lines]",
          "31: from airflow.security import permissions",
          "",
          "[Added Lines]",
          "30: from airflow.auth.managers.models.resource_details import DagAccessEntity",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39:     from airflow.api_connexion.types import APIResponse",
          "48: @format_parameters({\"limit\": check_limit})",
          "49: @provide_session",
          "50: def get_dag_warnings(",
          "",
          "[Removed Lines]",
          "42: @security.requires_access(",
          "43:     [",
          "44:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "45:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
          "46:     ]",
          "47: )",
          "",
          "[Added Lines]",
          "42: @security.requires_access_dag(\"GET\", DagAccessEntity.WARNING)",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/dataset_endpoint.py||airflow/api_connexion/endpoints/dataset_endpoint.py": [
          "File: airflow/api_connexion/endpoints/dataset_endpoint.py -> airflow/api_connexion/endpoints/dataset_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "32:     dataset_schema,",
          "33: )",
          "34: from airflow.models.dataset import DatasetEvent, DatasetModel",
          "36: from airflow.utils.db import get_query_count",
          "37: from airflow.utils.session import NEW_SESSION, provide_session",
          "",
          "[Removed Lines]",
          "35: from airflow.security import permissions",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "42:     from airflow.api_connexion.types import APIResponse",
          "46: @provide_session",
          "48:     \"\"\"Get a Dataset.\"\"\"",
          "49:     dataset = session.scalar(",
          "50:         select(DatasetModel)",
          "",
          "[Removed Lines]",
          "45: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_DATASET)])",
          "47: def get_dataset(uri: str, session: Session = NEW_SESSION) -> APIResponse:",
          "",
          "[Added Lines]",
          "44: @security.requires_access_dataset(\"GET\")",
          "46: def get_dataset(*, uri: str, session: Session = NEW_SESSION) -> APIResponse:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "59:     return dataset_schema.dump(dataset)",
          "63: @format_parameters({\"limit\": check_limit})",
          "64: @provide_session",
          "65: def get_datasets(",
          "",
          "[Removed Lines]",
          "62: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_DATASET)])",
          "",
          "[Added Lines]",
          "61: @security.requires_access_dataset(\"GET\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "86:     return dataset_collection_schema.dump(DatasetCollection(datasets=datasets, total_entries=total_entries))",
          "90: @provide_session",
          "91: @format_parameters({\"limit\": check_limit})",
          "92: def get_dataset_events(",
          "",
          "[Removed Lines]",
          "89: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_DATASET)])",
          "",
          "[Added Lines]",
          "88: @security.requires_access_dataset(\"GET\")",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/event_log_endpoint.py||airflow/api_connexion/endpoints/event_log_endpoint.py": [
          "File: airflow/api_connexion/endpoints/event_log_endpoint.py -> airflow/api_connexion/endpoints/event_log_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "28:     event_log_collection_schema,",
          "29:     event_log_schema,",
          "30: )",
          "31: from airflow.models import Log",
          "33: from airflow.utils import timezone",
          "34: from airflow.utils.session import NEW_SESSION, provide_session",
          "",
          "[Removed Lines]",
          "32: from airflow.security import permissions",
          "",
          "[Added Lines]",
          "31: from airflow.auth.managers.models.resource_details import DagAccessEntity",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "40:     from airflow.api_connexion.types import APIResponse",
          "44: @provide_session",
          "45: def get_event_log(*, event_log_id: int, session: Session = NEW_SESSION) -> APIResponse:",
          "46:     \"\"\"Get a log entry.\"\"\"",
          "",
          "[Removed Lines]",
          "43: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_AUDIT_LOG)])",
          "",
          "[Added Lines]",
          "43: @security.requires_access_dag(\"GET\", DagAccessEntity.AUDIT_LOG)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "50:     return event_log_schema.dump(event_log)",
          "54: @format_parameters({\"limit\": check_limit})",
          "55: @provide_session",
          "56: def get_event_logs(",
          "",
          "[Removed Lines]",
          "53: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_AUDIT_LOG)])",
          "",
          "[Added Lines]",
          "53: @security.requires_access_dag(\"GET\", DagAccessEntity.AUDIT_LOG)",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/extra_link_endpoint.py||airflow/api_connexion/endpoints/extra_link_endpoint.py": [
          "File: airflow/api_connexion/endpoints/extra_link_endpoint.py -> airflow/api_connexion/endpoints/extra_link_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: from airflow.api_connexion import security",
          "24: from airflow.api_connexion.exceptions import NotFound",
          "25: from airflow.exceptions import TaskNotFound",
          "27: from airflow.utils.airflow_flask_app import get_airflow_app",
          "28: from airflow.utils.session import NEW_SESSION, provide_session",
          "",
          "[Removed Lines]",
          "26: from airflow.security import permissions",
          "",
          "[Added Lines]",
          "25: from airflow.auth.managers.models.resource_details import DagAccessEntity",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "35:     from airflow.models.dagbag import DagBag",
          "45: @provide_session",
          "46: def get_extra_links(",
          "",
          "[Removed Lines]",
          "38: @security.requires_access(",
          "39:     [",
          "40:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "41:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "42:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_INSTANCE),",
          "43:     ],",
          "44: )",
          "",
          "[Added Lines]",
          "38: @security.requires_access_dag(\"GET\", DagAccessEntity.TASK_INSTANCE)",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/import_error_endpoint.py||airflow/api_connexion/endpoints/import_error_endpoint.py": [
          "File: airflow/api_connexion/endpoints/import_error_endpoint.py -> airflow/api_connexion/endpoints/import_error_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "28:     import_error_collection_schema,",
          "29:     import_error_schema,",
          "30: )",
          "31: from airflow.models.errors import ImportError as ImportErrorModel",
          "33: from airflow.utils.session import NEW_SESSION, provide_session",
          "35: if TYPE_CHECKING:",
          "",
          "[Removed Lines]",
          "32: from airflow.security import permissions",
          "",
          "[Added Lines]",
          "31: from airflow.auth.managers.models.resource_details import DagAccessEntity",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "38:     from airflow.api_connexion.types import APIResponse",
          "42: @provide_session",
          "43: def get_import_error(*, import_error_id: int, session: Session = NEW_SESSION) -> APIResponse:",
          "44:     \"\"\"Get an import error.\"\"\"",
          "",
          "[Removed Lines]",
          "41: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_IMPORT_ERROR)])",
          "",
          "[Added Lines]",
          "41: @security.requires_access_dag(\"GET\", DagAccessEntity.IMPORT_ERRORS)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "52:     return import_error_schema.dump(error)",
          "56: @format_parameters({\"limit\": check_limit})",
          "57: @provide_session",
          "58: def get_import_errors(",
          "",
          "[Removed Lines]",
          "55: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_IMPORT_ERROR)])",
          "",
          "[Added Lines]",
          "55: @security.requires_access_dag(\"GET\", DagAccessEntity.IMPORT_ERRORS)",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/log_endpoint.py||airflow/api_connexion/endpoints/log_endpoint.py": [
          "File: airflow/api_connexion/endpoints/log_endpoint.py -> airflow/api_connexion/endpoints/log_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: from airflow.api_connexion import security",
          "28: from airflow.api_connexion.exceptions import BadRequest, NotFound",
          "29: from airflow.api_connexion.schemas.log_schema import LogResponseObject, logs_schema",
          "30: from airflow.exceptions import TaskNotFound",
          "31: from airflow.models import TaskInstance, Trigger",
          "33: from airflow.utils.airflow_flask_app import get_airflow_app",
          "34: from airflow.utils.log.log_reader import TaskLogReader",
          "35: from airflow.utils.session import NEW_SESSION, provide_session",
          "",
          "[Removed Lines]",
          "32: from airflow.security import permissions",
          "",
          "[Added Lines]",
          "30: from airflow.auth.managers.models.resource_details import DagAccessEntity",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "40:     from airflow.api_connexion.types import APIResponse",
          "50: @provide_session",
          "51: def get_log(",
          "",
          "[Removed Lines]",
          "43: @security.requires_access(",
          "44:     [",
          "45:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "46:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "47:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_INSTANCE),",
          "48:     ],",
          "49: )",
          "",
          "[Added Lines]",
          "43: @security.requires_access_dag(\"GET\", DagAccessEntity.TASK_LOGS)",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/plugin_endpoint.py||airflow/api_connexion/endpoints/plugin_endpoint.py": [
          "File: airflow/api_connexion/endpoints/plugin_endpoint.py -> airflow/api_connexion/endpoints/plugin_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: from airflow.api_connexion.parameters import check_limit, format_parameters",
          "23: from airflow.api_connexion.schemas.plugin_schema import PluginCollection, plugin_collection_schema",
          "24: from airflow.plugins_manager import get_plugin_info",
          "27: if TYPE_CHECKING:",
          "28:     from airflow.api_connexion.types import APIResponse",
          "32: @format_parameters({\"limit\": check_limit})",
          "33: def get_plugins(*, limit: int, offset: int = 0) -> APIResponse:",
          "34:     \"\"\"Get plugins endpoint.\"\"\"",
          "",
          "[Removed Lines]",
          "25: from airflow.security import permissions",
          "31: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_PLUGIN)])",
          "",
          "[Added Lines]",
          "30: @security.requires_access_website()",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/pool_endpoint.py||airflow/api_connexion/endpoints/pool_endpoint.py": [
          "File: airflow/api_connexion/endpoints/pool_endpoint.py -> airflow/api_connexion/endpoints/pool_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: from airflow.api_connexion.parameters import apply_sorting, check_limit, format_parameters",
          "31: from airflow.api_connexion.schemas.pool_schema import PoolCollection, pool_collection_schema, pool_schema",
          "32: from airflow.models.pool import Pool",
          "34: from airflow.utils.session import NEW_SESSION, provide_session",
          "36: if TYPE_CHECKING:",
          "",
          "[Removed Lines]",
          "33: from airflow.security import permissions",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39:     from airflow.api_connexion.types import APIResponse, UpdateMask",
          "43: @provide_session",
          "44: def delete_pool(*, pool_name: str, session: Session = NEW_SESSION) -> APIResponse:",
          "45:     \"\"\"Delete a pool.\"\"\"",
          "",
          "[Removed Lines]",
          "42: @security.requires_access([(permissions.ACTION_CAN_DELETE, permissions.RESOURCE_POOL)])",
          "",
          "[Added Lines]",
          "41: @security.requires_access_pool(\"DELETE\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "52:     return Response(status=HTTPStatus.NO_CONTENT)",
          "56: @provide_session",
          "57: def get_pool(*, pool_name: str, session: Session = NEW_SESSION) -> APIResponse:",
          "58:     \"\"\"Get a pool.\"\"\"",
          "",
          "[Removed Lines]",
          "55: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_POOL)])",
          "",
          "[Added Lines]",
          "54: @security.requires_access_pool(\"GET\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "62:     return pool_schema.dump(obj)",
          "66: @format_parameters({\"limit\": check_limit})",
          "67: @provide_session",
          "68: def get_pools(",
          "",
          "[Removed Lines]",
          "65: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_POOL)])",
          "",
          "[Added Lines]",
          "64: @security.requires_access_pool(\"GET\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "82:     return pool_collection_schema.dump(PoolCollection(pools=pools, total_entries=total_entries))",
          "86: @provide_session",
          "87: def patch_pool(",
          "",
          "[Removed Lines]",
          "85: @security.requires_access([(permissions.ACTION_CAN_EDIT, permissions.RESOURCE_POOL)])",
          "",
          "[Added Lines]",
          "84: @security.requires_access_pool(\"PUT\")",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "138:     return pool_schema.dump(pool)",
          "142: @provide_session",
          "143: def post_pool(*, session: Session = NEW_SESSION) -> APIResponse:",
          "144:     \"\"\"Create a pool.\"\"\"",
          "",
          "[Removed Lines]",
          "141: @security.requires_access([(permissions.ACTION_CAN_CREATE, permissions.RESOURCE_POOL)])",
          "",
          "[Added Lines]",
          "140: @security.requires_access_pool(\"POST\")",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/provider_endpoint.py||airflow/api_connexion/endpoints/provider_endpoint.py": [
          "File: airflow/api_connexion/endpoints/provider_endpoint.py -> airflow/api_connexion/endpoints/provider_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27:     provider_collection_schema,",
          "28: )",
          "29: from airflow.providers_manager import ProvidersManager",
          "32: if TYPE_CHECKING:",
          "33:     from airflow.api_connexion.types import APIResponse",
          "",
          "[Removed Lines]",
          "30: from airflow.security import permissions",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "46:     )",
          "50: def get_providers() -> APIResponse:",
          "51:     \"\"\"Get providers.\"\"\"",
          "52:     providers = [_provider_mapper(d) for d in ProvidersManager().providers.values()]",
          "",
          "[Removed Lines]",
          "49: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_PROVIDER)])",
          "",
          "[Added Lines]",
          "48: @security.requires_access_website()",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/task_endpoint.py||airflow/api_connexion/endpoints/task_endpoint.py": [
          "File: airflow/api_connexion/endpoints/task_endpoint.py -> airflow/api_connexion/endpoints/task_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: from airflow.api_connexion import security",
          "23: from airflow.api_connexion.exceptions import BadRequest, NotFound",
          "24: from airflow.api_connexion.schemas.task_schema import TaskCollection, task_collection_schema, task_schema",
          "25: from airflow.exceptions import TaskNotFound",
          "27: from airflow.utils.airflow_flask_app import get_airflow_app",
          "29: if TYPE_CHECKING:",
          "",
          "[Removed Lines]",
          "26: from airflow.security import permissions",
          "",
          "[Added Lines]",
          "25: from airflow.auth.managers.models.resource_details import DagAccessEntity",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "31:     from airflow.api_connexion.types import APIResponse",
          "40: def get_task(*, dag_id: str, task_id: str) -> APIResponse:",
          "41:     \"\"\"Get simplified representation of a task.\"\"\"",
          "42:     dag: DAG = get_airflow_app().dag_bag.get_dag(dag_id)",
          "",
          "[Removed Lines]",
          "34: @security.requires_access(",
          "35:     [",
          "36:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "37:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_INSTANCE),",
          "38:     ],",
          "39: )",
          "",
          "[Added Lines]",
          "34: @security.requires_access_dag(\"GET\", DagAccessEntity.TASK)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "50:     return task_schema.dump(task)",
          "59: def get_tasks(*, dag_id: str, order_by: str = \"task_id\") -> APIResponse:",
          "60:     \"\"\"Get tasks for DAG.\"\"\"",
          "61:     dag: DAG = get_airflow_app().dag_bag.get_dag(dag_id)",
          "",
          "[Removed Lines]",
          "53: @security.requires_access(",
          "54:     [",
          "55:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "56:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_INSTANCE),",
          "57:     ],",
          "58: )",
          "",
          "[Added Lines]",
          "48: @security.requires_access_dag(\"GET\", DagAccessEntity.TASK)",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/task_instance_endpoint.py||airflow/api_connexion/endpoints/task_instance_endpoint.py": [
          "File: airflow/api_connexion/endpoints/task_instance_endpoint.py -> airflow/api_connexion/endpoints/task_instance_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "42:     task_instance_schema,",
          "43: )",
          "44: from airflow.api_connexion.security import get_readable_dags",
          "45: from airflow.models import SlaMiss",
          "46: from airflow.models.dagrun import DagRun as DR",
          "47: from airflow.models.operator import needs_expansion",
          "48: from airflow.models.taskinstance import TaskInstance as TI, clear_task_instances",
          "50: from airflow.utils.airflow_flask_app import get_airflow_app",
          "51: from airflow.utils.db import get_query_count",
          "52: from airflow.utils.session import NEW_SESSION, provide_session",
          "",
          "[Removed Lines]",
          "49: from airflow.security import permissions",
          "",
          "[Added Lines]",
          "45: from airflow.auth.managers.models.resource_details import DagAccessEntity, DagDetails",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "62: T = TypeVar(\"T\")",
          "72: @provide_session",
          "73: def get_task_instance(",
          "",
          "[Removed Lines]",
          "65: @security.requires_access(",
          "66:     [",
          "67:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "68:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "69:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_INSTANCE),",
          "70:     ],",
          "71: )",
          "",
          "[Added Lines]",
          "65: @security.requires_access_dag(\"GET\", DagAccessEntity.TASK_INSTANCE)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "110:     return task_instance_schema.dump(task_instance)",
          "120: @provide_session",
          "121: def get_mapped_task_instance(",
          "",
          "[Removed Lines]",
          "113: @security.requires_access(",
          "114:     [",
          "115:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "116:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "117:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_INSTANCE),",
          "118:     ],",
          "119: )",
          "",
          "[Added Lines]",
          "107: @security.requires_access_dag(\"GET\", DagAccessEntity.TASK_INSTANCE)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "162:         \"updated_at_lte\": format_datetime,",
          "163:     },",
          "164: )",
          "172: @provide_session",
          "173: def get_mapped_task_instances(",
          "",
          "[Removed Lines]",
          "165: @security.requires_access(",
          "166:     [",
          "167:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "168:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "169:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_INSTANCE),",
          "170:     ],",
          "171: )",
          "",
          "[Added Lines]",
          "153: @security.requires_access_dag(\"GET\", DagAccessEntity.TASK_INSTANCE)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "306:         \"updated_at_lte\": format_datetime,",
          "307:     },",
          "308: )",
          "316: @provide_session",
          "317: def get_task_instances(",
          "",
          "[Removed Lines]",
          "309: @security.requires_access(",
          "310:     [",
          "311:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "312:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "313:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_INSTANCE),",
          "314:     ],",
          "315: )",
          "",
          "[Added Lines]",
          "291: @security.requires_access_dag(\"GET\", DagAccessEntity.TASK_INSTANCE)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "389:     )",
          "399: @provide_session",
          "400: def get_task_instances_batch(session: Session = NEW_SESSION) -> APIResponse:",
          "401:     \"\"\"Get list of task instances.\"\"\"",
          "",
          "[Removed Lines]",
          "392: @security.requires_access(",
          "393:     [",
          "394:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "395:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "396:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_INSTANCE),",
          "397:     ],",
          "398: )",
          "",
          "[Added Lines]",
          "368: @security.requires_access_dag(\"GET\", DagAccessEntity.TASK_INSTANCE)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "408:     if dag_ids:",
          "409:         cannot_access_dag_ids = set()",
          "410:         for id in dag_ids:",
          "412:                 cannot_access_dag_ids.add(id)",
          "413:         if cannot_access_dag_ids:",
          "414:             raise PermissionDenied(",
          "",
          "[Removed Lines]",
          "411:             if not get_airflow_app().appbuilder.sm.can_read_dag(id, g.user):",
          "",
          "[Added Lines]",
          "381:             if not get_auth_manager().is_authorized_dag(method=\"GET\", details=DagDetails(id=id), user=g.user):",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "464:     )",
          "474: @provide_session",
          "475: def post_clear_task_instances(*, dag_id: str, session: Session = NEW_SESSION) -> APIResponse:",
          "476:     \"\"\"Clear task instances.\"\"\"",
          "",
          "[Removed Lines]",
          "467: @security.requires_access(",
          "468:     [",
          "469:         (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),",
          "470:         (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG_RUN),",
          "471:         (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_TASK_INSTANCE),",
          "472:     ],",
          "473: )",
          "",
          "[Added Lines]",
          "437: @security.requires_access_dag(\"PUT\", DagAccessEntity.TASK_INSTANCE)",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "530:     )",
          "540: @provide_session",
          "541: def post_set_task_instances_state(*, dag_id: str, session: Session = NEW_SESSION) -> APIResponse:",
          "542:     \"\"\"Set a state of task instances.\"\"\"",
          "",
          "[Removed Lines]",
          "533: @security.requires_access(",
          "534:     [",
          "535:         (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),",
          "536:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "537:         (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_TASK_INSTANCE),",
          "538:     ],",
          "539: )",
          "",
          "[Added Lines]",
          "497: @security.requires_access_dag(\"PUT\", DagAccessEntity.TASK_INSTANCE)",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "603:     return set_task_instance_note(dag_id=dag_id, dag_run_id=dag_run_id, task_id=task_id, map_index=map_index)",
          "613: @provide_session",
          "614: def patch_task_instance(",
          "",
          "[Removed Lines]",
          "606: @security.requires_access(",
          "607:     [",
          "608:         (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),",
          "609:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "610:         (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_TASK_INSTANCE),",
          "611:     ],",
          "612: )",
          "",
          "[Added Lines]",
          "564: @security.requires_access_dag(\"PUT\", DagAccessEntity.TASK_INSTANCE)",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "649:     return task_instance_reference_schema.dump(ti)",
          "659: @provide_session",
          "660: def patch_mapped_task_instance(",
          "",
          "[Removed Lines]",
          "652: @security.requires_access(",
          "653:     [",
          "654:         (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),",
          "655:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "656:         (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_TASK_INSTANCE),",
          "657:     ],",
          "658: )",
          "",
          "[Added Lines]",
          "604: @security.requires_access_dag(\"PUT\", DagAccessEntity.TASK_INSTANCE)",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "666:     )",
          "676: @provide_session",
          "677: def set_task_instance_note(",
          "",
          "[Removed Lines]",
          "669: @security.requires_access(",
          "670:     [",
          "671:         (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),",
          "672:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "673:         (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_TASK_INSTANCE),",
          "674:     ],",
          "675: )",
          "",
          "[Added Lines]",
          "615: @security.requires_access_dag(\"PUT\", DagAccessEntity.TASK_INSTANCE)",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/variable_endpoint.py||airflow/api_connexion/endpoints/variable_endpoint.py": [
          "File: airflow/api_connexion/endpoints/variable_endpoint.py -> airflow/api_connexion/endpoints/variable_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "43: RESOURCE_EVENT_PREFIX = \"variable\"",
          "47: @action_logging(",
          "48:     event=action_event_from_permission(",
          "49:         prefix=RESOURCE_EVENT_PREFIX,",
          "",
          "[Removed Lines]",
          "46: @security.requires_access([(permissions.ACTION_CAN_DELETE, permissions.RESOURCE_VARIABLE)])",
          "",
          "[Added Lines]",
          "46: @security.requires_access_variable(\"DELETE\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "57:     return Response(status=HTTPStatus.NO_CONTENT)",
          "61: @provide_session",
          "62: def get_variable(*, variable_key: str, session: Session = NEW_SESSION) -> Response:",
          "63:     \"\"\"Get a variable by key.\"\"\"",
          "",
          "[Removed Lines]",
          "60: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_VARIABLE)])",
          "",
          "[Added Lines]",
          "60: @security.requires_access_variable(\"DELETE\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "67:     return variable_schema.dump(var)",
          "71: @format_parameters({\"limit\": check_limit})",
          "72: @provide_session",
          "73: def get_variables(",
          "",
          "[Removed Lines]",
          "70: @security.requires_access([(permissions.ACTION_CAN_READ, permissions.RESOURCE_VARIABLE)])",
          "",
          "[Added Lines]",
          "70: @security.requires_access_variable(\"GET\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "92:     )",
          "96: @provide_session",
          "97: @action_logging(",
          "98:     event=action_event_from_permission(",
          "",
          "[Removed Lines]",
          "95: @security.requires_access([(permissions.ACTION_CAN_EDIT, permissions.RESOURCE_VARIABLE)])",
          "",
          "[Added Lines]",
          "95: @security.requires_access_variable(\"PUT\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "126:     return variable_schema.dump(variable)",
          "130: @action_logging(",
          "131:     event=action_event_from_permission(",
          "132:         prefix=RESOURCE_EVENT_PREFIX,",
          "",
          "[Removed Lines]",
          "129: @security.requires_access([(permissions.ACTION_CAN_CREATE, permissions.RESOURCE_VARIABLE)])",
          "",
          "[Added Lines]",
          "129: @security.requires_access_variable(\"POST\")",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/xcom_endpoint.py||airflow/api_connexion/endpoints/xcom_endpoint.py": [
          "File: airflow/api_connexion/endpoints/xcom_endpoint.py -> airflow/api_connexion/endpoints/xcom_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: from airflow.api_connexion.exceptions import BadRequest, NotFound",
          "27: from airflow.api_connexion.parameters import check_limit, format_parameters",
          "28: from airflow.api_connexion.schemas.xcom_schema import XComCollection, xcom_collection_schema, xcom_schema",
          "29: from airflow.models import DagRun as DR, XCom",
          "31: from airflow.settings import conf",
          "33: from airflow.utils.db import get_query_count",
          "34: from airflow.utils.session import NEW_SESSION, provide_session",
          "36: if TYPE_CHECKING:",
          "37:     from sqlalchemy.orm import Session",
          "",
          "[Removed Lines]",
          "30: from airflow.security import permissions",
          "32: from airflow.utils.airflow_flask_app import get_airflow_app",
          "",
          "[Added Lines]",
          "29: from airflow.auth.managers.models.resource_details import DagAccessEntity",
          "34: from airflow.www.extensions.init_auth_manager import get_auth_manager",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39:     from airflow.api_connexion.types import APIResponse",
          "50: @format_parameters({\"limit\": check_limit})",
          "51: @provide_session",
          "52: def get_xcom_entries(",
          "",
          "[Removed Lines]",
          "42: @security.requires_access(",
          "43:     [",
          "44:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "45:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "46:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_INSTANCE),",
          "47:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_XCOM),",
          "48:     ],",
          "49: )",
          "",
          "[Added Lines]",
          "42: @security.requires_access_dag(\"GET\", DagAccessEntity.XCOM)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "63:     \"\"\"Get all XCom values.\"\"\"",
          "64:     query = select(XCom)",
          "65:     if dag_id == \"~\":",
          "68:         query = query.where(XCom.dag_id.in_(readable_dag_ids))",
          "69:         query = query.join(DR, and_(XCom.dag_id == DR.dag_id, XCom.run_id == DR.run_id))",
          "70:     else:",
          "",
          "[Removed Lines]",
          "66:         appbuilder = get_airflow_app().appbuilder",
          "67:         readable_dag_ids = appbuilder.sm.get_readable_dag_ids(g.user)",
          "",
          "[Added Lines]",
          "59:         readable_dag_ids = get_auth_manager().get_permitted_dag_ids(methods=[\"GET\"], user=g.user)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "85:     return xcom_collection_schema.dump(XComCollection(xcom_entries=query, total_entries=total_entries))",
          "96: @provide_session",
          "97: def get_xcom_entry(",
          "",
          "[Removed Lines]",
          "88: @security.requires_access(",
          "89:     [",
          "90:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "91:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "92:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_INSTANCE),",
          "93:         (permissions.ACTION_CAN_READ, permissions.RESOURCE_XCOM),",
          "94:     ],",
          "95: )",
          "",
          "[Added Lines]",
          "80: @security.requires_access_dag(\"GET\", DagAccessEntity.XCOM)",
          "",
          "---------------"
        ],
        "airflow/api_connexion/security.py||airflow/api_connexion/security.py": [
          "File: airflow/api_connexion/security.py -> airflow/api_connexion/security.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "19: from functools import wraps",
          "22: from flask import Response, g",
          "24: from airflow.api_connexion.exceptions import PermissionDenied, Unauthenticated",
          "25: from airflow.utils.airflow_flask_app import get_airflow_app",
          "27: T = TypeVar(\"T\", bound=Callable)",
          "",
          "[Removed Lines]",
          "20: from typing import Callable, Sequence, TypeVar, cast",
          "",
          "[Added Lines]",
          "19: import warnings",
          "21: from typing import TYPE_CHECKING, Callable, Sequence, TypeVar, cast",
          "26: from airflow.auth.managers.models.resource_details import (",
          "27:     ConfigurationDetails,",
          "28:     ConnectionDetails,",
          "29:     DagAccessEntity,",
          "30:     DagDetails,",
          "31:     DatasetDetails,",
          "32:     PoolDetails,",
          "33:     VariableDetails,",
          "34: )",
          "35: from airflow.exceptions import RemovedInAirflow3Warning",
          "37: from airflow.www.extensions.init_auth_manager import get_auth_manager",
          "39: if TYPE_CHECKING:",
          "40:     from airflow.auth.managers.base_auth_manager import ResourceMethod",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "41: def requires_access(permissions: Sequence[tuple[str, str]] | None = None) -> Callable[[T], T]:",
          "47:     def requires_access_decorator(func: T):",
          "48:         @wraps(func)",
          "49:         def decorated(*args, **kwargs):",
          "55:         return cast(T, decorated)",
          "",
          "[Removed Lines]",
          "42:     \"\"\"Check current user's permissions against required permissions.\"\"\"",
          "43:     appbuilder = get_airflow_app().appbuilder",
          "44:     if appbuilder.update_perms:",
          "45:         appbuilder.sm.sync_resource_permissions(permissions)",
          "50:             check_authentication()",
          "51:             if appbuilder.sm.check_authorization(permissions, kwargs.get(\"dag_id\")):",
          "52:                 return func(*args, **kwargs)",
          "53:             raise PermissionDenied()",
          "",
          "[Added Lines]",
          "57:     \"\"\"",
          "58:     Check current user's permissions against required permissions.",
          "60:     Deprecated. Do not use this decorator, use one of the decorator `has_access_*` defined in",
          "61:     airflow/api_connexion/security.py instead.",
          "62:     This decorator will only work with FAB authentication and not with other auth providers.",
          "64:     This decorator might be used in user plugins, do not remove it.",
          "65:     \"\"\"",
          "66:     warnings.warn(",
          "67:         \"The 'requires_access' decorator is deprecated. Please use one of the decorator `requires_access_*`\"",
          "68:         \"defined in airflow/api_connexion/security.py instead.\",",
          "69:         RemovedInAirflow3Warning,",
          "70:         stacklevel=2,",
          "71:     )",
          "72:     from airflow.auth.managers.fab.decorators.auth import _requires_access_fab",
          "74:     return _requires_access_fab(permissions)",
          "77: def _requires_access(*, is_authorized_callback: Callable[[], bool], func: Callable, args, kwargs) -> bool:",
          "78:     \"\"\"",
          "79:     Define the behavior whether the user is authorized to access the resource.",
          "81:     :param is_authorized_callback: callback to execute to figure whether the user is authorized to access",
          "82:         the resource",
          "83:     :param func: the function to call if the user is authorized",
          "84:     :param args: the arguments of ``func``",
          "85:     :param kwargs: the keyword arguments ``func``",
          "87:     :meta private:",
          "88:     \"\"\"",
          "89:     check_authentication()",
          "90:     if is_authorized_callback():",
          "91:         return func(*args, **kwargs)",
          "92:     raise PermissionDenied()",
          "95: def requires_authentication(func: T):",
          "96:     \"\"\"Decorator for functions that require authentication.\"\"\"",
          "98:     @wraps(func)",
          "99:     def decorated(*args, **kwargs):",
          "100:         check_authentication()",
          "101:         return func(*args, **kwargs)",
          "103:     return cast(T, decorated)",
          "106: def requires_access_configuration(method: ResourceMethod) -> Callable[[T], T]:",
          "107:     def requires_access_decorator(func: T):",
          "108:         @wraps(func)",
          "109:         def decorated(*args, **kwargs):",
          "110:             section: str | None = kwargs.get(\"section\")",
          "111:             return _requires_access(",
          "112:                 is_authorized_callback=lambda: get_auth_manager().is_authorized_configuration(",
          "113:                     method=method, details=ConfigurationDetails(section=section)",
          "114:                 ),",
          "115:                 func=func,",
          "116:                 args=args,",
          "117:                 kwargs=kwargs,",
          "118:             )",
          "120:         return cast(T, decorated)",
          "122:     return requires_access_decorator",
          "125: def requires_access_connection(method: ResourceMethod) -> Callable[[T], T]:",
          "126:     def requires_access_decorator(func: T):",
          "127:         @wraps(func)",
          "128:         def decorated(*args, **kwargs):",
          "129:             connection_id: str | None = kwargs.get(\"connection_id\")",
          "130:             return _requires_access(",
          "131:                 is_authorized_callback=lambda: get_auth_manager().is_authorized_connection(",
          "132:                     method=method, details=ConnectionDetails(conn_id=connection_id)",
          "133:                 ),",
          "134:                 func=func,",
          "135:                 args=args,",
          "136:                 kwargs=kwargs,",
          "137:             )",
          "139:         return cast(T, decorated)",
          "141:     return requires_access_decorator",
          "144: def requires_access_dag(",
          "145:     method: ResourceMethod, access_entity: DagAccessEntity | None = None",
          "146: ) -> Callable[[T], T]:",
          "147:     def _is_authorized_callback(dag_id: str):",
          "148:         def callback():",
          "149:             access = get_auth_manager().is_authorized_dag(",
          "150:                 method=method,",
          "151:                 access_entity=access_entity,",
          "152:                 details=DagDetails(id=dag_id),",
          "153:             )",
          "155:             # ``access`` means here:",
          "156:             # - if a DAG id is provided (``dag_id`` not None): is the user authorized to access this DAG",
          "157:             # - if no DAG id is provided: is the user authorized to access all DAGs",
          "158:             if dag_id or access:",
          "159:                 return access",
          "161:             # No DAG id is provided and the user is not authorized to access all DAGs",
          "162:             # If method is \"GET\", return whether the user has read access to any DAGs",
          "163:             # If method is \"PUT\", return whether the user has edit access to any DAGs",
          "164:             return (method == \"GET\" and any(get_auth_manager().get_permitted_dag_ids(methods=[\"GET\"]))) or (",
          "165:                 method == \"PUT\" and any(get_auth_manager().get_permitted_dag_ids(methods=[\"PUT\"]))",
          "166:             )",
          "168:         return callback",
          "170:     def requires_access_decorator(func: T):",
          "171:         @wraps(func)",
          "172:         def decorated(*args, **kwargs):",
          "173:             dag_id: str | None = kwargs.get(\"dag_id\") if kwargs.get(\"dag_id\") != \"~\" else None",
          "174:             return _requires_access(",
          "175:                 is_authorized_callback=_is_authorized_callback(dag_id),",
          "176:                 func=func,",
          "177:                 args=args,",
          "178:                 kwargs=kwargs,",
          "179:             )",
          "181:         return cast(T, decorated)",
          "183:     return requires_access_decorator",
          "186: def requires_access_dataset(method: ResourceMethod) -> Callable[[T], T]:",
          "187:     def requires_access_decorator(func: T):",
          "188:         @wraps(func)",
          "189:         def decorated(*args, **kwargs):",
          "190:             uri: str | None = kwargs.get(\"uri\")",
          "191:             return _requires_access(",
          "192:                 is_authorized_callback=lambda: get_auth_manager().is_authorized_dataset(",
          "193:                     method=method, details=DatasetDetails(uri=uri)",
          "194:                 ),",
          "195:                 func=func,",
          "196:                 args=args,",
          "197:                 kwargs=kwargs,",
          "198:             )",
          "200:         return cast(T, decorated)",
          "202:     return requires_access_decorator",
          "205: def requires_access_pool(method: ResourceMethod) -> Callable[[T], T]:",
          "206:     def requires_access_decorator(func: T):",
          "207:         @wraps(func)",
          "208:         def decorated(*args, **kwargs):",
          "209:             pool_name: str | None = kwargs.get(\"pool_name\")",
          "210:             return _requires_access(",
          "211:                 is_authorized_callback=lambda: get_auth_manager().is_authorized_pool(",
          "212:                     method=method, details=PoolDetails(name=pool_name)",
          "213:                 ),",
          "214:                 func=func,",
          "215:                 args=args,",
          "216:                 kwargs=kwargs,",
          "217:             )",
          "219:         return cast(T, decorated)",
          "221:     return requires_access_decorator",
          "224: def requires_access_variable(method: ResourceMethod) -> Callable[[T], T]:",
          "225:     def requires_access_decorator(func: T):",
          "226:         @wraps(func)",
          "227:         def decorated(*args, **kwargs):",
          "228:             variable_key: str | None = kwargs.get(\"variable_key\")",
          "229:             return _requires_access(",
          "230:                 is_authorized_callback=lambda: get_auth_manager().is_authorized_variable(",
          "231:                     method=method, details=VariableDetails(key=variable_key)",
          "232:                 ),",
          "233:                 func=func,",
          "234:                 args=args,",
          "235:                 kwargs=kwargs,",
          "236:             )",
          "238:         return cast(T, decorated)",
          "240:     return requires_access_decorator",
          "243: def requires_access_website() -> Callable[[T], T]:",
          "247:             return _requires_access(",
          "248:                 is_authorized_callback=lambda: get_auth_manager().is_authorized_website(),",
          "249:                 func=func,",
          "250:                 args=args,",
          "251:                 kwargs=kwargs,",
          "252:             )",
          "",
          "---------------"
        ],
        "airflow/auth/managers/base_auth_manager.py||airflow/auth/managers/base_auth_manager.py": [
          "File: airflow/auth/managers/base_auth_manager.py -> airflow/auth/managers/base_auth_manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: from abc import abstractmethod",
          "23: from airflow.exceptions import AirflowException",
          "24: from airflow.utils.log.logging_mixin import LoggingMixin",
          "26: if TYPE_CHECKING:",
          "27:     from flask import Flask",
          "29:     from airflow.auth.managers.models.base_user import BaseUser",
          "30:     from airflow.auth.managers.models.resource_details import (",
          "31:         ConnectionDetails,",
          "32:         DagAccessEntity,",
          "33:         DagDetails,",
          "34:     )",
          "35:     from airflow.cli.cli_config import CLICommand",
          "36:     from airflow.www.security_manager import AirflowSecurityManagerV2",
          "",
          "[Removed Lines]",
          "21: from typing import TYPE_CHECKING, Literal",
          "",
          "[Added Lines]",
          "21: from typing import TYPE_CHECKING, Container, Literal",
          "23: from sqlalchemy import select",
          "26: from airflow.models import DagModel",
          "28: from airflow.utils.session import NEW_SESSION, provide_session",
          "32:     from sqlalchemy.orm import Session",
          "36:         ConfigurationDetails,",
          "40:         DatasetDetails,",
          "41:         PoolDetails,",
          "42:         VariableDetails,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "82:         self,",
          "84:         method: ResourceMethod,",
          "85:         user: BaseUser | None = None,",
          "86:     ) -> bool:",
          "87:         \"\"\"",
          "88:         Return whether the user is authorized to perform a given action on configuration.",
          "90:         :param method: the method to perform",
          "91:         :param user: the user to perform the action on. If not provided (or None), it uses the current user",
          "92:         \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "94:         details: ConfigurationDetails | None = None,",
          "101:         :param details: optional details about the configuration",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "110:         self,",
          "112:         method: ResourceMethod,",
          "114:         user: BaseUser | None = None,",
          "115:     ) -> bool:",
          "116:         \"\"\"",
          "117:         Return whether the user is authorized to perform a given action on a connection.",
          "119:         :param method: the method to perform",
          "121:         :param user: the user to perform the action on. If not provided (or None), it uses the current user",
          "122:         \"\"\"",
          "",
          "[Removed Lines]",
          "113:         connection_details: ConnectionDetails | None = None,",
          "120:         :param connection_details: optional details about the connection",
          "",
          "[Added Lines]",
          "124:         details: ConnectionDetails | None = None,",
          "131:         :param details: optional details about the connection",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "126:         self,",
          "128:         method: ResourceMethod,",
          "131:         user: BaseUser | None = None,",
          "132:     ) -> bool:",
          "133:         \"\"\"",
          "134:         Return whether the user is authorized to perform a given action on a DAG.",
          "136:         :param method: the method to perform",
          "138:             If not provided, the authorization request is about the DAG itself",
          "140:         :param user: the user to perform the action on. If not provided (or None), it uses the current user",
          "141:         \"\"\"",
          "",
          "[Removed Lines]",
          "129:         dag_access_entity: DagAccessEntity | None = None,",
          "130:         dag_details: DagDetails | None = None,",
          "137:         :param dag_access_entity: the kind of DAG information the authorization request is about.",
          "139:         :param dag_details: optional details about the DAG",
          "",
          "[Added Lines]",
          "140:         access_entity: DagAccessEntity | None = None,",
          "141:         details: DagDetails | None = None,",
          "148:         :param access_entity: the kind of DAG information the authorization request is about.",
          "150:         :param details: optional details about the DAG",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "145:         self,",
          "147:         method: ResourceMethod,",
          "148:         user: BaseUser | None = None,",
          "149:     ) -> bool:",
          "150:         \"\"\"",
          "151:         Return whether the user is authorized to perform a given action on a dataset.",
          "153:         :param method: the method to perform",
          "154:         :param user: the user to perform the action on. If not provided (or None), it uses the current user",
          "155:         \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "159:         details: DatasetDetails | None = None,",
          "166:         :param details: optional details about the dataset",
          "167:         :param user: the user to perform the action on. If not provided (or None), it uses the current user",
          "168:         \"\"\"",
          "170:     @abstractmethod",
          "171:     def is_authorized_pool(",
          "172:         self,",
          "174:         method: ResourceMethod,",
          "175:         details: PoolDetails | None = None,",
          "176:         user: BaseUser | None = None,",
          "177:     ) -> bool:",
          "178:         \"\"\"",
          "179:         Return whether the user is authorized to perform a given action on a pool.",
          "181:         :param method: the method to perform",
          "182:         :param details: optional details about the pool",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "159:         self,",
          "161:         method: ResourceMethod,",
          "162:         user: BaseUser | None = None,",
          "163:     ) -> bool:",
          "164:         \"\"\"",
          "165:         Return whether the user is authorized to perform a given action on a variable.",
          "167:         :param method: the method to perform",
          "168:         :param user: the user to perform the action on. If not provided (or None), it uses the current user",
          "169:         \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "191:         details: VariableDetails | None = None,",
          "198:         :param details: optional details about the variable",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "182:         :param user: the user to perform the action on. If not provided (or None), it uses the current user",
          "183:         \"\"\"",
          "185:     @abstractmethod",
          "186:     def get_url_login(self, **kwargs) -> str:",
          "187:         \"\"\"Return the login page url.\"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "216:     @provide_session",
          "217:     def get_permitted_dag_ids(",
          "218:         self,",
          "220:         methods: Container[ResourceMethod] | None = None,",
          "221:         user=None,",
          "222:         session: Session = NEW_SESSION,",
          "223:     ) -> set[str]:",
          "224:         \"\"\"",
          "225:         Get readable or writable DAGs for user.",
          "227:         By default, reads all the DAGs and check individually if the user has permissions to access the DAG.",
          "228:         Can lead to some poor performance. It is recommended to override this method in the auth manager",
          "229:         implementation to provide a more efficient implementation.",
          "230:         \"\"\"",
          "231:         if not methods:",
          "232:             methods = [\"PUT\", \"GET\"]",
          "234:         dag_ids = {dag.dag_id for dag in session.execute(select(DagModel.dag_id))}",
          "236:         if (\"GET\" in methods and self.is_authorized_dag(method=\"GET\", user=user)) or (",
          "237:             \"PUT\" in methods and self.is_authorized_dag(method=\"PUT\", user=user)",
          "238:         ):",
          "239:             # If user is authorized to read/edit all DAGs, return all DAGs",
          "240:             return dag_ids",
          "242:         def _is_permitted_dag_id(method: ResourceMethod, methods: Container[ResourceMethod], dag_id: str):",
          "243:             return method in methods and self.is_authorized_dag(",
          "244:                 method=method, details=DagDetails(id=dag_id), user=user",
          "245:             )",
          "247:         return {",
          "248:             dag_id",
          "249:             for dag_id in dag_ids",
          "250:             if _is_permitted_dag_id(\"GET\", methods, dag_id) or _is_permitted_dag_id(\"PUT\", methods, dag_id)",
          "251:         }",
          "",
          "---------------"
        ],
        "airflow/auth/managers/fab/decorators/auth.py||airflow/auth/managers/fab/decorators/auth.py": [
          "File: airflow/auth/managers/fab/decorators/auth.py -> airflow/auth/managers/fab/decorators/auth.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: from flask import current_app, render_template, request",
          "26: from airflow.configuration import conf",
          "27: from airflow.utils.net import get_hostname",
          "28: from airflow.www.auth import _has_access",
          "29: from airflow.www.extensions.init_auth_manager import get_auth_manager",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: from airflow.api_connexion.exceptions import PermissionDenied",
          "27: from airflow.api_connexion.security import check_authentication",
          "29: from airflow.utils.airflow_flask_app import get_airflow_app",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "33: log = logging.getLogger(__name__)",
          "36: def _has_access_fab(permissions: Sequence[tuple[str, str]] | None = None) -> Callable[[T], T]:",
          "37:     \"\"\"",
          "38:     Factory for decorator that checks current user's permissions against required permissions.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "39: def _requires_access_fab(permissions: Sequence[tuple[str, str]] | None = None) -> Callable[[T], T]:",
          "40:     \"\"\"",
          "41:     Check current user's permissions against required permissions.",
          "43:     This decorator is only kept for backward compatible reasons. The decorator",
          "44:     ``airflow.api_connexion.security.requires_access``, which redirects to this decorator, might be used in",
          "45:     user plugins. Thus, we need to keep it.",
          "47:     :meta private:",
          "48:     \"\"\"",
          "49:     appbuilder = get_airflow_app().appbuilder",
          "50:     if appbuilder.update_perms:",
          "51:         appbuilder.sm.sync_resource_permissions(permissions)",
          "53:     def requires_access_decorator(func: T):",
          "54:         @wraps(func)",
          "55:         def decorated(*args, **kwargs):",
          "56:             check_authentication()",
          "57:             if appbuilder.sm.check_authorization(permissions, kwargs.get(\"dag_id\")):",
          "58:                 return func(*args, **kwargs)",
          "59:             raise PermissionDenied()",
          "61:         return cast(T, decorated)",
          "63:     return requires_access_decorator",
          "",
          "---------------"
        ],
        "airflow/auth/managers/fab/fab_auth_manager.py||airflow/auth/managers/fab/fab_auth_manager.py": [
          "File: airflow/auth/managers/fab/fab_auth_manager.py -> airflow/auth/managers/fab/fab_auth_manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import warnings",
          "23: from flask import url_for",
          "24: from sqlalchemy import select",
          "26: from airflow.auth.managers.base_auth_manager import BaseAuthManager, ResourceMethod",
          "27: from airflow.auth.managers.fab.cli_commands.definition import (",
          "",
          "[Removed Lines]",
          "21: from typing import TYPE_CHECKING",
          "",
          "[Added Lines]",
          "21: from typing import TYPE_CHECKING, Container",
          "25: from sqlalchemy.orm import Session, joinedload",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "29:     SYNC_PERM_COMMAND,",
          "30:     USERS_COMMANDS,",
          "31: )",
          "33: from airflow.cli.cli_config import (",
          "34:     GroupCommand,",
          "35: )",
          "36: from airflow.exceptions import AirflowException",
          "37: from airflow.models import DagModel",
          "38: from airflow.security.permissions import (",
          "39:     ACTION_CAN_ACCESS_MENU,",
          "40:     ACTION_CAN_CREATE,",
          "",
          "[Removed Lines]",
          "32: from airflow.auth.managers.models.resource_details import ConnectionDetails, DagAccessEntity, DagDetails",
          "",
          "[Added Lines]",
          "33: from airflow.auth.managers.fab.models import Permission, Role, User",
          "34: from airflow.auth.managers.models.resource_details import (",
          "35:     ConfigurationDetails,",
          "36:     ConnectionDetails,",
          "37:     DagAccessEntity,",
          "38:     DagDetails,",
          "39:     DatasetDetails,",
          "40:     PoolDetails,",
          "41:     VariableDetails,",
          "42: )",
          "48: from airflow.security import permissions",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "50:     RESOURCE_DAG_DEPENDENCIES,",
          "51:     RESOURCE_DAG_PREFIX,",
          "52:     RESOURCE_DAG_RUN,",
          "53:     RESOURCE_DATASET,",
          "54:     RESOURCE_TASK_INSTANCE,",
          "55:     RESOURCE_TASK_LOG,",
          "56:     RESOURCE_VARIABLE,",
          "57:     RESOURCE_WEBSITE,",
          "58:     RESOURCE_XCOM,",
          "59: )",
          "61: if TYPE_CHECKING:",
          "63:     from airflow.auth.managers.models.base_user import BaseUser",
          "64:     from airflow.cli.cli_config import (",
          "65:         CLICommand,",
          "66:     )",
          "69:     \"POST\": ACTION_CAN_CREATE,",
          "70:     \"GET\": ACTION_CAN_READ,",
          "71:     \"PUT\": ACTION_CAN_EDIT,",
          "72:     \"DELETE\": ACTION_CAN_DELETE,",
          "73: }",
          "84: }",
          "",
          "[Removed Lines]",
          "62:     from airflow.auth.managers.fab.models import User",
          "68: _MAP_METHOD_NAME_TO_FAB_ACTION_NAME: dict[ResourceMethod, str] = {",
          "75: _MAP_DAG_ACCESS_ENTITY_TO_FAB_RESOURCE_TYPE = {",
          "76:     DagAccessEntity.AUDIT_LOG: RESOURCE_AUDIT_LOG,",
          "77:     DagAccessEntity.CODE: RESOURCE_DAG_CODE,",
          "78:     DagAccessEntity.DATASET: RESOURCE_DATASET,",
          "79:     DagAccessEntity.DEPENDENCIES: RESOURCE_DAG_DEPENDENCIES,",
          "80:     DagAccessEntity.RUN: RESOURCE_DAG_RUN,",
          "81:     DagAccessEntity.TASK_INSTANCE: RESOURCE_TASK_INSTANCE,",
          "82:     DagAccessEntity.TASK_LOGS: RESOURCE_TASK_LOG,",
          "83:     DagAccessEntity.XCOM: RESOURCE_XCOM,",
          "",
          "[Added Lines]",
          "64:     RESOURCE_DAG_WARNING,",
          "66:     RESOURCE_IMPORT_ERROR,",
          "67:     RESOURCE_PLUGIN,",
          "68:     RESOURCE_POOL,",
          "69:     RESOURCE_PROVIDER,",
          "72:     RESOURCE_TRIGGER,",
          "77: from airflow.utils.session import NEW_SESSION, provide_session",
          "86: MAP_METHOD_NAME_TO_FAB_ACTION_NAME: dict[ResourceMethod, str] = {",
          "93: _MAP_DAG_ACCESS_ENTITY_TO_FAB_RESOURCE_TYPE: dict[DagAccessEntity, tuple[str, ...]] = {",
          "94:     DagAccessEntity.AUDIT_LOG: (RESOURCE_AUDIT_LOG,),",
          "95:     DagAccessEntity.CODE: (RESOURCE_DAG_CODE,),",
          "96:     DagAccessEntity.DEPENDENCIES: (RESOURCE_DAG_DEPENDENCIES,),",
          "97:     DagAccessEntity.IMPORT_ERRORS: (RESOURCE_IMPORT_ERROR,),",
          "98:     DagAccessEntity.RUN: (RESOURCE_DAG_RUN,),",
          "99:     # RESOURCE_TASK_INSTANCE has been originally misused. RESOURCE_TASK_INSTANCE referred to task definition",
          "100:     # AND task instances without making the difference",
          "101:     # To be backward compatible, we translate DagAccessEntity.TASK_INSTANCE to RESOURCE_TASK_INSTANCE AND",
          "102:     # RESOURCE_DAG_RUN",
          "103:     # See https://github.com/apache/airflow/pull/34317#discussion_r1355917769",
          "104:     DagAccessEntity.TASK: (RESOURCE_TASK_INSTANCE,),",
          "105:     DagAccessEntity.TASK_INSTANCE: (RESOURCE_DAG_RUN, RESOURCE_TASK_INSTANCE),",
          "106:     DagAccessEntity.TASK_LOGS: (RESOURCE_TASK_LOG,),",
          "107:     DagAccessEntity.WARNING: (RESOURCE_DAG_WARNING,),",
          "108:     DagAccessEntity.XCOM: (RESOURCE_XCOM,),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "139:         \"\"\"Return whether the user is logged in.\"\"\"",
          "140:         return not self.get_user().is_anonymous",
          "143:         return self._is_authorized(method=method, resource_type=RESOURCE_CONFIG, user=user)",
          "145:     def is_authorized_cluster_activity(self, *, method: ResourceMethod, user: BaseUser | None = None) -> bool:",
          "",
          "[Removed Lines]",
          "142:     def is_authorized_configuration(self, *, method: ResourceMethod, user: BaseUser | None = None) -> bool:",
          "",
          "[Added Lines]",
          "167:     def is_authorized_configuration(",
          "168:         self,",
          "170:         method: ResourceMethod,",
          "171:         details: ConfigurationDetails | None = None,",
          "172:         user: BaseUser | None = None,",
          "173:     ) -> bool:",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "149:         self,",
          "151:         method: ResourceMethod,",
          "153:         user: BaseUser | None = None,",
          "154:     ) -> bool:",
          "155:         return self._is_authorized(method=method, resource_type=RESOURCE_CONNECTION, user=user)",
          "",
          "[Removed Lines]",
          "152:         connection_details: ConnectionDetails | None = None,",
          "",
          "[Added Lines]",
          "183:         details: ConnectionDetails | None = None,",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "158:         self,",
          "160:         method: ResourceMethod,",
          "163:         user: BaseUser | None = None,",
          "164:     ) -> bool:",
          "165:         \"\"\"",
          "",
          "[Removed Lines]",
          "161:         dag_access_entity: DagAccessEntity | None = None,",
          "162:         dag_details: DagDetails | None = None,",
          "",
          "[Added Lines]",
          "192:         access_entity: DagAccessEntity | None = None,",
          "193:         details: DagDetails | None = None,",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "171:         entity (e.g. DAG runs).",
          "172:         2. ``dag_access`` is provided which means the user wants to access a sub entity of the DAG",
          "173:         (e.g. DAG runs).",
          "177:         :param method: The method to authorize.",
          "180:         :param user: The user.",
          "181:         \"\"\"",
          "183:             # Scenario 1",
          "185:         else:",
          "186:             # Scenario 2",
          "188:             dag_method: ResourceMethod = \"GET\" if method == \"GET\" else \"PUT\"",
          "195:         return self._is_authorized(method=method, resource_type=RESOURCE_DATASET, user=user)",
          "198:         return self._is_authorized(method=method, resource_type=RESOURCE_VARIABLE, user=user)",
          "200:     def is_authorized_website(self, *, user: BaseUser | None = None) -> bool:",
          "203:     def get_security_manager_override_class(self) -> type:",
          "204:         \"\"\"Return the security manager override.\"\"\"",
          "",
          "[Removed Lines]",
          "174:             a. If ``method`` is GET, then check the user has READ permissions on the DAG and the sub entity",
          "175:             b. Else, check the user has EDIT permissions on the DAG and ``method`` on the sub entity",
          "178:         :param dag_access_entity: The dag access entity.",
          "179:         :param dag_details: The dag details.",
          "182:         if not dag_access_entity:",
          "184:             return self._is_authorized_dag(method=method, dag_details=dag_details, user=user)",
          "187:             resource_type = self._get_fab_resource_type(dag_access_entity)",
          "190:             return self._is_authorized_dag(",
          "191:                 method=dag_method, dag_details=dag_details, user=user",
          "192:             ) and self._is_authorized(method=method, resource_type=resource_type, user=user)",
          "194:     def is_authorized_dataset(self, *, method: ResourceMethod, user: BaseUser | None = None) -> bool:",
          "197:     def is_authorized_variable(self, *, method: ResourceMethod, user: BaseUser | None = None) -> bool:",
          "201:         return self._is_authorized(method=\"GET\", resource_type=RESOURCE_WEBSITE, user=user)",
          "",
          "[Added Lines]",
          "205:             a. If ``method`` is GET, then check the user has READ permissions on the DAG and the sub entity.",
          "206:             b. Else, check the user has EDIT permissions on the DAG and ``method`` on the sub entity.",
          "208:             However, if no specific DAG is targeted, just check the sub entity.",
          "211:         :param access_entity: The dag access entity.",
          "212:         :param details: The dag details.",
          "215:         if not access_entity:",
          "217:             return self._is_authorized_dag(method=method, details=details, user=user)",
          "220:             resource_types = self._get_fab_resource_types(access_entity)",
          "223:             if (details and details.id) and not self._is_authorized_dag(",
          "224:                 method=dag_method, details=details, user=user",
          "225:             ):",
          "226:                 return False",
          "228:             return all(",
          "229:                 self._is_authorized(method=method, resource_type=resource_type, user=user)",
          "230:                 for resource_type in resource_types",
          "231:             )",
          "233:     def is_authorized_dataset(",
          "234:         self, *, method: ResourceMethod, details: DatasetDetails | None = None, user: BaseUser | None = None",
          "235:     ) -> bool:",
          "238:     def is_authorized_pool(",
          "239:         self, *, method: ResourceMethod, details: PoolDetails | None = None, user: BaseUser | None = None",
          "240:     ) -> bool:",
          "241:         return self._is_authorized(method=method, resource_type=RESOURCE_POOL, user=user)",
          "243:     def is_authorized_variable(",
          "244:         self, *, method: ResourceMethod, details: VariableDetails | None = None, user: BaseUser | None = None",
          "245:     ) -> bool:",
          "249:         return (",
          "250:             self._is_authorized(method=\"GET\", resource_type=RESOURCE_PLUGIN, user=user)",
          "251:             or self._is_authorized(method=\"GET\", resource_type=RESOURCE_PROVIDER, user=user)",
          "252:             or self._is_authorized(method=\"GET\", resource_type=RESOURCE_TRIGGER, user=user)",
          "253:             or self._is_authorized(method=\"GET\", resource_type=RESOURCE_WEBSITE, user=user)",
          "254:         )",
          "256:     @provide_session",
          "257:     def get_permitted_dag_ids(",
          "258:         self,",
          "260:         methods: Container[ResourceMethod] | None = None,",
          "261:         user=None,",
          "262:         session: Session = NEW_SESSION,",
          "263:     ) -> set[str]:",
          "264:         if not methods:",
          "265:             methods = [\"PUT\", \"GET\"]",
          "267:         if not user:",
          "268:             user = self.get_user()",
          "270:         if not self.is_logged_in():",
          "271:             roles = user.roles",
          "272:         else:",
          "273:             if (\"GET\" in methods and self.is_authorized_dag(method=\"GET\", user=user)) or (",
          "274:                 \"PUT\" in methods and self.is_authorized_dag(method=\"PUT\", user=user)",
          "275:             ):",
          "276:                 # If user is authorized to read/edit all DAGs, return all DAGs",
          "277:                 return {dag.dag_id for dag in session.execute(select(DagModel.dag_id))}",
          "278:             user_query = session.scalar(",
          "279:                 select(User)",
          "280:                 .options(",
          "281:                     joinedload(User.roles)",
          "282:                     .subqueryload(Role.permissions)",
          "283:                     .options(joinedload(Permission.action), joinedload(Permission.resource))",
          "284:                 )",
          "285:                 .where(User.id == user.id)",
          "286:             )",
          "287:             roles = user_query.roles",
          "289:         map_fab_action_name_to_method_name = {v: k for k, v in MAP_METHOD_NAME_TO_FAB_ACTION_NAME.items()}",
          "290:         map_fab_action_name_to_method_name[ACTION_CAN_ACCESS_MENU] = \"GET\"",
          "291:         resources = set()",
          "292:         for role in roles:",
          "293:             for permission in role.permissions:",
          "294:                 action = permission.action.name",
          "295:                 if (",
          "296:                     action in map_fab_action_name_to_method_name",
          "297:                     and map_fab_action_name_to_method_name[action] in methods",
          "298:                 ):",
          "299:                     resource = permission.resource.name",
          "300:                     if resource == permissions.RESOURCE_DAG:",
          "301:                         return {dag.dag_id for dag in session.execute(select(DagModel.dag_id))}",
          "302:                     if resource.startswith(permissions.RESOURCE_DAG_PREFIX):",
          "303:                         resources.add(resource[len(permissions.RESOURCE_DAG_PREFIX) :])",
          "304:                     else:",
          "305:                         resources.add(resource)",
          "306:         return {",
          "307:             dag.dag_id",
          "308:             for dag in session.execute(select(DagModel.dag_id).where(DagModel.dag_id.in_(resources)))",
          "309:         }",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "270:     def _is_authorized_dag(",
          "271:         self,",
          "272:         method: ResourceMethod,",
          "274:         user: BaseUser | None = None,",
          "275:     ) -> bool:",
          "276:         \"\"\"",
          "277:         Return whether the user is authorized to perform a given action on a DAG.",
          "279:         :param method: the method to perform",
          "281:         :param user: the user to perform the action on. If not provided (or None), it uses the current user",
          "283:         :meta private:",
          "",
          "[Removed Lines]",
          "273:         dag_details: DagDetails | None = None,",
          "280:         :param dag_details: optional details about the DAG",
          "",
          "[Added Lines]",
          "381:         details: DagDetails | None = None,",
          "388:         :param details: optional details about the DAG",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "286:         if is_global_authorized:",
          "287:             return True",
          "290:             # Check whether the user has permissions to access a specific DAG",
          "292:             return self._is_authorized(method=method, resource_type=resource_dag_name, user=user)",
          "294:         return False",
          "",
          "[Removed Lines]",
          "289:         if dag_details and dag_details.id:",
          "291:             resource_dag_name = self._resource_name_for_dag(dag_details.id)",
          "",
          "[Added Lines]",
          "397:         if details and details.id:",
          "399:             resource_dag_name = self._resource_name_for_dag(details.id)",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "303:         :meta private:",
          "304:         \"\"\"",
          "306:             raise AirflowException(f\"Unknown method: {method}\")",
          "309:     @staticmethod",
          "311:         \"\"\"",
          "314:         :param dag_access_entity: the DAG access entity",
          "",
          "[Removed Lines]",
          "305:         if method not in _MAP_METHOD_NAME_TO_FAB_ACTION_NAME:",
          "307:         return _MAP_METHOD_NAME_TO_FAB_ACTION_NAME[method]",
          "310:     def _get_fab_resource_type(dag_access_entity: DagAccessEntity):",
          "312:         Convert a DAG access entity to a FAB resource type.",
          "",
          "[Added Lines]",
          "413:         if method not in MAP_METHOD_NAME_TO_FAB_ACTION_NAME:",
          "415:         return MAP_METHOD_NAME_TO_FAB_ACTION_NAME[method]",
          "418:     def _get_fab_resource_types(dag_access_entity: DagAccessEntity) -> tuple[str, ...]:",
          "420:         Convert a DAG access entity to a tuple of FAB resource type.",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "361:         :meta private:",
          "362:         \"\"\"",
          "363:         if \".\" in dag_id:",
          "365:                 select(DagModel.dag_id, DagModel.root_dag_id).where(DagModel.dag_id == dag_id).limit(1)",
          "366:             )",
          "368:         return dag_id",
          "",
          "[Removed Lines]",
          "364:             dm = self.security_manager.appbuilder.get_session.scalar(",
          "367:             return dm.root_dag_id or dm.dag_id",
          "",
          "[Added Lines]",
          "472:             return self.security_manager.appbuilder.get_session.scalar(",
          "",
          "---------------"
        ],
        "airflow/auth/managers/fab/security_manager/override.py||airflow/auth/managers/fab/security_manager/override.py": [
          "File: airflow/auth/managers/fab/security_manager/override.py -> airflow/auth/managers/fab/security_manager/override.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: import uuid",
          "26: import warnings",
          "27: from functools import cached_property",
          "30: import re2",
          "31: from flask import flash, g, session",
          "",
          "[Removed Lines]",
          "28: from typing import TYPE_CHECKING",
          "",
          "[Added Lines]",
          "28: from typing import TYPE_CHECKING, Container, Iterable, Sequence",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "42: from sqlalchemy.exc import MultipleResultsFound",
          "43: from werkzeug.security import generate_password_hash",
          "45: from airflow.auth.managers.fab.models import Action, Permission, RegisterUser, Resource, Role",
          "46: from airflow.auth.managers.fab.models.anonymous_user import AnonymousUser",
          "48: from airflow.www.security_manager import AirflowSecurityManagerV2",
          "49: from airflow.www.session import AirflowDatabaseSessionInterface",
          "51: if TYPE_CHECKING:",
          "52:     from airflow.auth.managers.fab.models import User",
          "54: log = logging.getLogger(__name__)",
          "",
          "[Removed Lines]",
          "47: from airflow.exceptions import AirflowException",
          "",
          "[Added Lines]",
          "45: from airflow.auth.managers.fab.fab_auth_manager import MAP_METHOD_NAME_TO_FAB_ACTION_NAME",
          "48: from airflow.exceptions import AirflowException, RemovedInAirflow3Warning",
          "49: from airflow.models import DagModel",
          "50: from airflow.security import permissions",
          "51: from airflow.utils.session import NEW_SESSION, provide_session",
          "52: from airflow.www.extensions.init_auth_manager import get_auth_manager",
          "57:     from sqlalchemy.orm import Session",
          "59:     from airflow.auth.managers.base_auth_manager import ResourceMethod",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "502:             log.error(const.LOGMSG_ERR_SEC_CREATE_DB, e)",
          "503:             exit(1)",
          "505:     \"\"\"",
          "506:     -----------",
          "507:     Role entity",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "513:     def get_readable_dags(self, user) -> Iterable[DagModel]:",
          "514:         \"\"\"Get the DAGs readable by authenticated user.\"\"\"",
          "515:         warnings.warn(",
          "516:             \"`get_readable_dags` has been deprecated. Please use `get_auth_manager().get_permitted_dag_ids` \"",
          "517:             \"instead.\",",
          "518:             RemovedInAirflow3Warning,",
          "519:             stacklevel=2,",
          "520:         )",
          "521:         with warnings.catch_warnings():",
          "522:             warnings.simplefilter(\"ignore\", RemovedInAirflow3Warning)",
          "523:             return self.get_accessible_dags([permissions.ACTION_CAN_READ], user)",
          "525:     def get_editable_dags(self, user) -> Iterable[DagModel]:",
          "526:         \"\"\"Get the DAGs editable by authenticated user.\"\"\"",
          "527:         warnings.warn(",
          "528:             \"`get_editable_dags` has been deprecated. Please use `get_auth_manager().get_permitted_dag_ids` \"",
          "529:             \"instead.\",",
          "530:             RemovedInAirflow3Warning,",
          "531:             stacklevel=2,",
          "532:         )",
          "533:         with warnings.catch_warnings():",
          "534:             warnings.simplefilter(\"ignore\", RemovedInAirflow3Warning)",
          "535:             return self.get_accessible_dags([permissions.ACTION_CAN_EDIT], user)",
          "537:     @provide_session",
          "538:     def get_accessible_dags(",
          "539:         self,",
          "540:         user_actions: Container[str] | None,",
          "541:         user,",
          "542:         session: Session = NEW_SESSION,",
          "543:     ) -> Iterable[DagModel]:",
          "544:         warnings.warn(",
          "545:             \"`get_accessible_dags` has been deprecated. Please use \"",
          "546:             \"`get_auth_manager().get_permitted_dag_ids` instead.\",",
          "547:             RemovedInAirflow3Warning,",
          "548:             stacklevel=3,",
          "549:         )",
          "551:         dag_ids = self.get_accessible_dag_ids(user, user_actions, session)",
          "552:         return session.scalars(select(DagModel).where(DagModel.dag_id.in_(dag_ids)))",
          "554:     @provide_session",
          "555:     def get_accessible_dag_ids(",
          "556:         self,",
          "557:         user,",
          "558:         user_actions: Container[str] | None = None,",
          "559:         session: Session = NEW_SESSION,",
          "560:     ) -> set[str]:",
          "561:         warnings.warn(",
          "562:             \"`get_accessible_dag_ids` has been deprecated. Please use \"",
          "563:             \"`get_auth_manager().get_permitted_dag_ids` instead.\",",
          "564:             RemovedInAirflow3Warning,",
          "565:             stacklevel=3,",
          "566:         )",
          "567:         if not user_actions:",
          "568:             user_actions = [permissions.ACTION_CAN_EDIT, permissions.ACTION_CAN_READ]",
          "569:         fab_action_name_to_method_name = {v: k for k, v in MAP_METHOD_NAME_TO_FAB_ACTION_NAME.items()}",
          "570:         user_methods: Container[ResourceMethod] = [",
          "571:             fab_action_name_to_method_name[action]",
          "572:             for action in fab_action_name_to_method_name",
          "573:             if action in user_actions",
          "574:         ]",
          "575:         return get_auth_manager().get_permitted_dag_ids(user=user, methods=user_methods, session=session)",
          "577:     @staticmethod",
          "578:     def get_readable_dag_ids(user=None) -> set[str]:",
          "579:         \"\"\"Get the DAG IDs readable by authenticated user.\"\"\"",
          "580:         return get_auth_manager().get_permitted_dag_ids(methods=[\"GET\"], user=user)",
          "582:     @staticmethod",
          "583:     def get_editable_dag_ids(user=None) -> set[str]:",
          "584:         \"\"\"Get the DAG IDs editable by authenticated user.\"\"\"",
          "585:         return get_auth_manager().get_permitted_dag_ids(methods=[\"PUT\"], user=user)",
          "587:     def can_access_some_dags(self, action: str, dag_id: str | None = None) -> bool:",
          "588:         \"\"\"Check if user has read or write access to some dags.\"\"\"",
          "589:         if dag_id and dag_id != \"~\":",
          "590:             root_dag_id = self._get_root_dag_id(dag_id)",
          "591:             return self.has_access(action, permissions.resource_name_for_dag(root_dag_id))",
          "593:         user = g.user",
          "594:         if action == permissions.ACTION_CAN_READ:",
          "595:             return any(self.get_readable_dag_ids(user))",
          "596:         return any(self.get_editable_dag_ids(user))",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1071:         log.debug(\"Token Get: %s\", token)",
          "1072:         return token",
          "1074:     @staticmethod",
          "1075:     def _azure_parse_jwt(token):",
          "1076:         \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1167:     def check_authorization(",
          "1168:         self,",
          "1169:         perms: Sequence[tuple[str, str]] | None = None,",
          "1170:         dag_id: str | None = None,",
          "1171:     ) -> bool:",
          "1172:         \"\"\"Checks that the logged in user has the specified permissions.\"\"\"",
          "1173:         if not perms:",
          "1174:             return True",
          "1176:         for perm in perms:",
          "1177:             if perm in (",
          "1178:                 (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "1179:                 (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),",
          "1180:                 (permissions.ACTION_CAN_DELETE, permissions.RESOURCE_DAG),",
          "1181:             ):",
          "1182:                 can_access_all_dags = self.has_access(*perm)",
          "1183:                 if not can_access_all_dags:",
          "1184:                     action = perm[0]",
          "1185:                     if not self.can_access_some_dags(action, dag_id):",
          "1186:                         return False",
          "1187:             elif not self.has_access(*perm):",
          "1188:                 return False",
          "1190:         return True",
          "",
          "---------------"
        ],
        "airflow/auth/managers/models/resource_details.py||airflow/auth/managers/models/resource_details.py": [
          "File: airflow/auth/managers/models/resource_details.py -> airflow/auth/managers/models/resource_details.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: from enum import Enum",
          "24: @dataclass",
          "25: class ConnectionDetails:",
          "26:     \"\"\"Represents the details of a connection.\"\"\"",
          "31: @dataclass",
          "32: class DagDetails:",
          "33:     \"\"\"Represents the details of a DAG.\"\"\"",
          "38: class DagAccessEntity(Enum):",
          "",
          "[Removed Lines]",
          "28:     conn_id: str",
          "35:     id: str",
          "",
          "[Added Lines]",
          "24: @dataclass",
          "25: class ConfigurationDetails:",
          "26:     \"\"\"Represents the details of a configuration.\"\"\"",
          "28:     section: str | None = None",
          "35:     conn_id: str | None = None",
          "42:     id: str | None = None",
          "45: @dataclass",
          "46: class DatasetDetails:",
          "47:     \"\"\"Represents the details of a dataset.\"\"\"",
          "49:     uri: str | None = None",
          "52: @dataclass",
          "53: class PoolDetails:",
          "54:     \"\"\"Represents the details of a pool.\"\"\"",
          "56:     name: str | None = None",
          "59: @dataclass",
          "60: class VariableDetails:",
          "61:     \"\"\"Represents the details of a variable.\"\"\"",
          "63:     key: str | None = None",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "41:     AUDIT_LOG = \"AUDIT_LOG\"",
          "42:     CODE = \"CODE\"",
          "44:     DEPENDENCIES = \"DEPENDENCIES\"",
          "45:     RUN = \"RUN\"",
          "46:     TASK_INSTANCE = \"TASK_INSTANCE\"",
          "47:     TASK_LOGS = \"TASK_LOGS\"",
          "48:     XCOM = \"XCOM\"",
          "",
          "[Removed Lines]",
          "43:     DATASET = \"DATASET\"",
          "",
          "[Added Lines]",
          "72:     IMPORT_ERRORS = \"IMPORT_ERRORS\"",
          "74:     TASK = \"TASK\"",
          "77:     WARNING = \"WARNING\"",
          "",
          "---------------"
        ],
        "airflow/www/auth.py||airflow/www/auth.py": [
          "File: airflow/www/auth.py -> airflow/www/auth.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "36: if TYPE_CHECKING:",
          "37:     from airflow.auth.managers.base_auth_manager import ResourceMethod",
          "40: T = TypeVar(\"T\", bound=Callable)",
          "",
          "[Removed Lines]",
          "38:     from airflow.models import Connection",
          "",
          "[Added Lines]",
          "38:     from airflow.models.connection import Connection",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "75:     This works only for resources with no details. This function is used in some ``has_access_`` functions",
          "76:     below.",
          "79:         the resource?",
          "80:     \"\"\"",
          "",
          "[Removed Lines]",
          "78:     :param is_authorized_callback: callback to execute to figure whether the user authorized to access",
          "",
          "[Added Lines]",
          "78:     :param is_authorized_callback: callback to execute to figure whether the user is authorized to access",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "140:             ]",
          "141:             is_authorized = all(",
          "142:                 [",
          "146:                     for connection_details in connections_details",
          "147:                 ]",
          "148:             )",
          "",
          "[Removed Lines]",
          "143:                     get_auth_manager().is_authorized_connection(",
          "144:                         method=method, connection_details=connection_details",
          "145:                     )",
          "",
          "[Added Lines]",
          "143:                     get_auth_manager().is_authorized_connection(method=method, details=connection_details)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "192:             is_authorized = get_auth_manager().is_authorized_dag(",
          "193:                 method=method,",
          "196:             )",
          "198:             return _has_access(",
          "",
          "[Removed Lines]",
          "194:                 dag_access_entity=access_entity,",
          "195:                 dag_details=None if not dag_id else DagDetails(id=dag_id),",
          "",
          "[Added Lines]",
          "192:                 access_entity=access_entity,",
          "193:                 details=None if not dag_id else DagDetails(id=dag_id),",
          "",
          "---------------"
        ],
        "airflow/www/extensions/init_jinja_globals.py||airflow/www/extensions/init_jinja_globals.py": [
          "File: airflow/www/extensions/init_jinja_globals.py -> airflow/www/extensions/init_jinja_globals.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "69:             \"git_version\": git_version,",
          "70:             \"k8s_or_k8scelery_executor\": IS_K8S_OR_K8SCELERY_EXECUTOR,",
          "71:             \"rest_api_enabled\": False,",
          "73:             \"config_test_connection\": conf.get(\"core\", \"test_connection\", fallback=\"Disabled\"),",
          "74:         }",
          "76:         backends = conf.get(\"api\", \"auth_backends\")",
          "77:         if backends and backends[0] != \"airflow.api.auth.backend.deny_all\":",
          "78:             extra_globals[\"rest_api_enabled\"] = True",
          "",
          "[Removed Lines]",
          "72:             \"auth_manager\": get_auth_manager(),",
          "",
          "[Added Lines]",
          "75:         # Extra global specific to auth manager",
          "76:         extra_globals[\"auth_manager\"] = get_auth_manager()",
          "",
          "---------------"
        ],
        "airflow/www/security_manager.py||airflow/www/security_manager.py": [
          "File: airflow/www/security_manager.py -> airflow/www/security_manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: import itertools",
          "20: import warnings",
          "23: from flask import g",
          "24: from sqlalchemy import or_, select",
          "25: from sqlalchemy.orm import joinedload",
          "28: from airflow.auth.managers.fab.views.permissions import (",
          "29:     ActionModelView,",
          "30:     PermissionPairModelView,",
          "",
          "[Removed Lines]",
          "21: from typing import TYPE_CHECKING, Any, Collection, Container, Iterable, Sequence",
          "27: from airflow.auth.managers.fab.models import Permission, Resource, Role, User",
          "",
          "[Added Lines]",
          "21: from typing import TYPE_CHECKING, Any, Collection, Iterable, Sequence",
          "27: from airflow.auth.managers.fab.models import Permission, Resource, Role",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "48: from airflow.models import DagBag, DagModel",
          "49: from airflow.security import permissions",
          "50: from airflow.utils.log.logging_mixin import LoggingMixin",
          "53: from airflow.www.fab_security.sqla.manager import SecurityManager",
          "54: from airflow.www.utils import CustomSQLAInterface",
          "",
          "[Removed Lines]",
          "51: from airflow.utils.session import NEW_SESSION, provide_session",
          "52: from airflow.www.extensions.init_auth_manager import get_auth_manager",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "62: }",
          "64: if TYPE_CHECKING:",
          "68: class AirflowSecurityManagerV2(SecurityManager, LoggingMixin):",
          "",
          "[Removed Lines]",
          "65:     from sqlalchemy.orm import Session",
          "",
          "[Added Lines]",
          "64:     pass",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "269:             user = g.user",
          "270:         return user.roles",
          "392:     def prefixed_dag_id(self, dag_id: str) -> str:",
          "393:         \"\"\"Return the permission name for a DAG id.\"\"\"",
          "394:         warnings.warn(",
          "",
          "[Removed Lines]",
          "272:     def get_readable_dags(self, user) -> Iterable[DagModel]:",
          "273:         \"\"\"Get the DAGs readable by authenticated user.\"\"\"",
          "274:         warnings.warn(",
          "275:             \"`get_readable_dags` has been deprecated. Please use `get_readable_dag_ids` instead.\",",
          "276:             RemovedInAirflow3Warning,",
          "277:             stacklevel=2,",
          "278:         )",
          "279:         with warnings.catch_warnings():",
          "280:             warnings.simplefilter(\"ignore\", RemovedInAirflow3Warning)",
          "281:             return self.get_accessible_dags([permissions.ACTION_CAN_READ], user)",
          "283:     def get_editable_dags(self, user) -> Iterable[DagModel]:",
          "284:         \"\"\"Get the DAGs editable by authenticated user.\"\"\"",
          "285:         warnings.warn(",
          "286:             \"`get_editable_dags` has been deprecated. Please use `get_editable_dag_ids` instead.\",",
          "287:             RemovedInAirflow3Warning,",
          "288:             stacklevel=2,",
          "289:         )",
          "290:         with warnings.catch_warnings():",
          "291:             warnings.simplefilter(\"ignore\", RemovedInAirflow3Warning)",
          "292:             return self.get_accessible_dags([permissions.ACTION_CAN_EDIT], user)",
          "294:     @provide_session",
          "295:     def get_accessible_dags(",
          "296:         self,",
          "297:         user_actions: Container[str] | None,",
          "298:         user,",
          "299:         session: Session = NEW_SESSION,",
          "300:     ) -> Iterable[DagModel]:",
          "301:         warnings.warn(",
          "302:             \"`get_accessible_dags` has been deprecated. Please use `get_accessible_dag_ids` instead.\",",
          "303:             RemovedInAirflow3Warning,",
          "304:             stacklevel=3,",
          "305:         )",
          "306:         dag_ids = self.get_accessible_dag_ids(user, user_actions, session)",
          "307:         return session.scalars(select(DagModel).where(DagModel.dag_id.in_(dag_ids)))",
          "309:     def get_readable_dag_ids(self, user) -> set[str]:",
          "310:         \"\"\"Get the DAG IDs readable by authenticated user.\"\"\"",
          "311:         return self.get_accessible_dag_ids(user, [permissions.ACTION_CAN_READ])",
          "313:     def get_editable_dag_ids(self, user) -> set[str]:",
          "314:         \"\"\"Get the DAG IDs editable by authenticated user.\"\"\"",
          "315:         return self.get_accessible_dag_ids(user, [permissions.ACTION_CAN_EDIT])",
          "317:     @provide_session",
          "318:     def get_accessible_dag_ids(",
          "319:         self,",
          "320:         user,",
          "321:         user_actions: Container[str] | None = None,",
          "322:         session: Session = NEW_SESSION,",
          "323:     ) -> set[str]:",
          "324:         \"\"\"Get readable or writable DAGs for user.\"\"\"",
          "325:         if not user_actions:",
          "326:             user_actions = [permissions.ACTION_CAN_EDIT, permissions.ACTION_CAN_READ]",
          "328:         if not get_auth_manager().is_logged_in():",
          "329:             roles = user.roles",
          "330:         else:",
          "331:             if (permissions.ACTION_CAN_EDIT in user_actions and self.can_edit_all_dags(user)) or (",
          "332:                 permissions.ACTION_CAN_READ in user_actions and self.can_read_all_dags(user)",
          "333:             ):",
          "334:                 return {dag.dag_id for dag in session.execute(select(DagModel.dag_id))}",
          "335:             user_query = session.scalar(",
          "336:                 select(User)",
          "337:                 .options(",
          "338:                     joinedload(User.roles)",
          "339:                     .subqueryload(Role.permissions)",
          "340:                     .options(joinedload(Permission.action), joinedload(Permission.resource))",
          "341:                 )",
          "342:                 .where(User.id == user.id)",
          "343:             )",
          "344:             roles = user_query.roles",
          "346:         resources = set()",
          "347:         for role in roles:",
          "348:             for permission in role.permissions:",
          "349:                 action = permission.action.name",
          "350:                 if action in user_actions:",
          "351:                     resource = permission.resource.name",
          "352:                     if resource == permissions.RESOURCE_DAG:",
          "353:                         return {dag.dag_id for dag in session.execute(select(DagModel.dag_id))}",
          "354:                     if resource.startswith(permissions.RESOURCE_DAG_PREFIX):",
          "355:                         resources.add(resource[len(permissions.RESOURCE_DAG_PREFIX) :])",
          "356:                     else:",
          "357:                         resources.add(resource)",
          "358:         return {",
          "359:             dag.dag_id",
          "360:             for dag in session.execute(select(DagModel.dag_id).where(DagModel.dag_id.in_(resources)))",
          "361:         }",
          "363:     def can_access_some_dags(self, action: str, dag_id: str | None = None) -> bool:",
          "364:         \"\"\"Check if user has read or write access to some dags.\"\"\"",
          "365:         if dag_id and dag_id != \"~\":",
          "366:             root_dag_id = self._get_root_dag_id(dag_id)",
          "367:             return self.has_access(action, permissions.resource_name_for_dag(root_dag_id))",
          "369:         user = g.user",
          "370:         if action == permissions.ACTION_CAN_READ:",
          "371:             return any(self.get_readable_dag_ids(user))",
          "372:         return any(self.get_editable_dag_ids(user))",
          "374:     def can_read_dag(self, dag_id: str, user=None) -> bool:",
          "375:         \"\"\"Determine whether a user has DAG read access.\"\"\"",
          "376:         root_dag_id = self._get_root_dag_id(dag_id)",
          "377:         dag_resource_name = permissions.resource_name_for_dag(root_dag_id)",
          "378:         return self.has_access(permissions.ACTION_CAN_READ, dag_resource_name, user=user)",
          "380:     def can_edit_dag(self, dag_id: str, user=None) -> bool:",
          "381:         \"\"\"Determine whether a user has DAG edit access.\"\"\"",
          "382:         root_dag_id = self._get_root_dag_id(dag_id)",
          "383:         dag_resource_name = permissions.resource_name_for_dag(root_dag_id)",
          "384:         return self.has_access(permissions.ACTION_CAN_EDIT, dag_resource_name, user=user)",
          "386:     def can_delete_dag(self, dag_id: str, user=None) -> bool:",
          "387:         \"\"\"Determine whether a user has DAG delete access.\"\"\"",
          "388:         root_dag_id = self._get_root_dag_id(dag_id)",
          "389:         dag_resource_name = permissions.resource_name_for_dag(root_dag_id)",
          "390:         return self.has_access(permissions.ACTION_CAN_DELETE, dag_resource_name, user=user)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "431:         return False",
          "463:     def clean_perms(self) -> None:",
          "464:         \"\"\"FAB leaves faulty permissions that need to be cleaned up.\"\"\"",
          "465:         self.log.debug(\"Cleaning faulty perms\")",
          "",
          "[Removed Lines]",
          "433:     def _has_role(self, role_name_or_list: Container, user) -> bool:",
          "434:         \"\"\"Whether the user has this role name.\"\"\"",
          "435:         if not isinstance(role_name_or_list, list):",
          "436:             role_name_or_list = [role_name_or_list]",
          "437:         return any(r.name in role_name_or_list for r in user.roles)",
          "439:     def has_all_dags_access(self, user) -> bool:",
          "440:         \"\"\"",
          "441:         Has all the dag access in any of the 3 cases.",
          "443:         1. Role needs to be in (Admin, Viewer, User, Op).",
          "444:         2. Has can_read action on dags resource.",
          "445:         3. Has can_edit action on dags resource.",
          "446:         \"\"\"",
          "447:         if not user:",
          "448:             user = g.user",
          "449:         return (",
          "450:             self._has_role([\"Admin\", \"Viewer\", \"Op\", \"User\"], user)",
          "451:             or self.can_read_all_dags(user)",
          "452:             or self.can_edit_all_dags(user)",
          "453:         )",
          "455:     def can_edit_all_dags(self, user=None) -> bool:",
          "456:         \"\"\"Has can_edit action on DAG resource.\"\"\"",
          "457:         return self.has_access(permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG, user)",
          "459:     def can_read_all_dags(self, user=None) -> bool:",
          "460:         \"\"\"Has can_read action on DAG resource.\"\"\"",
          "461:         return self.has_access(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG, user)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "740:         perms: Sequence[tuple[str, str]] | None = None,",
          "741:         dag_id: str | None = None,",
          "742:     ) -> bool:",
          "",
          "[Removed Lines]",
          "743:         \"\"\"Check that the logged in user has the specified permissions.\"\"\"",
          "744:         if not perms:",
          "745:             return True",
          "747:         for perm in perms:",
          "748:             if perm in (",
          "749:                 (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "750:                 (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),",
          "751:                 (permissions.ACTION_CAN_DELETE, permissions.RESOURCE_DAG),",
          "752:             ):",
          "753:                 can_access_all_dags = self.has_access(*perm)",
          "754:                 if not can_access_all_dags:",
          "755:                     action = perm[0]",
          "756:                     if not self.can_access_some_dags(action, dag_id):",
          "757:                         return False",
          "758:             elif not self.has_access(*perm):",
          "759:                 return False",
          "761:         return True",
          "",
          "[Added Lines]",
          "592:         raise NotImplementedError(",
          "593:             \"The method 'check_authorization' is only available with the auth manager FabAuthManager\"",
          "594:         )",
          "",
          "---------------"
        ],
        "airflow/www/templates/airflow/dag.html||airflow/www/templates/airflow/dag.html": [
          "File: airflow/www/templates/airflow/dag.html -> airflow/www/templates/airflow/dag.html",
          "--- Hunk 1 ---",
          "[Context before]",
          "110:         {% if dag.parent_dag is defined and dag.parent_dag %}",
          "111:           <span class=\"text-muted\">SUBDAG:</span> {{ dag.dag_id }}",
          "112:         {% else %}",
          "115:             {% set switch_tooltip = 'Pause/Unpause DAG' %}",
          "116:           {% else %}",
          "117:             {% set switch_tooltip = 'DAG is Paused' if dag_is_paused else 'DAG is Active' %}",
          "118:           {% endif %}",
          "120:             <input class=\"switch-input\" id=\"pause_resume\" data-dag-id=\"{{ dag.dag_id }}\"",
          "121:                    type=\"checkbox\"{{ \" checked\" if not dag_is_paused else \"\" }}",
          "123:             <span class=\"switch\" aria-hidden=\"true\"></span>",
          "124:           </label>",
          "125:           <span class=\"text-muted\">DAG:</span> {{ dag.dag_id }}",
          "",
          "[Removed Lines]",
          "113:           {% set can_edit = appbuilder.sm.can_edit_dag(dag.dag_id) %}",
          "114:           {% if appbuilder.sm.can_edit_dag(dag.dag_id) %}",
          "119:           <label class=\"switch-label{{' disabled' if not can_edit else ''  }} js-tooltip\" title=\"{{ switch_tooltip }}\">",
          "122:                    {{ \" disabled\" if not can_edit else \"\" }}>",
          "",
          "[Added Lines]",
          "113:           {% if can_edit_dag %}",
          "118:           <label class=\"switch-label{{' disabled' if not can_edit_dag else ''  }} js-tooltip\" title=\"{{ switch_tooltip }}\">",
          "121:                    {{ \" disabled\" if not can_edit_dag else \"\" }}>",
          "",
          "---------------"
        ],
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "83:     set_dag_run_state_to_success,",
          "84:     set_state,",
          "85: )",
          "87: from airflow.compat.functools import cache",
          "88: from airflow.configuration import AIRFLOW_CONFIG, conf",
          "89: from airflow.datasets import Dataset",
          "",
          "[Removed Lines]",
          "86: from airflow.auth.managers.models.resource_details import DagAccessEntity",
          "",
          "[Added Lines]",
          "86: from airflow.auth.managers.models.resource_details import DagAccessEntity, DagDetails",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "699:         # Add triggerer_job only if we need it",
          "700:         if TriggererJobRunner.is_needed():",
          "701:             kwargs[\"triggerer_job\"] = lazy_object_proxy.Proxy(TriggererJobRunner.most_recent_job)",
          "702:         return super().render_template(",
          "704:             # Cache this at most once per request, not for the lifetime of the view instance",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "703:         if \"dag\" in kwargs:",
          "704:             kwargs[\"can_edit_dag\"] = get_auth_manager().is_authorized_dag(",
          "705:                 method=\"PUT\", details=DagDetails(id=kwargs[\"dag\"].dag_id)",
          "706:             )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "768:         end = start + dags_per_page",
          "770:         # Get all the dag id the user could access",
          "773:         with create_session() as session:",
          "774:             # read orm_dags from the db",
          "",
          "[Removed Lines]",
          "771:         filter_dag_ids = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
          "",
          "[Added Lines]",
          "777:         filter_dag_ids = get_auth_manager().get_permitted_dag_ids(user=g.user)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "896:                 .unique()",
          "897:                 .all()",
          "898:             )",
          "905:             dataset_triggered_dag_ids = {dag.dag_id for dag in dags if dag.schedule_interval == \"Dataset\"}",
          "906:             if dataset_triggered_dag_ids:",
          "",
          "[Removed Lines]",
          "899:             user_permissions = g.user.perms",
          "900:             can_create_dag_run = (",
          "901:                 permissions.ACTION_CAN_CREATE,",
          "902:                 permissions.RESOURCE_DAG_RUN,",
          "903:             ) in user_permissions",
          "",
          "[Added Lines]",
          "905:             can_create_dag_run = get_auth_manager().is_authorized_dag(",
          "906:                 method=\"POST\", access_entity=DagAccessEntity.RUN, user=g.user",
          "907:             )",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "911:                 dataset_triggered_next_run_info = {}",
          "913:             for dag in dags:",
          "915:                 dag.can_trigger = dag.can_edit and can_create_dag_run",
          "918:             dagtags = session.execute(select(func.distinct(DagTag.name)).order_by(DagTag.name)).all()",
          "919:             tags = [",
          "",
          "[Removed Lines]",
          "914:                 dag.can_edit = get_airflow_app().appbuilder.sm.can_edit_dag(dag.dag_id, g.user)",
          "916:                 dag.can_delete = get_airflow_app().appbuilder.sm.can_delete_dag(dag.dag_id, g.user)",
          "",
          "[Added Lines]",
          "918:                 dag.can_edit = get_auth_manager().is_authorized_dag(",
          "919:                     method=\"PUT\", details=DagDetails(id=dag.dag_id), user=g.user",
          "920:                 )",
          "922:                 dag.can_delete = get_auth_manager().is_authorized_dag(",
          "923:                     method=\"DELETE\", details=DagDetails(id=dag.dag_id), user=g.user",
          "924:                 )",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "926:             import_errors = select(errors.ImportError).order_by(errors.ImportError.id)",
          "929:                 # if the user doesn't have access to all DAGs, only display errors from visible DAGs",
          "930:                 import_errors = import_errors.join(",
          "931:                     DagModel, DagModel.fileloc == errors.ImportError.filename",
          "",
          "[Removed Lines]",
          "928:             if (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG) not in user_permissions:",
          "",
          "[Added Lines]",
          "936:             if not get_auth_manager().is_authorized_dag(method=\"GET\"):",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "968:                         # Second segment is a version marker that we don't need to show.",
          "969:                         yield segments[-1], table_name",
          "975:             robots_file_access_count = (",
          "976:                 select(Log)",
          "977:                 .where(Log.event == \"robots\")",
          "",
          "[Removed Lines]",
          "971:         if (",
          "972:             permissions.ACTION_CAN_ACCESS_MENU,",
          "973:             permissions.RESOURCE_ADMIN_MENU,",
          "974:         ) in user_permissions and conf.getboolean(\"webserver\", \"warn_deployment_exposure\"):",
          "",
          "[Added Lines]",
          "979:         if get_auth_manager().is_authorized_configuration(method=\"GET\", user=g.user) and conf.getboolean(",
          "980:             \"webserver\", \"warn_deployment_exposure\"",
          "981:         ):",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "1057:         )",
          "1059:     @expose(\"/next_run_datasets_summary\", methods=[\"POST\"])",
          "1061:     @provide_session",
          "1062:     def next_run_datasets_summary(self, session: Session = NEW_SESSION):",
          "1063:         \"\"\"Next run info for dataset triggered DAGs.\"\"\"",
          "1066:         if not allowed_dag_ids:",
          "1067:             return flask.json.jsonify({})",
          "",
          "[Removed Lines]",
          "1060:     @auth.has_access_dag(\"GET\")",
          "1064:         allowed_dag_ids = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
          "",
          "[Added Lines]",
          "1070:         allowed_dag_ids = get_auth_manager().get_permitted_dag_ids(user=g.user)",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "1096:     @provide_session",
          "1097:     def dag_stats(self, session: Session = NEW_SESSION):",
          "1098:         \"\"\"Dag statistics.\"\"\"",
          "1101:         # Filter by post parameters",
          "1102:         selected_dag_ids = {unquote(dag_id) for dag_id in request.form.getlist(\"dag_ids\") if dag_id}",
          "",
          "[Removed Lines]",
          "1099:         allowed_dag_ids = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
          "",
          "[Added Lines]",
          "1105:         allowed_dag_ids = get_auth_manager().get_permitted_dag_ids(user=g.user)",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1128:     @provide_session",
          "1129:     def task_stats(self, session: Session = NEW_SESSION):",
          "1130:         \"\"\"Task Statistics.\"\"\"",
          "1133:         if not allowed_dag_ids:",
          "1134:             return flask.json.jsonify({})",
          "",
          "[Removed Lines]",
          "1131:         allowed_dag_ids = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
          "",
          "[Added Lines]",
          "1137:         allowed_dag_ids = get_auth_manager().get_permitted_dag_ids(user=g.user)",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "1227:     @provide_session",
          "1228:     def last_dagruns(self, session: Session = NEW_SESSION):",
          "1229:         \"\"\"Last DAG runs.\"\"\"",
          "1232:         # Filter by post parameters",
          "1233:         selected_dag_ids = {unquote(dag_id) for dag_id in request.form.getlist(\"dag_ids\") if dag_id}",
          "",
          "[Removed Lines]",
          "1230:         allowed_dag_ids = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
          "",
          "[Added Lines]",
          "1236:         allowed_dag_ids = get_auth_manager().get_permitted_dag_ids(user=g.user)",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "2334:     @provide_session",
          "2335:     def blocked(self, session: Session = NEW_SESSION):",
          "2336:         \"\"\"Mark Dag Blocked.\"\"\"",
          "2339:         # Filter by post parameters",
          "2340:         selected_dag_ids = {unquote(dag_id) for dag_id in request.form.getlist(\"dag_ids\") if dag_id}",
          "",
          "[Removed Lines]",
          "2337:         allowed_dag_ids = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
          "",
          "[Added Lines]",
          "2343:         allowed_dag_ids = get_auth_manager().get_permitted_dag_ids(user=g.user)",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "3558:         )",
          "3560:     @expose(\"/object/next_run_datasets/<string:dag_id>\")",
          "3562:     def next_run_datasets(self, dag_id):",
          "3563:         \"\"\"Return datasets necessary, and their status, for the next dag run.\"\"\"",
          "3564:         dag = get_airflow_app().dag_bag.get_dag(dag_id)",
          "",
          "[Removed Lines]",
          "3561:     @auth.has_access_dag(\"GET\", DagAccessEntity.DATASET)",
          "",
          "[Added Lines]",
          "3567:     @auth.has_access_dag(\"GET\", DagAccessEntity.RUN)",
          "3568:     @auth.has_access_dataset(\"GET\")",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "3901:     \"\"\"Filter using DagIDs.\"\"\"",
          "3903:     def apply(self, query, func):",
          "3905:             return query",
          "3907:         return query.where(self.model.dag_id.in_(filter_dag_ids))",
          "",
          "[Removed Lines]",
          "3904:         if get_airflow_app().appbuilder.sm.has_all_dags_access(g.user):",
          "3906:         filter_dag_ids = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
          "",
          "[Added Lines]",
          "3911:         if get_auth_manager().is_authorized_dag(method=\"GET\", user=g.user):",
          "3912:             return query",
          "3913:         if get_auth_manager().is_authorized_dag(method=\"PUT\", user=g.user):",
          "3915:         filter_dag_ids = get_auth_manager().get_permitted_dag_ids(user=g.user)",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "3953:     @staticmethod",
          "3954:     def validate_dag_edit_access(item: DagRun | TaskInstance):",
          "3955:         \"\"\"Validate whether the user has 'can_edit' access for this specific DAG.\"\"\"",
          "3957:             raise AirflowException(f\"Access denied for dag_id {item.dag_id}\")",
          "3959:     def pre_add(self, item: DagRun | TaskInstance):",
          "",
          "[Removed Lines]",
          "3956:         if not get_airflow_app().appbuilder.sm.can_edit_dag(item.dag_id):",
          "",
          "[Added Lines]",
          "3965:         if not get_auth_manager().is_authorized_dag(method=\"PUT\", details=DagDetails(id=item.dag_id)):",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "3999:             )",
          "4001:         for dag_id in dag_ids:",
          "4003:                 flash(f\"Access denied for dag_id {dag_id}\", \"danger\")",
          "4004:                 logging.warning(\"User %s tried to modify %s without having access.\", g.user.username, dag_id)",
          "4005:                 return redirect(self.get_default_url())",
          "",
          "[Removed Lines]",
          "4002:             if not get_airflow_app().appbuilder.sm.can_edit_dag(dag_id):",
          "",
          "[Added Lines]",
          "4011:             if not get_auth_manager().is_authorized_dag(method=\"PUT\", details=DagDetails(id=dag_id)):",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "5694: class AutocompleteView(AirflowBaseView):",
          "5695:     \"\"\"View to provide autocomplete results.\"\"\"",
          "5698:     @provide_session",
          "5699:     @expose(\"/dagmodel/autocomplete\")",
          "5700:     def autocomplete(self, session: Session = NEW_SESSION):",
          "",
          "[Removed Lines]",
          "5697:     @auth.has_access_dag(\"GET\")",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "5728:             dag_ids_query = dag_ids_query.where(DagModel.is_paused)",
          "5729:             owners_query = owners_query.where(DagModel.is_paused)",
          "5733:         dag_ids_query = dag_ids_query.where(DagModel.dag_id.in_(filter_dag_ids))",
          "5734:         owners_query = owners_query.where(DagModel.dag_id.in_(filter_dag_ids))",
          "",
          "[Removed Lines]",
          "5731:         filter_dag_ids = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
          "",
          "[Added Lines]",
          "5739:         filter_dag_ids = get_auth_manager().get_permitted_dag_ids(user=g.user)",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "5813:         permissions.ACTION_CAN_CREATE, permissions.RESOURCE_DAG_RUN",
          "5814:     )",
          "5817:     dag.can_trigger = dag.can_edit and can_create_dag_run",
          "5819:     context[\"dag\"] = dag",
          "",
          "[Removed Lines]",
          "5816:     dag.can_edit = get_airflow_app().appbuilder.sm.can_edit_dag(dag.dag_id)",
          "5818:     dag.can_delete = get_airflow_app().appbuilder.sm.can_delete_dag(dag.dag_id)",
          "",
          "[Added Lines]",
          "5824:     dag.can_edit = get_auth_manager().is_authorized_dag(method=\"PUT\", details=DagDetails(id=dag.dag_id))",
          "5826:     dag.can_delete = get_auth_manager().is_authorized_dag(method=\"DELETE\", details=DagDetails(id=dag.dag_id))",
          "",
          "---------------"
        ],
        "tests/api_connexion/endpoints/test_event_log_endpoint.py||tests/api_connexion/endpoints/test_event_log_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_event_log_endpoint.py -> tests/api_connexion/endpoints/test_event_log_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "36:         role_name=\"Test\",",
          "37:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_AUDIT_LOG)],  # type: ignore",
          "38:     )",
          "39:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
          "41:     yield app",
          "43:     delete_user(app, username=\"test\")  # type: ignore",
          "44:     delete_user(app, username=\"test_no_permissions\")  # type: ignore",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "39:     create_user(",
          "40:         app,  # type:ignore",
          "41:         username=\"test_granular\",",
          "42:         role_name=\"TestGranular\",",
          "43:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_AUDIT_LOG)],  # type: ignore",
          "44:     )",
          "45:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
          "46:         \"TEST_DAG_ID_1\",",
          "47:         access_control={\"TestGranular\": [permissions.ACTION_CAN_READ]},",
          "48:     )",
          "49:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
          "50:         \"TEST_DAG_ID_2\",",
          "51:         access_control={\"TestGranular\": [permissions.ACTION_CAN_READ]},",
          "52:     )",
          "58:     delete_user(app, username=\"test_granular\")  # type: ignore",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "253:         for attr in [\"dag_id\", \"task_id\", \"owner\", \"event\"]:",
          "254:             attr_value = f\"TEST_{attr}_1\".upper()",
          "255:             response = self.client.get(",
          "257:             )",
          "258:             assert response.status_code == 200",
          "259:             assert {eventlog[attr] for eventlog in response.json[\"event_logs\"]} == {attr_value}",
          "",
          "[Removed Lines]",
          "256:                 f\"/api/v1/eventLogs?{attr}={attr_value}\", environ_overrides={\"REMOTE_USER\": \"test\"}",
          "",
          "[Added Lines]",
          "271:                 f\"/api/v1/eventLogs?{attr}={attr_value}\", environ_overrides={\"REMOTE_USER\": \"test_granular\"}",
          "",
          "---------------"
        ],
        "tests/api_connexion/endpoints/test_log_endpoint.py||tests/api_connexion/endpoints/test_log_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_log_endpoint.py -> tests/api_connexion/endpoints/test_log_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "47:         role_name=\"Test\",",
          "48:         permissions=[",
          "49:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "52:         ],",
          "53:     )",
          "54:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")",
          "",
          "[Removed Lines]",
          "50:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "51:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_INSTANCE),",
          "",
          "[Added Lines]",
          "50:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_LOG),",
          "",
          "---------------"
        ],
        "tests/api_connexion/endpoints/test_xcom_endpoint.py||tests/api_connexion/endpoints/test_xcom_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_xcom_endpoint.py -> tests/api_connexion/endpoints/test_xcom_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "55:         role_name=\"Test\",",
          "56:         permissions=[",
          "57:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "60:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_XCOM),",
          "61:         ],",
          "62:     )",
          "",
          "[Removed Lines]",
          "58:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "59:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_INSTANCE),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "65:         username=\"test_granular_permissions\",",
          "66:         role_name=\"TestGranularDag\",",
          "67:         permissions=[",
          "70:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_XCOM),",
          "71:         ],",
          "72:     )",
          "",
          "[Removed Lines]",
          "68:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "69:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_INSTANCE),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/auth/managers/fab/test_fab_auth_manager.py||tests/auth/managers/fab/test_fab_auth_manager.py": [
          "File: tests/auth/managers/fab/test_fab_auth_manager.py -> tests/auth/managers/fab/test_fab_auth_manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "253:                 [(ACTION_CAN_READ, RESOURCE_DAG), (ACTION_CAN_READ, RESOURCE_DAG_RUN)],",
          "254:                 True,",
          "255:             ),",
          "257:             (",
          "258:                 \"GET\",",
          "259:                 DagAccessEntity.TASK_INSTANCE,",
          "260:                 DagDetails(id=\"test_dag_id\"),",
          "261:                 [(ACTION_CAN_READ, \"DAG:test_dag_id\"), (ACTION_CAN_READ, RESOURCE_TASK_INSTANCE)],",
          "262:                 True,",
          "263:             ),",
          "264:             # With edit permissions on a specific DAG and read on the DAG access entity",
          "",
          "[Removed Lines]",
          "256:             # With read permissions on a specific DAG",
          "",
          "[Added Lines]",
          "256:             # Without read permissions on a specific DAG",
          "257:             (",
          "258:                 \"GET\",",
          "259:                 DagAccessEntity.TASK_INSTANCE,",
          "260:                 DagDetails(id=\"test_dag_id\"),",
          "261:                 [(ACTION_CAN_READ, RESOURCE_TASK_INSTANCE)],",
          "262:                 False,",
          "263:             ),",
          "264:             # With read permissions on a specific DAG but not on the DAG run",
          "270:                 False,",
          "271:             ),",
          "272:             # With read permissions on a specific DAG but not on the DAG run",
          "273:             (",
          "274:                 \"GET\",",
          "275:                 DagAccessEntity.TASK_INSTANCE,",
          "276:                 DagDetails(id=\"test_dag_id\"),",
          "277:                 [",
          "278:                     (ACTION_CAN_READ, \"DAG:test_dag_id\"),",
          "279:                     (ACTION_CAN_READ, RESOURCE_TASK_INSTANCE),",
          "280:                     (ACTION_CAN_READ, RESOURCE_DAG_RUN),",
          "281:                 ],",
          "282:                 True,",
          "283:             ),",
          "284:             # With edit permissions on a specific DAG and read on the DAG access entity",
          "285:             (",
          "286:                 \"DELETE\",",
          "287:                 DagAccessEntity.TASK,",
          "288:                 DagDetails(id=\"test_dag_id\"),",
          "289:                 [(ACTION_CAN_EDIT, \"DAG:test_dag_id\"), (ACTION_CAN_DELETE, RESOURCE_TASK_INSTANCE)],",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "293:         user = Mock()",
          "294:         user.perms = user_permissions",
          "295:         result = auth_manager.is_authorized_dag(",
          "297:         )",
          "298:         assert result == expected_result",
          "",
          "[Removed Lines]",
          "296:             method=method, dag_access_entity=dag_access_entity, dag_details=dag_details, user=user",
          "",
          "[Added Lines]",
          "324:             method=method, access_entity=dag_access_entity, details=dag_details, user=user",
          "",
          "---------------"
        ],
        "tests/auth/managers/test_base_auth_manager.py||tests/auth/managers/test_base_auth_manager.py": [
          "File: tests/auth/managers/test_base_auth_manager.py -> tests/auth/managers/test_base_auth_manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "28: if TYPE_CHECKING:",
          "29:     from airflow.auth.managers.models.base_user import BaseUser",
          "33: class EmptyAuthManager(BaseAuthManager):",
          "",
          "[Removed Lines]",
          "30:     from airflow.auth.managers.models.resource_details import ConnectionDetails, DagAccessEntity, DagDetails",
          "",
          "[Added Lines]",
          "30:     from airflow.auth.managers.models.resource_details import (",
          "31:         ConfigurationDetails,",
          "32:         ConnectionDetails,",
          "33:         DagAccessEntity,",
          "34:         DagDetails,",
          "35:         DatasetDetails,",
          "36:         PoolDetails,",
          "37:         VariableDetails,",
          "38:     )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "40:     def get_user_id(self) -> str:",
          "41:         raise NotImplementedError()",
          "44:         raise NotImplementedError()",
          "46:     def is_authorized_cluster_activity(self, *, method: ResourceMethod, user: BaseUser | None = None) -> bool:",
          "",
          "[Removed Lines]",
          "43:     def is_authorized_configuration(self, *, method: ResourceMethod, user: BaseUser | None = None) -> bool:",
          "",
          "[Added Lines]",
          "51:     def is_authorized_configuration(",
          "52:         self,",
          "54:         method: ResourceMethod,",
          "55:         details: ConfigurationDetails | None = None,",
          "56:         user: BaseUser | None = None,",
          "57:     ) -> bool:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "50:         self,",
          "52:         method: ResourceMethod,",
          "54:         user: BaseUser | None = None,",
          "55:     ) -> bool:",
          "56:         raise NotImplementedError()",
          "",
          "[Removed Lines]",
          "53:         connection_details: ConnectionDetails | None = None,",
          "",
          "[Added Lines]",
          "67:         details: ConnectionDetails | None = None,",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "59:         self,",
          "61:         method: ResourceMethod,",
          "64:         user: BaseUser | None = None,",
          "65:     ) -> bool:",
          "66:         raise NotImplementedError()",
          "69:         raise NotImplementedError()",
          "72:         raise NotImplementedError()",
          "74:     def is_authorized_website(self, *, user: BaseUser | None = None) -> bool:",
          "",
          "[Removed Lines]",
          "62:         dag_access_entity: DagAccessEntity | None = None,",
          "63:         dag_details: DagDetails | None = None,",
          "68:     def is_authorized_dataset(self, *, method: ResourceMethod, user: BaseUser | None = None) -> bool:",
          "71:     def is_authorized_variable(self, *, method: ResourceMethod, user: BaseUser | None = None) -> bool:",
          "",
          "[Added Lines]",
          "76:         access_entity: DagAccessEntity | None = None,",
          "77:         details: DagDetails | None = None,",
          "82:     def is_authorized_dataset(",
          "83:         self, *, method: ResourceMethod, details: DatasetDetails | None = None, user: BaseUser | None = None",
          "84:     ) -> bool:",
          "87:     def is_authorized_pool(",
          "88:         self, *, method: ResourceMethod, details: PoolDetails | None = None, user: BaseUser | None = None",
          "89:     ) -> bool:",
          "90:         raise NotImplementedError()",
          "92:     def is_authorized_variable(",
          "93:         self, *, method: ResourceMethod, details: VariableDetails | None = None, user: BaseUser | None = None",
          "94:     ) -> bool:",
          "",
          "---------------"
        ],
        "tests/www/test_security.py||tests/www/test_security.py": [
          "File: tests/www/test_security.py -> tests/www/test_security.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "33: from airflow.auth.managers.fab.fab_auth_manager import FabAuthManager",
          "34: from airflow.auth.managers.fab.models import User, assoc_permission_role",
          "35: from airflow.auth.managers.fab.models.anonymous_user import AnonymousUser",
          "36: from airflow.configuration import initialize_config",
          "37: from airflow.exceptions import AirflowException",
          "38: from airflow.models import DagModel",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "36: from airflow.auth.managers.models.resource_details import DagDetails",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "41: from airflow.security import permissions",
          "42: from airflow.www import app as application",
          "43: from airflow.www.auth import get_access_denied_message",
          "44: from airflow.www.utils import CustomSQLAInterface",
          "45: from tests.test_utils.api_connexion_utils import (",
          "46:     create_user,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "45: from airflow.www.extensions.init_auth_manager import get_auth_manager",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "118:     _delete_dag_permissions(dag_model.dag_id, security_manager)",
          "121: @contextlib.contextmanager",
          "122: def _create_dag_model_context(dag_id, session, security_manager):",
          "123:     dag = _create_dag_model(dag_id, session, security_manager)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "123: def _can_read_dag(dag_id: str, user) -> bool:",
          "124:     return get_auth_manager().is_authorized_dag(method=\"GET\", details=DagDetails(id=dag_id), user=user)",
          "127: def _can_edit_dag(dag_id: str, user) -> bool:",
          "128:     return get_auth_manager().is_authorized_dag(method=\"PUT\", details=DagDetails(id=dag_id), user=user)",
          "131: def _can_delete_dag(dag_id: str, user) -> bool:",
          "132:     return get_auth_manager().is_authorized_dag(method=\"DELETE\", details=DagDetails(id=dag_id), user=user)",
          "135: def _has_all_dags_access(user) -> bool:",
          "136:     return get_auth_manager().is_authorized_dag(",
          "137:         method=\"GET\", user=user",
          "138:     ) or get_auth_manager().is_authorized_dag(method=\"PUT\", user=user)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "321:         with _create_dag_model_context(\"test_dag_id\", session, security_manager):",
          "322:             security_manager.sync_roles()",
          "327: def test_verify_default_anon_user_has_no_access_to_specific_dag(app, session, security_manager, has_dag_perm):",
          "",
          "[Removed Lines]",
          "324:             assert security_manager.get_accessible_dag_ids(user) == set()",
          "",
          "[Added Lines]",
          "344:             assert get_auth_manager().get_permitted_dag_ids(user=user) == set()",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "334:         with _create_dag_model_context(dag_id, session, security_manager):",
          "335:             security_manager.sync_roles()",
          "339:             assert has_dag_perm(permissions.ACTION_CAN_READ, dag_id, user) is False",
          "340:             assert has_dag_perm(permissions.ACTION_CAN_EDIT, dag_id, user) is False",
          "",
          "[Removed Lines]",
          "337:             assert security_manager.can_read_dag(dag_id, user) is False",
          "338:             assert security_manager.can_edit_dag(dag_id, user) is False",
          "",
          "[Added Lines]",
          "357:             assert _can_read_dag(dag_id, user) is False",
          "358:             assert _can_edit_dag(dag_id, user) is False",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "360:         security_manager.sync_roles()",
          "365: def test_verify_anon_user_with_admin_role_has_access_to_each_dag(",
          "",
          "[Removed Lines]",
          "362:         assert security_manager.get_accessible_dag_ids(user) == set(test_dag_ids)",
          "",
          "[Added Lines]",
          "382:         assert get_auth_manager().get_permitted_dag_ids(user=user) == set(test_dag_ids)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "379:             with _create_dag_model_context(dag_id, session, security_manager):",
          "380:                 security_manager.sync_roles()",
          "384:                 assert has_dag_perm(permissions.ACTION_CAN_READ, dag_id, user) is True",
          "385:                 assert has_dag_perm(permissions.ACTION_CAN_EDIT, dag_id, user) is True",
          "",
          "[Removed Lines]",
          "382:                 assert security_manager.can_read_dag(dag_id, user) is True",
          "383:                 assert security_manager.can_edit_dag(dag_id, user) is True",
          "",
          "[Added Lines]",
          "402:                 assert _can_read_dag(dag_id, user) is True",
          "403:                 assert _can_edit_dag(dag_id, user) is True",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "487:                 dag_id, access_control={role_name: permission_action}",
          "488:             )",
          "493: @patch.object(FabAuthManager, \"is_logged_in\")",
          "",
          "[Removed Lines]",
          "490:             assert security_manager.get_accessible_dag_ids(user) == {\"dag_id\"}",
          "",
          "[Added Lines]",
          "510:             assert get_auth_manager().get_permitted_dag_ids(user=user) == {\"dag_id\"}",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "495:     mock_is_logged_in, app, security_manager, session",
          "496: ):",
          "497:     # In this test case,",
          "499:     username = \"Monsieur User\"",
          "500:     role_name = \"MyRole1\"",
          "501:     permission_action = [permissions.ACTION_CAN_EDIT]",
          "",
          "[Removed Lines]",
          "498:     # get_readable_dag_ids() don't return DAGs to which the user has CAN_EDIT action",
          "",
          "[Added Lines]",
          "518:     # get_permitted_dag_ids() don't return DAGs to which the user has CAN_EDIT action",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "518:                 dag_id, access_control={role_name: permission_action}",
          "519:             )",
          "524: def test_has_access(security_manager):",
          "",
          "[Removed Lines]",
          "521:             assert security_manager.get_readable_dag_ids(user) == set()",
          "",
          "[Added Lines]",
          "541:             assert get_auth_manager().get_permitted_dag_ids(methods=[\"GET\"], user=user) == set()",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "551:             security_manager.sync_perm_for_dag(",
          "552:                 test_dag_id, access_control={test_role: {\"can_read\", \"can_edit\"}}",
          "553:             )",
          "559: def test_sync_perm_for_dag_removes_existing_permissions_if_empty(app, security_manager):",
          "",
          "[Removed Lines]",
          "554:             assert security_manager.can_read_dag(test_dag_id, user)",
          "555:             assert security_manager.can_edit_dag(test_dag_id, user)",
          "556:             assert not security_manager.can_delete_dag(test_dag_id, user)",
          "",
          "[Added Lines]",
          "574:             assert _can_read_dag(test_dag_id, user)",
          "575:             assert _can_edit_dag(test_dag_id, user)",
          "576:             assert not _can_delete_dag(test_dag_id, user)",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "581:                 ]",
          "582:             )",
          "588:             # Need to clear cache on user perms",
          "589:             user._perms = None",
          "591:             security_manager.sync_perm_for_dag(test_dag_id, access_control={test_role: {}})",
          "598: def test_sync_perm_for_dag_removes_permissions_from_other_roles(app, security_manager):",
          "",
          "[Removed Lines]",
          "584:             assert security_manager.can_read_dag(test_dag_id, user)",
          "585:             assert security_manager.can_edit_dag(test_dag_id, user)",
          "586:             assert security_manager.can_delete_dag(test_dag_id, user)",
          "593:             assert not security_manager.can_read_dag(test_dag_id, user)",
          "594:             assert not security_manager.can_edit_dag(test_dag_id, user)",
          "595:             assert not security_manager.can_delete_dag(test_dag_id, user)",
          "",
          "[Added Lines]",
          "604:             assert _can_read_dag(test_dag_id, user)",
          "605:             assert _can_edit_dag(test_dag_id, user)",
          "606:             assert _can_delete_dag(test_dag_id, user)",
          "613:             assert not _can_read_dag(test_dag_id, user)",
          "614:             assert not _can_edit_dag(test_dag_id, user)",
          "615:             assert not _can_delete_dag(test_dag_id, user)",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "621:                 ]",
          "622:             )",
          "628:             # Need to clear cache on user perms",
          "629:             user._perms = None",
          "631:             security_manager.sync_perm_for_dag(test_dag_id, access_control={\"other_role\": {\"can_read\"}})",
          "638: def test_sync_perm_for_dag_does_not_prune_roles_when_access_control_unset(app, security_manager):",
          "",
          "[Removed Lines]",
          "624:             assert security_manager.can_read_dag(test_dag_id, user)",
          "625:             assert security_manager.can_edit_dag(test_dag_id, user)",
          "626:             assert security_manager.can_delete_dag(test_dag_id, user)",
          "633:             assert not security_manager.can_read_dag(test_dag_id, user)",
          "634:             assert not security_manager.can_edit_dag(test_dag_id, user)",
          "635:             assert not security_manager.can_delete_dag(test_dag_id, user)",
          "",
          "[Added Lines]",
          "644:             assert _can_read_dag(test_dag_id, user)",
          "645:             assert _can_edit_dag(test_dag_id, user)",
          "646:             assert _can_delete_dag(test_dag_id, user)",
          "653:             assert not _can_read_dag(test_dag_id, user)",
          "654:             assert not _can_edit_dag(test_dag_id, user)",
          "655:             assert not _can_delete_dag(test_dag_id, user)",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "659:                 ]",
          "660:             )",
          "665:             # Need to clear cache on user perms",
          "666:             user._perms = None",
          "668:             security_manager.sync_perm_for_dag(test_dag_id, access_control=None)",
          "674: def test_has_all_dag_access(app, security_manager):",
          "",
          "[Removed Lines]",
          "662:             assert security_manager.can_read_dag(test_dag_id, user)",
          "663:             assert security_manager.can_edit_dag(test_dag_id, user)",
          "670:             assert security_manager.can_read_dag(test_dag_id, user)",
          "671:             assert security_manager.can_edit_dag(test_dag_id, user)",
          "",
          "[Added Lines]",
          "682:             assert _can_read_dag(test_dag_id, user)",
          "683:             assert _can_edit_dag(test_dag_id, user)",
          "690:             assert _can_read_dag(test_dag_id, user)",
          "691:             assert _can_edit_dag(test_dag_id, user)",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "679:                 username=\"user\",",
          "680:                 role_name=role_name,",
          "681:             ) as user:",
          "684:     with app.app_context():",
          "685:         with create_user_scope(",
          "",
          "[Removed Lines]",
          "682:                 assert security_manager.has_all_dags_access(user)",
          "",
          "[Added Lines]",
          "702:                 assert _has_all_dags_access(user)",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "688:             role_name=\"read_all\",",
          "689:             permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG)],",
          "690:         ) as user:",
          "693:     with app.app_context():",
          "694:         with create_user_scope(",
          "",
          "[Removed Lines]",
          "691:             assert security_manager.has_all_dags_access(user)",
          "",
          "[Added Lines]",
          "711:             assert _has_all_dags_access(user)",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "697:             role_name=\"edit_all\",",
          "698:             permissions=[(permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG)],",
          "699:         ) as user:",
          "702:     with app.app_context():",
          "703:         with create_user_scope(",
          "",
          "[Removed Lines]",
          "700:             assert security_manager.has_all_dags_access(user)",
          "",
          "[Added Lines]",
          "720:             assert _has_all_dags_access(user)",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "706:             role_name=\"nada\",",
          "707:             permissions=[],",
          "708:         ) as user:",
          "712: def test_access_control_with_non_existent_role(security_manager):",
          "",
          "[Removed Lines]",
          "709:             assert not security_manager.has_all_dags_access(user)",
          "",
          "[Added Lines]",
          "729:             assert not _has_all_dags_access(user)",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_acl.py||tests/www/views/test_views_acl.py": [
          "File: tests/www/views/test_views_acl.py -> tests/www/views/test_views_acl.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "488:         role_name=\"role_all_dags_tis\",",
          "489:         permissions=[",
          "490:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "491:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_INSTANCE),",
          "492:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_WEBSITE),",
          "493:         ],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "491:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "537:         role_name=\"role_dags_tis_logs\",",
          "538:         permissions=[",
          "539:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "540:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_INSTANCE),",
          "541:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_LOG),",
          "542:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_WEBSITE),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "541:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "693:         role_name=\"role_all_dags_edit_tis\",",
          "694:         permissions=[",
          "695:             (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),",
          "696:             (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_TASK_INSTANCE),",
          "697:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_WEBSITE),",
          "698:         ],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "698:             (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG_RUN),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "855:         permissions=[",
          "856:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_WEBSITE),",
          "857:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
          "858:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_INSTANCE),",
          "859:             (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_TASK_INSTANCE),",
          "860:             (permissions.ACTION_CAN_EDIT, permissions.resource_name_for_dag(\"example_bash_operator\")),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "861:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "862:             (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG_RUN),",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_decorators.py||tests/www/views/test_views_decorators.py": [
          "File: tests/www/views/test_views_decorators.py -> tests/www/views/test_views_decorators.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import urllib.parse",
          "23: import pytest",
          "26: from airflow.utils import timezone",
          "27: from airflow.utils.state import State",
          "28: from airflow.utils.types import DagRunType",
          "30: from airflow.www.views import action_has_dag_edit_access",
          "31: from tests.test_utils.db import clear_db_runs, clear_db_variables",
          "32: from tests.test_utils.www import _check_last_log, _check_last_log_masked_variable, check_content_in_response",
          "",
          "[Removed Lines]",
          "21: from unittest import mock",
          "25: from airflow.models import DagBag, DagRun, TaskInstance, Variable",
          "29: from airflow.www import app",
          "",
          "[Added Lines]",
          "24: from airflow.models import DagBag, Variable",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "187:     check_content_in_response(expected, resp)",
          "230: def test_action_has_dag_edit_access_exception():",
          "231:     with pytest.raises(ValueError):",
          "232:         some_view_action_which_requires_dag_edit_access(None, \"some_incorrect_value\")",
          "",
          "[Removed Lines]",
          "190: @pytest.mark.parametrize(",
          "191:     \"class_type, no_instances, no_unique_dags\",",
          "192:     [",
          "193:         (None, 0, 0),",
          "194:         (TaskInstance, 0, 0),",
          "195:         (TaskInstance, 1, 1),",
          "196:         (TaskInstance, 10, 1),",
          "197:         (TaskInstance, 10, 5),",
          "198:         (DagRun, 0, 0),",
          "199:         (DagRun, 1, 1),",
          "200:         (DagRun, 10, 1),",
          "201:         (DagRun, 10, 9),",
          "202:     ],",
          "203: )",
          "204: def test_action_has_dag_edit_access(create_task_instance, class_type, no_instances, no_unique_dags):",
          "205:     unique_dag_ids = [f\"test_dag_id_{nr}\" for nr in range(no_unique_dags)]",
          "206:     tis: list[TaskInstance] = [",
          "207:         create_task_instance(",
          "208:             task_id=f\"test_task_instance_{nr}\",",
          "209:             execution_date=timezone.datetime(2021, 1, 1 + nr),",
          "210:             dag_id=unique_dag_ids[nr % len(unique_dag_ids)],",
          "211:             run_id=f\"test_run_id_{nr}\",",
          "212:         )",
          "213:         for nr in range(no_instances)",
          "214:     ]",
          "215:     if class_type is None:",
          "216:         test_items = None",
          "217:     else:",
          "218:         test_items = tis if class_type == TaskInstance else [ti.get_dagrun() for ti in tis]",
          "219:         test_items = test_items[0] if len(test_items) == 1 else test_items",
          "220:     application = app.create_app(testing=True)",
          "221:     with application.app_context():",
          "222:         with mock.patch.object(application.appbuilder.sm, \"can_edit_dag\") as mocked_can_edit:",
          "223:             mocked_can_edit.return_value = True",
          "224:             assert not isinstance(test_items, list) or len(test_items) == no_instances",
          "225:             assert some_view_action_which_requires_dag_edit_access(None, test_items) is True",
          "226:             assert mocked_can_edit.call_count == no_unique_dags",
          "227:     clear_db_runs()",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_tasks.py||tests/www/views/test_views_tasks.py": [
          "File: tests/www/views/test_views_tasks.py -> tests/www/views/test_views_tasks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "687:         username=username,",
          "688:         role_name=\"User with permission to access only one dag\",",
          "689:         permissions=[",
          "690:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_INSTANCE),",
          "691:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_TASK_LOG),",
          "692:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_WEBSITE),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "690:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_RUN),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b915ed88715cbbf5a8cecb02c210cd529095026f",
      "candidate_info": {
        "commit_hash": "b915ed88715cbbf5a8cecb02c210cd529095026f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b915ed88715cbbf5a8cecb02c210cd529095026f",
        "files": [
          "airflow/api_connexion/endpoints/dag_source_endpoint.py"
        ],
        "message": "Use `batch_is_authorized_dag` to check if user has permission to read DAGs (#36279)\n\n(cherry picked from commit a7ab64e29b60502ee8b9d5088470113d836283d1)",
        "before_after_code_files": [
          "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py"
          ],
          "candidate": [
            "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py": [
          "File: airflow/api_connexion/endpoints/dag_source_endpoint.py -> airflow/api_connexion/endpoints/dag_source_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: from __future__ import annotations",
          "19: from http import HTTPStatus",
          "22: from flask import Response, current_app, request",
          "23: from itsdangerous import BadSignature, URLSafeSerializer",
          "",
          "[Removed Lines]",
          "20: from typing import TYPE_CHECKING",
          "",
          "[Added Lines]",
          "20: from typing import TYPE_CHECKING, Sequence",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "25: from airflow.api_connexion import security",
          "26: from airflow.api_connexion.exceptions import NotFound, PermissionDenied",
          "27: from airflow.api_connexion.schemas.dag_source_schema import dag_source_schema",
          "30: from airflow.models.dag import DagModel",
          "31: from airflow.models.dagcode import DagCode",
          "32: from airflow.utils.session import NEW_SESSION, provide_session",
          "34: if TYPE_CHECKING:",
          "35:     from sqlalchemy.orm import Session",
          "38: @security.requires_access_dag(\"GET\", DagAccessEntity.CODE)",
          "39: @provide_session",
          "",
          "[Removed Lines]",
          "28: from airflow.api_connexion.security import get_readable_dags",
          "29: from airflow.auth.managers.models.resource_details import DagAccessEntity",
          "",
          "[Added Lines]",
          "28: from airflow.auth.managers.models.resource_details import DagAccessEntity, DagDetails",
          "32: from airflow.www.extensions.init_auth_manager import get_auth_manager",
          "37:     from airflow.auth.managers.models.batch_apis import IsAuthorizedDagRequest",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "44:     try:",
          "45:         path = auth_s.loads(file_token)",
          "46:         dag_ids = session.query(DagModel.dag_id).filter(DagModel.fileloc == path).all()",
          "48:         # Check if user has read access to all the DAGs defined in the file",
          "50:             raise PermissionDenied()",
          "51:         dag_source = DagCode.code(path, session=session)",
          "52:     except (BadSignature, FileNotFoundError):",
          "",
          "[Removed Lines]",
          "47:         readable_dags = get_readable_dags()",
          "49:         if any(dag_id[0] not in readable_dags for dag_id in dag_ids):",
          "",
          "[Added Lines]",
          "49:         requests: Sequence[IsAuthorizedDagRequest] = [",
          "50:             {",
          "51:                 \"method\": \"GET\",",
          "52:                 \"details\": DagDetails(id=dag_id[0]),",
          "53:             }",
          "54:             for dag_id in dag_ids",
          "55:         ]",
          "58:         if not get_auth_manager().batch_is_authorized_dag(requests):",
          "",
          "---------------"
        ]
      }
    }
  ]
}