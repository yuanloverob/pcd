{
  "cve_id": "CVE-2023-35005",
  "cve_desc": "In Apache Airflow, some potentially sensitive values were being shown to the user in certain situations.\n\nThis vulnerability is mitigated by the fact configuration is not shown in the UI by default (only if `[webserver] expose_config` is set to `non-sensitive-only`), and not all uncensored values are actually sentitive.\n\n\nThis issue affects Apache Airflow: from 2.5.0 before 2.6.2. Users are recommended to update to version 2.6.2 or later.\n\n\n",
  "repo": "apache/airflow",
  "patch_hash": "f6cda8fb63250fc4700658999739c1c3c5f6625c",
  "patch_info": {
    "commit_hash": "f6cda8fb63250fc4700658999739c1c3c5f6625c",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/f6cda8fb63250fc4700658999739c1c3c5f6625c",
    "files": [
      "airflow/configuration.py"
    ],
    "message": "Mark `[secrets] backend_kwargs` as a sensitive config (#31788)\n\n(cherry picked from commit 8062756fa9e01eeeee1f2c6df74f376c0a526bd5)",
    "before_after_code_files": [
      "airflow/configuration.py||airflow/configuration.py"
    ]
  },
  "patch_diff": {
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "156:     (\"atlas\", \"password\"),",
      "157:     (\"smtp\", \"smtp_password\"),",
      "158:     (\"webserver\", \"secret_key\"),",
      "159:     # The following options are deprecated",
      "160:     (\"core\", \"sql_alchemy_conn\"),",
      "161: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "159:     (\"secrets\", \"backend_kwargs\"),",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "78bf9dd6b025554094e4265e8448ded552330754",
      "candidate_info": {
        "commit_hash": "78bf9dd6b025554094e4265e8448ded552330754",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/78bf9dd6b025554094e4265e8448ded552330754",
        "files": [
          "airflow/hooks/subprocess.py",
          "airflow/sensors/bash.py"
        ],
        "message": "Use only one line for tmp dir log (#31170)\n\n(cherry picked from commit b4ba6b7cb1c232c9956e0a969f015b2ce5964d15)",
        "before_after_code_files": [
          "airflow/hooks/subprocess.py||airflow/hooks/subprocess.py",
          "airflow/sensors/bash.py||airflow/sensors/bash.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/hooks/subprocess.py||airflow/hooks/subprocess.py": [
          "File: airflow/hooks/subprocess.py -> airflow/hooks/subprocess.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "60:         :return: :class:`namedtuple` containing ``exit_code`` and ``output``, the last line from stderr",
          "61:             or stdout",
          "62:         \"\"\"",
          "64:         with contextlib.ExitStack() as stack:",
          "65:             if cwd is None:",
          "66:                 cwd = stack.enter_context(TemporaryDirectory(prefix=\"airflowtmp\"))",
          "",
          "[Removed Lines]",
          "63:         self.log.info(\"Tmp dir root location: \\n %s\", gettempdir())",
          "",
          "[Added Lines]",
          "63:         self.log.info(\"Tmp dir root location: %s\", gettempdir())",
          "",
          "---------------"
        ],
        "airflow/sensors/bash.py||airflow/sensors/bash.py": [
          "File: airflow/sensors/bash.py -> airflow/sensors/bash.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "67:     def poke(self, context: Context):",
          "68:         \"\"\"Execute the bash command in a temporary directory.\"\"\"",
          "69:         bash_command = self.bash_command",
          "71:         with TemporaryDirectory(prefix=\"airflowtmp\") as tmp_dir:",
          "72:             with NamedTemporaryFile(dir=tmp_dir, prefix=self.task_id) as f:",
          "73:                 f.write(bytes(bash_command, \"utf_8\"))",
          "",
          "[Removed Lines]",
          "70:         self.log.info(\"Tmp dir root location: \\n %s\", gettempdir())",
          "",
          "[Added Lines]",
          "70:         self.log.info(\"Tmp dir root location: %s\", gettempdir())",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "9ae933f67bd5374b9ac051c4becaac71614fab2c",
      "candidate_info": {
        "commit_hash": "9ae933f67bd5374b9ac051c4becaac71614fab2c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/9ae933f67bd5374b9ac051c4becaac71614fab2c",
        "files": [
          "Dockerfile",
          "Dockerfile.ci",
          "scripts/docker/install_os_dependencies.sh",
          "scripts/in_container/verify_providers.py"
        ],
        "message": "Add libgeos-dev to dev dependencies in CI image (#31437)\n\nWhen running some Mlengine tests, pytest (when running assert rewrite)\nattempts to load libgeos-dev library, which breaks collection of some\nGoogle Provider tests on ARM image (it is used by Shapely library\nimported by recent BigQuery library)\n\nThis PR Adds the library as preinstalled in the CI image.\n\n(cherry picked from commit 9137740b2e8937aaf65c7c86dd11b096d028e7d2)",
        "before_after_code_files": [
          "Dockerfile.ci||Dockerfile.ci",
          "scripts/docker/install_os_dependencies.sh||scripts/docker/install_os_dependencies.sh",
          "scripts/in_container/verify_providers.py||scripts/in_container/verify_providers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "Dockerfile.ci||Dockerfile.ci": [
          "File: Dockerfile.ci -> Dockerfile.ci",
          "--- Hunk 1 ---",
          "[Context before]",
          "54: function get_dev_apt_deps() {",
          "55:     if [[ \"${DEV_APT_DEPS=}\" == \"\" ]]; then",
          "56:         DEV_APT_DEPS=\"apt-transport-https apt-utils build-essential ca-certificates dirmngr \\",
          "58: libkrb5-dev libldap2-dev libleveldb1d libleveldb-dev libsasl2-2 libsasl2-dev libsasl2-modules \\",
          "59: libssl-dev locales lsb-release openssh-client sasl2-bin \\",
          "60: software-properties-common sqlite3 sudo unixodbc unixodbc-dev\"",
          "",
          "[Removed Lines]",
          "57: freetds-bin freetds-dev git gosu graphviz graphviz-dev krb5-user ldap-utils libffi-dev \\",
          "",
          "[Added Lines]",
          "57: freetds-bin freetds-dev git gosu graphviz graphviz-dev krb5-user ldap-utils libffi-dev libgeos-dev \\",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1196: RUN echo \"Base image version: ${PYTHON_BASE_IMAGE}\"",
          "1199: ARG DEV_APT_COMMAND=\"\"",
          "1200: ARG ADDITIONAL_DEV_APT_COMMAND=\"\"",
          "1201: ARG ADDITIONAL_DEV_ENV_VARS=\"\"",
          "1202: ARG ADDITIONAL_DEV_APT_DEPS=\"bash-completion dumb-init git graphviz gosu krb5-user \\",
          "1204: openssh-server postgresql-client software-properties-common rsync tmux unzip vim xxd\"",
          "1206: ARG ADDITIONAL_DEV_APT_ENV=\"\"",
          "",
          "[Removed Lines]",
          "1198: ARG ADDITIONAL_DEV_APT_DEPS=\"git graphviz gosu libpq-dev netcat rsync\"",
          "1203: less libenchant-2-2 libgcc-10-dev libpq-dev net-tools netcat \\",
          "",
          "[Added Lines]",
          "1202: less libenchant-2-2 libgcc-10-dev libgeos-dev libpq-dev net-tools netcat \\",
          "",
          "---------------"
        ],
        "scripts/docker/install_os_dependencies.sh||scripts/docker/install_os_dependencies.sh": [
          "File: scripts/docker/install_os_dependencies.sh -> scripts/docker/install_os_dependencies.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "36: function get_dev_apt_deps() {",
          "37:     if [[ \"${DEV_APT_DEPS=}\" == \"\" ]]; then",
          "38:         DEV_APT_DEPS=\"apt-transport-https apt-utils build-essential ca-certificates dirmngr \\",
          "40: libkrb5-dev libldap2-dev libleveldb1d libleveldb-dev libsasl2-2 libsasl2-dev libsasl2-modules \\",
          "41: libssl-dev locales lsb-release openssh-client sasl2-bin \\",
          "42: software-properties-common sqlite3 sudo unixodbc unixodbc-dev\"",
          "",
          "[Removed Lines]",
          "39: freetds-bin freetds-dev git gosu graphviz graphviz-dev krb5-user ldap-utils libffi-dev \\",
          "",
          "[Added Lines]",
          "39: freetds-bin freetds-dev git gosu graphviz graphviz-dev krb5-user ldap-utils libffi-dev libgeos-dev \\",
          "",
          "---------------"
        ],
        "scripts/in_container/verify_providers.py||scripts/in_container/verify_providers.py": [
          "File: scripts/in_container/verify_providers.py -> scripts/in_container/verify_providers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import logging",
          "22: import os",
          "23: import pkgutil",
          "25: import re",
          "26: import subprocess",
          "27: import sys",
          "",
          "[Removed Lines]",
          "24: import platform",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "274:             try:",
          "275:                 with warnings.catch_warnings(record=True) as w:",
          "276:                     warnings.filterwarnings(\"always\", category=DeprecationWarning)",
          "294:                 if w:",
          "295:                     all_warnings.extend(w)",
          "296:             except AirflowOptionalProviderFeatureException:",
          "",
          "[Removed Lines]",
          "277:                     try:",
          "278:                         _module = importlib.import_module(modinfo.name)",
          "279:                         for attribute_name in dir(_module):",
          "280:                             class_name = modinfo.name + \".\" + attribute_name",
          "281:                             attribute = getattr(_module, attribute_name)",
          "282:                             if isclass(attribute):",
          "283:                                 imported_classes.append(class_name)",
          "284:                             if isclass(attribute) and (",
          "285:                                 issubclass(attribute, logging.Handler)",
          "286:                                 or issubclass(attribute, BaseSecretsBackend)",
          "287:                             ):",
          "288:                                 classes_with_potential_circular_import.append(class_name)",
          "289:                     except OSError as e:",
          "290:                         if \"geos_c\" in str(e) and platform.machine() in (\"aarch64\", \"arm64\"):",
          "291:                             # we ignore the missing geos_c library on Apple Silicon",
          "292:                             continue",
          "293:                         raise",
          "",
          "[Added Lines]",
          "276:                     _module = importlib.import_module(modinfo.name)",
          "277:                     for attribute_name in dir(_module):",
          "278:                         class_name = modinfo.name + \".\" + attribute_name",
          "279:                         attribute = getattr(_module, attribute_name)",
          "280:                         if isclass(attribute):",
          "281:                             imported_classes.append(class_name)",
          "282:                         if isclass(attribute) and (",
          "283:                             issubclass(attribute, logging.Handler)",
          "284:                             or issubclass(attribute, BaseSecretsBackend)",
          "285:                         ):",
          "286:                             classes_with_potential_circular_import.append(class_name)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b6e0c36160afefc9728919af1f9dfc77154b8d1d",
      "candidate_info": {
        "commit_hash": "b6e0c36160afefc9728919af1f9dfc77154b8d1d",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b6e0c36160afefc9728919af1f9dfc77154b8d1d",
        "files": [
          "airflow/api/common/airflow_health.py",
          "airflow/api_connexion/endpoints/health_endpoint.py",
          "airflow/www/views.py",
          "tests/api/__init__.py",
          "tests/api/common/__init__.py",
          "tests/api/common/test_airflow_health.py"
        ],
        "message": "Include triggerer health status in Airflow /health endpoint (#31529)\n\nPR https://github.com/apache/airflow/pull/27755 introduced sending triggerer\nhealth status in the `/api/v1/health` endpoint and also updated relevant\ndocs but we've the primary `/health` too which is missing this information.\nThe PR addresses this missing status report for triggerer health in the\n`/health` endpoint. It also attempts to deduplicate the code between those\nendpoints so that in future we need to make necessary changes in only one\nplace and at the same time ensure that change made in one endpoint is not\nmissed for the other endpoint serving the same purpose and thus ensuring\nconsistency in the responses.\n\nfixes: #31522\n(cherry picked from commit f048aba47e079e0c81417170a5ac582ed00595c4)",
        "before_after_code_files": [
          "airflow/api/common/airflow_health.py||airflow/api/common/airflow_health.py",
          "airflow/api_connexion/endpoints/health_endpoint.py||airflow/api_connexion/endpoints/health_endpoint.py",
          "airflow/www/views.py||airflow/www/views.py",
          "tests/api/__init__.py||tests/api/__init__.py",
          "tests/api/common/__init__.py||tests/api/common/__init__.py",
          "tests/api/common/test_airflow_health.py||tests/api/common/test_airflow_health.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api/common/airflow_health.py||airflow/api/common/airflow_health.py": [
          "File: airflow/api/common/airflow_health.py -> airflow/api/common/airflow_health.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "19: from typing import Any",
          "21: from airflow.jobs.scheduler_job_runner import SchedulerJobRunner",
          "22: from airflow.jobs.triggerer_job_runner import TriggererJobRunner",
          "24: HEALTHY = \"healthy\"",
          "25: UNHEALTHY = \"unhealthy\"",
          "28: def get_airflow_health() -> dict[str, Any]:",
          "29:     \"\"\"Get the health for Airflow metadatabase, scheduler and triggerer.\"\"\"",
          "30:     metadatabase_status = HEALTHY",
          "31:     latest_scheduler_heartbeat = None",
          "32:     latest_triggerer_heartbeat = None",
          "33:     scheduler_status = UNHEALTHY",
          "34:     triggerer_status: str | None = UNHEALTHY",
          "36:     try:",
          "37:         latest_scheduler_job = SchedulerJobRunner.most_recent_job()",
          "39:         if latest_scheduler_job:",
          "40:             latest_scheduler_heartbeat = latest_scheduler_job.latest_heartbeat.isoformat()",
          "41:             if latest_scheduler_job.is_alive():",
          "42:                 scheduler_status = HEALTHY",
          "43:     except Exception:",
          "44:         metadatabase_status = UNHEALTHY",
          "46:     try:",
          "47:         latest_triggerer_job = TriggererJobRunner.most_recent_job()",
          "49:         if latest_triggerer_job:",
          "50:             latest_triggerer_heartbeat = latest_triggerer_job.latest_heartbeat.isoformat()",
          "51:             if latest_triggerer_job.is_alive():",
          "52:                 triggerer_status = HEALTHY",
          "53:         else:",
          "54:             triggerer_status = None",
          "55:     except Exception:",
          "56:         metadatabase_status = UNHEALTHY",
          "58:     airflow_health_status = {",
          "59:         \"metadatabase\": {\"status\": metadatabase_status},",
          "60:         \"scheduler\": {",
          "61:             \"status\": scheduler_status,",
          "62:             \"latest_scheduler_heartbeat\": latest_scheduler_heartbeat,",
          "63:         },",
          "64:         \"triggerer\": {",
          "65:             \"status\": triggerer_status,",
          "66:             \"latest_triggerer_heartbeat\": latest_triggerer_heartbeat,",
          "67:         },",
          "68:     }",
          "70:     return airflow_health_status",
          "",
          "---------------"
        ],
        "airflow/api_connexion/endpoints/health_endpoint.py||airflow/api_connexion/endpoints/health_endpoint.py": [
          "File: airflow/api_connexion/endpoints/health_endpoint.py -> airflow/api_connexion/endpoints/health_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "19: from airflow.api_connexion.schemas.health_schema import health_schema",
          "20: from airflow.api_connexion.types import APIResponse",
          "28: def get_health() -> APIResponse:",
          "",
          "[Removed Lines]",
          "21: from airflow.jobs.scheduler_job_runner import SchedulerJobRunner",
          "22: from airflow.jobs.triggerer_job_runner import TriggererJobRunner",
          "24: HEALTHY = \"healthy\"",
          "25: UNHEALTHY = \"unhealthy\"",
          "29:     \"\"\"Return the health of the airflow scheduler and metadatabase.\"\"\"",
          "30:     metadatabase_status = HEALTHY",
          "31:     latest_scheduler_heartbeat = None",
          "32:     latest_triggerer_heartbeat = None",
          "33:     scheduler_status = UNHEALTHY",
          "34:     triggerer_status: str | None = UNHEALTHY",
          "35:     try:",
          "36:         scheduler_job = SchedulerJobRunner.most_recent_job()",
          "38:         if scheduler_job:",
          "39:             latest_scheduler_heartbeat = scheduler_job.latest_heartbeat.isoformat()",
          "40:             if scheduler_job.is_alive():",
          "41:                 scheduler_status = HEALTHY",
          "42:     except Exception:",
          "43:         metadatabase_status = UNHEALTHY",
          "44:     try:",
          "45:         triggerer_job = TriggererJobRunner.most_recent_job()",
          "47:         if triggerer_job:",
          "48:             latest_triggerer_heartbeat = triggerer_job.latest_heartbeat.isoformat()",
          "49:             if triggerer_job.is_alive():",
          "50:                 triggerer_status = HEALTHY",
          "51:         else:",
          "52:             triggerer_status = None",
          "53:     except Exception:",
          "54:         metadatabase_status = UNHEALTHY",
          "56:     payload = {",
          "57:         \"metadatabase\": {\"status\": metadatabase_status},",
          "58:         \"scheduler\": {",
          "59:             \"status\": scheduler_status,",
          "60:             \"latest_scheduler_heartbeat\": latest_scheduler_heartbeat,",
          "61:         },",
          "62:         \"triggerer\": {",
          "63:             \"status\": triggerer_status,",
          "64:             \"latest_triggerer_heartbeat\": latest_triggerer_heartbeat,",
          "65:         },",
          "66:     }",
          "68:     return health_schema.dump(payload)",
          "",
          "[Added Lines]",
          "19: from airflow.api.common.airflow_health import get_airflow_health",
          "25:     \"\"\"Return the health of the airflow scheduler, metadatabase and triggerer.\"\"\"",
          "26:     airflow_health_status = get_airflow_health()",
          "27:     return health_schema.dump(airflow_health_status)",
          "",
          "---------------"
        ],
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "78: import airflow",
          "79: from airflow import models, plugins_manager, settings",
          "80: from airflow.api.common.mark_tasks import (",
          "81:     set_dag_run_state_to_failed,",
          "82:     set_dag_run_state_to_queued,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "80: from airflow.api.common.airflow_health import get_airflow_health",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "673:     def health(self):",
          "674:         \"\"\"",
          "675:         An endpoint helping check the health status of the Airflow instance,",
          "677:         \"\"\"",
          "700:     @expose(\"/home\")",
          "701:     @auth.has_access(",
          "",
          "[Removed Lines]",
          "676:         including metadatabase and scheduler.",
          "678:         payload = {\"metadatabase\": {\"status\": \"unhealthy\"}}",
          "680:         latest_scheduler_heartbeat = None",
          "681:         scheduler_status = \"unhealthy\"",
          "682:         payload[\"metadatabase\"] = {\"status\": \"healthy\"}",
          "683:         try:",
          "684:             scheduler_job = SchedulerJobRunner.most_recent_job()",
          "686:             if scheduler_job:",
          "687:                 latest_scheduler_heartbeat = scheduler_job.latest_heartbeat.isoformat()",
          "688:                 if scheduler_job.is_alive():",
          "689:                     scheduler_status = \"healthy\"",
          "690:         except Exception:",
          "691:             payload[\"metadatabase\"][\"status\"] = \"unhealthy\"",
          "693:         payload[\"scheduler\"] = {",
          "694:             \"status\": scheduler_status,",
          "695:             \"latest_scheduler_heartbeat\": latest_scheduler_heartbeat,",
          "696:         }",
          "698:         return flask.json.jsonify(payload)",
          "",
          "[Added Lines]",
          "677:         including metadatabase, scheduler and triggerer.",
          "679:         airflow_health_status = get_airflow_health()",
          "681:         return flask.json.jsonify(airflow_health_status)",
          "",
          "---------------"
        ],
        "tests/api/__init__.py||tests/api/__init__.py": [
          "File: tests/api/__init__.py -> tests/api/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "",
          "---------------"
        ],
        "tests/api/common/__init__.py||tests/api/common/__init__.py": [
          "File: tests/api/common/__init__.py -> tests/api/common/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "",
          "---------------"
        ],
        "tests/api/common/test_airflow_health.py||tests/api/common/test_airflow_health.py": [
          "File: tests/api/common/test_airflow_health.py -> tests/api/common/test_airflow_health.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "19: from datetime import datetime",
          "20: from unittest.mock import MagicMock",
          "22: from airflow.api.common.airflow_health import (",
          "23:     HEALTHY,",
          "24:     UNHEALTHY,",
          "25:     SchedulerJobRunner,",
          "26:     TriggererJobRunner,",
          "27:     get_airflow_health,",
          "28: )",
          "31: def test_get_airflow_health_only_metadatabase_healthy():",
          "32:     SchedulerJobRunner.most_recent_job = MagicMock(return_value=None)",
          "33:     TriggererJobRunner.most_recent_job = MagicMock(return_value=None)",
          "35:     health_status = get_airflow_health()",
          "37:     expected_status = {",
          "38:         \"metadatabase\": {\"status\": HEALTHY},",
          "39:         \"scheduler\": {\"status\": UNHEALTHY, \"latest_scheduler_heartbeat\": None},",
          "40:         \"triggerer\": {\"status\": None, \"latest_triggerer_heartbeat\": None},",
          "41:     }",
          "43:     assert health_status == expected_status",
          "46: def test_get_airflow_health_metadatabase_unhealthy():",
          "47:     SchedulerJobRunner.most_recent_job = MagicMock(side_effect=Exception)",
          "48:     TriggererJobRunner.most_recent_job = MagicMock(side_effect=Exception)",
          "50:     health_status = get_airflow_health()",
          "52:     expected_status = {",
          "53:         \"metadatabase\": {\"status\": UNHEALTHY},",
          "54:         \"scheduler\": {\"status\": UNHEALTHY, \"latest_scheduler_heartbeat\": None},",
          "55:         \"triggerer\": {\"status\": UNHEALTHY, \"latest_triggerer_heartbeat\": None},",
          "56:     }",
          "58:     assert health_status == expected_status",
          "61: def test_get_airflow_health_scheduler_healthy_no_triggerer():",
          "62:     latest_scheduler_job_mock = MagicMock()",
          "63:     latest_scheduler_job_mock.latest_heartbeat = datetime.now()",
          "64:     latest_scheduler_job_mock.is_alive = MagicMock(return_value=True)",
          "65:     SchedulerJobRunner.most_recent_job = MagicMock(return_value=latest_scheduler_job_mock)",
          "66:     TriggererJobRunner.most_recent_job = MagicMock(return_value=None)",
          "68:     health_status = get_airflow_health()",
          "70:     expected_status = {",
          "71:         \"metadatabase\": {\"status\": HEALTHY},",
          "72:         \"scheduler\": {",
          "73:             \"status\": HEALTHY,",
          "74:             \"latest_scheduler_heartbeat\": latest_scheduler_job_mock.latest_heartbeat.isoformat(),",
          "75:         },",
          "76:         \"triggerer\": {\"status\": None, \"latest_triggerer_heartbeat\": None},",
          "77:     }",
          "79:     assert health_status == expected_status",
          "82: def test_get_airflow_health_triggerer_healthy_no_scheduler_job_record():",
          "83:     latest_triggerer_job_mock = MagicMock()",
          "84:     latest_triggerer_job_mock.latest_heartbeat = datetime.now()",
          "85:     latest_triggerer_job_mock.is_alive = MagicMock(return_value=True)",
          "86:     SchedulerJobRunner.most_recent_job = MagicMock(return_value=None)",
          "87:     TriggererJobRunner.most_recent_job = MagicMock(return_value=latest_triggerer_job_mock)",
          "89:     health_status = get_airflow_health()",
          "91:     expected_status = {",
          "92:         \"metadatabase\": {\"status\": HEALTHY},",
          "93:         \"scheduler\": {\"status\": UNHEALTHY, \"latest_scheduler_heartbeat\": None},",
          "94:         \"triggerer\": {",
          "95:             \"status\": HEALTHY,",
          "96:             \"latest_triggerer_heartbeat\": latest_triggerer_job_mock.latest_heartbeat.isoformat(),",
          "97:         },",
          "98:     }",
          "100:     assert health_status == expected_status",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "fd0719ddf50d1ab4b993e5eb58cba87295c53afa",
      "candidate_info": {
        "commit_hash": "fd0719ddf50d1ab4b993e5eb58cba87295c53afa",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/fd0719ddf50d1ab4b993e5eb58cba87295c53afa",
        "files": [
          "airflow/models/dagbag.py",
          "airflow/www/security.py",
          "tests/models/test_dagbag.py",
          "tests/www/views/test_views_home.py"
        ],
        "message": "Fix DAG.access_control can't sync when clean access_control (#30340)\n\n* Reset permission if `access_control` is empty\n\n* Check `resource` before call `_revoke_all_stale_permissions`\n\n* Fix static checks\n\n(cherry picked from commit 2c0c8b8bfb5287e10dc40b73f326bbf9a0437bb1)",
        "before_after_code_files": [
          "airflow/models/dagbag.py||airflow/models/dagbag.py",
          "airflow/www/security.py||airflow/www/security.py",
          "tests/models/test_dagbag.py||tests/models/test_dagbag.py",
          "tests/www/views/test_views_home.py||tests/www/views/test_views_home.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dagbag.py||airflow/models/dagbag.py": [
          "File: airflow/models/dagbag.py -> airflow/models/dagbag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "688:     @classmethod",
          "689:     @provide_session",
          "690:     def _sync_perm_for_dag(cls, dag: DAG, session: Session = NEW_SESSION):",
          "695:         root_dag_id = dag.parent_dag.dag_id if dag.parent_dag else dag.dag_id",
          "",
          "[Removed Lines]",
          "691:         \"\"\"Sync DAG specific permissions, if necessary\"\"\"",
          "692:         from airflow.security.permissions import DAG_ACTIONS, resource_name_for_dag",
          "693:         from airflow.www.fab_security.sqla.models import Action, Permission, Resource",
          "697:         def needs_perms(dag_id: str) -> bool:",
          "698:             dag_resource_name = resource_name_for_dag(dag_id)",
          "699:             for permission_name in DAG_ACTIONS:",
          "700:                 if not (",
          "701:                     session.query(Permission)",
          "702:                     .join(Action)",
          "703:                     .join(Resource)",
          "704:                     .filter(Action.name == permission_name)",
          "705:                     .filter(Resource.name == dag_resource_name)",
          "706:                     .one_or_none()",
          "707:                 ):",
          "708:                     return True",
          "709:             return False",
          "711:         if dag.access_control or needs_perms(root_dag_id):",
          "712:             cls.logger().debug(\"Syncing DAG permissions: %s to the DB\", root_dag_id)",
          "713:             from airflow.www.security import ApplessAirflowSecurityManager",
          "715:             security_manager = ApplessAirflowSecurityManager(session=session)",
          "716:             security_manager.sync_perm_for_dag(root_dag_id, dag.access_control)",
          "",
          "[Added Lines]",
          "691:         \"\"\"Sync DAG specific permissions\"\"\"",
          "694:         cls.logger().debug(\"Syncing DAG permissions: %s to the DB\", root_dag_id)",
          "695:         from airflow.www.security import ApplessAirflowSecurityManager",
          "697:         security_manager = ApplessAirflowSecurityManager(session=session)",
          "698:         security_manager.sync_perm_for_dag(root_dag_id, dag.access_control)",
          "",
          "---------------"
        ],
        "airflow/www/security.py||airflow/www/security.py": [
          "File: airflow/www/security.py -> airflow/www/security.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "655:         for dag_action_name in self.DAG_ACTIONS:",
          "656:             self.create_permission(dag_action_name, dag_resource_name)",
          "658:         if access_control:",
          "659:             self._sync_dag_view_permissions(dag_resource_name, access_control)",
          "661:     def _sync_dag_view_permissions(self, dag_id: str, access_control: dict[str, Collection[str]]) -> None:",
          "662:         \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "658:         def _revoke_all_stale_permissions(resource: Resource):",
          "659:             existing_dag_perms = self.get_resource_permissions(resource)",
          "660:             for perm in existing_dag_perms:",
          "661:                 non_admin_roles = [role for role in perm.role if role.name != \"Admin\"]",
          "662:                 for role in non_admin_roles:",
          "663:                     self.log.info(",
          "664:                         \"Revoking '%s' on DAG '%s' for role '%s'\",",
          "665:                         perm.action,",
          "666:                         dag_resource_name,",
          "667:                         role.name,",
          "668:                     )",
          "669:                     self.remove_permission_from_role(role, perm)",
          "673:         else:",
          "674:             resource = self.get_resource(dag_resource_name)",
          "675:             if resource:",
          "676:                 _revoke_all_stale_permissions(resource)",
          "",
          "---------------"
        ],
        "tests/models/test_dagbag.py||tests/models/test_dagbag.py": [
          "File: tests/models/test_dagbag.py -> tests/models/test_dagbag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "908:     def test_sync_perm_for_dag(self, mock_security_manager):",
          "909:         \"\"\"",
          "910:         Test that dagbag._sync_perm_for_dag will call ApplessAirflowSecurityManager.sync_perm_for_dag",
          "912:         \"\"\"",
          "913:         db_clean_up()",
          "914:         with create_session() as session:",
          "",
          "[Removed Lines]",
          "911:         when DAG specific perm views don't exist already or the DAG has access_control set.",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "933:             # perms now exist",
          "934:             _sync_perms()",
          "937:             # Always sync if we have access_control",
          "938:             dag.access_control = {\"Public\": {\"can_read\"}}",
          "",
          "[Removed Lines]",
          "935:             mock_sync_perm_for_dag.assert_not_called()",
          "",
          "[Added Lines]",
          "934:             mock_sync_perm_for_dag.assert_called_once_with(\"test_example_bash_operator\", None)",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_home.py||tests/www/views/test_views_home.py": [
          "File: tests/www/views/test_views_home.py -> tests/www/views/test_views_home.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "155:             _process_file(filename, session)",
          "158: @pytest.fixture()",
          "159: def broken_dags(tmpdir, working_dags):",
          "160:     with create_session() as session:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "158: @pytest.fixture()",
          "159: def working_dags_with_read_perm(tmpdir):",
          "160:     dag_contents_template = \"from airflow import DAG\\ndag = DAG('{}', tags=['{}'])\"",
          "161:     dag_contents_template_with_read_perm = (",
          "162:         \"from airflow import DAG\\ndag = DAG('{}', tags=['{}'], \"",
          "163:         \"access_control={{'role_single_dag':{{'can_read'}}}}) \"",
          "164:     )",
          "165:     with create_session() as session:",
          "166:         for dag_id, tag in list(zip(TEST_FILTER_DAG_IDS, TEST_TAGS)):",
          "167:             filename = os.path.join(tmpdir, f\"{dag_id}.py\")",
          "168:             if dag_id == \"filter_test_1\":",
          "169:                 with open(filename, \"w\") as f:",
          "170:                     f.writelines(dag_contents_template_with_read_perm.format(dag_id, tag))",
          "171:             else:",
          "172:                 with open(filename, \"w\") as f:",
          "173:                     f.writelines(dag_contents_template.format(dag_id, tag))",
          "174:             _process_file(filename, session)",
          "177: @pytest.fixture()",
          "178: def working_dags_with_edit_perm(tmpdir):",
          "179:     dag_contents_template = \"from airflow import DAG\\ndag = DAG('{}', tags=['{}'])\"",
          "180:     dag_contents_template_with_read_perm = (",
          "181:         \"from airflow import DAG\\ndag = DAG('{}', tags=['{}'], \"",
          "182:         \"access_control={{'role_single_dag':{{'can_edit'}}}}) \"",
          "183:     )",
          "184:     with create_session() as session:",
          "185:         for dag_id, tag in list(zip(TEST_FILTER_DAG_IDS, TEST_TAGS)):",
          "186:             filename = os.path.join(tmpdir, f\"{dag_id}.py\")",
          "187:             if dag_id == \"filter_test_1\":",
          "188:                 with open(filename, \"w\") as f:",
          "189:                     f.writelines(dag_contents_template_with_read_perm.format(dag_id, tag))",
          "190:             else:",
          "191:                 with open(filename, \"w\") as f:",
          "192:                     f.writelines(dag_contents_template.format(dag_id, tag))",
          "193:             _process_file(filename, session)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "165:             _process_file(filename, session)",
          "168: def test_home_filter_tags(working_dags, admin_client):",
          "169:     with admin_client:",
          "170:         admin_client.get(\"home?tags=example&tags=data\", follow_redirects=True)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "206: @pytest.fixture()",
          "207: def broken_dags_with_read_perm(tmpdir, working_dags_with_read_perm):",
          "208:     with create_session() as session:",
          "209:         for dag_id in TEST_FILTER_DAG_IDS:",
          "210:             filename = os.path.join(tmpdir, f\"{dag_id}.py\")",
          "211:             with open(filename, \"w\") as f:",
          "212:                 f.writelines(\"airflow DAG\")",
          "213:             _process_file(filename, session)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "185: @pytest.mark.parametrize(\"page\", [\"home\", \"home?status=active\", \"home?status=paused\", \"home?status=all\"])",
          "187:     # Users that can only see certain DAGs get a filtered list of import errors",
          "188:     resp = client_single_dag.get(page, follow_redirects=True)",
          "189:     check_content_in_response(\"Import Errors\", resp)",
          "",
          "[Removed Lines]",
          "186: def test_home_importerrors_filtered_singledag_user(broken_dags, client_single_dag, page):",
          "",
          "[Added Lines]",
          "234: def test_home_importerrors_filtered_singledag_user(broken_dags_with_read_perm, client_single_dag, page):",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "201:         check_content_in_response(f\"dag_id={dag_id}\", resp)",
          "205:     # Users that can only see certain DAGs get a filtered list",
          "206:     resp = client_single_dag.get(\"home\", follow_redirects=True)",
          "207:     # They can see the first DAG",
          "",
          "[Removed Lines]",
          "204: def test_home_dag_list_filtered_singledag_user(working_dags, client_single_dag):",
          "",
          "[Added Lines]",
          "252: def test_home_dag_list_filtered_singledag_user(working_dags_with_read_perm, client_single_dag):",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "219:     check_content_not_in_response(\"dag_id=a_first_dag_id_asc\", resp)",
          "223:     with capture_templates() as templates:",
          "224:         client_single_dag_edit.get(\"home\", follow_redirects=True)",
          "",
          "[Removed Lines]",
          "222: def test_home_dag_edit_permissions(capture_templates, working_dags, client_single_dag_edit):",
          "",
          "[Added Lines]",
          "270: def test_home_dag_edit_permissions(capture_templates, working_dags_with_edit_perm, client_single_dag_edit):",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "243a7ddce241fe45c3189ca90c592e97c313206a",
      "candidate_info": {
        "commit_hash": "243a7ddce241fe45c3189ca90c592e97c313206a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/243a7ddce241fe45c3189ca90c592e97c313206a",
        "files": [
          "airflow/www/static/js/dag/Main.tsx",
          "airflow/www/static/js/dag/grid/index.tsx"
        ],
        "message": "Resolving issue where Grid won't un-collapse when Details is collapsed (#31561)\n\n* spinner added\n\n* fixed static checks\n\n* isFetching\n\n* isFetching replaced with isLoading\n\n* Update CONTRIBUTING.rst\n\n* - fix where collapsed Grid won't show up when Details is uncollapsed\n- removing clearSelection when Details is collapsed\n- making experience uniform when collapsing or uncollapsing Grid or Detaials view\n\n* resize oberserver re-registering when Grid is un-collapsed\n\n* reverting 28px to show atleast one (latest) dag run instance\n\n(cherry picked from commit 6f86b6cd070097dafca196841c82de91faa882f4)",
        "before_after_code_files": [
          "airflow/www/static/js/dag/Main.tsx||airflow/www/static/js/dag/Main.tsx",
          "airflow/www/static/js/dag/grid/index.tsx||airflow/www/static/js/dag/grid/index.tsx"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/static/js/dag/Main.tsx||airflow/www/static/js/dag/Main.tsx": [
          "File: airflow/www/static/js/dag/Main.tsx -> airflow/www/static/js/dag/Main.tsx",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: import FilterBar from \"./nav/FilterBar\";",
          "32: import LegendRow from \"./nav/LegendRow\";",
          "33: import useToggleGroups from \"./useToggleGroups\";",
          "36: const detailsPanelKey = \"hideDetailsPanel\";",
          "37: const minPanelWidth = 300;",
          "",
          "[Removed Lines]",
          "34: import useSelection from \"./useSelection\";",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "68:   const gridRef = useRef<HTMLDivElement>(null);",
          "69:   const isPanelOpen = localStorage.getItem(detailsPanelKey) !== \"true\";",
          "70:   const { isOpen, onToggle } = useDisclosure({ defaultIsOpen: isPanelOpen });",
          "72:   const [hoveredTaskState, setHoveredTaskState] = useState<",
          "73:     string | null | undefined",
          "74:   >();",
          "",
          "[Removed Lines]",
          "71:   const { clearSelection } = useSelection();",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "91:     if (!isOpen) {",
          "92:       localStorage.setItem(detailsPanelKey, \"false\");",
          "93:     } else {",
          "95:       localStorage.setItem(detailsPanelKey, \"true\");",
          "96:     }",
          "97:     onToggle();",
          "98:   };",
          "",
          "[Removed Lines]",
          "94:       clearSelection();",
          "",
          "[Added Lines]",
          "93:       if (isGridCollapsed) {",
          "94:         setIsGridCollapsed(!isGridCollapsed);",
          "95:       }",
          "",
          "---------------"
        ],
        "airflow/www/static/js/dag/grid/index.tsx||airflow/www/static/js/dag/grid/index.tsx": [
          "File: airflow/www/static/js/dag/grid/index.tsx -> airflow/www/static/js/dag/grid/index.tsx",
          "--- Hunk 1 ---",
          "[Context before]",
          "95:   return (",
          "96:     <Box height=\"100%\" position=\"relative\">",
          "98:         <IconButton",
          "99:           fontSize=\"2xl\"",
          "100:           variant=\"ghost\"",
          "101:           color=\"gray.400\"",
          "102:           size=\"sm\"",
          "103:           position=\"absolute\"",
          "105:           zIndex={2}",
          "106:           top={-8}",
          "107:           onClick={() =>",
          "",
          "[Removed Lines]",
          "97:       {isPanelOpen && (",
          "104:           right={0}",
          "",
          "[Added Lines]",
          "97:       {(isPanelOpen || isGridCollapsed) && (",
          "104:           right={isGridCollapsed ? -10 : 0}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "114:           transitionProperty=\"none\"",
          "115:         />",
          "116:       )}",
          "133:       <Box",
          "134:         maxHeight={`calc(100% - ${offsetTop}px)`}",
          "135:         ref={scrollRef}",
          "",
          "[Removed Lines]",
          "117:       <IconButton",
          "118:         fontSize=\"2xl\"",
          "119:         variant=\"ghost\"",
          "120:         color=\"gray.400\"",
          "121:         size=\"sm\"",
          "122:         position=\"absolute\"",
          "123:         right={isPanelOpen ? -10 : 0}",
          "124:         zIndex={2}",
          "125:         top={-8}",
          "126:         onClick={onPanelToggle}",
          "127:         title={`${isPanelOpen ? \"Hide \" : \"Show \"} Details Panel`}",
          "128:         aria-label={isPanelOpen ? \"Show Details\" : \"Hide Details\"}",
          "129:         icon={<MdDoubleArrow />}",
          "130:         transform={isPanelOpen ? undefined : \"rotateZ(180deg)\"}",
          "131:         transitionProperty=\"none\"",
          "132:       />",
          "",
          "[Added Lines]",
          "117:       {!isGridCollapsed && (",
          "118:         <IconButton",
          "119:           fontSize=\"2xl\"",
          "120:           variant=\"ghost\"",
          "121:           color=\"gray.400\"",
          "122:           size=\"sm\"",
          "123:           position=\"absolute\"",
          "124:           right={isPanelOpen ? -10 : 0}",
          "125:           zIndex={2}",
          "126:           top={-8}",
          "127:           onClick={onPanelToggle}",
          "128:           title={`${isPanelOpen ? \"Hide \" : \"Show \"} Details Panel`}",
          "129:           aria-label={isPanelOpen ? \"Show Details\" : \"Hide Details\"}",
          "130:           icon={<MdDoubleArrow />}",
          "131:           transform={isPanelOpen ? undefined : \"rotateZ(180deg)\"}",
          "132:           transitionProperty=\"none\"",
          "133:         />",
          "134:       )}",
          "",
          "---------------"
        ]
      }
    }
  ]
}