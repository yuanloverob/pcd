{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "2268665663684dd381adf266feb74ac97a53900d",
      "candidate_info": {
        "commit_hash": "2268665663684dd381adf266feb74ac97a53900d",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/2268665663684dd381adf266feb74ac97a53900d",
        "files": [
          "core/src/main/scala/org/apache/spark/SparkConf.scala",
          "docs/running-on-kubernetes.md"
        ],
        "message": "[SPARK-39360][K8S] Remove deprecation of `spark.kubernetes.memoryOverheadFactor` and recover doc\n\n### What changes were proposed in this pull request?\n\nThis PR aims to avoid the deprecation of `spark.kubernetes.memoryOverheadFactor` from Apache Spark 3.3. In addition, also recovers the documentation which is removed mistakenly at the `deprecation`. `Deprecation` is not a removal.\n\n### Why are the changes needed?\n\n- Apache Spark 3.3.0 RC complains always about `spark.kubernetes.memoryOverheadFactor` because the configuration has the default value (which is not given by the users). There is no way to remove the warnings which means the directional message is not helpful and makes the users confused in a wrong way. In other words, we still get warnings even we use only new configurations or no configuration.\n```\n22/06/01 23:53:49 WARN SparkConf: The configuration key 'spark.kubernetes.memoryOverheadFactor' has been deprecated as of Spark 3.3.0 and may be removed in the future. Please use spark.driver.memoryOverheadFactor and spark.executor.memoryOverheadFactor\n22/06/01 23:53:49 WARN SparkConf: The configuration key 'spark.kubernetes.memoryOverheadFactor' has been deprecated as of Spark 3.3.0 and may be removed in the future. Please use spark.driver.memoryOverheadFactor and spark.executor.memoryOverheadFactor\n22/06/01 23:53:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n22/06/01 23:53:50 WARN SparkConf: The configuration key 'spark.kubernetes.memoryOverheadFactor' has been deprecated as of Spark 3.3.0 and may be removed in the future. Please use spark.driver.memoryOverheadFactor and spark.executor.memoryOverheadFactor\n```\n\n- The minimum constraint is slightly different because `spark.kubernetes.memoryOverheadFactor` allowed 0 since Apache Spark 2.4 while new configurations disallow `0`.\n\n- This documentation removal might be too early because the deprecation is not the removal of configuration. This PR recoveres the removed doc and added the following.\n```\nThis will be overridden by the value set by\n<code>spark.driver.memoryOverheadFactor</code> and\n<code>spark.executor.memoryOverheadFactor</code> explicitly.\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nNo. This is a consistent with the existing behavior.\n\n### How was this patch tested?\n\nPass the CIs.\n\nCloses #36744 from dongjoon-hyun/SPARK-39360.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 6d43556089a21b26d1a7590fbe1e25bd1ca7cedd)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/SparkConf.scala||core/src/main/scala/org/apache/spark/SparkConf.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/SparkConf.scala||core/src/main/scala/org/apache/spark/SparkConf.scala": [
          "File: core/src/main/scala/org/apache/spark/SparkConf.scala -> core/src/main/scala/org/apache/spark/SparkConf.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "636:       DeprecatedConfig(\"spark.blacklist.killBlacklistedExecutors\", \"3.1.0\",",
          "637:         \"Please use spark.excludeOnFailure.killExcludedExecutors\"),",
          "638:       DeprecatedConfig(\"spark.yarn.blacklist.executor.launch.blacklisting.enabled\", \"3.1.0\",",
          "642:     )",
          "644:     Map(configs.map { cfg => (cfg.key -> cfg) } : _*)",
          "",
          "[Removed Lines]",
          "639:         \"Please use spark.yarn.executor.launch.excludeOnFailure.enabled\"),",
          "640:       DeprecatedConfig(\"spark.kubernetes.memoryOverheadFactor\", \"3.3.0\",",
          "641:         \"Please use spark.driver.memoryOverheadFactor and spark.executor.memoryOverheadFactor\")",
          "",
          "[Added Lines]",
          "639:         \"Please use spark.yarn.executor.launch.excludeOnFailure.enabled\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8077944a021a9e52e7d55681799c62dbc974d458",
      "candidate_info": {
        "commit_hash": "8077944a021a9e52e7d55681799c62dbc974d458",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/8077944a021a9e52e7d55681799c62dbc974d458",
        "files": [
          "core/src/test/scala/org/apache/spark/ui/UISuite.scala"
        ],
        "message": "[SPARK-39458][CORE][TESTS] Fix `UISuite` for IPv6\n\n### What changes were proposed in this pull request?\n\nThis PR aims to fix `UISuite` to work in IPv6 environment.\n\n### Why are the changes needed?\n\nIPv6 address contains `:`.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nManual tests in Pure IPv6 environment.\n\nCloses #36858 from dongjoon-hyun/SPARK-39458.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 2182be81a32cdda691a3051a1591c232e8bd9f65)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "core/src/test/scala/org/apache/spark/ui/UISuite.scala||core/src/test/scala/org/apache/spark/ui/UISuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/test/scala/org/apache/spark/ui/UISuite.scala||core/src/test/scala/org/apache/spark/ui/UISuite.scala": [
          "File: core/src/test/scala/org/apache/spark/ui/UISuite.scala -> core/src/test/scala/org/apache/spark/ui/UISuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "195:       val ui = sc.ui.get",
          "196:       val splitUIAddress = ui.webUrl.split(':')",
          "197:       val boundPort = ui.boundPort",
          "199:     }",
          "200:   }",
          "",
          "[Removed Lines]",
          "198:       assert(splitUIAddress(2).toInt == boundPort)",
          "",
          "[Added Lines]",
          "198:       assert(splitUIAddress(splitUIAddress.length - 1).toInt == boundPort)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b2046c282a5be8ade421db61b583a6738f0e9ed6",
      "candidate_info": {
        "commit_hash": "b2046c282a5be8ade421db61b583a6738f0e9ed6",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/b2046c282a5be8ade421db61b583a6738f0e9ed6",
        "files": [
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ComputeCurrentTimeSuite.scala"
        ],
        "message": "[SPARK-39259][SQL][TEST][FOLLOWUP] Fix Scala 2.13 `ClassCastException` in `ComputeCurrentTimeSuite`\n\n### What changes were proposed in this pull request?\n\nUnfortunately, #36654 causes seven Scala 2.13 test failures in master/3.3 and Apache Spark 3.3 RC4.\nThis PR aims to fix Scala 2.13 ClassCastException in the test code.\n\n### Why are the changes needed?\n\n```\n$ dev/change-scala-version.sh 2.13\n$ build/sbt \"catalyst/testOnly *.ComputeCurrentTimeSuite\" -Pscala-2.13\n...\n[info] ComputeCurrentTimeSuite:\n[info] - analyzer should replace current_timestamp with literals *** FAILED *** (1 second, 189 milliseconds)\n[info]   java.lang.ClassCastException: scala.collection.mutable.ArrayBuffer cannot be cast to scala.collection.immutable.Seq\n[info]   at org.apache.spark.sql.catalyst.optimizer.ComputeCurrentTimeSuite.literals(ComputeCurrentTimeSuite.scala:146)\n[info]   at org.apache.spark.sql.catalyst.optimizer.ComputeCurrentTimeSuite.$anonfun$new$1(ComputeCurrentTimeSuite.scala:47)\n...\n[info] *** 7 TESTS FAILED ***\n[error] Failed tests:\n[error] \torg.apache.spark.sql.catalyst.optimizer.ComputeCurrentTimeSuite\n[error] (catalyst / Test / testOnly) sbt.TestsFailedException: Tests unsuccessful\n[error] Total time: 189 s (03:09), completed Jun 3, 2022 10:29:39 AM\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nPass the CIs and manually tests with Scala 2.13.\n\n```\n$ dev/change-scala-version.sh 2.13\n$ build/sbt \"catalyst/testOnly *.ComputeCurrentTimeSuite\" -Pscala-2.13\n...\n[info] ComputeCurrentTimeSuite:\n[info] - analyzer should replace current_timestamp with literals (545 milliseconds)\n[info] - analyzer should replace current_date with literals (11 milliseconds)\n[info] - SPARK-33469: Add current_timezone function (3 milliseconds)\n[info] - analyzer should replace localtimestamp with literals (4 milliseconds)\n[info] - analyzer should use equal timestamps across subqueries (182 milliseconds)\n[info] - analyzer should use consistent timestamps for different timezones (13 milliseconds)\n[info] - analyzer should use consistent timestamps for different timestamp functions (2 milliseconds)\n[info] Run completed in 1 second, 579 milliseconds.\n[info] Total number of tests run: 7\n[info] Suites: completed 1, aborted 0\n[info] Tests: succeeded 7, failed 0, canceled 0, ignored 0, pending 0\n[info] All tests passed.\n[success] Total time: 12 s, completed Jun 3, 2022, 10:54:03 AM\n```\n\nCloses #36762 from dongjoon-hyun/SPARK-39259.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit d79aa36b12d9d6816679ba6348705fdd3bd0061e)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ComputeCurrentTimeSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ComputeCurrentTimeSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ComputeCurrentTimeSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ComputeCurrentTimeSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ComputeCurrentTimeSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ComputeCurrentTimeSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "135:     assert(offsetsFromQuarterHour.size == 1)",
          "136:   }",
          "139:     val literals = new scala.collection.mutable.ArrayBuffer[T]",
          "140:     plan.transformWithSubqueries { case subQuery =>",
          "141:       subQuery.transformAllExpressions { case expression: Literal =>",
          "",
          "[Removed Lines]",
          "138:   private def literals[T](plan: LogicalPlan): Seq[T] = {",
          "",
          "[Added Lines]",
          "138:   private def literals[T](plan: LogicalPlan): scala.collection.mutable.ArrayBuffer[T] = {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "143:         expression",
          "144:       }",
          "145:     }",
          "147:   }",
          "148: }",
          "",
          "[Removed Lines]",
          "146:     literals.asInstanceOf[Seq[T]]",
          "",
          "[Added Lines]",
          "146:     literals",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "47c7ba2fcb573ac7f39fbe0518b3abbcde905522",
      "candidate_info": {
        "commit_hash": "47c7ba2fcb573ac7f39fbe0518b3abbcde905522",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/47c7ba2fcb573ac7f39fbe0518b3abbcde905522",
        "files": [
          "sql/core/benchmarks/DataSourceReadBenchmark-jdk11-results.txt",
          "sql/core/benchmarks/DataSourceReadBenchmark-jdk17-results.txt",
          "sql/core/benchmarks/DataSourceReadBenchmark-results.txt",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/DataSourceReadBenchmark.scala"
        ],
        "message": "[SPARK-34863][SQL][FOLLOWUP] Add benchmark for Parquet & ORC nested column scan\n\n### What changes were proposed in this pull request?\n\nThis adds benchmark for Parquet & ORC nested column scan, e.g., struct, list and map.\n\n### Why are the changes needed?\n\nBoth Parquet and ORC now support vectorized reader for nested column now, but there is no benchmark to measure the performance yet.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nN/A - benchmark only.\n\nCloses #36123 from sunchao/SPARK-34863-bench.\n\nAuthored-by: Chao Sun <sunchao@apple.com>\nSigned-off-by: Chao Sun <sunchao@apple.com>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/DataSourceReadBenchmark.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/DataSourceReadBenchmark.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/DataSourceReadBenchmark.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/DataSourceReadBenchmark.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/DataSourceReadBenchmark.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/benchmark/DataSourceReadBenchmark.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "69:     try f finally tableNames.foreach(spark.catalog.dropTempView)",
          "70:   }",
          "73:     val testDf = if (partition.isDefined) {",
          "74:       df.write.partitionBy(partition.get)",
          "75:     } else {",
          "76:       df.write",
          "77:     }",
          "81:     saveAsParquetV1Table(testDf, dir.getCanonicalPath + \"/parquetV1\")",
          "82:     saveAsParquetV2Table(testDf, dir.getCanonicalPath + \"/parquetV2\")",
          "83:     saveAsOrcTable(testDf, dir.getCanonicalPath + \"/orc\")",
          "",
          "[Removed Lines]",
          "72:   private def prepareTable(dir: File, df: DataFrame, partition: Option[String] = None): Unit = {",
          "79:     saveAsCsvTable(testDf, dir.getCanonicalPath + \"/csv\")",
          "80:     saveAsJsonTable(testDf, dir.getCanonicalPath + \"/json\")",
          "",
          "[Added Lines]",
          "72:   private def prepareTable(",
          "73:       dir: File,",
          "74:       df: DataFrame,",
          "75:       partition: Option[String] = None,",
          "76:       onlyParquetOrc: Boolean = false): Unit = {",
          "83:     if (!onlyParquetOrc) {",
          "84:       saveAsCsvTable(testDf, dir.getCanonicalPath + \"/csv\")",
          "85:       saveAsJsonTable(testDf, dir.getCanonicalPath + \"/json\")",
          "86:     }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "262:     }",
          "263:   }",
          "265:   def intStringScanBenchmark(values: Int): Unit = {",
          "266:     val benchmark = new Benchmark(\"Int and String Scan\", values, output = output)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "274:   def nestedNumericScanBenchmark(values: Int, dataType: DataType): Unit = {",
          "275:     val sqlBenchmark = new Benchmark(",
          "276:       s\"SQL Single ${dataType.sql} Column Scan in Struct\",",
          "277:       values,",
          "278:       output = output)",
          "280:     withTempPath { dir =>",
          "281:       withTempTable(\"t1\", \"parquetV1Table\", \"parquetV2Table\", \"orcTable\") {",
          "282:         import spark.implicits._",
          "283:         spark.range(values).map(_ => Random.nextLong).createOrReplaceTempView(\"t1\")",
          "285:         prepareTable(dir,",
          "286:           spark.sql(s\"SELECT named_struct('f', CAST(value as ${dataType.sql})) as col FROM t1\"),",
          "287:           onlyParquetOrc = true)",
          "289:         sqlBenchmark.addCase(s\"SQL ORC MR\") { _ =>",
          "290:           withSQLConf(SQLConf.ORC_VECTORIZED_READER_ENABLED.key -> \"false\") {",
          "291:             spark.sql(s\"select sum(col.f) from orcTable\").noop()",
          "292:           }",
          "293:         }",
          "295:         sqlBenchmark.addCase(s\"SQL ORC Vectorized (Nested Column Disabled)\") { _ =>",
          "296:           withSQLConf(SQLConf.ORC_VECTORIZED_READER_NESTED_COLUMN_ENABLED.key -> \"false\") {",
          "297:             spark.sql(s\"select sum(col.f) from orcTable\").noop()",
          "298:           }",
          "299:         }",
          "301:         sqlBenchmark.addCase(s\"SQL ORC Vectorized (Nested Column Enabled)\") { _ =>",
          "302:           withSQLConf(SQLConf.ORC_VECTORIZED_READER_NESTED_COLUMN_ENABLED.key -> \"true\") {",
          "303:             spark.sql(s\"select sum(col.f) from orcTable\").noop()",
          "304:           }",
          "305:         }",
          "307:         withParquetVersions { version =>",
          "308:           sqlBenchmark.addCase(s\"SQL Parquet MR: DataPage$version\") { _ =>",
          "309:             withSQLConf(SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key -> \"false\") {",
          "310:               spark.sql(s\"select sum(col.f) from parquet${version}Table\").noop()",
          "311:             }",
          "312:           }",
          "314:           sqlBenchmark.addCase(s\"SQL Parquet Vectorized: DataPage$version \" +",
          "315:               \"(Nested Column Disabled)\") { _ =>",
          "316:             withSQLConf(SQLConf.PARQUET_VECTORIZED_READER_NESTED_COLUMN_ENABLED.key -> \"false\") {",
          "317:               spark.sql(s\"select sum(col.f) from parquet${version}Table\").noop()",
          "318:             }",
          "319:           }",
          "321:           sqlBenchmark.addCase(s\"SQL Parquet Vectorized: DataPage$version \" +",
          "322:               \"(Nested Column Enabled)\") { _ =>",
          "323:             withSQLConf(SQLConf.PARQUET_VECTORIZED_READER_NESTED_COLUMN_ENABLED.key -> \"true\") {",
          "324:               spark.sql(s\"select sum(col.f) from parquet${version}Table\").noop()",
          "325:             }",
          "326:           }",
          "327:         }",
          "329:         sqlBenchmark.run()",
          "330:       }",
          "331:     }",
          "332:   }",
          "334:   def nestedColumnScanBenchmark(values: Int): Unit = {",
          "335:     val benchmark = new Benchmark(s\"SQL Nested Column Scan\", values, minNumIters = 10,",
          "336:       output = output)",
          "338:     withTempPath { dir =>",
          "339:       withTempTable(\"t1\", \"parquetV1Table\", \"parquetV2Table\", \"orcTable\") {",
          "340:         import spark.implicits._",
          "341:         spark.range(values).map(_ => Random.nextLong).map { x =>",
          "342:           val arrayOfStructColumn = (0 until 5).map(i => (x + i, s\"$x\" * 5))",
          "343:           val mapOfStructColumn = Map(",
          "344:             s\"$x\" -> (x * 0.1, (x, s\"$x\" * 100)),",
          "345:             (s\"$x\" * 2) -> (x * 0.2, (x, s\"$x\" * 200)),",
          "346:             (s\"$x\" * 3) -> (x * 0.3, (x, s\"$x\" * 300)))",
          "347:           (arrayOfStructColumn, mapOfStructColumn)",
          "348:         }.toDF(\"col1\", \"col2\").createOrReplaceTempView(\"t1\")",
          "350:         prepareTable(dir, spark.sql(s\"SELECT * FROM t1\"), onlyParquetOrc = true)",
          "352:         benchmark.addCase(\"SQL ORC MR\") { _ =>",
          "353:           withSQLConf(SQLConf.ORC_VECTORIZED_READER_ENABLED.key -> \"false\") {",
          "354:             spark.sql(\"SELECT SUM(SIZE(col1)), SUM(SIZE(col2)) FROM orcTable\").noop()",
          "355:           }",
          "356:         }",
          "358:         benchmark.addCase(\"SQL ORC Vectorized (Nested Column Disabled)\") { _ =>",
          "359:           withSQLConf(SQLConf.ORC_VECTORIZED_READER_NESTED_COLUMN_ENABLED.key -> \"false\") {",
          "360:             spark.sql(\"SELECT SUM(SIZE(col1)), SUM(SIZE(col2)) FROM orcTable\").noop()",
          "361:           }",
          "362:         }",
          "364:         benchmark.addCase(\"SQL ORC Vectorized (Nested Column Enabled)\") { _ =>",
          "365:           withSQLConf(SQLConf.ORC_VECTORIZED_READER_NESTED_COLUMN_ENABLED.key -> \"true\") {",
          "366:             spark.sql(\"SELECT SUM(SIZE(col1)), SUM(SIZE(col2)) FROM orcTable\").noop()",
          "367:           }",
          "368:         }",
          "371:         withParquetVersions { version =>",
          "372:           benchmark.addCase(s\"SQL Parquet MR: DataPage$version\") { _ =>",
          "373:             withSQLConf(SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key -> \"false\") {",
          "374:               spark.sql(s\"SELECT SUM(SIZE(col1)), SUM(SIZE(col2)) FROM parquet${version}Table\")",
          "375:                 .noop()",
          "376:             }",
          "377:           }",
          "379:           benchmark.addCase(s\"SQL Parquet Vectorized: DataPage$version \" +",
          "380:               s\"(Nested Column Disabled)\") { _ =>",
          "381:             withSQLConf(SQLConf.PARQUET_VECTORIZED_READER_NESTED_COLUMN_ENABLED.key -> \"false\") {",
          "382:               spark.sql(s\"SELECT SUM(SIZE(col1)), SUM(SIZE(col2)) FROM parquet${version}Table\")",
          "383:                 .noop()",
          "384:             }",
          "385:           }",
          "387:           benchmark.addCase(s\"SQL Parquet Vectorized: DataPage$version \" +",
          "388:               s\"(Nested Column Enabled)\") { _ =>",
          "389:             withSQLConf(SQLConf.PARQUET_VECTORIZED_READER_NESTED_COLUMN_ENABLED.key -> \"true\") {",
          "390:               spark.sql(s\"SELECT SUM(SIZE(col1)), SUM(SIZE(col2)) FROM parquet${version}Table\")",
          "391:                   .noop()",
          "392:             }",
          "393:           }",
          "394:         }",
          "396:         benchmark.run()",
          "397:       }",
          "398:     }",
          "399:   }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "615:         dataType => numericScanBenchmark(1024 * 1024 * 15, dataType)",
          "616:       }",
          "617:     }",
          "618:     runBenchmark(\"Int and String Scan\") {",
          "619:       intStringScanBenchmark(1024 * 1024 * 10)",
          "620:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "754:     runBenchmark(\"SQL Single Numeric Column Scan in Struct\") {",
          "755:       Seq(ByteType, ShortType, IntegerType, LongType, FloatType, DoubleType).foreach {",
          "756:         dataType => nestedNumericScanBenchmark(1024 * 1024 * 15, dataType)",
          "757:       }",
          "758:     }",
          "759:     runBenchmark(\"SQL Nested Column Scan\") {",
          "760:       nestedColumnScanBenchmark(1024 * 1024)",
          "761:     }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "669fc1b2c1cce7049a9f10e386ed1af050de3909",
      "candidate_info": {
        "commit_hash": "669fc1b2c1cce7049a9f10e386ed1af050de3909",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/669fc1b2c1cce7049a9f10e386ed1af050de3909",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ],
        "message": "[SPARK-39216][SQL] Do not collapse projects in CombineUnions if it hasCorrelatedSubquery\n\n### What changes were proposed in this pull request?\n\nMakes `CombineUnions` do not collapse projects if it hasCorrelatedSubquery. For example:\n```sql\nSELECT (SELECT IF(x, 1, 0)) AS a\nFROM (SELECT true) t(x)\nUNION\nSELECT 1 AS a\n```\n\nIt will throw exception:\n```\njava.lang.IllegalStateException: Couldn't find x#4 in []\n```\n\n### Why are the changes needed?\n\nFix bug.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nUnit test.\n\nCloses #36595 from wangyum/SPARK-39216.\n\nAuthored-by: Yuming Wang <yumwang@ebay.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 85bb7bf008d0346feaedc2aab55857d8f1b19908)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1340:     while (stack.nonEmpty) {",
          "1341:       stack.pop() match {",
          "1342:         case p1 @ Project(_, p2: Project)",
          "1344:           val newProjectList = buildCleanedProjectList(p1.projectList, p2.projectList)",
          "1345:           stack.pushAll(Seq(p2.copy(projectList = newProjectList)))",
          "1346:         case Distinct(Union(children, byName, allowMissingCol))",
          "",
          "[Removed Lines]",
          "1343:             if canCollapseExpressions(p1.projectList, p2.projectList, alwaysInline = false) =>",
          "",
          "[Added Lines]",
          "1343:             if canCollapseExpressions(p1.projectList, p2.projectList, alwaysInline = false) &&",
          "1344:               !p1.projectList.exists(SubqueryExpression.hasCorrelatedSubquery) &&",
          "1345:               !p2.projectList.exists(SubqueryExpression.hasCorrelatedSubquery) =>",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "4478:         ))",
          "4479:     }",
          "4480:   }",
          "4481: }",
          "4483: case class Foo(bar: Option[String])",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4482:   test(\"SPARK-39216: Don't collapse projects in CombineUnions if it hasCorrelatedSubquery\") {",
          "4483:     checkAnswer(",
          "4484:       sql(",
          "4485:         \"\"\"",
          "4486:           |SELECT (SELECT IF(x, 1, 0)) AS a",
          "4487:           |FROM (SELECT true) t(x)",
          "4488:           |UNION",
          "4489:           |SELECT 1 AS a",
          "4490:         \"\"\".stripMargin),",
          "4491:       Seq(Row(1)))",
          "4493:     checkAnswer(",
          "4494:       sql(",
          "4495:         \"\"\"",
          "4496:           |SELECT x + 1",
          "4497:           |FROM   (SELECT id",
          "4498:           |               + (SELECT Max(id)",
          "4499:           |                  FROM   range(2)) AS x",
          "4500:           |        FROM   range(1)) t",
          "4501:           |UNION",
          "4502:           |SELECT 1 AS a",
          "4503:         \"\"\".stripMargin),",
          "4504:       Seq(Row(2), Row(1)))",
          "4505:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}