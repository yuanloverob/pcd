{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "74ffaec0765e76cccd9b2180403ba50f3a449c13",
      "candidate_info": {
        "commit_hash": "74ffaec0765e76cccd9b2180403ba50f3a449c13",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/74ffaec0765e76cccd9b2180403ba50f3a449c13",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ],
        "message": "[SPARK-39012][SQL] SparkSQL cast partition value does not support all data types\n\n### What changes were proposed in this pull request?\n\nAdd binary type support for schema inference in SparkSQL.\n\n### Why are the changes needed?\n\nSpark does not support binary and boolean types when casting partition value to a target type. This PR adds the support for binary and boolean.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo. This is more like fixing a bug.\n\n### How was this patch tested?\n\nUT\n\nCloses #36344 from amaliujia/inferbinary.\n\nAuthored-by: Rui Wang <rui.wang@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 08c07a17717691f931d4d3206dd0385073f5bd08)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "545:       }",
          "546:     case it: AnsiIntervalType =>",
          "547:       Cast(Literal(unescapePathName(value)), it).eval()",
          "548:     case dt => throw QueryExecutionErrors.typeUnsupportedError(dt)",
          "549:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "548:     case BinaryType => value.getBytes()",
          "549:     case BooleanType => value.toBoolean",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import java.util.Locale",
          "25: import java.util.concurrent.atomic.AtomicBoolean",
          "27: import org.apache.commons.io.FileUtils",
          "29: import org.apache.spark.{AccumulatorSuite, SparkException}",
          "30: import org.apache.spark.scheduler.{SparkListener, SparkListenerJobStart}",
          "31: import org.apache.spark.sql.catalyst.analysis.FunctionRegistry",
          "33: import org.apache.spark.sql.catalyst.expressions.aggregate.{Complete, Partial}",
          "34: import org.apache.spark.sql.catalyst.optimizer.{ConvertToLocalRelation, NestedColumnAliasingSuite}",
          "35: import org.apache.spark.sql.catalyst.plans.logical.{LocalLimit, Project, RepartitionByExpression, Sort}",
          "",
          "[Removed Lines]",
          "32: import org.apache.spark.sql.catalyst.expressions.GenericRow",
          "",
          "[Added Lines]",
          "27: import scala.collection.mutable",
          "34: import org.apache.spark.sql.catalyst.expressions.{GenericRow, Hex}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "52: import org.apache.spark.sql.test.SQLTestData._",
          "53: import org.apache.spark.sql.types._",
          "54: import org.apache.spark.tags.ExtendedSQLTest",
          "56: import org.apache.spark.util.ResetSystemProperties",
          "58: @ExtendedSQLTest",
          "",
          "[Removed Lines]",
          "55: import org.apache.spark.unsafe.types.CalendarInterval",
          "",
          "[Added Lines]",
          "57: import org.apache.spark.unsafe.types.{CalendarInterval, UTF8String}",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "4368:       checkAnswer(df.repartitionByRange(2, col(\"v\")).selectExpr(\"try_avg(v)\"), Row(null))",
          "4369:     }",
          "4370:   }",
          "4371: }",
          "4373: case class Foo(bar: Option[String])",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4374:   test(\"SPARK-39012: SparkSQL cast partition value does not support all data types\") {",
          "4375:     withTempDir { dir =>",
          "4376:       val binary1 = Hex.hex(UTF8String.fromString(\"Spark\").getBytes).getBytes",
          "4377:       val binary2 = Hex.hex(UTF8String.fromString(\"SQL\").getBytes).getBytes",
          "4378:       val data = Seq[(Int, Boolean, Array[Byte])](",
          "4379:         (1, false, binary1),",
          "4380:         (2, true, binary2)",
          "4381:       )",
          "4382:       data.toDF(\"a\", \"b\", \"c\")",
          "4383:         .write",
          "4384:         .mode(\"overwrite\")",
          "4385:         .partitionBy(\"b\", \"c\")",
          "4386:         .parquet(dir.getCanonicalPath)",
          "4387:       val res = spark.read",
          "4388:         .schema(\"a INT, b BOOLEAN, c BINARY\")",
          "4389:         .parquet(dir.getCanonicalPath)",
          "4390:       checkAnswer(res,",
          "4391:         Seq(",
          "4392:           Row(1, false, mutable.WrappedArray.make(binary1)),",
          "4393:           Row(2, true, mutable.WrappedArray.make(binary2))",
          "4394:         ))",
          "4395:     }",
          "4396:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2ef56c295fc5c5e1060001cfe7158a2d00aa0d91",
      "candidate_info": {
        "commit_hash": "2ef56c295fc5c5e1060001cfe7158a2d00aa0d91",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/2ef56c295fc5c5e1060001cfe7158a2d00aa0d91",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasingSuite.scala"
        ],
        "message": "[SPARK-38530][SQL] Fix a bug that GeneratorNestedColumnAliasing can be incorrectly applied to some expressions\n\n### What changes were proposed in this pull request?\n\nThis PR makes GeneratorNestedColumnAliasing only be able to apply to GetStructField*(_: AttributeReference), here GetStructField* means nested GetStructField. The current way to collect expressions is a top-down way and it actually only checks 2 levels which is wrong. The rule is simple - If we see expressions other than GetStructField, we are done. When an expression E is pushed down into an Explode, the thing happens is:\nE(x) is now pushed down to apply to E(array(x)).\nSo only expressions that can operate on both x and array(x) can be pushed. GetStructField is special since we have GetArrayStructFields and when GetStructField is pushed down, it becomes GetArrayStructFields. Any other expressions are not applicable.\nWe also do not even need to check the child type is Array(Array()) or whether the rewritten expression has the pattern GetArrayStructFields(GetArrayStructFields()).\n1. When the child input type is Array(Array()), the ExtractValues expressions we get will always start from an innermost GetArrayStructFields, it does not align with GetStructField*(x).\n2. When we see GetArrayStructFields(GetArrayStructFields()) in the rewritten generator, we must have seen a GetArrayStructFields in the expressions before pushdown.\n\n### Why are the changes needed?\nIt fixes some correctness issues. See the above section for more details.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nUnit tests.\n\nCloses #35866 from minyyy/gnca_wrong_expr.\n\nLead-authored-by: minyyy <min.yang@databricks.com>\nCo-authored-by: minyyy <98760575+minyyy@users.noreply.github.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 13edafab9f45cc80aee41e2f82475367d88357ec)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasingSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasingSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "241:   def getAttributeToExtractValues(",
          "242:       exprList: Seq[Expression],",
          "245:     val nestedFieldReferences = new mutable.ArrayBuffer[ExtractValue]()",
          "246:     val otherRootReferences = new mutable.ArrayBuffer[AttributeReference]()",
          "247:     exprList.foreach { e =>",
          "250:         case ev: ExtractValue if !ev.exists(_.isInstanceOf[NamedLambdaVariable]) =>",
          "251:           if (ev.references.size == 1) {",
          "",
          "[Removed Lines]",
          "243:       exclusiveAttrs: Seq[Attribute]): Map[Attribute, Seq[ExtractValue]] = {",
          "248:       collectRootReferenceAndExtractValue(e).foreach {",
          "",
          "[Added Lines]",
          "243:       exclusiveAttrs: Seq[Attribute],",
          "244:       extractor: (Expression) => Seq[Expression] = collectRootReferenceAndExtractValue)",
          "245:     : Map[Attribute, Seq[ExtractValue]] = {",
          "250:       extractor(e).foreach {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "350:         return None",
          "351:       }",
          "352:       val generatorOutputSet = AttributeSet(g.qualifiedGeneratorOutput)",
          "354:         attrToExtractValues.partition { case (attr, _) =>",
          "355:           attr.references.subsetOf(generatorOutputSet) }",
          "357:       val pushedThrough = NestedColumnAliasing.rewritePlanWithAliases(",
          "358:         plan, attrToExtractValuesNotOnGenerator)",
          "364:       g.generator.children.head.dataType match {",
          "365:         case _: MapType => return Some(pushedThrough)",
          "366:         case ArrayType(_: ArrayType, _) => return Some(pushedThrough)",
          "367:         case _ =>",
          "368:       }",
          "",
          "[Removed Lines]",
          "353:       val (attrToExtractValuesOnGenerator, attrToExtractValuesNotOnGenerator) =",
          "",
          "[Added Lines]",
          "355:       var (attrToExtractValuesOnGenerator, attrToExtractValuesNotOnGenerator) =",
          "371:       def collectNestedGetStructFields(e: Expression): Seq[Expression] = {",
          "374:         def helper(e: Expression): (Seq[Expression], Seq[Expression]) = e match {",
          "375:           case _: AttributeReference => (Seq(e), Seq.empty)",
          "376:           case gsf: GetStructField =>",
          "377:             val child_res = helper(gsf.child)",
          "378:             (child_res._1.map(p => gsf.withNewChildren(Seq(p))), child_res._2)",
          "379:           case other =>",
          "380:             val child_res = other.children.map(helper)",
          "381:             val child_res_combined = (child_res.flatMap(_._1), child_res.flatMap(_._2))",
          "382:             (Seq.empty, child_res_combined._1 ++ child_res_combined._2)",
          "383:         }",
          "385:         val res = helper(e)",
          "386:         (res._1 ++ res._2).filterNot(_.isInstanceOf[Attribute])",
          "387:       }",
          "389:       attrToExtractValuesOnGenerator = NestedColumnAliasing.getAttributeToExtractValues(",
          "390:         attrToExtractValuesOnGenerator.flatMap(_._2).toSeq, Seq.empty,",
          "391:         collectNestedGetStructFields)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "391:                 e.withNewChildren(Seq(extractor))",
          "392:             }",
          "406:             val updatedGeneratorOutput = rewrittenG.generatorOutput",
          "407:               .zip(rewrittenG.generator.elementSchema.toAttributes)",
          "",
          "[Removed Lines]",
          "398:             val invalidExtractor = rewrittenG.generator.children.head.collect {",
          "399:               case GetArrayStructFields(_: GetArrayStructFields, _, _, _, _) => true",
          "400:             }",
          "401:             if (invalidExtractor.nonEmpty) {",
          "402:               return Some(pushedThrough)",
          "403:             }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "417:             val attrExprIdsOnGenerator = attrToExtractValuesOnGenerator.keys.map(_.exprId).toSet",
          "418:             val updatedProject = p.withNewChildren(Seq(updatedGenerate)).transformExpressions {",
          "420:                 updatedGenerate.output",
          "421:                   .find(a => attrExprIdsOnGenerator.contains(a.exprId))",
          "422:                   .getOrElse(f)",
          "",
          "[Removed Lines]",
          "419:               case f: ExtractValue if nestedFieldsOnGenerator.contains(f) =>",
          "",
          "[Added Lines]",
          "431:               case f: GetStructField if nestedFieldsOnGenerator.contains(f) =>",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasingSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasingSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasingSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasingSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import scala.collection.mutable.ArrayBuffer",
          "22: import org.apache.spark.sql.catalyst.SchemaPruningTest",
          "24: import org.apache.spark.sql.catalyst.dsl.expressions._",
          "25: import org.apache.spark.sql.catalyst.dsl.plans._",
          "26: import org.apache.spark.sql.catalyst.expressions._",
          "27: import org.apache.spark.sql.catalyst.plans.Cross",
          "28: import org.apache.spark.sql.catalyst.plans.logical._",
          "29: import org.apache.spark.sql.catalyst.rules.RuleExecutor",
          "32: class NestedColumnAliasingSuite extends SchemaPruningTest {",
          "",
          "[Removed Lines]",
          "23: import org.apache.spark.sql.catalyst.analysis.SimpleAnalyzer",
          "30: import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}",
          "",
          "[Added Lines]",
          "23: import org.apache.spark.sql.catalyst.analysis.{SimpleAnalyzer, UnresolvedExtractValue}",
          "30: import org.apache.spark.sql.types.{ArrayType, IntegerType, StringType, StructField, StructType}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "812:     val expected3 = contact.select($\"name\").rebalance($\"name\").select($\"name.first\").analyze",
          "813:     comparePlans(optimized3, expected3)",
          "814:   }",
          "815: }",
          "817: object NestedColumnAliasingSuite {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "816:   test(\"SPARK-38530: Do not push down nested ExtractValues with other expressions\") {",
          "817:     val inputType = StructType.fromDDL(",
          "818:       \"a int, b struct<c: array<int>, c2: int>\")",
          "819:     val simpleStruct = StructType.fromDDL(",
          "820:       \"b struct<c: struct<d: int, e: int>, c2 int>\"",
          "821:     )",
          "822:     val input = LocalRelation(",
          "823:       'id.int,",
          "824:       'col1.array(ArrayType(inputType)))",
          "826:     val query = input",
          "827:       .generate(Explode('col1))",
          "828:       .select(",
          "829:         UnresolvedExtractValue(",
          "830:           UnresolvedExtractValue(",
          "831:             CaseWhen(Seq(('col.getField(\"a\") === 1,",
          "832:               Literal.default(simpleStruct)))),",
          "833:             Literal(\"b\")),",
          "834:           Literal(\"c\")).as(\"result\"))",
          "835:       .analyze",
          "836:     val optimized = Optimize.execute(query)",
          "838:     val aliases = collectGeneratedAliases(optimized)",
          "841:     val expected = input",
          "842:       .select('col1.getField(\"a\").as(aliases(0)))",
          "843:       .generate(Explode($\"${aliases(0)}\"), unrequiredChildIndex = Seq(0))",
          "844:       .select(UnresolvedExtractValue(UnresolvedExtractValue(",
          "845:         CaseWhen(Seq(('col === 1,",
          "846:           Literal.default(simpleStruct)))), Literal(\"b\")), Literal(\"c\")).as(\"result\"))",
          "847:       .analyze",
          "849:     comparePlans(optimized, expected)",
          "850:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "15ebd56de6ae37587d750bb1e106c5dcb3e22958",
      "candidate_info": {
        "commit_hash": "15ebd56de6ae37587d750bb1e106c5dcb3e22958",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/15ebd56de6ae37587d750bb1e106c5dcb3e22958",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommandSuite.scala"
        ],
        "message": "[SPARK-39952][SQL] SaveIntoDataSourceCommand should recache result relation\n\n### What changes were proposed in this pull request?\n\nrecacheByPlan the result relation inside `SaveIntoDataSourceCommand`\n\n### Why are the changes needed?\n\nThe behavior of `SaveIntoDataSourceCommand` is similar with `InsertIntoDataSourceCommand` which supports append or overwirte data. In order to keep data consistent,  we should always do recacheByPlan the relation on post hoc.\n\n### Does this PR introduce _any_ user-facing change?\n\nyes, bug fix\n\n### How was this patch tested?\n\nadd test\n\nCloses #37380 from ulysses-you/refresh.\n\nAuthored-by: ulysses-you <ulyssesyou18@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 5fe0b245f7891a05bc4e1e641fd0aa9130118ea4)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommandSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommandSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommand.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.execution.datasources",
          "20: import org.apache.spark.sql.{Dataset, Row, SaveMode, SparkSession}",
          "21: import org.apache.spark.sql.catalyst.plans.QueryPlan",
          "22: import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import scala.util.control.NonFatal",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "41:   override def innerChildren: Seq[QueryPlan[_]] = Seq(query)",
          "43:   override def run(sparkSession: SparkSession): Seq[Row] = {",
          "45:       sparkSession.sqlContext, mode, options, Dataset.ofRows(sparkSession, query))",
          "47:     Seq.empty[Row]",
          "48:   }",
          "",
          "[Removed Lines]",
          "44:     dataSource.createRelation(",
          "",
          "[Added Lines]",
          "46:     val relation = dataSource.createRelation(",
          "49:     try {",
          "50:       val logicalRelation = LogicalRelation(relation, relation.schema.toAttributes, None, false)",
          "51:       sparkSession.sharedState.cacheManager.recacheByPlan(sparkSession, logicalRelation)",
          "52:     } catch {",
          "53:       case NonFatal(_) =>",
          "55:     }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommandSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommandSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommandSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SaveIntoDataSourceCommandSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.execution.datasources",
          "21: import org.apache.spark.sql.test.SharedSparkSession",
          "25:   test(\"simpleString is redacted\") {",
          "26:     val URL = \"connection.url\"",
          "",
          "[Removed Lines]",
          "20: import org.apache.spark.sql.SaveMode",
          "23: class SaveIntoDataSourceCommandSuite extends SharedSparkSession {",
          "",
          "[Added Lines]",
          "20: import org.apache.spark.rdd.RDD",
          "21: import org.apache.spark.sql.{DataFrame, QueryTest, Row, SaveMode, SparkSession, SQLContext}",
          "22: import org.apache.spark.sql.sources.{BaseRelation, CreatableRelationProvider, RelationProvider, TableScan}",
          "24: import org.apache.spark.sql.types.{LongType, StructField, StructType}",
          "26: class SaveIntoDataSourceCommandSuite extends QueryTest with SharedSparkSession {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "41:     assert(!logicalPlanString.contains(PASS))",
          "42:     assert(logicalPlanString.contains(DRIVER))",
          "43:   }",
          "44: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "48:   test(\"SPARK-39952: SaveIntoDataSourceCommand should recache result relation\") {",
          "49:     val provider = classOf[FakeV1DataSource].getName",
          "51:     def saveIntoDataSource(data: Int): Unit = {",
          "52:       spark.range(data)",
          "53:         .write",
          "54:         .mode(\"append\")",
          "55:         .format(provider)",
          "56:         .save()",
          "57:     }",
          "59:     def loadData: DataFrame = {",
          "60:       spark.read",
          "61:         .format(provider)",
          "62:         .load()",
          "63:     }",
          "65:     saveIntoDataSource(1)",
          "66:     val cached = loadData.cache()",
          "67:     checkAnswer(cached, Row(0))",
          "69:     saveIntoDataSource(2)",
          "70:     checkAnswer(loadData, Row(0) :: Row(1) :: Nil)",
          "72:     FakeV1DataSource.data = null",
          "73:   }",
          "74: }",
          "76: object FakeV1DataSource {",
          "77:   var data: RDD[Row] = _",
          "78: }",
          "80: class FakeV1DataSource extends RelationProvider with CreatableRelationProvider {",
          "81:   override def createRelation(",
          "82:      sqlContext: SQLContext,",
          "83:      parameters: Map[String, String]): BaseRelation = {",
          "84:     FakeRelation()",
          "85:   }",
          "87:   override def createRelation(",
          "88:      sqlContext: SQLContext,",
          "89:      mode: SaveMode,",
          "90:      parameters: Map[String, String],",
          "91:      data: DataFrame): BaseRelation = {",
          "92:     FakeV1DataSource.data = data.rdd",
          "93:     FakeRelation()",
          "94:   }",
          "95: }",
          "97: case class FakeRelation() extends BaseRelation with TableScan {",
          "98:   override def sqlContext: SQLContext = SparkSession.getActiveSession.get.sqlContext",
          "99:   override def schema: StructType = StructType(Seq(StructField(\"id\", LongType)))",
          "100:   override def buildScan(): RDD[Row] = FakeV1DataSource.data",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e9ee2c8787adfc07a7d1882b7749dd41c2990048",
      "candidate_info": {
        "commit_hash": "e9ee2c8787adfc07a7d1882b7749dd41c2990048",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/e9ee2c8787adfc07a7d1882b7749dd41c2990048",
        "files": [
          "LICENSE-binary",
          "core/pom.xml",
          "core/src/test/scala/org/apache/spark/storage/DiskBlockManagerSuite.scala",
          "pom.xml"
        ],
        "message": "[SPARK-37618][CORE][FOLLOWUP] Support cleaning up shuffle blocks from external shuffle service\n\n### What changes were proposed in this pull request?\n\nFix test failure in build.\nDepending on the umask of the process running tests (which is typically inherited from the user's default umask), the group writable bit for the files/directories could be set or unset. The test was assuming that by default the umask will be restrictive (and so files/directories wont be group writable). Since this is not a valid assumption, we use jnr to change the umask of the process to be more restrictive - so that the test can validate the behavior change - and reset it back once the test is done.\n\n### Why are the changes needed?\n\nFix test failure in build\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\nAdds jnr as a test scoped dependency, which does not bring in any other new dependency (asm is already a dep in spark).\n```\n[INFO] +- com.github.jnr:jnr-posix:jar:3.0.9:test\n[INFO] |  +- com.github.jnr:jnr-ffi:jar:2.0.1:test\n[INFO] |  |  +- com.github.jnr:jffi:jar:1.2.7:test\n[INFO] |  |  +- com.github.jnr:jffi:jar:native:1.2.7:test\n[INFO] |  |  +- org.ow2.asm:asm:jar:5.0.3:test\n[INFO] |  |  +- org.ow2.asm:asm-commons:jar:5.0.3:test\n[INFO] |  |  +- org.ow2.asm:asm-analysis:jar:5.0.3:test\n[INFO] |  |  +- org.ow2.asm:asm-tree:jar:5.0.3:test\n[INFO] |  |  +- org.ow2.asm:asm-util:jar:5.0.3:test\n[INFO] |  |  \\- com.github.jnr:jnr-x86asm:jar:1.0.2:test\n[INFO] |  \\- com.github.jnr:jnr-constants:jar:0.8.6:test\n```\n\n### How was this patch tested?\nModification to existing test.\nTested on Linux, skips test when native posix env is not found.\n\nCloses #36473 from mridulm/fix-SPARK-37618-test.\n\nAuthored-by: Mridul Muralidharan <mridulatgmail.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n(cherry picked from commit 317407171cb36439c371153cfd45c1482bf5e425)\nSigned-off-by: Sean Owen <srowen@gmail.com>",
        "before_after_code_files": [
          "core/src/test/scala/org/apache/spark/storage/DiskBlockManagerSuite.scala||core/src/test/scala/org/apache/spark/storage/DiskBlockManagerSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/test/scala/org/apache/spark/storage/DiskBlockManagerSuite.scala||core/src/test/scala/org/apache/spark/storage/DiskBlockManagerSuite.scala": [
          "File: core/src/test/scala/org/apache/spark/storage/DiskBlockManagerSuite.scala -> core/src/test/scala/org/apache/spark/storage/DiskBlockManagerSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: import com.fasterxml.jackson.core.`type`.TypeReference",
          "26: import com.fasterxml.jackson.databind.ObjectMapper",
          "27: import org.apache.commons.io.FileUtils",
          "28: import org.scalatest.{BeforeAndAfterAll, BeforeAndAfterEach}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: import jnr.posix.{POSIX, POSIXFactory}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "141:     assert(attemptId.equals(\"1\"))",
          "142:   }",
          "144:   test(\"SPARK-37618: Sub dirs are group writable when removing from shuffle service enabled\") {",
          "145:     val conf = testConf.clone",
          "146:     conf.set(\"spark.local.dir\", rootDirs)",
          "147:     conf.set(\"spark.shuffle.service.enabled\", \"true\")",
          "148:     conf.set(\"spark.shuffle.service.removeShuffle\", \"false\")",
          "166:   }",
          "168:   def writeToFile(file: File, numBytes: Int): Unit = {",
          "",
          "[Removed Lines]",
          "149:     val diskBlockManager = new DiskBlockManager(conf, deleteFilesOnStop = true, isDriver = false)",
          "150:     val blockId = new TestBlockId(\"test\")",
          "151:     val newFile = diskBlockManager.getFile(blockId)",
          "152:     val parentDir = newFile.getParentFile()",
          "153:     assert(parentDir.exists && parentDir.isDirectory)",
          "154:     val permission = Files.getPosixFilePermissions(parentDir.toPath)",
          "155:     assert(!permission.contains(PosixFilePermission.GROUP_WRITE))",
          "157:     assert(parentDir.delete())",
          "159:     conf.set(\"spark.shuffle.service.removeShuffle\", \"true\")",
          "160:     val diskBlockManager2 = new DiskBlockManager(conf, deleteFilesOnStop = true, isDriver = false)",
          "161:     val newFile2 = diskBlockManager2.getFile(blockId)",
          "162:     val parentDir2 = newFile2.getParentFile()",
          "163:     assert(parentDir2.exists && parentDir2.isDirectory)",
          "164:     val permission2 = Files.getPosixFilePermissions(parentDir2.toPath)",
          "165:     assert(permission2.contains(PosixFilePermission.GROUP_WRITE))",
          "",
          "[Added Lines]",
          "147:   private def getAndSetUmask(posix: POSIX, mask: String): String = {",
          "148:     val prev = posix.umask(BigInt(mask, 8).toInt)",
          "149:     \"0\" + \"%o\".format(prev)",
          "150:   }",
          "157:     val posix = POSIXFactory.getPOSIX",
          "159:     assume(posix.isNative, \"Skipping test for SPARK-37618, native posix support not found\")",
          "161:     val oldUmask = getAndSetUmask(posix, \"077\")",
          "162:     try {",
          "163:       val diskBlockManager = new DiskBlockManager(conf, deleteFilesOnStop = true,",
          "164:         isDriver = false)",
          "165:       val blockId = new TestBlockId(\"test\")",
          "166:       val newFile = diskBlockManager.getFile(blockId)",
          "167:       val parentDir = newFile.getParentFile()",
          "168:       assert(parentDir.exists && parentDir.isDirectory)",
          "169:       val permission = Files.getPosixFilePermissions(parentDir.toPath)",
          "170:       assert(!permission.contains(PosixFilePermission.GROUP_WRITE))",
          "172:       assert(parentDir.delete())",
          "174:       conf.set(\"spark.shuffle.service.removeShuffle\", \"true\")",
          "175:       val diskBlockManager2 = new DiskBlockManager(conf, deleteFilesOnStop = true,",
          "176:         isDriver = false)",
          "177:       val newFile2 = diskBlockManager2.getFile(blockId)",
          "178:       val parentDir2 = newFile2.getParentFile()",
          "179:       assert(parentDir2.exists && parentDir2.isDirectory)",
          "180:       val permission2 = Files.getPosixFilePermissions(parentDir2.toPath)",
          "181:       assert(permission2.contains(PosixFilePermission.GROUP_WRITE))",
          "182:     } finally {",
          "183:       getAndSetUmask(posix, oldUmask)",
          "184:     }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1804f5c8c02fd9beade9e986540dac248638e8a5",
      "candidate_info": {
        "commit_hash": "1804f5c8c02fd9beade9e986540dac248638e8a5",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/1804f5c8c02fd9beade9e986540dac248638e8a5",
        "files": [
          "R/create-docs.sh"
        ],
        "message": "[SPARK-37474][R][DOCS][FOLLOW-UP] Make SparkR documentation able to build on Mac OS\n\n### What changes were proposed in this pull request?\n\nCurrently SparkR documentation fails because of the usage `grep -oP `. Mac OS does not have this.\nThis PR fixes it via using the existing way used in the current scripts at: https://github.com/apache/spark/blob/0494dc90af48ce7da0625485a4dc6917a244d580/R/check-cran.sh#L52\n\n### Why are the changes needed?\n\nTo make the dev easier.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, dev-only.\n\n### How was this patch tested?\n\nManually tested via:\n\n```bash\ncd R\n./create-docs.sh\n```\n\nCloses #36423 from HyukjinKwon/SPARK-37474.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 6479455b8db40d584045cdb13e6c3cdfda7a2c0b)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "R/create-docs.sh||R/create-docs.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "R/create-docs.sh||R/create-docs.sh": [
          "File: R/create-docs.sh -> R/create-docs.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "57: # Determine Spark(R) version",
          "60: # Update url",
          "61: sed \"s/{SPARK_VERSION}/$SPARK_VERSION/\" ../pkgdown/_pkgdown_template.yml > ../_pkgdown.yml",
          "",
          "[Removed Lines]",
          "58: SPARK_VERSION=$(grep -oP \"(?<=Version:\\ ).*\" ../DESCRIPTION)",
          "",
          "[Added Lines]",
          "58: SPARK_VERSION=$(grep Version \"../DESCRIPTION\" | awk '{print $NF}')",
          "",
          "---------------"
        ]
      }
    }
  ]
}