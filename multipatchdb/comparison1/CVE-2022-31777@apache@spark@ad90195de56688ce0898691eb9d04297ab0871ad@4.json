{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "4a4e35a30c7bb7534aece8e917a2813d47c2c498",
      "candidate_info": {
        "commit_hash": "4a4e35a30c7bb7534aece8e917a2813d47c2c498",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/4a4e35a30c7bb7534aece8e917a2813d47c2c498",
        "files": [
          "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/aggregate/Aggregation.java",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/AggregatePushDownUtils.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcScan.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScan.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala"
        ],
        "message": "[SPARK-38997][SQL] DS V2 aggregate push-down supports group by expressions\n\n### What changes were proposed in this pull request?\nCurrently, Spark DS V2 aggregate push-down only supports group by column.\nBut the SQL show below is very useful and common.\n```\nSELECT\n  CASE\n    WHEN 'SALARY' > 8000.00\n      AND 'SALARY' < 10000.00\n    THEN 'SALARY'\n    ELSE 0.00\n  END AS key,\n  SUM('SALARY')\nFROM \"test\".\"employee\"\nGROUP BY key\n```\n\n### Why are the changes needed?\nLet DS V2 aggregate push-down supports group by expressions\n\n### Does this PR introduce _any_ user-facing change?\n'No'.\nNew feature.\n\n### How was this patch tested?\nNew tests\n\nCloses #36325 from beliefer/SPARK-38997.\n\nAuthored-by: Jiaan Geng <beliefer@163.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit ee6ea3c68694e35c36ad006a7762297800d1e463)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/aggregate/Aggregation.java||sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/aggregate/Aggregation.java",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/AggregatePushDownUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/AggregatePushDownUtils.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcScan.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcScan.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScan.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScan.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/aggregate/Aggregation.java||sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/aggregate/Aggregation.java": [
          "File: sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/aggregate/Aggregation.java -> sql/catalyst/src/main/java/org/apache/spark/sql/connector/expressions/aggregate/Aggregation.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import java.io.Serializable;",
          "22: import org.apache.spark.annotation.Evolving;",
          "",
          "[Removed Lines]",
          "23: import org.apache.spark.sql.connector.expressions.NamedReference;",
          "",
          "[Added Lines]",
          "23: import org.apache.spark.sql.connector.expressions.Expression;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "30: @Evolving",
          "31: public final class Aggregation implements Serializable {",
          "32:   private final AggregateFunc[] aggregateExpressions;",
          "36:     this.aggregateExpressions = aggregateExpressions;",
          "38:   }",
          "40:   public AggregateFunc[] aggregateExpressions() { return aggregateExpressions; }",
          "43: }",
          "",
          "[Removed Lines]",
          "33:   private final NamedReference[] groupByColumns;",
          "35:   public Aggregation(AggregateFunc[] aggregateExpressions, NamedReference[] groupByColumns) {",
          "37:     this.groupByColumns = groupByColumns;",
          "42:   public NamedReference[] groupByColumns() { return groupByColumns; }",
          "",
          "[Added Lines]",
          "33:   private final Expression[] groupByExpressions;",
          "35:   public Aggregation(AggregateFunc[] aggregateExpressions, Expression[] groupByExpressions) {",
          "37:     this.groupByExpressions = groupByExpressions;",
          "42:   public Expression[] groupByExpressions() { return groupByExpressions; }",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "163:       \"PushedFilters\" -> pushedFilters) ++",
          "164:       pushedDownOperators.aggregation.fold(Map[String, String]()) { v =>",
          "165:         Map(\"PushedAggregates\" -> seqToString(v.aggregateExpressions.map(_.describe())),",
          "167:       topNOrLimitInfo ++",
          "168:       pushedDownOperators.sample.map(v => \"PushedSample\" ->",
          "169:         s\"SAMPLE (${(v.upperBound - v.lowerBound) * 100}) ${v.withReplacement} SEED(${v.seed})\"",
          "",
          "[Removed Lines]",
          "166:           \"PushedGroupByColumns\" -> seqToString(v.groupByColumns.map(_.describe())))} ++",
          "",
          "[Added Lines]",
          "166:           \"PushedGroupByExpressions\" -> seqToString(v.groupByExpressions.map(_.describe())))} ++",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/AggregatePushDownUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/AggregatePushDownUtils.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/AggregatePushDownUtils.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/AggregatePushDownUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import org.apache.spark.sql.catalyst.InternalRow",
          "21: import org.apache.spark.sql.catalyst.expressions.{Expression, GenericInternalRow}",
          "22: import org.apache.spark.sql.connector.expressions.aggregate.{AggregateFunc, Aggregation, Count, CountStar, Max, Min}",
          "23: import org.apache.spark.sql.execution.RowToColumnConverter",
          "24: import org.apache.spark.sql.execution.datasources.v2.V2ColumnUtils",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: import org.apache.spark.sql.connector.expressions.{Expression => V2Expression, FieldReference}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "93:       return None",
          "94:     }",
          "",
          "[Removed Lines]",
          "96:     if (aggregation.groupByColumns.nonEmpty &&",
          "97:       partitionNames.size != aggregation.groupByColumns.length) {",
          "",
          "[Added Lines]",
          "97:     if (aggregation.groupByExpressions.nonEmpty &&",
          "98:       partitionNames.size != aggregation.groupByExpressions.length) {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "107:       return None",
          "108:     }",
          "114:     }",
          "116:     aggregation.aggregateExpressions.foreach {",
          "",
          "[Removed Lines]",
          "109:     aggregation.groupByColumns.foreach { col =>",
          "112:       if (col.fieldNames.length != 1 || !isPartitionCol(col.fieldNames.head)) return None",
          "113:       finalSchema = finalSchema.add(getStructFieldForCol(col.fieldNames.head))",
          "",
          "[Added Lines]",
          "110:     aggregation.groupByExpressions.map(extractColName).foreach { colName =>",
          "113:       if (colName.isEmpty || !isPartitionCol(colName.get)) return None",
          "114:       finalSchema = finalSchema.add(getStructFieldForCol(colName.get))",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "137:   def equivalentAggregations(a: Aggregation, b: Aggregation): Boolean = {",
          "138:     a.aggregateExpressions.sortBy(_.hashCode())",
          "139:       .sameElements(b.aggregateExpressions.sortBy(_.hashCode())) &&",
          "141:   }",
          "",
          "[Removed Lines]",
          "140:       a.groupByColumns.sortBy(_.hashCode()).sameElements(b.groupByColumns.sortBy(_.hashCode()))",
          "",
          "[Added Lines]",
          "141:       a.groupByExpressions.sortBy(_.hashCode())",
          "142:         .sameElements(b.groupByExpressions.sortBy(_.hashCode()))",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "164:   def getSchemaWithoutGroupingExpression(",
          "165:       aggSchema: StructType,",
          "166:       aggregation: Aggregation): StructType = {",
          "168:     if (numOfGroupByColumns > 0) {",
          "169:       new StructType(aggSchema.fields.drop(numOfGroupByColumns))",
          "170:     } else {",
          "",
          "[Removed Lines]",
          "167:     val numOfGroupByColumns = aggregation.groupByColumns.length",
          "",
          "[Added Lines]",
          "169:     val numOfGroupByColumns = aggregation.groupByExpressions.length",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "179:       partitionSchema: StructType,",
          "180:       aggregation: Aggregation,",
          "181:       partitionValues: InternalRow): InternalRow = {",
          "183:     assert(groupByColNames.length == partitionSchema.length &&",
          "184:       groupByColNames.length == partitionValues.numFields, \"The number of group by columns \" +",
          "185:       s\"${groupByColNames.length} should be the same as partition schema length \" +",
          "",
          "[Removed Lines]",
          "182:     val groupByColNames = aggregation.groupByColumns.map(_.fieldNames.head)",
          "",
          "[Added Lines]",
          "184:     val groupByColNames = aggregation.groupByExpressions.flatMap(extractColName)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "197:       partitionValues",
          "198:     }",
          "199:   }",
          "200: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "203:   private def extractColName(v2Expr: V2Expression): Option[String] = v2Expr match {",
          "204:     case f: FieldReference if f.fieldNames.length == 1 => Some(f.fieldNames.head)",
          "205:     case _ => None",
          "206:   }",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "759:   protected[sql] def translateAggregation(",
          "760:       aggregates: Seq[AggregateExpression], groupBy: Seq[Expression]): Option[Aggregation] = {",
          "765:       case _ => None",
          "766:     }",
          "768:     val translatedAggregates = aggregates.flatMap(translateAggregate)",
          "771:     if (translatedAggregates.length != aggregates.length ||",
          "772:       translatedGroupBys.length != groupBy.length) {",
          "",
          "[Removed Lines]",
          "762:     def columnAsString(e: Expression): Option[FieldReference] = e match {",
          "763:       case PushableColumnWithoutNestedColumn(name) =>",
          "764:         Some(FieldReference.column(name).asInstanceOf[FieldReference])",
          "769:     val translatedGroupBys = groupBy.flatMap(columnAsString)",
          "",
          "[Added Lines]",
          "762:     def translateGroupBy(e: Expression): Option[V2Expression] = e match {",
          "763:       case PushableExpression(expr) => Some(expr)",
          "768:     val translatedGroupBys = groupBy.flatMap(translateGroupBy)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "519:     val orcValuesDeserializer = new OrcDeserializer(schemaWithoutGroupBy,",
          "520:       (0 until schemaWithoutGroupBy.length).toArray)",
          "521:     val resultRow = orcValuesDeserializer.deserializeFromValues(aggORCValues)",
          "523:       val reOrderedPartitionValues = AggregatePushDownUtils.reOrderPartitionCol(",
          "524:         partitionSchema, aggregation, partitionValues)",
          "525:       new JoinedRow(reOrderedPartitionValues, resultRow)",
          "",
          "[Removed Lines]",
          "522:     if (aggregation.groupByColumns.nonEmpty) {",
          "",
          "[Added Lines]",
          "522:     if (aggregation.groupByExpressions.nonEmpty) {",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "279:         throw new SparkException(\"Unexpected parquet type name: \" + primitiveTypeNames(i))",
          "280:     }",
          "283:       val reorderedPartitionValues = AggregatePushDownUtils.reOrderPartitionCol(",
          "284:         partitionSchema, aggregation, partitionValues)",
          "285:       new JoinedRow(reorderedPartitionValues, converter.currentRecord)",
          "",
          "[Removed Lines]",
          "282:     if (aggregation.groupByColumns.nonEmpty) {",
          "",
          "[Added Lines]",
          "282:     if (aggregation.groupByExpressions.nonEmpty) {",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "184:                   val newOutput = scan.readSchema().toAttributes",
          "185:                   assert(newOutput.length == groupingExpressions.length + finalAggregates.length)",
          "189:                   }",
          "190:                   val aggOutput = newOutput.drop(groupAttrs.length)",
          "191:                   val output = groupAttrs ++ aggOutput",
          "",
          "[Removed Lines]",
          "186:                   val groupAttrs = normalizedGroupingExpressions.zip(newOutput).map {",
          "187:                     case (a: Attribute, b: Attribute) => b.withExprId(a.exprId)",
          "188:                     case (_, b) => b",
          "",
          "[Added Lines]",
          "186:                   val groupByExprToOutputOrdinal = mutable.HashMap.empty[Expression, Int]",
          "187:                   val groupAttrs = normalizedGroupingExpressions.zip(newOutput).zipWithIndex.map {",
          "188:                     case ((a: Attribute, b: Attribute), _) => b.withExprId(a.exprId)",
          "189:                     case ((expr, attr), ordinal) =>",
          "190:                       if (!groupByExprToOutputOrdinal.contains(expr.canonicalized)) {",
          "191:                         groupByExprToOutputOrdinal(expr.canonicalized) = ordinal",
          "192:                       }",
          "193:                       attr",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "196:                        |Pushed Aggregate Functions:",
          "197:                        | ${pushedAggregates.get.aggregateExpressions.mkString(\", \")}",
          "198:                        |Pushed Group by:",
          "200:                        |Output: ${output.mkString(\", \")}",
          "201:                       \"\"\".stripMargin)",
          "",
          "[Removed Lines]",
          "199:                        | ${pushedAggregates.get.groupByColumns.mkString(\", \")}",
          "",
          "[Added Lines]",
          "204:                        | ${pushedAggregates.get.groupByExpressions.mkString(\", \")}",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "205:                     DataSourceV2ScanRelation(sHolder.relation, wrappedScan, output)",
          "206:                   if (r.supportCompletePushDown(pushedAggregates.get)) {",
          "207:                     val projectExpressions = finalResultExpressions.map { expr =>",
          "211:                         case agg: AggregateExpression =>",
          "212:                           val ordinal = aggExprToOutputOrdinal(agg.canonicalized)",
          "213:                           val child =",
          "214:                             addCastIfNeeded(aggOutput(ordinal), agg.resultAttribute.dataType)",
          "215:                           Alias(child, agg.resultAttribute.name)(agg.resultAttribute.exprId)",
          "216:                       }",
          "217:                     }.asInstanceOf[Seq[NamedExpression]]",
          "218:                     Project(projectExpressions, scanRelation)",
          "",
          "[Removed Lines]",
          "210:                       expr.transform {",
          "",
          "[Added Lines]",
          "213:                       expr.transformDown {",
          "219:                         case expr if groupByExprToOutputOrdinal.contains(expr.canonicalized) =>",
          "220:                           val ordinal = groupByExprToOutputOrdinal(expr.canonicalized)",
          "221:                           addCastIfNeeded(groupAttrs(ordinal), expr.dataType)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "255:                             case other => other",
          "256:                           }",
          "257:                         agg.copy(aggregateFunction = aggFunction)",
          "258:                     }",
          "259:                   }",
          "260:                 }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "264:                       case expr if groupByExprToOutputOrdinal.contains(expr.canonicalized) =>",
          "265:                         val ordinal = groupByExprToOutputOrdinal(expr.canonicalized)",
          "266:                         addCastIfNeeded(groupAttrs(ordinal), expr.dataType)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/jdbc/JDBCScanBuilder.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import org.apache.spark.internal.Logging",
          "22: import org.apache.spark.sql.SparkSession",
          "24: import org.apache.spark.sql.connector.expressions.aggregate.Aggregation",
          "25: import org.apache.spark.sql.connector.expressions.filter.Predicate",
          "26: import org.apache.spark.sql.connector.read.{Scan, ScanBuilder, SupportsPushDownAggregates, SupportsPushDownLimit, SupportsPushDownRequiredColumns, SupportsPushDownTableSample, SupportsPushDownTopN, SupportsPushDownV2Filters}",
          "",
          "[Removed Lines]",
          "23: import org.apache.spark.sql.connector.expressions.SortOrder",
          "",
          "[Added Lines]",
          "23: import org.apache.spark.sql.connector.expressions.{FieldReference, SortOrder}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "71:   private var pushedAggregateList: Array[String] = Array()",
          "75:   override def supportCompletePushDown(aggregation: Aggregation): Boolean = {",
          "77:     jdbcOptions.numPartitions.map(_ == 1).getOrElse(true) ||",
          "79:         jdbcOptions.partitionColumn.exists(fieldNames(0).equalsIgnoreCase(_)))",
          "80:   }",
          "",
          "[Removed Lines]",
          "73:   private var pushedGroupByCols: Option[Array[String]] = None",
          "76:     lazy val fieldNames = aggregation.groupByColumns()(0).fieldNames()",
          "78:       (aggregation.groupByColumns().length == 1 && fieldNames.length == 1 &&",
          "",
          "[Added Lines]",
          "73:   private var pushedGroupBys: Option[Array[String]] = None",
          "76:     lazy val fieldNames = aggregation.groupByExpressions()(0) match {",
          "77:       case field: FieldReference => field.fieldNames",
          "78:       case _ => Array.empty[String]",
          "79:     }",
          "81:       (aggregation.groupByExpressions().length == 1 && fieldNames.length == 1 &&",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "86:     val compiledAggs = aggregation.aggregateExpressions.flatMap(dialect.compileAggregate)",
          "87:     if (compiledAggs.length != aggregation.aggregateExpressions.length) return false",
          "100:       \"\"",
          "101:     } else {",
          "103:     }",
          "105:     val aggQuery = s\"SELECT ${selectList.mkString(\",\")} FROM ${jdbcOptions.tableOrQuery} \" +",
          "",
          "[Removed Lines]",
          "89:     val groupByCols = aggregation.groupByColumns.map { col =>",
          "90:       if (col.fieldNames.length != 1) return false",
          "91:       dialect.quoteIdentifier(col.fieldNames.head)",
          "92:     }",
          "98:     val selectList = groupByCols ++ compiledAggs",
          "99:     val groupByClause = if (groupByCols.isEmpty) {",
          "102:       \"GROUP BY \" + groupByCols.mkString(\",\")",
          "",
          "[Added Lines]",
          "92:     val compiledGroupBys = aggregation.groupByExpressions.flatMap(dialect.compileExpression)",
          "93:     if (compiledGroupBys.length != aggregation.groupByExpressions.length) return false",
          "99:     val selectList = compiledGroupBys ++ compiledAggs",
          "100:     val groupByClause = if (compiledGroupBys.isEmpty) {",
          "103:       \"GROUP BY \" + compiledGroupBys.mkString(\",\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "107:     try {",
          "108:       finalSchema = JDBCRDD.getQueryOutputSchema(aggQuery, jdbcOptions, dialect)",
          "109:       pushedAggregateList = selectList",
          "111:       true",
          "112:     } catch {",
          "113:       case NonFatal(e) =>",
          "",
          "[Removed Lines]",
          "110:       pushedGroupByCols = Some(groupByCols)",
          "",
          "[Added Lines]",
          "111:       pushedGroupBys = Some(compiledGroupBys)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "175:     JDBCScan(JDBCRelation(schema, parts, jdbcOptions)(session), finalSchema, pushedPredicate,",
          "177:   }",
          "178: }",
          "",
          "[Removed Lines]",
          "176:       pushedAggregateList, pushedGroupByCols, tableSample, pushedLimit, sortOrders)",
          "",
          "[Added Lines]",
          "177:       pushedAggregateList, pushedGroupBys, tableSample, pushedLimit, sortOrders)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcScan.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcScan.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcScan.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/orc/OrcScan.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "84:   lazy private val (pushedAggregationsStr, pushedGroupByStr) = if (pushedAggregate.nonEmpty) {",
          "85:     (seqToString(pushedAggregate.get.aggregateExpressions),",
          "87:   } else {",
          "88:     (\"[]\", \"[]\")",
          "89:   }",
          "",
          "[Removed Lines]",
          "86:       seqToString(pushedAggregate.get.groupByColumns))",
          "",
          "[Added Lines]",
          "86:       seqToString(pushedAggregate.get.groupByExpressions))",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScan.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScan.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScan.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetScan.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "117:   lazy private val (pushedAggregationsStr, pushedGroupByStr) = if (pushedAggregate.nonEmpty) {",
          "118:     (seqToString(pushedAggregate.get.aggregateExpressions),",
          "120:   } else {",
          "121:     (\"[]\", \"[]\")",
          "122:   }",
          "",
          "[Removed Lines]",
          "119:       seqToString(pushedAggregate.get.groupByColumns))",
          "",
          "[Added Lines]",
          "119:       seqToString(pushedAggregate.get.groupByExpressions))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala -> sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "171:       .groupBy(\"DEPT\").sum(\"SALARY\")",
          "172:       .limit(1)",
          "173:     checkPushedInfo(df4,",
          "175:     checkAnswer(df4, Seq(Row(1, 19000.00)))",
          "177:     val name = udf { (x: String) => x.matches(\"cat|dav|amy\") }",
          "",
          "[Removed Lines]",
          "174:       \"PushedAggregates: [SUM(SALARY)], PushedFilters: [], PushedGroupByColumns: [DEPT], \")",
          "",
          "[Added Lines]",
          "174:       \"PushedAggregates: [SUM(SALARY)], PushedFilters: [], PushedGroupByExpressions: [DEPT], \")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "257:       .limit(1)",
          "258:     checkSortRemoved(df6, false)",
          "259:     checkPushedInfo(df6, \"PushedAggregates: [SUM(SALARY)],\" +",
          "261:     checkAnswer(df6, Seq(Row(1, 19000.00)))",
          "263:     val name = udf { (x: String) => x.matches(\"cat|dav|amy\") }",
          "",
          "[Removed Lines]",
          "260:       \" PushedFilters: [], PushedGroupByColumns: [DEPT], \")",
          "",
          "[Added Lines]",
          "260:       \" PushedFilters: [], PushedGroupByExpressions: [DEPT], \")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "609:     checkAggregateRemoved(df)",
          "610:     checkPushedInfo(df, \"PushedAggregates: [MAX(SALARY), AVG(BONUS)], \" +",
          "611:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], \" +",
          "613:     checkAnswer(df, Seq(Row(10000, 1100.0), Row(12000, 1250.0), Row(12000, 1200.0)))",
          "614:   }",
          "",
          "[Removed Lines]",
          "612:       \"PushedGroupByColumns: [DEPT], \")",
          "",
          "[Added Lines]",
          "612:       \"PushedGroupByExpressions: [DEPT], \")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "630:     checkAggregateRemoved(df)",
          "631:     checkPushedInfo(df, \"PushedAggregates: [MAX(ID), AVG(ID)], \" +",
          "632:       \"PushedFilters: [ID IS NOT NULL, ID > 0], \" +",
          "634:     checkAnswer(df, Seq(Row(2, 1.5)))",
          "635:   }",
          "",
          "[Removed Lines]",
          "633:       \"PushedGroupByColumns: [], \")",
          "",
          "[Added Lines]",
          "633:       \"PushedGroupByExpressions: [], \")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "712:   }",
          "714:   test(\"scan with aggregate push-down: SUM with group by\") {",
          "720:   }",
          "722:   test(\"scan with aggregate push-down: DISTINCT SUM with group by\") {",
          "723:     val df = sql(\"SELECT SUM(DISTINCT SALARY) FROM h2.test.employee GROUP BY DEPT\")",
          "724:     checkAggregateRemoved(df)",
          "725:     checkPushedInfo(df, \"PushedAggregates: [SUM(DISTINCT SALARY)], \" +",
          "727:     checkAnswer(df, Seq(Row(19000), Row(22000), Row(12000)))",
          "728:   }",
          "",
          "[Removed Lines]",
          "715:     val df = sql(\"SELECT SUM(SALARY) FROM h2.test.employee GROUP BY DEPT\")",
          "716:     checkAggregateRemoved(df)",
          "717:     checkPushedInfo(df, \"PushedAggregates: [SUM(SALARY)], \" +",
          "718:       \"PushedFilters: [], PushedGroupByColumns: [DEPT], \")",
          "719:     checkAnswer(df, Seq(Row(19000), Row(22000), Row(12000)))",
          "726:       \"PushedFilters: [], PushedGroupByColumns: [DEPT]\")",
          "",
          "[Added Lines]",
          "715:     val df1 = sql(\"SELECT SUM(SALARY) FROM h2.test.employee GROUP BY DEPT\")",
          "716:     checkAggregateRemoved(df1)",
          "717:     checkPushedInfo(df1, \"PushedAggregates: [SUM(SALARY)], \" +",
          "718:       \"PushedFilters: [], PushedGroupByExpressions: [DEPT], \")",
          "719:     checkAnswer(df1, Seq(Row(19000), Row(22000), Row(12000)))",
          "721:     val df2 = sql(",
          "722:       \"\"\"",
          "723:         |SELECT CASE WHEN SALARY > 8000 AND SALARY < 10000 THEN SALARY ELSE 0 END as key,",
          "724:         |  SUM(SALARY) FROM h2.test.employee GROUP BY key\"\"\".stripMargin)",
          "725:     checkAggregateRemoved(df2)",
          "726:     checkPushedInfo(df2,",
          "727:       \"\"\"",
          "728:         |PushedAggregates: [SUM(SALARY)],",
          "729:         |PushedFilters: [],",
          "730:         |PushedGroupByExpressions:",
          "731:         |[CASE WHEN (SALARY > 8000.00) AND (SALARY < 10000.00) THEN SALARY ELSE 0.00 END],",
          "732:         |\"\"\".stripMargin.replaceAll(\"\\n\", \" \"))",
          "733:     checkAnswer(df2, Seq(Row(0, 44000), Row(9000, 9000)))",
          "735:     val df3 = spark.read",
          "736:       .option(\"partitionColumn\", \"dept\")",
          "737:       .option(\"lowerBound\", \"0\")",
          "738:       .option(\"upperBound\", \"2\")",
          "739:       .option(\"numPartitions\", \"2\")",
          "740:       .table(\"h2.test.employee\")",
          "741:       .groupBy(when(($\"SALARY\" > 8000).and($\"SALARY\" < 10000), $\"SALARY\").otherwise(0).as(\"key\"))",
          "742:       .agg(sum($\"SALARY\"))",
          "743:     checkAggregateRemoved(df3, false)",
          "744:     checkPushedInfo(df3,",
          "745:       \"\"\"",
          "746:         |PushedAggregates: [SUM(SALARY)],",
          "747:         |PushedFilters: [],",
          "748:         |PushedGroupByExpressions:",
          "749:         |[CASE WHEN (SALARY > 8000.00) AND (SALARY < 10000.00) THEN SALARY ELSE 0.00 END],",
          "750:         |\"\"\".stripMargin.replaceAll(\"\\n\", \" \"))",
          "751:     checkAnswer(df3, Seq(Row(0, 44000), Row(9000, 9000)))",
          "753:     val df4 = sql(",
          "754:       \"\"\"",
          "755:         |SELECT DEPT, CASE WHEN SALARY > 8000 AND SALARY < 10000 THEN SALARY ELSE 0 END as key,",
          "756:         |  SUM(SALARY) FROM h2.test.employee GROUP BY DEPT, key\"\"\".stripMargin)",
          "757:     checkAggregateRemoved(df4)",
          "758:     checkPushedInfo(df4,",
          "759:       \"\"\"",
          "760:         |PushedAggregates: [SUM(SALARY)],",
          "761:         |PushedFilters: [],",
          "762:         |PushedGroupByExpressions:",
          "763:         |[DEPT, CASE WHEN (SALARY > 8000.00) AND (SALARY < 10000.00) THEN SALARY ELSE 0.00 END],",
          "764:         |\"\"\".stripMargin.replaceAll(\"\\n\", \" \"))",
          "765:     checkAnswer(df4, Seq(Row(1, 0, 10000), Row(1, 9000, 9000), Row(2, 0, 22000), Row(6, 0, 12000)))",
          "767:     val df5 = spark.read",
          "768:       .option(\"partitionColumn\", \"dept\")",
          "769:       .option(\"lowerBound\", \"0\")",
          "770:       .option(\"upperBound\", \"2\")",
          "771:       .option(\"numPartitions\", \"2\")",
          "772:       .table(\"h2.test.employee\")",
          "773:       .groupBy($\"DEPT\",",
          "774:         when(($\"SALARY\" > 8000).and($\"SALARY\" < 10000), $\"SALARY\").otherwise(0)",
          "775:           .as(\"key\"))",
          "776:       .agg(sum($\"SALARY\"))",
          "777:     checkAggregateRemoved(df5, false)",
          "778:     checkPushedInfo(df5,",
          "779:       \"\"\"",
          "780:         |PushedAggregates: [SUM(SALARY)],",
          "781:         |PushedFilters: [],",
          "782:         |PushedGroupByExpressions:",
          "783:         |[DEPT, CASE WHEN (SALARY > 8000.00) AND (SALARY < 10000.00) THEN SALARY ELSE 0.00 END],",
          "784:         |\"\"\".stripMargin.replaceAll(\"\\n\", \" \"))",
          "785:     checkAnswer(df5, Seq(Row(1, 0, 10000), Row(1, 9000, 9000), Row(2, 0, 22000), Row(6, 0, 12000)))",
          "792:       \"PushedFilters: [], PushedGroupByExpressions: [DEPT]\")",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "733:     checkFiltersRemoved(df)",
          "734:     checkAggregateRemoved(df)",
          "735:     checkPushedInfo(df, \"PushedAggregates: [MAX(SALARY), MIN(BONUS)], \" +",
          "737:     checkAnswer(df, Seq(Row(9000, 1200), Row(12000, 1200), Row(10000, 1300),",
          "738:       Row(10000, 1000), Row(12000, 1200)))",
          "739:   }",
          "",
          "[Removed Lines]",
          "736:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], PushedGroupByColumns: [DEPT, NAME]\")",
          "",
          "[Added Lines]",
          "802:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], PushedGroupByExpressions: [DEPT, NAME]\")",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "747:     assert(filters1.isEmpty)",
          "748:     checkAggregateRemoved(df1)",
          "749:     checkPushedInfo(df1, \"PushedAggregates: [MAX(SALARY)], \" +",
          "751:     checkAnswer(df1, Seq(Row(\"1#amy\", 10000), Row(\"1#cathy\", 9000), Row(\"2#alex\", 12000),",
          "752:       Row(\"2#david\", 10000), Row(\"6#jen\", 12000)))",
          "",
          "[Removed Lines]",
          "750:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], PushedGroupByColumns: [DEPT, NAME]\")",
          "",
          "[Added Lines]",
          "816:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], PushedGroupByExpressions: [DEPT, NAME]\")",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "759:     assert(filters2.isEmpty)",
          "760:     checkAggregateRemoved(df2)",
          "761:     checkPushedInfo(df2, \"PushedAggregates: [MAX(SALARY), MIN(BONUS)], \" +",
          "763:     checkAnswer(df2, Seq(Row(\"1#amy\", 11000), Row(\"1#cathy\", 10200), Row(\"2#alex\", 13200),",
          "764:       Row(\"2#david\", 11300), Row(\"6#jen\", 13200)))",
          "",
          "[Removed Lines]",
          "762:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], PushedGroupByColumns: [DEPT, NAME]\")",
          "",
          "[Added Lines]",
          "828:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], PushedGroupByExpressions: [DEPT, NAME]\")",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "779:     checkFiltersRemoved(df, false)",
          "780:     checkAggregateRemoved(df)",
          "781:     checkPushedInfo(df, \"PushedAggregates: [MAX(SALARY), MIN(BONUS)], \" +",
          "783:     checkAnswer(df, Seq(Row(12000, 1200), Row(12000, 1200)))",
          "784:   }",
          "",
          "[Removed Lines]",
          "782:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], PushedGroupByColumns: [DEPT]\")",
          "",
          "[Added Lines]",
          "848:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], PushedGroupByExpressions: [DEPT]\")",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "789:       .min(\"SALARY\").as(\"total\")",
          "790:     checkAggregateRemoved(df)",
          "791:     checkPushedInfo(df, \"PushedAggregates: [MIN(SALARY)], \" +",
          "793:     checkAnswer(df, Seq(Row(1, 9000), Row(2, 10000), Row(6, 12000)))",
          "794:   }",
          "",
          "[Removed Lines]",
          "792:       \"PushedFilters: [], PushedGroupByColumns: [DEPT]\")",
          "",
          "[Added Lines]",
          "858:       \"PushedFilters: [], PushedGroupByExpressions: [DEPT]\")",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "804:     checkFiltersRemoved(query, false)// filter over aggregate not pushed down",
          "805:     checkAggregateRemoved(query)",
          "806:     checkPushedInfo(query, \"PushedAggregates: [SUM(SALARY)], \" +",
          "808:     checkAnswer(query, Seq(Row(6, 12000), Row(1, 19000), Row(2, 22000)))",
          "809:   }",
          "",
          "[Removed Lines]",
          "807:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], PushedGroupByColumns: [DEPT]\")",
          "",
          "[Added Lines]",
          "873:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], PushedGroupByExpressions: [DEPT]\")",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "836:     checkFiltersRemoved(df)",
          "837:     checkAggregateRemoved(df)",
          "838:     checkPushedInfo(df, \"PushedAggregates: [VAR_POP(BONUS), VAR_SAMP(BONUS)], \" +",
          "840:     checkAnswer(df, Seq(Row(10000d, 20000d), Row(2500d, 5000d), Row(0d, null)))",
          "841:   }",
          "",
          "[Removed Lines]",
          "839:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], PushedGroupByColumns: [DEPT]\")",
          "",
          "[Added Lines]",
          "905:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], PushedGroupByExpressions: [DEPT]\")",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "846:     checkFiltersRemoved(df)",
          "847:     checkAggregateRemoved(df)",
          "848:     checkPushedInfo(df, \"PushedAggregates: [STDDEV_POP(BONUS), STDDEV_SAMP(BONUS)], \" +",
          "850:     checkAnswer(df, Seq(Row(100d, 141.4213562373095d), Row(50d, 70.71067811865476d), Row(0d, null)))",
          "851:   }",
          "",
          "[Removed Lines]",
          "849:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], PushedGroupByColumns: [DEPT]\")",
          "",
          "[Added Lines]",
          "915:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], PushedGroupByExpressions: [DEPT]\")",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "856:     checkFiltersRemoved(df)",
          "857:     checkAggregateRemoved(df)",
          "858:     checkPushedInfo(df, \"PushedAggregates: [COVAR_POP(BONUS, BONUS), COVAR_SAMP(BONUS, BONUS)], \" +",
          "860:     checkAnswer(df, Seq(Row(10000d, 20000d), Row(2500d, 5000d), Row(0d, null)))",
          "861:   }",
          "",
          "[Removed Lines]",
          "859:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], PushedGroupByColumns: [DEPT]\")",
          "",
          "[Added Lines]",
          "925:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], PushedGroupByExpressions: [DEPT]\")",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "866:     checkFiltersRemoved(df)",
          "867:     checkAggregateRemoved(df)",
          "868:     checkPushedInfo(df, \"PushedAggregates: [CORR(BONUS, BONUS)], \" +",
          "870:     checkAnswer(df, Seq(Row(1d), Row(1d), Row(null)))",
          "871:   }",
          "",
          "[Removed Lines]",
          "869:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], PushedGroupByColumns: [DEPT]\")",
          "",
          "[Added Lines]",
          "935:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], PushedGroupByExpressions: [DEPT]\")",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "878:     df2.queryExecution.optimizedPlan.collect {",
          "879:       case relation: DataSourceV2ScanRelation =>",
          "880:         val expectedPlanFragment =",
          "882:         checkKeywordsExistsInExplain(df2, expectedPlanFragment)",
          "883:         relation.scan match {",
          "884:           case v1: V1ScanWrapper =>",
          "",
          "[Removed Lines]",
          "881:           \"PushedAggregates: [SUM(SALARY)], PushedFilters: [], PushedGroupByColumns: []\"",
          "",
          "[Added Lines]",
          "947:           \"PushedAggregates: [SUM(SALARY)], PushedFilters: [], PushedGroupByExpressions: []\"",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "931:       \"PushedAggregates: [COUNT(CASE WHEN (SALARY > 8000.00) AND (SALARY < 10000.00)\" +",
          "932:       \" THEN SALARY ELSE 0.00 END), COUNT(CAS..., \" +",
          "933:       \"PushedFilters: [], \" +",
          "935:     checkAnswer(df, Seq(Row(1, 1, 1, 1, 1, 0d, 12000d, 0d, 12000d, 0d, 0d, 2, 0d),",
          "936:       Row(2, 2, 2, 2, 2, 10000d, 12000d, 10000d, 12000d, 0d, 0d, 3, 0d),",
          "937:       Row(2, 2, 2, 2, 2, 10000d, 9000d, 10000d, 10000d, 9000d, 0d, 2, 0d)))",
          "",
          "[Removed Lines]",
          "934:       \"PushedGroupByColumns: [DEPT], \")",
          "",
          "[Added Lines]",
          "1000:       \"PushedGroupByExpressions: [DEPT], \")",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "945:         val expectedPlanFragment = if (ansiMode) {",
          "946:           \"PushedAggregates: [SUM(2147483647 + DEPT)], \" +",
          "947:             \"PushedFilters: [], \" +",
          "949:         } else {",
          "950:           \"PushedFilters: []\"",
          "951:         }",
          "",
          "[Removed Lines]",
          "948:             \"PushedGroupByColumns: []\"",
          "",
          "[Added Lines]",
          "1014:             \"PushedGroupByExpressions: []\"",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "1094:       .filter($\"total\" > 1000)",
          "1095:     checkAggregateRemoved(df)",
          "1096:     checkPushedInfo(df,",
          "1098:     checkAnswer(df, Seq(Row(1, 19000.00), Row(2, 22000.00), Row(6, 12000.00)))",
          "1100:     val df2 = spark.table(\"h2.test.employee\")",
          "",
          "[Removed Lines]",
          "1097:       \"PushedAggregates: [SUM(SALARY)], PushedFilters: [], PushedGroupByColumns: [DEPT]\")",
          "",
          "[Added Lines]",
          "1163:       \"PushedAggregates: [SUM(SALARY)], PushedFilters: [], PushedGroupByExpressions: [DEPT]\")",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "1104:       .filter($\"total\" > 1000)",
          "1105:     checkAggregateRemoved(df2)",
          "1106:     checkPushedInfo(df2,",
          "1108:     checkAnswer(df2, Seq(Row(1, 19000.00), Row(2, 22000.00), Row(6, 12000.00)))",
          "1109:   }",
          "",
          "[Removed Lines]",
          "1107:       \"PushedAggregates: [SUM(SALARY)], PushedFilters: [], PushedGroupByColumns: [DEPT]\")",
          "",
          "[Added Lines]",
          "1173:       \"PushedAggregates: [SUM(SALARY)], PushedFilters: [], PushedGroupByExpressions: [DEPT]\")",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "1121:       .filter($\"total\" > 1000)",
          "1122:     checkAggregateRemoved(df, false)",
          "1123:     checkPushedInfo(df,",
          "1125:     checkAnswer(df, Seq(Row(\"alex\", 12000.00), Row(\"amy\", 10000.00),",
          "1126:       Row(\"cathy\", 9000.00), Row(\"david\", 10000.00), Row(\"jen\", 12000.00)))",
          "",
          "[Removed Lines]",
          "1124:       \"PushedAggregates: [SUM(SALARY)], PushedFilters: [], PushedGroupByColumns: [NAME]\")",
          "",
          "[Added Lines]",
          "1190:       \"PushedAggregates: [SUM(SALARY)], PushedFilters: [], PushedGroupByExpressions: [NAME]\")",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "1137:       .filter($\"total\" > 1000)",
          "1138:     checkAggregateRemoved(df2, false)",
          "1139:     checkPushedInfo(df2,",
          "1141:     checkAnswer(df2, Seq(Row(\"alex\", 12000.00), Row(\"amy\", 10000.00),",
          "1142:       Row(\"cathy\", 9000.00), Row(\"david\", 10000.00), Row(\"jen\", 12000.00)))",
          "1143:   }",
          "",
          "[Removed Lines]",
          "1140:       \"PushedAggregates: [SUM(SALARY)], PushedFilters: [], PushedGroupByColumns: [NAME]\")",
          "",
          "[Added Lines]",
          "1206:       \"PushedAggregates: [SUM(SALARY)], PushedFilters: [], PushedGroupByExpressions: [NAME]\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "39413db6f011b75504bb952423f3a5b579b36d97",
      "candidate_info": {
        "commit_hash": "39413db6f011b75504bb952423f3a5b579b36d97",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/39413db6f011b75504bb952423f3a5b579b36d97",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala"
        ],
        "message": "[SPARK-37753][FOLLOWUP][SQL] Fix unit tests sometimes failing\n\n### What changes were proposed in this pull request?\nThis unit test sometimes fails to run. for example, https://github.com/apache/spark/pull/35715#discussion_r892247619\n\nWhen the left side is completed first, and then the right side is completed, since it is known that there are many empty partitions on the left side, the broadcast on the right side is demoted.\n\nHowever, if the right side is completed first and the left side is still being executed, the right side does not know whether there are many empty partitions on the left side, so there is no demote, and then the right side is broadcast in the planning stage.\n\nThis PR does this\uff1a\nWhen it is found that the other side is QueryStage, if the QueryStage has not been materialized, demote it first. When the other side is completed, judge again whether demote is needed.\n\n### Why are the changes needed?\nFix small problems in logic\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nmanual testing\n\nCloses #36966 from mcdull-zhang/wait_other_side.\n\nAuthored-by: mcdull-zhang <work4dong@163.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 8c8801cf501ddbdeb4a4a869bc27c8a2331531fe)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import org.apache.logging.log4j.Level",
          "24: import org.scalatest.PrivateMethodTester",
          "26: import org.apache.spark.SparkException",
          "27: import org.apache.spark.scheduler.{SparkListener, SparkListenerEvent, SparkListenerJobStart}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: import org.scalatest.time.SpanSugar._",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "711:   test(\"SPARK-37753: Inhibit broadcast in left outer join when there are many empty\" +",
          "712:     \" partitions on outer/left side\") {",
          "725:       }",
          "726:     }",
          "727:   }",
          "",
          "[Removed Lines]",
          "713:     withSQLConf(",
          "714:       SQLConf.ADAPTIVE_EXECUTION_ENABLED.key -> \"true\",",
          "715:       SQLConf.NON_EMPTY_PARTITION_RATIO_FOR_BROADCAST_JOIN.key -> \"0.5\") {",
          "717:       withSQLConf(SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key -> \"200\") {",
          "718:         val (plan, adaptivePlan) = runAdaptiveAndVerifyResult(",
          "719:           \"SELECT * FROM (select * from testData where value = '1') td\" +",
          "720:             \" left outer join testData2 ON key = a\")",
          "721:         val smj = findTopLevelSortMergeJoin(plan)",
          "722:         assert(smj.size == 1)",
          "723:         val bhj = findTopLevelBroadcastHashJoin(adaptivePlan)",
          "724:         assert(bhj.isEmpty)",
          "",
          "[Added Lines]",
          "714:     eventually(timeout(15.seconds), interval(500.milliseconds)) {",
          "715:       withSQLConf(",
          "716:         SQLConf.ADAPTIVE_EXECUTION_ENABLED.key -> \"true\",",
          "717:         SQLConf.NON_EMPTY_PARTITION_RATIO_FOR_BROADCAST_JOIN.key -> \"0.5\") {",
          "719:         withSQLConf(SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key -> \"200\") {",
          "720:           val (plan, adaptivePlan) = runAdaptiveAndVerifyResult(",
          "721:             \"SELECT * FROM (select * from testData where value = '1') td\" +",
          "722:               \" left outer join testData2 ON key = a\")",
          "723:           val smj = findTopLevelSortMergeJoin(plan)",
          "724:           assert(smj.size == 1)",
          "725:           val bhj = findTopLevelBroadcastHashJoin(adaptivePlan)",
          "726:           assert(bhj.isEmpty)",
          "727:         }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1617eaded434069a38cd26cb1335d3fea2501bb0",
      "candidate_info": {
        "commit_hash": "1617eaded434069a38cd26cb1335d3fea2501bb0",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/1617eaded434069a38cd26cb1335d3fea2501bb0",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala"
        ],
        "message": "[SPARK-38391][SPARK-38768][SQL][FOLLOWUP] Add comments for `pushLimit` and `pushTopN` of `PushDownUtils`\n\n### What changes were proposed in this pull request?\n`pushLimit` and `pushTopN` of `PushDownUtils` returns tuple of boolean. It will be good to explain what the boolean value represents.\n\n### Why are the changes needed?\nMake DS V2 API more friendly to developers.\n\n### Does this PR introduce _any_ user-facing change?\n'No'.\nJust update comments.\n\n### How was this patch tested?\nN/A\n\nCloses #36092 from beliefer/SPARK-38391_SPARK-38768_followup.\n\nAuthored-by: Jiaan Geng <beliefer@163.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit c4397cb3dee4f9fa16297c224da15475b2d5a297)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/PushDownUtils.scala"
        ]
      }
    },
    {
      "candidate_hash": "fc6a6644ee12c32b8da60d29eb2e6f25fa91f30d",
      "candidate_info": {
        "commit_hash": "fc6a6644ee12c32b8da60d29eb2e6f25fa91f30d",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/fc6a6644ee12c32b8da60d29eb2e6f25fa91f30d",
        "files": [
          "dev/appveyor-install-dependencies.ps1",
          "docs/building-spark.md",
          "pom.xml"
        ],
        "message": "[SPARK-39599][BUILD] Upgrade maven to 3.8.6\n\n### What changes were proposed in this pull request?\nThis PR aims to upgrade Maven to 3.8.6 from 3.8.4.\n\n### Why are the changes needed?\nThe release notes and as follows:\n\n- https://maven.apache.org/docs/3.8.5/release-notes.html\n- https://maven.apache.org/docs/3.8.6/release-notes.html\n\nNote that the profile dependency bug should fixed by [MNG-7432] Resolver session contains non-MavenWorkspaceReader (#695)\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\n\n- Pass GitHub Actions\n- Manual test 1:\n\nrun `build/mvn -version` wll trigger download `apache-maven-3.8.6-bin.tar.gz`\n\n```\nexec: curl --silent --show-error -L https://www.apache.org/dyn/closer.lua/maven/maven-3/3.8.6/binaries/apache-maven-3.8.6-bin.tar.gz?action=download\n```\n\n- Manual test 2:\n\nrun `./dev/test-dependencies.sh --replace-manifest ` doesn't generate git diff, this behavior is consistent with maven 3.8.4,but there will git diff of `dev/deps/spark-deps-hadoop-2-hive-2.3`  when use maven 3.8.5.\n\nCloses #36978 from LuciferYang/mvn-386.\n\nAuthored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n(cherry picked from commit a9397484853843d84bd12048b5ca162acdba2549)\nSigned-off-by: Sean Owen <srowen@gmail.com>",
        "before_after_code_files": [
          "dev/appveyor-install-dependencies.ps1||dev/appveyor-install-dependencies.ps1"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/appveyor-install-dependencies.ps1||dev/appveyor-install-dependencies.ps1": [
          "File: dev/appveyor-install-dependencies.ps1 -> dev/appveyor-install-dependencies.ps1",
          "--- Hunk 1 ---",
          "[Context before]",
          "81: # ========================== Maven",
          "82: # Push-Location $tools",
          "83: #",
          "85: # Start-FileDownload \"https://archive.apache.org/dist/maven/maven-3/$mavenVer/binaries/apache-maven-$mavenVer-bin.zip\" \"maven.zip\"",
          "86: #",
          "87: # # extract",
          "",
          "[Removed Lines]",
          "84: # $mavenVer = \"3.8.4\"",
          "",
          "[Added Lines]",
          "84: # $mavenVer = \"3.8.6\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f4457e6310f6bd900d7634b279606436a6faf8fb",
      "candidate_info": {
        "commit_hash": "f4457e6310f6bd900d7634b279606436a6faf8fb",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/f4457e6310f6bd900d7634b279606436a6faf8fb",
        "files": [
          "python/pyspark/mllib/clustering.py",
          "python/pyspark/mllib/clustering.pyi"
        ],
        "message": "[SPARK-37402][PYTHON][MLLIB] Inline typehints for pyspark.mllib.clustering\n\n### What changes were proposed in this pull request?\n\nThis PR migrates type `pyspark.mllib.clustering` annotations from stub file to inline type hints.\n\n### Why are the changes needed?\n\nPart of ongoing migration of type hints.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting tests.\n\nCloses #35578 from zero323/SPARK-37402.\n\nAuthored-by: zero323 <mszymkiewicz@gmail.com>\nSigned-off-by: zero323 <mszymkiewicz@gmail.com>\n(cherry picked from commit e71cf3907b9ff2036dfe45bc8fe939f20cca741b)\nSigned-off-by: zero323 <mszymkiewicz@gmail.com>",
        "before_after_code_files": [
          "python/pyspark/mllib/clustering.py||python/pyspark/mllib/clustering.py",
          "python/pyspark/mllib/clustering.pyi||python/pyspark/mllib/clustering.pyi"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/mllib/clustering.py||python/pyspark/mllib/clustering.py": [
          "File: python/pyspark/mllib/clustering.py -> python/pyspark/mllib/clustering.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: import array as pyarray",
          "20: from math import exp, log",
          "21: from collections import namedtuple",
          "23: from numpy import array, random, tile",
          "25: from pyspark import SparkContext, since",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: from typing import Any, List, Optional, Tuple, TypeVar, Union, overload, TYPE_CHECKING",
          "24: import numpy as np",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "30: from pyspark.mllib.util import Saveable, Loader, inherit_doc, JavaLoader, JavaSaveable",
          "31: from pyspark.streaming import DStream",
          "33: __all__ = [",
          "34:     \"BisectingKMeansModel\",",
          "35:     \"BisectingKMeans\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "35: if TYPE_CHECKING:",
          "36:     from py4j.java_gateway import JavaObject",
          "37:     from pyspark.mllib._typing import VectorLike",
          "39: T = TypeVar(\"T\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "67:     0.0",
          "68:     \"\"\"",
          "71:         super(BisectingKMeansModel, self).__init__(java_model)",
          "72:         self.centers = [c.toArray() for c in self.call(\"clusterCenters\")]",
          "75:     @since(\"2.0.0\")",
          "77:         \"\"\"Get the cluster centers, represented as a list of NumPy",
          "78:         arrays.\"\"\"",
          "79:         return self.centers",
          "82:     @since(\"2.0.0\")",
          "84:         \"\"\"Get the number of clusters\"\"\"",
          "85:         return self.call(\"k\")",
          "88:         \"\"\"",
          "89:         Find the cluster that each of the points belongs to in this",
          "90:         model.",
          "",
          "[Removed Lines]",
          "70:     def __init__(self, java_model):",
          "74:     @property",
          "76:     def clusterCenters(self):",
          "81:     @property",
          "83:     def k(self):",
          "87:     def predict(self, x):",
          "",
          "[Added Lines]",
          "78:     def __init__(self, java_model: \"JavaObject\"):",
          "82:     @property  # type: ignore[misc]",
          "84:     def clusterCenters(self) -> List[np.ndarray]:",
          "89:     @property  # type: ignore[misc]",
          "91:     def k(self) -> int:",
          "95:     @overload",
          "96:     def predict(self, x: \"VectorLike\") -> int:",
          "97:         ...",
          "99:     @overload",
          "100:     def predict(self, x: RDD[\"VectorLike\"]) -> RDD[int]:",
          "101:         ...",
          "103:     def predict(self, x: Union[\"VectorLike\", RDD[\"VectorLike\"]]) -> Union[int, RDD[int]]:",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "111:         x = _convert_to_vector(x)",
          "112:         return self.call(\"predict\", x)",
          "115:         \"\"\"",
          "116:         Return the Bisecting K-means cost (sum of squared distances of",
          "117:         points to their nearest center) for this model on the given",
          "",
          "[Removed Lines]",
          "114:     def computeCost(self, x):",
          "",
          "[Added Lines]",
          "130:     def computeCost(self, x: Union[\"VectorLike\", RDD[\"VectorLike\"]]) -> float:",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "159:     \"\"\"",
          "161:     @classmethod",
          "163:         \"\"\"",
          "164:         Runs the bisecting k-means algorithm return the model.",
          "",
          "[Removed Lines]",
          "162:     def train(self, rdd, k=4, maxIterations=20, minDivisibleClusterSize=1.0, seed=-1888008604):",
          "",
          "[Added Lines]",
          "178:     def train(",
          "179:         self,",
          "180:         rdd: RDD[\"VectorLike\"],",
          "181:         k: int = 4,",
          "182:         maxIterations: int = 20,",
          "183:         minDivisibleClusterSize: float = 1.0,",
          "184:         seed: int = -1888008604,",
          "185:     ) -> BisectingKMeansModel:",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "199: @inherit_doc",
          "202:     \"\"\"A clustering model derived from the k-means method.",
          "",
          "[Removed Lines]",
          "200: class KMeansModel(Saveable, Loader):",
          "",
          "[Added Lines]",
          "223: class KMeansModel(Saveable, Loader[\"KMeansModel\"]):",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "255:     [array([-1000., -1000.]), array([ 5.,  5.]), array([ 1000.,  1000.])]",
          "256:     \"\"\"",
          "259:         self.centers = centers",
          "262:     @since(\"1.0.0\")",
          "264:         \"\"\"Get the cluster centers, represented as a list of NumPy arrays.\"\"\"",
          "265:         return self.centers",
          "268:     @since(\"1.4.0\")",
          "270:         \"\"\"Total number of clusters.\"\"\"",
          "271:         return len(self.centers)",
          "274:         \"\"\"",
          "275:         Find the cluster that each of the points belongs to in this",
          "276:         model.",
          "",
          "[Removed Lines]",
          "258:     def __init__(self, centers):",
          "261:     @property",
          "263:     def clusterCenters(self):",
          "267:     @property",
          "269:     def k(self):",
          "273:     def predict(self, x):",
          "",
          "[Added Lines]",
          "281:     def __init__(self, centers: List[\"VectorLike\"]):",
          "284:     @property  # type: ignore[misc]",
          "286:     def clusterCenters(self) -> List[\"VectorLike\"]:",
          "290:     @property  # type: ignore[misc]",
          "292:     def k(self) -> int:",
          "296:     @overload",
          "297:     def predict(self, x: \"VectorLike\") -> int:",
          "298:         ...",
          "300:     @overload",
          "301:     def predict(self, x: RDD[\"VectorLike\"]) -> RDD[int]:",
          "302:         ...",
          "304:     def predict(self, x: Union[\"VectorLike\", RDD[\"VectorLike\"]]) -> Union[int, RDD[int]]:",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "298:         x = _convert_to_vector(x)",
          "299:         for i in range(len(self.centers)):",
          "301:             if distance < best_distance:",
          "302:                 best = i",
          "303:                 best_distance = distance",
          "304:         return best",
          "307:         \"\"\"",
          "308:         Return the K-means cost (sum of squared distances of points to",
          "309:         their nearest center) for this model on the given",
          "",
          "[Removed Lines]",
          "300:             distance = x.squared_distance(self.centers[i])",
          "306:     def computeCost(self, rdd):",
          "",
          "[Added Lines]",
          "331:             distance = x.squared_distance(self.centers[i])  # type: ignore[attr-defined]",
          "337:     def computeCost(self, rdd: RDD[\"VectorLike\"]) -> float:",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "324:         return cost",
          "326:     @since(\"1.4.0\")",
          "328:         \"\"\"",
          "329:         Save this model to the given path.",
          "330:         \"\"\"",
          "331:         java_centers = _py2java(sc, [_convert_to_vector(c) for c in self.centers])",
          "332:         java_model = sc._jvm.org.apache.spark.mllib.clustering.KMeansModel(java_centers)",
          "333:         java_model.save(sc._jsc.sc(), path)",
          "335:     @classmethod",
          "336:     @since(\"1.4.0\")",
          "338:         \"\"\"",
          "339:         Load a model from the given path.",
          "340:         \"\"\"",
          "341:         java_model = sc._jvm.org.apache.spark.mllib.clustering.KMeansModel.load(sc._jsc.sc(), path)",
          "342:         return KMeansModel(_java2py(sc, java_model.clusterCenters()))",
          "",
          "[Removed Lines]",
          "327:     def save(self, sc, path):",
          "337:     def load(cls, sc, path):",
          "",
          "[Added Lines]",
          "358:     def save(self, sc: SparkContext, path: str) -> None:",
          "362:         assert sc._jvm is not None",
          "370:     def load(cls, sc: SparkContext, path: str) -> \"KMeansModel\":",
          "374:         assert sc._jvm is not None",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "352:     @classmethod",
          "353:     def train(",
          "354:         cls,",
          "365:         \"\"\"",
          "366:         Train a k-means clustering model.",
          "",
          "[Removed Lines]",
          "355:         rdd,",
          "356:         k,",
          "357:         maxIterations=100,",
          "358:         initializationMode=\"k-means||\",",
          "359:         seed=None,",
          "360:         initializationSteps=2,",
          "361:         epsilon=1e-4,",
          "362:         initialModel=None,",
          "363:         distanceMeasure=\"euclidean\",",
          "364:     ):",
          "",
          "[Added Lines]",
          "390:         rdd: RDD[\"VectorLike\"],",
          "391:         k: int,",
          "392:         maxIterations: int = 100,",
          "393:         initializationMode: str = \"k-means||\",",
          "394:         seed: Optional[int] = None,",
          "395:         initializationSteps: int = 2,",
          "396:         epsilon: float = 1e-4,",
          "397:         initialModel: Optional[KMeansModel] = None,",
          "398:         distanceMeasure: str = \"euclidean\",",
          "399:     ) -> \"KMeansModel\":",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "430: @inherit_doc",
          "433:     \"\"\"",
          "434:     A clustering model derived from the Gaussian Mixture Model method.",
          "",
          "[Removed Lines]",
          "431: class GaussianMixtureModel(JavaModelWrapper, JavaSaveable, JavaLoader):",
          "",
          "[Added Lines]",
          "466: class GaussianMixtureModel(JavaModelWrapper, JavaSaveable, JavaLoader[\"GaussianMixtureModel\"]):",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "497:     True",
          "498:     \"\"\"",
          "501:     @since(\"1.4.0\")",
          "503:         \"\"\"",
          "504:         Weights for each Gaussian distribution in the mixture, where weights[i] is",
          "505:         the weight for Gaussian i, and weights.sum == 1.",
          "506:         \"\"\"",
          "507:         return array(self.call(\"weights\"))",
          "510:     @since(\"1.4.0\")",
          "512:         \"\"\"",
          "513:         Array of MultivariateGaussian where gaussians[i] represents",
          "514:         the Multivariate Gaussian (Normal) Distribution for Gaussian i.",
          "",
          "[Removed Lines]",
          "500:     @property",
          "502:     def weights(self):",
          "509:     @property",
          "511:     def gaussians(self):",
          "",
          "[Added Lines]",
          "535:     @property  # type: ignore[misc]",
          "537:     def weights(self) -> np.ndarray:",
          "544:     @property  # type: ignore[misc]",
          "546:     def gaussians(self) -> List[MultivariateGaussian]:",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "517:             MultivariateGaussian(gaussian[0], gaussian[1]) for gaussian in self.call(\"gaussians\")",
          "518:         ]",
          "521:     @since(\"1.4.0\")",
          "523:         \"\"\"Number of gaussians in mixture.\"\"\"",
          "524:         return len(self.weights)",
          "527:         \"\"\"",
          "528:         Find the cluster to which the point 'x' or each point in RDD 'x'",
          "529:         has maximum membership in this model.",
          "",
          "[Removed Lines]",
          "520:     @property",
          "522:     def k(self):",
          "526:     def predict(self, x):",
          "",
          "[Added Lines]",
          "555:     @property  # type: ignore[misc]",
          "557:     def k(self) -> int:",
          "561:     @overload",
          "562:     def predict(self, x: \"VectorLike\") -> np.int64:",
          "563:         ...",
          "565:     @overload",
          "566:     def predict(self, x: RDD[\"VectorLike\"]) -> RDD[int]:",
          "567:         ...",
          "569:     def predict(self, x: Union[\"VectorLike\", RDD[\"VectorLike\"]]) -> Union[np.int64, RDD[int]]:",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "548:             z = self.predictSoft(x)",
          "549:             return z.argmax()",
          "552:         \"\"\"",
          "553:         Find the membership of point 'x' or each point in RDD 'x' to all mixture components.",
          "",
          "[Removed Lines]",
          "551:     def predictSoft(self, x):",
          "",
          "[Added Lines]",
          "594:     @overload",
          "595:     def predictSoft(self, x: \"VectorLike\") -> np.ndarray:",
          "596:         ...",
          "598:     @overload",
          "599:     def predictSoft(self, x: RDD[\"VectorLike\"]) -> RDD[pyarray.array]:",
          "600:         ...",
          "602:     def predictSoft(",
          "603:         self, x: Union[\"VectorLike\", RDD[\"VectorLike\"]]",
          "604:     ) -> Union[np.ndarray, RDD[pyarray.array]]:",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "579:             return self.call(\"predictSoft\", _convert_to_vector(x)).toArray()",
          "581:     @classmethod",
          "583:         \"\"\"Load the GaussianMixtureModel from disk.",
          "585:         .. versionadded:: 1.5.0",
          "",
          "[Removed Lines]",
          "582:     def load(cls, sc, path):",
          "",
          "[Added Lines]",
          "635:     def load(cls, sc: SparkContext, path: str) -> \"GaussianMixtureModel\":",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "590:         path : str",
          "591:             Path to where the model is stored.",
          "592:         \"\"\"",
          "593:         model = cls._load_java(sc, path)",
          "594:         wrapper = sc._jvm.org.apache.spark.mllib.api.python.GaussianMixtureModelWrapper(model)",
          "595:         return cls(wrapper)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "646:         assert sc._jvm is not None",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "603:     \"\"\"",
          "605:     @classmethod",
          "607:         \"\"\"",
          "608:         Train a Gaussian Mixture clustering model.",
          "",
          "[Removed Lines]",
          "606:     def train(cls, rdd, k, convergenceTol=1e-3, maxIterations=100, seed=None, initialModel=None):",
          "",
          "[Added Lines]",
          "661:     def train(",
          "662:         cls,",
          "663:         rdd: RDD[\"VectorLike\"],",
          "664:         k: int,",
          "665:         convergenceTol: float = 1e-3,",
          "666:         maxIterations: int = 100,",
          "667:         seed: Optional[int] = None,",
          "668:         initialModel: Optional[GaussianMixtureModel] = None,",
          "669:     ) -> GaussianMixtureModel:",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "658:         return GaussianMixtureModel(java_model)",
          "663:     \"\"\"",
          "664:     Model produced by :py:class:`PowerIterationClustering`.",
          "",
          "[Removed Lines]",
          "661: class PowerIterationClusteringModel(JavaModelWrapper, JavaSaveable, JavaLoader):",
          "",
          "[Added Lines]",
          "724: class PowerIterationClusteringModel(",
          "725:     JavaModelWrapper, JavaSaveable, JavaLoader[\"PowerIterationClusteringModel\"]",
          "726: ):",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "711:     ...     pass",
          "712:     \"\"\"",
          "715:     @since(\"1.5.0\")",
          "717:         \"\"\"",
          "718:         Returns the number of clusters.",
          "719:         \"\"\"",
          "720:         return self.call(\"k\")",
          "722:     @since(\"1.5.0\")",
          "724:         \"\"\"",
          "725:         Returns the cluster assignments of this model.",
          "726:         \"\"\"",
          "",
          "[Removed Lines]",
          "714:     @property",
          "716:     def k(self):",
          "723:     def assignments(self):",
          "",
          "[Added Lines]",
          "779:     @property  # type: ignore[misc]",
          "781:     def k(self) -> int:",
          "788:     def assignments(self) -> RDD[\"PowerIterationClustering.Assignment\"]:",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "729:     @classmethod",
          "730:     @since(\"1.5.0\")",
          "732:         \"\"\"",
          "733:         Load a model from the given path.",
          "734:         \"\"\"",
          "735:         model = cls._load_java(sc, path)",
          "736:         wrapper = sc._jvm.org.apache.spark.mllib.api.python.PowerIterationClusteringModelWrapper(",
          "737:             model",
          "",
          "[Removed Lines]",
          "731:     def load(cls, sc, path):",
          "",
          "[Added Lines]",
          "796:     def load(cls, sc: SparkContext, path: str) -> \"PowerIterationClusteringModel\":",
          "800:         assert sc._jvm is not None",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "757:     \"\"\"",
          "759:     @classmethod",
          "761:         r\"\"\"",
          "762:         Train PowerIterationClusteringModel",
          "",
          "[Removed Lines]",
          "760:     def train(cls, rdd, k, maxIterations=100, initMode=\"random\"):",
          "",
          "[Added Lines]",
          "827:     def train(",
          "828:         cls,",
          "829:         rdd: RDD[Tuple[int, int, float]],",
          "830:         k: int,",
          "831:         maxIterations: int = 100,",
          "832:         initMode: str = \"random\",",
          "833:     ) -> PowerIterationClusteringModel:",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "867:     1",
          "868:     \"\"\"",
          "871:         super(StreamingKMeansModel, self).__init__(centers=clusterCenters)",
          "875:     @since(\"1.5.0\")",
          "877:         \"\"\"Return the cluster weights.\"\"\"",
          "878:         return self._clusterWeights",
          "880:     @since(\"1.5.0\")",
          "882:         \"\"\"Update the centroids, according to data",
          "884:         .. versionadded:: 1.5.0",
          "",
          "[Removed Lines]",
          "870:     def __init__(self, clusterCenters, clusterWeights):",
          "872:         self._clusterWeights = list(clusterWeights)",
          "874:     @property",
          "876:     def clusterWeights(self):",
          "881:     def update(self, data, decayFactor, timeUnit):",
          "",
          "[Added Lines]",
          "943:     def __init__(self, clusterCenters: List[\"VectorLike\"], clusterWeights: \"VectorLike\"):",
          "945:         self._clusterWeights = list(clusterWeights)  # type: ignore[arg-type]",
          "947:     @property  # type: ignore[misc]",
          "949:     def clusterWeights(self) -> List[np.float64]:",
          "954:     def update(",
          "955:         self, data: RDD[\"VectorLike\"], decayFactor: float, timeUnit: str",
          "956:     ) -> \"StreamingKMeansModel\":",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "909:             decayFactor,",
          "910:             timeUnit,",
          "911:         )",
          "913:         self._clusterWeights = list(updatedModel[1])",
          "914:         return self",
          "",
          "[Removed Lines]",
          "912:         self.centers = array(updatedModel[0])",
          "",
          "[Added Lines]",
          "987:         self.centers = array(updatedModel[0])  # type: ignore[assignment]",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "938:         (default: \"batches\")",
          "939:     \"\"\"",
          "942:         self._k = k",
          "943:         self._decayFactor = decayFactor",
          "944:         if timeUnit not in [\"batches\", \"points\"]:",
          "945:             raise ValueError(\"timeUnit should be 'batches' or 'points', got %s.\" % timeUnit)",
          "946:         self._timeUnit = timeUnit",
          "949:     @since(\"1.5.0\")",
          "951:         \"\"\"Return the latest model\"\"\"",
          "952:         return self._model",
          "955:         if self._model is None:",
          "956:             raise ValueError(",
          "957:                 \"Initial centers should be set either by setInitialCenters \" \"or setRandomCenters.\"",
          "",
          "[Removed Lines]",
          "941:     def __init__(self, k=2, decayFactor=1.0, timeUnit=\"batches\"):",
          "947:         self._model = None",
          "950:     def latestModel(self):",
          "954:     def _validate(self, dstream):",
          "",
          "[Added Lines]",
          "1016:     def __init__(self, k: int = 2, decayFactor: float = 1.0, timeUnit: str = \"batches\"):",
          "1022:         self._model: Optional[StreamingKMeansModel] = None",
          "1025:     def latestModel(self) -> Optional[StreamingKMeansModel]:",
          "1029:     def _validate(self, dstream: Any) -> None:",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "962:             )",
          "964:     @since(\"1.5.0\")",
          "966:         \"\"\"Set number of clusters.\"\"\"",
          "967:         self._k = k",
          "968:         return self",
          "970:     @since(\"1.5.0\")",
          "972:         \"\"\"Set decay factor.\"\"\"",
          "973:         self._decayFactor = decayFactor",
          "974:         return self",
          "976:     @since(\"1.5.0\")",
          "978:         \"\"\"",
          "979:         Set number of batches after which the centroids of that",
          "980:         particular batch has half the weightage.",
          "",
          "[Removed Lines]",
          "965:     def setK(self, k):",
          "971:     def setDecayFactor(self, decayFactor):",
          "977:     def setHalfLife(self, halfLife, timeUnit):",
          "",
          "[Added Lines]",
          "1040:     def setK(self, k: int) -> \"StreamingKMeans\":",
          "1046:     def setDecayFactor(self, decayFactor: float) -> \"StreamingKMeans\":",
          "1052:     def setHalfLife(self, halfLife: float, timeUnit: str) -> \"StreamingKMeans\":",
          "",
          "---------------",
          "--- Hunk 26 ---",
          "[Context before]",
          "984:         return self",
          "986:     @since(\"1.5.0\")",
          "988:         \"\"\"",
          "989:         Set initial centers. Should be set before calling trainOn.",
          "990:         \"\"\"",
          "",
          "[Removed Lines]",
          "987:     def setInitialCenters(self, centers, weights):",
          "",
          "[Added Lines]",
          "1062:     def setInitialCenters(",
          "1063:         self, centers: List[\"VectorLike\"], weights: List[float]",
          "1064:     ) -> \"StreamingKMeans\":",
          "",
          "---------------",
          "--- Hunk 27 ---",
          "[Context before]",
          "992:         return self",
          "994:     @since(\"1.5.0\")",
          "996:         \"\"\"",
          "997:         Set the initial centers to be random samples from",
          "998:         a gaussian population with constant weights.",
          "",
          "[Removed Lines]",
          "995:     def setRandomCenters(self, dim, weight, seed):",
          "",
          "[Added Lines]",
          "1072:     def setRandomCenters(self, dim: int, weight: float, seed: int) -> \"StreamingKMeans\":",
          "",
          "---------------",
          "--- Hunk 28 ---",
          "[Context before]",
          "1000:         rng = random.RandomState(seed)",
          "1001:         clusterCenters = rng.randn(self._k, dim)",
          "1002:         clusterWeights = tile(weight, self._k)",
          "1004:         return self",
          "1006:     @since(\"1.5.0\")",
          "1008:         \"\"\"Train the model on the incoming dstream.\"\"\"",
          "1009:         self._validate(dstream)",
          "1014:         dstream.foreachRDD(update)",
          "1016:     @since(\"1.5.0\")",
          "1018:         \"\"\"",
          "1019:         Make predictions on a dstream.",
          "1020:         Returns a transformed dstream object",
          "1021:         \"\"\"",
          "1022:         self._validate(dstream)",
          "1025:     @since(\"1.5.0\")",
          "1027:         \"\"\"",
          "1028:         Make predictions on a keyed dstream.",
          "1029:         Returns a transformed dstream object.",
          "1030:         \"\"\"",
          "1031:         self._validate(dstream)",
          "1037:     \"\"\"A clustering model derived from the LDA method.",
          "",
          "[Removed Lines]",
          "1003:         self._model = StreamingKMeansModel(clusterCenters, clusterWeights)",
          "1007:     def trainOn(self, dstream):",
          "1011:         def update(rdd):",
          "1012:             self._model.update(rdd, self._decayFactor, self._timeUnit)",
          "1017:     def predictOn(self, dstream):",
          "1023:         return dstream.map(lambda x: self._model.predict(x))",
          "1026:     def predictOnValues(self, dstream):",
          "1032:         return dstream.mapValues(lambda x: self._model.predict(x))",
          "1035: class LDAModel(JavaModelWrapper, JavaSaveable, Loader):",
          "",
          "[Added Lines]",
          "1080:         self._model = StreamingKMeansModel(clusterCenters, clusterWeights)  # type: ignore[arg-type]",
          "1084:     def trainOn(self, dstream: \"DStream[VectorLike]\") -> None:",
          "1088:         def update(rdd: RDD[\"VectorLike\"]) -> None:",
          "1089:             self._model.update(rdd, self._decayFactor, self._timeUnit)  # type: ignore[union-attr]",
          "1094:     def predictOn(self, dstream: \"DStream[VectorLike]\") -> \"DStream[int]\":",
          "1100:         return dstream.map(lambda x: self._model.predict(x))  # type: ignore[union-attr]",
          "1103:     def predictOnValues(self, dstream: \"DStream[Tuple[T, VectorLike]]\") -> \"DStream[Tuple[T, int]]\":",
          "1109:         return dstream.mapValues(lambda x: self._model.predict(x))  # type: ignore[union-attr]",
          "1112: class LDAModel(JavaModelWrapper, JavaSaveable, Loader[\"LDAModel\"]):",
          "",
          "---------------",
          "--- Hunk 29 ---",
          "[Context before]",
          "1089:     \"\"\"",
          "1091:     @since(\"1.5.0\")",
          "1093:         \"\"\"Inferred topics, where each topic is represented by a distribution over terms.\"\"\"",
          "1094:         return self.call(\"topicsMatrix\").toArray()",
          "1096:     @since(\"1.5.0\")",
          "1098:         \"\"\"Vocabulary size (number of terms or terms in the vocabulary)\"\"\"",
          "1099:         return self.call(\"vocabSize\")",
          "1102:         \"\"\"Return the topics described by weighted terms.",
          "1104:         .. versionadded:: 1.6.0",
          "",
          "[Removed Lines]",
          "1092:     def topicsMatrix(self):",
          "1097:     def vocabSize(self):",
          "1101:     def describeTopics(self, maxTermsPerTopic=None):",
          "",
          "[Added Lines]",
          "1169:     def topicsMatrix(self) -> np.ndarray:",
          "1174:     def vocabSize(self) -> int:",
          "1178:     def describeTopics(",
          "1179:         self, maxTermsPerTopic: Optional[int] = None",
          "1180:     ) -> List[Tuple[List[int], List[float]]]:",
          "",
          "---------------",
          "--- Hunk 30 ---",
          "[Context before]",
          "1124:         return topics",
          "1126:     @classmethod",
          "1128:         \"\"\"Load the LDAModel from disk.",
          "1130:         .. versionadded:: 1.5.0",
          "",
          "[Removed Lines]",
          "1127:     def load(cls, sc, path):",
          "",
          "[Added Lines]",
          "1206:     def load(cls, sc: SparkContext, path: str) -> \"LDAModel\":",
          "",
          "---------------",
          "--- Hunk 31 ---",
          "[Context before]",
          "1153:     @classmethod",
          "1154:     def train(",
          "1155:         cls,",
          "1165:         \"\"\"Train a LDA model.",
          "1167:         .. versionadded:: 1.5.0",
          "",
          "[Removed Lines]",
          "1156:         rdd,",
          "1157:         k=10,",
          "1158:         maxIterations=20,",
          "1159:         docConcentration=-1.0,",
          "1160:         topicConcentration=-1.0,",
          "1161:         seed=None,",
          "1162:         checkpointInterval=10,",
          "1163:         optimizer=\"em\",",
          "1164:     ):",
          "",
          "[Added Lines]",
          "1235:         rdd: RDD[Tuple[int, \"VectorLike\"]],",
          "1236:         k: int = 10,",
          "1237:         maxIterations: int = 20,",
          "1238:         docConcentration: float = -1.0,",
          "1239:         topicConcentration: float = -1.0,",
          "1240:         seed: Optional[int] = None,",
          "1241:         checkpointInterval: int = 10,",
          "1242:         optimizer: str = \"em\",",
          "1243:     ) -> LDAModel:",
          "",
          "---------------",
          "--- Hunk 32 ---",
          "[Context before]",
          "1215:         return LDAModel(model)",
          "1219:     import doctest",
          "1220:     import numpy",
          "1221:     import pyspark.mllib.clustering",
          "",
          "[Removed Lines]",
          "1218: def _test():",
          "",
          "[Added Lines]",
          "1297: def _test() -> None:",
          "",
          "---------------"
        ],
        "python/pyspark/mllib/clustering.pyi||python/pyspark/mllib/clustering.pyi": [
          "File: python/pyspark/mllib/clustering.pyi -> python/pyspark/mllib/clustering.pyi",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    }
  ]
}