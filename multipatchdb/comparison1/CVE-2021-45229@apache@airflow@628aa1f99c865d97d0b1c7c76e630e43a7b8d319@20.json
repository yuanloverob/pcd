{
  "cve_id": "CVE-2021-45229",
  "cve_desc": "It was discovered that the \"Trigger DAG with config\" screen was susceptible to XSS attacks via the `origin` query argument. This issue affects Apache Airflow versions 2.2.3 and below.",
  "repo": "apache/airflow",
  "patch_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
  "patch_info": {
    "commit_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "files": [
      "airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py"
    ],
    "message": "Simplify trigger cancel button (#21591)\n\nCo-authored-by: Jed Cunningham <jedcunningham@apache.org>\n(cherry picked from commit 65297673a318660fba76797e50d0c06804dfcafc)",
    "before_after_code_files": [
      "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
    ]
  },
  "patch_diff": {
    "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html": [
      "File: airflow/www/templates/airflow/trigger.html -> airflow/www/templates/airflow/trigger.html",
      "--- Hunk 1 ---",
      "[Context before]",
      "63:       </label>",
      "64:     </div>",
      "65:     <button type=\"submit\" class=\"btn btn-primary\">Trigger</button>",
      "67:   </form>",
      "68: {% endblock %}",
      "",
      "[Removed Lines]",
      "66:     <button type=\"button\" class=\"btn\" onclick=\"location.href = '{{ origin }}'; return false\">Cancel</button>",
      "",
      "[Added Lines]",
      "66:     <a class=\"btn\" href=\"{{ origin }}\">Cancel</a>",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
      "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "133:         (\"javascript:alert(1)\", \"/home\"),",
      "134:         (\"http://google.com\", \"/home\"),",
      "135:         (\"36539'%3balert(1)%2f%2f166\", \"/home\"),",
      "136:         (",
      "137:             \"%2Ftree%3Fdag_id%3Dexample_bash_operator';alert(33)//\",",
      "138:             \"/home\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "136:         (",
      "137:             '\"><script>alert(99)</script><a href=\"',",
      "138:             \"&#34;&gt;&lt;script&gt;alert(99)&lt;/script&gt;&lt;a href=&#34;\",",
      "139:         ),",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "145:     test_dag_id = \"example_bash_operator\"",
      "147:     resp = admin_client.get(f'trigger?dag_id={test_dag_id}&origin={test_origin}')",
      "156: @pytest.mark.parametrize(",
      "",
      "[Removed Lines]",
      "148:     check_content_in_response(",
      "149:         '<button type=\"button\" class=\"btn\" onclick=\"location.href = \\'{}\\'; return false\">'.format(",
      "150:             expected_origin",
      "151:         ),",
      "152:         resp,",
      "153:     )",
      "",
      "[Added Lines]",
      "152:     check_content_in_response(f'<a class=\"btn\" href=\"{expected_origin}\">Cancel</a>', resp)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "d4660a8a071390137817d3e2e3e8a2ba812e03d0",
      "candidate_info": {
        "commit_hash": "d4660a8a071390137817d3e2e3e8a2ba812e03d0",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/d4660a8a071390137817d3e2e3e8a2ba812e03d0",
        "files": [
          "setup.py"
        ],
        "message": "Update Celery requirements\n\nCelery 5.2.3 is Python 3.6 only and Airflow 2.2.* line is still\nPython 3.6 compatible, so we have to add a condition for Python\n3.6 and Celery.",
        "before_after_code_files": [
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "224:     'cassandra-driver>=3.13.0,<4',",
          "225: ]",
          "226: celery = [",
          "228:     'flower~=1.0.0',",
          "229: ]",
          "230: cgroups = [",
          "",
          "[Removed Lines]",
          "227:     'celery>=5.2.3',",
          "",
          "[Added Lines]",
          "227:     'celery~=5.1,>=5.1.2;python_version<\"3.7\"',",
          "228:     'celery>=5.2.3;python_version>=\"3.7\"',",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "55a4abbe1631f34325327d1494f1faaaa0c7e359",
      "candidate_info": {
        "commit_hash": "55a4abbe1631f34325327d1494f1faaaa0c7e359",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/55a4abbe1631f34325327d1494f1faaaa0c7e359",
        "files": [
          "airflow/api/common/experimental/mark_tasks.py"
        ],
        "message": "bugfix: deferred tasks does not cancel when DAG is marked fail (#20649)\n\n(cherry picked from commit 64c0bd50155dfdb84671ac35d645b812fafa78a1)",
        "before_after_code_files": [
          "airflow/api/common/experimental/mark_tasks.py||airflow/api/common/experimental/mark_tasks.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api/common/experimental/mark_tasks.py||airflow/api/common/experimental/mark_tasks.py": [
          "File: airflow/api/common/experimental/mark_tasks.py -> airflow/api/common/experimental/mark_tasks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: \"\"\"Marks tasks APIs.\"\"\"",
          "24: from sqlalchemy.orm import contains_eager",
          "26: from airflow.models.baseoperator import BaseOperator",
          "27: from airflow.models.dagrun import DagRun",
          "28: from airflow.models.taskinstance import TaskInstance",
          "29: from airflow.operators.subdag import SubDagOperator",
          "30: from airflow.utils import timezone",
          "33: from airflow.utils.types import DagRunType",
          "37:     \"\"\"",
          "38:     Infers from the dates which dag runs need to be created and does so.",
          "",
          "[Removed Lines]",
          "20: import datetime",
          "21: from typing import Iterable",
          "23: from sqlalchemy import or_",
          "31: from airflow.utils.session import provide_session",
          "32: from airflow.utils.state import State",
          "36: def _create_dagruns(dag, execution_dates, state, run_type):",
          "",
          "[Added Lines]",
          "20: from datetime import datetime",
          "21: from typing import Generator, Iterable, List, Optional",
          "24: from sqlalchemy.orm.session import Session as SASession",
          "25: from sqlalchemy.sql.expression import or_",
          "27: from airflow import DAG",
          "33: from airflow.utils.session import NEW_SESSION, provide_session",
          "34: from airflow.utils.state import State, TaskInstanceState",
          "38: def _create_dagruns(",
          "39:     dag: DAG, execution_dates: List[datetime], state: TaskInstanceState, run_type: DagRunType",
          "40: ) -> List[DagRun]:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "63: @provide_session",
          "64: def set_state(",
          "65:     tasks: Iterable[BaseOperator],",
          "67:     upstream: bool = False,",
          "68:     downstream: bool = False,",
          "69:     future: bool = False,",
          "70:     past: bool = False,",
          "71:     state: str = State.SUCCESS,",
          "72:     commit: bool = False,",
          "75:     \"\"\"",
          "76:     Set the state of a task instance and if needed its relatives. Can set state",
          "77:     for future tasks (calculated from execution_date) and retroactively",
          "",
          "[Removed Lines]",
          "66:     execution_date: datetime.datetime,",
          "73:     session=None,",
          "74: ):",
          "",
          "[Added Lines]",
          "70:     execution_date: datetime,",
          "77:     session: SASession = NEW_SESSION,",
          "78: ) -> List[TaskInstance]:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "134:     return tis_altered",
          "138:     \"\"\"Get *all* tasks of the sub dags\"\"\"",
          "139:     qry_sub_dag = (",
          "140:         session.query(TaskInstance)",
          "",
          "[Removed Lines]",
          "137: def all_subdag_tasks_query(sub_dag_run_ids, session, state, confirmed_dates):",
          "",
          "[Added Lines]",
          "141: def all_subdag_tasks_query(",
          "142:     sub_dag_run_ids: List[str], session: SASession, state: TaskInstanceState, confirmed_dates: List[datetime]",
          "143: ):",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "144:     return qry_sub_dag",
          "148:     \"\"\"Get all tasks of the main dag that will be affected by a state change\"\"\"",
          "149:     qry_dag = (",
          "150:         session.query(TaskInstance)",
          "",
          "[Removed Lines]",
          "147: def get_all_dag_task_query(dag, session, state, task_ids, confirmed_dates):",
          "",
          "[Added Lines]",
          "153: def get_all_dag_task_query(",
          "154:     dag: DAG,",
          "155:     session: SASession,",
          "156:     state: TaskInstanceState,",
          "157:     task_ids: List[str],",
          "158:     confirmed_dates: List[datetime],",
          "159: ):",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "160:     return qry_dag",
          "164:     \"\"\"Go through subdag operators and create dag runs. We will only work",
          "165:     within the scope of the subdag. We won't propagate to the parent dag,",
          "166:     but we will propagate from parent to subdag.",
          "",
          "[Removed Lines]",
          "163: def get_subdag_runs(dag, session, state, task_ids, commit, confirmed_dates):",
          "",
          "[Added Lines]",
          "175: def get_subdag_runs(",
          "176:     dag: DAG,",
          "177:     session: SASession,",
          "178:     state: TaskInstanceState,",
          "179:     task_ids: List[str],",
          "180:     commit: bool,",
          "181:     confirmed_dates: List[datetime],",
          "182: ) -> List[str]:",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "181:                 dag_runs = _create_dagruns(",
          "182:                     current_task.subdag,",
          "183:                     execution_dates=confirmed_dates,",
          "185:                     run_type=DagRunType.BACKFILL_JOB,",
          "186:                 )",
          "",
          "[Removed Lines]",
          "184:                     state=State.RUNNING,",
          "",
          "[Added Lines]",
          "203:                     state=TaskInstanceState.RUNNING,",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "192:     return sub_dag_ids",
          "196:     \"\"\"Verifies integrity of dag_runs.",
          "198:     :param dag_runs: dag runs to verify",
          "",
          "[Removed Lines]",
          "195: def verify_dagruns(dag_runs, commit, state, session, current_task):",
          "",
          "[Added Lines]",
          "214: def verify_dagruns(",
          "215:     dag_runs: List[DagRun],",
          "216:     commit: bool,",
          "217:     state: TaskInstanceState,",
          "218:     session: SASession,",
          "219:     current_task: BaseOperator,",
          "220: ):",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "210:             session.merge(dag_run)",
          "214:     \"\"\"",
          "215:     Verify the integrity of the dag runs in case a task was added or removed",
          "216:     set the confirmed execution dates as they might be different",
          "",
          "[Removed Lines]",
          "213: def verify_dag_run_integrity(dag, dates):",
          "",
          "[Added Lines]",
          "238: def verify_dag_run_integrity(dag: DAG, dates: List[datetime]) -> List[datetime]:",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "225:     return confirmed_dates",
          "229:     \"\"\"Yield task ids and optionally ancestor and descendant ids.\"\"\"",
          "230:     for task in tasks:",
          "231:         yield task.task_id",
          "",
          "[Removed Lines]",
          "228: def find_task_relatives(tasks, downstream, upstream):",
          "",
          "[Added Lines]",
          "253: def find_task_relatives(",
          "254:     tasks: Iterable[BaseOperator], downstream: bool, upstream: bool",
          "255: ) -> Generator[str, None, None]:",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "237:                 yield relative.task_id",
          "241:     \"\"\"Returns dates of DAG execution\"\"\"",
          "242:     latest_execution_date = dag.get_latest_execution_date()",
          "243:     if latest_execution_date is None:",
          "",
          "[Removed Lines]",
          "240: def get_execution_dates(dag, execution_date, future, past):",
          "",
          "[Added Lines]",
          "267: def get_execution_dates(dag: DAG, execution_date: datetime, future: bool, past: bool) -> List[datetime]:",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "268: @provide_session",
          "270:     \"\"\"",
          "271:     Helper method that set dag run state in the DB.",
          "",
          "[Removed Lines]",
          "269: def _set_dag_run_state(dag_id, execution_date, state, session=None):",
          "",
          "[Added Lines]",
          "296: def _set_dag_run_state(",
          "297:     dag_id: str, execution_date: datetime, state: TaskInstanceState, session: SASession = NEW_SESSION",
          "298: ):",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "279:         session.query(DagRun).filter(DagRun.dag_id == dag_id, DagRun.execution_date == execution_date).one()",
          "280:     )",
          "281:     dag_run.state = state",
          "283:         dag_run.start_date = timezone.utcnow()",
          "284:         dag_run.end_date = None",
          "285:     else:",
          "",
          "[Removed Lines]",
          "282:     if state == State.RUNNING:",
          "",
          "[Added Lines]",
          "311:     if state == TaskInstanceState.RUNNING:",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "290: @provide_session",
          "292:     \"\"\"",
          "293:     Set the dag run for a specific execution date and its task instances",
          "294:     to success.",
          "",
          "[Removed Lines]",
          "291: def set_dag_run_state_to_success(dag, execution_date, commit=False, session=None):",
          "",
          "[Added Lines]",
          "320: def set_dag_run_state_to_success(",
          "321:     dag: Optional[DAG],",
          "322:     execution_date: Optional[datetime],",
          "323:     commit: bool = False,",
          "324:     session: SASession = NEW_SESSION,",
          "325: ) -> List[TaskInstance]:",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "307:     # Mark the dag run to success.",
          "308:     if commit:",
          "311:     # Mark all task instances of the dag run to success.",
          "312:     for task in dag.tasks:",
          "313:         task.dag = dag",
          "314:     return set_state(",
          "316:     )",
          "319: @provide_session",
          "321:     \"\"\"",
          "322:     Set the dag run for a specific execution date and its running task instances",
          "323:     to failed.",
          "",
          "[Removed Lines]",
          "309:         _set_dag_run_state(dag.dag_id, execution_date, State.SUCCESS, session)",
          "315:         tasks=dag.tasks, execution_date=execution_date, state=State.SUCCESS, commit=commit, session=session",
          "320: def set_dag_run_state_to_failed(dag, execution_date, commit=False, session=None):",
          "",
          "[Added Lines]",
          "343:         _set_dag_run_state(dag.dag_id, execution_date, TaskInstanceState.SUCCESS, session)",
          "349:         tasks=dag.tasks,",
          "350:         execution_date=execution_date,",
          "351:         state=TaskInstanceState.SUCCESS,",
          "352:         commit=commit,",
          "353:         session=session,",
          "358: def set_dag_run_state_to_failed(",
          "359:     dag: Optional[DAG],",
          "360:     execution_date: Optional[datetime],",
          "361:     commit: bool = False,",
          "362:     session: SASession = NEW_SESSION,",
          "363: ) -> List[TaskInstance]:",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "336:     # Mark the dag run to failed.",
          "337:     if commit:",
          "341:     task_ids = [task.task_id for task in dag.tasks]",
          "350:     )",
          "351:     task_ids_of_running_tis = [task_instance.task_id for task_instance in tis]",
          "",
          "[Removed Lines]",
          "338:         _set_dag_run_state(dag.dag_id, execution_date, State.FAILED, session)",
          "340:     # Mark only RUNNING task instances.",
          "342:     tis = (",
          "343:         session.query(TaskInstance)",
          "344:         .filter(",
          "345:             TaskInstance.dag_id == dag.dag_id,",
          "346:             TaskInstance.execution_date == execution_date,",
          "347:             TaskInstance.task_id.in_(task_ids),",
          "348:         )",
          "349:         .filter(TaskInstance.state == State.RUNNING)",
          "",
          "[Added Lines]",
          "381:         _set_dag_run_state(dag.dag_id, execution_date, TaskInstanceState.FAILED, session)",
          "383:     # Mark only running task instances.",
          "385:     tis = session.query(TaskInstance).filter(",
          "386:         TaskInstance.dag_id == dag.dag_id,",
          "387:         TaskInstance.execution_date == execution_date,",
          "388:         TaskInstance.task_id.in_(task_ids),",
          "389:         TaskInstance.state.in_(State.running),",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "358:         tasks.append(task)",
          "360:     return set_state(",
          "362:     )",
          "365: @provide_session",
          "367:     \"\"\"",
          "368:     Set the dag run for a specific execution date to running.",
          "",
          "[Removed Lines]",
          "361:         tasks=tasks, execution_date=execution_date, state=State.FAILED, commit=commit, session=session",
          "366: def set_dag_run_state_to_running(dag, execution_date, commit=False, session=None):",
          "",
          "[Added Lines]",
          "401:         tasks=tasks,",
          "402:         execution_date=execution_date,",
          "403:         state=TaskInstanceState.FAILED,",
          "404:         commit=commit,",
          "405:         session=session,",
          "410: def set_dag_run_state_to_running(",
          "411:     dag: Optional[DAG],",
          "412:     execution_date: Optional[datetime],",
          "413:     commit: bool = False,",
          "414:     session: SASession = NEW_SESSION,",
          "415: ) -> List[TaskInstance]:",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "381:     # Mark the dag run to running.",
          "382:     if commit:",
          "385:     # To keep the return type consistent with the other similar functions.",
          "386:     return res",
          "",
          "[Removed Lines]",
          "383:         _set_dag_run_state(dag.dag_id, execution_date, State.RUNNING, session)",
          "",
          "[Added Lines]",
          "432:         _set_dag_run_state(dag.dag_id, execution_date, TaskInstanceState.RUNNING, session)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ec08a5c1a639dad70a2ca63a34b116f3e1f1c16e",
      "candidate_info": {
        "commit_hash": "ec08a5c1a639dad70a2ca63a34b116f3e1f1c16e",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ec08a5c1a639dad70a2ca63a34b116f3e1f1c16e",
        "files": [
          "dev/provider_packages/prepare_provider_packages.py"
        ],
        "message": "Add possibility to ignore common deprecated message (#20444)\n\nThere are some cases where deprecation of a commonly used\ndependency causes deprecation message in multiple dependencies.\n\nThis happened in December 2021 with distutils deprecation.\nThe disutil deprecation started to appear as new versions of\nmultiple packages were released.\n\nThis change adds such \"common\" deprecation messages that should\nbe filtered out independently where they were generated.\n\n(cherry picked from commit daeeb7d401cd30063ca3de3bf5153e8ffb3741b6)",
        "before_after_code_files": [
          "dev/provider_packages/prepare_provider_packages.py||dev/provider_packages/prepare_provider_packages.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/provider_packages/prepare_provider_packages.py||dev/provider_packages/prepare_provider_packages.py": [
          "File: dev/provider_packages/prepare_provider_packages.py -> dev/provider_packages/prepare_provider_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1238:         console.print(\"[red]Provider info not validated against runtime schema[/]\")",
          "1239:         raise Exception(",
          "1240:             \"Error when validating schema. The schema must be compatible with \"",
          "1242:             ex,",
          "1243:         )",
          "",
          "[Removed Lines]",
          "1241:             + \"airflow/provider_info.schema.json.\",",
          "",
          "[Added Lines]",
          "1241:             \"airflow/provider_info.schema.json.\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2049:         console.print(\"[yellow]There are two cases that are legitimate deprecation warnings though:[/]\")",
          "2050:         console.print(\"[yellow] 1) when you deprecate whole module or class and replace it in provider[/]\")",
          "2051:         console.print(\"[yellow] 2) when 3rd-party module generates Deprecation and you cannot upgrade it[/]\")",
          "2052:         console.print()",
          "2053:         console.print(",
          "2054:             \"[yellow]In case 1), add the deprecation message to \"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2052:         console.print(",
          "2053:             \"[yellow] 3) when many 3rd-party module generates same Deprecation warning that \"",
          "2054:             \"comes from another common library[/]\"",
          "2055:         )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2058:             \"[yellow]In case 2), add the deprecation message together with module it generates to \"",
          "2059:             \"the KNOWN_DEPRECATED_MESSAGES in prepare_provider_packages.py[/]\"",
          "2060:         )",
          "2061:         console.print()",
          "2062:         raise_error = True",
          "2063:     else:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2065:         console.print(",
          "2066:             \"[yellow]In case 3), add the deprecation message to \"",
          "2067:             \"the KNOWN_COMMON_DEPRECATED_MESSAGES in prepare_provider_packages.py[/]\"",
          "2068:         )",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "2117:     (\"The module cloudant is now deprecated. The replacement is ibmcloudant.\", \"cloudant\"),",
          "2118: }",
          "2120: # The set of warning messages generated by direct importing of some deprecated modules. We should only",
          "2121: # ignore those messages when the warnings are generated directly by importlib - which means that",
          "2122: # we imported it directly during module walk by the importlib library",
          "2123: KNOWN_DEPRECATED_DIRECT_IMPORTS: Set[str] = {",
          "2124:     \"This module is deprecated. Please use `airflow.providers.microsoft.azure.hooks.data_factory`.\",",
          "2125:     \"This module is deprecated. Please use `airflow.providers.amazon.aws.hooks.dynamodb`.\",",
          "2126:     \"This module is deprecated. Please use `airflow.providers.microsoft.azure.transfers.local_to_wasb`.\",",
          "2127:     \"This module is deprecated. Please use `airflow.providers.tableau.operators.tableau_refresh_workbook`.\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2128: KNOWN_COMMON_DEPRECATED_MESSAGES: Set[str] = {",
          "2129:     \"distutils Version classes are deprecated. Use packaging.version instead.\"",
          "2130: }",
          "2136:     \"This module is deprecated. Please use `airflow.providers.microsoft.azure.hooks.batch`.\",",
          "2137:     \"This module is deprecated. Please use `airflow.providers.microsoft.azure.hooks.container_instance`.\",",
          "2138:     \"This module is deprecated. Please use `airflow.providers.microsoft.azure.hooks.container_registry`.\",",
          "2139:     \"This module is deprecated. Please use `airflow.providers.microsoft.azure.hooks.container_volume`.\",",
          "2140:     \"This module is deprecated. Please use `airflow.providers.microsoft.azure.hooks.cosmos`.\",",
          "2142:     \"This module is deprecated. Please use `airflow.providers.microsoft.azure.hooks.data_lake`.\",",
          "2143:     \"This module is deprecated. Please use `airflow.providers.microsoft.azure.hooks.fileshare`.\",",
          "2144:     \"This module is deprecated. Please use `airflow.providers.microsoft.azure.operators.batch`.\",",
          "2145:     \"This module is deprecated. \"",
          "2146:     \"Please use `airflow.providers.microsoft.azure.operators.container_instances`.\",",
          "2147:     \"This module is deprecated. Please use `airflow.providers.microsoft.azure.operators.cosmos`.\",",
          "2148:     \"This module is deprecated. Please use `airflow.providers.microsoft.azure.secrets.key_vault`.\",",
          "2149:     \"This module is deprecated. Please use `airflow.providers.microsoft.azure.sensors.cosmos`.\",",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "2131:     \"This module is deprecated. Please use `kubernetes.client.models.V1VolumeMount`.\",",
          "2132:     'numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header,'",
          "2133:     ' got 216 from PyObject',",
          "2134: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2159:     \"This module is deprecated. Please use `airflow.providers.amazon.aws.sensors.step_function`.\",",
          "2160:     \"This module is deprecated. Please use `airflow.providers.amazon.aws.operators.step_function`.\",",
          "2161:     'This module is deprecated. Please use `airflow.providers.amazon.aws.operators.ec2`.',",
          "2162:     'This module is deprecated. Please use `airflow.providers.amazon.aws.sensors.ec2`.',",
          "2163:     \"This module is deprecated. Please use `airflow.providers.amazon.aws.sensors.s3`.\",",
          "2164:     \"This module is deprecated. Please use `airflow.providers.amazon.aws.operators.s3`.\",",
          "2165:     \"This module is deprecated. Please use `airflow.providers.amazon.aws.operators.dms`.\",",
          "2166:     \"This module is deprecated. Please use `airflow.providers.amazon.aws.sensors.dms`.\",",
          "2167:     'This module is deprecated. Please use `airflow.providers.amazon.aws.operators.emr`.',",
          "2168:     'This module is deprecated. Please use `airflow.providers.amazon.aws.sensors.emr`.',",
          "2169:     \"This module is deprecated. Please use `airflow.providers.amazon.aws.hooks.redshift_cluster` \"",
          "2170:     \"or `airflow.providers.amazon.aws.hooks.redshift_sql` as appropriate.\",",
          "2171:     \"This module is deprecated. Please use `airflow.providers.amazon.aws.operators.redshift_sql` \"",
          "2172:     \"or `airflow.providers.amazon.aws.operators.redshift_cluster` as appropriate.\",",
          "2173:     \"This module is deprecated. Please use `airflow.providers.amazon.aws.sensors.redshift_cluster`.\",",
          "2174:     'This module is deprecated. Please use `airflow.providers.amazon.aws.operators.sagemaker`.',",
          "2175:     'This module is deprecated. Please use `airflow.providers.amazon.aws.sensors.sagemaker`.',",
          "2176:     'This module is deprecated. Please use `airflow.providers.amazon.aws.hooks.emr`.',",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "2151:     return True",
          "2154: @cli.command()",
          "2155: def verify_provider_classes():",
          "2156:     \"\"\"Verifies names for all provider classes.\"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2197: def filter_known_common_deprecated_messages(warn: warnings.WarningMessage) -> bool:",
          "2198:     msg_string = str(warn.message).replace(\"\\n\", \" \")",
          "2199:     for m in KNOWN_COMMON_DEPRECATED_MESSAGES:",
          "2200:         if msg_string == m:",
          "2201:             return False",
          "2202:     return True",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "2172:             bad += inc_bad",
          "2173:         warns = list(filter(filter_known_warnings, warns))",
          "2174:         warns = list(filter(filter_direct_importlib_warning, warns))",
          "2175:         if not summarise_total_vs_bad_and_warnings(total, bad, warns):",
          "2176:             sys.exit(1)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2226:         warns = list(filter(filter_known_common_deprecated_messages, warns))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "94865f9c6b780ab80bc78f6287752c426e769c60",
      "candidate_info": {
        "commit_hash": "94865f9c6b780ab80bc78f6287752c426e769c60",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/94865f9c6b780ab80bc78f6287752c426e769c60",
        "files": [
          "airflow/models/taskinstance.py"
        ],
        "message": "Adds retry on taskinstance retrieval lock (#20030)\n\nFixes: #19832\n\nCo-authored-by: Jaroslaw Potiuk <jarek@Jaroslaws-MacBook-Pro.local>\n(cherry picked from commit 78c815e22b67e442982b53f41d7d899723d5de9f)",
        "before_after_code_files": [
          "airflow/models/taskinstance.py||airflow/models/taskinstance.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/taskinstance.py||airflow/models/taskinstance.py": [
          "File: airflow/models/taskinstance.py -> airflow/models/taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "91: from airflow.utils.net import get_hostname",
          "92: from airflow.utils.operator_helpers import context_to_airflow_vars",
          "93: from airflow.utils.platform import getuser",
          "94: from airflow.utils.session import create_session, provide_session",
          "95: from airflow.utils.sqlalchemy import ExtendedJSON, UtcDateTime",
          "96: from airflow.utils.state import DagRunState, State",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "94: from airflow.utils.retries import run_with_db_retries",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "723:         )",
          "725:         if lock_for_update:",
          "727:         else:",
          "728:             ti = qry.first()",
          "729:         if ti:",
          "",
          "[Removed Lines]",
          "726:             ti: Optional[TaskInstance] = qry.with_for_update().first()",
          "",
          "[Added Lines]",
          "727:             for attempt in run_with_db_retries(logger=self.log):",
          "728:                 with attempt:",
          "729:                     ti: Optional[TaskInstance] = qry.with_for_update().first()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5f1236f062a3980ad36dc24213366d1db0e00112",
      "candidate_info": {
        "commit_hash": "5f1236f062a3980ad36dc24213366d1db0e00112",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/5f1236f062a3980ad36dc24213366d1db0e00112",
        "files": [
          "setup.cfg",
          "tests/www/views/test_views_rendered.py"
        ],
        "message": "update upper bound for MarkupSafe (#19953)\n\nCo-authored-by: Tzu-ping Chung <tp@astronomer.io>\n(cherry picked from commit ba6b7c7424f6b5ea2c1464304be8738ea482f8c1)",
        "before_after_code_files": [
          "setup.cfg||setup.cfg",
          "tests/www/views/test_views_rendered.py||tests/www/views/test_views_rendered.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.cfg||setup.cfg": [
          "File: setup.cfg -> setup.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "123:     lazy-object-proxy",
          "124:     lockfile>=0.12.2",
          "125:     markdown>=2.5.2, <4.0",
          "127:     marshmallow-oneofschema>=2.0.1",
          "128:     # Required by vendored-in connexion",
          "129:     openapi-spec-validator>=0.2.4",
          "",
          "[Removed Lines]",
          "126:     markupsafe>=1.1.1",
          "",
          "[Added Lines]",
          "126:     markupsafe>=1.1.1, <=2.0",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "206: line_length=110",
          "207: combine_as_imports = true",
          "208: default_section = THIRDPARTY",
          "210: # Need to be consistent with the exclude config defined in pre-commit-config.yaml",
          "211: skip=build,.tox,venv",
          "212: profile = black",
          "",
          "[Removed Lines]",
          "209: known_first_party=airflow,tests",
          "",
          "[Added Lines]",
          "209: known_first_party=airflow,airflow_breeze,tests",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_rendered.py||tests/www/views/test_views_rendered.py": [
          "File: tests/www/views/test_views_rendered.py -> tests/www/views/test_views_rendered.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "157:     assert resp.status_code == 200",
          "159:     resp_html: str = resp.data.decode(\"utf-8\")",
          "161:     assert (",
          "162:         \"Webserver does not have access to User-defined Macros or Filters when \"",
          "163:         \"Dag Serialization is enabled. Hence for the task that have not yet \"",
          "",
          "[Removed Lines]",
          "160:     assert \"echo Hello Apache Airflow\" not in resp_html",
          "",
          "[Added Lines]",
          "160:     assert \"echo Hello Apache Airflow\" in resp_html",
          "",
          "---------------"
        ]
      }
    }
  ]
}