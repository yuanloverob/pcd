{
  "cve_id": "CVE-2021-45456",
  "cve_desc": "Apache kylin checks the legitimacy of the project before executing some commands with the project name passed in by the user. There is a mismatch between what is being checked and what is being used as the shell command argument in DiagnosisService. This may cause an illegal project name to pass the check and perform the following steps, resulting in a command injection vulnerability. This issue affects Apache Kylin 4.0.0.",
  "repo": "apache/kylin",
  "patch_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
  "patch_info": {
    "commit_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
    "repo": "apache/kylin",
    "commit_url": "https://github.com/apache/kylin/commit/f4daf14dde99b934c92ce2c832509f24342bc845",
    "files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "server/src/main/webapp/WEB-INF/web.xml"
    ],
    "message": "test fix",
    "before_after_code_files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java"
    ]
  },
  "patch_diff": {
    "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "3403:     public String getKerberosPrincipal() {",
      "3404:         return getOptional(\"kylin.kerberos.principal\");",
      "3405:     }",
      "3406: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "3407:     public String getEncryptCipherIvSpec() {",
      "3408:         return getOptional(\"kylin.security.encrypt.cipher.ivSpec\", \"AAAAAAAAAAAAAAAA\");",
      "3409:     }",
      "",
      "---------------"
    ],
    "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java -> core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "25: import java.security.NoSuchAlgorithmException;",
      "27: import org.apache.commons.codec.binary.Base64;",
      "29: import javax.crypto.Cipher;",
      "30: import javax.crypto.NoSuchPaddingException;",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "28: import org.apache.kylin.common.KylinConfig;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "42:             InvalidKeyException, NoSuchPaddingException, NoSuchAlgorithmException, UnsupportedEncodingException {",
      "43:         Cipher cipher = Cipher.getInstance(\"AES/CFB/PKCS5Padding\");",
      "44:         final SecretKeySpec secretKey = new SecretKeySpec(key, \"AES\");",
      "46:         cipher.init(cipherMode, secretKey, ivSpec);",
      "47:         return cipher;",
      "48:     }",
      "",
      "[Removed Lines]",
      "45:         IvParameterSpec ivSpec = new IvParameterSpec(\"AAAAAAAAAAAAAAAA\".getBytes(\"UTF-8\"));",
      "",
      "[Added Lines]",
      "46:         IvParameterSpec ivSpec = new IvParameterSpec(KylinConfig.getInstanceFromEnv().getEncryptCipherIvSpec().getBytes(\"UTF-8\"));",
      "",
      "---------------"
    ],
    "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java": [
      "File: core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java -> core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: package org.apache.kylin.common.util;",
      "21: import org.junit.Assert;",
      "22: import org.junit.Test;",
      "26:     @Test",
      "27:     public void testAESEncrypt(){",
      "",
      "[Removed Lines]",
      "24: public class EncryptUtilTest {",
      "",
      "[Added Lines]",
      "21: import org.junit.After;",
      "23: import org.junit.Before;",
      "26: public class EncryptUtilTest extends LocalFileMetadataTestCase {",
      "27:     @Before",
      "28:     public void setUp() throws Exception {",
      "29:         this.createTestMetadata();",
      "30:     }",
      "32:     @After",
      "33:     public void after() throws Exception {",
      "34:         this.cleanupTestMetadata();",
      "35:     }",
      "",
      "---------------"
    ],
    "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java": [
      "File: server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java -> server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "87:     public String dumpProjectDiagnosisInfo(String project, File exportPath) throws IOException {",
      "88:         Message msg = MsgPicker.getMsg();",
      "89:         ProjectInstance projectInstance =",
      "90:                 ProjectManager.getInstance(KylinConfig.getInstanceFromEnv())",
      "92:         if (null == projectInstance) {",
      "93:             throw new BadRequestException(",
      "95:         }",
      "96:         aclEvaluate.checkProjectOperationPermission(projectInstance);",
      "98:         runDiagnosisCLI(args);",
      "99:         return getDiagnosisPackageName(exportPath);",
      "100:     }",
      "",
      "[Removed Lines]",
      "91:                         .getProject(ValidateUtil.convertStringToBeAlphanumericUnderscore(project));",
      "94:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), project));",
      "97:         String[] args = { project, exportPath.getAbsolutePath() };",
      "",
      "[Added Lines]",
      "89:         String projectName = ValidateUtil.convertStringToBeAlphanumericUnderscore(project);",
      "92:                         .getProject(projectName);",
      "95:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), projectName));",
      "98:         String[] args = { projectName, exportPath.getAbsolutePath() };",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "857aaeaf3c6dba0ad2587761f917ddcc62844932",
      "candidate_info": {
        "commit_hash": "857aaeaf3c6dba0ad2587761f917ddcc62844932",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/857aaeaf3c6dba0ad2587761f917ddcc62844932",
        "files": [
          "build/conf/spark-driver-log4j-default.properties",
          "build/conf/spark-executor-log4j-default.properties",
          "build/conf/spark-executor-log4j.properties",
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "core-common/src/main/resources/kylin-defaults.properties",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java",
          "pom.xml"
        ],
        "message": "Minor, using default log4j properties for spark driver and exectutor (#1891)\n\n* using default log4j properties for spark driver and executor\n\n* Incompatible to hadoop 2.x env\n\nRevert \"KYLIN-3610 bump the version of curator and zookeeper\"\n\nThis reverts commit 8cde6dda\n\n* minor, adapt default properties",
        "before_after_code_files": [
          "build/conf/spark-driver-log4j-default.properties||build/conf/spark-driver-log4j-default.properties",
          "build/conf/spark-executor-log4j-default.properties||build/conf/spark-executor-log4j-default.properties",
          "build/conf/spark-executor-log4j.properties||build/conf/spark-executor-log4j.properties",
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "core-common/src/main/resources/kylin-defaults.properties||core-common/src/main/resources/kylin-defaults.properties",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ],
          "candidate": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ]
        }
      },
      "candidate_diff": {
        "build/conf/spark-driver-log4j-default.properties||build/conf/spark-driver-log4j-default.properties": [
          "File: build/conf/spark-driver-log4j-default.properties -> build/conf/spark-driver-log4j-default.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #     http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing, software",
          "13: # distributed under the License is distributed on an \"AS IS\" BASIS,",
          "14: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
          "15: # See the License for the specific language governing permissions and",
          "16: # limitations under the License.",
          "17: #",
          "19: # It's called spark-driver-log4j-default.properties so that it won't distract users from the other more important log4j config file: kylin-server-log4j.properties",
          "20: # enable this by -Dlog4j.configuration=spark-driver-log4j-default.properties",
          "22: #overall config",
          "23: log4j.rootLogger=INFO,stderr",
          "24: log4j.logger.org.apache.kylin=DEBUG",
          "25: log4j.logger.org.springframework=WARN",
          "26: log4j.logger.org.apache.spark=WARN",
          "28: log4j.appender.stderr=org.apache.log4j.ConsoleAppender",
          "29: log4j.appender.stderr.layout=org.apache.kylin.common.logging.SensitivePatternLayout",
          "30: log4j.appender.stderr.target=System.err",
          "31: log4j.appender.stderr.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2} : %m%n",
          "",
          "---------------"
        ],
        "build/conf/spark-executor-log4j-default.properties||build/conf/spark-executor-log4j-default.properties": [
          "File: build/conf/spark-executor-log4j-default.properties -> build/conf/spark-executor-log4j-default.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #     http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing, software",
          "13: # distributed under the License is distributed on an \"AS IS\" BASIS,",
          "14: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
          "15: # See the License for the specific language governing permissions and",
          "16: # limitations under the License.",
          "17: #",
          "19: # It's called spark-executor-log4j.properties so that it won't distract users from the other more important log4j config file: kylin-server-log4j.properties",
          "20: # enable this by -Dlog4j.configuration=spark-executor-log4j-default.properties",
          "21: log4j.rootLogger=INFO,stderr",
          "23: log4j.appender.stderr=org.apache.log4j.ConsoleAppender",
          "24: log4j.appender.stderr.layout=org.apache.kylin.common.logging.SensitivePatternLayout",
          "25: log4j.appender.stderr.target=System.err",
          "26: #Don't add line number (%L) as it's too costly!",
          "27: log4j.appender.stderr.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2} : %m%n",
          "",
          "---------------"
        ],
        "build/conf/spark-executor-log4j.properties||build/conf/spark-executor-log4j.properties": [
          "File: build/conf/spark-executor-log4j.properties -> build/conf/spark-executor-log4j.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "44: log4j.appender.hdfs.layout=org.apache.kylin.common.logging.SensitivePatternLayout",
          "45: #Don't add line number (%L) as it's too costly!",
          "",
          "[Removed Lines]",
          "46: log4j.appender.hdfs.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2} : %m%n",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "265:     final protected void reloadKylinConfig(Properties properties) {",
          "266:         this.properties = BCC.check(properties);",
          "267:         setProperty(\"kylin.metadata.url.identifier\", getMetadataUrlPrefix());",
          "269:     }",
          "271:     private Map<Integer, String> convertKeyToInteger(Map<String, String> map) {",
          "",
          "[Removed Lines]",
          "268:         setProperty(\"kylin.log.spark-executor-properties-file\", getLogSparkExecutorPropertiesFile());",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2521:     }",
          "2523:     public String getLogSparkDriverPropertiesFile() {",
          "2525:     }",
          "2527:     public String getLogSparkExecutorPropertiesFile() {",
          "2529:     }",
          "2531:     private String getLogPropertyFile(String filename) {",
          "",
          "[Removed Lines]",
          "2524:         return getLogPropertyFile(\"spark-driver-log4j.properties\");",
          "2528:         return getLogPropertyFile(\"spark-executor-log4j.properties\");",
          "",
          "[Added Lines]",
          "2523:         return getLogPropertyFile(getLogSparkDriverProperties());",
          "2524:     }",
          "2526:     public boolean isDefaultLogSparkDriverProperties() {",
          "2527:         return \"spark-driver-log4j-default.properties\".equals(getLogSparkDriverProperties());",
          "2528:     }",
          "2530:     public String getLogSparkDriverProperties() {",
          "2531:         return getOptional(\"kylin.spark.driver.log4j.properties\", \"spark-driver-log4j-default.properties\");",
          "2535:         return getLogPropertyFile(getLogSparkExecutorProperties());",
          "2536:     }",
          "2538:     public boolean isDefaultLogSparkExecutorProperties() {",
          "2539:         return \"spark-executor-log4j-default.properties\".equals(getLogSparkExecutorProperties());",
          "2540:     }",
          "2542:     public String getLogSparkExecutorProperties() {",
          "2543:         return getOptional(\"kylin.spark.executor.log4j.properties\", \"spark-executor-log4j-default.properties\");",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2627:                 String executorLogPath = \"\";",
          "2628:                 String driverLogPath = \"\";",
          "2629:                 File executorLogFile = FileUtils.findFile(KylinConfigBase.getKylinHome() + \"/conf\",",
          "2631:                 if (executorLogFile != null) {",
          "2632:                     executorLogPath = executorLogFile.getCanonicalPath();",
          "2633:                 }",
          "2634:                 path = executorLogPath;",
          "2635:                 if (isYarnCluster) {",
          "2636:                     File driverLogFile = FileUtils.findFile(KylinConfigBase.getKylinHome() + \"/conf\",",
          "2638:                     if (driverLogFile != null) {",
          "2639:                         driverLogPath = driverLogFile.getCanonicalPath();",
          "2640:                     }",
          "",
          "[Removed Lines]",
          "2630:                         \"spark-executor-log4j.properties\");",
          "2637:                             \"spark-driver-log4j.properties\");",
          "",
          "[Added Lines]",
          "2645:                         getLogSparkExecutorProperties());",
          "2652:                             getLogSparkDriverProperties());",
          "",
          "---------------"
        ],
        "core-common/src/main/resources/kylin-defaults.properties||core-common/src/main/resources/kylin-defaults.properties": [
          "File: core-common/src/main/resources/kylin-defaults.properties -> core-common/src/main/resources/kylin-defaults.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "222: # Read-Write separation deployment for Kylin 4, please check https://cwiki.apache.org/confluence/display/KYLIN/Read-Write+Separation+Deployment+for+Kylin+4.0",
          "223: #kylin.engine.submit-hadoop-conf-dir=",
          "225: # Spark conf (default is in spark/conf/spark-defaults.conf)",
          "226: kylin.engine.spark-conf.spark.master=yarn",
          "227: kylin.engine.spark-conf.spark.submit.deployMode=client",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "225: # log4j properties file for spark",
          "226: kylin.spark.driver.log4j.properties=spark-driver-log4j-default.properties",
          "227: kylin.spark.executor.log4j.properties=spark-executor-log4j-default.properties",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "238: kylin.engine.spark-conf.spark.eventLog.dir=hdfs\\:///kylin/spark-history",
          "239: kylin.engine.spark-conf.spark.history.fs.logDirectory=hdfs\\:///kylin/spark-history",
          "240: kylin.engine.spark-conf.spark.hadoop.yarn.timeline-service.enabled=false",
          "242: #kylin.engine.spark-conf.spark.sql.shuffle.partitions=1",
          "244: # manually upload spark-assembly jar to HDFS and then set this property will avoid repeatedly uploading jar at runtime",
          "",
          "[Removed Lines]",
          "241: kylin.engine.spark-conf.spark.executor.extraJavaOptions=-Dfile.encoding=UTF-8 -Dhdp.version=current -Dlog4j.configuration=spark-executor-log4j.properties -Dlog4j.debug -Dkylin.hdfs.working.dir=${hdfs.working.dir} -Dkylin.metadata.identifier=${kylin.metadata.url.identifier} -Dkylin.spark.category=job -Dkylin.spark.project=${job.project} -Dkylin.spark.identifier=${job.id} -Dkylin.spark.jobName=${job.stepId} -Duser.timezone=${user.timezone}",
          "",
          "[Added Lines]",
          "245: kylin.engine.spark-conf.spark.executor.extraJavaOptions=-Dfile.encoding=UTF-8 -Dhdp.version=current -Dlog4j.configuration=${kylin.spark.executor.log4j.properties} -Dlog4j.debug -Dkylin.hdfs.working.dir=${hdfs.working.dir} -Dkylin.metadata.identifier=${kylin.metadata.url.identifier} -Dkylin.spark.category=job -Dkylin.spark.project=${job.project} -Dkylin.spark.identifier=${job.id} -Dkylin.spark.jobName=${job.stepId} -Duser.timezone=${user.timezone}",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "272: #kylin.query.spark-conf.spark.yarn.jars=hdfs://localhost:9000/spark2_jars/*",
          "273: kylin.query.spark-conf.spark.hadoop.yarn.timeline-service.enabled=false",
          "276: # uncomment for HDP",
          "277: #kylin.query.spark-conf.spark.driver.extraJavaOptions=-Dhdp.version=current",
          "278: #kylin.query.spark-conf.spark.yarn.am.extraJavaOptions=-Dhdp.version=current",
          "",
          "[Removed Lines]",
          "275: kylin.query.spark-conf.spark.executor.extraJavaOptions=-Dhdp.version=current -Dlog4j.configuration=spark-executor-log4j.properties -Dlog4j.debug -Dkylin.hdfs.working.dir=${kylin.env.hdfs-working-dir} -Dkylin.metadata.identifier=${kylin.metadata.url.identifier} -Dkylin.spark.category=sparder -Dkylin.spark.identifier={{APP_ID}}",
          "",
          "[Added Lines]",
          "279: kylin.query.spark-conf.spark.executor.extraJavaOptions=-Dhdp.version=current -Dlog4j.configuration=${kylin.spark.executor.log4j.properties} -Dlog4j.debug -Dkylin.hdfs.working.dir=${kylin.env.hdfs-working-dir} -Dkylin.metadata.identifier=${kylin.metadata.url.identifier} -Dkylin.spark.category=sparder -Dkylin.spark.identifier={{APP_ID}}",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "447:     }",
          "449:     private void wrapLog4jConf(StringBuilder sb, KylinConfig config) {",
          "450:         final String localLog4j = config.getLogSparkDriverPropertiesFile();",
          "451:         final String log4jName = Paths.get(localLog4j).getFileName().toString();",
          "452:         if (isYarnCluster) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "450:         if (config.isDefaultLogSparkDriverProperties()) {",
          "451:             logger.info(\"Current using default log4j properties for spark driver in using `ConsoleAppender`.\" +",
          "452:                     \"Please modify `kylin.spark.driver.log4j.properties` to be `spark-driver-log4j.properties`\" +",
          "453:                     \"for uploading log file to hdfs.\");",
          "454:         }",
          "456:         if (config.isDefaultLogSparkExecutorProperties()) {",
          "457:             logger.info(\"Current using default log4j properties for spark executor in using `ConsoleAppender`.\" +",
          "458:                     \"Please modify `kylin.spark.executor.log4j.properties` to be `spark-executor-log4j.properties`\" +",
          "459:                     \"for uploading log file to hdfs.\");",
          "460:         }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "04f1361ff5216bf53662fb9afa24a135341c7e19",
      "candidate_info": {
        "commit_hash": "04f1361ff5216bf53662fb9afa24a135341c7e19",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/04f1361ff5216bf53662fb9afa24a135341c7e19",
        "files": [
          "core-cube/src/main/java/org/apache/kylin/cube/RawQueryLastHacker.java"
        ],
        "message": "KYLIN-5250 work round for no hack aggregation group",
        "before_after_code_files": [
          "core-cube/src/main/java/org/apache/kylin/cube/RawQueryLastHacker.java||core-cube/src/main/java/org/apache/kylin/cube/RawQueryLastHacker.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core-cube/src/main/java/org/apache/kylin/cube/RawQueryLastHacker.java||core-cube/src/main/java/org/apache/kylin/cube/RawQueryLastHacker.java": [
          "File: core-cube/src/main/java/org/apache/kylin/cube/RawQueryLastHacker.java -> core-cube/src/main/java/org/apache/kylin/cube/RawQueryLastHacker.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "35:     private static final Logger logger = LoggerFactory.getLogger(RawQueryLastHacker.class);",
          "37:     public static void hackNoAggregations(SQLDigest sqlDigest, CubeDesc cubeDesc, TupleInfo tupleInfo) {",
          "42:                 logger.info(\"No hack for aggregation sql by kylin.query.enable-no-aggregate-query which is 'true'.\");",
          "43:             }",
          "44:             return;",
          "",
          "[Removed Lines]",
          "38:         if (!sqlDigest.isRawQuery ||",
          "39:                 BackdoorToggles.getDisabledRawQueryLastHacker() ||",
          "40:                 KylinConfig.getInstanceFromEnv().isEnabledNoAggQuery()) {",
          "41:             if (KylinConfig.getInstanceFromEnv().isEnabledNoAggQuery()) {",
          "",
          "[Added Lines]",
          "38:         boolean isEnabledNoAggQuery = KylinConfig.getInstanceFromEnv().isEnabledNoAggQuery();",
          "39:         if (!sqlDigest.isRawQuery || BackdoorToggles.getDisabledRawQueryLastHacker() || isEnabledNoAggQuery) {",
          "40:             if (isEnabledNoAggQuery) {",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0c3ad9f42a81a15bf115a8d2cadfe58deba116a4",
      "candidate_info": {
        "commit_hash": "0c3ad9f42a81a15bf115a8d2cadfe58deba116a4",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/0c3ad9f42a81a15bf115a8d2cadfe58deba116a4",
        "files": [
          "kylin-spark-project/kylin-spark-query/src/main/java/org/apache/kylin/query/pushdown/SparkSubmitter.java",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/SparkEngine.java",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/ResultPlan.scala",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContextFacade.scala"
        ],
        "message": "KYLIN-5271 Query memory leaks",
        "before_after_code_files": [
          "kylin-spark-project/kylin-spark-query/src/main/java/org/apache/kylin/query/pushdown/SparkSubmitter.java||kylin-spark-project/kylin-spark-query/src/main/java/org/apache/kylin/query/pushdown/SparkSubmitter.java",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/SparkEngine.java||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/SparkEngine.java",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/ResultPlan.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/ResultPlan.scala",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContextFacade.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContextFacade.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "kylin-spark-project/kylin-spark-query/src/main/java/org/apache/kylin/query/pushdown/SparkSubmitter.java||kylin-spark-project/kylin-spark-query/src/main/java/org/apache/kylin/query/pushdown/SparkSubmitter.java": [
          "File: kylin-spark-project/kylin-spark-query/src/main/java/org/apache/kylin/query/pushdown/SparkSubmitter.java -> kylin-spark-project/kylin-spark-query/src/main/java/org/apache/kylin/query/pushdown/SparkSubmitter.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "30:     public static final Logger logger = LoggerFactory.getLogger(SparkSubmitter.class);",
          "32:     public static PushdownResponse submitPushDownTask(String sql) {",
          "36:         return new PushdownResponse(pair.getSecond(), pair.getFirst());",
          "37:     }",
          "",
          "[Removed Lines]",
          "33:         Pair<List<List<String>>, List<StructField>> pair =",
          "34:                 SparkSqlClient.executeSql(SparderContext.getSparkSession(), sql);",
          "35:         SparderContext.closeThreadSparkSession();",
          "",
          "[Added Lines]",
          "33:         Pair<List<List<String>>, List<StructField>> pair = null;",
          "34:         try {",
          "35:             pair = SparkSqlClient.executeSql(SparderContext.getSparkSession(), sql);",
          "36:         } finally {",
          "37:             SparderContext.closeThreadSparkSession();",
          "38:         }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/SparkEngine.java||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/SparkEngine.java": [
          "File: kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/SparkEngine.java -> kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/SparkEngine.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: import org.apache.kylin.query.runtime.plans.ResultType;",
          "30: import org.apache.spark.sql.Dataset;",
          "31: import org.apache.spark.sql.Row;",
          "32: import org.slf4j.Logger;",
          "33: import org.slf4j.LoggerFactory;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32: import org.apache.spark.sql.SparderContext;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39:     @Override",
          "40:     public Enumerable<Object> computeSCALA(DataContext dataContext, RelNode relNode, RelDataType resultType) {",
          "44:         }",
          "47:     }",
          "49:     @Override",
          "50:     public Enumerable<Object[]> compute(DataContext dataContext, RelNode relNode, RelDataType resultType) {",
          "54:         }",
          "56:     }",
          "58:     private Dataset<Row> toSparkPlan(DataContext dataContext, RelNode relNode) {",
          "",
          "[Removed Lines]",
          "41:         Dataset<Row> sparkPlan = toSparkPlan(dataContext, relNode);",
          "42:         if (System.getProperty(\"calcite.debug\") != null) {",
          "43:             log.debug(\"SPARK LOGICAL PLAN {}\", sparkPlan.queryExecution());",
          "45:         return ResultPlan.getResult(sparkPlan, resultType, ResultType.SCALA()).right().get();",
          "51:         Dataset<Row> sparkPlan = toSparkPlan(dataContext, relNode);",
          "52:         if (System.getProperty(\"calcite.debug\") != null) {",
          "53:             log.info(\"SPARK LOGICAL PLAN {}\", sparkPlan.queryExecution());",
          "55:         return ResultPlan.getResult(sparkPlan, resultType, ResultType.NORMAL()).left().get();",
          "",
          "[Added Lines]",
          "42:         try {",
          "43:             Dataset<Row> sparkPlan = toSparkPlan(dataContext, relNode);",
          "44:             if (System.getProperty(\"calcite.debug\") != null) {",
          "45:                 log.debug(\"SPARK LOGICAL PLAN {}\", sparkPlan.queryExecution());",
          "46:             }",
          "47:             return ResultPlan.getResult(sparkPlan, resultType, ResultType.SCALA()).right().get();",
          "48:         } finally {",
          "49:             SparderContext.closeThreadSparkSession();",
          "55:         try {",
          "56:             Dataset<Row> sparkPlan = toSparkPlan(dataContext, relNode);",
          "57:             if (System.getProperty(\"calcite.debug\") != null) {",
          "58:                 log.info(\"SPARK LOGICAL PLAN {}\", sparkPlan.queryExecution());",
          "59:             }",
          "60:             return ResultPlan.getResult(sparkPlan, resultType, ResultType.NORMAL()).left().get();",
          "61:         } finally {",
          "62:             SparderContext.closeThreadSparkSession();",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/ResultPlan.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/ResultPlan.scala": [
          "File: kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/ResultPlan.scala -> kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/kylin/query/runtime/plans/ResultPlan.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "190:           }",
          "191:       }",
          "192:     SparderContext.cleanQueryInfo()",
          "194:     result",
          "195:   }",
          "196: }",
          "",
          "[Removed Lines]",
          "193:     SparderContext.closeThreadSparkSession()",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala": [
          "File: kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala -> kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "61:   }",
          "63:   def getSparkSession: SparkSession = {",
          "65:     SparderContextFacade.current().getFirst",
          "66:   }",
          "",
          "[Removed Lines]",
          "64:     logInfo(s\"Current thread ${Thread.currentThread().getId} create a SparkSession.\")",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContextFacade.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContextFacade.scala": [
          "File: kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContextFacade.scala -> kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContextFacade.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "32:   def current(): Pair[SparkSession, UdfManager] = {",
          "33:     if (CURRENT_SPARKSESSION.get() == null) {",
          "34:       val spark = SparderContext.getOriginalSparkSession.cloneSession()",
          "35:       CURRENT_SPARKSESSION.set(new Pair[SparkSession, UdfManager](spark,",
          "36:         UdfManager.createWithoutBuildInFunc(spark)))",
          "37:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "35:       logInfo(s\"Current thread ${Thread.currentThread().getId} create a SparkSession.\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "18c52f3b42fcc8e1f92ae32c6e12f4ddf45fdf64",
      "candidate_info": {
        "commit_hash": "18c52f3b42fcc8e1f92ae32c6e12f4ddf45fdf64",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/18c52f3b42fcc8e1f92ae32c6e12f4ddf45fdf64",
        "files": [
          "core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java",
          "core-common/src/main/java/org/apache/kylin/common/util/RingBuffer.java",
          "core-common/src/test/java/org/apache/kylin/common/util/RingBufferTest.java"
        ],
        "message": "KYLIN-5246 long running job's log staying in mem, may cause job server oom.\n\nkeep only the last 10k logs by ring buffer\n\nadd a UT for the [RingBuffer.java]",
        "before_after_code_files": [
          "core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java||core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java",
          "core-common/src/main/java/org/apache/kylin/common/util/RingBuffer.java||core-common/src/main/java/org/apache/kylin/common/util/RingBuffer.java",
          "core-common/src/test/java/org/apache/kylin/common/util/RingBufferTest.java||core-common/src/test/java/org/apache/kylin/common/util/RingBufferTest.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java||core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java -> core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "58:     }",
          "60:     public void copyFile(String localFile, String destDir) throws IOException {",
          "62:             copyNative(localFile, destDir);",
          "64:             copyRemote(localFile, destDir);",
          "65:     }",
          "67:     private void copyNative(String localFile, String destDir) throws IOException {",
          "",
          "[Removed Lines]",
          "61:         if (remoteHost == null)",
          "63:         else",
          "",
          "[Added Lines]",
          "61:         if (remoteHost == null) {",
          "63:         } else {",
          "65:         }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "93:             r = runRemoteCommand(command, logAppender);",
          "94:         }",
          "98:                     + \", error message: \" + r.getSecond() + \"The command is: \\n\" + command",
          "100:             );",
          "102:         return r;",
          "103:     }",
          "",
          "[Removed Lines]",
          "96:         if (r.getFirst() != 0)",
          "97:             throw new IOException(\"OS command error exit with return code: \" + r.getFirst() //",
          "99:                     + (remoteHost == null ? \"\" : \" (remoteHost:\" + remoteHost + \")\") //",
          "",
          "[Added Lines]",
          "97:         if (r.getFirst() != 0) {",
          "98:             throw new IOException(\"OS command error exit with return code: \" + r.getFirst()",
          "100:                     + (remoteHost == null ? \"\" : \" (remoteHost:\" + remoteHost + \")\")",
          "102:         }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "143:             BufferedReader reader = new BufferedReader(",
          "144:                     new InputStreamReader(proc.getInputStream(), StandardCharsets.UTF_8));",
          "145:             String line;",
          "147:             while ((line = reader.readLine()) != null && !Thread.currentThread().isInterrupted()) {",
          "149:                 if (logAppender != null) {",
          "150:                     logAppender.log(line);",
          "151:                 }",
          "",
          "[Removed Lines]",
          "146:             StringBuilder result = new StringBuilder();",
          "148:                 result.append(line).append('\\n');",
          "",
          "[Added Lines]",
          "149:             RingBuffer ringBuffer = RingBuffer.allocate(10240);",
          "151:                 ringBuffer.put((line + '\\n').getBytes(StandardCharsets.UTF_8));",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "165:             try {",
          "166:                 int exitCode = proc.waitFor();",
          "168:             } catch (InterruptedException e) {",
          "169:                 Thread.currentThread().interrupt();",
          "170:                 throw new IOException(e);",
          "",
          "[Removed Lines]",
          "167:                 return Pair.newPair(exitCode, result.toString());",
          "",
          "[Added Lines]",
          "170:                 return Pair.newPair(exitCode, new String(ringBuffer.get(), StandardCharsets.UTF_8));",
          "",
          "---------------"
        ],
        "core-common/src/main/java/org/apache/kylin/common/util/RingBuffer.java||core-common/src/main/java/org/apache/kylin/common/util/RingBuffer.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/util/RingBuffer.java -> core-common/src/main/java/org/apache/kylin/common/util/RingBuffer.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.common.util;",
          "25: public class RingBuffer {",
          "26:     private final byte[] bytes;",
          "31:     private int writePos;",
          "36:     private int size;",
          "38:     private RingBuffer(int capacity) {",
          "39:         this.bytes = new byte[capacity];",
          "40:     }",
          "42:     public static RingBuffer allocate(int capacity) {",
          "43:         return new RingBuffer(capacity);",
          "44:     }",
          "46:     public RingBuffer put(byte[] src) {",
          "47:         for (int i = 0; i < src.length; i++) {",
          "48:             if (writePos >= bytes.length) {",
          "50:                 writePos = 0;",
          "51:             }",
          "52:             bytes[writePos++] = src[i];",
          "53:             size = size < bytes.length ? size + 1 : size;",
          "54:         }",
          "55:         return this;",
          "56:     }",
          "58:     public byte[] get() {",
          "59:         byte[] res;",
          "60:         if (size == bytes.length && writePos < size) {",
          "62:             res = new byte[size];",
          "63:             System.arraycopy(bytes, writePos, res, 0, size - writePos);",
          "64:             System.arraycopy(bytes, 0, res, size - writePos, writePos);",
          "65:         } else {",
          "66:             res = new byte[writePos];",
          "67:             System.arraycopy(bytes, 0, res, 0, writePos);",
          "68:         }",
          "69:         return res;",
          "70:     }",
          "71: }",
          "",
          "---------------"
        ],
        "core-common/src/test/java/org/apache/kylin/common/util/RingBufferTest.java||core-common/src/test/java/org/apache/kylin/common/util/RingBufferTest.java": [
          "File: core-common/src/test/java/org/apache/kylin/common/util/RingBufferTest.java -> core-common/src/test/java/org/apache/kylin/common/util/RingBufferTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.common.util;",
          "21: import org.junit.Assert;",
          "22: import org.junit.Test;",
          "24: import java.io.BufferedReader;",
          "25: import java.io.ByteArrayInputStream;",
          "26: import java.io.IOException;",
          "27: import java.io.InputStreamReader;",
          "28: import java.nio.charset.StandardCharsets;",
          "34: public class RingBufferTest {",
          "35:     @Test",
          "36:     public void test() throws IOException {",
          "37:         String log1 = \"2022-11-09 18:25:48,678 INFO  [task-result-getter-0] scheduler.TaskSetManager:54 : Starting task 0.0 in stage 384.0 (TID 621, kylin-hadoop-001, executor 7, partition 0, NODE_LOCAL, 7780 bytes)\\n\";",
          "38:         String log2 = \"2022-11-09 18:25:48,678 INFO  [dispatcher-event-loop-26] spark.MapOutputTrackerMasterEndpoint:54 : Asked to send map output locations for shuffle 138 to x.x.x.x:46334\\n\";",
          "39:         String log3 = \"2022-11-09 18:25:48,689 INFO  [task-result-getter-0] scheduler.TaskSetManager:54 : Finished task 0.0 in stage 384.0 (TID 621) in 31 ms on kylin-hadoop-001 (executor 7) (1/1)\\n\";",
          "40:         String log4 = \"2022-11-09 18:25:48,689 INFO  [task-result-getter-0] cluster.YarnScheduler:54 : Removed TaskSet 384.0, whose tasks have all completed, from pool vip_tasks\\n\";",
          "43:         byte[] logBytes = (log1 + log2 + log3 + log4).getBytes(StandardCharsets.UTF_8);",
          "44:         try (BufferedReader reader = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(logBytes)))) {",
          "45:             String line = null;",
          "47:             RingBuffer ringBuffer = RingBuffer.allocate(300);",
          "48:             while ((line = reader.readLine()) != null) {",
          "49:                 ringBuffer.put((line + '\\n').getBytes(StandardCharsets.UTF_8));",
          "50:             }",
          "53:             Assert.assertTrue(logBytes.length > 600 && ringBuffer.get().length == 300);",
          "55:             Assert.assertEquals(\" [task-result-getter-0] scheduler.TaskSetManager:54 : Finished task 0.0 in stage 384.0 (TID 621) in 31 ms on kylin-hadoop-001 (executor 7) (1/1)\\n\" +",
          "56:                             \"2022-11-09 18:25:48,689 INFO  [task-result-getter-0] cluster.YarnScheduler:54 : Removed TaskSet 384.0, whose tasks have all completed, from pool vip_tasks\\n\",",
          "57:                     new String(ringBuffer.get(), StandardCharsets.UTF_8));",
          "58:         }",
          "59:     }",
          "60: }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b06d6ae7e4c98e43c3642a6cd027507527de2ed7",
      "candidate_info": {
        "commit_hash": "b06d6ae7e4c98e43c3642a6cd027507527de2ed7",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/b06d6ae7e4c98e43c3642a6cd027507527de2ed7",
        "files": [
          "build-engine/src/main/java/org/apache/kylin/engine/mr/common/DefaultSslProtocolSocketFactory.java",
          "core-common/src/test/java/org/apache/kylin/common/util/ClassUtilTest.java",
          "core-cube/src/main/java/org/apache/kylin/cube/model/RowKeyColDesc.java",
          "core-metadata/src/test/java/org/apache/kylin/measure/raw/RawAggregatorTest.java",
          "core-metadata/src/test/java/org/apache/kylin/measure/raw/RawSerializerTest.java",
          "kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/monitor/MonitorExecutorExtension.scala",
          "kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/catalyst/expressions/ExpressionUtils.scala",
          "kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala",
          "kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/datasource/FilterExt.scala",
          "kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/hive/utils/QueryMetricUtils.scala",
          "kylin-spark-project/kylin-spark-metadata/src/main/spark24/org/apache/kylin/engine/spark/cross/CrossDateTimeUtils.scala",
          "kylin-spark-project/kylin-spark-query/src/main/spark24/org/apache/spark/sql/hive/KylinHiveSessionStateBuilder.scala",
          "pom.xml",
          "server/pom.xml"
        ],
        "message": "KYLIN-5225 update haddop version and make spark3 as the default profile",
        "before_after_code_files": [
          "build-engine/src/main/java/org/apache/kylin/engine/mr/common/DefaultSslProtocolSocketFactory.java||build-engine/src/main/java/org/apache/kylin/engine/mr/common/DefaultSslProtocolSocketFactory.java",
          "core-common/src/test/java/org/apache/kylin/common/util/ClassUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/ClassUtilTest.java",
          "core-cube/src/main/java/org/apache/kylin/cube/model/RowKeyColDesc.java||core-cube/src/main/java/org/apache/kylin/cube/model/RowKeyColDesc.java",
          "core-metadata/src/test/java/org/apache/kylin/measure/raw/RawAggregatorTest.java||core-metadata/src/test/java/org/apache/kylin/measure/raw/RawAggregatorTest.java",
          "core-metadata/src/test/java/org/apache/kylin/measure/raw/RawSerializerTest.java||core-metadata/src/test/java/org/apache/kylin/measure/raw/RawSerializerTest.java",
          "kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/monitor/MonitorExecutorExtension.scala||kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/monitor/MonitorExecutorExtension.scala",
          "kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/catalyst/expressions/ExpressionUtils.scala||kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/catalyst/expressions/ExpressionUtils.scala",
          "kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala||kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala",
          "kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/datasource/FilterExt.scala||kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/datasource/FilterExt.scala",
          "kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/hive/utils/QueryMetricUtils.scala||kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/hive/utils/QueryMetricUtils.scala",
          "kylin-spark-project/kylin-spark-metadata/src/main/spark24/org/apache/kylin/engine/spark/cross/CrossDateTimeUtils.scala||kylin-spark-project/kylin-spark-metadata/src/main/spark24/org/apache/kylin/engine/spark/cross/CrossDateTimeUtils.scala",
          "kylin-spark-project/kylin-spark-query/src/main/spark24/org/apache/spark/sql/hive/KylinHiveSessionStateBuilder.scala||kylin-spark-project/kylin-spark-query/src/main/spark24/org/apache/spark/sql/hive/KylinHiveSessionStateBuilder.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "build-engine/src/main/java/org/apache/kylin/engine/mr/common/DefaultSslProtocolSocketFactory.java||build-engine/src/main/java/org/apache/kylin/engine/mr/common/DefaultSslProtocolSocketFactory.java": [
          "File: build-engine/src/main/java/org/apache/kylin/engine/mr/common/DefaultSslProtocolSocketFactory.java -> build-engine/src/main/java/org/apache/kylin/engine/mr/common/DefaultSslProtocolSocketFactory.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "core-common/src/test/java/org/apache/kylin/common/util/ClassUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/ClassUtilTest.java": [
          "File: core-common/src/test/java/org/apache/kylin/common/util/ClassUtilTest.java -> core-common/src/test/java/org/apache/kylin/common/util/ClassUtilTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "core-cube/src/main/java/org/apache/kylin/cube/model/RowKeyColDesc.java||core-cube/src/main/java/org/apache/kylin/cube/model/RowKeyColDesc.java": [
          "File: core-cube/src/main/java/org/apache/kylin/cube/model/RowKeyColDesc.java -> core-cube/src/main/java/org/apache/kylin/cube/model/RowKeyColDesc.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "37: import org.apache.kylin.shaded.com.google.common.base.MoreObjects;",
          "38: import org.apache.kylin.shaded.com.google.common.base.Preconditions;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "40: import java.util.Objects;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "92:         if (DictionaryDimEnc.ENCODING_NAME.equals(encodingName) && cubeDesc.getConfig().isRowKeyEncodingAutoConvert()) {",
          "93:             if (type.isDate()) {",
          "94:                 encoding = encodingName = DateDimEnc.ENCODING_NAME;",
          "96:             }",
          "97:             if (type.isTimeFamily()) {",
          "98:                 encoding = encodingName = TimeDimEnc.ENCODING_NAME;",
          "100:             }",
          "101:         }",
          "",
          "[Removed Lines]",
          "95:                 logger.info(\"Implicitly convert encoding to {}\", encodingName);",
          "99:                 logger.info(\"Implicitly convert encoding to {}\", encodingName);",
          "",
          "[Added Lines]",
          "97:                 logger.debug(\"Implicitly convert encoding to {}\", encodingName);",
          "101:                 logger.debug(\"Implicitly convert encoding to {}\", encodingName);",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "192:         RowKeyColDesc that = (RowKeyColDesc) o;",
          "195:             return false;",
          "196:         }",
          "",
          "[Removed Lines]",
          "194:         if (column != null ? !column.equals(that.column) : that.column != null) {",
          "",
          "[Added Lines]",
          "196:         if (!Objects.equals(column, that.column)) {",
          "",
          "---------------"
        ],
        "core-metadata/src/test/java/org/apache/kylin/measure/raw/RawAggregatorTest.java||core-metadata/src/test/java/org/apache/kylin/measure/raw/RawAggregatorTest.java": [
          "File: core-metadata/src/test/java/org/apache/kylin/measure/raw/RawAggregatorTest.java -> core-metadata/src/test/java/org/apache/kylin/measure/raw/RawAggregatorTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import org.apache.kylin.common.util.ByteArray;",
          "27: import org.apache.kylin.common.util.BytesUtil;",
          "28: import org.junit.Test;",
          "30: public class RawAggregatorTest {",
          "31:     private RawAggregator agg = new RawAggregator();",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "28: import org.junit.Ignore;",
          "31: @Ignore",
          "",
          "---------------"
        ],
        "core-metadata/src/test/java/org/apache/kylin/measure/raw/RawSerializerTest.java||core-metadata/src/test/java/org/apache/kylin/measure/raw/RawSerializerTest.java": [
          "File: core-metadata/src/test/java/org/apache/kylin/measure/raw/RawSerializerTest.java -> core-metadata/src/test/java/org/apache/kylin/measure/raw/RawSerializerTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: import org.apache.kylin.metadata.datatype.DataType;",
          "31: import org.junit.AfterClass;",
          "32: import org.junit.BeforeClass;",
          "33: import org.junit.Test;",
          "35: public class RawSerializerTest extends LocalFileMetadataTestCase {",
          "36:     private static RawSerializer rawSerializer;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33: import org.junit.Ignore;",
          "36: @Ignore",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/monitor/MonitorExecutorExtension.scala||kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/monitor/MonitorExecutorExtension.scala": [
          "File: kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/monitor/MonitorExecutorExtension.scala -> kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/monitor/MonitorExecutorExtension.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/catalyst/expressions/ExpressionUtils.scala||kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/catalyst/expressions/ExpressionUtils.scala": [
          "File: kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/catalyst/expressions/ExpressionUtils.scala -> kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/catalyst/expressions/ExpressionUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala||kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala": [
          "File: kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala -> kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/datasource/FilterExt.scala||kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/datasource/FilterExt.scala": [
          "File: kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/datasource/FilterExt.scala -> kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/execution/datasource/FilterExt.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/hive/utils/QueryMetricUtils.scala||kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/hive/utils/QueryMetricUtils.scala": [
          "File: kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/hive/utils/QueryMetricUtils.scala -> kylin-spark-project/kylin-spark-common/src/main/spark24/org/apache/spark/sql/hive/utils/QueryMetricUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-metadata/src/main/spark24/org/apache/kylin/engine/spark/cross/CrossDateTimeUtils.scala||kylin-spark-project/kylin-spark-metadata/src/main/spark24/org/apache/kylin/engine/spark/cross/CrossDateTimeUtils.scala": [
          "File: kylin-spark-project/kylin-spark-metadata/src/main/spark24/org/apache/kylin/engine/spark/cross/CrossDateTimeUtils.scala -> kylin-spark-project/kylin-spark-metadata/src/main/spark24/org/apache/kylin/engine/spark/cross/CrossDateTimeUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-query/src/main/spark24/org/apache/spark/sql/hive/KylinHiveSessionStateBuilder.scala||kylin-spark-project/kylin-spark-query/src/main/spark24/org/apache/spark/sql/hive/KylinHiveSessionStateBuilder.scala": [
          "File: kylin-spark-project/kylin-spark-query/src/main/spark24/org/apache/spark/sql/hive/KylinHiveSessionStateBuilder.scala -> kylin-spark-project/kylin-spark-query/src/main/spark24/org/apache/spark/sql/hive/KylinHiveSessionStateBuilder.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    }
  ]
}