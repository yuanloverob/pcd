{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "8bd7d886e0570ed6d01ebbadca83c77821aee93f",
      "candidate_info": {
        "commit_hash": "8bd7d886e0570ed6d01ebbadca83c77821aee93f",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/8bd7d886e0570ed6d01ebbadca83c77821aee93f",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/numberFormatExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala"
        ],
        "message": "[SPARK-38796][SQL] Update to_number and try_to_number functions to restrict S and MI sequence to start or end only\n\n### What changes were proposed in this pull request?\n\nUpdate `to_number` and `try_to_number` functions to restrict MI sequence to start or end only.\n\nThis satisfies the following specification:\n\n```\nto_number(expr, fmt)\nfmt\n  { ' [ MI | S ] [ L | $ ]\n      [ 0 | 9 | G | , ] [...]\n      [ . | D ]\n      [ 0 | 9 ] [...]\n      [ L | $ ] [ PR | MI | S ] ' }\n```\n\n### Why are the changes needed?\n\nAfter reviewing the specification, this behavior makes the most sense.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, a slight change in the behavior of the format string.\n\n### How was this patch tested?\n\nExisting and updated unit test coverage.\n\nCloses #36154 from dtenedor/mi-anywhere.\n\nAuthored-by: Daniel Tenedorio <daniel.tenedorio@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 242ee22c00394c29e21bc3de0a93cb6d9746d93c)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/numberFormatExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/numberFormatExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/numberFormatExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/numberFormatExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/numberFormatExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/numberFormatExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "46:            grouping separator relevant for the size of the number.",
          "47:          '$': Specifies the location of the $ currency sign. This character may only be specified",
          "48:            once.",
          "51:          'PR': Only allowed at the end of the format string; specifies that 'expr' indicates a",
          "52:            negative number with wrapping angled brackets.",
          "53:            ('<1>').",
          "",
          "[Removed Lines]",
          "49:          'S': Specifies the position of a '-' or '+' sign (optional, only allowed once).",
          "50:          'MI': Specifies that 'expr' has an optional '-' sign, but no '+' (only allowed once).",
          "",
          "[Added Lines]",
          "49:          'S' or 'MI': Specifies the position of a '-' or '+' sign (optional, only allowed once at",
          "50:            the beginning or end of the format string). Note that 'S' allows '-' but 'MI' does not.",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ToNumberParser.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "49:   final val WRAPPING_ANGLE_BRACKETS_TO_NEGATIVE_NUMBER_END = 'R'",
          "53:   abstract class InputToken()",
          "55:   abstract class Digits extends InputToken",
          "79: }",
          "",
          "[Removed Lines]",
          "57:   case class ExactlyAsManyDigits(num: Int) extends Digits",
          "59:   case class AtMostAsManyDigits(num: Int) extends Digits",
          "61:   case class DecimalPoint() extends InputToken",
          "63:   case class ThousandsSeparator() extends InputToken",
          "66:   case class DigitGroups(tokens: Seq[InputToken], digits: Seq[Digits]) extends InputToken",
          "68:   case class DollarSign() extends InputToken",
          "70:   case class OptionalPlusOrMinusSign() extends InputToken",
          "72:   case class OptionalMinusSign() extends InputToken",
          "74:   case class OpeningAngleBracket() extends InputToken",
          "76:   case class ClosingAngleBracket() extends InputToken",
          "78:   case class InvalidUnrecognizedCharacter(char: Char) extends InputToken",
          "",
          "[Added Lines]",
          "58:   case class ExactlyAsManyDigits(num: Int) extends Digits {",
          "59:     override def toString: String = \"digit sequence\"",
          "60:   }",
          "62:   case class AtMostAsManyDigits(num: Int) extends Digits {",
          "63:     override def toString: String = \"digit sequence\"",
          "64:   }",
          "66:   case class DecimalPoint() extends InputToken {",
          "67:     override def toString: String = \". or D\"",
          "68:   }",
          "70:   case class ThousandsSeparator() extends InputToken {",
          "71:     override def toString: String = \", or G\"",
          "72:   }",
          "75:   case class DigitGroups(tokens: Seq[InputToken], digits: Seq[Digits]) extends InputToken {",
          "76:     override def toString: String = \"digit sequence\"",
          "77:   }",
          "79:   case class DollarSign() extends InputToken {",
          "80:     override def toString: String = \"$\"",
          "81:   }",
          "83:   case class OptionalPlusOrMinusSign() extends InputToken {",
          "84:     override def toString: String = \"S\"",
          "85:   }",
          "87:   case class OptionalMinusSign() extends InputToken {",
          "88:     override def toString: String = \"MI\"",
          "89:   }",
          "91:   case class OpeningAngleBracket() extends InputToken {",
          "92:     override def toString: String = \"PR\"",
          "93:   }",
          "95:   case class ClosingAngleBracket() extends InputToken {",
          "96:     override def toString: String = \"PR\"",
          "97:   }",
          "99:   case class InvalidUnrecognizedCharacter(char: Char) extends InputToken {",
          "100:     override def toString: String = s\"character '$char''\"",
          "101:   }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "243:   private def validateFormatString: String = {",
          "254:     val firstDollarSignIndex: Int = formatTokens.indexOf(DollarSign())",
          "255:     val firstDigitIndex: Int = formatTokens.indexWhere {",
          "256:       case _: DigitGroups => true",
          "",
          "[Removed Lines]",
          "244:     def multipleSignInNumberFormatError(message: String) = {",
          "245:       s\"At most one $message is allowed in the number format: '$numberFormat'\"",
          "246:     }",
          "248:     def notAtEndOfNumberFormatError(message: String) = {",
          "249:       s\"$message must be at the end of the number format: '$numberFormat'\"",
          "250:     }",
          "252:     val inputTokenCounts = formatTokens.groupBy(identity).mapValues(_.size)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "278:     if (numberFormat.isEmpty) {",
          "291:     }",
          "294:       token => token.isInstanceOf[DigitGroups])) {",
          "318:     }",
          "322:     }",
          "325:       firstDecimalPointIndex < firstDollarSignIndex) {",
          "327:         s\"number format: '$numberFormat'\"",
          "328:     }",
          "331:       case DigitGroups(tokens, _) =>",
          "332:         tokens.zipWithIndex.exists({",
          "333:           case (_: ThousandsSeparator, j: Int) if j == 0 || j == tokens.length - 1 =>",
          "",
          "[Removed Lines]",
          "279:       \"The format string cannot be empty\"",
          "280:     }",
          "282:     else if (formatTokens.exists(_.isInstanceOf[InvalidUnrecognizedCharacter])) {",
          "283:       val unrecognizedChars =",
          "284:         formatTokens.filter {",
          "285:           _.isInstanceOf[InvalidUnrecognizedCharacter]",
          "286:         }.map {",
          "287:           case i: InvalidUnrecognizedCharacter => i.char",
          "288:         }",
          "289:       val char: Char = unrecognizedChars.head",
          "290:       s\"Encountered invalid character $char in the number format: '$numberFormat'\"",
          "293:     else if (!formatTokens.exists(",
          "295:       \"The format string requires at least one number digit\"",
          "296:     }",
          "298:     else if (inputTokenCounts.getOrElse(DecimalPoint(), 0) > 1) {",
          "299:       multipleSignInNumberFormatError(s\"'$POINT_LETTER' or '$POINT_SIGN'\")",
          "300:     }",
          "302:     else if (inputTokenCounts.getOrElse(OptionalPlusOrMinusSign(), 0) > 1) {",
          "303:       multipleSignInNumberFormatError(s\"'$OPTIONAL_PLUS_OR_MINUS_LETTER'\")",
          "304:     }",
          "306:     else if (inputTokenCounts.getOrElse(DollarSign(), 0) > 1) {",
          "307:       multipleSignInNumberFormatError(s\"'$DOLLAR_SIGN'\")",
          "308:     }",
          "310:     else if (inputTokenCounts.getOrElse(OptionalMinusSign(), 0) > 1) {",
          "311:       multipleSignInNumberFormatError(s\"'$OPTIONAL_MINUS_STRING'\")",
          "312:     }",
          "314:     else if (inputTokenCounts.getOrElse(ClosingAngleBracket(), 0) > 1 ||",
          "315:       (inputTokenCounts.getOrElse(ClosingAngleBracket(), 0) == 1 &&",
          "316:         formatTokens.last != ClosingAngleBracket())) {",
          "317:       notAtEndOfNumberFormatError(s\"'$WRAPPING_ANGLE_BRACKETS_TO_NEGATIVE_NUMBER'\")",
          "320:     else if (firstDigitIndex < firstDollarSignIndex) {",
          "321:       s\"Currency characters must appear before digits in the number format: '$numberFormat'\"",
          "324:     else if (firstDecimalPointIndex != -1 &&",
          "326:       \"Currency characters must appear before any decimal point in the \" +",
          "330:     else if (digitGroupsBeforeDecimalPoint.exists {",
          "",
          "[Added Lines]",
          "292:       return \"The format string cannot be empty\"",
          "295:     if (!formatTokens.exists(",
          "297:       return \"The format string requires at least one number digit\"",
          "300:     if (firstDigitIndex < firstDollarSignIndex) {",
          "301:       return s\"Currency characters must appear before digits in the number format: '$numberFormat'\"",
          "304:     if (firstDecimalPointIndex != -1 &&",
          "306:       return \"Currency characters must appear before any decimal point in the \" +",
          "310:     if (digitGroupsBeforeDecimalPoint.exists {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "340:             false",
          "341:         })",
          "342:     }) {",
          "344:         s\"in the number format: '$numberFormat'\"",
          "345:     }",
          "348:       case DigitGroups(tokens, digits) =>",
          "349:         tokens.length > digits.length",
          "350:     }) {",
          "352:         s\"in the number format: '$numberFormat'\"",
          "353:     }",
          "357:     }",
          "358:   }",
          "",
          "[Removed Lines]",
          "343:       \"Thousands separators (,) must have digits in between them \" +",
          "347:     else if (digitGroupsAfterDecimalPoint.exists {",
          "351:       \"Thousands separators (,) may not appear after the decimal point \" +",
          "355:     else {",
          "356:       \"\"",
          "",
          "[Added Lines]",
          "323:       return \"Thousands separators (,) must have digits in between them \" +",
          "327:     if (digitGroupsAfterDecimalPoint.exists {",
          "331:       return \"Thousands separators (,) may not appear after the decimal point \" +",
          "335:     val inputTokenCounts = formatTokens.groupBy(identity).mapValues(_.size)",
          "336:     Seq(DecimalPoint(),",
          "337:       OptionalPlusOrMinusSign(),",
          "338:       OptionalMinusSign(),",
          "339:       DollarSign(),",
          "340:       ClosingAngleBracket()).foreach {",
          "341:       token => if (inputTokenCounts.getOrElse(token, 0) > 1) {",
          "342:         return s\"At most one ${token.toString} is allowed in the number format: '$numberFormat'\"",
          "343:       }",
          "344:     }",
          "351:     val allowedFormatTokens: Seq[Seq[InputToken]] = Seq(",
          "352:       Seq(OpeningAngleBracket()),",
          "353:       Seq(OptionalMinusSign(), OptionalPlusOrMinusSign()),",
          "354:       Seq(DollarSign()),",
          "355:       Seq(DigitGroups(Seq(), Seq())),",
          "356:       Seq(DecimalPoint()),",
          "357:       Seq(DigitGroups(Seq(), Seq())),",
          "358:       Seq(DollarSign()),",
          "359:       Seq(OptionalMinusSign(), OptionalPlusOrMinusSign(), ClosingAngleBracket())",
          "360:     )",
          "361:     var formatTokenIndex = 0",
          "362:     for (allowedTokens: Seq[InputToken] <- allowedFormatTokens) {",
          "363:       def tokensMatch(lhs: InputToken, rhs: InputToken): Boolean = {",
          "364:         lhs match {",
          "365:           case _: DigitGroups => rhs.isInstanceOf[DigitGroups]",
          "366:           case _ => lhs == rhs",
          "367:         }",
          "368:       }",
          "369:       if (formatTokenIndex < formatTokens.length &&",
          "370:         allowedTokens.exists(tokensMatch(_, formatTokens(formatTokenIndex)))) {",
          "371:         formatTokenIndex += 1",
          "372:       }",
          "374:     if (formatTokenIndex < formatTokens.length) {",
          "375:       return s\"Unexpected ${formatTokens(formatTokenIndex).toString} found in the format string \" +",
          "376:         s\"'$numberFormat'; the structure of the format string must match: \" +",
          "377:         \"[MI|S] [$] [0|9|G|,]* [.|D] [0|9]* [$] [PR|MI|S]\"",
          "378:     }",
          "380:     \"\"",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "972:       (\"<454>\", \"999PR\") -> Decimal(-454),",
          "973:       (\"454-\", \"999MI\") -> Decimal(-454),",
          "974:       (\"-$54\", \"MI$99\") -> Decimal(-54),",
          "977:       (\"123,456,789,123,456,789,123\", \"999,999,999,999,999,999,999\") ->",
          "978:         Decimal(new JavaBigDecimal(\"123456789123456789123\"))",
          "",
          "[Removed Lines]",
          "975:       (\"$4-4\", \"$9MI9\") -> Decimal(-44),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1009:   }",
          "1011:   test(\"ToNumber: negative tests (the format string is invalid)\") {",
          "1013:     val thousandsSeparatorDigitsBetween =",
          "1014:       \"Thousands separators (,) must have digits in between them\"",
          "1015:     val mustBeAtEnd = \"must be at the end of the number format\"",
          "",
          "[Removed Lines]",
          "1012:     val invalidCharacter = \"Encountered invalid character\"",
          "",
          "[Added Lines]",
          "1011:     val unexpectedCharacter = \"the structure of the format string must match: \" +",
          "1012:       \"[MI|S] [$] [0|9|G|,]* [.|D] [0|9]* [$] [PR|MI|S]\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1019:       (\"454\", \"\") -> \"The format string cannot be empty\",",
          "1025:       (\"454\", \"$\") -> \"The format string requires at least one number digit\",",
          "1027:       (\"454\", \"99.99.99\") -> atMostOne,",
          "1029:       (\"454\", \"$$99\") -> atMostOne,",
          "1031:       (\"--$54\", \"SS$99\") -> atMostOne,",
          "1032:       (\"-$54\", \"MI$99MI\") -> atMostOne,",
          "1033:       (\"$4-4\", \"$9MI9MI\") -> atMostOne,",
          "1039:       (\"4$54\", \"9$99\") -> \"Currency characters must appear before digits\",",
          "",
          "[Removed Lines]",
          "1021:       (\"454\", \"999@\") -> invalidCharacter,",
          "1022:       (\"454\", \"999M\") -> invalidCharacter,",
          "1023:       (\"454\", \"999P\") -> invalidCharacter,",
          "1035:       (\"<$45>\", \"PR$99\") -> mustBeAtEnd,",
          "1036:       (\"$4<4>\", \"$9PR9\") -> mustBeAtEnd,",
          "1037:       (\"<<454>>\", \"999PRPR\") -> mustBeAtEnd,",
          "",
          "[Added Lines]",
          "1021:       (\"454\", \"999@\") -> unexpectedCharacter,",
          "1022:       (\"454\", \"999M\") -> unexpectedCharacter,",
          "1023:       (\"454\", \"999P\") -> unexpectedCharacter,",
          "1031:       (\"$4-4\", \"$9MI9\") -> unexpectedCharacter,",
          "1032:       (\"--4\", \"SMI9\") -> unexpectedCharacter,",
          "1037:       (\"<$45>\", \"PR$99\") -> unexpectedCharacter,",
          "1038:       (\"$4<4>\", \"$9PR9\") -> unexpectedCharacter,",
          "1039:       (\"<<454>>\", \"999PRPR\") -> atMostOne,",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d9dc28075bcf6e4c6756418ae872fc8db36867f2",
      "candidate_info": {
        "commit_hash": "d9dc28075bcf6e4c6756418ae872fc8db36867f2",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/d9dc28075bcf6e4c6756418ae872fc8db36867f2",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala",
          "sql/core/src/test/resources/sql-tests/inputs/charvarchar.sql",
          "sql/core/src/test/resources/sql-tests/results/charvarchar.sql.out"
        ],
        "message": "[SPARK-40213][SQL] Support ASCII value conversion for Latin-1 characters\n\n### What changes were proposed in this pull request?\nThis PR proposes to support ASCII value conversion for Latin-1 Supplement characters.\n\n### Why are the changes needed?\n`ascii()` should be the inverse of `chr()`. But for latin-1 char, we get incorrect ascii value. For example:\n```sql\nselect ascii('\u00a7') -- output: -62, expect: 167\nselect chr(167) -- output: '\u00a7'\n```\n\n### Does this PR introduce _any_ user-facing change?\nYes, fixes the incorrect ASCII conversion for Latin-1 Supplement characters\n\n### How was this patch tested?\nUT\n\nCloses #37651 from linhongliu-db/SPARK-40213.\n\nAuthored-by: Linhong Liu <linhong.liu@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit c07852380471f02955d6d17cddb3150231daa71f)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala",
          "sql/core/src/test/resources/sql-tests/inputs/charvarchar.sql||sql/core/src/test/resources/sql-tests/inputs/charvarchar.sql"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2335:   override def inputTypes: Seq[DataType] = Seq(StringType)",
          "2337:   protected override def nullSafeEval(string: Any): Any = {",
          "2341:     } else {",
          "2342:       0",
          "2343:     }",
          "",
          "[Removed Lines]",
          "2338:     val bytes = string.asInstanceOf[UTF8String].getBytes",
          "2339:     if (bytes.length > 0) {",
          "2340:       bytes(0).asInstanceOf[Int]",
          "",
          "[Added Lines]",
          "2339:     val firstCharStr = string.asInstanceOf[UTF8String].substring(0, 1)",
          "2340:     if (firstCharStr.numChars > 0) {",
          "2341:       firstCharStr.toString.codePointAt(0)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2346:   override def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {",
          "2347:     nullSafeCodeGen(ctx, ev, (child) => {",
          "2349:       s\"\"\"",
          "2353:         } else {",
          "2354:           ${ev.value} = 0;",
          "2355:         }",
          "",
          "[Removed Lines]",
          "2348:       val bytes = ctx.freshName(\"bytes\")",
          "2350:         byte[] $bytes = $child.getBytes();",
          "2351:         if ($bytes.length > 0) {",
          "2352:           ${ev.value} = (int) $bytes[0];",
          "",
          "[Added Lines]",
          "2349:       val firstCharStr = ctx.freshName(\"firstCharStr\")",
          "2351:         UTF8String $firstCharStr = $child.substring(0, 1);",
          "2352:         if ($firstCharStr.numChars() > 0) {",
          "2353:           ${ev.value} = $firstCharStr.toString().codePointAt(0);",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/StringExpressionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "307:       SubstringIndex(Literal(\"www||apache||org\"), Literal( \"||\"), Literal(2)), \"www||apache\")",
          "308:   }",
          "310:   test(\"ascii for string\") {",
          "311:     val a = 'a.string.at(0)",
          "312:     checkEvaluation(Ascii(Literal(\"efg\")), 101, create_row(\"abdef\"))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "310:   test(\"SPARK-40213: ascii for Latin-1 Supplement characters\") {",
          "312:     checkEvaluation(Ascii(Literal(\"\u00a5\")), 165, create_row(\"\u00a5\"))",
          "313:     checkEvaluation(Ascii(Literal(\"\u00ae\")), 174, create_row(\"\u00ae\"))",
          "314:     checkEvaluation(Ascii(Literal(\"\u00a9\")), 169, create_row(\"\u00a9\"))",
          "316:     (128 until 256).foreach { c =>",
          "317:       checkEvaluation(Ascii(Chr(Literal(c.toLong))), c, create_row(c.toLong))",
          "318:     }",
          "319:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/charvarchar.sql||sql/core/src/test/resources/sql-tests/inputs/charvarchar.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/charvarchar.sql -> sql/core/src/test/resources/sql-tests/inputs/charvarchar.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "113: drop table char_tbl2;",
          "114: drop table char_tbl3;",
          "115: drop table char_tbl4;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "117: -- ascii value for Latin-1 Supplement characters",
          "118: select ascii('\u00a7'), ascii('\u00f7'), ascii('\u00d710');",
          "119: select chr(167), chr(247), chr(215);",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c3a171d9875b517b9cb9286db2249cc60c96ade4",
      "candidate_info": {
        "commit_hash": "c3a171d9875b517b9cb9286db2249cc60c96ade4",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/c3a171d9875b517b9cb9286db2249cc60c96ade4",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/ScalaReflectionSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoderSuite.scala"
        ],
        "message": "[SPARK-38681][SQL] Support nested generic case classes\n\n### What changes were proposed in this pull request?\n\nMaster and branch-3.3 will fail to derive schema for case classes with generic parameters if the parameter was not used directly as a field, but instead pass on as a generic parameter to another type. e.g.\n```\ncase class NestedGeneric[T](\n  generic: GenericData[T])\n```\n\nThis is a regression from the latest release of 3.2.1 where this works as expected.\n\n### Why are the changes needed?\nSupport more general case classes that user might have.\n\n### Does this PR introduce _any_ user-facing change?\nBetter support for generic case classes.\n\n### How was this patch tested?\nNew specs in ScalaReflectionSuite and ExpressionEncoderSuite. All the new test cases that does not use value classes pass if added to the 3.2 branch\n\nCloses #36004 from eejbyfeldt/SPARK-38681-nested-generic.\n\nAuthored-by: Emil Ejbyfeldt <eejbyfeldt@liveintent.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n(cherry picked from commit 49c68020e702f9258f3c693f446669bea66b12f4)\nSigned-off-by: Sean Owen <srowen@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/ScalaReflectionSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/ScalaReflectionSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoderSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoderSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "964:   }",
          "966:   private def isValueClass(tpe: Type): Boolean = {",
          "972:   }",
          "",
          "[Removed Lines]",
          "967:     tpe.typeSymbol.asClass.isDerivedValueClass",
          "968:   }",
          "970:   private def isTypeParameter(tpe: Type): Boolean = {",
          "971:     tpe.typeSymbol.isParameter",
          "",
          "[Added Lines]",
          "967:     tpe.typeSymbol.isClass && tpe.typeSymbol.asClass.isDerivedValueClass",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "989:     val params = constructParams(dealiasedTpe)",
          "990:     params.map { p =>",
          "991:       val paramTpe = p.typeSignature",
          "998:         p.name.decodedName.toString -> getUnderlyingTypeOfValueClass(paramTpe)",
          "999:       } else {",
          "1001:       }",
          "1002:     }",
          "1003:   }",
          "",
          "[Removed Lines]",
          "992:       if (isTypeParameter(paramTpe)) {",
          "995:         p.name.decodedName.toString -> paramTpe.substituteTypes(formalTypeArgs, actualTypeArgs)",
          "996:       } else if (isValueClass(paramTpe)) {",
          "1000:         p.name.decodedName.toString -> paramTpe",
          "",
          "[Added Lines]",
          "988:       if (isValueClass(paramTpe)) {",
          "992:         p.name.decodedName.toString -> paramTpe.substituteTypes(formalTypeArgs, actualTypeArgs)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/ScalaReflectionSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/ScalaReflectionSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/ScalaReflectionSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/ScalaReflectionSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "81:   type IntData = GenericData[Int]",
          "82: }",
          "84: case class MultipleConstructorsData(a: Int, b: String, c: Double) {",
          "85:   def this(b: String, a: Int) = this(a, b, c = 1.0)",
          "86: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "84: case class NestedGeneric[T](",
          "85:   generic: GenericData[T])",
          "87: case class SeqNestedGeneric[T](",
          "88:   generic: Seq[T])",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "295:       nullable = true))",
          "296:   }",
          "298:   test(\"tuple data\") {",
          "299:     val schema = schemaFor[(Int, String)]",
          "300:     assert(schema === Schema(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "305:   test(\"SPARK-38681: Nested generic data\") {",
          "306:     val schema = schemaFor[NestedGeneric[Int]]",
          "307:     assert(schema === Schema(",
          "308:       StructType(Seq(",
          "309:         StructField(",
          "310:           \"generic\",",
          "311:           StructType(Seq(",
          "312:             StructField(\"genericField\", IntegerType, nullable = false))),",
          "313:           nullable = true))),",
          "314:       nullable = true))",
          "315:   }",
          "317:   test(\"SPARK-38681: List nested generic\") {",
          "318:     val schema = schemaFor[SeqNestedGeneric[Int]]",
          "319:     assert(schema === Schema(",
          "320:       StructType(Seq(",
          "321:         StructField(",
          "322:           \"generic\",",
          "323:           ArrayType(IntegerType, false),",
          "324:           nullable = true))),",
          "325:       nullable = true))",
          "326:   }",
          "328:   test(\"SPARK-38681: List nested generic with value class\") {",
          "329:     val schema = schemaFor[SeqNestedGeneric[IntWrapper]]",
          "330:     assert(schema === Schema(",
          "331:       StructType(Seq(",
          "332:         StructField(",
          "333:           \"generic\",",
          "334:           ArrayType(StructType(Seq(StructField(\"i\", IntegerType, false))), true),",
          "335:           nullable = true))),",
          "336:       nullable = true))",
          "337:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoderSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoderSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoderSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoderSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "131: case class MapOfValueClassValue(m: Map[String, StringWrapper])",
          "132: case class OptionOfValueClassValue(o: Option[StringWrapper])",
          "133: case class CaseClassWithGeneric[T](generic: T, value: IntWrapper)",
          "135: class ExpressionEncoderSuite extends CodegenInterpretedPlanTest with AnalysisTest {",
          "136:   OuterScopes.addOuterScope(this)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "134: case class NestedGeneric[T](generic: CaseClassWithGeneric[T])",
          "135: case class SeqNestedGeneric[T](list: Seq[T])",
          "136: case class OptionNestedGeneric[T](list: Option[T])",
          "137: case class MapNestedGenericKey[T](list: Map[T, Int])",
          "138: case class MapNestedGenericValue[T](list: Map[Int, T])",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "454:     \"nested tuple._2 of class value\")",
          "455:   encodeDecodeTest(CaseClassWithGeneric(IntWrapper(1), IntWrapper(2)),",
          "456:     \"case class with value class in generic parameter\")",
          "458:   encodeDecodeTest(Option(31), \"option of int\")",
          "459:   encodeDecodeTest(Option.empty[Int], \"empty option of int\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "462:   encodeDecodeTest(NestedGeneric(CaseClassWithGeneric(IntWrapper(1), IntWrapper(2))),",
          "463:     \"case class with nested generic parameter\")",
          "464:   encodeDecodeTest(SeqNestedGeneric(List(2)),",
          "465:     \"case class with nested generic parameter seq\")",
          "466:   encodeDecodeTest(SeqNestedGeneric(List(IntWrapper(2))),",
          "467:     \"case class with value class and nested generic parameter seq\")",
          "468:   encodeDecodeTest(OptionNestedGeneric(Some(2)),",
          "469:     \"case class with nested generic option\")",
          "470:   encodeDecodeTest(MapNestedGenericKey(Map(1 -> 2)),",
          "471:     \"case class with nested generic map key \")",
          "472:   encodeDecodeTest(MapNestedGenericValue(Map(1 -> 2)),",
          "473:     \"case class with nested generic map value\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0a180c0637e352e6a00a9b67d4f5d261f851ea5f",
      "candidate_info": {
        "commit_hash": "0a180c0637e352e6a00a9b67d4f5d261f851ea5f",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/0a180c0637e352e6a00a9b67d4f5d261f851ea5f",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala"
        ],
        "message": "[SPARK-40292][SQL] Fix column names in \"arrays_zip\" function when arrays are referenced from nested structs\n\n### What changes were proposed in this pull request?\n\nThis PR fixes an issue in `arrays_zip` function where a field index was used as a column name in the resulting schema which was a regression from Spark 3.1. With this change, the original behaviour is restored: a corresponding struct field name will be used instead of a field index.\n\nExample:\n```sql\nwith q as (\n  select\n    named_struct(\n      'my_array', array(1, 2, 3),\n      'my_array2', array(4, 5, 6)\n    ) as my_struct\n)\nselect\n  arrays_zip(my_struct.my_array, my_struct.my_array2)\nfrom\n  q\n```\n\nwould return schema:\n```\nroot\n |-- arrays_zip(my_struct.my_array, my_struct.my_array2): array (nullable = false)\n |    |-- element: struct (containsNull = false)\n |    |    |-- 0: integer (nullable = true)\n |    |    |-- 1: integer (nullable = true)\n```\n\nwhich is somewhat inaccurate. PR adds handling of `GetStructField` expression to return the struct field names like this:\n```\nroot\n |-- arrays_zip(my_struct.my_array, my_struct.my_array2): array (nullable = false)\n |    |-- element: struct (containsNull = false)\n |    |    |-- my_array: integer (nullable = true)\n |    |    |-- my_array2: integer (nullable = true)\n```\n\n### Why are the changes needed?\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, `arrays_zip` function returns struct field names now as in Spark 3.1 instead of field indices.\nSome users might have worked around this issue so this patch would affect them by bringing back the original behaviour.\n\n### How was this patch tested?\n\nExisting unit tests. I also added a test case that reproduces the problem.\n\nCloses #37833 from sadikovi/SPARK-40292.\n\nAuthored-by: Ivan Sadikov <ivan.sadikov@databricks.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>\n(cherry picked from commit 443eea97578c41870c343cdb88cf69bfdf27033a)\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "267:         case (u: UnresolvedAttribute, _) => Literal(u.nameParts.last)",
          "268:         case (e: NamedExpression, _) if e.resolved => Literal(e.name)",
          "269:         case (e: NamedExpression, _) => NamePlaceholder",
          "270:         case (_, idx) => Literal(idx.toString)",
          "271:       })",
          "272:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "270:         case (e: GetStructField, _) => Literal(e.extractFieldName)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "721:     }",
          "722:   }",
          "724:   def testSizeOfMap(sizeOfNull: Any): Unit = {",
          "725:     val df = Seq(",
          "726:       (Map[Int, Int](1 -> 1, 2 -> 2), \"x\"),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "724:   test(\"SPARK-40292: arrays_zip should retain field names in nested structs\") {",
          "725:     val df = spark.sql(\"\"\"",
          "726:       select",
          "727:         named_struct(",
          "728:           'arr_1', array(named_struct('a', 1, 'b', 2)),",
          "729:           'arr_2', array(named_struct('p', 1, 'q', 2)),",
          "730:           'field', named_struct(",
          "731:             'arr_3', array(named_struct('x', 1, 'y', 2))",
          "732:           )",
          "733:         ) as obj",
          "734:       \"\"\")",
          "736:     val res = df.selectExpr(\"arrays_zip(obj.arr_1, obj.arr_2, obj.field.arr_3) as arr\")",
          "738:     val fieldNames = res.schema.head.dataType.asInstanceOf[ArrayType]",
          "739:       .elementType.asInstanceOf[StructType].fieldNames",
          "740:     assert(fieldNames.toSeq === Seq(\"arr_1\", \"arr_2\", \"arr_3\"))",
          "741:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "3f969ada5fecddab272f2abbc849d2591f30f44c",
      "candidate_info": {
        "commit_hash": "3f969ada5fecddab272f2abbc849d2591f30f44c",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/3f969ada5fecddab272f2abbc849d2591f30f44c",
        "files": [
          "core/src/test/scala/org/apache/spark/util/JsonProtocolSuite.scala"
        ],
        "message": "[SPARK-39676][CORE][TESTS] Add task partition id for TaskInfo assertEquals method in JsonProtocolSuite\n\n### What changes were proposed in this pull request?\n\nIn https://github.com/apache/spark/pull/35185 , task partition id was added in taskInfo. And, JsonProtocolSuite#assertEquals about TaskInfo doesn't have partitionId.\n\n### Why are the changes needed?\n\nShould assert partitionId equals or not.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nNo need to add unit test.\n\nCloses #37081 from dcoliversun/SPARK-39676.\n\nAuthored-by: Qian.Sun <qian.sun2020@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "core/src/test/scala/org/apache/spark/util/JsonProtocolSuite.scala||core/src/test/scala/org/apache/spark/util/JsonProtocolSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/test/scala/org/apache/spark/util/JsonProtocolSuite.scala||core/src/test/scala/org/apache/spark/util/JsonProtocolSuite.scala": [
          "File: core/src/test/scala/org/apache/spark/util/JsonProtocolSuite.scala -> core/src/test/scala/org/apache/spark/util/JsonProtocolSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "790:     assert(info1.taskId === info2.taskId)",
          "791:     assert(info1.index === info2.index)",
          "792:     assert(info1.attemptNumber === info2.attemptNumber)",
          "793:     assert(info1.launchTime === info2.launchTime)",
          "794:     assert(info1.executorId === info2.executorId)",
          "795:     assert(info1.host === info2.host)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "794:     assert(info1.partitionId === info2.partitionId)",
          "",
          "---------------"
        ]
      }
    }
  ]
}