{
  "cve_id": "CVE-2022-40754",
  "cve_desc": "In Apache Airflow 2.3.0 through 2.3.4, there was an open redirect in the webserver's `/confirm` endpoint.",
  "repo": "apache/airflow",
  "patch_hash": "3871f00230b7064e6234aa65b8a00ee2b6d33e4c",
  "patch_info": {
    "commit_hash": "3871f00230b7064e6234aa65b8a00ee2b6d33e4c",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/3871f00230b7064e6234aa65b8a00ee2b6d33e4c",
    "files": [
      "airflow/www/views.py"
    ],
    "message": "Fix UI redirect (#26409)\n\nCo-authored-by: Konstantin Weddige <konstantin.weddige@lutrasecurity.com>\n(cherry picked from commit 56e7555c42f013f789a4b718676ff09b4a9d5135)",
    "before_after_code_files": [
      "airflow/www/views.py||airflow/www/views.py"
    ]
  },
  "patch_diff": {
    "airflow/www/views.py||airflow/www/views.py": [
      "File: airflow/www/views.py -> airflow/www/views.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "2329:         task_id = args.get('task_id')",
      "2330:         dag_run_id = args.get('dag_run_id')",
      "2331:         state = args.get('state')",
      "2334:         if 'map_index' not in args:",
      "2335:             map_indexes: list[int] | None = None",
      "",
      "[Removed Lines]",
      "2332:         origin = args.get('origin')",
      "",
      "[Added Lines]",
      "2332:         origin = get_safe_url(args.get('origin'))",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "a10fc9aefee0054effb2ccb24717d90bad53c6a9",
      "candidate_info": {
        "commit_hash": "a10fc9aefee0054effb2ccb24717d90bad53c6a9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a10fc9aefee0054effb2ccb24717d90bad53c6a9",
        "files": [
          "airflow/example_dags/example_branch_day_of_week_operator.py",
          "airflow/operators/weekday.py",
          "airflow/sensors/weekday.py"
        ],
        "message": "Add more weekday operator and sensor examples #26071 (#26098)\n\n(cherry picked from commit dd6b2e4e6cb89d9eea2f3db790cb003a2e89aeff)",
        "before_after_code_files": [
          "airflow/example_dags/example_branch_day_of_week_operator.py||airflow/example_dags/example_branch_day_of_week_operator.py",
          "airflow/operators/weekday.py||airflow/operators/weekday.py",
          "airflow/sensors/weekday.py||airflow/sensors/weekday.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/example_dags/example_branch_day_of_week_operator.py||airflow/example_dags/example_branch_day_of_week_operator.py": [
          "File: airflow/example_dags/example_branch_day_of_week_operator.py -> airflow/example_dags/example_branch_day_of_week_operator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: from airflow import DAG",
          "25: from airflow.operators.empty import EmptyOperator",
          "26: from airflow.operators.weekday import BranchDayOfWeekOperator",
          "28: with DAG(",
          "29:     dag_id=\"example_weekday_branch_operator\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: from airflow.utils.weekday import WeekDay",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "35:     # [START howto_operator_day_of_week_branch]",
          "36:     empty_task_1 = EmptyOperator(task_id='branch_true')",
          "37:     empty_task_2 = EmptyOperator(task_id='branch_false')",
          "39:     branch = BranchDayOfWeekOperator(",
          "40:         task_id=\"make_choice\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "39:     empty_task_3 = EmptyOperator(task_id='branch_weekend')",
          "40:     empty_task_4 = EmptyOperator(task_id='branch_mid_week')",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "42:         follow_task_ids_if_false=\"branch_false\",",
          "43:         week_day=\"Monday\",",
          "44:     )",
          "47:     branch >> [empty_task_1, empty_task_2]",
          "48:     # [END howto_operator_day_of_week_branch]",
          "",
          "[Removed Lines]",
          "46:     # Run empty_task_1 if branch executes on Monday",
          "",
          "[Added Lines]",
          "48:     branch_weekend = BranchDayOfWeekOperator(",
          "49:         task_id=\"make_weekend_choice\",",
          "50:         follow_task_ids_if_true=\"branch_weekend\",",
          "51:         follow_task_ids_if_false=\"branch_mid_week\",",
          "52:         week_day={WeekDay.SATURDAY, WeekDay.SUNDAY},",
          "53:     )",
          "55:     # Run empty_task_1 if branch executes on Monday, empty_task_2 otherwise",
          "57:     # Run empty_task_3 if it's a weekend, empty_task_4 otherwise",
          "58:     empty_task_2 >> branch_weekend >> [empty_task_3, empty_task_4]",
          "",
          "---------------"
        ],
        "airflow/operators/weekday.py||airflow/operators/weekday.py": [
          "File: airflow/operators/weekday.py -> airflow/operators/weekday.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31:     For more information on how to use this operator, take a look at the guide:",
          "32:     :ref:`howto/operator:BranchDayOfWeekOperator`",
          "34:     :param follow_task_ids_if_true: task id or task ids to follow if criteria met",
          "35:     :param follow_task_ids_if_false: task id or task ids to follow if criteria does not met",
          "36:     :param week_day: Day of the week to check (full name). Optionally, a set",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "36:         from airflow.operators.empty import EmptyOperator",
          "38:         monday = EmptyOperator(task_id='monday')",
          "39:         other_day = EmptyOperator(task_id='other_day')",
          "41:         monday_check = DayOfWeekSensor(",
          "42:             task_id='monday_check',",
          "43:             week_day='Monday',",
          "44:             use_task_logical_date=True,",
          "45:             follow_task_ids_if_true='monday',",
          "46:             follow_task_ids_if_false='other_day',",
          "47:             dag=dag)",
          "48:         monday_check >> [monday, other_day]",
          "52:         # import WeekDay Enum",
          "53:         from airflow.utils.weekday import WeekDay",
          "54:         from airflow.operators.empty import EmptyOperator",
          "56:         workday = EmptyOperator(task_id='workday')",
          "57:         weekend = EmptyOperator(task_id='weekend')",
          "58:         weekend_check = BranchDayOfWeekOperator(",
          "59:             task_id='weekend_check',",
          "60:             week_day={WeekDay.SATURDAY, WeekDay.SUNDAY},",
          "61:             use_task_logical_date=True,",
          "62:             follow_task_ids_if_true='weekend',",
          "63:             follow_task_ids_if_false='workday',",
          "64:             dag=dag)",
          "65:         # add downstream dependencies as you would do with any branch operator",
          "66:         weekend_check >> [workday, weekend]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "45:     :param use_task_logical_date: If ``True``, uses task's logical date to compare",
          "46:         with is_today. Execution Date is Useful for backfilling.",
          "47:         If ``False``, uses system's day of the week.",
          "48:     \"\"\"",
          "50:     def __init__(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "79:         To use `WeekDay` enum, import it from `airflow.utils.weekday`",
          "84:     :param use_task_execution_day: deprecated parameter, same effect as `use_task_logical_date`",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "53:         follow_task_ids_if_true: Union[str, Iterable[str]],",
          "54:         follow_task_ids_if_false: Union[str, Iterable[str]],",
          "56:         use_task_logical_date: bool = False,",
          "57:         use_task_execution_day: bool = False,",
          "",
          "[Removed Lines]",
          "55:         week_day: Union[str, Iterable[str]],",
          "",
          "[Added Lines]",
          "92:         week_day: Union[str, Iterable[str], WeekDay, Iterable[WeekDay]],",
          "",
          "---------------"
        ],
        "airflow/sensors/weekday.py||airflow/sensors/weekday.py": [
          "File: airflow/sensors/weekday.py -> airflow/sensors/weekday.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: import warnings",
          "20: from airflow.exceptions import RemovedInAirflow3Warning",
          "21: from airflow.sensors.base import BaseSensorOperator",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: from typing import Iterable, Union",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "69:     :param use_task_logical_date: If ``True``, uses task's logical date to compare",
          "70:         with week_day. Execution Date is Useful for backfilling.",
          "71:         If ``False``, uses system's day of the week. Useful when you",
          "72:         don't want to run anything on weekdays on the system.",
          "73:     \"\"\"",
          "76:         super().__init__(**kwargs)",
          "77:         self.week_day = week_day",
          "78:         self.use_task_logical_date = use_task_logical_date",
          "",
          "[Removed Lines]",
          "75:     def __init__(self, *, week_day, use_task_logical_date=False, use_task_execution_day=False, **kwargs):",
          "",
          "[Added Lines]",
          "70:         To use ``WeekDay`` enum, import it from ``airflow.utils.weekday``",
          "76:     :param use_task_execution_day: deprecated parameter, same effect as `use_task_logical_date`",
          "79:     def __init__(",
          "80:         self,",
          "82:         week_day: Union[str, Iterable[str], WeekDay, Iterable[WeekDay]],",
          "83:         use_task_logical_date: bool = False,",
          "84:         use_task_execution_day: bool = False,",
          "86:     ) -> None:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "85:             )",
          "86:         self._week_day_num = WeekDay.validate_week_day(week_day)",
          "89:         self.log.info(",
          "90:             'Poking until weekday is in %s, Today is %s',",
          "91:             self.week_day,",
          "",
          "[Removed Lines]",
          "88:     def poke(self, context: Context):",
          "",
          "[Added Lines]",
          "99:     def poke(self, context: Context) -> bool:",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "05a6fcfd3e6fc7ec4274e06ef3a8cea1ee4bdecd",
      "candidate_info": {
        "commit_hash": "05a6fcfd3e6fc7ec4274e06ef3a8cea1ee4bdecd",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/05a6fcfd3e6fc7ec4274e06ef3a8cea1ee4bdecd",
        "files": [
          "airflow/datasets/manager.py",
          "airflow/jobs/scheduler_job.py",
          "tests/models/test_taskinstance.py"
        ],
        "message": "Flush dataset events before queuing dagruns (#26276)\n\nWhen we go to schedule dagruns from the dataset dagrun queue, we assume\nthe events will happen before the queue records, which isn't the case\nunless we explicitly flush them first. This ensures that dagruns are\nproperly related to their upstream dataset events.\n\n(cherry picked from commit 954349a952d929dc82087e4bb20d19736f84d381)",
        "before_after_code_files": [
          "airflow/datasets/manager.py||airflow/datasets/manager.py",
          "airflow/jobs/scheduler_job.py||airflow/jobs/scheduler_job.py",
          "tests/models/test_taskinstance.py||tests/models/test_taskinstance.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/datasets/manager.py||airflow/datasets/manager.py": [
          "File: airflow/datasets/manager.py -> airflow/datasets/manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "61:                 extra=extra,",
          "62:             )",
          "63:         )",
          "64:         if dataset_model.consuming_dags:",
          "65:             self._queue_dagruns(dataset_model, session)",
          "66:         session.flush()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "64:         session.flush()",
          "",
          "---------------"
        ],
        "airflow/jobs/scheduler_job.py||airflow/jobs/scheduler_job.py": [
          "File: airflow/jobs/scheduler_job.py -> airflow/jobs/scheduler_job.py"
        ],
        "tests/models/test_taskinstance.py||tests/models/test_taskinstance.py": [
          "File: tests/models/test_taskinstance.py -> tests/models/test_taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1732:             DatasetEvent.source_task_instance == ti",
          "1733:         ).one() == ('s3://dag1/output_1.txt',)",
          "1735:     def test_outlet_datasets_failed(self, create_task_instance):",
          "1736:         \"\"\"",
          "1737:         Verify that when we have an outlet dataset on a task, and the task",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1735:         # check that the dataset event has an earlier timestamp than the DDRQ's",
          "1736:         ddrq_timestamps = (",
          "1737:             session.query(DatasetDagRunQueue.created_at).filter_by(dataset_id=event.dataset.id).all()",
          "1738:         )",
          "1739:         assert all([event.timestamp < ddrq_timestamp for (ddrq_timestamp,) in ddrq_timestamps])",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8c71dd8b13ccba199a77b205425c08087a7f177f",
      "candidate_info": {
        "commit_hash": "8c71dd8b13ccba199a77b205425c08087a7f177f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8c71dd8b13ccba199a77b205425c08087a7f177f",
        "files": [
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py"
        ],
        "message": "Make breeze works with latest docker-compose (#26233)\n\nThe latest docker-compose dropped an alias for `docker-compose` and\nwe need to detect it and use \"docker compose\" instead.\n\n(cherry picked from commit 7f47006effb330429fb510eb52644d22544f4fad)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "dev/breeze/src/airflow_breeze/commands/testing_commands.py||dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/developer_commands.py -> dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "69: from airflow_breeze.utils.console import get_console",
          "70: from airflow_breeze.utils.custom_param_types import BetterChoice, NotVerifiedBetterChoice",
          "71: from airflow_breeze.utils.docker_command_utils import (",
          "72:     check_docker_resources,",
          "73:     get_env_variables_for_docker_commands,",
          "74:     get_extra_docker_flags,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "72:     DOCKER_COMPOSE_COMMAND,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "468:     is_flag=True,",
          "469: )",
          "470: def stop(verbose: bool, dry_run: bool, preserve_volumes: bool):",
          "472:     if not preserve_volumes:",
          "473:         command_to_execute.append(\"--volumes\")",
          "474:     shell_params = ShellParams(verbose=verbose, backend=\"all\", include_mypy_volume=True)",
          "",
          "[Removed Lines]",
          "471:     command_to_execute = ['docker-compose', 'down', \"--remove-orphans\"]",
          "",
          "[Added Lines]",
          "472:     perform_environment_checks(verbose=verbose)",
          "473:     command_to_execute = [*DOCKER_COMPOSE_COMMAND, 'down', \"--remove-orphans\"]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "544:     :param shell_params: parameters of the execution",
          "545:     \"\"\"",
          "546:     shell_params.print_badge_info()",
          "548:     cmd_added = shell_params.command_passed",
          "549:     env_variables = get_env_variables_for_docker_commands(shell_params)",
          "550:     if cmd_added is not None:",
          "",
          "[Removed Lines]",
          "547:     cmd = ['docker-compose', 'run', '--service-ports', \"-e\", \"BREEZE\", '--rm', 'airflow']",
          "",
          "[Added Lines]",
          "549:     cmd = [*DOCKER_COMPOSE_COMMAND, 'run', '--service-ports', \"-e\", \"BREEZE\", '--rm', 'airflow']",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "578:     check_docker_resources(exec_shell_params.airflow_image_name, verbose=verbose, dry_run=dry_run)",
          "579:     exec_shell_params.print_badge_info()",
          "580:     env_variables = get_env_variables_for_docker_commands(exec_shell_params)",
          "582:     docker_compose_ps_command = run_command(",
          "583:         cmd, verbose=verbose, dry_run=dry_run, text=True, capture_output=True, env=env_variables, check=False",
          "584:     )",
          "",
          "[Removed Lines]",
          "581:     cmd = ['docker-compose', 'ps', '--all', '--filter', 'status=running', 'airflow']",
          "",
          "[Added Lines]",
          "583:     cmd = [*DOCKER_COMPOSE_COMMAND, 'ps', '--all', '--filter', 'status=running', 'airflow']",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/testing_commands.py||dev/breeze/src/airflow_breeze/commands/testing_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/testing_commands.py -> dev/breeze/src/airflow_breeze/commands/testing_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "50: from airflow_breeze.utils.console import get_console, message_type_from_return_code",
          "51: from airflow_breeze.utils.custom_param_types import NotVerifiedBetterChoice",
          "52: from airflow_breeze.utils.docker_command_utils import (",
          "53:     get_env_variables_for_docker_commands,",
          "54:     perform_environment_checks,",
          "55: )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "53:     DOCKER_COMPOSE_COMMAND,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "269:     if db_reset:",
          "270:         env_variables[\"DB_RESET\"] = \"true\"",
          "271:     perform_environment_checks(verbose=verbose)",
          "273:     cmd.extend(list(extra_pytest_args))",
          "274:     version = (",
          "275:         mssql_version",
          "",
          "[Removed Lines]",
          "272:     cmd = ['docker-compose', 'run', '--service-ports', '--rm', 'airflow']",
          "",
          "[Added Lines]",
          "273:     cmd = [*DOCKER_COMPOSE_COMMAND, 'run', '--service-ports', '--rm', 'airflow']",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "326:     env_variables['RUN_TESTS'] = \"true\"",
          "327:     env_variables['TEST_TYPE'] = 'Helm'",
          "328:     perform_environment_checks(verbose=verbose)",
          "330:     cmd.extend(list(extra_pytest_args))",
          "331:     result = run_command(cmd, verbose=verbose, dry_run=dry_run, env=env_variables, check=False)",
          "332:     sys.exit(result.returncode)",
          "",
          "[Removed Lines]",
          "329:     cmd = ['docker-compose', 'run', '--service-ports', '--rm', 'airflow']",
          "",
          "[Added Lines]",
          "330:     cmd = [*DOCKER_COMPOSE_COMMAND, 'run', '--service-ports', '--rm', 'airflow']",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "266:                 )",
          "269: def check_docker_compose_version(verbose: bool):",
          "270:     \"\"\"",
          "271:     Checks if the docker compose version is as expected, including some specific modifications done by",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "269: DOCKER_COMPOSE_COMMAND = [\"docker-compose\"]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "293:             capture_output=True,",
          "294:             text=True,",
          "295:         )",
          "297:     if docker_compose_version_result.returncode == 0:",
          "298:         docker_compose_version = docker_compose_version_result.stdout",
          "299:         version_extracted = version_pattern.search(docker_compose_version)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "299:         DOCKER_COMPOSE_COMMAND.clear()",
          "300:         DOCKER_COMPOSE_COMMAND.extend(['docker', 'compose'])",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c7ea01d67652dbc057ba2f6e52525ded0a4d4762",
      "candidate_info": {
        "commit_hash": "c7ea01d67652dbc057ba2f6e52525ded0a4d4762",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c7ea01d67652dbc057ba2f6e52525ded0a4d4762",
        "files": [
          "airflow/datasets/__init__.py",
          "tests/datasets/test_dataset.py",
          "tests/models/test_dataset.py"
        ],
        "message": "Better validation of Dataset URI during dag parse (#26389)\n\nPreviously we had the validation on the Dataset model, but we since\nmoved the \"dag\" facing class to a separate one. This adds validation to\nthe public class, and extends the validation to not allow space-only\nstrings\n\n(cherry picked from commit bd181daced707680ed22f5fd74e1e13094f6b164)",
        "before_after_code_files": [
          "airflow/datasets/__init__.py||airflow/datasets/__init__.py",
          "tests/datasets/test_dataset.py||tests/datasets/test_dataset.py",
          "tests/models/test_dataset.py||tests/models/test_dataset.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/datasets/__init__.py||airflow/datasets/__init__.py": [
          "File: airflow/datasets/__init__.py -> airflow/datasets/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: from __future__ import annotations",
          "19: from typing import Any",
          "21: import attr",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: from urllib.parse import urlparse",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "25: class Dataset:",
          "26:     \"\"\"A Dataset is used for marking data dependencies between workflows.\"\"\"",
          "29:     extra: dict[str, Any] | None = None",
          "",
          "[Removed Lines]",
          "28:     uri: str",
          "",
          "[Added Lines]",
          "29:     uri: str = attr.field(validator=[attr.validators.min_len(1), attr.validators.max_len(3000)])",
          "32:     @uri.validator",
          "33:     def _check_uri(self, attr, uri: str):",
          "34:         if uri.isspace():",
          "35:             raise ValueError(f'{attr.name} cannot be just whitespace')",
          "36:         try:",
          "37:             uri.encode('ascii')",
          "38:         except UnicodeEncodeError:",
          "39:             raise ValueError(f'{attr.name!r} must be ascii')",
          "40:         parsed = urlparse(uri)",
          "41:         if parsed.scheme and parsed.scheme.lower() == 'airflow':",
          "42:             raise ValueError(f'{attr.name!r} scheme `airflow` is reserved')",
          "",
          "---------------"
        ],
        "tests/datasets/test_dataset.py||tests/datasets/test_dataset.py": [
          "File: tests/datasets/test_dataset.py -> tests/datasets/test_dataset.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "18: from __future__ import annotations",
          "20: import pytest",
          "22: from airflow.datasets import Dataset",
          "23: from airflow.operators.empty import EmptyOperator",
          "26: @pytest.mark.parametrize(",
          "27:     [\"uri\"],",
          "28:     [",
          "29:         pytest.param(\"\", id=\"empty\"),",
          "30:         pytest.param(\"\\n\\t\", id=\"whitespace\"),",
          "31:         pytest.param(\"a\" * 3001, id=\"too_long\"),",
          "32:         pytest.param(\"airflow:\" * 3001, id=\"reserved_scheme\"),",
          "33:         pytest.param(\"\ud83d\ude0a\" * 3001, id=\"non-ascii\"),",
          "34:     ],",
          "35: )",
          "36: def test_invalid_uris(uri):",
          "37:     with pytest.raises(ValueError):",
          "38:         Dataset(uri=uri)",
          "41: def test_uri_with_scheme():",
          "42:     dataset = Dataset(uri=\"s3://example_dataset\")",
          "43:     EmptyOperator(task_id=\"task1\", outlets=[dataset])",
          "46: def test_uri_without_scheme():",
          "47:     dataset = Dataset(uri=\"example_dataset\")",
          "48:     EmptyOperator(task_id=\"task1\", outlets=[dataset])",
          "",
          "---------------"
        ],
        "tests/models/test_dataset.py||tests/models/test_dataset.py": [
          "File: tests/models/test_dataset.py -> tests/models/test_dataset.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e6177a0270fdb73a42bb966b35e52012faecc800",
      "candidate_info": {
        "commit_hash": "e6177a0270fdb73a42bb966b35e52012faecc800",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e6177a0270fdb73a42bb966b35e52012faecc800",
        "files": [
          "airflow/serialization/serialized_objects.py",
          "tests/serialization/test_dag_serialization.py"
        ],
        "message": "Handle list when serializing expand_kwargs (#26369)\n\n(cherry picked from commit b816a6b243d16da87ca00e443619c75e9f6f5816)",
        "before_after_code_files": [
          "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py",
          "tests/serialization/test_dag_serialization.py||tests/serialization/test_dag_serialization.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py": [
          "File: airflow/serialization/serialized_objects.py -> airflow/serialization/serialized_objects.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: \"\"\"Serialized DAG and BaseOperator\"\"\"",
          "18: from __future__ import annotations",
          "20: import datetime",
          "21: import enum",
          "22: import logging",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import collections.abc",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "24: import weakref",
          "25: from dataclasses import dataclass",
          "26: from inspect import Parameter, signature",
          "29: import cattr",
          "30: import lazy_object_proxy",
          "",
          "[Removed Lines]",
          "27: from typing import TYPE_CHECKING, Any, Iterable, NamedTuple, Type",
          "",
          "[Added Lines]",
          "28: from typing import TYPE_CHECKING, Any, Collection, Iterable, Mapping, NamedTuple, Type, Union",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "207:         return deserialize_xcom_arg(self.data, dag)",
          "210: class _ExpandInputRef(NamedTuple):",
          "211:     \"\"\"Used to store info needed to create a mapped operator's expand input.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "211: # These two should be kept in sync. Note that these are intentionally not using",
          "212: # the type declarations in expandinput.py so we always remember to update",
          "213: # serialization logic when adding new ExpandInput variants. If you add things to",
          "214: # the unions, be sure to update _ExpandInputRef to match.",
          "215: _ExpandInputOriginalValue = Union[",
          "216:     # For .expand(**kwargs).",
          "217:     Mapping[str, Any],",
          "218:     # For expand_kwargs(arg).",
          "219:     XComArg,",
          "220:     Collection[Union[XComArg, Mapping[str, Any]]],",
          "221: ]",
          "222: _ExpandInputSerializedValue = Union[",
          "223:     # For .expand(**kwargs).",
          "224:     Mapping[str, Any],",
          "225:     # For expand_kwargs(arg).",
          "226:     _XComRef,",
          "227:     Collection[Union[_XComRef, Mapping[str, Any]]],",
          "228: ]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "215:     \"\"\"",
          "217:     key: str",
          "220:     def deref(self, dag: DAG) -> ExpandInput:",
          "221:         if isinstance(self.value, _XComRef):",
          "222:             value: Any = self.value.deref(dag)",
          "224:             value = {k: v.deref(dag) if isinstance(v, _XComRef) else v for k, v in self.value.items()}",
          "225:         return create_expand_input(self.key, value)",
          "",
          "[Removed Lines]",
          "218:     value: _XComRef | dict[str, Any]",
          "223:         else:",
          "",
          "[Added Lines]",
          "239:     value: _ExpandInputSerializedValue",
          "241:     @classmethod",
          "242:     def validate_expand_input_value(cls, value: _ExpandInputOriginalValue) -> None:",
          "243:         \"\"\"Validate we've covered all ``ExpandInput.value`` types.",
          "245:         This function does not actually do anything, but is called during",
          "246:         serialization so Mypy will *statically* check we have handled all",
          "247:         possible ExpandInput cases.",
          "248:         \"\"\"",
          "251:         \"\"\"De-reference into a concrete ExpandInput object.",
          "253:         If you add more cases here, be sure to update _ExpandInputOriginalValue",
          "254:         and _ExpandInputSerializedValue to match the logic.",
          "255:         \"\"\"",
          "258:         elif isinstance(self.value, collections.abc.Mapping):",
          "260:         else:",
          "261:             value = [v.deref(dag) if isinstance(v, _XComRef) else v for v in self.value]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "663:         serialized_op = cls._serialize_node(op, include_deps=op.deps != MappedOperator.deps_for(BaseOperator))",
          "664:         # Handle expand_input and op_kwargs_expand_input.",
          "665:         expansion_kwargs = op._get_specified_expand_input()",
          "666:         serialized_op[op._expand_input_attr] = {",
          "667:             \"type\": get_map_type_key(expansion_kwargs),",
          "668:             \"value\": cls.serialize(expansion_kwargs.value),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "703:         if TYPE_CHECKING:  # Let Mypy check the input type for us!",
          "704:             _ExpandInputRef.validate_expand_input_value(expansion_kwargs.value)",
          "",
          "---------------"
        ],
        "tests/serialization/test_dag_serialization.py||tests/serialization/test_dag_serialization.py": [
          "File: tests/serialization/test_dag_serialization.py -> tests/serialization/test_dag_serialization.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1963: @pytest.mark.parametrize(\"strict\", [True, False])",
          "1965:     from airflow.models.xcom_arg import PlainXComArg, XComArg",
          "1966:     from airflow.serialization.serialized_objects import _XComRef",
          "",
          "[Removed Lines]",
          "1964: def test_operator_expand_kwargs_serde(strict):",
          "",
          "[Added Lines]",
          "1964: def test_operator_expand_kwargs_literal_serde(strict):",
          "1965:     from airflow.models.xcom_arg import PlainXComArg, XComArg",
          "1966:     from airflow.serialization.serialized_objects import _XComRef",
          "1968:     with DAG(\"test-dag\", start_date=datetime(2020, 1, 1)) as dag:",
          "1969:         task1 = BaseOperator(task_id=\"op1\")",
          "1970:         mapped = MockOperator.partial(task_id='task_2').expand_kwargs(",
          "1971:             [{\"a\": \"x\"}, {\"a\": XComArg(task1)}],",
          "1972:             strict=strict,",
          "1973:         )",
          "1975:     serialized = SerializedBaseOperator.serialize(mapped)",
          "1976:     assert serialized == {",
          "1977:         '_is_empty': False,",
          "1978:         '_is_mapped': True,",
          "1979:         '_task_module': 'tests.test_utils.mock_operators',",
          "1980:         '_task_type': 'MockOperator',",
          "1981:         'downstream_task_ids': [],",
          "1982:         'expand_input': {",
          "1983:             \"type\": \"list-of-dicts\",",
          "1984:             \"value\": [",
          "1985:                 {\"__type\": \"dict\", \"__var\": {\"a\": \"x\"}},",
          "1986:                 {",
          "1987:                     \"__type\": \"dict\",",
          "1988:                     \"__var\": {\"a\": {'__type': 'xcomref', '__var': {'task_id': 'op1', 'key': 'return_value'}}},",
          "1989:                 },",
          "1990:             ],",
          "1991:         },",
          "1992:         'partial_kwargs': {},",
          "1993:         'task_id': 'task_2',",
          "1994:         'template_fields': ['arg1', 'arg2'],",
          "1995:         'template_ext': [],",
          "1996:         'template_fields_renderers': {},",
          "1997:         'operator_extra_links': [],",
          "1998:         'ui_color': '#fff',",
          "1999:         'ui_fgcolor': '#000',",
          "2000:         \"_disallow_kwargs_override\": strict,",
          "2001:         '_expand_input_attr': 'expand_input',",
          "2002:     }",
          "2004:     op = SerializedBaseOperator.deserialize_operator(serialized)",
          "2005:     assert op.deps is MappedOperator.deps_for(BaseOperator)",
          "2006:     assert op._disallow_kwargs_override == strict",
          "2008:     # The XComArg can't be deserialized before the DAG is.",
          "2009:     expand_value = op.expand_input.value",
          "2010:     assert expand_value == [{\"a\": \"x\"}, {\"a\": _XComRef({\"task_id\": \"op1\", \"key\": XCOM_RETURN_KEY})}]",
          "2012:     serialized_dag: DAG = SerializedDAG.from_dict(SerializedDAG.to_dict(dag))",
          "2014:     resolved_expand_value = serialized_dag.task_dict['task_2'].expand_input.value",
          "2015:     resolved_expand_value == [{\"a\": \"x\"}, {\"a\": PlainXComArg(serialized_dag.task_dict['op1'])}]",
          "2018: @pytest.mark.parametrize(\"strict\", [True, False])",
          "2019: def test_operator_expand_kwargs_xcomarg_serde(strict):",
          "",
          "---------------"
        ]
      }
    }
  ]
}