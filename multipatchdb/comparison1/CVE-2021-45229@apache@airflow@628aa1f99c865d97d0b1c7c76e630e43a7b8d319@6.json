{
  "cve_id": "CVE-2021-45229",
  "cve_desc": "It was discovered that the \"Trigger DAG with config\" screen was susceptible to XSS attacks via the `origin` query argument. This issue affects Apache Airflow versions 2.2.3 and below.",
  "repo": "apache/airflow",
  "patch_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
  "patch_info": {
    "commit_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "files": [
      "airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py"
    ],
    "message": "Simplify trigger cancel button (#21591)\n\nCo-authored-by: Jed Cunningham <jedcunningham@apache.org>\n(cherry picked from commit 65297673a318660fba76797e50d0c06804dfcafc)",
    "before_after_code_files": [
      "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
    ]
  },
  "patch_diff": {
    "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html": [
      "File: airflow/www/templates/airflow/trigger.html -> airflow/www/templates/airflow/trigger.html",
      "--- Hunk 1 ---",
      "[Context before]",
      "63:       </label>",
      "64:     </div>",
      "65:     <button type=\"submit\" class=\"btn btn-primary\">Trigger</button>",
      "67:   </form>",
      "68: {% endblock %}",
      "",
      "[Removed Lines]",
      "66:     <button type=\"button\" class=\"btn\" onclick=\"location.href = '{{ origin }}'; return false\">Cancel</button>",
      "",
      "[Added Lines]",
      "66:     <a class=\"btn\" href=\"{{ origin }}\">Cancel</a>",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
      "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "133:         (\"javascript:alert(1)\", \"/home\"),",
      "134:         (\"http://google.com\", \"/home\"),",
      "135:         (\"36539'%3balert(1)%2f%2f166\", \"/home\"),",
      "136:         (",
      "137:             \"%2Ftree%3Fdag_id%3Dexample_bash_operator';alert(33)//\",",
      "138:             \"/home\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "136:         (",
      "137:             '\"><script>alert(99)</script><a href=\"',",
      "138:             \"&#34;&gt;&lt;script&gt;alert(99)&lt;/script&gt;&lt;a href=&#34;\",",
      "139:         ),",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "145:     test_dag_id = \"example_bash_operator\"",
      "147:     resp = admin_client.get(f'trigger?dag_id={test_dag_id}&origin={test_origin}')",
      "156: @pytest.mark.parametrize(",
      "",
      "[Removed Lines]",
      "148:     check_content_in_response(",
      "149:         '<button type=\"button\" class=\"btn\" onclick=\"location.href = \\'{}\\'; return false\">'.format(",
      "150:             expected_origin",
      "151:         ),",
      "152:         resp,",
      "153:     )",
      "",
      "[Added Lines]",
      "152:     check_content_in_response(f'<a class=\"btn\" href=\"{expected_origin}\">Cancel</a>', resp)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "8443c730621df9cfeeeedf4df1c6d3cc41c63a21",
      "candidate_info": {
        "commit_hash": "8443c730621df9cfeeeedf4df1c6d3cc41c63a21",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8443c730621df9cfeeeedf4df1c6d3cc41c63a21",
        "files": [
          "dev/provider_packages/prepare_provider_packages.py"
        ],
        "message": "Add known warning generated by snowflake new version (#20604)\n\nThe new snowflake library version generates a different warning\nmessage as they bumped pyarrow version used. This PR adds the\nwarning to known warnings.\n\n(cherry picked from commit 80bccfded3d45220a7f6e80c4e616ab3164da198)",
        "before_after_code_files": [
          "dev/provider_packages/prepare_provider_packages.py||dev/provider_packages/prepare_provider_packages.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/provider_packages/prepare_provider_packages.py||dev/provider_packages/prepare_provider_packages.py": [
          "File: dev/provider_packages/prepare_provider_packages.py -> dev/provider_packages/prepare_provider_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2121:         \" adheres to: 'pyarrow<3.1.0,>=3.0.0; extra == \\\"pandas\\\"'\",",
          "2122:         \"snowflake\",",
          "2123:     ),",
          "2124:     (\"SelectableGroups dict interface is deprecated. Use select.\", \"kombu\"),",
          "2125:     (\"The module cloudant is now deprecated. The replacement is ibmcloudant.\", \"cloudant\"),",
          "2126: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2124:     (",
          "2125:         \"You have an incompatible version of 'pyarrow' installed (6.0.1), please install a version that\"",
          "2126:         \" adheres to: 'pyarrow<5.1.0,>=5.0.0; extra == \\\"pandas\\\"'\",",
          "2127:         \"snowflake\",",
          "2128:     ),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "59177190d01a722d3cc49d9b8c06f6daa1ac2ad3",
      "candidate_info": {
        "commit_hash": "59177190d01a722d3cc49d9b8c06f6daa1ac2ad3",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/59177190d01a722d3cc49d9b8c06f6daa1ac2ad3",
        "files": [
          "dev/README_RELEASE_AIRFLOW.md",
          "dev/airflow-github",
          "dev/prepare_release_issue.py"
        ],
        "message": "Update docs/tools for releasing core Airflow (#20211)\n\nWhen building the \"testing status\" issue, don't include things skipped\non the changelog or doc-only changes.\n\nAlso, don't add skipped changelog entries in the changelog.\n\n(cherry picked from commit 993ed933e95970d14e0b0b5659ad28f15a0e5fde)",
        "before_after_code_files": [
          "dev/prepare_release_issue.py||dev/prepare_release_issue.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/prepare_release_issue.py||dev/prepare_release_issue.py": [
          "File: dev/prepare_release_issue.py -> dev/prepare_release_issue.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "258:                 except UnknownObjectException:",
          "259:                     console.print(f\"[red]The PR #{pr_number} could not be found[/]\")",
          "260:                 continue",
          "261:             pull_requests[pr_number] = pr",
          "262:             # GitHub does not have linked issues in PR - but we quite rigorously add Fixes/Closes",
          "263:             # Relate so we can find those from the body",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "262:             # Ignore doc-only and skipped PRs",
          "263:             label_names = [label.name for label in pr.labels]",
          "264:             if \"type:doc-only\" in label_names or \"changelog:skip\" in label_names:",
          "265:                 continue",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "67fc1d78fef6eb342f77d1536fa7d3d6dcf590dd",
      "candidate_info": {
        "commit_hash": "67fc1d78fef6eb342f77d1536fa7d3d6dcf590dd",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/67fc1d78fef6eb342f77d1536fa7d3d6dcf590dd",
        "files": [
          "airflow/providers/google/cloud/example_dags/example_dataproc.py",
          "airflow/providers/google/cloud/hooks/dataproc.py",
          "airflow/providers/google/cloud/operators/dataproc.py",
          "docs/apache-airflow-providers-google/index.rst",
          "docs/apache-airflow-providers-google/operators/cloud/dataproc.rst",
          "setup.py",
          "tests/always/test_project_structure.py",
          "tests/providers/google/cloud/hooks/test_dataproc.py",
          "tests/providers/google/cloud/operators/test_dataproc.py",
          "tests/providers/google/cloud/operators/test_dataproc_system.py"
        ],
        "message": "Create dataproc serverless spark batches operator (#19248)\n\n(cherry picked from commit bf68b9a8461eda634a7d91aa56575fb950960eaa)",
        "before_after_code_files": [
          "airflow/providers/google/cloud/example_dags/example_dataproc.py||airflow/providers/google/cloud/example_dags/example_dataproc.py",
          "airflow/providers/google/cloud/hooks/dataproc.py||airflow/providers/google/cloud/hooks/dataproc.py",
          "airflow/providers/google/cloud/operators/dataproc.py||airflow/providers/google/cloud/operators/dataproc.py",
          "setup.py||setup.py",
          "tests/always/test_project_structure.py||tests/always/test_project_structure.py",
          "tests/providers/google/cloud/hooks/test_dataproc.py||tests/providers/google/cloud/hooks/test_dataproc.py",
          "tests/providers/google/cloud/operators/test_dataproc.py||tests/providers/google/cloud/operators/test_dataproc.py",
          "tests/providers/google/cloud/operators/test_dataproc_system.py||tests/providers/google/cloud/operators/test_dataproc_system.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/google/cloud/example_dags/example_dataproc.py||airflow/providers/google/cloud/example_dags/example_dataproc.py": [
          "File: airflow/providers/google/cloud/example_dags/example_dataproc.py -> airflow/providers/google/cloud/example_dags/example_dataproc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "149:     },",
          "150:     \"jobs\": [{\"step_id\": \"pig_job_1\", \"pig_job\": PIG_JOB[\"pig_job\"]}],",
          "151: }",
          "154: with models.DAG(\"example_gcp_dataproc\", schedule_interval='@once', start_date=days_ago(1)) as dag:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "152: BATCH_ID = \"test-batch-id\"",
          "153: BATCH_CONFIG = {",
          "154:     \"spark_batch\": {",
          "155:         \"jar_file_uris\": [\"file:///usr/lib/spark/examples/jars/spark-examples.jar\"],",
          "156:         \"main_class\": \"org.apache.spark.examples.SparkPi\",",
          "157:     },",
          "158: }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "249:     scale_cluster >> pyspark_task >> delete_cluster",
          "250:     scale_cluster >> sparkr_task >> delete_cluster",
          "251:     scale_cluster >> hadoop_task >> delete_cluster",
          "",
          "[Removed Lines]",
          "253:     # Task dependency created via `XComArgs`:",
          "254:     #   spark_task_async >> spark_task_async_sensor",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/hooks/dataproc.py||airflow/providers/google/cloud/hooks/dataproc.py": [
          "File: airflow/providers/google/cloud/hooks/dataproc.py -> airflow/providers/google/cloud/hooks/dataproc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple, Union",
          "26: from google.api_core.exceptions import ServerError",
          "27: from google.api_core.retry import Retry",
          "28: from google.cloud.dataproc_v1 import (",
          "29:     Cluster,",
          "30:     ClusterControllerClient,",
          "31:     Job,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: from google.api_core.operation import Operation",
          "30:     Batch,",
          "31:     BatchControllerClient,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "267:             credentials=self._get_credentials(), client_info=self.client_info, client_options=client_options",
          "268:         )",
          "270:     @GoogleBaseHook.fallback_to_default_project_id",
          "271:     def create_cluster(",
          "272:         self,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "273:     def get_batch_client(",
          "274:         self, region: Optional[str] = None, location: Optional[str] = None",
          "275:     ) -> BatchControllerClient:",
          "276:         \"\"\"Returns BatchControllerClient\"\"\"",
          "277:         if location is not None:",
          "278:             warnings.warn(",
          "279:                 \"Parameter `location` will be deprecated. \"",
          "280:                 \"Please provide value through `region` parameter instead.\",",
          "281:                 DeprecationWarning,",
          "282:                 stacklevel=2,",
          "283:             )",
          "284:             region = location",
          "285:         client_options = None",
          "286:         if region and region != 'global':",
          "287:             client_options = {'api_endpoint': f'{region}-dataproc.googleapis.com:443'}",
          "289:         return BatchControllerClient(",
          "290:             credentials=self._get_credentials(), client_info=self.client_info, client_options=client_options",
          "291:         )",
          "293:     def wait_for_operation(self, timeout: float, operation: Operation):",
          "294:         \"\"\"Waits for long-lasting operation to complete.\"\"\"",
          "295:         try:",
          "296:             return operation.result(timeout=timeout)",
          "297:         except Exception:",
          "298:             error = operation.exception(timeout=timeout)",
          "299:             raise AirflowException(error)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1030:             metadata=metadata,",
          "1031:         )",
          "1032:         return job",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1065:     @GoogleBaseHook.fallback_to_default_project_id",
          "1066:     def create_batch(",
          "1067:         self,",
          "1068:         region: str,",
          "1069:         project_id: str,",
          "1070:         batch: Union[Dict, Batch],",
          "1071:         batch_id: Optional[str] = None,",
          "1072:         request_id: Optional[str] = None,",
          "1073:         retry: Optional[Retry] = None,",
          "1074:         timeout: Optional[float] = None,",
          "1075:         metadata: Optional[Sequence[Tuple[str, str]]] = \"\",",
          "1076:     ):",
          "1077:         \"\"\"",
          "1078:         Creates a batch workload.",
          "1080:         :param project_id: Required. The ID of the Google Cloud project that the cluster belongs to.",
          "1081:         :type project_id: str",
          "1082:         :param region: Required. The Cloud Dataproc region in which to handle the request.",
          "1083:         :type region: str",
          "1084:         :param batch: Required. The batch to create.",
          "1085:         :type batch: google.cloud.dataproc_v1.types.Batch",
          "1086:         :param batch_id: Optional. The ID to use for the batch, which will become the final component",
          "1087:             of the batch's resource name.",
          "1088:             This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.",
          "1089:         :type batch_id: str",
          "1090:         :param request_id: Optional. A unique id used to identify the request. If the server receives two",
          "1091:             ``CreateBatchRequest`` requests with the same id, then the second request will be ignored and",
          "1092:             the first ``google.longrunning.Operation`` created and stored in the backend is returned.",
          "1093:         :type request_id: str",
          "1094:         :param retry: A retry object used to retry requests. If ``None`` is specified, requests will not be",
          "1095:             retried.",
          "1096:         :type retry: google.api_core.retry.Retry",
          "1097:         :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if",
          "1098:             ``retry`` is specified, the timeout applies to each individual attempt.",
          "1099:         :type timeout: float",
          "1100:         :param metadata: Additional metadata that is provided to the method.",
          "1101:         :type metadata: Sequence[Tuple[str, str]]",
          "1102:         \"\"\"",
          "1103:         client = self.get_batch_client(region)",
          "1104:         parent = f'projects/{project_id}/regions/{region}'",
          "1106:         result = client.create_batch(",
          "1107:             request={",
          "1108:                 'parent': parent,",
          "1109:                 'batch': batch,",
          "1110:                 'batch_id': batch_id,",
          "1111:                 'request_id': request_id,",
          "1112:             },",
          "1113:             retry=retry,",
          "1114:             timeout=timeout,",
          "1115:             metadata=metadata,",
          "1116:         )",
          "1117:         return result",
          "1119:     @GoogleBaseHook.fallback_to_default_project_id",
          "1120:     def delete_batch(",
          "1121:         self,",
          "1122:         batch_id: str,",
          "1123:         region: str,",
          "1124:         project_id: str,",
          "1125:         retry: Optional[Retry] = None,",
          "1126:         timeout: Optional[float] = None,",
          "1127:         metadata: Optional[Sequence[Tuple[str, str]]] = None,",
          "1128:     ):",
          "1129:         \"\"\"",
          "1130:         Deletes the batch workload resource.",
          "1132:         :param batch_id: Required. The ID to use for the batch, which will become the final component",
          "1133:             of the batch's resource name.",
          "1134:             This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.",
          "1135:         :type batch_id: str",
          "1136:         :param project_id: Required. The ID of the Google Cloud project that the cluster belongs to.",
          "1137:         :type project_id: str",
          "1138:         :param region: Required. The Cloud Dataproc region in which to handle the request.",
          "1139:         :type region: str",
          "1140:         :param retry: A retry object used to retry requests. If ``None`` is specified, requests will not be",
          "1141:             retried.",
          "1142:         :type retry: google.api_core.retry.Retry",
          "1143:         :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if",
          "1144:             ``retry`` is specified, the timeout applies to each individual attempt.",
          "1145:         :type timeout: float",
          "1146:         :param metadata: Additional metadata that is provided to the method.",
          "1147:         :type metadata: Sequence[Tuple[str, str]]",
          "1148:         \"\"\"",
          "1149:         client = self.get_batch_client(region)",
          "1150:         name = f\"projects/{project_id}/regions/{region}/batches/{batch_id}\"",
          "1152:         result = client.delete_batch(",
          "1153:             request={",
          "1154:                 'name': name,",
          "1155:             },",
          "1156:             retry=retry,",
          "1157:             timeout=timeout,",
          "1158:             metadata=metadata,",
          "1159:         )",
          "1160:         return result",
          "1162:     @GoogleBaseHook.fallback_to_default_project_id",
          "1163:     def get_batch(",
          "1164:         self,",
          "1165:         batch_id: str,",
          "1166:         region: str,",
          "1167:         project_id: str,",
          "1168:         retry: Optional[Retry] = None,",
          "1169:         timeout: Optional[float] = None,",
          "1170:         metadata: Optional[Sequence[Tuple[str, str]]] = None,",
          "1171:     ):",
          "1172:         \"\"\"",
          "1173:         Gets the batch workload resource representation.",
          "1175:         :param batch_id: Required. The ID to use for the batch, which will become the final component",
          "1176:             of the batch's resource name.",
          "1177:             This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.",
          "1178:         :type batch_id: str",
          "1179:         :param project_id: Required. The ID of the Google Cloud project that the cluster belongs to.",
          "1180:         :type project_id: str",
          "1181:         :param region: Required. The Cloud Dataproc region in which to handle the request.",
          "1182:         :type region: str",
          "1183:         :param retry: A retry object used to retry requests. If ``None`` is specified, requests will not be",
          "1184:             retried.",
          "1185:         :type retry: google.api_core.retry.Retry",
          "1186:         :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if",
          "1187:             ``retry`` is specified, the timeout applies to each individual attempt.",
          "1188:         :type timeout: float",
          "1189:         :param metadata: Additional metadata that is provided to the method.",
          "1190:         :type metadata: Sequence[Tuple[str, str]]",
          "1191:         \"\"\"",
          "1192:         client = self.get_batch_client(region)",
          "1193:         name = f\"projects/{project_id}/regions/{region}/batches/{batch_id}\"",
          "1195:         result = client.get_batch(",
          "1196:             request={",
          "1197:                 'name': name,",
          "1198:             },",
          "1199:             retry=retry,",
          "1200:             timeout=timeout,",
          "1201:             metadata=metadata,",
          "1202:         )",
          "1203:         return result",
          "1205:     @GoogleBaseHook.fallback_to_default_project_id",
          "1206:     def list_batches(",
          "1207:         self,",
          "1208:         region: str,",
          "1209:         project_id: str,",
          "1210:         page_size: Optional[int] = None,",
          "1211:         page_token: Optional[str] = None,",
          "1212:         retry: Optional[Retry] = None,",
          "1213:         timeout: Optional[float] = None,",
          "1214:         metadata: Optional[Sequence[Tuple[str, str]]] = None,",
          "1215:     ):",
          "1216:         \"\"\"",
          "1217:         Lists batch workloads.",
          "1219:         :param project_id: Required. The ID of the Google Cloud project that the cluster belongs to.",
          "1220:         :type project_id: str",
          "1221:         :param region: Required. The Cloud Dataproc region in which to handle the request.",
          "1222:         :type region: str",
          "1223:         :param page_size: Optional. The maximum number of batches to return in each response. The service may",
          "1224:             return fewer than this value. The default page size is 20; the maximum page size is 1000.",
          "1225:         :type page_size: int",
          "1226:         :param page_token: Optional. A page token received from a previous ``ListBatches`` call.",
          "1227:             Provide this token to retrieve the subsequent page.",
          "1228:         :type page_token: str",
          "1229:         :param retry: A retry object used to retry requests. If ``None`` is specified, requests will not be",
          "1230:             retried.",
          "1231:         :type retry: google.api_core.retry.Retry",
          "1232:         :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if",
          "1233:             ``retry`` is specified, the timeout applies to each individual attempt.",
          "1234:         :type timeout: float",
          "1235:         :param metadata: Additional metadata that is provided to the method.",
          "1236:         :type metadata: Sequence[Tuple[str, str]]",
          "1237:         \"\"\"",
          "1238:         client = self.get_batch_client(region)",
          "1239:         parent = f'projects/{project_id}/regions/{region}'",
          "1241:         result = client.list_batches(",
          "1242:             request={",
          "1243:                 'parent': parent,",
          "1244:                 'page_size': page_size,",
          "1245:                 'page_token': page_token,",
          "1246:             },",
          "1247:             retry=retry,",
          "1248:             timeout=timeout,",
          "1249:             metadata=metadata,",
          "1250:         )",
          "1251:         return result",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/operators/dataproc.py||airflow/providers/google/cloud/operators/dataproc.py": [
          "File: airflow/providers/google/cloud/operators/dataproc.py -> airflow/providers/google/cloud/operators/dataproc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "28: from datetime import datetime, timedelta",
          "29: from typing import Dict, List, Optional, Sequence, Set, Tuple, Union",
          "31: from google.api_core.exceptions import AlreadyExists, NotFound",
          "32: from google.api_core.retry import Retry, exponential_sleep_generator",
          "34: from google.protobuf.duration_pb2 import Duration",
          "35: from google.protobuf.field_mask_pb2 import FieldMask",
          "",
          "[Removed Lines]",
          "33: from google.cloud.dataproc_v1 import Cluster",
          "",
          "[Added Lines]",
          "31: from google.api_core import operation  # type: ignore",
          "34: from google.cloud.dataproc_v1 import Batch, Cluster",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2162:         )",
          "2163:         operation.result()",
          "2164:         self.log.info(\"Updated %s cluster.\", self.cluster_name)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2168: class DataprocCreateBatchOperator(BaseOperator):",
          "2169:     \"\"\"",
          "2170:     Creates a batch workload.",
          "2172:     :param project_id: Required. The ID of the Google Cloud project that the cluster belongs to.",
          "2173:     :type project_id: str",
          "2174:     :param region: Required. The Cloud Dataproc region in which to handle the request.",
          "2175:     :type region: str",
          "2176:     :param batch: Required. The batch to create.",
          "2177:     :type batch: google.cloud.dataproc_v1.types.Batch",
          "2178:     :param batch_id: Optional. The ID to use for the batch, which will become the final component",
          "2179:         of the batch's resource name.",
          "2180:         This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.",
          "2181:     :type batch_id: str",
          "2182:     :param request_id: Optional. A unique id used to identify the request. If the server receives two",
          "2183:         ``CreateBatchRequest`` requests with the same id, then the second request will be ignored and",
          "2184:         the first ``google.longrunning.Operation`` created and stored in the backend is returned.",
          "2185:     :type request_id: str",
          "2186:     :param retry: A retry object used to retry requests. If ``None`` is specified, requests will not be",
          "2187:         retried.",
          "2188:     :type retry: google.api_core.retry.Retry",
          "2189:     :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if",
          "2190:         ``retry`` is specified, the timeout applies to each individual attempt.",
          "2191:     :type timeout: float",
          "2192:     :param metadata: Additional metadata that is provided to the method.",
          "2193:     :type metadata: Sequence[Tuple[str, str]]",
          "2194:     :param gcp_conn_id: The connection ID to use connecting to Google Cloud.",
          "2195:     :type gcp_conn_id: str",
          "2196:     :param impersonation_chain: Optional service account to impersonate using short-term",
          "2197:         credentials, or chained list of accounts required to get the access_token",
          "2198:         of the last account in the list, which will be impersonated in the request.",
          "2199:         If set as a string, the account must grant the originating account",
          "2200:         the Service Account Token Creator IAM role.",
          "2201:         If set as a sequence, the identities from the list must grant",
          "2202:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "2203:         account from the list granting this role to the originating account (templated).",
          "2204:     :type impersonation_chain: Union[str, Sequence[str]]",
          "2205:     \"\"\"",
          "2207:     template_fields = (",
          "2208:         'project_id',",
          "2209:         'batch_id',",
          "2210:         'region',",
          "2211:         'impersonation_chain',",
          "2212:     )",
          "2214:     def __init__(",
          "2215:         self,",
          "2217:         region: str = None,",
          "2218:         project_id: str,",
          "2219:         batch: Union[Dict, Batch],",
          "2220:         batch_id: Optional[str] = None,",
          "2221:         request_id: Optional[str] = None,",
          "2222:         retry: Optional[Retry] = None,",
          "2223:         timeout: Optional[float] = None,",
          "2224:         metadata: Optional[Sequence[Tuple[str, str]]] = \"\",",
          "2225:         gcp_conn_id: str = \"google_cloud_default\",",
          "2226:         impersonation_chain: Optional[Union[str, Sequence[str]]] = None,",
          "2228:     ):",
          "2229:         super().__init__(**kwargs)",
          "2230:         self.region = region",
          "2231:         self.project_id = project_id",
          "2232:         self.batch = batch",
          "2233:         self.batch_id = batch_id",
          "2234:         self.request_id = request_id",
          "2235:         self.retry = retry",
          "2236:         self.timeout = timeout",
          "2237:         self.metadata = metadata",
          "2238:         self.gcp_conn_id = gcp_conn_id",
          "2239:         self.impersonation_chain = impersonation_chain",
          "2240:         self.operation: Optional[operation.Operation] = None",
          "2242:     def execute(self, context):",
          "2243:         hook = DataprocHook(gcp_conn_id=self.gcp_conn_id, impersonation_chain=self.impersonation_chain)",
          "2244:         self.log.info(\"Creating batch\")",
          "2245:         try:",
          "2246:             self.operation = hook.create_batch(",
          "2247:                 region=self.region,",
          "2248:                 project_id=self.project_id,",
          "2249:                 batch=self.batch,",
          "2250:                 batch_id=self.batch_id,",
          "2251:                 request_id=self.request_id,",
          "2252:                 retry=self.retry,",
          "2253:                 timeout=self.timeout,",
          "2254:                 metadata=self.metadata,",
          "2255:             )",
          "2256:             result = hook.wait_for_operation(self.timeout, self.operation)",
          "2257:             self.log.info(\"Batch %s created\", self.batch_id)",
          "2258:         except AlreadyExists:",
          "2259:             self.log.info(\"Batch with given id already exists\")",
          "2260:             result = hook.get_batch(",
          "2261:                 batch_id=self.batch_id,",
          "2262:                 region=self.region,",
          "2263:                 project_id=self.project_id,",
          "2264:                 retry=self.retry,",
          "2265:                 timeout=self.timeout,",
          "2266:                 metadata=self.metadata,",
          "2267:             )",
          "2268:         return Batch.to_dict(result)",
          "2270:     def on_kill(self):",
          "2271:         if self.operation:",
          "2272:             self.operation.cancel()",
          "2275: class DataprocDeleteBatchOperator(BaseOperator):",
          "2276:     \"\"\"",
          "2277:     Deletes the batch workload resource.",
          "2279:     :param batch_id: Required. The ID to use for the batch, which will become the final component",
          "2280:         of the batch's resource name.",
          "2281:         This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.",
          "2282:     :type batch_id: str",
          "2283:     :param project_id: Required. The ID of the Google Cloud project that the cluster belongs to.",
          "2284:     :type project_id: str",
          "2285:     :param region: Required. The Cloud Dataproc region in which to handle the request.",
          "2286:     :type region: str",
          "2287:     :param retry: A retry object used to retry requests. If ``None`` is specified, requests will not be",
          "2288:         retried.",
          "2289:     :type retry: google.api_core.retry.Retry",
          "2290:     :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if",
          "2291:         ``retry`` is specified, the timeout applies to each individual attempt.",
          "2292:     :type timeout: float",
          "2293:     :param metadata: Additional metadata that is provided to the method.",
          "2294:     :type metadata: Sequence[Tuple[str, str]]",
          "2295:     :param gcp_conn_id: The connection ID to use connecting to Google Cloud.",
          "2296:     :type gcp_conn_id: str",
          "2297:     :param impersonation_chain: Optional service account to impersonate using short-term",
          "2298:         credentials, or chained list of accounts required to get the access_token",
          "2299:         of the last account in the list, which will be impersonated in the request.",
          "2300:         If set as a string, the account must grant the originating account",
          "2301:         the Service Account Token Creator IAM role.",
          "2302:         If set as a sequence, the identities from the list must grant",
          "2303:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "2304:         account from the list granting this role to the originating account (templated).",
          "2305:     :type impersonation_chain: Union[str, Sequence[str]]",
          "2306:     \"\"\"",
          "2308:     template_fields = (\"batch_id\", \"region\", \"project_id\", \"impersonation_chain\")",
          "2310:     def __init__(",
          "2311:         self,",
          "2313:         batch_id: str,",
          "2314:         region: str,",
          "2315:         project_id: str,",
          "2316:         retry: Optional[Retry] = None,",
          "2317:         timeout: Optional[float] = None,",
          "2318:         metadata: Optional[Sequence[Tuple[str, str]]] = \"\",",
          "2319:         gcp_conn_id: str = \"google_cloud_default\",",
          "2320:         impersonation_chain: Optional[Union[str, Sequence[str]]] = None,",
          "2322:     ):",
          "2323:         super().__init__(**kwargs)",
          "2324:         self.batch_id = batch_id",
          "2325:         self.region = region",
          "2326:         self.project_id = project_id",
          "2327:         self.retry = retry",
          "2328:         self.timeout = timeout",
          "2329:         self.metadata = metadata",
          "2330:         self.gcp_conn_id = gcp_conn_id",
          "2331:         self.impersonation_chain = impersonation_chain",
          "2333:     def execute(self, context):",
          "2334:         hook = DataprocHook(gcp_conn_id=self.gcp_conn_id, impersonation_chain=self.impersonation_chain)",
          "2335:         self.log.info(\"Deleting batch: %s\", self.batch_id)",
          "2336:         hook.delete_batch(",
          "2337:             batch_id=self.batch_id,",
          "2338:             region=self.region,",
          "2339:             project_id=self.project_id,",
          "2340:             retry=self.retry,",
          "2341:             timeout=self.timeout,",
          "2342:             metadata=self.metadata,",
          "2343:         )",
          "2344:         self.log.info(\"Batch deleted.\")",
          "2347: class DataprocGetBatchOperator(BaseOperator):",
          "2348:     \"\"\"",
          "2349:     Gets the batch workload resource representation.",
          "2351:     :param batch_id: Required. The ID to use for the batch, which will become the final component",
          "2352:         of the batch's resource name.",
          "2353:         This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.",
          "2354:     :type batch_id: str",
          "2355:     :param project_id: Required. The ID of the Google Cloud project that the cluster belongs to.",
          "2356:     :type project_id: str",
          "2357:     :param region: Required. The Cloud Dataproc region in which to handle the request.",
          "2358:     :type region: str",
          "2359:     :param retry: A retry object used to retry requests. If ``None`` is specified, requests will not be",
          "2360:         retried.",
          "2361:     :type retry: google.api_core.retry.Retry",
          "2362:     :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if",
          "2363:         ``retry`` is specified, the timeout applies to each individual attempt.",
          "2364:     :type timeout: float",
          "2365:     :param metadata: Additional metadata that is provided to the method.",
          "2366:     :type metadata: Sequence[Tuple[str, str]]",
          "2367:     :param gcp_conn_id: The connection ID to use connecting to Google Cloud.",
          "2368:     :type gcp_conn_id: str",
          "2369:     :param impersonation_chain: Optional service account to impersonate using short-term",
          "2370:         credentials, or chained list of accounts required to get the access_token",
          "2371:         of the last account in the list, which will be impersonated in the request.",
          "2372:         If set as a string, the account must grant the originating account",
          "2373:         the Service Account Token Creator IAM role.",
          "2374:         If set as a sequence, the identities from the list must grant",
          "2375:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "2376:         account from the list granting this role to the originating account (templated).",
          "2377:     :type impersonation_chain: Union[str, Sequence[str]]",
          "2378:     \"\"\"",
          "2380:     template_fields = (\"batch_id\", \"region\", \"project_id\", \"impersonation_chain\")",
          "2382:     def __init__(",
          "2383:         self,",
          "2385:         batch_id: str,",
          "2386:         region: str,",
          "2387:         project_id: str,",
          "2388:         retry: Optional[Retry] = None,",
          "2389:         timeout: Optional[float] = None,",
          "2390:         metadata: Optional[Sequence[Tuple[str, str]]] = \"\",",
          "2391:         gcp_conn_id: str = \"google_cloud_default\",",
          "2392:         impersonation_chain: Optional[Union[str, Sequence[str]]] = None,",
          "2394:     ):",
          "2395:         super().__init__(**kwargs)",
          "2396:         self.batch_id = batch_id",
          "2397:         self.region = region",
          "2398:         self.project_id = project_id",
          "2399:         self.retry = retry",
          "2400:         self.timeout = timeout",
          "2401:         self.metadata = metadata",
          "2402:         self.gcp_conn_id = gcp_conn_id",
          "2403:         self.impersonation_chain = impersonation_chain",
          "2405:     def execute(self, context):",
          "2406:         hook = DataprocHook(gcp_conn_id=self.gcp_conn_id, impersonation_chain=self.impersonation_chain)",
          "2407:         self.log.info(\"Getting batch: %s\", self.batch_id)",
          "2408:         batch = hook.get_batch(",
          "2409:             batch_id=self.batch_id,",
          "2410:             region=self.region,",
          "2411:             project_id=self.project_id,",
          "2412:             retry=self.retry,",
          "2413:             timeout=self.timeout,",
          "2414:             metadata=self.metadata,",
          "2415:         )",
          "2416:         return Batch.to_dict(batch)",
          "2419: class DataprocListBatchesOperator(BaseOperator):",
          "2420:     \"\"\"",
          "2421:     Lists batch workloads.",
          "2423:     :param project_id: Required. The ID of the Google Cloud project that the cluster belongs to.",
          "2424:     :type project_id: str",
          "2425:     :param region: Required. The Cloud Dataproc region in which to handle the request.",
          "2426:     :type region: str",
          "2427:     :param page_size: Optional. The maximum number of batches to return in each response. The service may",
          "2428:         return fewer than this value. The default page size is 20; the maximum page size is 1000.",
          "2429:     :type page_size: int",
          "2430:     :param page_token: Optional. A page token received from a previous ``ListBatches`` call.",
          "2431:         Provide this token to retrieve the subsequent page.",
          "2432:     :type page_token: str",
          "2433:     :param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests",
          "2434:         will not be retried.",
          "2435:     :type retry: Optional[Retry]",
          "2436:     :param timeout: Optional, the amount of time, in seconds, to wait for the request to complete.",
          "2437:         Note that if `retry` is specified, the timeout applies to each individual attempt.",
          "2438:     :type timeout: Optional[float]",
          "2439:     :param metadata: Optional, additional metadata that is provided to the method.",
          "2440:     :type metadata: Optional[Sequence[Tuple[str, str]]]",
          "2441:     :param gcp_conn_id: Optional, the connection ID used to connect to Google Cloud Platform.",
          "2442:     :type gcp_conn_id: Optional[str]",
          "2443:     :param impersonation_chain: Optional service account to impersonate using short-term",
          "2444:         credentials, or chained list of accounts required to get the access_token",
          "2445:         of the last account in the list, which will be impersonated in the request.",
          "2446:         If set as a string, the account must grant the originating account",
          "2447:         the Service Account Token Creator IAM role.",
          "2448:         If set as a sequence, the identities from the list must grant",
          "2449:         Service Account Token Creator IAM role to the directly preceding identity, with first",
          "2450:         account from the list granting this role to the originating account (templated).",
          "2451:     :type impersonation_chain: Union[str, Sequence[str]]",
          "2453:     :rtype: List[dict]",
          "2454:     \"\"\"",
          "2456:     template_fields = (\"region\", \"project_id\", \"impersonation_chain\")",
          "2458:     def __init__(",
          "2459:         self,",
          "2461:         region: str,",
          "2462:         project_id: Optional[str] = None,",
          "2463:         page_size: Optional[int] = None,",
          "2464:         page_token: Optional[str] = None,",
          "2465:         retry: Optional[Retry] = None,",
          "2466:         timeout: Optional[float] = None,",
          "2467:         metadata: Optional[Sequence[Tuple[str, str]]] = \"\",",
          "2468:         gcp_conn_id: str = \"google_cloud_default\",",
          "2469:         impersonation_chain: Optional[Union[str, Sequence[str]]] = None,",
          "2471:     ) -> None:",
          "2472:         super().__init__(**kwargs)",
          "2473:         self.region = region",
          "2474:         self.project_id = project_id",
          "2475:         self.page_size = page_size",
          "2476:         self.page_token = page_token",
          "2477:         self.retry = retry",
          "2478:         self.timeout = timeout",
          "2479:         self.metadata = metadata",
          "2480:         self.gcp_conn_id = gcp_conn_id",
          "2481:         self.impersonation_chain = impersonation_chain",
          "2483:     def execute(self, context):",
          "2484:         hook = DataprocHook(gcp_conn_id=self.gcp_conn_id, impersonation_chain=self.impersonation_chain)",
          "2485:         results = hook.list_batches(",
          "2486:             region=self.region,",
          "2487:             project_id=self.project_id,",
          "2488:             page_size=self.page_size,",
          "2489:             page_token=self.page_token,",
          "2490:             retry=self.retry,",
          "2491:             timeout=self.timeout,",
          "2492:             metadata=self.metadata,",
          "2493:         )",
          "2494:         return [Batch.to_dict(result) for result in results]",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "303:     'google-cloud-build>=3.0.0,<4.0.0',",
          "304:     'google-cloud-container>=0.1.1,<2.0.0',",
          "305:     'google-cloud-datacatalog>=3.0.0,<4.0.0',",
          "307:     'google-cloud-dataproc-metastore>=1.2.0,<2.0.0',",
          "308:     'google-cloud-dlp>=0.11.0,<2.0.0',",
          "309:     'google-cloud-kms>=2.0.0,<3.0.0',",
          "",
          "[Removed Lines]",
          "306:     'google-cloud-dataproc>=2.2.0,<4.0.0',",
          "",
          "[Added Lines]",
          "306:     'google-cloud-dataproc>=3.1.0,<4.0.0',",
          "",
          "---------------"
        ],
        "tests/always/test_project_structure.py||tests/always/test_project_structure.py": [
          "File: tests/always/test_project_structure.py -> tests/always/test_project_structure.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "219:         'airflow.providers.google.cloud.operators.datastore.CloudDatastoreGetOperationOperator',",
          "220:         'airflow.providers.google.cloud.sensors.gcs.GCSObjectUpdateSensor',",
          "221:         'airflow.providers.google.cloud.sensors.gcs.GCSUploadSessionCompleteSensor',",
          "222:     }",
          "224:     def test_example_dags(self):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "222:         'airflow.providers.google.cloud.operators.dataproc.DataprocGetBatchOperator',",
          "223:         'airflow.providers.google.cloud.operators.dataproc.DataprocCreateBatchOperator',",
          "224:         'airflow.providers.google.cloud.operators.dataproc.DataprocListBatchesOperator',",
          "225:         'airflow.providers.google.cloud.operators.dataproc.DataprocDeleteBatchOperator',",
          "",
          "---------------"
        ],
        "tests/providers/google/cloud/hooks/test_dataproc.py||tests/providers/google/cloud/hooks/test_dataproc.py": [
          "File: tests/providers/google/cloud/hooks/test_dataproc.py -> tests/providers/google/cloud/hooks/test_dataproc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "42:     \"labels\": LABELS,",
          "43:     \"project_id\": GCP_PROJECT,",
          "44: }",
          "46: BASE_STRING = \"airflow.providers.google.common.hooks.base_google.{}\"",
          "47: DATAPROC_STRING = \"airflow.providers.google.cloud.hooks.dataproc.{}\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "45: BATCH = {\"batch\": \"test-batch\"}",
          "46: BATCH_ID = \"batch-id\"",
          "47: BATCH_NAME = \"projects/{}/regions/{}/batches/{}\"",
          "48: PARENT = \"projects/{}/regions/{}\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "179:             )",
          "180:             assert warning_message == str(warnings[0].message)",
          "182:     @mock.patch(DATAPROC_STRING.format(\"DataprocHook.get_cluster_client\"))",
          "183:     def test_create_cluster(self, mock_client):",
          "184:         self.hook.create_cluster(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "186:     @mock.patch(DATAPROC_STRING.format(\"DataprocHook._get_credentials\"))",
          "187:     @mock.patch(DATAPROC_STRING.format(\"DataprocHook.client_info\"), new_callable=mock.PropertyMock)",
          "188:     @mock.patch(DATAPROC_STRING.format(\"BatchControllerClient\"))",
          "189:     def test_get_batch_client(self, mock_client, mock_client_info, mock_get_credentials):",
          "190:         self.hook.get_batch_client(region=GCP_LOCATION)",
          "191:         mock_client.assert_called_once_with(",
          "192:             credentials=mock_get_credentials.return_value,",
          "193:             client_info=mock_client_info.return_value,",
          "194:             client_options=None,",
          "195:         )",
          "197:     @mock.patch(DATAPROC_STRING.format(\"DataprocHook._get_credentials\"))",
          "198:     @mock.patch(DATAPROC_STRING.format(\"DataprocHook.client_info\"), new_callable=mock.PropertyMock)",
          "199:     @mock.patch(DATAPROC_STRING.format(\"BatchControllerClient\"))",
          "200:     def test_get_batch_client_region(self, mock_client, mock_client_info, mock_get_credentials):",
          "201:         self.hook.get_batch_client(region='region1')",
          "202:         mock_client.assert_called_once_with(",
          "203:             credentials=mock_get_credentials.return_value,",
          "204:             client_info=mock_client_info.return_value,",
          "205:             client_options={'api_endpoint': 'region1-dataproc.googleapis.com:443'},",
          "206:         )",
          "208:     @mock.patch(DATAPROC_STRING.format(\"DataprocHook._get_credentials\"))",
          "209:     @mock.patch(DATAPROC_STRING.format(\"DataprocHook.client_info\"), new_callable=mock.PropertyMock)",
          "210:     @mock.patch(DATAPROC_STRING.format(\"BatchControllerClient\"))",
          "211:     def test_get_batch_client_region_deprecation_warning(",
          "212:         self, mock_client, mock_client_info, mock_get_credentials",
          "213:     ):",
          "214:         warning_message = (",
          "215:             \"Parameter `location` will be deprecated. \"",
          "216:             \"Please provide value through `region` parameter instead.\"",
          "217:         )",
          "218:         with pytest.warns(DeprecationWarning) as warnings:",
          "219:             self.hook.get_batch_client(location='region1')",
          "220:             mock_client.assert_called_once_with(",
          "221:                 credentials=mock_get_credentials.return_value,",
          "222:                 client_info=mock_client_info.return_value,",
          "223:                 client_options={'api_endpoint': 'region1-dataproc.googleapis.com:443'},",
          "224:             )",
          "225:             assert warning_message == str(warnings[0].message)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "615:             )",
          "616:             assert warning_message == str(warnings[0].message)",
          "619: class TestDataProcJobBuilder(unittest.TestCase):",
          "620:     def setUp(self) -> None:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "663:     @mock.patch(DATAPROC_STRING.format(\"DataprocHook.get_batch_client\"))",
          "664:     def test_create_batch(self, mock_client):",
          "665:         self.hook.create_batch(",
          "666:             project_id=GCP_PROJECT,",
          "667:             region=GCP_LOCATION,",
          "668:             batch=BATCH,",
          "669:             batch_id=BATCH_ID,",
          "670:         )",
          "671:         mock_client.assert_called_once_with(GCP_LOCATION)",
          "672:         mock_client.return_value.create_batch.assert_called_once_with(",
          "673:             request=dict(",
          "674:                 parent=PARENT.format(GCP_PROJECT, GCP_LOCATION),",
          "675:                 batch=BATCH,",
          "676:                 batch_id=BATCH_ID,",
          "677:                 request_id=None,",
          "678:             ),",
          "679:             metadata=\"\",",
          "680:             retry=None,",
          "681:             timeout=None,",
          "682:         )",
          "684:     @mock.patch(DATAPROC_STRING.format(\"DataprocHook.get_batch_client\"))",
          "685:     def test_delete_batch(self, mock_client):",
          "686:         self.hook.delete_batch(",
          "687:             batch_id=BATCH_ID,",
          "688:             region=GCP_LOCATION,",
          "689:             project_id=GCP_PROJECT,",
          "690:         )",
          "691:         mock_client.assert_called_once_with(GCP_LOCATION)",
          "692:         mock_client.return_value.delete_batch.assert_called_once_with(",
          "693:             request=dict(",
          "694:                 name=BATCH_NAME.format(GCP_PROJECT, GCP_LOCATION, BATCH_ID),",
          "695:             ),",
          "696:             metadata=None,",
          "697:             retry=None,",
          "698:             timeout=None,",
          "699:         )",
          "701:     @mock.patch(DATAPROC_STRING.format(\"DataprocHook.get_batch_client\"))",
          "702:     def test_get_batch(self, mock_client):",
          "703:         self.hook.get_batch(",
          "704:             batch_id=BATCH_ID,",
          "705:             region=GCP_LOCATION,",
          "706:             project_id=GCP_PROJECT,",
          "707:         )",
          "708:         mock_client.assert_called_once_with(GCP_LOCATION)",
          "709:         mock_client.return_value.get_batch.assert_called_once_with(",
          "710:             request=dict(",
          "711:                 name=BATCH_NAME.format(GCP_PROJECT, GCP_LOCATION, BATCH_ID),",
          "712:             ),",
          "713:             metadata=None,",
          "714:             retry=None,",
          "715:             timeout=None,",
          "716:         )",
          "718:     @mock.patch(DATAPROC_STRING.format(\"DataprocHook.get_batch_client\"))",
          "719:     def test_list_batches(self, mock_client):",
          "720:         self.hook.list_batches(",
          "721:             project_id=GCP_PROJECT,",
          "722:             region=GCP_LOCATION,",
          "723:         )",
          "724:         mock_client.assert_called_once_with(GCP_LOCATION)",
          "725:         mock_client.return_value.list_batches.assert_called_once_with(",
          "726:             request=dict(",
          "727:                 parent=PARENT.format(GCP_PROJECT, GCP_LOCATION),",
          "728:                 page_size=None,",
          "729:                 page_token=None,",
          "730:             ),",
          "731:             metadata=None,",
          "732:             retry=None,",
          "733:             timeout=None,",
          "734:         )",
          "",
          "---------------"
        ],
        "tests/providers/google/cloud/operators/test_dataproc.py||tests/providers/google/cloud/operators/test_dataproc.py": [
          "File: tests/providers/google/cloud/operators/test_dataproc.py -> tests/providers/google/cloud/operators/test_dataproc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: from airflow.providers.google.cloud.operators.dataproc import (",
          "30:     ClusterGenerator,",
          "31:     DataprocClusterLink,",
          "32:     DataprocCreateClusterOperator,",
          "33:     DataprocCreateWorkflowTemplateOperator,",
          "34:     DataprocDeleteClusterOperator,",
          "35:     DataprocInstantiateInlineWorkflowTemplateOperator,",
          "36:     DataprocInstantiateWorkflowTemplateOperator,",
          "37:     DataprocJobLink,",
          "38:     DataprocScaleClusterOperator,",
          "39:     DataprocSubmitHadoopJobOperator,",
          "40:     DataprocSubmitHiveJobOperator,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32:     DataprocCreateBatchOperator,",
          "35:     DataprocDeleteBatchOperator,",
          "37:     DataprocGetBatchOperator,",
          "41:     DataprocListBatchesOperator,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "199:     \"region\": GCP_LOCATION,",
          "200:     \"project_id\": GCP_PROJECT,",
          "201: }",
          "204: def assert_warning(msg: str, warnings):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "206: BATCH_ID = \"test-batch-id\"",
          "207: BATCH = {",
          "208:     \"spark_batch\": {",
          "209:         \"jar_file_uris\": [\"file:///usr/lib/spark/examples/jars/spark-examples.jar\"],",
          "210:         \"main_class\": \"org.apache.spark.examples.SparkPi\",",
          "211:     },",
          "212: }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1661:                 template=WORKFLOW_TEMPLATE,",
          "1662:             )",
          "1663:             op.execute(context={})",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1677: class TestDataprocCreateBatchOperator:",
          "1678:     @mock.patch(DATAPROC_PATH.format(\"Batch.to_dict\"))",
          "1679:     @mock.patch(DATAPROC_PATH.format(\"DataprocHook\"))",
          "1680:     def test_execute(self, mock_hook, to_dict_mock):",
          "1681:         op = DataprocCreateBatchOperator(",
          "1682:             task_id=TASK_ID,",
          "1683:             gcp_conn_id=GCP_CONN_ID,",
          "1684:             impersonation_chain=IMPERSONATION_CHAIN,",
          "1685:             region=GCP_LOCATION,",
          "1686:             project_id=GCP_PROJECT,",
          "1687:             batch=BATCH,",
          "1688:             batch_id=BATCH_ID,",
          "1689:             request_id=REQUEST_ID,",
          "1690:             retry=RETRY,",
          "1691:             timeout=TIMEOUT,",
          "1692:             metadata=METADATA,",
          "1693:         )",
          "1694:         op.execute(context={})",
          "1695:         mock_hook.assert_called_once_with(gcp_conn_id=GCP_CONN_ID, impersonation_chain=IMPERSONATION_CHAIN)",
          "1696:         mock_hook.return_value.create_batch.assert_called_once_with(",
          "1697:             region=GCP_LOCATION,",
          "1698:             project_id=GCP_PROJECT,",
          "1699:             batch=BATCH,",
          "1700:             batch_id=BATCH_ID,",
          "1701:             request_id=REQUEST_ID,",
          "1702:             retry=RETRY,",
          "1703:             timeout=TIMEOUT,",
          "1704:             metadata=METADATA,",
          "1705:         )",
          "1708: class TestDataprocDeleteBatchOperator:",
          "1709:     @mock.patch(DATAPROC_PATH.format(\"DataprocHook\"))",
          "1710:     def test_execute(self, mock_hook):",
          "1711:         op = DataprocDeleteBatchOperator(",
          "1712:             task_id=TASK_ID,",
          "1713:             gcp_conn_id=GCP_CONN_ID,",
          "1714:             impersonation_chain=IMPERSONATION_CHAIN,",
          "1715:             project_id=GCP_PROJECT,",
          "1716:             region=GCP_LOCATION,",
          "1717:             batch_id=BATCH_ID,",
          "1718:             retry=RETRY,",
          "1719:             timeout=TIMEOUT,",
          "1720:             metadata=METADATA,",
          "1721:         )",
          "1722:         op.execute(context={})",
          "1723:         mock_hook.assert_called_once_with(gcp_conn_id=GCP_CONN_ID, impersonation_chain=IMPERSONATION_CHAIN)",
          "1724:         mock_hook.return_value.delete_batch.assert_called_once_with(",
          "1725:             project_id=GCP_PROJECT,",
          "1726:             region=GCP_LOCATION,",
          "1727:             batch_id=BATCH_ID,",
          "1728:             retry=RETRY,",
          "1729:             timeout=TIMEOUT,",
          "1730:             metadata=METADATA,",
          "1731:         )",
          "1734: class TestDataprocGetBatchOperator:",
          "1735:     @mock.patch(DATAPROC_PATH.format(\"Batch.to_dict\"))",
          "1736:     @mock.patch(DATAPROC_PATH.format(\"DataprocHook\"))",
          "1737:     def test_execute(self, mock_hook, to_dict_mock):",
          "1738:         op = DataprocGetBatchOperator(",
          "1739:             task_id=TASK_ID,",
          "1740:             gcp_conn_id=GCP_CONN_ID,",
          "1741:             impersonation_chain=IMPERSONATION_CHAIN,",
          "1742:             project_id=GCP_PROJECT,",
          "1743:             region=GCP_LOCATION,",
          "1744:             batch_id=BATCH_ID,",
          "1745:             retry=RETRY,",
          "1746:             timeout=TIMEOUT,",
          "1747:             metadata=METADATA,",
          "1748:         )",
          "1749:         op.execute(context={})",
          "1750:         mock_hook.assert_called_once_with(gcp_conn_id=GCP_CONN_ID, impersonation_chain=IMPERSONATION_CHAIN)",
          "1751:         mock_hook.return_value.get_batch.assert_called_once_with(",
          "1752:             project_id=GCP_PROJECT,",
          "1753:             region=GCP_LOCATION,",
          "1754:             batch_id=BATCH_ID,",
          "1755:             retry=RETRY,",
          "1756:             timeout=TIMEOUT,",
          "1757:             metadata=METADATA,",
          "1758:         )",
          "1761: class TestDataprocListBatchesOperator:",
          "1762:     @mock.patch(DATAPROC_PATH.format(\"DataprocHook\"))",
          "1763:     def test_execute(self, mock_hook):",
          "1764:         page_token = \"page_token\"",
          "1765:         page_size = 42",
          "1767:         op = DataprocListBatchesOperator(",
          "1768:             task_id=TASK_ID,",
          "1769:             gcp_conn_id=GCP_CONN_ID,",
          "1770:             impersonation_chain=IMPERSONATION_CHAIN,",
          "1771:             region=GCP_LOCATION,",
          "1772:             project_id=GCP_PROJECT,",
          "1773:             page_size=page_size,",
          "1774:             page_token=page_token,",
          "1775:             retry=RETRY,",
          "1776:             timeout=TIMEOUT,",
          "1777:             metadata=METADATA,",
          "1778:         )",
          "1779:         op.execute(context={})",
          "1780:         mock_hook.assert_called_once_with(gcp_conn_id=GCP_CONN_ID, impersonation_chain=IMPERSONATION_CHAIN)",
          "1781:         mock_hook.return_value.list_batches.assert_called_once_with(",
          "1782:             region=GCP_LOCATION,",
          "1783:             project_id=GCP_PROJECT,",
          "1784:             page_size=page_size,",
          "1785:             page_token=page_token,",
          "1786:             retry=RETRY,",
          "1787:             timeout=TIMEOUT,",
          "1788:             metadata=METADATA,",
          "1789:         )",
          "",
          "---------------"
        ],
        "tests/providers/google/cloud/operators/test_dataproc_system.py||tests/providers/google/cloud/operators/test_dataproc_system.py": [
          "File: tests/providers/google/cloud/operators/test_dataproc_system.py -> tests/providers/google/cloud/operators/test_dataproc_system.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "63:     @provide_gcp_context(GCP_DATAPROC_KEY)",
          "64:     def test_run_example_dag(self):",
          "65:         self.run_dag(dag_id=\"example_gcp_dataproc\", dag_folder=CLOUD_DAG_FOLDER)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "67:     @provide_gcp_context(GCP_DATAPROC_KEY)",
          "68:     def test_run_batch_example_dag(self):",
          "69:         self.run_dag(dag_id=\"example_gcp_batch_dataproc\", dag_folder=CLOUD_DAG_FOLDER)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f05725ebe339430e46ea8dccfeefa97cd9f2d9a4",
      "candidate_info": {
        "commit_hash": "f05725ebe339430e46ea8dccfeefa97cd9f2d9a4",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f05725ebe339430e46ea8dccfeefa97cd9f2d9a4",
        "files": [
          "Dockerfile",
          "Dockerfile.ci",
          "IMAGES.rst",
          "dev/README_RELEASE_PROVIDER_PACKAGES.md",
          "docs/docker-stack/build-arg-ref.rst",
          "scripts/ci/kubernetes/ci_run_kubernetes_tests.sh",
          "scripts/ci/libraries/_initialization.sh",
          "scripts/docker/common.sh",
          "scripts/docker/compile_www_assets.sh",
          "scripts/docker/install_additional_dependencies.sh",
          "scripts/docker/install_airflow.sh",
          "scripts/docker/install_airflow_dependencies_from_branch_tip.sh",
          "scripts/docker/install_from_docker_context_files.sh",
          "scripts/docker/install_mssql.sh",
          "scripts/docker/install_mysql.sh",
          "scripts/docker/install_pip_version.sh",
          "scripts/in_container/_in_container_utils.sh",
          "scripts/in_container/prod/entrypoint_prod.sh",
          "scripts/in_container/run_prepare_airflow_packages.sh"
        ],
        "message": "Cleaner output of docker image building scripts (#20679)\n\n(cherry picked from commit fb8780013227a20462878b7f9286d083630c0bc2)",
        "before_after_code_files": [
          "Dockerfile.ci||Dockerfile.ci",
          "scripts/ci/kubernetes/ci_run_kubernetes_tests.sh||scripts/ci/kubernetes/ci_run_kubernetes_tests.sh",
          "scripts/ci/libraries/_initialization.sh||scripts/ci/libraries/_initialization.sh",
          "scripts/docker/common.sh||scripts/docker/common.sh",
          "scripts/docker/compile_www_assets.sh||scripts/docker/compile_www_assets.sh",
          "scripts/docker/install_additional_dependencies.sh||scripts/docker/install_additional_dependencies.sh",
          "scripts/docker/install_airflow.sh||scripts/docker/install_airflow.sh",
          "scripts/docker/install_airflow_dependencies_from_branch_tip.sh||scripts/docker/install_airflow_dependencies_from_branch_tip.sh",
          "scripts/docker/install_from_docker_context_files.sh||scripts/docker/install_from_docker_context_files.sh",
          "scripts/docker/install_mssql.sh||scripts/docker/install_mssql.sh",
          "scripts/docker/install_mysql.sh||scripts/docker/install_mysql.sh",
          "scripts/docker/install_pip_version.sh||scripts/docker/install_pip_version.sh",
          "scripts/in_container/_in_container_utils.sh||scripts/in_container/_in_container_utils.sh",
          "scripts/in_container/prod/entrypoint_prod.sh||scripts/in_container/prod/entrypoint_prod.sh",
          "scripts/in_container/run_prepare_airflow_packages.sh||scripts/in_container/run_prepare_airflow_packages.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "Dockerfile.ci||Dockerfile.ci": [
          "File: Dockerfile.ci -> Dockerfile.ci",
          "--- Hunk 1 ---",
          "[Context before]",
          "225: # By default in the image, we are installing all providers when installing from sources",
          "226: ARG INSTALL_PROVIDERS_FROM_SOURCES=\"true\"",
          "227: ARG INSTALL_FROM_PYPI=\"true\"",
          "229: # Setup PIP",
          "230: # By default PIP install run without cache to make image smaller",
          "231: ARG PIP_NO_CACHE_DIR=\"true\"",
          "",
          "[Removed Lines]",
          "228: ARG AIRFLOW_PIP_VERSION=21.2.4",
          "",
          "[Added Lines]",
          "228: ARG AIRFLOW_PIP_VERSION=21.3.1",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "260:     INSTALL_MYSQL_CLIENT=\"true\" \\",
          "261:     INSTALL_MSSQL_CLIENT=\"true\" \\",
          "262:     AIRFLOW_INSTALLATION_METHOD=\".\" \\",
          "264:     AIRFLOW_INSTALL_EDITABLE_FLAG=\"--editable\" \\",
          "265:     AIRFLOW_VERSION_SPECIFICATION=\"\" \\",
          "266:     PIP_NO_CACHE_DIR=${PIP_NO_CACHE_DIR} \\",
          "",
          "[Removed Lines]",
          "263:     AIRFLOW_INSTALL_USER_FLAG=\"\" \\",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "scripts/ci/kubernetes/ci_run_kubernetes_tests.sh||scripts/ci/kubernetes/ci_run_kubernetes_tests.sh": [
          "File: scripts/ci/kubernetes/ci_run_kubernetes_tests.sh -> scripts/ci/kubernetes/ci_run_kubernetes_tests.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "83:     . \"${virtualenv_path}/bin/activate\"",
          "87:     local constraints=(",
          "88:         --constraint",
          "",
          "[Removed Lines]",
          "85:     pip install --upgrade \"pip==${AIRFLOW_PIP_VERSION}\" \"wheel==${WHEEL_VERSION}\"",
          "",
          "[Added Lines]",
          "85:     pip install \"pip==${AIRFLOW_PIP_VERSION}\" \"wheel==${WHEEL_VERSION}\"",
          "",
          "---------------"
        ],
        "scripts/ci/libraries/_initialization.sh||scripts/ci/libraries/_initialization.sh": [
          "File: scripts/ci/libraries/_initialization.sh -> scripts/ci/libraries/_initialization.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "422:     export INSTALLED_EXTRAS=\"async,amazon,celery,cncf.kubernetes,docker,dask,elasticsearch,ftp,grpc,hashicorp,http,imap,ldap,google,microsoft.azure,mysql,postgres,redis,sendgrid,sftp,slack,ssh,statsd,virtualenv\"",
          "425:     export AIRFLOW_PIP_VERSION",
          "427:     # We also pin version of wheel used to get consistent builds",
          "",
          "[Removed Lines]",
          "424:     AIRFLOW_PIP_VERSION=${AIRFLOW_PIP_VERSION:=\"21.2.4\"}",
          "",
          "[Added Lines]",
          "424:     AIRFLOW_PIP_VERSION=${AIRFLOW_PIP_VERSION:=\"21.3.1\"}",
          "",
          "---------------"
        ],
        "scripts/docker/common.sh||scripts/docker/common.sh": [
          "File: scripts/docker/common.sh -> scripts/docker/common.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: set -euo pipefail",
          "29: function common::get_airflow_version_specification() {",
          "30:     if [[ -z ${AIRFLOW_VERSION_SPECIFICATION}",
          "",
          "[Removed Lines]",
          "20: test -v INSTALL_MYSQL_CLIENT",
          "21: test -v INSTALL_MSSQL_CLIENT",
          "22: test -v AIRFLOW_INSTALL_USER_FLAG",
          "23: test -v AIRFLOW_REPO",
          "24: test -v AIRFLOW_BRANCH",
          "25: test -v AIRFLOW_PIP_VERSION",
          "27: set -x",
          "",
          "[Added Lines]",
          "20: : \"${INSTALL_MYSQL_CLIENT:?Should be true or false}\"",
          "21: : \"${INSTALL_MSSQL_CLIENT:?Should be true or false}\"",
          "22: : \"${AIRFLOW_REPO:?Should be set}\"",
          "23: : \"${AIRFLOW_BRANCH:?Should be set}\"",
          "24: : \"${AIRFLOW_PIP_VERSION:?Should be set}\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "60:         AIRFLOW_CONSTRAINTS_LOCATION=\"${constraints_base}/${AIRFLOW_CONSTRAINTS}-${python_version}.txt\"",
          "61:     fi",
          "62: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "60: function common::show_pip_version_and_location() {",
          "61:    echo \"PATH=${PATH}\"",
          "62:    echo \"pip on path: $(which pip)\"",
          "63:    echo \"Using pip: $(pip --version)\"",
          "64: }",
          "",
          "---------------"
        ],
        "scripts/docker/compile_www_assets.sh||scripts/docker/compile_www_assets.sh": [
          "File: scripts/docker/compile_www_assets.sh -> scripts/docker/compile_www_assets.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: # shellcheck disable=SC2086",
          "19: set -euo pipefail",
          "22: # Installs additional dependencies passed as Argument to the Docker build command",
          "23: function compile_www_assets() {",
          "",
          "[Removed Lines]",
          "20: set -x",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "35:         www_dir=\"$(python -m site --user-site)/airflow/www\"",
          "36:     fi",
          "37:     pushd ${www_dir} || exit 1",
          "40:     find package.json yarn.lock static/css static/js -type f | sort | xargs md5sum > \"${md5sum_file}\"",
          "41:     rm -rf \"${www_dir}/node_modules\"",
          "42:     rm -vf \"${www_dir}\"/{package.json,yarn.lock,.eslintignore,.eslintrc,.stylelintignore,.stylelintrc,compile_assets.sh,webpack.config.js}",
          "",
          "[Removed Lines]",
          "38:     yarn install --frozen-lockfile --no-cache",
          "39:     yarn run prod",
          "",
          "[Added Lines]",
          "37:     set +e",
          "38:     yarn install --frozen-lockfile --no-cache 2>/tmp/out-yarn-install.txt",
          "39:     local res=$?",
          "40:     if [[ ${res} != 0 ]]; then",
          "41:         >&2 echo",
          "42:         >&2 echo \"Error when running yarn install:\"",
          "43:         >&2 echo",
          "44:         >&2 cat /tmp/out-yarn-install.txt && rm -f /tmp/out-yarn-install.txt",
          "45:         exit 1",
          "46:     fi",
          "47:     yarn run prod 2>/tmp/out-yarn-run.txt",
          "48:     res=$?",
          "49:     if [[ ${res} != 0 ]]; then",
          "50:         >&2 echo",
          "51:         >&2 echo \"Error when running yarn install:\"",
          "52:         >&2 echo",
          "53:         >&2 cat /tmp/out-yarn-run.txt && rm -f /tmp/out-yarn-run.txt",
          "54:         exit 1",
          "55:     fi",
          "56:     rm -f /tmp/out-yarn-run.txt",
          "57:     set -e",
          "",
          "---------------"
        ],
        "scripts/docker/install_additional_dependencies.sh||scripts/docker/install_additional_dependencies.sh": [
          "File: scripts/docker/install_additional_dependencies.sh -> scripts/docker/install_additional_dependencies.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: # shellcheck disable=SC2086",
          "19: set -euo pipefail",
          "27: # shellcheck source=scripts/docker/common.sh",
          "28: . \"$( dirname \"${BASH_SOURCE[0]}\" )/common.sh\"",
          "",
          "[Removed Lines]",
          "21: test -v UPGRADE_TO_NEWER_DEPENDENCIES",
          "22: test -v ADDITIONAL_PYTHON_DEPS",
          "23: test -v EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS",
          "24: test -v AIRFLOW_INSTALL_USER_FLAG",
          "25: test -v AIRFLOW_PIP_VERSION",
          "",
          "[Added Lines]",
          "21: : \"${UPGRADE_TO_NEWER_DEPENDENCIES:?Should be true or false}\"",
          "22: : \"${ADDITIONAL_PYTHON_DEPS:?Should be set}\"",
          "23: : \"${EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS:?Should be set}\"",
          "24: : \"${AIRFLOW_PIP_VERSION:?Should be set}\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "36:         echo",
          "37:         echo Installing additional dependencies while upgrading to newer dependencies",
          "38:         echo",
          "40:             ${ADDITIONAL_PYTHON_DEPS} ${EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS}",
          "41:         # make sure correct PIP version is used",
          "43:         pip check",
          "44:     else",
          "45:         echo",
          "46:         echo Installing additional dependencies upgrading only if needed",
          "47:         echo",
          "50:             ${ADDITIONAL_PYTHON_DEPS}",
          "51:         # make sure correct PIP version is used",
          "53:         pip check",
          "54:     fi",
          "55: }",
          "",
          "[Removed Lines]",
          "39:         pip install ${AIRFLOW_INSTALL_USER_FLAG} --upgrade --upgrade-strategy eager \\",
          "42:         pip install ${AIRFLOW_INSTALL_USER_FLAG} --upgrade \"pip==${AIRFLOW_PIP_VERSION}\"",
          "48:         pip install ${AIRFLOW_INSTALL_USER_FLAG} \\",
          "49:             --upgrade --upgrade-strategy only-if-needed \\",
          "52:         pip install ${AIRFLOW_INSTALL_USER_FLAG} --upgrade \"pip==${AIRFLOW_PIP_VERSION}\"",
          "",
          "[Added Lines]",
          "38:         pip install --upgrade --upgrade-strategy eager \\",
          "41:         pip install --disable-pip-version-check \"pip==${AIRFLOW_PIP_VERSION}\"",
          "47:         pip install --upgrade --upgrade-strategy only-if-needed \\",
          "50:         pip install --disable-pip-version-check \"pip==${AIRFLOW_PIP_VERSION}\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "57: common::get_airflow_version_specification",
          "58: common::override_pip_version_if_needed",
          "59: common::get_constraints_location",
          "61: install_additional_dependencies",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "58: common::show_pip_version_and_location",
          "",
          "---------------"
        ],
        "scripts/docker/install_airflow.sh||scripts/docker/install_airflow.sh": [
          "File: scripts/docker/install_airflow.sh -> scripts/docker/install_airflow.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "47:         echo Installing all packages with eager upgrade",
          "48:         echo",
          "49:         # eager upgrade",
          "51:             \"${AIRFLOW_INSTALLATION_METHOD}[${AIRFLOW_EXTRAS}]${AIRFLOW_VERSION_SPECIFICATION}\" \\",
          "52:             ${EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS}",
          "53:         if [[ -n \"${AIRFLOW_INSTALL_EDITABLE_FLAG}\" ]]; then",
          "",
          "[Removed Lines]",
          "50:         pip install ${AIRFLOW_INSTALL_USER_FLAG} --upgrade --upgrade-strategy eager \\",
          "",
          "[Added Lines]",
          "50:         pip install --upgrade --upgrade-strategy eager \\",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "59:         fi",
          "61:         # make sure correct PIP version is used",
          "63:         pip check",
          "64:     else \\",
          "65:         echo",
          "66:         echo Installing all packages with constraints and upgrade if needed",
          "67:         echo",
          "69:             \"${AIRFLOW_INSTALLATION_METHOD}[${AIRFLOW_EXTRAS}]${AIRFLOW_VERSION_SPECIFICATION}\" \\",
          "70:             --constraint \"${AIRFLOW_CONSTRAINTS_LOCATION}\"",
          "71:         # make sure correct PIP version is used",
          "73:         # then upgrade if needed without using constraints to account for new limits in setup.py",
          "75:             ${AIRFLOW_INSTALL_EDITABLE_FLAG} \\",
          "77:         # make sure correct PIP version is used",
          "79:         pip check",
          "80:     fi",
          "",
          "[Removed Lines]",
          "62:         pip install ${AIRFLOW_INSTALL_USER_FLAG} --upgrade \"pip==${AIRFLOW_PIP_VERSION}\"",
          "68:         pip install ${AIRFLOW_INSTALL_USER_FLAG} ${AIRFLOW_INSTALL_EDITABLE_FLAG} \\",
          "72:         pip install ${AIRFLOW_INSTALL_USER_FLAG} --upgrade \"pip==${AIRFLOW_PIP_VERSION}\"",
          "74:         pip install ${AIRFLOW_INSTALL_USER_FLAG} --upgrade --upgrade-strategy only-if-needed \\",
          "76:             \"${AIRFLOW_INSTALLATION_METHOD}[${AIRFLOW_EXTRAS}]${AIRFLOW_VERSION_SPECIFICATION}\" \\",
          "78:         pip install ${AIRFLOW_INSTALL_USER_FLAG} --upgrade \"pip==${AIRFLOW_PIP_VERSION}\"",
          "",
          "[Added Lines]",
          "62:         pip install --disable-pip-version-check \"pip==${AIRFLOW_PIP_VERSION}\"",
          "68:         pip install ${AIRFLOW_INSTALL_EDITABLE_FLAG} \\",
          "72:         pip install --disable-pip-version-check \"pip==${AIRFLOW_PIP_VERSION}\"",
          "74:         pip install --upgrade --upgrade-strategy only-if-needed \\",
          "76:             \"${AIRFLOW_INSTALLATION_METHOD}[${AIRFLOW_EXTRAS}]${AIRFLOW_VERSION_SPECIFICATION}\"",
          "78:         pip install --disable-pip-version-check \"pip==${AIRFLOW_PIP_VERSION}\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "84: common::get_airflow_version_specification",
          "85: common::override_pip_version_if_needed",
          "86: common::get_constraints_location",
          "88: install_airflow",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "87: common::show_pip_version_and_location",
          "",
          "---------------"
        ],
        "scripts/docker/install_airflow_dependencies_from_branch_tip.sh||scripts/docker/install_airflow_dependencies_from_branch_tip.sh": [
          "File: scripts/docker/install_airflow_dependencies_from_branch_tip.sh -> scripts/docker/install_airflow_dependencies_from_branch_tip.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "39:     fi",
          "40:     # Install latest set of dependencies using constraints. In case constraints were upgraded and there",
          "41:     # are conflicts, this might fail, but it should be fixed in the following installation steps",
          "43:       \"https://github.com/${AIRFLOW_REPO}/archive/${AIRFLOW_BRANCH}.tar.gz#egg=apache-airflow[${AIRFLOW_EXTRAS}]\" \\",
          "44:       --constraint \"${AIRFLOW_CONSTRAINTS_LOCATION}\" || true",
          "45:     # make sure correct PIP version is used",
          "48:     echo",
          "49:     echo Uninstalling just airflow. Dependencies remain.",
          "50:     echo",
          "",
          "[Removed Lines]",
          "42:     pip install ${AIRFLOW_INSTALL_USER_FLAG} \\",
          "46:     pip install ${AIRFLOW_INSTALL_USER_FLAG} --upgrade \"pip==${AIRFLOW_PIP_VERSION}\"",
          "47:     pip freeze | grep apache-airflow-providers | xargs pip uninstall --yes || true",
          "",
          "[Added Lines]",
          "42:     pip install \\",
          "46:     pip install --disable-pip-version-check \"pip==${AIRFLOW_PIP_VERSION}\"",
          "47:     pip freeze | grep apache-airflow-providers | xargs pip uninstall --yes 2>/dev/null || true",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "54: common::get_airflow_version_specification",
          "55: common::override_pip_version_if_needed",
          "56: common::get_constraints_location",
          "58: install_airflow_dependencies_from_branch_tip",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "57: common::show_pip_version_and_location",
          "",
          "---------------"
        ],
        "scripts/docker/install_from_docker_context_files.sh||scripts/docker/install_from_docker_context_files.sh": [
          "File: scripts/docker/install_from_docker_context_files.sh -> scripts/docker/install_from_docker_context_files.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "34:     local pip_flags=(",
          "35:         # Don't quote this -- if it is empty we don't want it to create an",
          "36:         # empty array element",
          "38:         --find-links=\"file:///docker-context-files\"",
          "39:     )",
          "",
          "[Removed Lines]",
          "37:         ${AIRFLOW_INSTALL_USER_FLAG}",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "88:             --constraint /tmp/constraints.txt",
          "89:         rm /tmp/constraints.txt",
          "90:         # make sure correct PIP version is used \\",
          "92:         # then upgrade if needed without using constraints to account for new limits in setup.py",
          "94:              ${reinstalling_apache_airflow_package} ${reinstalling_apache_airflow_providers_packages}",
          "95:     fi",
          "97:     # make sure correct PIP version is left installed",
          "99:     pip check",
          "101: }",
          "",
          "[Removed Lines]",
          "91:         pip install ${AIRFLOW_INSTALL_USER_FLAG} --upgrade \"pip==${AIRFLOW_PIP_VERSION}\"",
          "93:         pip install ${AIRFLOW_INSTALL_USER_FLAG} --upgrade --upgrade-strategy only-if-needed \\",
          "98:     pip install ${AIRFLOW_INSTALL_USER_FLAG} --upgrade \"pip==${AIRFLOW_PIP_VERSION}\"",
          "",
          "[Added Lines]",
          "90:         pip install \"pip==${AIRFLOW_PIP_VERSION}\"",
          "92:         pip install --upgrade --upgrade-strategy only-if-needed \\",
          "97:     pip install \"pip==${AIRFLOW_PIP_VERSION}\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "104: # without dependencies. This is extremely useful in case you want to install via pip-download",
          "105: # method on air-gaped system where you do not want to download any dependencies from remote hosts",
          "106: # which is a requirement for serious installations",
          "108:     echo",
          "109:     echo Force re-installing all other package from local files without dependencies",
          "110:     echo",
          "",
          "[Removed Lines]",
          "107: install_all_other_packages_from_docker_context_files() {",
          "",
          "[Added Lines]",
          "106: function install_all_other_packages_from_docker_context_files() {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "113:     reinstalling_other_packages=$(ls /docker-context-files/*.{whl,tar.gz} 2>/dev/null | \\",
          "114:         grep -v apache_airflow | grep -v apache-airflow || true)",
          "115:     if [[ -n \"${reinstalling_other_packages}\" ]]; then \\",
          "117:         # make sure correct PIP version is used",
          "119:     fi",
          "120: }",
          "122: common::get_airflow_version_specification",
          "123: common::override_pip_version_if_needed",
          "124: common::get_constraints_location",
          "126: install_airflow_and_providers_from_docker_context_files",
          "127: install_all_other_packages_from_docker_context_files",
          "",
          "[Removed Lines]",
          "116:         pip install ${AIRFLOW_INSTALL_USER_FLAG} --force-reinstall --no-deps --no-index ${reinstalling_other_packages}",
          "118:         pip install ${AIRFLOW_INSTALL_USER_FLAG} --upgrade \"pip==${AIRFLOW_PIP_VERSION}\"",
          "",
          "[Added Lines]",
          "115:         pip install --force-reinstall --no-deps --no-index ${reinstalling_other_packages}",
          "117:         pip install \"pip==${AIRFLOW_PIP_VERSION}\"",
          "124: common::show_pip_version_and_location",
          "128: common::show_pip_version_and_location",
          "",
          "---------------"
        ],
        "scripts/docker/install_mssql.sh||scripts/docker/install_mssql.sh": [
          "File: scripts/docker/install_mssql.sh -> scripts/docker/install_mssql.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "19: function install_mssql_client() {",
          "20:     echo",
          "21:     echo Installing mssql client",
          "22:     echo",
          "25:     apt-get update -yqq",
          "26:     apt-get upgrade -yqq",
          "27:     ACCEPT_EULA=Y apt-get -yqq install -y --no-install-recommends msodbcsql17 mssql-tools",
          "",
          "[Removed Lines]",
          "18: set -exuo pipefail",
          "23:     curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -",
          "24:     curl https://packages.microsoft.com/config/debian/10/prod.list > /etc/apt/sources.list.d/mssql-release.list",
          "",
          "[Added Lines]",
          "18: set -euo pipefail",
          "23:     curl --silent https://packages.microsoft.com/keys/microsoft.asc | apt-key add - >/dev/null 2>&1",
          "24:     curl --silent https://packages.microsoft.com/config/debian/10/prod.list > /etc/apt/sources.list.d/mssql-release.list",
          "",
          "---------------"
        ],
        "scripts/docker/install_mysql.sh||scripts/docker/install_mysql.sh": [
          "File: scripts/docker/install_mysql.sh -> scripts/docker/install_mysql.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "19: declare -a packages",
          "21: MYSQL_VERSION=\"8.0\"",
          "",
          "[Removed Lines]",
          "18: set -exuo pipefail",
          "",
          "[Added Lines]",
          "18: set -euo pipefail",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "46:     for keyserver in $(shuf -e ha.pool.sks-keyservers.net hkp://p80.pool.sks-keyservers.net:80 \\",
          "47:                                keyserver.ubuntu.com hkp://keyserver.ubuntu.com:80)",
          "48:     do",
          "50:     done",
          "51:     set -e",
          "52:     gpg --export \"${key}\" > /etc/apt/trusted.gpg.d/mysql.gpg",
          "53:     gpgconf --kill all",
          "54:     rm -rf \"${GNUPGHOME}\"",
          "55:     unset GNUPGHOME",
          "57:     echo \"deb http://repo.mysql.com/apt/debian/ buster mysql-${MYSQL_VERSION}\" | tee -a /etc/apt/sources.list.d/mysql.list",
          "58:     apt-get update",
          "59:     apt-get install --no-install-recommends -y \"${packages[@]}\"",
          "",
          "[Removed Lines]",
          "49:         gpg --keyserver \"${keyserver}\" --recv-keys \"${key}\" && break",
          "56:     apt-key list > /dev/null 2>&1",
          "",
          "[Added Lines]",
          "49:         gpg --keyserver \"${keyserver}\" --recv-keys \"${key}\" 2>&1 && break",
          "",
          "---------------"
        ],
        "scripts/docker/install_pip_version.sh||scripts/docker/install_pip_version.sh": [
          "File: scripts/docker/install_pip_version.sh -> scripts/docker/install_pip_version.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: . \"$( dirname \"${BASH_SOURCE[0]}\" )/common.sh\"",
          "32: function install_pip_version() {",
          "34: }",
          "36: common::get_airflow_version_specification",
          "37: common::override_pip_version_if_needed",
          "38: common::get_constraints_location",
          "40: install_pip_version",
          "",
          "[Removed Lines]",
          "33:     pip install --no-cache-dir --upgrade \"pip==${AIRFLOW_PIP_VERSION}\" && mkdir -p /root/.local/bin",
          "",
          "[Added Lines]",
          "33:     pip install --disable-pip-version-check --no-cache-dir --upgrade \"pip==${AIRFLOW_PIP_VERSION}\" &&",
          "34:         mkdir -p ${HOME}/.local/bin",
          "40: common::show_pip_version_and_location",
          "",
          "---------------"
        ],
        "scripts/in_container/_in_container_utils.sh||scripts/in_container/_in_container_utils.sh": [
          "File: scripts/in_container/_in_container_utils.sh -> scripts/in_container/_in_container_utils.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "242:     echo",
          "244:     rm -rf \"${AIRFLOW_SOURCES}\"/*.egg-info",
          "246: }",
          "248: function install_local_airflow_with_eager_upgrade() {",
          "",
          "[Removed Lines]",
          "245:     pip install --upgrade \"apache-airflow==${version}\"",
          "",
          "[Added Lines]",
          "245:     pip install \"apache-airflow==${version}\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "333: function install_supported_pip_version() {",
          "334:     group_start \"Install supported PIP version ${AIRFLOW_PIP_VERSION}\"",
          "336:     group_end",
          "337: }",
          "",
          "[Removed Lines]",
          "335:     pip install --upgrade \"pip==${AIRFLOW_PIP_VERSION}\"",
          "",
          "[Added Lines]",
          "335:     pip install --disable-pip-version-check \"pip==${AIRFLOW_PIP_VERSION}\"",
          "",
          "---------------"
        ],
        "scripts/in_container/prod/entrypoint_prod.sh||scripts/in_container/prod/entrypoint_prod.sh": [
          "File: scripts/in_container/prod/entrypoint_prod.sh -> scripts/in_container/prod/entrypoint_prod.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "290:     >&2 echo \"         the container starts, so it is onlny useful for testing and trying out\"",
          "291:     >&2 echo \"         of adding dependencies.\"",
          "292:     >&2 echo",
          "294: fi",
          "",
          "[Removed Lines]",
          "293:     pip install --no-cache-dir --user ${_PIP_ADDITIONAL_REQUIREMENTS}",
          "",
          "[Added Lines]",
          "293:     pip install --no-cache-dir ${_PIP_ADDITIONAL_REQUIREMENTS}",
          "",
          "---------------"
        ],
        "scripts/in_container/run_prepare_airflow_packages.sh||scripts/in_container/run_prepare_airflow_packages.sh": [
          "File: scripts/in_container/run_prepare_airflow_packages.sh -> scripts/in_container/run_prepare_airflow_packages.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "34:     rm -rf -- *egg-info*",
          "35:     rm -rf -- build",
          "39:     local packages=()",
          "",
          "[Removed Lines]",
          "37:     pip install --upgrade \"pip==${AIRFLOW_PIP_VERSION}\" \"wheel==${WHEEL_VERSION}\"",
          "",
          "[Added Lines]",
          "37:     pip install --disable-pip-version-check \"pip==${AIRFLOW_PIP_VERSION}\" \"wheel==${WHEEL_VERSION}\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b43fb3d9be4bac809183aef95df96c678c387664",
      "candidate_info": {
        "commit_hash": "b43fb3d9be4bac809183aef95df96c678c387664",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b43fb3d9be4bac809183aef95df96c678c387664",
        "files": [
          "dev/prepare_release_issue.py"
        ],
        "message": "Only list linked issues once in release issues (#20299)\n\n(cherry picked from commit 58464d830eb018575033a4a88c92facc7cd41e9a)",
        "before_after_code_files": [
          "dev/prepare_release_issue.py||dev/prepare_release_issue.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/prepare_release_issue.py||dev/prepare_release_issue.py": [
          "File: dev/prepare_release_issue.py -> dev/prepare_release_issue.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "273:             # Relate so we can find those from the body",
          "274:             if pr.body:",
          "275:                 body = pr.body.replace(\"\\n\", \" \").replace(\"\\r\", \" \")",
          "278:                     progress.console.print(",
          "279:                         f\"Retrieving Linked issue PR#{linked_issue_number}: \"",
          "280:                         f\"https://github.com/apache/airflow/issue/{linked_issue_number}\"",
          "",
          "[Removed Lines]",
          "276:                 for issue_match in ISSUE_MATCH_IN_BODY.finditer(body):",
          "277:                     linked_issue_number = int(issue_match.group(1))",
          "",
          "[Added Lines]",
          "276:                 linked_issue_numbers = {",
          "277:                     int(issue_match.group(1)) for issue_match in ISSUE_MATCH_IN_BODY.finditer(body)",
          "278:                 }",
          "279:                 for linked_issue_number in linked_issue_numbers:",
          "",
          "---------------"
        ]
      }
    }
  ]
}