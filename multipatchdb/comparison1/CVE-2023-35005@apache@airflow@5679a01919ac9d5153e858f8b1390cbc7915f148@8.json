{
  "cve_id": "CVE-2023-35005",
  "cve_desc": "In Apache Airflow, some potentially sensitive values were being shown to the user in certain situations.\n\nThis vulnerability is mitigated by the fact configuration is not shown in the UI by default (only if `[webserver] expose_config` is set to `non-sensitive-only`), and not all uncensored values are actually sentitive.\n\n\nThis issue affects Apache Airflow: from 2.5.0 before 2.6.2. Users are recommended to update to version 2.6.2 or later.\n\n\n",
  "repo": "apache/airflow",
  "patch_hash": "5679a01919ac9d5153e858f8b1390cbc7915f148",
  "patch_info": {
    "commit_hash": "5679a01919ac9d5153e858f8b1390cbc7915f148",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/5679a01919ac9d5153e858f8b1390cbc7915f148",
    "files": [
      "airflow/config_templates/config.yml",
      "airflow/config_templates/default_airflow.cfg",
      "airflow/configuration.py",
      "airflow/www/views.py",
      "tests/core/test_configuration.py",
      "tests/www/views/test_views_configuration.py"
    ],
    "message": "Use single source of truth for sensitive config items (#31820)\n\nPreviously we had them defined both in constant and in config.yml.\n\nNow just config.yml\n\n(cherry picked from commit cab342ee010bfd048006ca458c760b37470b6ea5)",
    "before_after_code_files": [
      "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg",
      "airflow/configuration.py||airflow/configuration.py",
      "airflow/www/views.py||airflow/www/views.py",
      "tests/core/test_configuration.py||tests/core/test_configuration.py",
      "tests/www/views/test_views_configuration.py||tests/www/views/test_views_configuration.py"
    ]
  },
  "patch_diff": {
    "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg": [
      "File: airflow/config_templates/default_airflow.cfg -> airflow/config_templates/default_airflow.cfg",
      "--- Hunk 1 ---",
      "[Context before]",
      "995: # Example: result_backend = db+postgresql://postgres:airflow@postgres/airflow",
      "996: # result_backend =",
      "998: # Celery Flower is a sweet UI for Celery. Airflow has a shortcut to start",
      "999: # it ``airflow celery flower``. This defines the IP that Celery Flower runs on",
      "1000: flower_host = 0.0.0.0",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "998: # Optional configuration dictionary to pass to the Celery result backend SQLAlchemy engine.",
      "999: # Example: result_backend_sqlalchemy_engine_options = {{\"pool_recycle\": 1800}}",
      "1000: result_backend_sqlalchemy_engine_options =",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "1018: # Import path for celery configuration options",
      "1019: celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG",
      "1020: ssl_active = False",
      "1021: ssl_key =",
      "1022: ssl_cert =",
      "1023: ssl_cacert =",
      "1025: # Celery Pool implementation.",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1026: # Path to the client key.",
      "1029: # Path to the client certificate.",
      "1032: # Path to the CA certificate.",
      "",
      "---------------"
    ],
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "37: from contextlib import contextmanager, suppress",
      "38: from json.decoder import JSONDecodeError",
      "39: from re import Pattern",
      "41: from urllib.parse import urlsplit",
      "43: from typing_extensions import overload",
      "",
      "[Removed Lines]",
      "40: from typing import IO, Any, Dict, Iterable, Tuple, Union",
      "",
      "[Added Lines]",
      "40: from typing import IO, Any, Dict, Iterable, Set, Tuple, Union",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "147:         return yaml.safe_load(config_file)",
      "165: class AirflowConfigParser(ConfigParser):",
      "166:     \"\"\"Custom Airflow Configparser supporting defaults and deprecated options.\"\"\"",
      "",
      "[Removed Lines]",
      "150: SENSITIVE_CONFIG_VALUES = {",
      "151:     (\"database\", \"sql_alchemy_conn\"),",
      "152:     (\"core\", \"fernet_key\"),",
      "153:     (\"celery\", \"broker_url\"),",
      "154:     (\"celery\", \"flower_basic_auth\"),",
      "155:     (\"celery\", \"result_backend\"),",
      "156:     (\"atlas\", \"password\"),",
      "157:     (\"smtp\", \"smtp_password\"),",
      "158:     (\"webserver\", \"secret_key\"),",
      "159:     (\"secrets\", \"backend_kwargs\"),",
      "160:     # The following options are deprecated",
      "161:     (\"core\", \"sql_alchemy_conn\"),",
      "162: }",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "171:     # These configs can also be fetched from Secrets backend",
      "172:     # following the \"{section}__{name}__secret\" pattern",
      "176:     # A mapping of (new section, new option) -> (old section, old option, since_version).",
      "177:     # When reading new option, the old option will be checked to see if it exists. If it does a",
      "",
      "[Removed Lines]",
      "174:     sensitive_config_values: set[tuple[str, str]] = SENSITIVE_CONFIG_VALUES",
      "",
      "[Added Lines]",
      "159:     @cached_property",
      "160:     def sensitive_config_values(self) -> Set[tuple[str, str]]:  # noqa: UP006",
      "161:         default_config = default_config_yaml()",
      "162:         flattened = {",
      "163:             (s, k): item for s, s_c in default_config.items() for k, item in s_c.get(\"options\").items()",
      "164:         }",
      "165:         sensitive = {(section, key) for (section, key), v in flattened.items() if v.get(\"sensitive\") is True}",
      "166:         depr_option = {self.deprecated_options[x][:-1] for x in sensitive if x in self.deprecated_options}",
      "167:         depr_section = {",
      "168:             (self.deprecated_sections[s][0], k) for s, k in sensitive if s in self.deprecated_sections",
      "169:         }",
      "170:         sensitive.update(depr_section, depr_option)",
      "171:         return sensitive",
      "",
      "---------------"
    ],
    "airflow/www/views.py||airflow/www/views.py": [
      "File: airflow/www/views.py -> airflow/www/views.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "3951:         # TODO remove \"if raw\" usage in Airflow 3.0. Configuration can be fetched via the REST API.",
      "3952:         if raw:",
      "3953:             if expose_config == \"non-sensitive-only\":",
      "3956:                 updater = configupdater.ConfigUpdater()",
      "3957:                 updater.read(AIRFLOW_CONFIG)",
      "3959:                     if updater.has_option(sect, key):",
      "3960:                         updater[sect][key].value = \"< hidden >\"",
      "3961:                 config = str(updater)",
      "",
      "[Removed Lines]",
      "3954:                 from airflow.configuration import SENSITIVE_CONFIG_VALUES",
      "3958:                 for sect, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "3956:                 for sect, key in conf.sensitive_config_values:",
      "",
      "---------------"
    ],
    "tests/core/test_configuration.py||tests/core/test_configuration.py": [
      "File: tests/core/test_configuration.py -> tests/core/test_configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "36:     AirflowConfigException,",
      "37:     AirflowConfigParser,",
      "38:     conf,",
      "39:     expand_env_var,",
      "40:     get_airflow_config,",
      "41:     get_airflow_home,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "39:     default_config_yaml,",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "1447:             w = captured.pop()",
      "1448:             assert \"your `conf.get*` call to use the new name\" in str(w.message)",
      "1449:             assert w.category == FutureWarning",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1453: def test_sensitive_values():",
      "1454:     from airflow.settings import conf",
      "1456:     # this list was hardcoded prior to 2.6.2",
      "1457:     # included here to avoid regression in refactor",
      "1458:     # inclusion of keys ending in \"password\" or \"kwargs\" is automated from 2.6.2",
      "1459:     # items not matching this pattern must be added here manually",
      "1460:     sensitive_values = {",
      "1461:         (\"database\", \"sql_alchemy_conn\"),",
      "1462:         (\"core\", \"fernet_key\"),",
      "1463:         (\"celery\", \"broker_url\"),",
      "1464:         (\"celery\", \"flower_basic_auth\"),",
      "1465:         (\"celery\", \"result_backend\"),",
      "1466:         (\"atlas\", \"password\"),",
      "1467:         (\"smtp\", \"smtp_password\"),",
      "1468:         (\"webserver\", \"secret_key\"),",
      "1469:         (\"secrets\", \"backend_kwargs\"),",
      "1470:         (\"sentry\", \"sentry_dsn\"),",
      "1471:         (\"database\", \"sql_alchemy_engine_args\"),",
      "1472:         (\"core\", \"sql_alchemy_conn\"),",
      "1473:     }",
      "1474:     default_config = default_config_yaml()",
      "1475:     all_keys = {(s, k) for s, v in default_config.items() for k in v.get(\"options\")}",
      "1476:     suspected_sensitive = {(s, k) for (s, k) in all_keys if k.endswith((\"password\", \"kwargs\"))}",
      "1477:     exclude_list = {",
      "1478:         (\"kubernetes_executor\", \"delete_option_kwargs\"),",
      "1479:     }",
      "1480:     suspected_sensitive -= exclude_list",
      "1481:     sensitive_values.update(suspected_sensitive)",
      "1482:     assert sensitive_values == conf.sensitive_config_values",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_configuration.py||tests/www/views/test_views_configuration.py": [
      "File: tests/www/views/test_views_configuration.py -> tests/www/views/test_views_configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: import html",
      "22: from tests.test_utils.config import conf_vars",
      "23: from tests.test_utils.www import check_content_in_response, check_content_not_in_response",
      "",
      "[Removed Lines]",
      "21: from airflow.configuration import SENSITIVE_CONFIG_VALUES, conf",
      "",
      "[Added Lines]",
      "21: from airflow.configuration import conf",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "36: @conf_vars({(\"webserver\", \"expose_config\"): \"True\"})",
      "37: def test_user_can_view_configuration(admin_client):",
      "38:     resp = admin_client.get(\"configuration\", follow_redirects=True)",
      "40:         value = conf.get(section, key, fallback=\"\")",
      "41:         if not value:",
      "42:             continue",
      "",
      "[Removed Lines]",
      "39:     for section, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "39:     for section, key in conf.sensitive_config_values:",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "46: @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "47: def test_configuration_redacted(admin_client):",
      "48:     resp = admin_client.get(\"configuration\", follow_redirects=True)",
      "50:         value = conf.get(section, key, fallback=\"\")",
      "51:         if not value or value == \"airflow\":",
      "52:             continue",
      "",
      "[Removed Lines]",
      "49:     for section, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "49:     for section, key in conf.sensitive_config_values:",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "58: @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "59: def test_configuration_redacted_in_running_configuration(admin_client):",
      "60:     resp = admin_client.get(\"configuration\", follow_redirects=True)",
      "62:         value = conf.get(section, key, fallback=\"\")",
      "63:         if not value or value == \"airflow\":",
      "64:             continue",
      "",
      "[Removed Lines]",
      "61:     for section, key in SENSITIVE_CONFIG_VALUES:",
      "",
      "[Added Lines]",
      "61:     for section, key in conf.sensitive_config_values:",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "46c0da8dbd8f4cd21300d05da89d7e7bd85a12aa",
      "candidate_info": {
        "commit_hash": "46c0da8dbd8f4cd21300d05da89d7e7bd85a12aa",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/46c0da8dbd8f4cd21300d05da89d7e7bd85a12aa",
        "files": [
          "airflow/triggers/external_task.py"
        ],
        "message": "Fix typing in external task triggers (#31490)\n\n(cherry picked from commit b3536217cd80bda8b56068a2efb3fa6979d17b3f)",
        "before_after_code_files": [
          "airflow/triggers/external_task.py||airflow/triggers/external_task.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/triggers/external_task.py||airflow/triggers/external_task.py": [
          "File: airflow/triggers/external_task.py -> airflow/triggers/external_task.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: import asyncio",
          "20: import datetime",
          "21: import typing",
          "24: from asgiref.sync import sync_to_async",
          "25: from sqlalchemy import func",
          "",
          "[Removed Lines]",
          "22: from typing import Any",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "28: from airflow.models import DagRun, TaskInstance",
          "29: from airflow.triggers.base import BaseTrigger, TriggerEvent",
          "33: class TaskStateTrigger(BaseTrigger):",
          "",
          "[Removed Lines]",
          "30: from airflow.utils.session import provide_session",
          "",
          "[Added Lines]",
          "29: from airflow.utils.session import NEW_SESSION, provide_session",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "59:         self.execution_dates = execution_dates",
          "60:         self.poll_interval = poll_interval",
          "63:         \"\"\"Serializes TaskStateTrigger arguments and classpath.\"\"\"",
          "64:         return (",
          "65:             \"airflow.triggers.external_task.TaskStateTrigger\",",
          "",
          "[Removed Lines]",
          "62:     def serialize(self) -> tuple[str, dict[str, Any]]:",
          "",
          "[Added Lines]",
          "61:     def serialize(self) -> tuple[str, dict[str, typing.Any]]:",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "86:     @sync_to_async",
          "87:     @provide_session",
          "89:         \"\"\"Count how many task instances in the database match our criteria.\"\"\"",
          "90:         count = (",
          "91:             session.query(func.count(\"*\"))  # .count() is inefficient",
          "",
          "[Removed Lines]",
          "88:     def count_tasks(self, session: Session) -> int | None:",
          "",
          "[Added Lines]",
          "87:     def count_tasks(self, *, session: Session = NEW_SESSION) -> int | None:",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "124:         self.execution_dates = execution_dates",
          "125:         self.poll_interval = poll_interval",
          "128:         \"\"\"Serializes DagStateTrigger arguments and classpath.\"\"\"",
          "129:         return (",
          "130:             \"airflow.triggers.external_task.DagStateTrigger\",",
          "",
          "[Removed Lines]",
          "127:     def serialize(self) -> tuple[str, dict[str, Any]]:",
          "",
          "[Added Lines]",
          "126:     def serialize(self) -> tuple[str, dict[str, typing.Any]]:",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "150:     @sync_to_async",
          "151:     @provide_session",
          "153:         \"\"\"Count how many dag runs in the database match our criteria.\"\"\"",
          "154:         count = (",
          "155:             session.query(func.count(\"*\"))  # .count() is inefficient",
          "",
          "[Removed Lines]",
          "152:     def count_dags(self, session: Session) -> int | None:",
          "",
          "[Added Lines]",
          "151:     def count_dags(self, *, session: Session = NEW_SESSION) -> int | None:",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ea8c0a1f0fef11da86f26109b98c88b26a296ecd",
      "candidate_info": {
        "commit_hash": "ea8c0a1f0fef11da86f26109b98c88b26a296ecd",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ea8c0a1f0fef11da86f26109b98c88b26a296ecd",
        "files": [
          "airflow/migrations/versions/0043_1_10_4_make_taskinstance_pool_not_nullable.py",
          "airflow/migrations/versions/0054_1_10_10_add_dag_code_table.py",
          "airflow/migrations/versions/0060_2_0_0_remove_id_column_from_xcom.py",
          "airflow/migrations/versions/0089_2_2_0_make_xcom_pkey_columns_non_nullable.py",
          "airflow/migrations/versions/0102_2_3_0_switch_xcom_table_to_use_run_id.py",
          "docs/apache-airflow/img/airflow_erd.sha256",
          "setup.py"
        ],
        "message": "Use keyword paramaters for migration methods for mssql (#31309)\n\nThe new alembic (1.11.0) requires parameters to be keyword\nparameters in methods used in migrations. Part of the problem has\nbeen fixed in #31306 and #31302 but still some migrations that are\nspecifically run in case of mssql need to use migration parameters.\n\n(cherry picked from commit 54790274c2edf39c06f494a900bc85523349a77f)",
        "before_after_code_files": [
          "airflow/migrations/versions/0043_1_10_4_make_taskinstance_pool_not_nullable.py||airflow/migrations/versions/0043_1_10_4_make_taskinstance_pool_not_nullable.py",
          "airflow/migrations/versions/0054_1_10_10_add_dag_code_table.py||airflow/migrations/versions/0054_1_10_10_add_dag_code_table.py",
          "airflow/migrations/versions/0060_2_0_0_remove_id_column_from_xcom.py||airflow/migrations/versions/0060_2_0_0_remove_id_column_from_xcom.py",
          "airflow/migrations/versions/0089_2_2_0_make_xcom_pkey_columns_non_nullable.py||airflow/migrations/versions/0089_2_2_0_make_xcom_pkey_columns_non_nullable.py",
          "airflow/migrations/versions/0102_2_3_0_switch_xcom_table_to_use_run_id.py||airflow/migrations/versions/0102_2_3_0_switch_xcom_table_to_use_run_id.py",
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/migrations/versions/0043_1_10_4_make_taskinstance_pool_not_nullable.py||airflow/migrations/versions/0043_1_10_4_make_taskinstance_pool_not_nullable.py": [
          "File: airflow/migrations/versions/0043_1_10_4_make_taskinstance_pool_not_nullable.py -> airflow/migrations/versions/0043_1_10_4_make_taskinstance_pool_not_nullable.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "65:     conn = op.get_bind()",
          "66:     if conn.dialect.name == \"mssql\":",
          "69:     # use batch_alter_table to support SQLite workaround",
          "70:     with op.batch_alter_table(\"task_instance\") as batch_op:",
          "",
          "[Removed Lines]",
          "67:         op.drop_index(\"ti_pool\", table_name=\"task_instance\")",
          "",
          "[Added Lines]",
          "67:         op.drop_index(index_name=\"ti_pool\", table_name=\"task_instance\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "75:         )",
          "77:     if conn.dialect.name == \"mssql\":",
          "81: def downgrade():",
          "82:     \"\"\"Make TaskInstance.pool field nullable.\"\"\"",
          "83:     conn = op.get_bind()",
          "84:     if conn.dialect.name == \"mssql\":",
          "87:     # use batch_alter_table to support SQLite workaround",
          "88:     with op.batch_alter_table(\"task_instance\") as batch_op:",
          "",
          "[Removed Lines]",
          "78:         op.create_index(\"ti_pool\", \"task_instance\", [\"pool\", \"state\", \"priority_weight\"])",
          "85:         op.drop_index(\"ti_pool\", table_name=\"task_instance\")",
          "",
          "[Added Lines]",
          "78:         op.create_index(",
          "79:             index_name=\"ti_pool\", table_name=\"task_instance\", columns=[\"pool\", \"state\", \"priority_weight\"]",
          "80:         )",
          "87:         op.drop_index(index_name=\"ti_pool\", table_name=\"task_instance\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "93:         )",
          "95:     if conn.dialect.name == \"mssql\":",
          "98:     with create_session() as session:",
          "99:         session.query(TaskInstance).filter(TaskInstance.pool == \"default_pool\").update(",
          "",
          "[Removed Lines]",
          "96:         op.create_index(\"ti_pool\", \"task_instance\", [\"pool\", \"state\", \"priority_weight\"])",
          "",
          "[Added Lines]",
          "98:         op.create_index(",
          "99:             index_name=\"ti_pool\", table_name=\"task_instance\", columns=[\"pool\", \"state\", \"priority_weight\"]",
          "100:         )",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0054_1_10_10_add_dag_code_table.py||airflow/migrations/versions/0054_1_10_10_add_dag_code_table.py": [
          "File: airflow/migrations/versions/0054_1_10_10_add_dag_code_table.py -> airflow/migrations/versions/0054_1_10_10_add_dag_code_table.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "63:     conn = op.get_bind()",
          "64:     if conn.dialect.name != \"sqlite\":",
          "65:         if conn.dialect.name == \"mssql\":",
          "68:         op.alter_column(",
          "69:             table_name=\"serialized_dag\", column_name=\"fileloc_hash\", type_=sa.BigInteger(), nullable=False",
          "70:         )",
          "71:         if conn.dialect.name == \"mssql\":",
          "74:     sessionmaker = sa.orm.sessionmaker()",
          "75:     session = sessionmaker(bind=conn)",
          "",
          "[Removed Lines]",
          "66:             op.drop_index(\"idx_fileloc_hash\", \"serialized_dag\")",
          "72:             op.create_index(\"idx_fileloc_hash\", \"serialized_dag\", [\"fileloc_hash\"])",
          "",
          "[Added Lines]",
          "66:             op.drop_index(index_name=\"idx_fileloc_hash\", table_name=\"serialized_dag\")",
          "72:             op.create_index(",
          "73:                 index_name=\"idx_fileloc_hash\", table_name=\"serialized_dag\", columns=[\"fileloc_hash\"]",
          "74:             )",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0060_2_0_0_remove_id_column_from_xcom.py||airflow/migrations/versions/0060_2_0_0_remove_id_column_from_xcom.py": [
          "File: airflow/migrations/versions/0060_2_0_0_remove_id_column_from_xcom.py -> airflow/migrations/versions/0060_2_0_0_remove_id_column_from_xcom.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "102:         if \"id\" in xcom_columns:",
          "103:             if conn.dialect.name == \"mssql\":",
          "104:                 constraint_dict = get_table_constraints(conn, \"xcom\")",
          "106:             bop.drop_column(\"id\")",
          "107:             bop.drop_index(\"idx_xcom_dag_task_date\")",
          "108:             # mssql doesn't allow primary keys with nullable columns",
          "",
          "[Removed Lines]",
          "105:                 drop_column_constraints(bop, \"id\", constraint_dict)",
          "",
          "[Added Lines]",
          "105:                 drop_column_constraints(operator=bop, column_name=\"id\", constraint_dict=constraint_dict)",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0089_2_2_0_make_xcom_pkey_columns_non_nullable.py||airflow/migrations/versions/0089_2_2_0_make_xcom_pkey_columns_non_nullable.py": [
          "File: airflow/migrations/versions/0089_2_2_0_make_xcom_pkey_columns_non_nullable.py -> airflow/migrations/versions/0089_2_2_0_make_xcom_pkey_columns_non_nullable.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "43:         bop.alter_column(\"key\", type_=StringID(length=512), nullable=False)",
          "44:         bop.alter_column(\"execution_date\", type_=TIMESTAMP, nullable=False)",
          "45:         if conn.dialect.name == \"mssql\":",
          "49: def downgrade():",
          "",
          "[Removed Lines]",
          "46:             bop.create_primary_key(\"pk_xcom\", [\"dag_id\", \"task_id\", \"key\", \"execution_date\"])",
          "",
          "[Added Lines]",
          "46:             bop.create_primary_key(",
          "47:                 constraint_name=\"pk_xcom\", columns=[\"dag_id\", \"task_id\", \"key\", \"execution_date\"]",
          "48:             )",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0102_2_3_0_switch_xcom_table_to_use_run_id.py||airflow/migrations/versions/0102_2_3_0_switch_xcom_table_to_use_run_id.py": [
          "File: airflow/migrations/versions/0102_2_3_0_switch_xcom_table_to_use_run_id.py -> airflow/migrations/versions/0102_2_3_0_switch_xcom_table_to_use_run_id.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "172:         constraints = get_mssql_table_constraints(conn, \"xcom\")",
          "173:         pk, _ = constraints[\"PRIMARY KEY\"].popitem()",
          "174:         op.drop_constraint(pk, \"xcom\", type_=\"primary\")",
          "",
          "[Removed Lines]",
          "175:         op.create_primary_key(\"pk_xcom\", \"xcom\", [\"dag_id\", \"task_id\", \"execution_date\", \"key\"])",
          "",
          "[Added Lines]",
          "175:         op.create_primary_key(",
          "176:             constraint_name=\"pk_xcom\",",
          "177:             table_name=\"xcom\",",
          "178:             columns=[\"dag_id\", \"task_id\", \"execution_date\", \"key\"],",
          "179:         )",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: \"\"\"Setup.py for the Airflow project.\"\"\"",
          "23: from __future__ import annotations",
          "25: import glob",
          "",
          "[Removed Lines]",
          "19: # To make sure the CI build is using \"upgrade to newer dependencies\", which is useful when you want to check",
          "20: # if the dependencies are still compatible with the latest versions as they seem to break some unrelated",
          "21: # tests in main, you can modify this file. The modification can be simply modifying this particular comment.",
          "22: # e.g. you can modify the following number \"00001\" to something else to trigger it.",
          "",
          "[Added Lines]",
          "19: # This file can be modified if you want to make sure the CI build is using \"upgrade to newer dependencies\"",
          "20: # Which is useful when you want to check if the dependencies are still compatible with the latest versions",
          "21: # And they seem to break some unrelated tests in main. You can modify this number = 00001 to trigger it.",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cfbe2c2aad708450507cbffcf1ff69c96f9d6362",
      "candidate_info": {
        "commit_hash": "cfbe2c2aad708450507cbffcf1ff69c96f9d6362",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cfbe2c2aad708450507cbffcf1ff69c96f9d6362",
        "files": [
          "scripts/in_container/verify_providers.py"
        ],
        "message": "Check Logging Handlers and Secret Backends for recursion (#31385)\n\nLogging Handlers and secret backends have the potential of triggering\nImport/Attribute error due to circular imports. This check will\nattempt to import all those classes from our providers via\nairflow_local_settings, which should detect the case where such\ncircular import condition is generated.\n\n(cherry picked from commit 33709f07afc337ddf429afcc74707a403fcf3513)",
        "before_after_code_files": [
          "scripts/in_container/verify_providers.py||scripts/in_container/verify_providers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/in_container/verify_providers.py||scripts/in_container/verify_providers.py": [
          "File: scripts/in_container/verify_providers.py -> scripts/in_container/verify_providers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import importlib",
          "21: import os",
          "22: import pkgutil",
          "23: import re",
          "24: import subprocess",
          "25: import sys",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import logging",
          "24: import platform",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "34: from rich.console import Console",
          "36: from airflow.exceptions import AirflowOptionalProviderFeatureException",
          "38: console = Console(width=400, color_system=\"standard\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "39: from airflow.secrets import BaseSecretsBackend",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "214:     provider_ids: list[str] | None = None,",
          "215:     print_imports: bool = False,",
          "216:     print_skips: bool = False,",
          "218:     \"\"\"",
          "219:     Imports all classes in providers packages. This method loads and imports",
          "220:     all the classes found in providers, so that we can find all the subclasses",
          "",
          "[Removed Lines]",
          "217: ) -> tuple[list[str], list[WarningMessage]]:",
          "",
          "[Added Lines]",
          "220: ) -> tuple[list[str], list[WarningMessage], list[str]]:",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "226:     :param provider_ids - provider ids that should be loaded.",
          "227:     :param print_imports - if imported class should also be printed in output",
          "228:     :param print_skips - if skipped classes should also be printed in output",
          "230:     \"\"\"",
          "231:     console.print()",
          "232:     console.print(f\"Walking all package with prefixes in {walkable_paths_and_prefixes}\")",
          "233:     console.print()",
          "234:     imported_classes = []",
          "235:     tracebacks: list[tuple[str, str]] = []",
          "236:     printed_packages: set[str] = set()",
          "",
          "[Removed Lines]",
          "229:     :return: tuple of list of all imported classes and all warnings generated",
          "",
          "[Added Lines]",
          "232:     :return: tuple of list of all imported classes and all warnings and all classes",
          "233:         with potential recursion side effects",
          "239:     classes_with_potential_circular_import = []",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "269:             try:",
          "270:                 with warnings.catch_warnings(record=True) as w:",
          "271:                     warnings.filterwarnings(\"always\", category=DeprecationWarning)",
          "278:                 if w:",
          "279:                     all_warnings.extend(w)",
          "280:             except AirflowOptionalProviderFeatureException:",
          "",
          "[Removed Lines]",
          "272:                     _module = importlib.import_module(modinfo.name)",
          "273:                     for attribute_name in dir(_module):",
          "274:                         class_name = modinfo.name + \".\" + attribute_name",
          "275:                         attribute = getattr(_module, attribute_name)",
          "276:                         if isclass(attribute):",
          "277:                             imported_classes.append(class_name)",
          "",
          "[Added Lines]",
          "277:                     try:",
          "278:                         _module = importlib.import_module(modinfo.name)",
          "279:                         for attribute_name in dir(_module):",
          "280:                             class_name = modinfo.name + \".\" + attribute_name",
          "281:                             attribute = getattr(_module, attribute_name)",
          "282:                             if isclass(attribute):",
          "283:                                 imported_classes.append(class_name)",
          "284:                             if isclass(attribute) and (",
          "285:                                 issubclass(attribute, logging.Handler)",
          "286:                                 or issubclass(attribute, BaseSecretsBackend)",
          "287:                             ):",
          "288:                                 classes_with_potential_circular_import.append(class_name)",
          "289:                     except OSError as e:",
          "290:                         if \"geos_c\" in str(e) and platform.machine() in (\"aarch64\", \"arm64\"):",
          "291:                             # we ignore the missing geos_c library on Apple Silicon",
          "292:                             continue",
          "293:                         raise",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "304:             console.print(\"[red]----------------------------------------[/]\")",
          "305:         sys.exit(1)",
          "306:     else:",
          "310: def is_imported_from_same_module(the_class: str, imported_name: str) -> bool:",
          "",
          "[Removed Lines]",
          "307:         return imported_classes, all_warnings",
          "",
          "[Added Lines]",
          "323:         return imported_classes, all_warnings, classes_with_potential_circular_import",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "767:             walkable_paths_and_prefixes[str(candidate_path)] = provider_prefix + subpackage + \".\"",
          "771:     provider_ids = get_all_providers()",
          "772:     walkable_paths_and_prefixes: dict[str, str] = {}",
          "773:     provider_prefix = \"airflow.providers.\"",
          "774:     for provider_path in get_providers_paths():",
          "775:         walkable_paths_and_prefixes[provider_path] = provider_prefix",
          "776:         add_all_namespaced_packages(walkable_paths_and_prefixes, provider_path, provider_prefix)",
          "778:         walkable_paths_and_prefixes=walkable_paths_and_prefixes,",
          "779:         provider_ids=provider_ids,",
          "780:         print_imports=True,",
          "",
          "[Removed Lines]",
          "770: def verify_provider_classes():",
          "777:     imported_classes, warns = import_all_classes(",
          "",
          "[Added Lines]",
          "786: def verify_provider_classes() -> tuple[list[str], list[str]]:",
          "787:     \"\"\"Verifies all provider classes.",
          "789:     :return: Tuple: list of all classes and list of all classes that have potential recursion side effects",
          "790:     \"\"\"",
          "797:     imported_classes, warns, classes_with_potential_circular_import = import_all_classes(",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "809:     )",
          "810:     console.print(f\"Imported {len(imported_classes)} classes.\")",
          "811:     console.print()",
          "814: def run_provider_discovery():",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "832:     return imported_classes, classes_with_potential_circular_import",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "830:     subprocess.run([\"airflow\", \"providers\", \"auth\"], check=True)",
          "833: if __name__ == \"__main__\":",
          "834:     sys.path.insert(0, str(AIRFLOW_SOURCES_ROOT))",
          "836:     run_provider_discovery()",
          "",
          "[Removed Lines]",
          "835:     verify_provider_classes()",
          "",
          "[Added Lines]",
          "854: AIRFLOW_LOCAL_SETTINGS_PATH = Path(\"/opt/airflow\") / \"airflow_local_settings.py\"",
          "859:     all_imported_classes, all_classes_with_potential_for_circular_import = verify_provider_classes()",
          "860:     try:",
          "861:         AIRFLOW_LOCAL_SETTINGS_PATH.write_text(",
          "862:             \"\\n\".join(",
          "863:                 [",
          "864:                     \"from {} import {}\".format(*class_name.rsplit(\".\", 1))",
          "865:                     for class_name in all_classes_with_potential_for_circular_import",
          "866:                 ]",
          "867:             )",
          "868:         )",
          "869:         console.print(",
          "870:             \"[bright_blue]Importing all provider classes with potential for circular imports\"",
          "871:             \" via airflow_local_settings.py:\\n\\n\"",
          "872:         )",
          "873:         console.print(AIRFLOW_LOCAL_SETTINGS_PATH.read_text())",
          "874:         console.print(\"\\n\")",
          "875:         proc = subprocess.run([sys.executable, \"-c\", \"import airflow\"], check=False)",
          "876:         if proc.returncode != 0:",
          "877:             console.print(",
          "878:                 \"[red] Importing all provider classes with potential for recursion  \"",
          "879:                 \"via airflow_local_settings.py failed!\\n\\n\"",
          "880:             )",
          "881:             console.print(",
          "882:                 \"\\n[bright_blue]If you see AttributeError or ImportError, it might mean that there is \"",
          "883:                 \"a circular import from a provider that should be solved\\n\"",
          "884:             )",
          "885:             console.print(",
          "886:                 \"\\nThe reason for the circular imports might be that if Airflow is configured \"",
          "887:                 \"to use some of the provider's logging/secret backends in settings\\n\"",
          "888:                 \"the extensions might attempt to import airflow configuration, \"",
          "889:                 \"version or settings packages.\\n\"",
          "890:                 \"Accessing those packages will trigger attribute/import errors, because \"",
          "891:                 \"they are not fully imported at this time.\\n\"",
          "892:             )",
          "893:             console.print(",
          "894:                 \"\\n[info]Look at the stack trace above and see where `airflow` core classes have failed to be\"",
          "895:                 \"imported from and fix it so that the class does not do it.\\n\"",
          "896:             )",
          "897:             sys.exit(proc.returncode)",
          "898:     finally:",
          "899:         AIRFLOW_LOCAL_SETTINGS_PATH.unlink()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c5f79d833bb37236f5392c1b1a4a70d4854cdf47",
      "candidate_info": {
        "commit_hash": "c5f79d833bb37236f5392c1b1a4a70d4854cdf47",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c5f79d833bb37236f5392c1b1a4a70d4854cdf47",
        "files": [
          "airflow/www/static/js/task_instances.js"
        ],
        "message": "Don't present an undefined execution date (#31196)\n\nIf a task had entered a Failed state but had not yet started, the UI\nwould display the current time as the \"Run:\" time. This is confusing.\n\n(cherry picked from commit e16207409998b38b91c1f1697557d5c229ed32d1)",
        "before_after_code_files": [
          "airflow/www/static/js/task_instances.js||airflow/www/static/js/task_instances.js"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/static/js/task_instances.js||airflow/www/static/js/task_instances.js": [
          "File: airflow/www/static/js/task_instances.js -> airflow/www/static/js/task_instances.js",
          "--- Hunk 1 ---",
          "[Context before]",
          "99:   if (ti.task_id !== undefined) {",
          "100:     tt += `Task_id: ${escapeHtml(ti.task_id)}<br>`;",
          "101:   }",
          "103:   if (ti.run_id !== undefined) {",
          "104:     tt += `Run Id: <nobr>${escapeHtml(ti.run_id)}</nobr><br>`;",
          "105:   }",
          "",
          "[Removed Lines]",
          "102:   tt += `Run: ${formatDateTime(ti.execution_date)}<br>`;",
          "",
          "[Added Lines]",
          "102:   if (ti.execution_date !== undefined) {",
          "103:     tt += `Run: ${formatDateTime(ti.execution_date)}<br>`;",
          "104:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d60e0a55c5b4010ea25a953bcd4f51d2ef93ad5e",
      "candidate_info": {
        "commit_hash": "d60e0a55c5b4010ea25a953bcd4f51d2ef93ad5e",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/d60e0a55c5b4010ea25a953bcd4f51d2ef93ad5e",
        "files": [
          "airflow/models/taskinstance.py",
          "tests/models/test_taskinstance.py"
        ],
        "message": "Fix `max_active_tis_per_dagrun` for Dynamic Task Mapping (#31406)\n\n* assign filter output to variable\n\n* add tests for max_active_tis_per_dagrun\n\n* fix tests and code style\n\n(cherry picked from commit 342b4fe836c864272ca298bb3dcac61f2eccac45)",
        "before_after_code_files": [
          "airflow/models/taskinstance.py||airflow/models/taskinstance.py",
          "tests/models/test_taskinstance.py||tests/models/test_taskinstance.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/taskinstance.py||airflow/models/taskinstance.py": [
          "File: airflow/models/taskinstance.py -> airflow/models/taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2468:             TaskInstance.state == State.RUNNING,",
          "2469:         )",
          "2470:         if same_dagrun:",
          "2472:         return num_running_task_instances_query.scalar()",
          "2474:     def init_run_context(self, raw: bool = False) -> None:",
          "",
          "[Removed Lines]",
          "2471:             num_running_task_instances_query.filter(TaskInstance.run_id == self.run_id)",
          "",
          "[Added Lines]",
          "2471:             num_running_task_instances_query = num_running_task_instances_query.filter(",
          "2472:                 TaskInstance.run_id == self.run_id",
          "2473:             )",
          "",
          "---------------"
        ],
        "tests/models/test_taskinstance.py||tests/models/test_taskinstance.py": [
          "File: tests/models/test_taskinstance.py -> tests/models/test_taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "81: from airflow.utils.db import merge_conn",
          "82: from airflow.utils.module_loading import qualname",
          "83: from airflow.utils.session import create_session, provide_session",
          "85: from airflow.utils.task_group import TaskGroup",
          "86: from airflow.utils.types import DagRunType",
          "87: from airflow.utils.xcom import XCOM_RETURN_KEY",
          "",
          "[Removed Lines]",
          "84: from airflow.utils.state import State, TaskInstanceState",
          "",
          "[Added Lines]",
          "84: from airflow.utils.state import DagRunState, State, TaskInstanceState",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1620:         assert 1 == ti2.get_num_running_task_instances(session=session)",
          "1621:         assert 1 == ti3.get_num_running_task_instances(session=session)",
          "1623:     def test_log_url(self, create_task_instance):",
          "1624:         ti = create_task_instance(dag_id=\"dag\", task_id=\"op\", execution_date=timezone.datetime(2018, 1, 1))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1623:     def test_get_num_running_task_instances_per_dagrun(self, create_task_instance, dag_maker):",
          "1624:         session = settings.Session()",
          "1626:         with dag_maker(dag_id=\"test_dag\"):",
          "1627:             MockOperator.partial(task_id=\"task_1\").expand_kwargs([{\"a\": 1, \"b\": 2}, {\"a\": 3, \"b\": 4}])",
          "1628:             MockOperator.partial(task_id=\"task_2\").expand_kwargs([{\"a\": 1, \"b\": 2}])",
          "1629:             MockOperator.partial(task_id=\"task_3\").expand_kwargs([{\"a\": 1, \"b\": 2}])",
          "1631:         dr1 = dag_maker.create_dagrun(",
          "1632:             execution_date=timezone.utcnow(), state=DagRunState.RUNNING, run_id=\"run_id_1\", session=session",
          "1633:         )",
          "1634:         tis1 = {(ti.task_id, ti.map_index): ti for ti in dr1.task_instances}",
          "1635:         print(f\"tis1: {tis1}\")",
          "1637:         dr2 = dag_maker.create_dagrun(",
          "1638:             execution_date=timezone.utcnow(), state=DagRunState.RUNNING, run_id=\"run_id_2\", session=session",
          "1639:         )",
          "1640:         tis2 = {(ti.task_id, ti.map_index): ti for ti in dr2.task_instances}",
          "1642:         assert tis1[(\"task_1\", 0)] in session",
          "1643:         assert tis1[(\"task_1\", 1)] in session",
          "1644:         assert tis1[(\"task_2\", 0)] in session",
          "1645:         assert tis1[(\"task_3\", 0)] in session",
          "1646:         assert tis2[(\"task_1\", 0)] in session",
          "1647:         assert tis2[(\"task_1\", 1)] in session",
          "1648:         assert tis2[(\"task_2\", 0)] in session",
          "1649:         assert tis2[(\"task_3\", 0)] in session",
          "1651:         tis1[(\"task_1\", 0)].state = State.RUNNING",
          "1652:         tis1[(\"task_1\", 1)].state = State.QUEUED",
          "1653:         tis1[(\"task_2\", 0)].state = State.RUNNING",
          "1654:         tis1[(\"task_3\", 0)].state = State.RUNNING",
          "1655:         tis2[(\"task_1\", 0)].state = State.RUNNING",
          "1656:         tis2[(\"task_1\", 1)].state = State.QUEUED",
          "1657:         tis2[(\"task_2\", 0)].state = State.RUNNING",
          "1658:         tis2[(\"task_3\", 0)].state = State.RUNNING",
          "1660:         session.commit()",
          "1662:         assert 1 == tis1[(\"task_1\", 0)].get_num_running_task_instances(session=session, same_dagrun=True)",
          "1663:         assert 1 == tis1[(\"task_1\", 1)].get_num_running_task_instances(session=session, same_dagrun=True)",
          "1664:         assert 2 == tis1[(\"task_2\", 0)].get_num_running_task_instances(session=session)",
          "1665:         assert 1 == tis1[(\"task_3\", 0)].get_num_running_task_instances(session=session, same_dagrun=True)",
          "1667:         assert 1 == tis2[(\"task_1\", 0)].get_num_running_task_instances(session=session, same_dagrun=True)",
          "1668:         assert 1 == tis2[(\"task_1\", 1)].get_num_running_task_instances(session=session, same_dagrun=True)",
          "1669:         assert 2 == tis2[(\"task_2\", 0)].get_num_running_task_instances(session=session)",
          "1670:         assert 1 == tis2[(\"task_3\", 0)].get_num_running_task_instances(session=session, same_dagrun=True)",
          "",
          "---------------"
        ]
      }
    }
  ]
}