{
  "cve_id": "CVE-2023-35005",
  "cve_desc": "In Apache Airflow, some potentially sensitive values were being shown to the user in certain situations.\n\nThis vulnerability is mitigated by the fact configuration is not shown in the UI by default (only if `[webserver] expose_config` is set to `non-sensitive-only`), and not all uncensored values are actually sentitive.\n\n\nThis issue affects Apache Airflow: from 2.5.0 before 2.6.2. Users are recommended to update to version 2.6.2 or later.\n\n\n",
  "repo": "apache/airflow",
  "patch_hash": "f6cda8fb63250fc4700658999739c1c3c5f6625c",
  "patch_info": {
    "commit_hash": "f6cda8fb63250fc4700658999739c1c3c5f6625c",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/f6cda8fb63250fc4700658999739c1c3c5f6625c",
    "files": [
      "airflow/configuration.py"
    ],
    "message": "Mark `[secrets] backend_kwargs` as a sensitive config (#31788)\n\n(cherry picked from commit 8062756fa9e01eeeee1f2c6df74f376c0a526bd5)",
    "before_after_code_files": [
      "airflow/configuration.py||airflow/configuration.py"
    ]
  },
  "patch_diff": {
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "156:     (\"atlas\", \"password\"),",
      "157:     (\"smtp\", \"smtp_password\"),",
      "158:     (\"webserver\", \"secret_key\"),",
      "159:     # The following options are deprecated",
      "160:     (\"core\", \"sql_alchemy_conn\"),",
      "161: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "159:     (\"secrets\", \"backend_kwargs\"),",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "8062756fa9e01eeeee1f2c6df74f376c0a526bd5",
      "candidate_info": {
        "commit_hash": "8062756fa9e01eeeee1f2c6df74f376c0a526bd5",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8062756fa9e01eeeee1f2c6df74f376c0a526bd5",
        "files": [
          "airflow/configuration.py"
        ],
        "message": "Mark `[secrets] backend_kwargs` as a sensitive config (#31788)",
        "before_after_code_files": [
          "airflow/configuration.py||airflow/configuration.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "diff_branch_cherry_pick": 1,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "airflow/configuration.py||airflow/configuration.py"
          ],
          "candidate": [
            "airflow/configuration.py||airflow/configuration.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/configuration.py||airflow/configuration.py": [
          "File: airflow/configuration.py -> airflow/configuration.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "155:     (\"atlas\", \"password\"),",
          "156:     (\"smtp\", \"smtp_password\"),",
          "157:     (\"webserver\", \"secret_key\"),",
          "158:     # The following options are deprecated",
          "159:     (\"core\", \"sql_alchemy_conn\"),",
          "160: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "158:     (\"secrets\", \"backend_kwargs\"),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "6605a8ffa1f4ea5a7c6c15c3e1aaa0b5cf5718bd",
      "candidate_info": {
        "commit_hash": "6605a8ffa1f4ea5a7c6c15c3e1aaa0b5cf5718bd",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/6605a8ffa1f4ea5a7c6c15c3e1aaa0b5cf5718bd",
        "files": [
          "airflow/api_connexion/openapi/v1.yaml",
          "airflow/www/static/js/types/api-generated.ts"
        ],
        "message": "fixing broken links in openapi/v1.yaml (#31619)\n\n* fixing broken links in openapi/v1.yaml\n\n* fixed static check\n\n(cherry picked from commit 956aac2b2d719e76c9c8cf6a77e9373021900307)",
        "before_after_code_files": [
          "airflow/www/static/js/types/api-generated.ts||airflow/www/static/js/types/api-generated.ts"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/static/js/types/api-generated.ts||airflow/www/static/js/types/api-generated.ts": [
          "File: airflow/www/static/js/types/api-generated.ts -> airflow/www/static/js/types/api-generated.ts"
        ]
      }
    },
    {
      "candidate_hash": "71bcb669aeaa7d6ce72a24836a7643df5f6e2254",
      "candidate_info": {
        "commit_hash": "71bcb669aeaa7d6ce72a24836a7643df5f6e2254",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/71bcb669aeaa7d6ce72a24836a7643df5f6e2254",
        "files": [
          "airflow/config_templates/default_celery.py"
        ],
        "message": "Include rediss to the list of supported URL schemes (#31028)\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 471fdacd853a5bcb190e1ffc017a4e650097ed69)",
        "before_after_code_files": [
          "airflow/config_templates/default_celery.py||airflow/config_templates/default_celery.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/config_templates/default_celery.py||airflow/config_templates/default_celery.py": [
          "File: airflow/config_templates/default_celery.py -> airflow/config_templates/default_celery.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: from __future__ import annotations",
          "21: import logging",
          "22: import ssl",
          "24: from airflow.configuration import conf",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: import re",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "28: def _broker_supports_visibility_timeout(url):",
          "32: log = logging.getLogger(__name__)",
          "",
          "[Removed Lines]",
          "29:     return url.startswith(\"redis://\") or url.startswith(\"sqs://\")",
          "",
          "[Added Lines]",
          "30:     return url.startswith((\"redis://\", \"rediss://\", \"sqs://\", \"sentinel://\"))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "38:     if _broker_supports_visibility_timeout(broker_url):",
          "39:         broker_transport_options[\"visibility_timeout\"] = 21600",
          "41: if conf.has_option(\"celery\", \"RESULT_BACKEND\"):",
          "42:     result_backend = conf.get_mandatory_value(\"celery\", \"RESULT_BACKEND\")",
          "43: else:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42: broker_transport_options_for_celery: dict = broker_transport_options.copy()",
          "43: if \"sentinel_kwargs\" in broker_transport_options:",
          "44:     try:",
          "45:         sentinel_kwargs = broker_transport_options.get(\"sentinel_kwargs\")",
          "46:         if not isinstance(sentinel_kwargs, dict):",
          "47:             raise ValueError",
          "48:         broker_transport_options_for_celery[\"sentinel_kwargs\"] = sentinel_kwargs",
          "49:     except Exception:",
          "50:         raise AirflowException(\"sentinel_kwargs should be written in the correct dictionary format.\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "74:                 \"ca_certs\": conf.get(\"celery\", \"SSL_CACERT\"),",
          "75:                 \"cert_reqs\": ssl.CERT_REQUIRED,",
          "76:             }",
          "78:             broker_use_ssl = {",
          "79:                 \"ssl_keyfile\": conf.get(\"celery\", \"SSL_KEY\"),",
          "80:                 \"ssl_certfile\": conf.get(\"celery\", \"SSL_CERT\"),",
          "",
          "[Removed Lines]",
          "77:         elif broker_url and \"redis://\" in broker_url:",
          "",
          "[Added Lines]",
          "88:         elif broker_url and re.search(\"rediss?://|sentinel://\", broker_url):",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "100:         f\"all necessary certs and key ({e}).\"",
          "101:     )",
          "104:     log.warning(",
          "105:         \"You have configured a result_backend of %s, it is highly recommended \"",
          "106:         \"to use an alternative result_backend (i.e. a database).\",",
          "",
          "[Removed Lines]",
          "103: if \"amqp://\" in result_backend or \"redis://\" in result_backend or \"rpc://\" in result_backend:",
          "",
          "[Added Lines]",
          "114: if re.search(\"rediss?://|amqp://|rpc://\", result_backend):",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e39ca993983fc7808b70955769a3c85683cdd4a4",
      "candidate_info": {
        "commit_hash": "e39ca993983fc7808b70955769a3c85683cdd4a4",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e39ca993983fc7808b70955769a3c85683cdd4a4",
        "files": [
          "airflow/example_dags/example_dynamic_task_mapping_with_no_taskflow_operators.py",
          "docs/apache-airflow/authoring-and-scheduling/dynamic-task-mapping.rst",
          "tests/serialization/test_dag_serialization.py",
          "tests/www/views/test_views_acl.py"
        ],
        "message": "Add an example for dynamic task mapping with non-TaskFlow operator (#29762)\n\n(cherry picked from commit 4d4c2b9d8b5de4bf03524acf01a298c162e1d9e4)",
        "before_after_code_files": [
          "airflow/example_dags/example_dynamic_task_mapping_with_no_taskflow_operators.py||airflow/example_dags/example_dynamic_task_mapping_with_no_taskflow_operators.py",
          "tests/serialization/test_dag_serialization.py||tests/serialization/test_dag_serialization.py",
          "tests/www/views/test_views_acl.py||tests/www/views/test_views_acl.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/example_dags/example_dynamic_task_mapping_with_no_taskflow_operators.py||airflow/example_dags/example_dynamic_task_mapping_with_no_taskflow_operators.py": [
          "File: airflow/example_dags/example_dynamic_task_mapping_with_no_taskflow_operators.py -> airflow/example_dags/example_dynamic_task_mapping_with_no_taskflow_operators.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: \"\"\"Example DAG demonstrating the usage of dynamic task mapping with non-TaskFlow operators.\"\"\"",
          "19: from __future__ import annotations",
          "21: from datetime import datetime",
          "23: from airflow import DAG",
          "24: from airflow.models.baseoperator import BaseOperator",
          "27: class AddOneOperator(BaseOperator):",
          "28:     \"\"\"A custom operator that adds one to the input.\"\"\"",
          "30:     def __init__(self, value, **kwargs):",
          "31:         super().__init__(**kwargs)",
          "32:         self.value = value",
          "34:     def execute(self, context):",
          "35:         return self.value + 1",
          "38: class SumItOperator(BaseOperator):",
          "39:     \"\"\"A custom operator that sums the input.\"\"\"",
          "41:     template_fields = (\"values\",)",
          "43:     def __init__(self, values, **kwargs):",
          "44:         super().__init__(**kwargs)",
          "45:         self.values = values",
          "47:     def execute(self, context):",
          "48:         total = sum(self.values)",
          "49:         print(f\"Total was {total}\")",
          "50:         return total",
          "53: with DAG(",
          "54:     dag_id=\"example_dynamic_task_mapping_with_no_taskflow_operators\",",
          "55:     start_date=datetime(2022, 3, 4),",
          "56:     catchup=False,",
          "57: ):",
          "58:     # map the task to a list of values",
          "59:     add_one_task = AddOneOperator.partial(task_id=\"add_one\").expand(value=[1, 2, 3])",
          "61:     # aggregate (reduce) the mapped tasks results",
          "62:     sum_it_task = SumItOperator(task_id=\"sum_it\", values=add_one_task.output)",
          "",
          "---------------"
        ],
        "tests/serialization/test_dag_serialization.py||tests/serialization/test_dag_serialization.py": [
          "File: tests/serialization/test_dag_serialization.py -> tests/serialization/test_dag_serialization.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "39: import airflow",
          "40: from airflow.datasets import Dataset",
          "41: from airflow.decorators import teardown",
          "42: from airflow.exceptions import AirflowException, SerializationError",
          "43: from airflow.hooks.base import BaseHook",
          "44: from airflow.kubernetes.pod_generator import PodGenerator",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42: from airflow.decorators.base import DecoratedOperator",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "615:             # data; checking its entirety basically duplicates this validation",
          "616:             # function, so we just do some satiny checks.",
          "617:             serialized_task.operator_class[\"_task_type\"] == type(task).__name__",
          "620:             # Serialization cleans up default values in partial_kwargs, this",
          "621:             # adds them back to both sides.",
          "",
          "[Removed Lines]",
          "618:             serialized_task.operator_class[\"_operator_name\"] == task._operator_name",
          "",
          "[Added Lines]",
          "619:             if isinstance(serialized_task.operator_class, DecoratedOperator):",
          "620:                 serialized_task.operator_class[\"_operator_name\"] == task._operator_name",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_acl.py||tests/www/views/test_views_acl.py": [
          "File: tests/www/views/test_views_acl.py -> tests/www/views/test_views_acl.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "252:     )",
          "253:     expected = [",
          "254:         {\"name\": \"airflow\", \"type\": \"owner\"},",
          "255:         {\"name\": \"test_mapped_taskflow\", \"type\": \"dag\"},",
          "256:         {\"name\": \"tutorial_taskflow_api\", \"type\": \"dag\"},",
          "257:         {\"name\": \"tutorial_taskflow_api_virtualenv\", \"type\": \"dag\"},",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "255:         {\"name\": \"example_dynamic_task_mapping_with_no_taskflow_operators\", \"type\": \"dag\"},",
          "256:         {\"name\": \"example_setup_teardown_taskflow\", \"type\": \"dag\"},",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c01e029df04a8db77ba22631075a726af75e35c1",
      "candidate_info": {
        "commit_hash": "c01e029df04a8db77ba22631075a726af75e35c1",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c01e029df04a8db77ba22631075a726af75e35c1",
        "files": [
          "airflow/cli/commands/kubernetes_command.py",
          "tests/cli/commands/test_kubernetes_command.py"
        ],
        "message": "Use kube_client over default CoreV1Api for deleting pods (#31477)\n\n(cherry picked from commit adf0cae48ad4e87612c00fe9facffca9b5728e7d)",
        "before_after_code_files": [
          "airflow/cli/commands/kubernetes_command.py||airflow/cli/commands/kubernetes_command.py",
          "tests/cli/commands/test_kubernetes_command.py||tests/cli/commands/test_kubernetes_command.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/commands/kubernetes_command.py||airflow/cli/commands/kubernetes_command.py": [
          "File: airflow/cli/commands/kubernetes_command.py -> airflow/cli/commands/kubernetes_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "149: def _delete_pod(name, namespace):",
          "150:     \"\"\"Helper Function for cleanup_pods.\"\"\"",
          "152:     delete_options = client.V1DeleteOptions()",
          "153:     print(f'Deleting POD \"{name}\" from \"{namespace}\" namespace')",
          "155:     print(api_response)",
          "",
          "[Removed Lines]",
          "151:     core_v1 = client.CoreV1Api()",
          "154:     api_response = core_v1.delete_namespaced_pod(name=name, namespace=namespace, body=delete_options)",
          "",
          "[Added Lines]",
          "151:     kube_client = get_kube_client()",
          "154:     api_response = kube_client.delete_namespaced_pod(name=name, namespace=namespace, body=delete_options)",
          "",
          "---------------"
        ],
        "tests/cli/commands/test_kubernetes_command.py||tests/cli/commands/test_kubernetes_command.py": [
          "File: tests/cli/commands/test_kubernetes_command.py -> tests/cli/commands/test_kubernetes_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "64:         cls.parser = cli_parser.get_parser()",
          "66:     @mock.patch(\"kubernetes.client.CoreV1Api.delete_namespaced_pod\")",
          "68:         kubernetes_command._delete_pod(\"dummy\", \"awesome-namespace\")",
          "69:         delete_namespaced_pod.assert_called_with(body=mock.ANY, name=\"dummy\", namespace=\"awesome-namespace\")",
          "71:     @mock.patch(\"airflow.cli.commands.kubernetes_command._delete_pod\")",
          "72:     @mock.patch(\"kubernetes.client.CoreV1Api.list_namespaced_pod\")",
          "",
          "[Removed Lines]",
          "67:     def test_delete_pod(self, delete_namespaced_pod):",
          "",
          "[Added Lines]",
          "67:     @mock.patch(\"airflow.kubernetes.kube_client.config.load_incluster_config\")",
          "68:     def test_delete_pod(self, load_incluster_config, delete_namespaced_pod):",
          "71:         load_incluster_config.assert_called_once()",
          "",
          "---------------"
        ]
      }
    }
  ]
}