{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "ae13b453f6b239af4c7f57cff99e7b8ef939cc9e",
      "candidate_info": {
        "commit_hash": "ae13b453f6b239af4c7f57cff99e7b8ef939cc9e",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/ae13b453f6b239af4c7f57cff99e7b8ef939cc9e",
        "files": [
          "python/pyspark/pandas/frame.py",
          "python/pyspark/pandas/tests/test_dataframe.py"
        ],
        "message": "[SPARK-38763][PYTHON] Support lambda `column` parameter of `DataFrame.rename`\n\n### What changes were proposed in this pull request?\nSupport lambda `column` parameter of `DataFrame.rename`.\n\nWe may want to backport this to 3.3 since this is a regression.\n\n### Why are the changes needed?\nTo reach parity with Pandas.\n\n### Does this PR introduce _any_ user-facing change?\nYes. The regression is fixed; lambda `column` is supported again.\n\n```py\n>>> psdf = ps.DataFrame({'x': [1, 2], 'y': [3, 4]})\n\n>>> psdf.rename(columns=lambda x: x + 'o')\n   xo  yo\n0   1   3\n1   2   4\n```\n\n### How was this patch tested?\nUnit tests.\n\nCloses #36042 from xinrong-databricks/frame.rename.\n\nAuthored-by: Xinrong Meng <xinrong.meng@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 037d07c8acb864f495ea74afba94531c28c163ce)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/pandas/frame.py||python/pyspark/pandas/frame.py",
          "python/pyspark/pandas/tests/test_dataframe.py||python/pyspark/pandas/tests/test_dataframe.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/pandas/frame.py||python/pyspark/pandas/frame.py": [
          "File: python/pyspark/pandas/frame.py -> python/pyspark/pandas/frame.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "10580:         \"\"\"",
          "10582:         def gen_mapper_fn(",
          "10584:         ) -> Tuple[Callable[[Any], Any], Dtype, DataType]:",
          "10585:             if isinstance(mapper, dict):",
          "10586:                 mapper_dict = mapper",
          "",
          "[Removed Lines]",
          "10583:             mapper: Union[Dict, Callable[[Any], Any]]",
          "",
          "[Added Lines]",
          "10583:             mapper: Union[Dict, Callable[[Any], Any]], skip_return_type: bool = False",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "10598:                             raise KeyError(\"Index include value which is not in the `mapper`\")",
          "10599:                         return x",
          "10601:             elif callable(mapper):",
          "10602:                 mapper_callable = cast(Callable, mapper)",
          "10607:                 def mapper_fn(x: Any) -> Any:",
          "10608:                     return mapper_callable(x)",
          "10610:             else:",
          "10611:                 raise ValueError(",
          "10612:                     \"`mapper` or `index` or `columns` should be \"",
          "10613:                     \"either dict-like or function type.\"",
          "10614:                 )",
          "10617:         index_mapper_fn = None",
          "10618:         index_mapper_ret_stype = None",
          "",
          "[Removed Lines]",
          "10603:                 return_type = cast(ScalarType, infer_return_type(mapper))",
          "10604:                 dtype = return_type.dtype",
          "10605:                 spark_return_type = return_type.spark_type",
          "10615:             return mapper_fn, dtype, spark_return_type",
          "",
          "[Added Lines]",
          "10601:                 return mapper_fn, dtype, spark_return_type",
          "10608:                 if skip_return_type:",
          "10609:                     return mapper_fn, None, None",
          "10610:                 else:",
          "10611:                     return_type = cast(ScalarType, infer_return_type(mapper))",
          "10612:                     dtype = return_type.dtype",
          "10613:                     spark_return_type = return_type.spark_type",
          "10614:                     return mapper_fn, dtype, spark_return_type",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "10633:                     index",
          "10634:                 )",
          "10635:             if columns:",
          "10638:             if not index and not columns:",
          "10639:                 raise ValueError(\"Either `index` or `columns` should be provided.\")",
          "",
          "[Removed Lines]",
          "10636:                 columns_mapper_fn, _, _ = gen_mapper_fn(columns)",
          "",
          "[Added Lines]",
          "10640:                 columns_mapper_fn, _, _ = gen_mapper_fn(columns, skip_return_type=True)",
          "",
          "---------------"
        ],
        "python/pyspark/pandas/tests/test_dataframe.py||python/pyspark/pandas/tests/test_dataframe.py": [
          "File: python/pyspark/pandas/tests/test_dataframe.py -> python/pyspark/pandas/tests/test_dataframe.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "817:             pdf1.rename(columns=str_lower, index={1: 10, 2: 20}),",
          "818:         )",
          "820:         idx = pd.MultiIndex.from_tuples([(\"X\", \"A\"), (\"X\", \"B\"), (\"Y\", \"C\"), (\"Y\", \"D\")])",
          "821:         pdf2 = pd.DataFrame([[1, 2, 3, 4], [5, 6, 7, 8]], columns=idx)",
          "822:         psdf2 = ps.from_pandas(pdf2)",
          "824:         self.assert_eq(psdf2.rename(columns=str_lower), pdf2.rename(columns=str_lower))",
          "826:         self.assert_eq(",
          "827:             psdf2.rename(columns=str_lower, level=0), pdf2.rename(columns=str_lower, level=0)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "820:         self.assert_eq(",
          "821:             psdf1.rename(columns=lambda x: str.lower(x)),",
          "822:             pdf1.rename(columns=lambda x: str.lower(x)),",
          "823:         )",
          "830:         self.assert_eq(",
          "831:             psdf2.rename(columns=lambda x: str.lower(x)),",
          "832:             pdf2.rename(columns=lambda x: str.lower(x)),",
          "833:         )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "aaa82928bcc763fee9ea5b0b43f984b862d65467",
      "candidate_info": {
        "commit_hash": "aaa82928bcc763fee9ea5b0b43f984b862d65467",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/aaa82928bcc763fee9ea5b0b43f984b862d65467",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ],
        "message": "[SPARK-40389][SQL][FOLLOWUP][3.3] Fix a test failure in SQLQuerySuite\n\n### What changes were proposed in this pull request?\n\nFix a test failure in SQLQuerySuite on branch-3.3. It's from the backport of https://github.com/apache/spark/pull/37832 since there is no error class \"CAST_OVERFLOW\" on branch-3.3\n\n### Why are the changes needed?\n\nFix test failure\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n### How was this patch tested?\n\nGA\n\nCloses #37848 from gengliangwang/fixTest.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "4559:         val msg = intercept[SparkException] {",
          "4560:           sql(\"select cast(cast(d as int) as long) from dt\").collect()",
          "4561:         }.getCause.getMessage",
          "4563:       }",
          "4564:     }",
          "4565:   }",
          "",
          "[Removed Lines]",
          "4562:         assert(msg.contains(\"[CAST_OVERFLOW]\"))",
          "",
          "[Added Lines]",
          "4562:         assert(msg.contains(\"The value 9000000000BD of the type \\\"DECIMAL(10,0)\\\" \" +",
          "4563:           \"cannot be cast to \\\"INT\\\" due to an overflow\"))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "10cd3ac16bc5b90efc8a2a729c4bfaf2e1cee034",
      "candidate_info": {
        "commit_hash": "10cd3ac16bc5b90efc8a2a729c4bfaf2e1cee034",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/10cd3ac16bc5b90efc8a2a729c4bfaf2e1cee034",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala"
        ],
        "message": "[SPARK-40380][SQL] Fix constant-folding of InvokeLike to avoid non-serializable literal embedded in the plan\n\n### What changes were proposed in this pull request?\n\nBlock `InvokeLike` expressions with `ObjectType` result from constant-folding, to ensure constant-folded results are trusted to be serializable.\nThis is a conservative fix for ease of backport to Spark 3.3. A separate future change can relax the restriction and support constant-folding to serializable `ObjectType` as well.\n\n### Why are the changes needed?\n\nThis fixes a regression introduced by https://github.com/apache/spark/pull/35207 . It enabled the constant-folding logic to aggressively fold `InvokeLike` expressions (e.g. `Invoke`, `StaticInvoke`), when all arguments are foldable and the expression itself is deterministic. But it could go overly aggressive and constant-fold to non-serializable results, which would be problematic when that result needs to be serialized and sent over the wire.\n\nIn the wild, users of sparksql-scalapb have hit this issue. The constant folding logic would fold a chain of `Invoke` / `StaticInvoke` expressions from only holding onto a serializable literal to holding onto a non-serializable literal:\n```\nLiteral(com.example.protos.demo.Person$...).scalaDescriptor.findFieldByNumber.get\n```\nthis expression works fine before constant-folding because the literal that gets sent to the executors is serializable, but when aggressive constant-folding kicks in it ends up as a `Literal(scalapb.descriptors.FieldDescriptor...)` which isn't serializable.\n\nThe following minimal repro demonstrates this issue:\n```\nimport org.apache.spark.sql.Column\nimport org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute\nimport org.apache.spark.sql.catalyst.expressions.Literal\nimport org.apache.spark.sql.catalyst.expressions.objects.{Invoke, StaticInvoke}\nimport org.apache.spark.sql.types.{LongType, ObjectType}\nclass NotSerializableBoxedLong(longVal: Long) { def add(other: Long): Long = longVal + other }\ncase class SerializableBoxedLong(longVal: Long) { def toNotSerializable(): NotSerializableBoxedLong = new NotSerializableBoxedLong(longVal) }\nval litExpr = Literal.fromObject(SerializableBoxedLong(42L), ObjectType(classOf[SerializableBoxedLong]))\nval toNotSerializableExpr = Invoke(litExpr, \"toNotSerializable\", ObjectType(classOf[NotSerializableBoxedLong]))\nval addExpr = Invoke(toNotSerializableExpr, \"add\", LongType, Seq(UnresolvedAttribute.quotedString(\"id\")))\nval df = spark.range(2).select(new Column(addExpr))\ndf.collect\n```\nwould result in an error if aggressive constant-folding kicked in:\n```\n...\nCaused by: java.io.NotSerializableException: NotSerializableBoxedLong\nSerialization stack:\n\t- object not serializable (class: NotSerializableBoxedLong, value: NotSerializableBoxedLong71231636)\n\t- element of array (index: 1)\n\t- array (class [Ljava.lang.Object;, size 2)\n\t- element of array (index: 1)\n\t- array (class [Ljava.lang.Object;, size 3)\n\t- field (class: java.lang.invoke.SerializedLambda, name: capturedArgs, type: class [Ljava.lang.Object;)\n\t- object (class java.lang.invoke.SerializedLambda, SerializedLambda[capturingClass=class org.apache.spark.sql.execution.WholeStageCodegenExec, functionalInterfaceMethod=scala/Function2.apply:(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;, implementation=invokeStatic org/apache/spark/sql/execution/WholeStageCodegenExec.$anonfun$doExecute$4$adapted:(Lorg/apache/spark/sql/catalyst/expressions/codegen/CodeAndComment;[Ljava/lang/Object;Lorg/apache/spark/sql/execution/metric/SQLMetric;Ljava/lang/Object;Lscala/collection/Iterator;)Lscala/collection/Iterator;, instantiatedMethodType=(Ljava/lang/Object;Lscala/collection/Iterator;)Lscala/collection/Iterator;, numCaptured=3])\n\t- writeReplace data (class: java.lang.invoke.SerializedLambda)\n\t- object (class org.apache.spark.sql.execution.WholeStageCodegenExec$$Lambda$3123/1641694389, org.apache.spark.sql.execution.WholeStageCodegenExec$$Lambda$3123/1641694389185db22c)\n  at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:41)\n  at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:49)\n  at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115)\n  at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:441)\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, a regression in ObjectType expression starting from Spark 3.3.0 is fixed.\n\n### How was this patch tested?\n\nThe existing test cases in `ConstantFoldingSuite` continues to pass; added a new test case to demonstrate the regression issue.\n\nCloses #37823 from rednaxelafx/fix-invokelike-constantfold.\n\nAuthored-by: Kris Mok <kris.mok@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 5b96e82ad6a4f5d5e4034d9d7112077159cf5044)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "51:   def propagateNull: Boolean",
          "54:   protected lazy val needNullCheck: Boolean = needNullCheckForIndex.contains(true)",
          "55:   protected lazy val needNullCheckForIndex: Array[Boolean] =",
          "56:     arguments.map(a => a.nullable && (propagateNull ||",
          "",
          "[Removed Lines]",
          "53:   override def foldable: Boolean = children.forall(_.foldable) && deterministic",
          "",
          "[Added Lines]",
          "53:   override def foldable: Boolean =",
          "54:     children.forall(_.foldable) && deterministic && trustedSerializable(dataType)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "62:       .map(cls => v => cls.cast(v))",
          "63:       .getOrElse(identity)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "67:   private def trustedSerializable(dt: DataType): Boolean = {",
          "72:     !dt.existsRecursively(_.isInstanceOf[ObjectType])",
          "73:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "334:     comparePlans(optimized, correctAnswer)",
          "335:   }",
          "337:   test(\"SPARK-39106: Correct conditional expression constant folding\") {",
          "338:     val t = LocalRelation.fromExternalRows(",
          "339:       $\"c\".double :: Nil,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "337:   test(\"SPARK-40380: InvokeLike should only constant-fold to serializable types\") {",
          "338:     val serializableObjType = ObjectType(classOf[SerializableBoxedInt])",
          "339:     val notSerializableObjType = ObjectType(classOf[NotSerializableBoxedInt])",
          "341:     val originalQuery =",
          "342:       testRelation",
          "343:         .select(",
          "345:           Invoke(",
          "346:             Invoke(",
          "347:               Invoke(",
          "348:                 Literal.fromObject(SerializableBoxedInt(42), serializableObjType),",
          "349:                 \"add\",",
          "350:                 serializableObjType,",
          "351:                 Literal(1) :: Nil",
          "352:               ),",
          "353:               \"toNotSerializable\",",
          "354:               notSerializableObjType),",
          "355:             \"addAsInt\",",
          "356:             IntegerType,",
          "357:             $\"a\" :: Nil).as(\"c1\"))",
          "359:     val optimized = Optimize.execute(originalQuery.analyze)",
          "361:     val correctAnswer = originalQuery.analyze",
          "379:     comparePlans(optimized, correctAnswer)",
          "380:   }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "369:     }",
          "370:   }",
          "371: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "418: case class SerializableBoxedInt(intVal: Int) {",
          "419:   def add(other: Int): SerializableBoxedInt = SerializableBoxedInt(intVal + other)",
          "420:   def toNotSerializable(): NotSerializableBoxedInt = new NotSerializableBoxedInt(intVal)",
          "421: }",
          "423: class NotSerializableBoxedInt(intVal: Int) {",
          "424:   def addAsInt(other: Int): Int = intVal + other",
          "425: }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "811c92f7c5f0e1bc4c12d9b121912a91fc67c208",
      "candidate_info": {
        "commit_hash": "811c92f7c5f0e1bc4c12d9b121912a91fc67c208",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/811c92f7c5f0e1bc4c12d9b121912a91fc67c208",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/UnivocityGenerator.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala"
        ],
        "message": "[SPARK-37575][SQL][FOLLOWUP] Add legacy flag for the breaking change of write null value in csv to unquoted empty string\n\n### What changes were proposed in this pull request?\n\nAdd a legacy flag `spark.sql.legacy.nullValueWrittenAsQuotedEmptyStringCsv` for the breaking change introduced in https://github.com/apache/spark/pull/34853 and https://github.com/apache/spark/pull/34905 (followup).\n\nThe flag is disabled by default, so the null values written as csv will output an unquoted empty string. When the legacy flag is enabled, the null will output quoted empty string.\n\n### Why are the changes needed?\nThe original commit is a breaking change, and breaking changes should be encouraged to add a flag to turn it off for smooth migration between versions.\n\n### Does this PR introduce _any_ user-facing change?\nWith the default value of the conf, there is no user-facing difference.\nIf users turn this conf off, they can restore the pre-change behavior.\n\n### How was this patch tested?\nThrough unit tests.\n\nCloses #36110 from anchovYu/flags-null-to-csv.\n\nAuthored-by: Xinyi Yu <xinyi.yu@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 965f872500a3554142cab3078a7a4d513d2d2ee8)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/UnivocityGenerator.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/UnivocityGenerator.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/UnivocityGenerator.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/UnivocityGenerator.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/UnivocityGenerator.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/UnivocityGenerator.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import org.apache.spark.sql.catalyst.InternalRow",
          "25: import org.apache.spark.sql.catalyst.util.{DateFormatter, DateTimeUtils, IntervalStringStyles, IntervalUtils, TimestampFormatter}",
          "26: import org.apache.spark.sql.catalyst.util.LegacyDateFormats.FAST_DATE_FORMAT",
          "27: import org.apache.spark.sql.types._",
          "29: class UnivocityGenerator(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: import org.apache.spark.sql.internal.SQLConf",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "95:     while (i < row.numFields) {",
          "96:       if (!row.isNullAt(i)) {",
          "97:         values(i) = valueConverters(i).apply(row, i)",
          "98:       }",
          "99:       i += 1",
          "100:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "99:       } else if (",
          "100:         SQLConf.get.getConf(SQLConf.LEGACY_NULL_VALUE_WRITTEN_AS_QUOTED_EMPTY_STRING_CSV)) {",
          "101:         values(i) = options.nullValue",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "3724:       .booleanConf",
          "3725:       .createWithDefault(false)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3727:   val LEGACY_NULL_VALUE_WRITTEN_AS_QUOTED_EMPTY_STRING_CSV =",
          "3728:     buildConf(\"spark.sql.legacy.nullValueWrittenAsQuotedEmptyStringCsv\")",
          "3729:       .internal()",
          "3730:       .doc(\"When set to false, nulls are written as unquoted empty strings in CSV data source. \" +",
          "3731:         \"If set to false, it restores the legacy behavior that nulls were written as quoted \" +",
          "3732:         \"empty strings, `\\\"\\\"`.\")",
          "3733:       .version(\"3.3.0\")",
          "3734:       .booleanConf",
          "3735:       .createWithDefault(false)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/csv/CSVSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "808:   test(\"SPARK-37575: null values should be saved as nothing rather than \" +",
          "809:     \"quoted empty Strings \\\"\\\" with default settings\") {",
          "816:     }",
          "817:   }",
          "",
          "[Removed Lines]",
          "810:     withTempPath { path =>",
          "811:       Seq((\"Tesla\", null: String, \"\"))",
          "812:         .toDF(\"make\", \"comment\", \"blank\")",
          "813:         .write",
          "814:         .csv(path.getCanonicalPath)",
          "815:       checkAnswer(spark.read.text(path.getCanonicalPath), Row(\"Tesla,,\\\"\\\"\"))",
          "",
          "[Added Lines]",
          "810:     Seq(\"true\", \"false\").foreach { confVal =>",
          "811:       withSQLConf(SQLConf.LEGACY_NULL_VALUE_WRITTEN_AS_QUOTED_EMPTY_STRING_CSV.key -> confVal) {",
          "812:         withTempPath { path =>",
          "813:           Seq((\"Tesla\", null: String, \"\"))",
          "814:             .toDF(\"make\", \"comment\", \"blank\")",
          "815:             .write",
          "816:             .csv(path.getCanonicalPath)",
          "817:           if (confVal == \"false\") {",
          "818:             checkAnswer(spark.read.text(path.getCanonicalPath), Row(\"Tesla,,\\\"\\\"\"))",
          "819:           } else {",
          "820:             checkAnswer(spark.read.text(path.getCanonicalPath), Row(\"Tesla,\\\"\\\",\\\"\\\"\"))",
          "821:           }",
          "822:         }",
          "823:       }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "3b549f4309497ecbe9f0b7a20d22a9a4417abb8b",
      "candidate_info": {
        "commit_hash": "3b549f4309497ecbe9f0b7a20d22a9a4417abb8b",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/3b549f4309497ecbe9f0b7a20d22a9a4417abb8b",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameJoinSuite.scala"
        ],
        "message": "[SPARK-39376][SQL] Hide duplicated columns in star expansion of subquery alias from NATURAL/USING JOIN\n\n### What changes were proposed in this pull request?\n\nFollows up from https://github.com/apache/spark/pull/31666. This PR introduced a bug where the qualified star expansion of a subquery alias containing a NATURAL/USING output duplicated columns.\n\n### Why are the changes needed?\n\nDuplicated, hidden columns should not be output from a star expansion.\n\n### Does this PR introduce _any_ user-facing change?\n\nThe query\n\n```\nval df1 = Seq((3, 8)).toDF(\"a\", \"b\")\nval df2 = Seq((8, 7)).toDF(\"b\", \"d\")\nval joinDF = df1.join(df2, \"b\")\njoinDF.alias(\"r\").select(\"r.*\")\n```\n\nNow outputs a single column `b`, instead of two (duplicate) columns for `b`.\n\n### How was this patch tested?\n\nUTs\n\nCloses #36763 from karenfeng/SPARK-39376.\n\nAuthored-by: Karen Feng <karen.feng@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 18ca369f01905b421a658144e23b5a4e60702655)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameJoinSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameJoinSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1329:   override def metadataOutput: Seq[Attribute] = {",
          "1330:     val qualifierList = identifier.qualifier :+ alias",
          "1332:   }",
          "1334:   override def maxRows: Option[Long] = child.maxRows",
          "",
          "[Removed Lines]",
          "1331:     child.metadataOutput.map(_.withQualifier(qualifierList))",
          "",
          "[Added Lines]",
          "1331:     val nonHiddenMetadataOutput = child.metadataOutput.filter(!_.supportsQualifiedStar)",
          "1332:     nonHiddenMetadataOutput.map(_.withQualifier(qualifierList))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/DataFrameJoinSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameJoinSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/DataFrameJoinSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/DataFrameJoinSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "499:       )",
          "500:     }",
          "501:   }",
          "502: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "503:   test(\"SPARK-39376: Hide duplicated columns in star expansion of subquery alias from USING JOIN\") {",
          "504:     val joinDf = testData2.as(\"testData2\").join(",
          "505:       testData3.as(\"testData3\"), usingColumns = Seq(\"a\"), joinType = \"fullouter\")",
          "506:     val equivalentQueries = Seq(",
          "507:       joinDf.select($\"*\"),",
          "508:       joinDf.as(\"r\").select($\"*\"),",
          "509:       joinDf.as(\"r\").select($\"r.*\")",
          "510:     )",
          "511:     equivalentQueries.foreach { query =>",
          "512:       checkAnswer(query,",
          "513:         Seq(",
          "514:           Row(1, 1, null),",
          "515:           Row(1, 2, null),",
          "516:           Row(2, 1, 2),",
          "517:           Row(2, 2, 2),",
          "518:           Row(3, 1, null),",
          "519:           Row(3, 2, null)",
          "520:         )",
          "521:       )",
          "522:     }",
          "523:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}