{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "c0acd3f7398c695b9889378b26d4a775b00df452",
      "candidate_info": {
        "commit_hash": "c0acd3f7398c695b9889378b26d4a775b00df452",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/c0acd3f7398c695b9889378b26d4a775b00df452",
        "files": [
          "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/shuffle/KubernetesLocalDiskShuffleExecutorComponents.scala"
        ],
        "message": "[SPARK-40459][K8S] `recoverDiskStore` should not stop by existing recomputed files\n\n### What changes were proposed in this pull request?\n\nThis PR aims to ignore `FileExistsException` during `recoverDiskStore` processing.\n\n### Why are the changes needed?\n\nAlthough `recoverDiskStore` is already wrapped by `tryLogNonFatalError`, a single file recovery exception should not block the whole `recoverDiskStore` .\n\nhttps://github.com/apache/spark/blob/5938e84e72b81663ccacf0b36c2f8271455de292/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/shuffle/KubernetesLocalDiskShuffleExecutorComponents.scala#L45-L47\n\n```\norg.apache.commons.io.FileExistsException: ...\n  at org.apache.commons.io.FileUtils.requireAbsent(FileUtils.java:2587)\n  at org.apache.commons.io.FileUtils.moveFile(FileUtils.java:2305)\n  at org.apache.commons.io.FileUtils.moveFile(FileUtils.java:2283)\n  at org.apache.spark.storage.DiskStore.moveFileToBlock(DiskStore.scala:150)\n  at org.apache.spark.storage.BlockManager$TempFileBasedBlockStoreUpdater.saveToDiskStore(BlockManager.scala:487)\n  at org.apache.spark.storage.BlockManager$BlockStoreUpdater.$anonfun$save$1(BlockManager.scala:407)\n  at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n  at org.apache.spark.storage.BlockManager$BlockStoreUpdater.save(BlockManager.scala:380)\n  at org.apache.spark.storage.BlockManager$TempFileBasedBlockStoreUpdater.save(BlockManager.scala:490)\n  at org.apache.spark.shuffle.KubernetesLocalDiskShuffleExecutorComponents$.$anonfun$recoverDiskStore$14(KubernetesLocalDiskShuffleExecutorComponents.scala:95)\n  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n  at org.apache.spark.shuffle.KubernetesLocalDiskShuffleExecutorComponents$.recoverDiskStore(KubernetesLocalDiskShuffleExecutorComponents.scala:91)\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, this will improve the recover rate.\n\n### How was this patch tested?\n\nPass the CIs.\n\nCloses #37903 from dongjoon-hyun/SPARK-40459.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit f24bb430122eaa311070cfdefbc82d34b0341701)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/shuffle/KubernetesLocalDiskShuffleExecutorComponents.scala||resource-managers/kubernetes/core/src/main/scala/org/apache/spark/shuffle/KubernetesLocalDiskShuffleExecutorComponents.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/shuffle/KubernetesLocalDiskShuffleExecutorComponents.scala||resource-managers/kubernetes/core/src/main/scala/org/apache/spark/shuffle/KubernetesLocalDiskShuffleExecutorComponents.scala": [
          "File: resource-managers/kubernetes/core/src/main/scala/org/apache/spark/shuffle/KubernetesLocalDiskShuffleExecutorComponents.scala -> resource-managers/kubernetes/core/src/main/scala/org/apache/spark/shuffle/KubernetesLocalDiskShuffleExecutorComponents.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import scala.reflect.ClassTag",
          "25: import org.apache.spark.{SparkConf, SparkEnv}",
          "26: import org.apache.spark.internal.Logging",
          "27: import org.apache.spark.shuffle.api.{ShuffleExecutorComponents, ShuffleMapOutputWriter, SingleSpillShuffleMapOutputWriter}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: import org.apache.commons.io.FileExistsException",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "95:         bm.TempFileBasedBlockStoreUpdater(id, level, classTag, f, decryptedSize).save()",
          "96:       } catch {",
          "97:         case _: UnrecognizedBlockId =>",
          "98:       }",
          "99:     }",
          "100:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "100:         case _: FileExistsException =>",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "81cb08b7b3ae6a4ccfa9787ec39a6041fae8143f",
      "candidate_info": {
        "commit_hash": "81cb08b7b3ae6a4ccfa9787ec39a6041fae8143f",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/81cb08b7b3ae6a4ccfa9787ec39a6041fae8143f",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala"
        ],
        "message": "add back a mistakenly removed test case",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/connector/DataSourceV2SQLSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2288:     }",
          "2289:   }",
          "2291:   test(\"SPARK-34576: drop/add columns to a dataset of `DESCRIBE COLUMN`\") {",
          "2292:     val tbl = s\"${catalogAndNamespace}tbl\"",
          "2293:     withTable(tbl) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2291:   test(\"SPARK-34561: drop/add columns to a dataset of `DESCRIBE TABLE`\") {",
          "2292:     val tbl = s\"${catalogAndNamespace}tbl\"",
          "2293:     withTable(tbl) {",
          "2294:       sql(s\"CREATE TABLE $tbl (c0 INT) USING $v2Format\")",
          "2295:       val description = sql(s\"DESCRIBE TABLE $tbl\")",
          "2296:       val noCommentDataset = description.drop(\"comment\")",
          "2297:       val expectedSchema = new StructType()",
          "2298:         .add(",
          "2299:           name = \"col_name\",",
          "2300:           dataType = StringType,",
          "2301:           nullable = false,",
          "2302:           metadata = new MetadataBuilder().putString(\"comment\", \"name of the column\").build())",
          "2303:         .add(",
          "2304:           name = \"data_type\",",
          "2305:           dataType = StringType,",
          "2306:           nullable = false,",
          "2307:           metadata = new MetadataBuilder().putString(\"comment\", \"data type of the column\").build())",
          "2308:       assert(noCommentDataset.schema === expectedSchema)",
          "2309:       val isNullDataset = noCommentDataset",
          "2310:         .withColumn(\"is_null\", noCommentDataset(\"col_name\").isNull)",
          "2311:       assert(isNullDataset.schema === expectedSchema.add(\"is_null\", BooleanType, false))",
          "2312:     }",
          "2313:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b3cd07b236f46e8c402b06820d6f3a25fe608593",
      "candidate_info": {
        "commit_hash": "b3cd07b236f46e8c402b06820d6f3a25fe608593",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/b3cd07b236f46e8c402b06820d6f3a25fe608593",
        "files": [
          "sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java",
          "sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala"
        ],
        "message": "[SPARK-38761][SQL] DS V2 supports push down misc non-aggregate functions\n\n### What changes were proposed in this pull request?\nCurrently, Spark have some misc non-aggregate functions of ANSI standard. Please refer https://github.com/apache/spark/blob/2f8613f22c0750c00cf1dcfb2f31c431d8dc1be7/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala#L362.\nThese functions show below:\n`abs`,\n`coalesce`,\n`nullif`,\n`CASE WHEN`\nDS V2 should supports push down these misc non-aggregate functions.\nBecause DS V2 already support push down `CASE WHEN`, so this PR no need do the job again.\nBecause `nullif` extends `RuntimeReplaceable`, so this PR no need do the job too.\n\n### Why are the changes needed?\nDS V2 supports push down misc non-aggregate functions\n\n### Does this PR introduce _any_ user-facing change?\n'No'.\nNew feature.\n\n### How was this patch tested?\nNew tests.\n\nCloses #36039 from beliefer/SPARK-38761.\n\nAuthored-by: Jiaan Geng <beliefer@163.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 9ce4ba02d3f67116a4a9786af453d869596fb3ec)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java||sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java",
          "sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala||sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java||sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java": [
          "File: sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java -> sql/catalyst/src/main/java/org/apache/spark/sql/connector/util/V2ExpressionSQLBuilder.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "93:           return visitNot(build(e.children()[0]));",
          "94:         case \"~\":",
          "95:           return visitUnaryArithmetic(name, inputToSQL(e.children()[0]));",
          "96:         case \"CASE_WHEN\": {",
          "97:           List<String> children =",
          "98:             Arrays.stream(e.children()).map(c -> build(c)).collect(Collectors.toList());",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "96:         case \"ABS\":",
          "97:         case \"COALESCE\":",
          "98:           return visitSQLFunction(name,",
          "99:             Arrays.stream(e.children()).map(c -> build(c)).toArray(String[]::new));",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "210:     return sb.toString();",
          "211:   }",
          "213:   protected String visitUnexpectedExpr(Expression expr) throws IllegalArgumentException {",
          "214:     throw new IllegalArgumentException(\"Unexpected V2 expression: \" + expr);",
          "215:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "217:   protected String visitSQLFunction(String funcName, String[] inputs) {",
          "218:     return funcName + \"(\" + Arrays.stream(inputs).collect(Collectors.joining(\", \")) + \")\";",
          "219:   }",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala||sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala -> sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.catalyst.util",
          "21: import org.apache.spark.sql.connector.expressions.{Cast => V2Cast, Expression => V2Expression, FieldReference, GeneralScalarExpression, LiteralValue}",
          "22: import org.apache.spark.sql.connector.expressions.filter.{AlwaysFalse, AlwaysTrue, And => V2And, Not => V2Not, Or => V2Or, Predicate => V2Predicate}",
          "23: import org.apache.spark.sql.execution.datasources.PushableColumn",
          "",
          "[Removed Lines]",
          "20: import org.apache.spark.sql.catalyst.expressions.{Add, And, BinaryComparison, BinaryOperator, BitwiseAnd, BitwiseNot, BitwiseOr, BitwiseXor, CaseWhen, Cast, Contains, Divide, EndsWith, EqualTo, Expression, In, InSet, IsNotNull, IsNull, Literal, Multiply, Not, Or, Predicate, Remainder, StartsWith, StringPredicate, Subtract, UnaryMinus}",
          "",
          "[Added Lines]",
          "20: import org.apache.spark.sql.catalyst.expressions.{Abs, Add, And, BinaryComparison, BinaryOperator, BitwiseAnd, BitwiseNot, BitwiseOr, BitwiseXor, CaseWhen, Cast, Coalesce, Contains, Divide, EndsWith, EqualTo, Expression, In, InSet, IsNotNull, IsNull, Literal, Multiply, Not, Or, Predicate, Remainder, StartsWith, StringPredicate, Subtract, UnaryMinus}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "95:       }",
          "96:     case Cast(child, dataType, _, true) =>",
          "97:       generateExpression(child).map(v => new V2Cast(v, dataType))",
          "98:     case and: And =>",
          "100:       val l = generateExpression(and.left, true)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "98:     case Abs(child, true) => generateExpression(child)",
          "99:       .map(v => new GeneralScalarExpression(\"ABS\", Array[V2Expression](v)))",
          "100:     case Coalesce(children) =>",
          "101:       val childrenExpressions = children.flatMap(generateExpression(_))",
          "102:       if (children.length == childrenExpressions.length) {",
          "103:         Some(new GeneralScalarExpression(\"COALESCE\", childrenExpressions.toArray[V2Expression]))",
          "104:       } else {",
          "105:         None",
          "106:       }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala -> sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, Filter, Sort}",
          "27: import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation, V1ScanWrapper}",
          "28: import org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog",
          "30: import org.apache.spark.sql.internal.SQLConf",
          "31: import org.apache.spark.sql.test.SharedSparkSession",
          "32: import org.apache.spark.util.Utils",
          "",
          "[Removed Lines]",
          "29: import org.apache.spark.sql.functions.{avg, count, count_distinct, lit, not, sum, udf, when}",
          "",
          "[Added Lines]",
          "29: import org.apache.spark.sql.functions.{abs, avg, coalesce, count, count_distinct, lit, not, sum, udf, when}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "381:         checkAnswer(df, Seq(Row(\"fred\", 1), Row(\"mary\", 2)))",
          "383:         val df2 = spark.table(\"h2.test.people\").filter($\"id\" + Int.MaxValue > 1)",
          "385:         checkFiltersRemoved(df2, ansiMode)",
          "395:         }",
          "397:         if (ansiMode) {",
          "398:           val e = intercept[SparkException] {",
          "399:             checkAnswer(df2, Seq.empty)",
          "",
          "[Removed Lines]",
          "387:         df2.queryExecution.optimizedPlan.collect {",
          "388:           case _: DataSourceV2ScanRelation =>",
          "389:             val expected_plan_fragment = if (ansiMode) {",
          "390:               \"PushedFilters: [ID IS NOT NULL, (ID + 2147483647) > 1], \"",
          "391:             } else {",
          "392:               \"PushedFilters: [ID IS NOT NULL], \"",
          "393:             }",
          "394:             checkKeywordsExistsInExplain(df2, expected_plan_fragment)",
          "",
          "[Added Lines]",
          "385:         val expectedPlanFragment2 = if (ansiMode) {",
          "386:           \"PushedFilters: [ID IS NOT NULL, (ID + 2147483647) > 1], \"",
          "387:         } else {",
          "388:           \"PushedFilters: [ID IS NOT NULL], \"",
          "390:         checkPushedInfo(df2, expectedPlanFragment2)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "423:         val df4 = spark.table(\"h2.test.employee\")",
          "424:           .filter(($\"salary\" > 1000d).and($\"salary\" < 12000d))",
          "426:         checkFiltersRemoved(df4, ansiMode)",
          "437:         }",
          "439:         checkAnswer(df4, Seq(Row(1, \"amy\", 10000, 1000, true),",
          "440:           Row(1, \"cathy\", 9000, 1200, false), Row(2, \"david\", 10000, 1300, true)))",
          "441:       }",
          "442:     }",
          "443:   }",
          "",
          "[Removed Lines]",
          "428:         df4.queryExecution.optimizedPlan.collect {",
          "429:           case _: DataSourceV2ScanRelation =>",
          "430:             val expected_plan_fragment = if (ansiMode) {",
          "431:               \"PushedFilters: [SALARY IS NOT NULL, \" +",
          "432:                 \"CAST(SALARY AS double) > 1000.0, CAST(SALARY AS double) < 12000.0], \"",
          "433:             } else {",
          "434:               \"PushedFilters: [SALARY IS NOT NULL], \"",
          "435:             }",
          "436:             checkKeywordsExistsInExplain(df4, expected_plan_fragment)",
          "",
          "[Added Lines]",
          "420:         val expectedPlanFragment4 = if (ansiMode) {",
          "421:           \"PushedFilters: [SALARY IS NOT NULL, \" +",
          "422:             \"CAST(SALARY AS double) > 1000.0, CAST(SALARY AS double) < 12000.0], \"",
          "423:         } else {",
          "424:           \"PushedFilters: [SALARY IS NOT NULL], \"",
          "426:         checkPushedInfo(df4, expectedPlanFragment4)",
          "430:         val df5 = spark.table(\"h2.test.employee\")",
          "431:           .filter(abs($\"dept\" - 3) > 1)",
          "432:           .filter(coalesce($\"salary\", $\"bonus\") > 2000)",
          "433:         checkFiltersRemoved(df5, ansiMode)",
          "434:         val expectedPlanFragment5 = if (ansiMode) {",
          "435:           \"PushedFilters: [DEPT IS NOT NULL, ABS(DEPT - 3) > 1, \" +",
          "436:             \"(COALESCE(CAST(SALARY AS double), BONUS)) > 2000.0]\"",
          "437:         } else {",
          "438:           \"PushedFilters: [DEPT IS NOT NULL]\"",
          "439:         }",
          "440:         checkPushedInfo(df5, expectedPlanFragment5)",
          "441:         checkAnswer(df5, Seq(Row(1, \"amy\", 10000, 1000, true),",
          "442:           Row(1, \"cathy\", 9000, 1200, false), Row(6, \"jen\", 12000, 1200, true)))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2db9d78d71662de276dd65e582674b6088eff119",
      "candidate_info": {
        "commit_hash": "2db9d78d71662de276dd65e582674b6088eff119",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/2db9d78d71662de276dd65e582674b6088eff119",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/Catalogs.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/test/java/org/apache/spark/sql/connector/catalog/CatalogLoadingSuite.java"
        ],
        "message": "[SPARK-39174][SQL] Catalogs loading swallows missing classname for ClassNotFoundException\n\n### What changes were proposed in this pull request?\n\nthis PR captures the actual missing classname when catalog loading meets ClassNotFoundException\n\n### Why are the changes needed?\n\nClassNotFoundException can occur when missing dependencies, we shall not always report the catalog class is missing\n\n### Does this PR introduce _any_ user-facing change?\n\nyes, when loading catalogs and ClassNotFoundException occurs, it shows the correct missing class.\n\n### How was this patch tested?\n\nnew test added\n\nCloses #36534 from yaooqinn/SPARK-39174.\n\nAuthored-by: Kent Yao <yao@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 1b37f19876298e995596a30edc322c856ea1bbb4)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/Catalogs.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/Catalogs.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/test/java/org/apache/spark/sql/connector/catalog/CatalogLoadingSuite.java||sql/catalyst/src/test/java/org/apache/spark/sql/connector/catalog/CatalogLoadingSuite.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/Catalogs.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/Catalogs.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/Catalogs.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/Catalogs.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "60:       plugin.initialize(name, catalogOptions(name, conf))",
          "61:       plugin",
          "62:     } catch {",
          "65:       case e: NoSuchMethodException =>",
          "66:         throw QueryExecutionErrors.catalogFailToFindPublicNoArgConstructorError(",
          "67:           name, pluginClassName, e)",
          "",
          "[Removed Lines]",
          "63:       case _: ClassNotFoundException =>",
          "64:         throw QueryExecutionErrors.catalogPluginClassNotFoundForCatalogError(name, pluginClassName)",
          "",
          "[Added Lines]",
          "63:       case e: ClassNotFoundException =>",
          "64:         throw QueryExecutionErrors.catalogPluginClassNotFoundForCatalogError(",
          "65:           name, pluginClassName, e)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1587:   def catalogPluginClassNotFoundForCatalogError(",
          "1588:       name: String,",
          "1591:   }",
          "1593:   def catalogFailToFindPublicNoArgConstructorError(",
          "",
          "[Removed Lines]",
          "1589:       pluginClassName: String): Throwable = {",
          "1590:     new SparkException(s\"Cannot find catalog plugin class for catalog '$name': $pluginClassName\")",
          "",
          "[Added Lines]",
          "1589:       pluginClassName: String,",
          "1590:       e: Exception): Throwable = {",
          "1591:     new SparkException(s\"Cannot find catalog plugin class for catalog '$name': $pluginClassName\", e)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/java/org/apache/spark/sql/connector/catalog/CatalogLoadingSuite.java||sql/catalyst/src/test/java/org/apache/spark/sql/connector/catalog/CatalogLoadingSuite.java": [
          "File: sql/catalyst/src/test/java/org/apache/spark/sql/connector/catalog/CatalogLoadingSuite.java -> sql/catalyst/src/test/java/org/apache/spark/sql/connector/catalog/CatalogLoadingSuite.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import org.apache.spark.SparkException;",
          "21: import org.apache.spark.sql.internal.SQLConf;",
          "22: import org.apache.spark.sql.util.CaseInsensitiveStringMap;",
          "23: import org.junit.Assert;",
          "24: import org.junit.Test;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "23: import org.apache.spark.util.Utils;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "91:         exc.getMessage().contains(\"com.example.NoSuchCatalogPlugin\"));",
          "92:   }",
          "94:   @Test",
          "95:   public void testLoadNonCatalogPlugin() {",
          "96:     SQLConf conf = new SQLConf();",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "96:   @Test",
          "97:   public void testLoadMissingDependentClasses() {",
          "98:     SQLConf conf = new SQLConf();",
          "99:     String catalogClass = ClassFoundCatalogPlugin.class.getCanonicalName();",
          "100:     conf.setConfString(\"spark.sql.catalog.missing\", catalogClass);",
          "102:     SparkException exc =",
          "103:         Assert.assertThrows(SparkException.class, () -> Catalogs.load(\"missing\", conf));",
          "105:     Assert.assertTrue(exc.getCause() instanceof ClassNotFoundException);",
          "106:     Assert.assertTrue(exc.getCause().getMessage().contains(catalogClass + \"Dep\"));",
          "107:   }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "209:   public void initialize(CaseInsensitiveStringMap options) {",
          "210:   }",
          "211: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "228: class ClassFoundCatalogPlugin implements CatalogPlugin {",
          "230:   @Override",
          "231:   public void initialize(String name, CaseInsensitiveStringMap options) {",
          "232:     Utils.classForName(this.getClass().getCanonicalName() + \"Dep\", true, true);",
          "233:   }",
          "235:   @Override",
          "236:   public String name() {",
          "237:     return null;",
          "238:   }",
          "239: }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "3cf304855be8ec04158d976d15210da1fa22ac03",
      "candidate_info": {
        "commit_hash": "3cf304855be8ec04158d976d15210da1fa22ac03",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/3cf304855be8ec04158d976d15210da1fa22ac03",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/InvalidAQEPlanException.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ValidateSparkPlan.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala"
        ],
        "message": "[SPARK-39551][SQL] Add AQE invalid plan check\n\nThis PR adds a check for invalid plans in AQE replanning process. The check will throw exceptions when it detects an invalid plan, causing AQE to void the current replanning result and keep using the latest valid plan.\n\nAQE logical optimization rules can lead to invalid physical plans and cause runtime exceptions as certain physical plan nodes are not compatible with others. E.g., `BroadcastExchangeExec` can only work as a direct child of broadcast join nodes, but it could appear under other incompatible physical plan nodes because of empty relation propagation.\n\nNo.\n\nAdded UT.\n\nCloses #36953 from maryannxue/validate-aqe.\n\nAuthored-by: Maryann Xue <maryann.xue@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 58b91b1fa381f0a173c7b3c015337113f8f2b6c6)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/InvalidAQEPlanException.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/InvalidAQEPlanException.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ValidateSparkPlan.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ValidateSparkPlan.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "116:     Seq(",
          "117:       RemoveRedundantProjects,",
          "118:       ensureRequirements,",
          "119:       ReplaceHashWithSortAgg,",
          "120:       RemoveRedundantSorts,",
          "121:       DisableUnnecessaryBucketedScan,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "119:       ValidateSparkPlan,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "301:         val logicalPlan = replaceWithQueryStagesInLogicalPlan(currentLogicalPlan, stagesToReplace)",
          "306:             (newCost == origCost && currentPhysicalPlan != newPhysicalPlan)) {",
          "312:         }",
          "314:         result = createQueryStages(currentPhysicalPlan)",
          "",
          "[Removed Lines]",
          "302:         val (newPhysicalPlan, newLogicalPlan) = reOptimize(logicalPlan)",
          "303:         val origCost = costEvaluator.evaluateCost(currentPhysicalPlan)",
          "304:         val newCost = costEvaluator.evaluateCost(newPhysicalPlan)",
          "305:         if (newCost < origCost ||",
          "307:           logOnLevel(s\"Plan changed from $currentPhysicalPlan to $newPhysicalPlan\")",
          "308:           cleanUpTempTags(newPhysicalPlan)",
          "309:           currentPhysicalPlan = newPhysicalPlan",
          "310:           currentLogicalPlan = newLogicalPlan",
          "311:           stagesToReplace = Seq.empty[QueryStageExec]",
          "",
          "[Added Lines]",
          "303:         val afterReOptimize = reOptimize(logicalPlan)",
          "304:         if (afterReOptimize.isDefined) {",
          "305:           val (newPhysicalPlan, newLogicalPlan) = afterReOptimize.get",
          "306:           val origCost = costEvaluator.evaluateCost(currentPhysicalPlan)",
          "307:           val newCost = costEvaluator.evaluateCost(newPhysicalPlan)",
          "308:           if (newCost < origCost ||",
          "310:             logOnLevel(s\"Plan changed from $currentPhysicalPlan to $newPhysicalPlan\")",
          "311:             cleanUpTempTags(newPhysicalPlan)",
          "312:             currentPhysicalPlan = newPhysicalPlan",
          "313:             currentLogicalPlan = newLogicalPlan",
          "314:             stagesToReplace = Seq.empty[QueryStageExec]",
          "315:           }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "664:   }",
          "",
          "[Removed Lines]",
          "641:   private def reOptimize(logicalPlan: LogicalPlan): (SparkPlan, LogicalPlan) = {",
          "642:     logicalPlan.invalidateStatsCache()",
          "643:     val optimized = optimizer.execute(logicalPlan)",
          "644:     val sparkPlan = context.session.sessionState.planner.plan(ReturnAnswer(optimized)).next()",
          "645:     val newPlan = applyPhysicalRules(",
          "646:       sparkPlan,",
          "647:       preprocessingRules ++ queryStagePreparationRules,",
          "648:       Some((planChangeLogger, \"AQE Replanning\")))",
          "657:     val finalPlan = currentPhysicalPlan match {",
          "658:       case b: BroadcastExchangeLike",
          "659:         if (!newPlan.isInstanceOf[BroadcastExchangeLike]) => b.withNewChildren(Seq(newPlan))",
          "660:       case _ => newPlan",
          "661:     }",
          "663:     (finalPlan, optimized)",
          "",
          "[Added Lines]",
          "645:   private def reOptimize(logicalPlan: LogicalPlan): Option[(SparkPlan, LogicalPlan)] = {",
          "646:     try {",
          "647:       logicalPlan.invalidateStatsCache()",
          "648:       val optimized = optimizer.execute(logicalPlan)",
          "649:       val sparkPlan = context.session.sessionState.planner.plan(ReturnAnswer(optimized)).next()",
          "650:       val newPlan = applyPhysicalRules(",
          "651:         sparkPlan,",
          "652:         preprocessingRules ++ queryStagePreparationRules,",
          "653:         Some((planChangeLogger, \"AQE Replanning\")))",
          "662:       val finalPlan = currentPhysicalPlan match {",
          "663:         case b: BroadcastExchangeLike",
          "664:           if (!newPlan.isInstanceOf[BroadcastExchangeLike]) => b.withNewChildren(Seq(newPlan))",
          "665:         case _ => newPlan",
          "666:       }",
          "668:       Some((finalPlan, optimized))",
          "669:     } catch {",
          "670:       case e: InvalidAQEPlanException[_] =>",
          "671:         logOnLevel(s\"Re-optimize - ${e.getMessage()}:\\n${e.plan}\")",
          "672:         None",
          "673:     }",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/InvalidAQEPlanException.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/InvalidAQEPlanException.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/InvalidAQEPlanException.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/InvalidAQEPlanException.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.sql.execution.adaptive",
          "20: import org.apache.spark.sql.catalyst.plans.QueryPlan",
          "29: case class InvalidAQEPlanException[QueryType <: QueryPlan[_]](message: String, plan: QueryType)",
          "30:   extends Exception(message)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ValidateSparkPlan.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ValidateSparkPlan.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ValidateSparkPlan.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/ValidateSparkPlan.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.sql.execution.adaptive",
          "20: import org.apache.spark.sql.catalyst.optimizer.{BuildLeft, BuildRight}",
          "21: import org.apache.spark.sql.catalyst.rules.Rule",
          "22: import org.apache.spark.sql.execution.SparkPlan",
          "23: import org.apache.spark.sql.execution.joins.{BroadcastHashJoinExec, BroadcastNestedLoopJoinExec}",
          "30: object ValidateSparkPlan extends Rule[SparkPlan] {",
          "32:   def apply(plan: SparkPlan): SparkPlan = {",
          "33:     validate(plan)",
          "34:     plan",
          "35:   }",
          "42:   private def validate(plan: SparkPlan): Unit = plan match {",
          "43:     case b: BroadcastHashJoinExec =>",
          "44:       val (buildPlan, probePlan) = b.buildSide match {",
          "45:         case BuildLeft => (b.left, b.right)",
          "46:         case BuildRight => (b.right, b.left)",
          "47:       }",
          "48:       if (!buildPlan.isInstanceOf[BroadcastQueryStageExec]) {",
          "49:         validate(buildPlan)",
          "50:       }",
          "51:       validate(probePlan)",
          "52:     case b: BroadcastNestedLoopJoinExec =>",
          "53:       val (buildPlan, probePlan) = b.buildSide match {",
          "54:         case BuildLeft => (b.left, b.right)",
          "55:         case BuildRight => (b.right, b.left)",
          "56:       }",
          "57:       if (!buildPlan.isInstanceOf[BroadcastQueryStageExec]) {",
          "58:         validate(buildPlan)",
          "59:       }",
          "60:       validate(probePlan)",
          "61:     case q: BroadcastQueryStageExec => errorOnInvalidBroadcastQueryStage(q)",
          "62:     case _ => plan.children.foreach(validate)",
          "63:   }",
          "65:   private def errorOnInvalidBroadcastQueryStage(plan: SparkPlan): Unit = {",
          "66:     throw InvalidAQEPlanException(\"Invalid broadcast query stage\", plan)",
          "67:   }",
          "68: }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: import org.apache.spark.sql.execution.datasources.noop.NoopDataSource",
          "35: import org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec",
          "36: import org.apache.spark.sql.execution.exchange.{BroadcastExchangeExec, ENSURE_REQUIREMENTS, Exchange, REPARTITION_BY_COL, REPARTITION_BY_NUM, ReusedExchangeExec, ShuffleExchangeExec, ShuffleExchangeLike, ShuffleOrigin}",
          "38: import org.apache.spark.sql.execution.metric.SQLShuffleReadMetricsReporter",
          "39: import org.apache.spark.sql.execution.ui.SparkListenerSQLAdaptiveExecutionUpdate",
          "40: import org.apache.spark.sql.functions._",
          "",
          "[Removed Lines]",
          "37: import org.apache.spark.sql.execution.joins.{BaseJoinExec, BroadcastHashJoinExec, ShuffledHashJoinExec, ShuffledJoin, SortMergeJoinExec}",
          "",
          "[Added Lines]",
          "37: import org.apache.spark.sql.execution.joins.{BaseJoinExec, BroadcastHashJoinExec, BroadcastNestedLoopJoinExec, ShuffledHashJoinExec, ShuffledJoin, SortMergeJoinExec}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "104:     }",
          "105:   }",
          "107:   private def findTopLevelSortMergeJoin(plan: SparkPlan): Seq[SortMergeJoinExec] = {",
          "108:     collect(plan) {",
          "109:       case j: SortMergeJoinExec => j",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "107:   def findTopLevelBroadcastNestedLoopJoin(plan: SparkPlan): Seq[BaseJoinExec] = {",
          "108:     collect(plan) {",
          "109:       case j: BroadcastNestedLoopJoinExec => j",
          "110:     }",
          "111:   }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2576:       assert(findTopLevelAggregate(adaptive5).size == 4)",
          "2577:     }",
          "2578:   }",
          "2579: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2586:   test(\"SPARK-39551: Invalid plan check - invalid broadcast query stage\") {",
          "2587:     withSQLConf(",
          "2588:       SQLConf.ADAPTIVE_EXECUTION_ENABLED.key -> \"true\") {",
          "2589:       val (_, adaptivePlan) = runAdaptiveAndVerifyResult(",
          "2590:         \"\"\"",
          "2591:           |SELECT /*+ BROADCAST(t3) */ t3.b, count(t3.a) FROM testData2 t1",
          "2592:           |INNER JOIN testData2 t2",
          "2593:           |ON t1.b = t2.b AND t1.a = 0",
          "2594:           |RIGHT OUTER JOIN testData2 t3",
          "2595:           |ON t1.a > t3.a",
          "2596:           |GROUP BY t3.b",
          "2597:         \"\"\".stripMargin",
          "2598:       )",
          "2599:       assert(findTopLevelBroadcastNestedLoopJoin(adaptivePlan).size == 1)",
          "2600:     }",
          "2601:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}