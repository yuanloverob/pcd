{
  "cve_id": "CVE-2023-50943",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows a potential attacker to poison the XCom data by bypassing the protection of \"enable_xcom_pickling=False\" configuration setting resulting in poisoned data after XCom deserialization. This vulnerability is considered low since it requires a DAG author to exploit it. Users are recommended to upgrade to version 2.8.1 or later, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
  "patch_info": {
    "commit_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "files": [
      "airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py"
    ],
    "message": "Stop deserializing pickle when enable_xcom_pickling is False (#36255)\n\n* Stop deserializing pickle when enable_xcom_pickling is False\n\n* Fix unit tests\n\n(cherry picked from commit 63e97abec5d56bc62a293c93f5227f364561e51c)",
    "before_after_code_files": [
      "airflow/models/xcom.py||airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py||tests/models/test_xcom.py"
    ]
  },
  "patch_diff": {
    "airflow/models/xcom.py||airflow/models/xcom.py": [
      "File: airflow/models/xcom.py -> airflow/models/xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "685:             except pickle.UnpicklingError:",
      "686:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "687:         else:",
      "693:     @staticmethod",
      "694:     def deserialize_value(result: XCom) -> Any:",
      "",
      "[Removed Lines]",
      "688:             try:",
      "689:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "690:             except (json.JSONDecodeError, UnicodeDecodeError):",
      "691:                 return pickle.loads(result.value)",
      "",
      "[Added Lines]",
      "688:             # Since xcom_pickling is disabled, we should only try to deserialize with JSON",
      "689:             return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "",
      "---------------"
    ],
    "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py": [
      "File: tests/api_connexion/schemas/test_xcom_schema.py -> tests/api_connexion/schemas/test_xcom_schema.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from airflow.models import DagRun, XCom",
      "31: from airflow.utils.dates import parse_execution_date",
      "32: from airflow.utils.session import create_session",
      "34: pytestmark = pytest.mark.db_test",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "188:     default_time = \"2016-04-02T21:00:00+00:00\"",
      "189:     default_time_parsed = parse_execution_date(default_time)",
      "191:     def test_serialize(self, create_xcom, session):",
      "192:         create_xcom(",
      "193:             dag_id=\"test_dag\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "192:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "208:             \"map_index\": -1,",
      "209:         }",
      "211:     def test_deserialize(self):",
      "212:         xcom_dump = {",
      "213:             \"key\": \"test_key\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "213:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------"
    ],
    "tests/models/test_xcom.py||tests/models/test_xcom.py": [
      "File: tests/models/test_xcom.py -> tests/models/test_xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "140:             ret_value = XCom.get_value(key=\"xcom_test3\", ti_key=ti_key, session=session)",
      "141:         assert ret_value == {\"key\": \"value\"}",
      "144:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"}):",
      "145:             XCom.set(",
      "146:                 key=\"xcom_test3\",",
      "",
      "[Removed Lines]",
      "143:     def test_xcom_deserialize_with_pickle_to_json_switch(self, task_instance, session):",
      "",
      "[Added Lines]",
      "143:     def test_xcom_deserialize_pickle_when_xcom_pickling_is_disabled(self, task_instance, session):",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "151:                 session=session,",
      "152:             )",
      "153:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"False\"}):",
      "163:     @conf_vars({(\"core\", \"xcom_enable_pickling\"): \"False\"})",
      "164:     def test_xcom_disable_pickle_type_fail_on_non_json(self, task_instance, session):",
      "",
      "[Removed Lines]",
      "154:             ret_value = XCom.get_one(",
      "155:                 key=\"xcom_test3\",",
      "156:                 dag_id=task_instance.dag_id,",
      "157:                 task_id=task_instance.task_id,",
      "158:                 run_id=task_instance.run_id,",
      "159:                 session=session,",
      "160:             )",
      "161:         assert ret_value == {\"key\": \"value\"}",
      "",
      "[Added Lines]",
      "154:             with pytest.raises(UnicodeDecodeError):",
      "155:                 XCom.get_one(",
      "156:                     key=\"xcom_test3\",",
      "157:                     dag_id=task_instance.dag_id,",
      "158:                     task_id=task_instance.task_id,",
      "159:                     run_id=task_instance.run_id,",
      "160:                     session=session,",
      "161:                 )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "408e2d2fe7578030bb3ca9df7f2ace4b654d67f8",
      "candidate_info": {
        "commit_hash": "408e2d2fe7578030bb3ca9df7f2ace4b654d67f8",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/408e2d2fe7578030bb3ca9df7f2ace4b654d67f8",
        "files": [
          "setup.cfg"
        ],
        "message": "Ignore MyPy error introduced by the new Apache Beam (#36607)\n\nThe new Apache Beam 2.53.0 introduced a MyPy error that breaks\nour MyPy checks.\n\nThis is captured in https://github.com/apache/beam/issues/29927\nbut until it is addressed we need to ignore it.\n\n(cherry picked from commit 0c10ddb3c6e9d8cbc1592d1e0bf5532c5ed4dfa9)",
        "before_after_code_files": [
          "setup.cfg||setup.cfg"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.cfg||setup.cfg": [
          "File: setup.cfg -> setup.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "236: [mypy-referencing.*]",
          "237: # Referencing has some old type annotations that are not compatible with new versions of mypy",
          "238: ignore_errors = True",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "241: [mypy-apache_beam.*]",
          "242: # Beam has some old type annotations and they introduced an error recently with bad signature of",
          "243: # a function. This is captured in https://github.com/apache/beam/issues/29927",
          "244: # and we should remove this exclusion when it is fixed.",
          "245: ignore_errors = True",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7b5395bb64db9d131bf8e809adc332eabd9763c3",
      "candidate_info": {
        "commit_hash": "7b5395bb64db9d131bf8e809adc332eabd9763c3",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7b5395bb64db9d131bf8e809adc332eabd9763c3",
        "files": [
          "airflow/www/auth.py",
          "tests/www/test_auth.py"
        ],
        "message": "Redirect to index when user does not have permission to access a page (#36623)\n\n(cherry picked from commit 535c8be599f5e1a9455b6e6ab1840aa446ce3b1e)",
        "before_after_code_files": [
          "airflow/www/auth.py||airflow/www/auth.py",
          "tests/www/test_auth.py||tests/www/test_auth.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/auth.py||airflow/www/auth.py": [
          "File: airflow/www/auth.py -> airflow/www/auth.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: from functools import wraps",
          "23: from typing import TYPE_CHECKING, Callable, Sequence, TypeVar, cast",
          "26: from flask_appbuilder._compat import as_unicode",
          "27: from flask_appbuilder.const import (",
          "28:     FLAMSG_ERR_SEC_ACCESS_DENIED,",
          "",
          "[Removed Lines]",
          "25: from flask import flash, redirect, render_template, request",
          "",
          "[Added Lines]",
          "25: from flask import flash, redirect, render_template, request, url_for",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "176:             ),",
          "177:             403,",
          "178:         )",
          "179:     else:",
          "180:         access_denied = get_access_denied_message()",
          "181:         flash(access_denied, \"danger\")",
          "185: def has_access_cluster_activity(method: ResourceMethod) -> Callable[[T], T]:",
          "",
          "[Removed Lines]",
          "182:     return redirect(get_auth_manager().get_url_login(next=request.url))",
          "",
          "[Added Lines]",
          "179:     elif not get_auth_manager().is_logged_in():",
          "180:         return redirect(get_auth_manager().get_url_login(next=request.url))",
          "184:     return redirect(url_for(\"Airflow.index\"))",
          "",
          "---------------"
        ],
        "tests/www/test_auth.py||tests/www/test_auth.py": [
          "File: tests/www/test_auth.py -> tests/www/test_auth.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "206:             result = auth.has_access_dag_entities(\"GET\", dag_access_entity)(self.method_test)(None, items)",
          "208:         mock_call.assert_not_called()",
          "212: @pytest.mark.db_test",
          "",
          "[Removed Lines]",
          "209:         assert result.status_code == 302",
          "",
          "[Added Lines]",
          "209:         assert result.headers[\"Location\"] == \"/home\"",
          "211:     @pytest.mark.db_test",
          "212:     @patch(\"airflow.www.auth.get_auth_manager\")",
          "213:     def test_has_access_dag_entities_when_logged_out(self, mock_get_auth_manager, app, dag_access_entity):",
          "214:         auth_manager = Mock()",
          "215:         auth_manager.batch_is_authorized_dag.return_value = False",
          "216:         auth_manager.is_logged_in.return_value = False",
          "217:         auth_manager.get_url_login.return_value = \"login_url\"",
          "218:         mock_get_auth_manager.return_value = auth_manager",
          "219:         items = [Mock(dag_id=\"dag_1\"), Mock(dag_id=\"dag_2\")]",
          "221:         with app.test_request_context():",
          "222:             result = auth.has_access_dag_entities(\"GET\", dag_access_entity)(self.method_test)(None, items)",
          "224:         mock_call.assert_not_called()",
          "225:         assert result.headers[\"Location\"] == \"login_url\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "de1f0230921246f0a85055a4ea988914b11d7bf6",
      "candidate_info": {
        "commit_hash": "de1f0230921246f0a85055a4ea988914b11d7bf6",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/de1f0230921246f0a85055a4ea988914b11d7bf6",
        "files": [
          "airflow/www/fab_security/manager.py"
        ],
        "message": "Add back FAB constant in legacy security manager (#36719)\n\n(cherry picked from commit acdbd577ab305718fbc05377c03f16c113b5ca26)",
        "before_after_code_files": [
          "airflow/www/fab_security/manager.py||airflow/www/fab_security/manager.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/fab_security/manager.py||airflow/www/fab_security/manager.py": [
          "File: airflow/www/fab_security/manager.py -> airflow/www/fab_security/manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: log = logging.getLogger(__name__)",
          "28: __lazy_imports = {",
          "29:     \"AUTH_DB\": \"flask_appbuilder.const\",",
          "30:     \"AUTH_LDAP\": \"flask_appbuilder.const\",",
          "31:     \"LOGMSG_WAR_SEC_LOGIN_FAILED\": \"flask_appbuilder.const\",",
          "32: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "29:     \"AUTH_OID\": \"flask_appbuilder.const\",",
          "32:     \"AUTH_REMOTE_USER\": \"flask_appbuilder.const\",",
          "33:     \"AUTH_OAUTH\": \"flask_appbuilder.const\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "fcd235777ec8df4dd7d43ce56cfec3762c79dbaf",
      "candidate_info": {
        "commit_hash": "fcd235777ec8df4dd7d43ce56cfec3762c79dbaf",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/fcd235777ec8df4dd7d43ce56cfec3762c79dbaf",
        "files": [
          "airflow/utils/log/file_processor_handler.py",
          "tests/utils/log/test_file_processor_handler.py"
        ],
        "message": "Create latest log dir symlink as relative link (#36019)\n\nSo `logs/**/*` is self-contained and the symlink doesn't break for\nexporting, build artifact, etc\n\n(cherry picked from commit 17e91727b29448e46470a8dd4b5909a0bdf38eb2)",
        "before_after_code_files": [
          "airflow/utils/log/file_processor_handler.py||airflow/utils/log/file_processor_handler.py",
          "tests/utils/log/test_file_processor_handler.py||tests/utils/log/test_file_processor_handler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/log/file_processor_handler.py||airflow/utils/log/file_processor_handler.py": [
          "File: airflow/utils/log/file_processor_handler.py -> airflow/utils/log/file_processor_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "117:         log_directory = self._get_log_directory()",
          "118:         latest_log_directory_path = os.path.join(self.base_log_folder, \"latest\")",
          "119:         if os.path.isdir(log_directory):",
          "120:             try:",
          "121:                 # if symlink exists but is stale, update it",
          "122:                 if os.path.islink(latest_log_directory_path):",
          "124:                         os.unlink(latest_log_directory_path)",
          "126:                 elif os.path.isdir(latest_log_directory_path) or os.path.isfile(latest_log_directory_path):",
          "127:                     logging.warning(",
          "128:                         \"%s already exists as a dir/file. Skip creating symlink.\", latest_log_directory_path",
          "129:                     )",
          "130:                 else:",
          "132:             except OSError:",
          "133:                 logging.warning(\"OSError while attempting to symlink the latest log directory\")",
          "",
          "[Removed Lines]",
          "123:                     if os.readlink(latest_log_directory_path) != log_directory:",
          "125:                         os.symlink(log_directory, latest_log_directory_path)",
          "131:                     os.symlink(log_directory, latest_log_directory_path)",
          "",
          "[Added Lines]",
          "120:             rel_link_target = Path(log_directory).relative_to(Path(latest_log_directory_path).parent)",
          "124:                     if os.path.realpath(latest_log_directory_path) != log_directory:",
          "126:                         os.symlink(rel_link_target, latest_log_directory_path)",
          "132:                     os.symlink(rel_link_target, latest_log_directory_path)",
          "",
          "---------------"
        ],
        "tests/utils/log/test_file_processor_handler.py||tests/utils/log/test_file_processor_handler.py": [
          "File: tests/utils/log/test_file_processor_handler.py -> tests/utils/log/test_file_processor_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "80:         with time_machine.travel(date1, tick=False):",
          "81:             handler.set_context(filename=os.path.join(self.dag_dir, \"log1\"))",
          "82:             assert os.path.islink(link)",
          "84:             assert os.path.exists(os.path.join(link, \"log1\"))",
          "86:         with time_machine.travel(date2, tick=False):",
          "87:             handler.set_context(filename=os.path.join(self.dag_dir, \"log2\"))",
          "88:             assert os.path.islink(link)",
          "90:             assert os.path.exists(os.path.join(link, \"log2\"))",
          "92:     def test_symlink_latest_log_directory_exists(self):",
          "",
          "[Removed Lines]",
          "83:             assert os.path.basename(os.readlink(link)) == date1",
          "89:             assert os.path.basename(os.readlink(link)) == date2",
          "",
          "[Added Lines]",
          "83:             assert os.path.basename(os.path.realpath(link)) == date1",
          "89:             assert os.path.basename(os.path.realpath(link)) == date2",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "986f6d8ea2b6cc47613bbcbb226eeb2c0ba48e7b",
      "candidate_info": {
        "commit_hash": "986f6d8ea2b6cc47613bbcbb226eeb2c0ba48e7b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/986f6d8ea2b6cc47613bbcbb226eeb2c0ba48e7b",
        "files": [
          "tests/models/test_mappedoperator.py"
        ],
        "message": "Add few tests on the mapped task group. (#36149)\n\n(cherry picked from commit bed2789c246c71656bad1cf45374ebef4d28fb2d)",
        "before_after_code_files": [
          "tests/models/test_mappedoperator.py||tests/models/test_mappedoperator.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/models/test_mappedoperator.py||tests/models/test_mappedoperator.py": [
          "File: tests/models/test_mappedoperator.py -> tests/models/test_mappedoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1568:             \"tg_2.my_work\": \"skipped\",",
          "1569:         }",
          "1570:         assert states == expected",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1572:     def test_skip_one_mapped_task_from_task_group_with_generator(self, dag_maker):",
          "1573:         with dag_maker() as dag:",
          "1575:             @task",
          "1576:             def make_list():",
          "1577:                 return [1, 2, 3]",
          "1579:             @task",
          "1580:             def double(n):",
          "1581:                 if n == 2:",
          "1582:                     raise AirflowSkipException()",
          "1583:                 return n * 2",
          "1585:             @task",
          "1586:             def last(n):",
          "1587:                 ...",
          "1589:             @task_group",
          "1590:             def group(n: int) -> None:",
          "1591:                 last(double(n))",
          "1593:             group.expand(n=make_list())",
          "1595:         dr = dag.test()",
          "1596:         states = self.get_states(dr)",
          "1597:         expected = {",
          "1598:             \"group.double\": {0: \"success\", 1: \"skipped\", 2: \"success\"},",
          "1599:             \"group.last\": {0: \"success\", 1: \"skipped\", 2: \"success\"},",
          "1600:             \"make_list\": \"success\",",
          "1601:         }",
          "1602:         assert states == expected",
          "1604:     def test_skip_one_mapped_task_from_task_group(self, dag_maker):",
          "1605:         with dag_maker() as dag:",
          "1607:             @task",
          "1608:             def double(n):",
          "1609:                 if n == 2:",
          "1610:                     raise AirflowSkipException()",
          "1611:                 return n * 2",
          "1613:             @task",
          "1614:             def last(n):",
          "1615:                 ...",
          "1617:             @task_group",
          "1618:             def group(n: int) -> None:",
          "1619:                 last(double(n))",
          "1621:             group.expand(n=[1, 2, 3])",
          "1623:         dr = dag.test()",
          "1624:         states = self.get_states(dr)",
          "1625:         expected = {",
          "1626:             \"group.double\": {0: \"success\", 1: \"skipped\", 2: \"success\"},",
          "1627:             \"group.last\": {0: \"success\", 1: \"skipped\", 2: \"success\"},",
          "1628:         }",
          "1629:         assert states == expected",
          "",
          "---------------"
        ]
      }
    }
  ]
}