{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "fa400c666c41cf864103ba8705116a24092b3687",
      "candidate_info": {
        "commit_hash": "fa400c666c41cf864103ba8705116a24092b3687",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/fa400c666c41cf864103ba8705116a24092b3687",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala"
        ],
        "message": "[SPARK-39243][SQL][DOCS] Rules of quoting elements in error messages\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to describe the rules of quoting elements in error messages introduced by the PRs:\n- https://github.com/apache/spark/pull/36210\n- https://github.com/apache/spark/pull/36233\n- https://github.com/apache/spark/pull/36259\n- https://github.com/apache/spark/pull/36324\n- https://github.com/apache/spark/pull/36335\n- https://github.com/apache/spark/pull/36359\n- https://github.com/apache/spark/pull/36579\n\n### Why are the changes needed?\nTo improve code maintenance, and the process of code review.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nBy existing GAs.\n\nCloses #36621 from MaxGekk/update-error-class-guide.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>\n(cherry picked from commit 2a4d8a4ea709339175257027e31a75bdeed5daec)\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala"
        ]
      }
    },
    {
      "candidate_hash": "ec40006aa3bda9f6fd03bb9c0bda561c139ed5ce",
      "candidate_info": {
        "commit_hash": "ec40006aa3bda9f6fd03bb9c0bda561c139ed5ce",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/ec40006aa3bda9f6fd03bb9c0bda561c139ed5ce",
        "files": [
          "docs/running-on-kubernetes.md",
          "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/YuniKornSuite.scala"
        ],
        "message": "[SPARK-40423][K8S][TESTS] Add explicit YuniKorn queue submission test coverage\n\n### What changes were proposed in this pull request?\n\nThis PR aims to add explicit Yunikorn queue submission test coverage instead of implicit assignment by admission controller.\n\n### Why are the changes needed?\n\n- To provide a proper test coverage.\n- To prevent the side effect of YuniKorn admission controller which overrides all Spark's scheduler settings by default (if we do not edit the rule explicitly). This breaks Apache Spark's default scheduler K8s IT test coverage.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nManually run the CI and check the YuniKorn queue UI.\n```\n$ build/sbt -Psparkr -Pkubernetes -Pkubernetes-integration-tests -Dspark.kubernetes.test.deployMode=docker-desktop \"kubernetes-integration-tests/test\" -Dtest.exclude.tags=minikube,local,decom -Dtest.default.exclude.tags=\n```\n\n<img width=\"1197\" alt=\"Screen Shot 2022-09-14 at 2 07 38 AM\" src=\"https://user-images.githubusercontent.com/9700541/190112005-5863bdd3-2e43-4ec7-b34b-a286d1a7c95e.png\">\n\nCloses #37877 from dongjoon-hyun/SPARK-40423.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 12e48527846d993a78b159fbba3e900a4feb7b55)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/YuniKornSuite.scala||resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/YuniKornSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/YuniKornSuite.scala||resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/YuniKornSuite.scala": [
          "File: resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/YuniKornSuite.scala -> resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/YuniKornSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "22:   override protected def setUpTest(): Unit = {",
          "23:     super.setUpTest()",
          "24:     sparkAppConf",
          "25:       .set(\"spark.kubernetes.scheduler.name\", \"yunikorn\")",
          "26:       .set(\"spark.kubernetes.driver.annotation.yunikorn.apache.org/app-id\", \"{{APP_ID}}\")",
          "27:       .set(\"spark.kubernetes.executor.annotation.yunikorn.apache.org/app-id\", \"{{APP_ID}}\")",
          "28:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24:     val namespace = sparkAppConf.get(\"spark.kubernetes.namespace\")",
          "27:       .set(\"spark.kubernetes.driver.label.queue\", \"root.\" + namespace)",
          "28:       .set(\"spark.kubernetes.executor.label.queue\", \"root.\" + namespace)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "37a2416ca4c37eebeabfefc3be812594804f5ff5",
      "candidate_info": {
        "commit_hash": "37a2416ca4c37eebeabfefc3be812594804f5ff5",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/37a2416ca4c37eebeabfefc3be812594804f5ff5",
        "files": [
          "python/pyspark/sql/tests/test_dataframe.py"
        ],
        "message": "[SPARK-39252][PYSPARK][TESTS] Remove flaky test_df_is_empty\n\n### What changes were proposed in this pull request?\n\n### Why are the changes needed?\n\nThis PR removes flaky `test_df_is_empty` as reported in https://issues.apache.org/jira/browse/SPARK-39252. I will open a follow-up PR to reintroduce the test and fix the flakiness (or see if it was a regression).\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting unit tests.\n\nCloses #36656 from sadikovi/SPARK-39252.\n\nAuthored-by: Ivan Sadikov <ivan.sadikov@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 9823bb385cd6dca7c4fb5a6315721420ad42f80a)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/sql/tests/test_dataframe.py||python/pyspark/sql/tests/test_dataframe.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/sql/tests/test_dataframe.py||python/pyspark/sql/tests/test_dataframe.py": [
          "File: python/pyspark/sql/tests/test_dataframe.py -> python/pyspark/sql/tests/test_dataframe.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import tempfile",
          "23: import time",
          "24: import unittest",
          "26: from typing import cast",
          "28: from pyspark.sql import SparkSession, Row",
          "",
          "[Removed Lines]",
          "25: import uuid",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1142:         with self.assertRaisesRegex(TypeError, \"Parameter 'truncate=foo'\"):",
          "1143:             df.show(truncate=\"foo\")",
          "1180:     @unittest.skipIf(",
          "1181:         not have_pandas or not have_pyarrow,",
          "1182:         cast(str, pandas_requirement_message or pyarrow_requirement_message),",
          "",
          "[Removed Lines]",
          "1145:     def test_df_is_empty(self):",
          "1146:         # SPARK-39084: Fix df.rdd.isEmpty() resulting in JVM crash.",
          "1148:         # This particular example of DataFrame reproduces an issue in isEmpty call",
          "1149:         # which could result in JVM crash.",
          "1150:         data = []",
          "1151:         for t in range(0, 10000):",
          "1152:             id = str(uuid.uuid4())",
          "1153:             if t == 0:",
          "1154:                 for i in range(0, 99):",
          "1155:                     data.append((id,))",
          "1156:             elif t < 10:",
          "1157:                 for i in range(0, 75):",
          "1158:                     data.append((id,))",
          "1159:             elif t < 100:",
          "1160:                 for i in range(0, 50):",
          "1161:                     data.append((id,))",
          "1162:             elif t < 1000:",
          "1163:                 for i in range(0, 25):",
          "1164:                     data.append((id,))",
          "1165:             else:",
          "1166:                 for i in range(0, 10):",
          "1167:                     data.append((id,))",
          "1169:         tmpPath = tempfile.mkdtemp()",
          "1170:         shutil.rmtree(tmpPath)",
          "1171:         try:",
          "1172:             df = self.spark.createDataFrame(data, [\"col\"])",
          "1173:             df.coalesce(1).write.parquet(tmpPath)",
          "1175:             res = self.spark.read.parquet(tmpPath).groupBy(\"col\").count()",
          "1176:             self.assertFalse(res.rdd.isEmpty())",
          "1177:         finally:",
          "1178:             shutil.rmtree(tmpPath)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a7554c34b59d1cacf53b2e239acd746a886bdde6",
      "candidate_info": {
        "commit_hash": "a7554c34b59d1cacf53b2e239acd746a886bdde6",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/a7554c34b59d1cacf53b2e239acd746a886bdde6",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparison.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/UnwrapCastInComparisonEndToEndSuite.scala"
        ],
        "message": "[SPARK-39476][SQL] Disable Unwrap cast optimize when casting from Long to Float/ Double or from Integer to Float\n\n### What changes were proposed in this pull request?\nCast from Integer to Float or from Long to Double/Float may loss precision if the length of Integer/Long beyonds the **significant digits** of a Double(which is 15 or 16 digits) or Float(which is 7 or 8 digits).\n\nFor example, ```select *, cast(a as int) from (select cast(33554435 as foat) a )``` gives `33554436` instead of `33554435`.\n\nWhen it comes the optimization rule `UnwrapCastInBinaryComparison`, it may result in incorrect (confused) result .\nWe can reproduce it with following script.\n```\nspark.range(10).map(i => 64707595868612313L).createOrReplaceTempView(\"tbl\")\nval df = sql(\"select * from tbl where cast(value as double) = cast('64707595868612313' as double)\")\ndf.explain(true)\ndf.show()\n```\n\nWith we disable this optimization rule , it returns 10 records.\nBut if we enable this optimization rule, it returns empty, since the sql is optimized to\n```\nselect * from tbl where value = 64707595868612312L\n```\n\n### Why are the changes needed?\nFix the behavior that may confuse users (or maybe a bug?)\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nAdd a new UT\n\nCloses #36873 from WangGuangxin/SPARK-24994-followup.\n\nAuthored-by: wangguangxin.cn <wangguangxin.cn@bytedance.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 9612db3fc9c38204b2bf9f724dedb9ec5f636556)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparison.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparison.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/UnwrapCastInComparisonEndToEndSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/UnwrapCastInComparisonEndToEndSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparison.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparison.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparison.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparison.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "358:     toType.sameType(literalType) &&",
          "359:       !fromExp.foldable &&",
          "360:       toType.isInstanceOf[NumericType] &&",
          "363:   }",
          "365:   private[optimizer] def getRange(dt: DataType): Option[(Any, Any)] = dt match {",
          "",
          "[Removed Lines]",
          "361:       ((fromExp.dataType.isInstanceOf[NumericType] && Cast.canUpCast(fromExp.dataType, toType)) ||",
          "362:         fromExp.dataType.isInstanceOf[BooleanType])",
          "",
          "[Added Lines]",
          "361:       canUnwrapCast(fromExp.dataType, toType)",
          "362:   }",
          "364:   private def canUnwrapCast(from: DataType, to: DataType): Boolean = (from, to) match {",
          "365:     case (BooleanType, _) => true",
          "368:     case (IntegerType, FloatType) => false",
          "369:     case (LongType, FloatType) => false",
          "370:     case (LongType, DoubleType) => false",
          "371:     case _ if from.isInstanceOf[NumericType] => Cast.canUpCast(from, to)",
          "372:     case _ => false",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/UnwrapCastInComparisonEndToEndSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/UnwrapCastInComparisonEndToEndSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/UnwrapCastInComparisonEndToEndSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/UnwrapCastInComparisonEndToEndSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "209:     }",
          "210:   }",
          "212:   private def decimal(v: BigDecimal): Decimal = Decimal(v, 5, 2)",
          "213: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "212:   test(\"SPARK-39476: Should not unwrap cast from Long to Double/Float\") {",
          "213:     withTable(t) {",
          "214:       Seq((6470759586864300301L))",
          "215:         .toDF(\"c1\").write.saveAsTable(t)",
          "216:       val df = spark.table(t)",
          "218:       checkAnswer(",
          "219:         df.where(\"cast(c1 as double) == cast(6470759586864300301L as double)\")",
          "220:           .select(\"c1\"),",
          "221:         Row(6470759586864300301L))",
          "223:       checkAnswer(",
          "224:         df.where(\"cast(c1 as float) == cast(6470759586864300301L as float)\")",
          "225:           .select(\"c1\"),",
          "226:         Row(6470759586864300301L))",
          "227:     }",
          "228:   }",
          "230:   test(\"SPARK-39476: Should not unwrap cast from Integer to Float\") {",
          "231:     withTable(t) {",
          "232:       Seq((33554435))",
          "233:         .toDF(\"c1\").write.saveAsTable(t)",
          "234:       val df = spark.table(t)",
          "236:       checkAnswer(",
          "237:         df.where(\"cast(c1 as float) == cast(33554435 as float)\")",
          "238:           .select(\"c1\"),",
          "239:         Row(33554435))",
          "240:     }",
          "241:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4cb2ae220cf6c04221eaf70fa2af1526507a38de",
      "candidate_info": {
        "commit_hash": "4cb2ae220cf6c04221eaf70fa2af1526507a38de",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/4cb2ae220cf6c04221eaf70fa2af1526507a38de",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala"
        ],
        "message": "[SPARK-38666][SQL] Add missing aggregate filter checks\n\n### What changes were proposed in this pull request?\n\nAdd checks in `ResolveFunctions#validateFunction` to ensure the following about each aggregate filter:\n\n- has a datatype of boolean\n- doesn't contain an aggregate expression\n- doesn't contain a window expression\n\n`ExtractGenerator` already handles the case of a generator in an aggregate filter.\n\n### Why are the changes needed?\n\nThere are three cases where a query with an aggregate filter produces non-helpful error messages.\n\n1) Window expression in aggregate filter\n\n```\nselect sum(a) filter (where nth_value(a, 2) over (order by b) > 1)\nfrom (select 1 a, '2' b);\n```\nThe above query should produce an analysis error, but instead produces a stack overflow:\n```\njava.lang.StackOverflowError: null\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62) ~[scala-library.jar:?]\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53) ~[scala-library.jar:?]\n\tat scala.collection.immutable.VectorBuilder.$plus$plus$eq(Vector.scala:668) ~[scala-library.jar:?]\n\tat scala.collection.immutable.VectorBuilder.$plus$plus$eq(Vector.scala:645) ~[scala-library.jar:?]\n\tat scala.collection.generic.GenericCompanion.apply(GenericCompanion.scala:56) ~[scala-library.jar:?]\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.children(TreeNode.scala:1172) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.children$(TreeNode.scala:1172) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.children$lzycompute(Expression.scala:494) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.children(Expression.scala:494) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]\n\tat org.apache.spark.sql.catalyst.expressions.Expression.childrenResolved(Expression.scala:223) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]\n\tat org.apache.spark.sql.catalyst.expressions.Alias.resolved$lzycompute(namedExpressions.scala:155) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]\n\tat org.apache.spark.sql.catalyst.expressions.Alias.resolved(namedExpressions.scala:155) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]\n```\nWith this PR, the query will instead produce\n```\norg.apache.spark.sql.AnalysisException: FILTER expression contains window function. It cannot be used in an aggregate function; line 1 pos 7\n```\n\n2) Non-boolean filter expression\n\n```\nselect sum(a) filter (where a) from (select 1 a, '2' b);\n```\nThis query should produce an analysis error, but instead causes a projection compilation error or whole-stage codegen error (depending on the datatype of the expression):\n````\norg.codehaus.commons.compiler.CompileException: File 'generated.java', Line 50, Column 6: Not a boolean expression\n\tat org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12021) ~[janino-3.0.16.jar:?]\n\tat org.codehaus.janino.UnitCompiler.compileBoolean2(UnitCompiler.java:4049) ~[janino-3.0.16.jar:?]\n\tat org.codehaus.janino.UnitCompiler.access$6300(UnitCompiler.java:226) ~[janino-3.0.16.jar:?]\n\tat org.codehaus.janino.UnitCompiler$14.visitIntegerLiteral(UnitCompiler.java:4016) ~[janino-3.0.16.jar:?]\n\tat org.codehaus.janino.UnitCompiler$14.visitIntegerLiteral(UnitCompiler.java:3986) ~[janino-3.0.16.jar:?]\n...\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599) ~[guava-14.0.1.jar:?]\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379) ~[guava-14.0.1.jar:?]\n\t... 37 more\nNULL\nTime taken: 6.132 seconds, Fetched 1 row(s)\n````\nAfter the compilation error, _the query returns a result as if `a` was a boolean `false`_.\n\nWith this PR, the query will instead produce\n```\norg.apache.spark.sql.AnalysisException: FILTER expression is not of type boolean. It cannot be used in an aggregate function; line 1 pos 7\n```\n\n3) Aggregate expression in filter expression\n\n```\nselect max(b) filter (where max(a) > 1) from (select 1 a, '2' b);\n```\nThe above query should produce an analysis error, but instead causes a projection compilation error or whole-stage codegen error (depending on the datatype of the expression being aggregated):\n```\norg.apache.spark.SparkUnsupportedOperationException: Cannot generate code for expression: max(1)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotGenerateCodeForExpressionError(QueryExecutionErrors.scala:84) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode(Expression.scala:347) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode$(Expression.scala:346) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]\n\tat org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression.doGenCode(interfaces.scala:99) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]\n```\nWith this PR, the query will instead produce\n```\norg.apache.spark.sql.AnalysisException: FILTER expression contains aggregate. It cannot be used in an aggregate function; line 1 pos 7\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, except in error conditions.\n\n### How was this patch tested?\n\nNew unit tests.\n\nCloses #36072 from bersprockets/aggregate_in_aggregate_filter_issue.\n\nAuthored-by: Bruce Robbins <bersprockets@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 49d2f3c2458863eefd63c8ce38064757874ab4ad)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2218:           }",
          "2220:         case agg: AggregateFunction =>",
          "2223:           }",
          "2224:           if (u.ignoreNulls) {",
          "2225:             val aggFunc = agg match {",
          "",
          "[Removed Lines]",
          "2221:           if (u.filter.isDefined && !u.filter.get.deterministic) {",
          "2222:             throw QueryCompilationErrors.nonDeterministicFilterInAggregateError",
          "",
          "[Added Lines]",
          "2221:           u.filter match {",
          "2222:             case Some(filter) if !filter.deterministic =>",
          "2223:               throw QueryCompilationErrors.nonDeterministicFilterInAggregateError",
          "2224:             case Some(filter) if filter.dataType != BooleanType =>",
          "2225:               throw QueryCompilationErrors.nonBooleanFilterInAggregateError",
          "2226:             case Some(filter) if filter.exists(_.isInstanceOf[AggregateExpression]) =>",
          "2227:               throw QueryCompilationErrors.aggregateInAggregateFilterError",
          "2228:             case Some(filter) if filter.exists(_.isInstanceOf[WindowExpression]) =>",
          "2229:               throw QueryCompilationErrors.windowFunctionInAggregateFilterError",
          "2230:             case _ =>",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "334:       \"it cannot be used in aggregate functions\")",
          "335:   }",
          "337:   def aliasNumberNotMatchColumnNumberError(",
          "338:       columnSize: Int, outputSize: Int, t: TreeNode[_]): Throwable = {",
          "339:     new AnalysisException(\"Number of column aliases does not match number of columns. \" +",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "337:   def nonBooleanFilterInAggregateError(): Throwable = {",
          "338:     new AnalysisException(\"FILTER expression is not of type boolean. \" +",
          "339:       \"It cannot be used in an aggregate function\")",
          "340:   }",
          "342:   def aggregateInAggregateFilterError(): Throwable = {",
          "343:     new AnalysisException(\"FILTER expression contains aggregate. \" +",
          "344:       \"It cannot be used in an aggregate function\")",
          "345:   }",
          "347:   def windowFunctionInAggregateFilterError(): Throwable = {",
          "348:     new AnalysisException(\"FILTER expression contains window function. \" +",
          "349:       \"It cannot be used in an aggregate function\")",
          "350:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisErrorSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "545:       \"explode(array(min(a))), explode(array(max(a)))\" :: Nil",
          "546:   )",
          "548:   test(\"SPARK-6452 regression test\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "548:   errorTest(",
          "549:     \"SPARK-38666: non-boolean aggregate filter\",",
          "550:     CatalystSqlParser.parsePlan(\"SELECT sum(c) filter (where e) FROM TaBlE2\"),",
          "551:     \"FILTER expression is not of type boolean\" :: Nil)",
          "553:   errorTest(",
          "554:     \"SPARK-38666: aggregate in aggregate filter\",",
          "555:     CatalystSqlParser.parsePlan(\"SELECT sum(c) filter (where max(e) > 1) FROM TaBlE2\"),",
          "556:     \"FILTER expression contains aggregate\" :: Nil)",
          "558:   errorTest(",
          "559:     \"SPARK-38666: window function in aggregate filter\",",
          "560:     CatalystSqlParser.parsePlan(\"SELECT sum(c) \" +",
          "561:        \"filter (where nth_value(e, 2) over(order by b) > 1) FROM TaBlE2\"),",
          "562:     \"FILTER expression contains window function\" :: Nil)",
          "",
          "---------------"
        ]
      }
    }
  ]
}