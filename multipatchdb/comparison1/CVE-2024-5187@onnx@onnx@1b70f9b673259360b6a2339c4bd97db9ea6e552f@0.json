{
  "cve_id": "CVE-2024-5187",
  "cve_desc": "A vulnerability in the `download_model_with_test_data` function of the onnx/onnx framework, version 1.16.0, allows for arbitrary file overwrite due to inadequate prevention of path traversal attacks in malicious tar files. This vulnerability enables attackers to overwrite any file on the system, potentially leading to remote code execution, deletion of system, personal, or application files, thus impacting the integrity and availability of the system. The issue arises from the function's handling of tar file extraction without performing security checks on the paths within the tar file, as demonstrated by the ability to overwrite the `/home/kali/.ssh/authorized_keys` file by specifying an absolute path in the malicious tar file.",
  "repo": "onnx/onnx",
  "patch_hash": "1b70f9b673259360b6a2339c4bd97db9ea6e552f",
  "patch_info": {
    "commit_hash": "1b70f9b673259360b6a2339c4bd97db9ea6e552f",
    "repo": "onnx/onnx",
    "commit_url": "https://github.com/onnx/onnx/commit/1b70f9b673259360b6a2339c4bd97db9ea6e552f",
    "files": [
      "onnx/backend/test/runner/__init__.py",
      "onnx/hub.py",
      "onnx/utils.py"
    ],
    "message": "Refactor safe extract method to fix issue 6215 (#6222)\n\n### Description\n#6215\n\n---------\n\nSigned-off-by: liqunfu <liqun.fu@microsoft.com>\nSigned-off-by: liqun Fu <liqun.fu@microsoft.com>\nCo-authored-by: G. Ramalingam <grama@microsoft.com>\nCo-authored-by: Justin Chu <justinchuby@users.noreply.github.com>",
    "before_after_code_files": [
      "onnx/backend/test/runner/__init__.py||onnx/backend/test/runner/__init__.py",
      "onnx/hub.py||onnx/hub.py",
      "onnx/utils.py||onnx/utils.py"
    ]
  },
  "patch_diff": {
    "onnx/backend/test/runner/__init__.py||onnx/backend/test/runner/__init__.py": [
      "File: onnx/backend/test/runner/__init__.py -> onnx/backend/test/runner/__init__.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "10: import re",
      "11: import shutil",
      "12: import sys",
      "14: import tempfile",
      "15: import time",
      "16: import unittest",
      "",
      "[Removed Lines]",
      "13: import tarfile",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "242:             )",
      "243:             urlretrieve(model_test.url, download_file.name)",
      "244:             print(\"Done\")",
      "247:         except Exception as e:",
      "248:             print(f\"Failed to prepare data for model {model_test.model_name}: {e}\")",
      "249:             raise",
      "",
      "[Removed Lines]",
      "245:             with tarfile.open(download_file.name) as t:",
      "246:                 t.extractall(models_dir)",
      "",
      "[Added Lines]",
      "244:             onnx.utils._extract_model_safe(download_file.name, models_dir)",
      "",
      "---------------"
    ],
    "onnx/hub.py||onnx/hub.py": [
      "File: onnx/hub.py -> onnx/hub.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "11: import json",
      "12: import os",
      "13: import sys",
      "15: from io import BytesIO",
      "16: from os.path import join",
      "17: from typing import IO, Any, Dict, List, cast",
      "",
      "[Removed Lines]",
      "14: import tarfile",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "290:     return onnx.load(cast(IO[bytes], BytesIO(model_bytes)))",
      "322: def download_model_with_test_data(",
      "323:     model: str,",
      "324:     repo: str = \"onnx/models:main\",",
      "",
      "[Removed Lines]",
      "293: def _tar_members_filter(tar: tarfile.TarFile, base: str) -> list[tarfile.TarInfo]:",
      "294:     \"\"\"Check that the content of ``tar`` will be extracted safely",
      "296:     Args:",
      "297:         tar: The tarball file",
      "298:         base: The directory where the tarball will be extracted",
      "300:     Returns:",
      "301:         list of tarball members",
      "302:     \"\"\"",
      "303:     result = []",
      "304:     for member in tar:",
      "305:         member_path = os.path.join(base, member.name)",
      "306:         abs_base = os.path.abspath(base)",
      "307:         abs_member = os.path.abspath(member_path)",
      "308:         if not abs_member.startswith(abs_base):",
      "309:             raise RuntimeError(",
      "310:                 f\"The tarball member {member_path} in downloading model contains \"",
      "311:                 f\"directory traversal sequence which may contain harmful payload.\"",
      "312:             )",
      "313:         elif member.issym() or member.islnk():",
      "314:             raise RuntimeError(",
      "315:                 f\"The tarball member {member_path} in downloading model contains \"",
      "316:                 f\"symbolic links which may contain harmful payload.\"",
      "317:             )",
      "318:         result.append(member)",
      "319:     return result",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "393:                 \"download the model from the model hub.\"",
      "394:             )",
      "413:     model_with_data_path = (",
      "414:         local_model_with_data_dir_path",
      "415:         + \"/\"",
      "",
      "[Removed Lines]",
      "396:     with tarfile.open(local_model_with_data_path) as model_with_data_zipped:",
      "397:         # FIXME: Avoid index manipulation with magic numbers",
      "398:         local_model_with_data_dir_path = local_model_with_data_path[",
      "399:             0 : len(local_model_with_data_path) - 7",
      "400:         ]",
      "401:         # Mitigate tarball directory traversal risks",
      "402:         if hasattr(tarfile, \"data_filter\"):",
      "403:             model_with_data_zipped.extractall(",
      "404:                 path=local_model_with_data_dir_path, filter=\"data\"",
      "405:             )",
      "406:         else:",
      "407:             model_with_data_zipped.extractall(",
      "408:                 path=local_model_with_data_dir_path,",
      "409:                 members=_tar_members_filter(",
      "410:                     model_with_data_zipped, local_model_with_data_dir_path",
      "411:                 ),",
      "412:             )",
      "",
      "[Added Lines]",
      "366:     # FIXME: Avoid index manipulation with magic numbers,",
      "367:     # remove \".tar.gz\"",
      "368:     local_model_with_data_dir_path = local_model_with_data_path[",
      "369:         0 : len(local_model_with_data_path) - 7",
      "370:     ]",
      "371:     onnx.utils._extract_model_safe(",
      "372:         local_model_with_data_path, local_model_with_data_dir_path",
      "373:     )",
      "",
      "---------------"
    ],
    "onnx/utils.py||onnx/utils.py": [
      "File: onnx/utils.py -> onnx/utils.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "4: from __future__ import annotations",
      "6: import os",
      "8: import onnx.checker",
      "9: import onnx.helper",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "7: import tarfile",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "232:     onnx.save(extracted, output_path)",
      "233:     if check_model:",
      "234:         onnx.checker.check_model(output_path)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "238: def _tar_members_filter(",
      "239:     tar: tarfile.TarFile, base: str | os.PathLike",
      "240: ) -> list[tarfile.TarInfo]:",
      "241:     \"\"\"Check that the content of ``tar`` will be extracted safely",
      "243:     Args:",
      "244:         tar: The tarball file",
      "245:         base: The directory where the tarball will be extracted",
      "247:     Returns:",
      "248:         list of tarball members",
      "249:     \"\"\"",
      "250:     result = []",
      "251:     for member in tar:",
      "252:         member_path = os.path.join(base, member.name)",
      "253:         abs_base = os.path.abspath(base)",
      "254:         abs_member = os.path.abspath(member_path)",
      "255:         if not abs_member.startswith(abs_base):",
      "256:             raise RuntimeError(",
      "257:                 f\"The tarball member {member_path} in downloading model contains \"",
      "258:                 f\"directory traversal sequence which may contain harmful payload.\"",
      "259:             )",
      "260:         elif member.issym() or member.islnk():",
      "261:             raise RuntimeError(",
      "262:                 f\"The tarball member {member_path} in downloading model contains \"",
      "263:                 f\"symbolic links which may contain harmful payload.\"",
      "264:             )",
      "265:         result.append(member)",
      "266:     return result",
      "269: def _extract_model_safe(",
      "270:     model_tar_path: str | os.PathLike, local_model_with_data_dir_path: str | os.PathLike",
      "271: ) -> None:",
      "272:     \"\"\"Safely extracts a tar file to a specified directory.",
      "274:     This function ensures that the extraction process mitigates against",
      "275:     directory traversal vulnerabilities by validating or sanitizing paths",
      "276:     within the tar file. It also provides compatibility for different versions",
      "277:     of the tarfile module by checking for the availability of certain attributes",
      "278:     or methods before invoking them.",
      "280:     Args:",
      "281:         model_tar_path: The path to the tar file to be extracted.",
      "282:         local_model_with_data_dir_path: The directory path where the tar file",
      "283:       contents will be extracted to.",
      "284:     \"\"\"",
      "285:     with tarfile.open(model_tar_path) as model_with_data_zipped:",
      "286:         # Mitigate tarball directory traversal risks",
      "287:         if hasattr(tarfile, \"data_filter\"):",
      "288:             model_with_data_zipped.extractall(",
      "289:                 path=local_model_with_data_dir_path, filter=\"data\"",
      "290:             )",
      "291:         else:",
      "292:             model_with_data_zipped.extractall(",
      "293:                 path=local_model_with_data_dir_path,",
      "294:                 members=_tar_members_filter(",
      "295:                     model_with_data_zipped, local_model_with_data_dir_path",
      "296:                 ),",
      "297:             )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "1faaf7dcf7f918c2d4aa68bcc424842981a93227",
      "candidate_info": {
        "commit_hash": "1faaf7dcf7f918c2d4aa68bcc424842981a93227",
        "repo": "onnx/onnx",
        "commit_url": "https://github.com/onnx/onnx/commit/1faaf7dcf7f918c2d4aa68bcc424842981a93227",
        "files": [
          "onnx/backend/test/runner/__init__.py",
          "onnx/hub.py",
          "onnx/utils.py"
        ],
        "message": "Refactor safe extract method to fix issue 6215 (#6222)\n\n---------\n\nSigned-off-by: liqunfu <liqfu@microsoft.com>\nSigned-off-by: liqun Fu <liqfu@microsoft.com>\nCo-authored-by: G. Ramalingam <grama@microsoft.com>\nCo-authored-by: Justin Chu <justinchuby@users.noreply.github.com>\n(cherry picked from commit 1b70f9b673259360b6a2339c4bd97db9ea6e552f)",
        "before_after_code_files": [
          "onnx/backend/test/runner/__init__.py||onnx/backend/test/runner/__init__.py",
          "onnx/hub.py||onnx/hub.py",
          "onnx/utils.py||onnx/utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "diff_branch_cherry_pick": 1,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "onnx/backend/test/runner/__init__.py||onnx/backend/test/runner/__init__.py",
            "onnx/hub.py||onnx/hub.py",
            "onnx/utils.py||onnx/utils.py"
          ],
          "candidate": [
            "onnx/backend/test/runner/__init__.py||onnx/backend/test/runner/__init__.py",
            "onnx/hub.py||onnx/hub.py",
            "onnx/utils.py||onnx/utils.py"
          ]
        }
      },
      "candidate_diff": {
        "onnx/backend/test/runner/__init__.py||onnx/backend/test/runner/__init__.py": [
          "File: onnx/backend/test/runner/__init__.py -> onnx/backend/test/runner/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "10: import re",
          "11: import shutil",
          "12: import sys",
          "14: import tempfile",
          "15: import time",
          "16: import unittest",
          "",
          "[Removed Lines]",
          "13: import tarfile",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "238:             )",
          "239:             urlretrieve(model_test.url, download_file.name)",
          "240:             print(\"Done\")",
          "243:         except Exception as e:",
          "244:             print(f\"Failed to prepare data for model {model_test.model_name}: {e}\")",
          "245:             raise",
          "",
          "[Removed Lines]",
          "241:             with tarfile.open(download_file.name) as t:",
          "242:                 t.extractall(models_dir)",
          "",
          "[Added Lines]",
          "240:             onnx.utils._extract_model_safe(download_file.name, models_dir)",
          "",
          "---------------"
        ],
        "onnx/hub.py||onnx/hub.py": [
          "File: onnx/hub.py -> onnx/hub.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "9: import json",
          "10: import os",
          "11: import sys",
          "13: from io import BytesIO",
          "14: from os.path import join",
          "15: from typing import IO, Any, Dict, List, Optional, Set, Tuple, cast",
          "",
          "[Removed Lines]",
          "12: import tarfile",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "288:     return onnx.load(cast(IO[bytes], BytesIO(model_bytes)))",
          "320: def download_model_with_test_data(",
          "321:     model: str,",
          "322:     repo: str = \"onnx/models:main\",",
          "",
          "[Removed Lines]",
          "291: def _tar_members_filter(tar: tarfile.TarFile, base: str) -> list[tarfile.TarInfo]:",
          "292:     \"\"\"Check that the content of ``tar`` will be extracted safely",
          "294:     Args:",
          "295:         tar: The tarball file",
          "296:         base: The directory where the tarball will be extracted",
          "298:     Returns:",
          "299:         list of tarball members",
          "300:     \"\"\"",
          "301:     result = []",
          "302:     for member in tar:",
          "303:         member_path = os.path.join(base, member.name)",
          "304:         abs_base = os.path.abspath(base)",
          "305:         abs_member = os.path.abspath(member_path)",
          "306:         if not abs_member.startswith(abs_base):",
          "307:             raise RuntimeError(",
          "308:                 f\"The tarball member {member_path} in downloading model contains \"",
          "309:                 f\"directory traversal sequence which may contain harmful payload.\"",
          "310:             )",
          "311:         elif member.issym() or member.islnk():",
          "312:             raise RuntimeError(",
          "313:                 f\"The tarball member {member_path} in downloading model contains \"",
          "314:                 f\"symbolic links which may contain harmful payload.\"",
          "315:             )",
          "316:         result.append(member)",
          "317:     return result",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "391:                 \"download the model from the model hub.\"",
          "392:             )",
          "411:     model_with_data_path = (",
          "412:         local_model_with_data_dir_path",
          "413:         + \"/\"",
          "",
          "[Removed Lines]",
          "394:     with tarfile.open(local_model_with_data_path) as model_with_data_zipped:",
          "395:         # FIXME: Avoid index manipulation with magic numbers",
          "396:         local_model_with_data_dir_path = local_model_with_data_path[",
          "397:             0 : len(local_model_with_data_path) - 7",
          "398:         ]",
          "399:         # Mitigate tarball directory traversal risks",
          "400:         if hasattr(tarfile, \"data_filter\"):",
          "401:             model_with_data_zipped.extractall(",
          "402:                 path=local_model_with_data_dir_path, filter=\"data\"",
          "403:             )",
          "404:         else:",
          "405:             model_with_data_zipped.extractall(",
          "406:                 path=local_model_with_data_dir_path,",
          "407:                 members=_tar_members_filter(",
          "408:                     model_with_data_zipped, local_model_with_data_dir_path",
          "409:                 ),",
          "410:             )",
          "",
          "[Added Lines]",
          "364:     # FIXME: Avoid index manipulation with magic numbers,",
          "365:     # remove \".tar.gz\"",
          "366:     local_model_with_data_dir_path = local_model_with_data_path[",
          "367:         0 : len(local_model_with_data_path) - 7",
          "368:     ]",
          "369:     onnx.utils._extract_model_safe(",
          "370:         local_model_with_data_path, local_model_with_data_dir_path",
          "371:     )",
          "",
          "---------------"
        ],
        "onnx/utils.py||onnx/utils.py": [
          "File: onnx/utils.py -> onnx/utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "4: from __future__ import annotations",
          "6: import os",
          "8: import onnx.checker",
          "9: import onnx.helper",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "7: import tarfile",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "212:     onnx.save(extracted, output_path)",
          "213:     if check_model:",
          "214:         onnx.checker.check_model(output_path)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "218: def _tar_members_filter(",
          "219:     tar: tarfile.TarFile, base: str | os.PathLike",
          "220: ) -> list[tarfile.TarInfo]:",
          "221:     \"\"\"Check that the content of ``tar`` will be extracted safely",
          "223:     Args:",
          "224:         tar: The tarball file",
          "225:         base: The directory where the tarball will be extracted",
          "227:     Returns:",
          "228:         list of tarball members",
          "229:     \"\"\"",
          "230:     result = []",
          "231:     for member in tar:",
          "232:         member_path = os.path.join(base, member.name)",
          "233:         abs_base = os.path.abspath(base)",
          "234:         abs_member = os.path.abspath(member_path)",
          "235:         if not abs_member.startswith(abs_base):",
          "236:             raise RuntimeError(",
          "237:                 f\"The tarball member {member_path} in downloading model contains \"",
          "238:                 f\"directory traversal sequence which may contain harmful payload.\"",
          "239:             )",
          "240:         elif member.issym() or member.islnk():",
          "241:             raise RuntimeError(",
          "242:                 f\"The tarball member {member_path} in downloading model contains \"",
          "243:                 f\"symbolic links which may contain harmful payload.\"",
          "244:             )",
          "245:         result.append(member)",
          "246:     return result",
          "249: def _extract_model_safe(",
          "250:     model_tar_path: str | os.PathLike, local_model_with_data_dir_path: str | os.PathLike",
          "251: ) -> None:",
          "252:     \"\"\"Safely extracts a tar file to a specified directory.",
          "254:     This function ensures that the extraction process mitigates against",
          "255:     directory traversal vulnerabilities by validating or sanitizing paths",
          "256:     within the tar file. It also provides compatibility for different versions",
          "257:     of the tarfile module by checking for the availability of certain attributes",
          "258:     or methods before invoking them.",
          "260:     Args:",
          "261:         model_tar_path: The path to the tar file to be extracted.",
          "262:         local_model_with_data_dir_path: The directory path where the tar file",
          "263:       contents will be extracted to.",
          "264:     \"\"\"",
          "265:     with tarfile.open(model_tar_path) as model_with_data_zipped:",
          "266:         # Mitigate tarball directory traversal risks",
          "267:         if hasattr(tarfile, \"data_filter\"):",
          "268:             model_with_data_zipped.extractall(",
          "269:                 path=local_model_with_data_dir_path, filter=\"data\"",
          "270:             )",
          "271:         else:",
          "272:             model_with_data_zipped.extractall(",
          "273:                 path=local_model_with_data_dir_path,",
          "274:                 members=_tar_members_filter(",
          "275:                     model_with_data_zipped, local_model_with_data_dir_path",
          "276:                 ),",
          "277:             )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f186d8e382b1d26a39a9b88d402c86bdfa12331b",
      "candidate_info": {
        "commit_hash": "f186d8e382b1d26a39a9b88d402c86bdfa12331b",
        "repo": "onnx/onnx",
        "commit_url": "https://github.com/onnx/onnx/commit/f186d8e382b1d26a39a9b88d402c86bdfa12331b",
        "files": [
          "onnx/backend/test/runner/__init__.py",
          "onnx/hub.py",
          "onnx/utils.py"
        ],
        "message": "Refactor safe extract method to fix issue 6215 (#6222)\n\n---------\n\nSigned-off-by: liqunfu <liqfu@microsoft.com>\nSigned-off-by: liqun Fu <liqfu@microsoft.com>\nCo-authored-by: G. Ramalingam <grama@microsoft.com>\nCo-authored-by: Justin Chu <justinchuby@users.noreply.github.com>\n(cherry picked from commit 1b70f9b673259360b6a2339c4bd97db9ea6e552f)\nSigned-off-by: Andreas Fehlner <fehlner@arcor.de>",
        "before_after_code_files": [
          "onnx/backend/test/runner/__init__.py||onnx/backend/test/runner/__init__.py",
          "onnx/hub.py||onnx/hub.py",
          "onnx/utils.py||onnx/utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "diff_branch_cherry_pick": 1,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "onnx/backend/test/runner/__init__.py||onnx/backend/test/runner/__init__.py",
            "onnx/hub.py||onnx/hub.py",
            "onnx/utils.py||onnx/utils.py"
          ],
          "candidate": [
            "onnx/backend/test/runner/__init__.py||onnx/backend/test/runner/__init__.py",
            "onnx/hub.py||onnx/hub.py",
            "onnx/utils.py||onnx/utils.py"
          ]
        }
      },
      "candidate_diff": {
        "onnx/backend/test/runner/__init__.py||onnx/backend/test/runner/__init__.py": [
          "File: onnx/backend/test/runner/__init__.py -> onnx/backend/test/runner/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "10: import re",
          "11: import shutil",
          "12: import sys",
          "14: import tempfile",
          "15: import time",
          "16: import unittest",
          "",
          "[Removed Lines]",
          "13: import tarfile",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "238:             )",
          "239:             urlretrieve(model_test.url, download_file.name)",
          "240:             print(\"Done\")",
          "243:         except Exception as e:",
          "244:             print(f\"Failed to prepare data for model {model_test.model_name}: {e}\")",
          "245:             raise",
          "",
          "[Removed Lines]",
          "241:             with tarfile.open(download_file.name) as t:",
          "242:                 t.extractall(models_dir)",
          "",
          "[Added Lines]",
          "240:             onnx.utils._extract_model_safe(download_file.name, models_dir)",
          "",
          "---------------"
        ],
        "onnx/hub.py||onnx/hub.py": [
          "File: onnx/hub.py -> onnx/hub.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "9: import json",
          "10: import os",
          "11: import sys",
          "13: from io import BytesIO",
          "14: from os.path import join",
          "15: from typing import IO, Any, Dict, List, Optional, Set, Tuple, cast",
          "",
          "[Removed Lines]",
          "12: import tarfile",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "288:     return onnx.load(cast(IO[bytes], BytesIO(model_bytes)))",
          "320: def download_model_with_test_data(",
          "321:     model: str,",
          "322:     repo: str = \"onnx/models:main\",",
          "",
          "[Removed Lines]",
          "291: def _tar_members_filter(tar: tarfile.TarFile, base: str) -> list[tarfile.TarInfo]:",
          "292:     \"\"\"Check that the content of ``tar`` will be extracted safely",
          "294:     Args:",
          "295:         tar: The tarball file",
          "296:         base: The directory where the tarball will be extracted",
          "298:     Returns:",
          "299:         list of tarball members",
          "300:     \"\"\"",
          "301:     result = []",
          "302:     for member in tar:",
          "303:         member_path = os.path.join(base, member.name)",
          "304:         abs_base = os.path.abspath(base)",
          "305:         abs_member = os.path.abspath(member_path)",
          "306:         if not abs_member.startswith(abs_base):",
          "307:             raise RuntimeError(",
          "308:                 f\"The tarball member {member_path} in downloading model contains \"",
          "309:                 f\"directory traversal sequence which may contain harmful payload.\"",
          "310:             )",
          "311:         elif member.issym() or member.islnk():",
          "312:             raise RuntimeError(",
          "313:                 f\"The tarball member {member_path} in downloading model contains \"",
          "314:                 f\"symbolic links which may contain harmful payload.\"",
          "315:             )",
          "316:         result.append(member)",
          "317:     return result",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "391:                 \"download the model from the model hub.\"",
          "392:             )",
          "411:     model_with_data_path = (",
          "412:         local_model_with_data_dir_path",
          "413:         + \"/\"",
          "",
          "[Removed Lines]",
          "394:     with tarfile.open(local_model_with_data_path) as model_with_data_zipped:",
          "395:         # FIXME: Avoid index manipulation with magic numbers",
          "396:         local_model_with_data_dir_path = local_model_with_data_path[",
          "397:             0 : len(local_model_with_data_path) - 7",
          "398:         ]",
          "399:         # Mitigate tarball directory traversal risks",
          "400:         if hasattr(tarfile, \"data_filter\"):",
          "401:             model_with_data_zipped.extractall(",
          "402:                 path=local_model_with_data_dir_path, filter=\"data\"",
          "403:             )",
          "404:         else:",
          "405:             model_with_data_zipped.extractall(",
          "406:                 path=local_model_with_data_dir_path,",
          "407:                 members=_tar_members_filter(",
          "408:                     model_with_data_zipped, local_model_with_data_dir_path",
          "409:                 ),",
          "410:             )",
          "",
          "[Added Lines]",
          "364:     # FIXME: Avoid index manipulation with magic numbers,",
          "365:     # remove \".tar.gz\"",
          "366:     local_model_with_data_dir_path = local_model_with_data_path[",
          "367:         0 : len(local_model_with_data_path) - 7",
          "368:     ]",
          "369:     onnx.utils._extract_model_safe(",
          "370:         local_model_with_data_path, local_model_with_data_dir_path",
          "371:     )",
          "",
          "---------------"
        ],
        "onnx/utils.py||onnx/utils.py": [
          "File: onnx/utils.py -> onnx/utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "4: from __future__ import annotations",
          "6: import os",
          "8: import onnx.checker",
          "9: import onnx.helper",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "7: import tarfile",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "212:     onnx.save(extracted, output_path)",
          "213:     if check_model:",
          "214:         onnx.checker.check_model(output_path)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "218: def _tar_members_filter(",
          "219:     tar: tarfile.TarFile, base: str | os.PathLike",
          "220: ) -> list[tarfile.TarInfo]:",
          "221:     \"\"\"Check that the content of ``tar`` will be extracted safely",
          "223:     Args:",
          "224:         tar: The tarball file",
          "225:         base: The directory where the tarball will be extracted",
          "227:     Returns:",
          "228:         list of tarball members",
          "229:     \"\"\"",
          "230:     result = []",
          "231:     for member in tar:",
          "232:         member_path = os.path.join(base, member.name)",
          "233:         abs_base = os.path.abspath(base)",
          "234:         abs_member = os.path.abspath(member_path)",
          "235:         if not abs_member.startswith(abs_base):",
          "236:             raise RuntimeError(",
          "237:                 f\"The tarball member {member_path} in downloading model contains \"",
          "238:                 f\"directory traversal sequence which may contain harmful payload.\"",
          "239:             )",
          "240:         elif member.issym() or member.islnk():",
          "241:             raise RuntimeError(",
          "242:                 f\"The tarball member {member_path} in downloading model contains \"",
          "243:                 f\"symbolic links which may contain harmful payload.\"",
          "244:             )",
          "245:         result.append(member)",
          "246:     return result",
          "249: def _extract_model_safe(",
          "250:     model_tar_path: str | os.PathLike, local_model_with_data_dir_path: str | os.PathLike",
          "251: ) -> None:",
          "252:     \"\"\"Safely extracts a tar file to a specified directory.",
          "254:     This function ensures that the extraction process mitigates against",
          "255:     directory traversal vulnerabilities by validating or sanitizing paths",
          "256:     within the tar file. It also provides compatibility for different versions",
          "257:     of the tarfile module by checking for the availability of certain attributes",
          "258:     or methods before invoking them.",
          "260:     Args:",
          "261:         model_tar_path: The path to the tar file to be extracted.",
          "262:         local_model_with_data_dir_path: The directory path where the tar file",
          "263:       contents will be extracted to.",
          "264:     \"\"\"",
          "265:     with tarfile.open(model_tar_path) as model_with_data_zipped:",
          "266:         # Mitigate tarball directory traversal risks",
          "267:         if hasattr(tarfile, \"data_filter\"):",
          "268:             model_with_data_zipped.extractall(",
          "269:                 path=local_model_with_data_dir_path, filter=\"data\"",
          "270:             )",
          "271:         else:",
          "272:             model_with_data_zipped.extractall(",
          "273:                 path=local_model_with_data_dir_path,",
          "274:                 members=_tar_members_filter(",
          "275:                     model_with_data_zipped, local_model_with_data_dir_path",
          "276:                 ),",
          "277:             )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1e6e0db7dd03e35e6227fa9955a469866d3efdfe",
      "candidate_info": {
        "commit_hash": "1e6e0db7dd03e35e6227fa9955a469866d3efdfe",
        "repo": "onnx/onnx",
        "commit_url": "https://github.com/onnx/onnx/commit/1e6e0db7dd03e35e6227fa9955a469866d3efdfe",
        "files": [
          "onnx/reference/ops/op_resize.py",
          "requirements-lintrunner.txt",
          "tools/protoc-gen-mypy.py"
        ],
        "message": "Bump ruff from 0.0.280 to 0.0.283 (#5483)\n\nBumps [ruff](https://github.com/astral-sh/ruff) from 0.0.280 to 0.0.283.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a\nhref=\"https://github.com/astral-sh/ruff/releases\">ruff's\nreleases</a>.</em></p>\n<blockquote>\n<h2>v0.0.283</h2>\n<!-- raw HTML omitted -->\n<h2>What's Changed</h2>\n<h3>Breaking Changes</h3>\n<ul>\n<li>Assume Python 3.8 instead of 3.10 for target version by <a\nhref=\"https://github.com/zanieb\"><code>@\u200bzanieb</code></a> in <a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/6397\">astral-sh/ruff#6397</a></li>\n</ul>\n<h3>Rules</h3>\n<ul>\n<li>[<code>flake8-pyi</code>] <code>PYI019</code>: Detects if a type\nvariable is used instead of <code>Self</code> in return annotations by\n<a href=\"https://github.com/qdegraaf\"><code>@\u200bqdegraaf</code></a> in <a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/6204\">astral-sh/ruff#6204</a></li>\n<li>[<code>flake8-pyi</code>] <code>PYI051</code>: Detects unions of\n<code>Literal</code> types by <a\nhref=\"https://github.com/LaBatata101\"><code>@\u200bLaBatata101</code></a> in\n<a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/6215\">astral-sh/ruff#6215</a></li>\n<li>[<code>flake8-pyi</code>] <code>PYI055</code>: Detects unions of\n<code>type</code>s by <a\nhref=\"https://github.com/LaBatata101\"><code>@\u200bLaBatata101</code></a> in\n<a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/6316\">astral-sh/ruff#6316</a></li>\n<li>[<code>pylint</code>] <code>E1300</code>: Detects invalid string\nformat characters by <a\nhref=\"https://github.com/silvanocerza\"><code>@\u200bsilvanocerza</code></a>\nin <a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/6171\">astral-sh/ruff#6171</a></li>\n<li>[<code>pyupgrade</code>] <code>UP040</code>: Upgrades type alias\nannotations to use PEP-695 syntax by <a\nhref=\"https://github.com/zanieb\"><code>@\u200bzanieb</code></a> in <a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/6289\">astral-sh/ruff#6289</a></li>\n</ul>\n<h3>Rule Changes</h3>\n<ul>\n<li>[<code>flake8-boolean-trap</code>] <code>FBT003</code>: Add\n<code>is_</code> and <code>is_not</code> to excluded functions by <a\nhref=\"https://github.com/zanieb\"><code>@\u200bzanieb</code></a> in <a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/6307\">astral-sh/ruff#6307</a></li>\n<li>[<code>flake8-logging-format</code>] Allow capitalized names for\nlogger candidate heuristic match by <a\nhref=\"https://github.com/charliermarsh\"><code>@\u200bcharliermarsh</code></a>\nin <a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/6356\">astral-sh/ruff#6356</a></li>\n<li>[<code>flake8-pyi</code>] Applicable rules are now checked non-stub\ncode by <a href=\"https://github.com/andersk\"><code>@\u200bandersk</code></a>\nin <a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/6297\">astral-sh/ruff#6297</a>\n<ul>\n<li><code>PYI013</code>: <a\nhref=\"https://beta.ruff.rs/docs/rules/ellipsis-in-non-empty-class-body\"><code>ellipsis-in-non-empty-class-body</code></a></li>\n<li><code>PYI016</code>: <a\nhref=\"https://beta.ruff.rs/docs/rules/duplicate-union-member\"><code>duplicate-union-member</code></a></li>\n<li><code>PYI018</code>: <a\nhref=\"https://beta.ruff.rs/docs/rules/unused-private-type-var\"><code>unused-private-type-var</code></a></li>\n<li><code>PYI019</code>: <a\nhref=\"https://beta.ruff.rs/docs/rules/custom-type-var-return-type\"><code>custom-type-var-return-type</code></a></li>\n<li><code>PYI024</code>: <a\nhref=\"https://beta.ruff.rs/docs/rules/collections-named-tuple\"><code>collections-named-tuple</code></a></li>\n<li><code>PYI025</code>: <a\nhref=\"https://beta.ruff.rs/docs/rules/unaliased-collections-abc-set-import\"><code>unaliased-collections-abc-set-import</code></a></li>\n<li><code>PYI030</code>: <a\nhref=\"https://beta.ruff.rs/docs/rules/unnecessary-literal-union\"><code>unnecessary-literal-union</code></a></li>\n<li><code>PYI032</code>: <a\nhref=\"https://beta.ruff.rs/docs/rules/any-eq-ne-annotation\"><code>any-eq-ne-annotation</code></a></li>\n<li><code>PYI034</code>: <a\nhref=\"https://beta.ruff.rs/docs/rules/non-self-return-type\"><code>non-self-return-type</code></a></li>\n<li><code>PYI036</code>: <a\nhref=\"https://beta.ruff.rs/docs/rules/bad-exit-annotation\"><code>bad-exit-annotation</code></a></li>\n<li><code>PYI041</code>: <a\nhref=\"https://beta.ruff.rs/docs/rules/redundant-numeric-union\"><code>redundant-numeric-union</code></a></li>\n<li><code>PYI042</code>: <a\nhref=\"https://beta.ruff.rs/docs/rules/snake-case-type-alias\"><code>snake-case-type-alias</code></a></li>\n<li><code>PYI043</code>: <a\nhref=\"https://beta.ruff.rs/docs/rules/t-suffixed-type-alias\"><code>t-suffixed-type-alias</code></a></li>\n<li><code>PYI045</code>: <a\nhref=\"https://beta.ruff.rs/docs/rules/iter-method-return-iterable\"><code>iter-method-return-iterable</code></a></li>\n<li><code>PYI046</code>: <a\nhref=\"https://beta.ruff.rs/docs/rules/unused-private-protocol\"><code>unused-private-protocol</code></a></li>\n<li><code>PYI047</code>: <a\nhref=\"https://beta.ruff.rs/docs/rules/unused-private-type-alias\"><code>unused-private-type-alias</code></a></li>\n<li><code>PYI049</code>: <a\nhref=\"https://beta.ruff.rs/docs/rules/unused-private-typed-dict\"><code>unused-private-typed-dict</code></a></li>\n<li><code>PYI050</code>: <a\nhref=\"https://beta.ruff.rs/docs/rules/no-return-argument-annotation-in-stub\"><code>no-return-argument-annotation-in-stub</code></a>\n(Python \u2265 3.11)</li>\n<li><code>PYI051</code>: <a\nhref=\"https://beta.ruff.rs/docs/rules/redundant-literal-union\"><code>redundant-literal-union</code></a></li>\n<li><code>PYI056</code>: <a\nhref=\"https://beta.ruff.rs/docs/rules/unsupported-method-call-on-all\"><code>unsupported-method-call-on-all</code></a></li>\n</ul>\n</li>\n<li>[<code>flake8-pyi</code>] <code>PYI027</code> is being replaced by\n<code>PYI022</code> / <code>UP035</code> by <a\nhref=\"https://github.com/LaBatata101\"><code>@\u200bLaBatata101</code></a> in\n<a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/6354\">astral-sh/ruff#6354</a></li>\n<li>[<code>pydocstyle</code>] <code>D103</code>: Don't require\ndocstrings in <code>.pyi</code> files by <a\nhref=\"https://github.com/charliermarsh\"><code>@\u200bcharliermarsh</code></a>\nin <a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/6239\">astral-sh/ruff#6239</a></li>\n<li>[<code>pydocstyle</code>] <code>D203</code>: Ignore same-line\ndocstrings for lines-before and lines-after rules by <a\nhref=\"https://github.com/charliermarsh\"><code>@\u200bcharliermarsh</code></a>\nin <a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/6344\">astral-sh/ruff#6344</a></li>\n<li>[<code>pylint</code>] <code>PLE0605</code>: Allow generic tuple and\nlist calls in <code>__all__</code> by <a\nhref=\"https://github.com/charliermarsh\"><code>@\u200bcharliermarsh</code></a>\nin <a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/6247\">astral-sh/ruff#6247</a></li>\n<li>[<code>pylint</code>] <code>PLR0124</code>: Add detection of\ncomparisons with built-in calls by <a\nhref=\"https://github.com/charliermarsh\"><code>@\u200bcharliermarsh</code></a>\nin <a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/6324\">astral-sh/ruff#6324</a></li>\n<li>[<code>pyupgrade</code>] <code>UP032</code>: Add support for\n<code>await</code> expressions in f-strings by <a\nhref=\"https://github.com/harupy\"><code>@\u200bharupy</code></a> in <a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/6304\">astral-sh/ruff#6304</a></li>\n<li>[<code>pyupgrade</code>] <code>UP032</code>: Add support for\nimplicitly concatenated strings by <a\nhref=\"https://github.com/harupy\"><code>@\u200bharupy</code></a> in <a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/6263\">astral-sh/ruff#6263</a></li>\n<li>[<code>pyupgrade</code>] <code>UP032</code>: Add support for\nrepeated format fields by <a\nhref=\"https://github.com/harupy\"><code>@\u200bharupy</code></a> in <a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/6266\">astral-sh/ruff#6266</a></li>\n<li>[<code>ruff</code>] <code>RUF012</code>: Permit\n<code>ClassVar</code> and <code>Final</code> without subscript by <a\nhref=\"https://github.com/bluetech\"><code>@\u200bbluetech</code></a> in <a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/6273\">astral-sh/ruff#6273</a></li>\n</ul>\n<h3>Bug Fixes</h3>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Changelog</summary>\n<p><em>Sourced from <a\nhref=\"https://github.com/astral-sh/ruff/blob/main/BREAKING_CHANGES.md\">ruff's\nchangelog</a>.</em></p>\n<blockquote>\n<h2>0.0.283</h2>\n<h3>The target Python version now defaults to 3.8 instead of 3.10 (<a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/6397\">#6397</a>)</h3>\n<p>Previously, when a target Python version was not specified, Ruff\nwould use a default of Python 3.10. However, it is safer to default to\nan <em>older</em> Python version to avoid assuming the availability of\nnew features. We now default to the oldest supported Python version\nwhich is currently Python 3.8.</p>\n<p>(We still support Python 3.7 but since <a\nhref=\"https://devguide.python.org/versions/#unsupported-versions\">it has\nreached EOL</a> we've decided not to make it the default here.)</p>\n<h2>0.0.277</h2>\n<h3><code>.ipynb_checkpoints</code>, <code>.pyenv</code>,\n<code>.pytest_cache</code>, and <code>.vscode</code> are now excluded by\ndefault (<a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/5513\">#5513</a>)</h3>\n<p>Ruff maintains a list of default exclusions, which now consists of\nthe following patterns:</p>\n<ul>\n<li><code>.bzr</code></li>\n<li><code>.direnv</code></li>\n<li><code>.eggs</code></li>\n<li><code>.git</code></li>\n<li><code>.git-rewrite</code></li>\n<li><code>.hg</code></li>\n<li><code>.ipynb_checkpoints</code></li>\n<li><code>.mypy_cache</code></li>\n<li><code>.nox</code></li>\n<li><code>.pants.d</code></li>\n<li><code>.pyenv</code></li>\n<li><code>.pytest_cache</code></li>\n<li><code>.pytype</code></li>\n<li><code>.ruff_cache</code></li>\n<li><code>.svn</code></li>\n<li><code>.tox</code></li>\n<li><code>.venv</code></li>\n<li><code>.vscode</code></li>\n<li><code>__pypackages__</code></li>\n<li><code>_build</code></li>\n<li><code>buck-out</code></li>\n<li><code>build</code></li>\n<li><code>dist</code></li>\n<li><code>node_modules</code></li>\n<li><code>venv</code></li>\n</ul>\n<p>Previously, the <code>.ipynb_checkpoints</code>, <code>.pyenv</code>,\n<code>.pytest_cache</code>, and <code>.vscode</code> directories were\nnot\nexcluded by default. This change brings Ruff's default exclusions in\nline with other tools like\nBlack.</p>\n<h2>0.0.276</h2>\n<h3>The <code>keep-runtime-typing</code> setting has been reinstated (<a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/5470\">#5470</a>)</h3>\n<p>The <code>keep-runtime-typing</code> setting has been reinstated with\nrevised semantics. This setting was\nremoved in <a\nhref=\"https://redirect.github.com/astral-sh/ruff/pull/4427\">#4427</a>,\nas it was equivalent to ignoring</p>\n<!-- raw HTML omitted -->\n</blockquote>\n<p>... (truncated)</p>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a\nhref=\"https://github.com/astral-sh/ruff/commit/fe9590f39f4d41deb1d4ed7c655cff0bcb02bdc4\"><code>fe9590f</code></a>\nBump version number to 0.0.283 (<a\nhref=\"https://redirect.github.com/astral-sh/ruff/issues/6407\">#6407</a>)</li>\n<li><a\nhref=\"https://github.com/astral-sh/ruff/commit/e769c74899a744eb773b80da9f3a18dbc42e072b\"><code>e769c74</code></a>\nCheck .git in formatter progress checkouts for build (<a\nhref=\"https://redirect.github.com/astral-sh/ruff/issues/6387\">#6387</a>)</li>\n<li><a\nhref=\"https://github.com/astral-sh/ruff/commit/d815a25b11d8abe0151628fd84282ed48d823750\"><code>d815a25</code></a>\nUpdate <code>StmtMatch</code> formatting snapshots (<a\nhref=\"https://redirect.github.com/astral-sh/ruff/issues/6427\">#6427</a>)</li>\n<li><a\nhref=\"https://github.com/astral-sh/ruff/commit/001aa486df8566aaf74f7f601684321f9ef2d53b\"><code>001aa48</code></a>\nAdd formatting for <code>StmtMatch</code> (<a\nhref=\"https://redirect.github.com/astral-sh/ruff/issues/6286\">#6286</a>)</li>\n<li><a\nhref=\"https://github.com/astral-sh/ruff/commit/87984e9ac7387204c49703d9dcf50237b5a33d89\"><code>87984e9</code></a>\nExpand parents whenever open-parenthesis comments are present (<a\nhref=\"https://redirect.github.com/astral-sh/ruff/issues/6389\">#6389</a>)</li>\n<li><a\nhref=\"https://github.com/astral-sh/ruff/commit/6aefe71c565b402c8045ea74b09331be4f69bafb\"><code>6aefe71</code></a>\nFix name of rule in example of <code>extend-per-file-ignores</code> in\noptions.rs (<a\nhref=\"https://redirect.github.com/astral-sh/ruff/issues/6417\">#6417</a>)</li>\n<li><a\nhref=\"https://github.com/astral-sh/ruff/commit/90ba40c23c4a179637da81f5a38f50b2f4b4951c\"><code>90ba40c</code></a>\nFix zulip unstable formatting with end-of-line comments (<a\nhref=\"https://redirect.github.com/astral-sh/ruff/issues/6386\">#6386</a>)</li>\n<li><a\nhref=\"https://github.com/astral-sh/ruff/commit/2bd345358f14a21ecf52b5514d6b5dbe3e8cbca0\"><code>2bd3453</code></a>\nSimplify <code>parenthesized</code> formatting (<a\nhref=\"https://redirect.github.com/astral-sh/ruff/issues/6419\">#6419</a>)</li>\n<li><a\nhref=\"https://github.com/astral-sh/ruff/commit/289d1e85bfc11b1c8137cb9c88da9386d1cc6491\"><code>289d1e8</code></a>\nManually parenthesize tuple expr in <code>B014</code> autofix (<a\nhref=\"https://redirect.github.com/astral-sh/ruff/issues/6415\">#6415</a>)</li>\n<li><a\nhref=\"https://github.com/astral-sh/ruff/commit/6df5ab40987cba7bb5681c2445caaf83ebcfbf7d\"><code>6df5ab4</code></a>\nRemove duplicate line from project structure docs (<a\nhref=\"https://redirect.github.com/astral-sh/ruff/issues/6408\">#6408</a>)</li>\n<li>Additional commits viewable in <a\nhref=\"https://github.com/astral-sh/ruff/compare/v0.0.280...v0.0.283\">compare\nview</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility\nscore](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=ruff&package-manager=pip&previous-version=0.0.280&new-version=0.0.283)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)\n\nDependabot will resolve any conflicts with this PR as long as you don't\nalter it yourself. You can also trigger a rebase manually by commenting\n`@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits\nthat have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after\nyour CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge\nand block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating\nit. You can achieve the same result by closing it manually\n- `@dependabot show <dependency name> ignore conditions` will show all\nof the ignore conditions of the specified dependency\n- `@dependabot ignore this major version` will close this PR and stop\nDependabot creating any more for this major version (unless you reopen\nthe PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop\nDependabot creating any more for this minor version (unless you reopen\nthe PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop\nDependabot creating any more for this dependency (unless you reopen the\nPR or upgrade to it yourself)\n\n\n</details>\n\n---------\n\nSigned-off-by: dependabot[bot] <support@github.com>\nSigned-off-by: Justin Chu <justinchuby@users.noreply.github.com>\nCo-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>\nCo-authored-by: Justin Chu <justinchuby@users.noreply.github.com>",
        "before_after_code_files": [
          "onnx/reference/ops/op_resize.py||onnx/reference/ops/op_resize.py",
          "tools/protoc-gen-mypy.py||tools/protoc-gen-mypy.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_issue": 1,
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "onnx/reference/ops/op_resize.py||onnx/reference/ops/op_resize.py": [
          "File: onnx/reference/ops/op_resize.py -> onnx/reference/ops/op_resize.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: # Copyright (c) ONNX Project Contributors",
          "3: # SPDX-License-Identifier: Apache-2.0",
          "8: import numpy as np",
          "10: from onnx.reference.op_run import OpRun",
          "16:     \"\"\"",
          "17:     From https://stackoverflow.com/a/1235363",
          "18:     Generate a cartesian product of input arrays.",
          "",
          "[Removed Lines]",
          "4: # pylint: disable=C0123,C3001,R0912,R0913,R0914,R1730,W0221,W0613",
          "6: from typing import Any, Callable, List, Optional, Tuple",
          "13: def _cartesian(",
          "14:     arrays: List[np.ndarray], out: Optional[np.ndarray] = None",
          "15: ) -> np.ndarray:",
          "",
          "[Added Lines]",
          "4: from __future__ import annotations",
          "6: from typing import Any, Callable",
          "13: def _cartesian(arrays: list[np.ndarray], out: np.ndarray | None = None) -> np.ndarray:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "60:     return out",
          "65:         return np.array([0, 1])",
          "66:     if mode == \"round_prefer_floor\":",
          "67:         return np.array([ratio <= 0.5, ratio > 0.5])",
          "",
          "[Removed Lines]",
          "63: def _nearest_coeffs(ratio: float, mode: str = \"round_prefer_floor\") -> np.ndarray:",
          "64:     if type(ratio) == int or ratio.is_integer():",
          "",
          "[Added Lines]",
          "61: def _nearest_coeffs(",
          "62:     ratio: float | int | np.ndarray, mode: str = \"round_prefer_floor\"",
          "63: ) -> np.ndarray:",
          "64:     if isinstance(ratio, int) or ratio.is_integer():",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "77: def _cubic_coeffs(",
          "79: ) -> np.ndarray:",
          "81:     coeffs = [",
          "82:         ((A * (ratio + 1) - 5 * A) * (ratio + 1) + 8 * A) * (ratio + 1) - 4 * A,",
          "83:         ((A + 2) * ratio - (A + 3)) * ratio * ratio + 1,",
          "",
          "[Removed Lines]",
          "78:     ratio: float, scale: Optional[float] = None, A: float = -0.75",
          "80:     # scale is unused",
          "",
          "[Added Lines]",
          "78:     ratio: float, scale: float | None = None, A: float = -0.75",
          "80:     del scale  # Unused",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "92: def _cubic_coeffs_antialias(ratio: float, scale: float, A: float = -0.75) -> np.ndarray:",
          "96:     def compute_coeff(x: float) -> float:",
          "97:         x = abs(x)",
          "",
          "[Removed Lines]",
          "93:     if scale > 1.0:  # Antialias is applied when downsampling",
          "94:         scale = 1.0",
          "",
          "[Added Lines]",
          "93:     # Antialias is applied when downsampling",
          "94:     scale = min(scale, 1.0)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "110:     return np.array(coeffs) / sum(coeffs)",
          "115:     return np.array([1 - ratio, ratio])",
          "118: def _linear_coeffs_antialias(ratio: float, scale: float) -> np.ndarray:",
          "121:     start = int(np.floor(-1 / scale) + 1)",
          "122:     footprint = 2 - 2 * start",
          "123:     args = (np.arange(start, start + footprint) - ratio) * scale",
          "",
          "[Removed Lines]",
          "113: def _linear_coeffs(ratio: float, scale: Optional[float] = None) -> np.ndarray:",
          "114:     # scale is unused",
          "119:     if scale > 1.0:  # Antialias is applied when downsampling",
          "120:         scale = 1.0",
          "",
          "[Added Lines]",
          "113: def _linear_coeffs(ratio: float, scale: float | None = None) -> np.ndarray:",
          "114:     del scale  # unused",
          "119:     # Antialias is applied when downsampling",
          "120:     scale = min(scale, 1.0)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "151:     return np.array(idxes)",
          "155:     \"\"\"",
          "156:     Pad `data` in 'edge' mode, and get n nearest elements in the padded array",
          "157:     and their indexes in the original array.",
          "",
          "[Removed Lines]",
          "154: def _get_neighbor(x: float, n: int, data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:",
          "",
          "[Added Lines]",
          "155: def _get_neighbor(x: float, n: int, data: np.ndarray) -> tuple[np.ndarray, np.ndarray]:",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "172:     return idxes - pad_width, ret",
          "176:     data: np.ndarray,",
          "177:     scale_factor: float,",
          "178:     output_width_int: int,",
          "179:     x: float,",
          "180:     get_coeffs: Callable[[float, float], np.ndarray],",
          "182:     extrapolation_value: float = 0.0,",
          "183:     coordinate_transformation_mode: str = \"half_pixel\",",
          "184:     exclude_outside: bool = False,",
          "",
          "[Removed Lines]",
          "175: def _interpolate_1d_with_x(",
          "181:     roi: Optional[np.ndarray] = None,",
          "",
          "[Added Lines]",
          "176: def _interpolate_1d_with_x(  # pylint: disable=too-many-branches",
          "182:     roi: np.ndarray | None = None,",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "247: def _interpolate_nd_with_x(",
          "248:     data: np.ndarray,",
          "249:     n: int,",
          "253:     get_coeffs: Callable[[float, float], np.ndarray],",
          "255:     exclude_outside: bool = False,",
          "257: ) -> np.ndarray:",
          "",
          "[Removed Lines]",
          "250:     scale_factors: List[float],",
          "251:     output_size: List[int],",
          "252:     x: List[float],",
          "254:     roi: Optional[np.ndarray] = None,",
          "",
          "[Added Lines]",
          "251:     scale_factors: list[float],",
          "252:     output_size: list[int],",
          "253:     x: list[float],",
          "255:     roi: np.ndarray | None = None,",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "300:     )",
          "304:     data: np.ndarray,",
          "305:     get_coeffs: Callable[[float, float], np.ndarray],",
          "311:     exclude_outside: bool = False,",
          "313: ) -> np.ndarray:",
          "",
          "[Removed Lines]",
          "303: def _interpolate_nd(",
          "306:     output_size: Optional[List[int]] = None,",
          "307:     scale_factors: Optional[List[float]] = None,",
          "308:     axes: Optional[List[int]] = None,",
          "309:     roi: Optional[np.ndarray] = None,",
          "310:     keep_aspect_ratio_policy: Optional[str] = \"stretch\",",
          "",
          "[Added Lines]",
          "304: def _interpolate_nd(  # pylint: disable=too-many-branches",
          "307:     output_size: list[int] | None = None,",
          "308:     scale_factors: list[float] | None = None,",
          "309:     axes: list[int] | None = None,",
          "310:     roi: np.ndarray | None = None,",
          "311:     keep_aspect_ratio_policy: str | None = \"stretch\",",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "387: class Resize(OpRun):",
          "389:         self,",
          "390:         X,",
          "391:         roi,",
          "",
          "[Removed Lines]",
          "388:     def _run(  # type: ignore",
          "",
          "[Added Lines]",
          "389:     def _run(  # type: ignore  # pylint: disable=arguments-differ",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "398:         exclude_outside=None,",
          "399:         extrapolation_value=None,",
          "400:         keep_aspect_ratio_policy=None,",
          "402:         nearest_mode=None,",
          "403:     ):",
          "405:             if antialias:",
          "406:                 raise RuntimeError(",
          "407:                     f\"antilias={antialias!r} is not supported for mode={mode!r}.\"",
          "",
          "[Removed Lines]",
          "401:         mode=None,",
          "404:         if mode == \"nearest\":  # type: ignore",
          "",
          "[Added Lines]",
          "402:         mode: str | None = None,",
          "405:         if mode == \"nearest\":",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "409:             if nearest_mode is not None:",
          "411:                 def fct(x, scale_factor):",
          "412:                     return _nearest_coeffs(x, mode=nearest_mode)",
          "414:             else:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "413:                     del scale_factor  # unused",
          "",
          "---------------"
        ],
        "tools/protoc-gen-mypy.py||tools/protoc-gen-mypy.py": [
          "File: tools/protoc-gen-mypy.py -> tools/protoc-gen-mypy.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "36: d: Any = d_typed",
          "38: # Split the string so phabricator doesn't think this file is generated",
          "40: HEADER = (",
          "41:     f\"# {GENERATED} by protoc-gen-mypy.py.  Do not edit!\\n\"",
          "42:     \"# mypy: disable-error-code=override\\n\"",
          "",
          "[Removed Lines]",
          "39: GENERATED = \"@ge\" + \"nerated\"  # noqa: ISC003",
          "",
          "[Added Lines]",
          "39: GENERATED = \"@ge\" + \"nerated\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "425f6b93af1ed39ebc6d5162fb1ad57f48796d1e",
      "candidate_info": {
        "commit_hash": "425f6b93af1ed39ebc6d5162fb1ad57f48796d1e",
        "repo": "onnx/onnx",
        "commit_url": "https://github.com/onnx/onnx/commit/425f6b93af1ed39ebc6d5162fb1ad57f48796d1e",
        "files": [
          "onnx/backend/test/runner/__init__.py",
          "pyproject.toml",
          "requirements-lintrunner.txt"
        ],
        "message": "Update ruff to 0.7.0 (#6482)\n\n### Description\n<!-- - Describe your changes. -->\nUse latest ruff\n### Motivation and Context\n<!-- - Why is this change required? What problem does it solve? -->\n<!-- - If it fixes an open issue, please link to the issue here. -->\n\n---------\n\nSigned-off-by: cyy <cyyever@outlook.com>\nCo-authored-by: Xavier Dupr\u00e9 <xadupre@users.noreply.github.com>",
        "before_after_code_files": [
          "onnx/backend/test/runner/__init__.py||onnx/backend/test/runner/__init__.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "onnx/backend/test/runner/__init__.py||onnx/backend/test/runner/__init__.py"
          ],
          "candidate": [
            "onnx/backend/test/runner/__init__.py||onnx/backend/test/runner/__init__.py"
          ]
        }
      },
      "candidate_diff": {
        "onnx/backend/test/runner/__init__.py||onnx/backend/test/runner/__init__.py": [
          "File: onnx/backend/test/runner/__init__.py -> onnx/backend/test/runner/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "226:     def download_model(",
          "227:         cls,",
          "228:         model_test: TestCase,",
          "230:         models_dir: str,",
          "231:     ) -> None:",
          "251:     @classmethod",
          "252:     def prepare_model_data(cls, model_test: TestCase) -> str:",
          "",
          "[Removed Lines]",
          "229:         model_dir: str,",
          "232:         # On Windows, NamedTemporaryFile can not be opened for a",
          "233:         # second time",
          "234:         del model_dir",
          "235:         download_file = tempfile.NamedTemporaryFile(delete=False)",
          "236:         try:",
          "237:             download_file.close()",
          "238:             assert model_test.url",
          "239:             print(",
          "240:                 f\"Start downloading model {model_test.model_name} from {model_test.url}\"",
          "241:             )",
          "242:             urlretrieve(model_test.url, download_file.name)",
          "243:             print(\"Done\")",
          "244:             onnx.utils._extract_model_safe(download_file.name, models_dir)",
          "245:         except Exception as e:",
          "246:             print(f\"Failed to prepare data for model {model_test.model_name}: {e}\")",
          "247:             raise",
          "248:         finally:",
          "249:             os.remove(download_file.name)",
          "",
          "[Added Lines]",
          "231:         with tempfile.TemporaryDirectory() as tmpdir:",
          "232:             try:",
          "233:                 assert model_test.url",
          "234:                 print(",
          "235:                     f\"Start downloading model {model_test.model_name} from {model_test.url}\"",
          "236:                 )",
          "237:                 filename = os.path.join(tmpdir, \"file\")",
          "238:                 urlretrieve(model_test.url, filename)",
          "239:                 print(\"Done\")",
          "240:                 onnx.utils._extract_model_safe(filename, models_dir)",
          "241:             except Exception as e:",
          "242:                 print(f\"Failed to prepare data for model {model_test.model_name}: {e}\")",
          "243:                 raise",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "267:                     break",
          "268:             os.makedirs(model_dir)",
          "273:         return model_dir",
          "275:     def _add_test(",
          "",
          "[Removed Lines]",
          "270:             cls.download_model(",
          "271:                 model_test=model_test, model_dir=model_dir, models_dir=models_dir",
          "272:             )",
          "",
          "[Added Lines]",
          "264:             cls.download_model(model_test=model_test, models_dir=models_dir)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f615b26d564ca5d2e2a28af81f4b86a99a1865f1",
      "candidate_info": {
        "commit_hash": "f615b26d564ca5d2e2a28af81f4b86a99a1865f1",
        "repo": "onnx/onnx",
        "commit_url": "https://github.com/onnx/onnx/commit/f615b26d564ca5d2e2a28af81f4b86a99a1865f1",
        "files": [
          "onnx/compose.py",
          "onnx/gen_proto.py",
          "onnx/helper.py",
          "onnx/hub.py",
          "onnx/utils.py"
        ],
        "message": "Add type annotations and fix some lint warnings in Python code (#6470)\n\n### Description\n<!-- - Describe your changes. -->\nMore robust type checking in Python.\n### Motivation and Context\nFor better code.\n<!-- - Why is this change required? What problem does it solve? -->\n<!-- - If it fixes an open issue, please link to the issue here. -->\n\n---------\n\nSigned-off-by: cyy <cyyever@outlook.com>\nSigned-off-by: cyyever <cyyever@outlook.com>\nCo-authored-by: Justin Chu <justinchuby@users.noreply.github.com>",
        "before_after_code_files": [
          "onnx/compose.py||onnx/compose.py",
          "onnx/gen_proto.py||onnx/gen_proto.py",
          "onnx/helper.py||onnx/helper.py",
          "onnx/hub.py||onnx/hub.py",
          "onnx/utils.py||onnx/utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "onnx/hub.py||onnx/hub.py",
            "onnx/utils.py||onnx/utils.py"
          ],
          "candidate": [
            "onnx/hub.py||onnx/hub.py",
            "onnx/utils.py||onnx/utils.py"
          ]
        }
      },
      "candidate_diff": {
        "onnx/compose.py||onnx/compose.py": [
          "File: onnx/compose.py -> onnx/compose.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "28:     Optionally, it takes an io_map, representing the output/inputs to be connected. It provided, overlapping",
          "29:     present in the io_map argument will be ignored.",
          "30:     \"\"\"",
          "36:     def _overlapping(c1: list[str], c2: list[str]) -> list[str]:",
          "37:         return list(set(c1) & set(c2))",
          "",
          "[Removed Lines]",
          "31:     if type(g1) is not GraphProto:",
          "32:         raise ValueError(\"g1 argument is not an ONNX graph\")",
          "33:     if type(g2) is not GraphProto:",
          "34:         raise ValueError(\"g2 argument is not an ONNX graph\")",
          "",
          "[Added Lines]",
          "31:     if not isinstance(g1, GraphProto):",
          "32:         raise TypeError(\"g1 argument is not an ONNX graph\")",
          "33:     if not isinstance(g2, GraphProto):",
          "34:         raise TypeError(\"g2 argument is not an ONNX graph\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "123:     Returns:",
          "124:         GraphProto",
          "125:     \"\"\"",
          "131:     # Prefixing names in the graph if requested, adjusting io_map accordingly",
          "132:     if prefix1 or prefix2:",
          "",
          "[Removed Lines]",
          "126:     if type(g1) is not GraphProto:",
          "127:         raise ValueError(\"g1 argument is not an ONNX graph\")",
          "128:     if type(g2) is not GraphProto:",
          "129:         raise ValueError(\"g2 argument is not an ONNX graph\")",
          "",
          "[Added Lines]",
          "126:     if not isinstance(g1, GraphProto):",
          "127:         raise TypeError(\"g1 argument is not an ONNX graph\")",
          "128:     if not isinstance(g2, GraphProto):",
          "129:         raise TypeError(\"g2 argument is not an ONNX graph\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "329:     Returns:",
          "330:         ModelProto",
          "331:     \"\"\"",
          "337:     if m1.ir_version != m2.ir_version:",
          "338:         raise ValueError(",
          "",
          "[Removed Lines]",
          "332:     if type(m1) is not ModelProto:",
          "333:         raise ValueError(\"m1 argument is not an ONNX model\")",
          "334:     if type(m2) is not ModelProto:",
          "335:         raise ValueError(\"m2 argument is not an ONNX model\")",
          "",
          "[Added Lines]",
          "332:     if not isinstance(m1, ModelProto):",
          "333:         raise TypeError(\"m1 argument is not an ONNX model\")",
          "334:     if not isinstance(m2, ModelProto):",
          "335:         raise TypeError(\"m2 argument is not an ONNX model\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "460:     Returns:",
          "461:         GraphProto",
          "462:     \"\"\"",
          "466:     if not inplace:",
          "467:         g = GraphProto()",
          "",
          "[Removed Lines]",
          "463:     if type(graph) is not GraphProto:",
          "464:         raise ValueError(\"graph argument is not an ONNX graph\")",
          "",
          "[Added Lines]",
          "463:     if not isinstance(graph, GraphProto):",
          "464:         raise TypeError(\"graph argument is not an ONNX graph\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "577:     Returns:",
          "578:         ModelProto",
          "579:     \"\"\"",
          "583:     if not inplace:",
          "584:         m = ModelProto()",
          "",
          "[Removed Lines]",
          "580:     if type(model) is not ModelProto:",
          "581:         raise ValueError(\"model argument is not an ONNX model\")",
          "",
          "[Added Lines]",
          "580:     if not isinstance(model, ModelProto):",
          "581:         raise TypeError(\"model argument is not an ONNX model\")",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "637:     Returns:",
          "638:         GraphProto",
          "639:     \"\"\"",
          "643:     if not inplace:",
          "644:         g = GraphProto()",
          "",
          "[Removed Lines]",
          "640:     if type(graph) is not GraphProto:",
          "641:         raise ValueError(\"graph argument is not an ONNX graph\")",
          "",
          "[Added Lines]",
          "640:     if not isinstance(graph, GraphProto):",
          "641:         raise TypeError(\"graph argument is not an ONNX graph\")",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "717:     Returns:",
          "718:         ModelProto",
          "719:     \"\"\"",
          "723:     if not inplace:",
          "724:         m = ModelProto()",
          "",
          "[Removed Lines]",
          "720:     if type(model) is not ModelProto:",
          "721:         raise ValueError(\"model argument is not an ONNX model\")",
          "",
          "[Added Lines]",
          "720:     if not isinstance(model, ModelProto):",
          "721:         raise TypeError(\"model argument is not an ONNX model\")",
          "",
          "---------------"
        ],
        "onnx/gen_proto.py||onnx/gen_proto.py": [
          "File: onnx/gen_proto.py -> onnx/gen_proto.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "43:             assert in_if == 1",
          "44:             in_if = 2",
          "45:         elif ENDIF_ONNX_ML_REGEX.match(line):",
          "47:             in_if = 0",
          "48:         else:  # noqa: PLR5501",
          "49:             if in_if == 0:",
          "",
          "[Removed Lines]",
          "46:             assert in_if == 1 or in_if == 2  # noqa: PLR1714, PLR2004",
          "",
          "[Added Lines]",
          "46:             assert in_if in (1, 2)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "119:         lines = convert_to_proto3(lines)",
          "120:     else:",
          "121:         assert proto == 2  # noqa: PLR2004",
          "125: def qualify(f: str, pardir: str | None = None) -> str:",
          "",
          "[Removed Lines]",
          "122:     return \"\\n\".join(lines)  # TODO: not Windows friendly",
          "",
          "[Added Lines]",
          "122:     return os.linesep.join(lines)",
          "",
          "---------------"
        ],
        "onnx/helper.py||onnx/helper.py": [
          "File: onnx/helper.py -> onnx/helper.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "307:     model.ir_version = IR_VERSION",
          "308:     model.graph.CopyFrom(graph)",
          "312:     if opset_imports is not None:",
          "313:         model.opset_import.extend(opset_imports)",
          "314:     else:",
          "",
          "[Removed Lines]",
          "310:     opset_imports: Sequence[OperatorSetIdProto] | None = None",
          "311:     opset_imports = kwargs.pop(\"opset_imports\", None)  # type: ignore",
          "",
          "[Added Lines]",
          "310:     opset_imports: Sequence[OperatorSetIdProto] | None = kwargs.pop(",
          "311:         \"opset_imports\", None",
          "312:     )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "316:         imp = model.opset_import.add()",
          "317:         imp.version = defs.onnx_opset_version()",
          "321:     if functions is not None:",
          "322:         model.functions.extend(functions)",
          "",
          "[Removed Lines]",
          "319:     functions: Sequence[FunctionProto] | None = None",
          "320:     functions = kwargs.pop(\"functions\", None)  # type: ignore",
          "",
          "[Added Lines]",
          "320:     functions: Sequence[FunctionProto] | None = kwargs.pop(\"functions\", None)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "665:     if is_odd_volume:",
          "666:         array_flat = np.append(array_flat, np.array([0]))",
          "669:     func = np.frompyfunc(single_func, 2, 1)",
          "675: def pack_float32_to_float4e2m1(array: np.ndarray | Sequence) -> np.ndarray:",
          "",
          "[Removed Lines]",
          "668:     single_func = lambda x, y: subbyte.float32x2_to_4bitx2(x, y, signed)  # noqa: E731",
          "671:     arr = func(array_flat[0::2], array_flat[1::2])",
          "672:     return arr.astype(np.uint8)  # type: ignore[no-any-return]",
          "",
          "[Added Lines]",
          "668:     def single_func(x, y) -> np.ndarray:",
          "669:         return subbyte.float32x2_to_4bitx2(x, y, signed)",
          "673:     arr: np.ndarray = func(array_flat[0::2], array_flat[1::2])",
          "674:     return arr.astype(np.uint8)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "742:         else:",
          "743:             expected_size = np_dtype.itemsize",
          "746:         vals = vals.flatten()",
          "747:     for d in dims:",
          "748:         expected_size *= d",
          "",
          "[Removed Lines]",
          "745:     if type(vals) is np.ndarray and len(vals.shape) > 1:",
          "",
          "[Added Lines]",
          "747:     if isinstance(vals, np.ndarray) and len(vals.shape) > 1:",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "892:         map_proto.string_keys.extend(keys)",
          "893:     elif key_type in valid_key_int_types:",
          "894:         map_proto.keys.extend(keys)",
          "896:     return map_proto",
          "899: def make_optional(",
          "900:     name: str,",
          "901:     elem_type: OptionalProto.DataType,",
          "903: ) -> OptionalProto:",
          "904:     \"\"\"Make an Optional with specified value arguments.\"\"\"",
          "905:     optional = OptionalProto()",
          "",
          "[Removed Lines]",
          "895:     map_proto.values.CopyFrom(values)",
          "902:     value: Any | None,",
          "",
          "[Added Lines]",
          "897:     map_proto.values.CopyFrom(values)  # type: ignore[arg-type]",
          "904:     value: google.protobuf.message.Message | None,",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "921:     else:",
          "922:         raise TypeError(\"The element type in the input optional is not supported.\")",
          "925:     return optional",
          "928: def _to_bytes(value: str | bytes) -> bytes:",
          "929:     \"\"\"Coerce a string (or bytes) value into UTF-8 bytes.\"\"\"",
          "933: def make_attribute(",
          "",
          "[Removed Lines]",
          "924:     attribute.CopyFrom(value)  # type: ignore[arg-type]",
          "930:     return value if isinstance(value, bytes) else value.encode(\"utf-8\")",
          "",
          "[Added Lines]",
          "926:     assert value is not None",
          "927:     attribute.CopyFrom(value)",
          "933:     if isinstance(value, str):",
          "934:         return value.encode(\"utf-8\")",
          "935:     return value",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "1462:     if len(graph.input):",
          "1463:         header.append(\"(\")",
          "1464:         in_strs = []  # required inputs",
          "1466:             []",
          "1467:         )  # optional inputs with initializer providing default value",
          "1468:         for inp in graph.input:",
          "",
          "[Removed Lines]",
          "1465:         in_with_init_strs = (",
          "",
          "[Added Lines]",
          "1470:         in_with_init_strs: list = (",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "1659:     return mapping.TENSOR_TYPE_MAP.keys()",
          "1665: def _attr_type_to_str(attr_type: int) -> str:",
          "",
          "[Removed Lines]",
          "1662: _ATTRIBUTE_TYPE_TO_STR = {k: v for v, k in AttributeProto.AttributeType.items()}",
          "",
          "[Added Lines]",
          "1667: _ATTRIBUTE_TYPE_TO_STR: dict[int, str] = {",
          "1668:     k: v for v, k in AttributeProto.AttributeType.items()",
          "1669: }",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "1672:         String representing the supplied attr_type.",
          "1673:     \"\"\"",
          "1674:     if attr_type in AttributeProto.AttributeType.values():",
          "",
          "[Removed Lines]",
          "1675:         return _ATTRIBUTE_TYPE_TO_STR[attr_type]  # type: ignore[no-any-return]",
          "1676:     return AttributeProto.AttributeType.keys()[0]  # type: ignore[no-any-return]",
          "",
          "[Added Lines]",
          "1682:         return _ATTRIBUTE_TYPE_TO_STR[attr_type]",
          "1683:     return AttributeProto.AttributeType.keys()[0]",
          "",
          "---------------"
        ],
        "onnx/hub.py||onnx/hub.py": [
          "File: onnx/hub.py -> onnx/hub.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "363:                 \"download the model from the model hub.\"",
          "364:             )",
          "368:     local_model_with_data_dir_path = local_model_with_data_path[",
          "370:     ]",
          "371:     onnx.utils._extract_model_safe(",
          "372:         local_model_with_data_path, local_model_with_data_dir_path",
          "",
          "[Removed Lines]",
          "366:     # FIXME: Avoid index manipulation with magic numbers,",
          "367:     # remove \".tar.gz\"",
          "369:         0 : len(local_model_with_data_path) - 7",
          "",
          "[Added Lines]",
          "368:         0 : len(local_model_with_data_path) - len(\".tar.gz\")",
          "",
          "---------------"
        ],
        "onnx/utils.py||onnx/utils.py": [
          "File: onnx/utils.py -> onnx/utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22:         self.vimap = self._build_name2obj_dict(self.graph.value_info)",
          "24:     @staticmethod",
          "26:         return {obj.name: obj for obj in objs}",
          "28:     def _collect_new_io_core(",
          "",
          "[Removed Lines]",
          "25:     def _build_name2obj_dict(objs):  # type: ignore",
          "",
          "[Added Lines]",
          "25:     def _build_name2obj_dict(objs) -> dict:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "105:     def _collect_referred_local_functions(",
          "106:         self,",
          "109:         # a node in a model graph may refer a function.",
          "110:         # a function contains nodes, some of which may in turn refer a function.",
          "111:         # we need to find functions referred by graph nodes and",
          "112:         # by nodes used to define functions.",
          "114:             new_nodes = []  # type: list[NodeProto]",
          "115:             for node in nodes:",
          "116:                 # check if the node is a function op",
          "",
          "[Removed Lines]",
          "107:         nodes,  # type: list[NodeProto]",
          "108:     ):  # type: (...) -> list[FunctionProto]",
          "113:         def find_referred_funcs(nodes, referred_local_functions):  # type: ignore",
          "",
          "[Added Lines]",
          "107:         nodes: list[NodeProto],",
          "108:     ) -> list[FunctionProto]:",
          "113:         def find_referred_funcs(",
          "114:             nodes: list[NodeProto], referred_local_functions: list[FunctionProto]",
          "115:         ) -> list[NodeProto]:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "284:                 f\"The tarball member {member_path} in downloading model contains \"",
          "285:                 f\"directory traversal sequence which may contain harmful payload.\"",
          "286:             )",
          "288:             raise RuntimeError(",
          "289:                 f\"The tarball member {member_path} in downloading model contains \"",
          "290:                 f\"symbolic links which may contain harmful payload.\"",
          "",
          "[Removed Lines]",
          "287:         elif member.issym() or member.islnk():",
          "",
          "[Added Lines]",
          "289:         if member.issym() or member.islnk():",
          "",
          "---------------"
        ]
      }
    }
  ]
}