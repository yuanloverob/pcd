{
  "cve_id": "CVE-2023-42780",
  "cve_desc": "Apache Airflow, versions prior to 2.7.2, contains a security vulnerability that allows authenticated users of Airflow to list warnings for all DAGs, even if the user had no permission to see those DAGs. It would reveal the dag_ids and the stack-traces of import errors for those DAGs with import errors.\nUsers of Apache Airflow are advised to upgrade to version 2.7.2 or newer to mitigate the risk associated with this vulnerability.\n\n",
  "repo": "apache/airflow",
  "patch_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
  "patch_info": {
    "commit_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ],
    "message": "Fix dag warning endpoint permissions (#34355)\n\n* Fix dag warning endpoint permissions\n\n* update the query to have an accurate result for total entries and pagination\n\n* add unit tests\n\n* Update test_dag_warning_endpoint.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 3570bbfbea69e2965f91b9964ce28bc268c68129)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_warning_endpoint.py -> airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "16: # under the License.",
      "17: from __future__ import annotations",
      "19: from sqlalchemy import select",
      "20: from sqlalchemy.orm import Session",
      "22: from airflow.api_connexion import security",
      "23: from airflow.api_connexion.parameters import apply_sorting, check_limit, format_parameters",
      "24: from airflow.api_connexion.schemas.dag_warning_schema import (",
      "25:     DagWarningCollection,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "19: from flask import g",
      "24: from airflow.api_connexion.exceptions import PermissionDenied",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "28: from airflow.api_connexion.types import APIResponse",
      "29: from airflow.models.dagwarning import DagWarning as DagWarningModel",
      "30: from airflow.security import permissions",
      "31: from airflow.utils.db import get_query_count",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from airflow.utils.airflow_flask_app import get_airflow_app",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "52:     allowed_filter_attrs = [\"dag_id\", \"warning_type\", \"message\", \"timestamp\"]",
      "53:     query = select(DagWarningModel)",
      "54:     if dag_id:",
      "55:         query = query.where(DagWarningModel.dag_id == dag_id)",
      "56:     if warning_type:",
      "57:         query = query.where(DagWarningModel.warning_type == warning_type)",
      "58:     total_entries = get_query_count(query, session=session)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "58:         if not get_airflow_app().appbuilder.sm.can_read_dag(dag_id, g.user):",
      "59:             raise PermissionDenied(detail=f\"User not allowed to access this DAG: {dag_id}\")",
      "61:     else:",
      "62:         readable_dags = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
      "63:         query = query.where(DagWarningModel.dag_id.in_(readable_dags))",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_warning_endpoint.py -> tests/api_connexion/endpoints/test_dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35:         app,  # type:ignore",
      "36:         username=\"test\",",
      "37:         role_name=\"Test\",",
      "39:     )",
      "40:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "42:     yield minimal_app_for_api",
      "44:     delete_user(app, username=\"test\")  # type: ignore",
      "45:     delete_user(app, username=\"test_no_permissions\")  # type: ignore",
      "48: class TestBaseDagWarning:",
      "",
      "[Removed Lines]",
      "38:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING)],  # type: ignore",
      "",
      "[Added Lines]",
      "38:         permissions=[",
      "39:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "40:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
      "41:         ],  # type: ignore",
      "44:     create_user(",
      "45:         app,  # type:ignore",
      "46:         username=\"test_with_dag2_read\",",
      "47:         role_name=\"TestWithDag2Read\",",
      "48:         permissions=[",
      "49:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "50:             (permissions.ACTION_CAN_READ, f\"{permissions.RESOURCE_DAG_PREFIX}dag2\"),",
      "51:         ],  # type: ignore",
      "52:     )",
      "58:     delete_user(app, username=\"test_with_dag2_read\")  # type: ignore",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "147:             \"/api/v1/dagWarnings\", environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"}",
      "148:         )",
      "149:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "164:     def test_should_raise_403_forbidden_when_user_has_no_dag_read_permission(self):",
      "165:         response = self.client.get(",
      "166:             \"/api/v1/dagWarnings\",",
      "167:             environ_overrides={\"REMOTE_USER\": \"test_with_dag2_read\"},",
      "168:             query_string={\"dag_id\": \"dag1\"},",
      "169:         )",
      "170:         assert response.status_code == 403",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
      "candidate_info": {
        "commit_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
        "files": [
          "airflow/api_connexion/endpoints/config_endpoint.py",
          "airflow/configuration.py",
          "tests/api_connexion/endpoints/test_config_endpoint.py"
        ],
        "message": "Check if the lower of provided values are sensitives in config endpoint (#34712)\n\n* Check if the lower of provided values are sensitives in config endpoint\n\n* update unit test\n\n* ensure that all values in sensitive dict are in lower characters\n\n(cherry picked from commit f044589b685855a8fce8f5376bea2564c5a001f7)",
        "before_after_code_files": [
          "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py",
          "airflow/configuration.py||airflow/configuration.py",
          "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py": [
          "File: airflow/api_connexion/endpoints/config_endpoint.py -> airflow/api_connexion/endpoints/config_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "123:                 \"Config not found.\", detail=f\"The option [{section}/{option}] is not found in config.\"",
          "124:             )",
          "127:             value = \"< hidden >\"",
          "128:         else:",
          "129:             value = conf.get(section, option)",
          "",
          "[Removed Lines]",
          "126:         if (section, option) in conf.sensitive_config_values:",
          "",
          "[Added Lines]",
          "126:         if (section.lower(), option.lower()) in conf.sensitive_config_values:",
          "",
          "---------------"
        ],
        "airflow/configuration.py||airflow/configuration.py": [
          "File: airflow/configuration.py -> airflow/configuration.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "311:             for s, s_c in self.configuration_description.items()",
          "312:             for k, item in s_c.get(\"options\").items()  # type: ignore[union-attr]",
          "313:         }",
          "315:         depr_option = {self.deprecated_options[x][:-1] for x in sensitive if x in self.deprecated_options}",
          "316:         depr_section = {",
          "317:             (self.deprecated_sections[s][0], k) for s, k in sensitive if s in self.deprecated_sections",
          "",
          "[Removed Lines]",
          "314:         sensitive = {(section, key) for (section, key), v in flattened.items() if v.get(\"sensitive\") is True}",
          "",
          "[Added Lines]",
          "314:         sensitive = {",
          "315:             (section.lower(), key.lower())",
          "316:             for (section, key), v in flattened.items()",
          "317:             if v.get(\"sensitive\") is True",
          "318:         }",
          "",
          "---------------"
        ],
        "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_config_endpoint.py -> tests/api_connexion/endpoints/test_config_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "247:         return_value=MOCK_CONF_WITH_SENSITIVE_VALUE,",
          "248:     )",
          "249:     @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
          "251:         response = self.client.get(",
          "253:             headers={\"Accept\": \"text/plain\"},",
          "254:             environ_overrides={\"REMOTE_USER\": \"test\"},",
          "255:         )",
          "256:         assert response.status_code == 200",
          "257:         expected = textwrap.dedent(",
          "261:         \"\"\"",
          "262:         )",
          "263:         assert expected == response.data.decode()",
          "",
          "[Removed Lines]",
          "250:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict):",
          "252:             \"/api/v1/config/section/core/option/sql_alchemy_conn\",",
          "258:             \"\"\"\\",
          "259:         [core]",
          "260:         sql_alchemy_conn = < hidden >",
          "",
          "[Added Lines]",
          "250:     @pytest.mark.parametrize(",
          "251:         \"section, option\",",
          "252:         [",
          "253:             (\"core\", \"sql_alchemy_conn\"),",
          "254:             (\"core\", \"SQL_ALCHEMY_CONN\"),",
          "255:             (\"corE\", \"sql_alchemy_conn\"),",
          "256:             (\"CORE\", \"sql_alchemy_conn\"),",
          "257:         ],",
          "258:     )",
          "259:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict, section, option):",
          "261:             f\"/api/v1/config/section/{section}/option/{option}\",",
          "267:             f\"\"\"\\",
          "268:         [{section}]",
          "269:         {option} = < hidden >",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4595aaf97ab5bbdfac755fa4dcc7f73b6d4f7c99",
      "candidate_info": {
        "commit_hash": "4595aaf97ab5bbdfac755fa4dcc7f73b6d4f7c99",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4595aaf97ab5bbdfac755fa4dcc7f73b6d4f7c99",
        "files": [
          "airflow/models/dag.py"
        ],
        "message": "Make param validation consistent for DAG validation and triggering (#34248)\n\n(cherry picked from commit 3e340797ab98a06b51b2930610b0abb0ad20a750)",
        "before_after_code_files": [
          "airflow/models/dag.py||airflow/models/dag.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "87:     AirflowSkipException,",
          "88:     DuplicateTaskIdFound,",
          "89:     FailStopDagInvalidTriggerRule,",
          "90:     RemovedInAirflow3Warning,",
          "91:     TaskNotFound,",
          "92: )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "90:     ParamValidationError,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3276:     def validate_schedule_and_params(self):",
          "3277:         \"\"\"",
          "3282:         \"\"\"",
          "3283:         if not self.timetable.can_be_scheduled:",
          "3284:             return",
          "3293:     def iter_invalid_owner_links(self) -> Iterator[tuple[str, str]]:",
          "3294:         \"\"\"",
          "",
          "[Removed Lines]",
          "3278:         Validate Param values when the schedule_interval is not None.",
          "3280:         Raise exception if there are any Params in the DAG which neither have a default value nor",
          "3281:         have the null in schema['type'] list, but the DAG have a schedule_interval which is not None.",
          "3286:         for v in self.params.values():",
          "3287:             # As type can be an array, we would check if `null` is an allowed type or not",
          "3288:             if not v.has_value and (\"type\" not in v.schema or \"null\" not in v.schema[\"type\"]):",
          "3289:                 raise AirflowException(",
          "3290:                     \"DAG Schedule must be None, if there are any required params without default values\"",
          "3291:                 )",
          "",
          "[Added Lines]",
          "3279:         Validate Param values when the DAG has schedule defined.",
          "3281:         Raise exception if there are any Params which can not be resolved by their schema definition.",
          "3286:         try:",
          "3287:             self.params.validate()",
          "3288:         except ParamValidationError as pverr:",
          "3289:             raise AirflowException(",
          "3290:                 \"DAG is not allowed to define a Schedule, \"",
          "3291:                 \"if there are any required params without default values or default values are not valid.\"",
          "3292:             ) from pverr",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c20fc8a2a78ba8c4fd548973af800461b8fae7f8",
      "candidate_info": {
        "commit_hash": "c20fc8a2a78ba8c4fd548973af800461b8fae7f8",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c20fc8a2a78ba8c4fd548973af800461b8fae7f8",
        "files": [
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/tests/test_docker_command_utils.py"
        ],
        "message": "Fix autodetect_docker_context for list of dict case (#34779)\n\n* Fix autodetect_docker_context for list of dict case\n\n* Handle the case of dict\n\n(cherry picked from commit 5c2dc53bcb17ae515f9565c41d15cc6d1693382c)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/tests/test_docker_command_utils.py||dev/breeze/tests/test_docker_command_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "826:     if result.returncode != 0:",
          "827:         get_console().print(\"[warning]Could not detect docker builder. Using default.[/]\")",
          "828:         return \"default\"",
          "830:     known_contexts = {info[\"Name\"]: info for info in context_dicts}",
          "831:     if not known_contexts:",
          "832:         get_console().print(\"[warning]Could not detect docker builder. Using default.[/]\")",
          "",
          "[Removed Lines]",
          "829:     context_dicts = (json.loads(line) for line in result.stdout.splitlines() if line.strip())",
          "",
          "[Added Lines]",
          "829:     try:",
          "830:         context_dicts = json.loads(result.stdout)",
          "831:         if isinstance(context_dicts, dict):",
          "832:             context_dicts = [context_dicts]",
          "833:     except json.decoder.JSONDecodeError:",
          "834:         context_dicts = (json.loads(line) for line in result.stdout.splitlines() if line.strip())",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_docker_command_utils.py||dev/breeze/tests/test_docker_command_utils.py": [
          "File: dev/breeze/tests/test_docker_command_utils.py -> dev/breeze/tests/test_docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "228:             \"desktop-linux\",",
          "229:             \"[info]Using desktop-linux as context\",",
          "230:         ),",
          "231:     ],",
          "232: )",
          "233: def test_autodetect_docker_context(context_output: str, selected_context: str, console_output: str):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "231:         (",
          "232:             _fake_ctx_output(\"a\", \"default\", \"desktop-linux\"),",
          "233:             \"desktop-linux\",",
          "234:             \"[info]Using desktop-linux as context\",",
          "235:         ),",
          "236:         (",
          "237:             '[{\"Name\": \"desktop-linux\", \"DockerEndpoint\": \"unix://desktop-linux\"}]',",
          "238:             \"desktop-linux\",",
          "239:             \"[info]Using desktop-linux as context\",",
          "240:         ),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2b717a827477d83de16f6a1437a1682b0787e3ed",
      "candidate_info": {
        "commit_hash": "2b717a827477d83de16f6a1437a1682b0787e3ed",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/2b717a827477d83de16f6a1437a1682b0787e3ed",
        "files": [
          "airflow/metrics/validators.py",
          "tests/core/test_otel_logger.py"
        ],
        "message": "Add more exemptions to lengthy metric list (#34531)\n\nCo-authored-by: Saurabh Kumar <mail@sa1.me>\n(cherry picked from commit fa6ca5d5316a9bd759a702e1688a69b19e4e63bc)",
        "before_after_code_files": [
          "airflow/metrics/validators.py||airflow/metrics/validators.py",
          "tests/core/test_otel_logger.py||tests/core/test_otel_logger.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/metrics/validators.py||airflow/metrics/validators.py": [
          "File: airflow/metrics/validators.py -> airflow/metrics/validators.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "70:     r\"^pool\\.open_slots\\.(?P<pool_name>.*)$\",",
          "71:     r\"^pool\\.queued_slots\\.(?P<pool_name>.*)$\",",
          "72:     r\"^pool\\.running_slots\\.(?P<pool_name>.*)$\",",
          "73:     r\"^pool\\.starving_tasks\\.(?P<pool_name>.*)$\",",
          "74:     r\"^dagrun\\.dependency-check\\.(?P<dag_id>.*)$\",",
          "75:     r\"^dag\\.(?P<dag_id>.*)\\.(?P<task_id>.*)\\.duration$\",",
          "76:     r\"^dag_processing\\.last_duration\\.(?P<dag_file>.*)$\",",
          "77:     r\"^dagrun\\.duration\\.success\\.(?P<dag_id>.*)$\",",
          "78:     r\"^dagrun\\.duration\\.failed\\.(?P<dag_id>.*)$\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "73:     r\"^pool\\.deferred_slots\\.(?P<pool_name>.*)$\",",
          "77:     r\"^dag\\.(?P<dag_id>.*)\\.(?P<task_id>.*)\\.queued_duration$\",",
          "78:     r\"^dag\\.(?P<dag_id>.*)\\.(?P<task_id>.*)\\.scheduled_duration$\",",
          "",
          "---------------"
        ],
        "tests/core/test_otel_logger.py||tests/core/test_otel_logger.py": [
          "File: tests/core/test_otel_logger.py -> tests/core/test_otel_logger.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "66:         assert not _is_up_down_counter(\"this_is_not_a_udc\")",
          "68:     def test_exemption_list_has_not_grown(self):",
          "70:             \"This test exists solely to ensure that nobody is adding names to the exemption list. \"",
          "72:             \"only ever go down as these names are deprecated.  If this test is failing, please \"",
          "73:             \"adjust your new stat's name; do not add as exemption without a very good reason.\"",
          "74:         )",
          "",
          "[Removed Lines]",
          "69:         assert len(BACK_COMPAT_METRIC_NAMES) <= 23, (",
          "71:             \"There are 23 names which are potentially too long for OTel and that number should \"",
          "",
          "[Added Lines]",
          "69:         assert len(BACK_COMPAT_METRIC_NAMES) <= 26, (",
          "71:             \"There are 26 names which are potentially too long for OTel and that number should \"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f9a05d3695c3eb2b41f06cc73b57155d11981cd5",
      "candidate_info": {
        "commit_hash": "f9a05d3695c3eb2b41f06cc73b57155d11981cd5",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f9a05d3695c3eb2b41f06cc73b57155d11981cd5",
        "files": [
          "airflow/models/dag.py",
          "airflow/serialization/serialized_objects.py",
          "airflow/www/security.py",
          "tests/serialization/test_dag_serialization.py"
        ],
        "message": "Fix issues related to access_control={} (#34114)\n\n* allow empty access control on dags\n\n* Add test which demonstrates loss of information during ser/de\n\n* Keep empty value when serializing access_control dict\n\n(cherry picked from commit a61b5e893af83d3022aa8265f230b52c5b2ad93d)",
        "before_after_code_files": [
          "airflow/models/dag.py||airflow/models/dag.py",
          "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py",
          "airflow/www/security.py||airflow/www/security.py",
          "tests/serialization/test_dag_serialization.py||tests/serialization/test_dag_serialization.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "777:         will be replaced with 'can_read', in {'role2': {'can_dag_read', 'can_dag_edit'}}",
          "778:         'can_dag_edit' will be replaced with 'can_edit', etc.",
          "779:         \"\"\"",
          "781:             return None",
          "782:         new_perm_mapping = {",
          "783:             permissions.DEPRECATED_ACTION_CAN_DAG_READ: permissions.ACTION_CAN_READ,",
          "",
          "[Removed Lines]",
          "780:         if not access_control:",
          "",
          "[Added Lines]",
          "780:         if access_control is None:",
          "",
          "---------------"
        ],
        "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py": [
          "File: airflow/serialization/serialized_objects.py -> airflow/serialization/serialized_objects.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1386:         return dag",
          "1388:     @classmethod",
          "1389:     def to_dict(cls, var: Any) -> dict:",
          "1390:         \"\"\"Stringifies DAGs and operators contained by var and returns a dict of var.\"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1388:     @classmethod",
          "1389:     def _is_excluded(cls, var: Any, attrname: str, op: DAGNode):",
          "1390:         # {} is explicitly different from None in the case of DAG-level access control",
          "1391:         # and as a result we need to preserve empty dicts through serialization for this field",
          "1392:         if attrname == \"_access_control\" and var is not None:",
          "1393:             return False",
          "1394:         return super()._is_excluded(var, attrname, op)",
          "",
          "---------------"
        ],
        "airflow/www/security.py||airflow/www/security.py": [
          "File: airflow/www/security.py -> airflow/www/security.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "626:                 if (action_name, dag_resource_name) not in perms:",
          "627:                     self._merge_perm(action_name, dag_resource_name)",
          "630:                 self.sync_perm_for_dag(dag_resource_name, dag.access_control)",
          "632:     def update_admin_permission(self) -> None:",
          "",
          "[Removed Lines]",
          "629:             if dag.access_control:",
          "",
          "[Added Lines]",
          "629:             if dag.access_control is not None:",
          "",
          "---------------"
        ],
        "tests/serialization/test_dag_serialization.py||tests/serialization/test_dag_serialization.py": [
          "File: tests/serialization/test_dag_serialization.py -> tests/serialization/test_dag_serialization.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "427:                 print(task[\"task_id\"], k, v)",
          "428:         assert actual == expected",
          "430:     def test_dag_serialization_unregistered_custom_timetable(self):",
          "431:         \"\"\"Verify serialization fails without timetable registration.\"\"\"",
          "432:         dag = get_timetable_based_simple_dag(CustomSerializationTimetable(\"bar\"))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "430:     def test_dag_serialization_preserves_empty_access_roles(self):",
          "431:         \"\"\"Verify that an explicitly empty access_control dict is preserved.\"\"\"",
          "432:         dag = collect_dags([\"airflow/example_dags\"])[\"simple_dag\"]",
          "433:         dag.access_control = {}",
          "434:         serialized_dag = SerializedDAG.to_dict(dag)",
          "435:         SerializedDAG.validate_schema(serialized_dag)",
          "437:         assert serialized_dag[\"dag\"][\"_access_control\"] == {\"__type\": \"dict\", \"__var\": {}}",
          "",
          "---------------"
        ]
      }
    }
  ]
}