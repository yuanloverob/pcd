{
  "cve_id": "CVE-2021-45229",
  "cve_desc": "It was discovered that the \"Trigger DAG with config\" screen was susceptible to XSS attacks via the `origin` query argument. This issue affects Apache Airflow versions 2.2.3 and below.",
  "repo": "apache/airflow",
  "patch_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
  "patch_info": {
    "commit_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "files": [
      "airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py"
    ],
    "message": "Simplify trigger cancel button (#21591)\n\nCo-authored-by: Jed Cunningham <jedcunningham@apache.org>\n(cherry picked from commit 65297673a318660fba76797e50d0c06804dfcafc)",
    "before_after_code_files": [
      "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
    ]
  },
  "patch_diff": {
    "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html": [
      "File: airflow/www/templates/airflow/trigger.html -> airflow/www/templates/airflow/trigger.html",
      "--- Hunk 1 ---",
      "[Context before]",
      "63:       </label>",
      "64:     </div>",
      "65:     <button type=\"submit\" class=\"btn btn-primary\">Trigger</button>",
      "67:   </form>",
      "68: {% endblock %}",
      "",
      "[Removed Lines]",
      "66:     <button type=\"button\" class=\"btn\" onclick=\"location.href = '{{ origin }}'; return false\">Cancel</button>",
      "",
      "[Added Lines]",
      "66:     <a class=\"btn\" href=\"{{ origin }}\">Cancel</a>",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
      "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "133:         (\"javascript:alert(1)\", \"/home\"),",
      "134:         (\"http://google.com\", \"/home\"),",
      "135:         (\"36539'%3balert(1)%2f%2f166\", \"/home\"),",
      "136:         (",
      "137:             \"%2Ftree%3Fdag_id%3Dexample_bash_operator';alert(33)//\",",
      "138:             \"/home\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "136:         (",
      "137:             '\"><script>alert(99)</script><a href=\"',",
      "138:             \"&#34;&gt;&lt;script&gt;alert(99)&lt;/script&gt;&lt;a href=&#34;\",",
      "139:         ),",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "145:     test_dag_id = \"example_bash_operator\"",
      "147:     resp = admin_client.get(f'trigger?dag_id={test_dag_id}&origin={test_origin}')",
      "156: @pytest.mark.parametrize(",
      "",
      "[Removed Lines]",
      "148:     check_content_in_response(",
      "149:         '<button type=\"button\" class=\"btn\" onclick=\"location.href = \\'{}\\'; return false\">'.format(",
      "150:             expected_origin",
      "151:         ),",
      "152:         resp,",
      "153:     )",
      "",
      "[Added Lines]",
      "152:     check_content_in_response(f'<a class=\"btn\" href=\"{expected_origin}\">Cancel</a>', resp)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "9d5c403ac285dae4a78e4152708f4f9fa506cec9",
      "candidate_info": {
        "commit_hash": "9d5c403ac285dae4a78e4152708f4f9fa506cec9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/9d5c403ac285dae4a78e4152708f4f9fa506cec9",
        "files": [
          "Dockerfile",
          "Dockerfile.ci"
        ],
        "message": "Add extra sync when adding executable flag to installation scripts (#20987)\n\nSeems that when AUFS is a backing storage for Docker, changing\nthe script to executable and executing it right after during the\nbuild phase might cause an error: 'text file busy'\n\nhttps://github.com/moby/moby/issues/13594\n\nWorkaround for that is to add extra `sync` command after changing\nthe executable flag to make sure that the filesystem change has\npropageted to the underlying AUFS storage.\n\nThis PR adds the sync and also makes sure that both CI And PROD\nimage use same formatting, executable bits and `&&` between\ncommands rather than `;`. The `&&` is better to separate the\ncommands because it will not continue with execution steps in the\nsame bash command after previous command fails. This caused\nconfusion as to what is the reason for docker build failure.\n\nThe problem was raised in the #20971 discussion.\n\n(cherry picked from commit c13558186c678c15938db5ea3a65f948061af72a)",
        "before_after_code_files": [
          "Dockerfile.ci||Dockerfile.ci"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "Dockerfile.ci||Dockerfile.ci": [
          "File: Dockerfile.ci -> Dockerfile.ci",
          "--- Hunk 1 ---",
          "[Context before]",
          "97:     && rm -rf /var/lib/apt/lists/*",
          "99: # Only copy mysql/mssql installation scripts for now - so that changing the other",
          "101: COPY scripts/docker/install_mysql.sh scripts/docker/install_mssql.sh /scripts/docker/",
          "103:     && adduser --gecos \"First Last,RoomNumber,WorkPhone,HomePhone\" --disabled-password \\",
          "104:               --quiet \"airflow\" --home \"/home/airflow\" \\",
          "105:     && echo -e \"airflow\\nairflow\" | passwd airflow 2>&1 \\",
          "",
          "[Removed Lines]",
          "100: # scripts which are needed much later will not invalidate the docker layer here",
          "102: RUN /scripts/docker/install_mysql.sh dev && /scripts/docker/install_mssql.sh \\",
          "",
          "[Added Lines]",
          "100: # scripts which are needed much later will not invalidate the docker layer here.",
          "102: # We run chmod +x to fix permission issue in Azure DevOps when running the scripts",
          "103: # However when AUFS Docker backend is used, this might cause \"text file busy\" error",
          "104: # when script is executed right after it's executable flag has been changed, so",
          "105: # we run additional sync afterwards. See https://github.com/moby/moby/issues/13594",
          "106: RUN chmod a+x /scripts/docker/install_mysql.sh /scripts/docker/install_mssql.sh \\",
          "107:     && sync  \\",
          "108:     && /scripts/docker/install_mysql.sh prod  \\",
          "109:     && /scripts/docker/install_mysql.sh dev  \\",
          "110:     && /scripts/docker/install_mssql.sh \\",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "32768645e291e7dfcac2618028f9b4d5b7bd787d",
      "candidate_info": {
        "commit_hash": "32768645e291e7dfcac2618028f9b4d5b7bd787d",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/32768645e291e7dfcac2618028f9b4d5b7bd787d",
        "files": [
          "scripts/docker/compile_www_assets.sh"
        ],
        "message": "Fix speed of yarn installation (#19697)\n\nThe --network-concurrency=1 is very slow and even if this has\nbeen added in #17293 to battle connection refused, it slows regular\nbuilds far too much.\n\nThere is a new optimisation in progress that should significantly\nreduce the yarn installations on kind-cluster deploy: #19210 and\nit should solve the problem much better.\n\n(cherry picked from commit 43e84ccb8c909ad78862a9411ab72dbac6c7169c)",
        "before_after_code_files": [
          "scripts/docker/compile_www_assets.sh||scripts/docker/compile_www_assets.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/docker/compile_www_assets.sh||scripts/docker/compile_www_assets.sh": [
          "File: scripts/docker/compile_www_assets.sh -> scripts/docker/compile_www_assets.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "35:         www_dir=\"$(python -m site --user-site)/airflow/www\"",
          "36:     fi",
          "37:     pushd ${www_dir} || exit 1",
          "39:     yarn run prod",
          "40:     find package.json yarn.lock static/css static/js -type f | sort | xargs md5sum > \"${md5sum_file}\"",
          "41:     rm -rf \"${www_dir}/node_modules\"",
          "",
          "[Removed Lines]",
          "38:     yarn install --frozen-lockfile --no-cache --network-concurrency=1",
          "",
          "[Added Lines]",
          "38:     yarn install --frozen-lockfile --no-cache",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4dc8b909bedd04094be3079c3f7384ea044ec011",
      "candidate_info": {
        "commit_hash": "4dc8b909bedd04094be3079c3f7384ea044ec011",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4dc8b909bedd04094be3079c3f7384ea044ec011",
        "files": [
          "airflow/api/common/delete_dag.py",
          "tests/api/common/test_delete_dag.py"
        ],
        "message": "Avoid unintentional data loss when deleting DAGs (#20758)\n\n(cherry picked from commit 5980d2b05eee484256c634d5efae9410265c65e9)",
        "before_after_code_files": [
          "airflow/api/common/delete_dag.py||airflow/api/common/delete_dag.py",
          "tests/api/common/test_delete_dag.py||tests/api/common/test_delete_dag.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api/common/delete_dag.py||airflow/api/common/delete_dag.py": [
          "File: airflow/api/common/delete_dag.py -> airflow/api/common/delete_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: \"\"\"Delete DAGs APIs.\"\"\"",
          "19: import logging",
          "23: from airflow import models",
          "24: from airflow.exceptions import AirflowException, DagNotFound",
          "",
          "[Removed Lines]",
          "21: from sqlalchemy import or_",
          "",
          "[Added Lines]",
          "21: from sqlalchemy import and_, or_",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "54:     if dag is None:",
          "55:         raise DagNotFound(f\"Dag id {dag_id} not found\")",
          "57:     # Scheduler removes DAGs without files from serialized_dag table every dag_dir_list_interval.",
          "58:     # There may be a lag, so explicitly removes serialized DAG here.",
          "59:     if SerializedDagModel.has_dag(dag_id=dag_id, session=session):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "57:     # deleting a DAG should also delete all of its subdags",
          "58:     dags_to_delete_query = session.query(DagModel.dag_id).filter(",
          "59:         or_(",
          "60:             DagModel.dag_id == dag_id,",
          "61:             and_(DagModel.dag_id.like(f\"{dag_id}.%\"), DagModel.is_subdag),",
          "62:         )",
          "63:     )",
          "64:     dags_to_delete = [dag_id for dag_id, in dags_to_delete_query]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "65:         if hasattr(model, \"dag_id\"):",
          "66:             if keep_records_in_log and model.__name__ == 'Log':",
          "67:                 continue",
          "70:     if dag.is_subdag:",
          "71:         parent_dag_id, task_id = dag_id.rsplit(\".\", 1)",
          "72:         for model in TaskFail, models.TaskInstance:",
          "",
          "[Removed Lines]",
          "68:             cond = or_(model.dag_id == dag_id, model.dag_id.like(dag_id + \".%\"))",
          "69:             count += session.query(model).filter(cond).delete(synchronize_session='fetch')",
          "",
          "[Added Lines]",
          "77:             count += (",
          "78:                 session.query(model)",
          "79:                 .filter(model.dag_id.in_(dags_to_delete))",
          "80:                 .delete(synchronize_session='fetch')",
          "81:             )",
          "",
          "---------------"
        ],
        "tests/api/common/test_delete_dag.py||tests/api/common/test_delete_dag.py": [
          "File: tests/api/common/test_delete_dag.py -> tests/api/common/test_delete_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "162:         self.check_dag_models_exists()",
          "163:         delete_dag(dag_id=self.key, keep_records_in_log=False)",
          "164:         self.check_dag_models_removed(expect_logs=0)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "166:     def test_delete_dag_preserves_other_dags(self):",
          "168:         self.setup_dag_models()",
          "170:         with create_session() as session:",
          "171:             session.add(DM(dag_id=self.key + \".other_dag\", fileloc=self.dag_file_path))",
          "172:             session.add(DM(dag_id=self.key + \".subdag\", fileloc=self.dag_file_path, is_subdag=True))",
          "174:         delete_dag(self.key)",
          "176:         with create_session() as session:",
          "177:             assert session.query(DM).filter(DM.dag_id == self.key + \".other_dag\").count() == 1",
          "178:             assert session.query(DM).filter(DM.dag_id.like(self.key + \"%\")).count() == 1",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2066812960018ce0d7ba774dd2f9fe5c0d8b52a4",
      "candidate_info": {
        "commit_hash": "2066812960018ce0d7ba774dd2f9fe5c0d8b52a4",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/2066812960018ce0d7ba774dd2f9fe5c0d8b52a4",
        "files": [
          "airflow/api/common/experimental/get_code.py",
          "airflow/api/common/experimental/get_dag_run_state.py",
          "airflow/api/common/experimental/get_task.py",
          "airflow/api/common/experimental/get_task_instance.py",
          "airflow/api/common/experimental/pool.py"
        ],
        "message": "Update version to 2.2.4 for things in that release (#21196)\n\n(cherry picked from commit 093702e9f579ee028a103cdc9acf0e6acccd6d79)",
        "before_after_code_files": [
          "airflow/api/common/experimental/get_code.py||airflow/api/common/experimental/get_code.py",
          "airflow/api/common/experimental/get_dag_run_state.py||airflow/api/common/experimental/get_dag_run_state.py",
          "airflow/api/common/experimental/get_task.py||airflow/api/common/experimental/get_task.py",
          "airflow/api/common/experimental/get_task_instance.py||airflow/api/common/experimental/get_task_instance.py",
          "airflow/api/common/experimental/pool.py||airflow/api/common/experimental/pool.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api/common/experimental/get_code.py||airflow/api/common/experimental/get_code.py": [
          "File: airflow/api/common/experimental/get_code.py -> airflow/api/common/experimental/get_code.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: from airflow.models.dagcode import DagCode",
          "27: def get_code(dag_id: str) -> str:",
          "28:     \"\"\"Return python code of a given dag_id.",
          "",
          "[Removed Lines]",
          "26: @deprecated(reason=\"Use DagCode().get_code_by_fileloc() instead\", version=\"2.2.3\")",
          "",
          "[Added Lines]",
          "26: @deprecated(reason=\"Use DagCode().get_code_by_fileloc() instead\", version=\"2.2.4\")",
          "",
          "---------------"
        ],
        "airflow/api/common/experimental/get_dag_run_state.py||airflow/api/common/experimental/get_dag_run_state.py": [
          "File: airflow/api/common/experimental/get_dag_run_state.py -> airflow/api/common/experimental/get_dag_run_state.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: from airflow.api.common.experimental import check_and_get_dag, check_and_get_dagrun",
          "28: def get_dag_run_state(dag_id: str, execution_date: datetime) -> Dict[str, str]:",
          "29:     \"\"\"Return the Dag Run state identified by the given dag_id and execution_date.",
          "",
          "[Removed Lines]",
          "27: @deprecated(reason=\"Use DagRun().get_state() instead\", version=\"2.2.3\")",
          "",
          "[Added Lines]",
          "27: @deprecated(reason=\"Use DagRun().get_state() instead\", version=\"2.2.4\")",
          "",
          "---------------"
        ],
        "airflow/api/common/experimental/get_task.py||airflow/api/common/experimental/get_task.py": [
          "File: airflow/api/common/experimental/get_task.py -> airflow/api/common/experimental/get_task.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: from airflow.models import TaskInstance",
          "26: def get_task(dag_id: str, task_id: str) -> TaskInstance:",
          "27:     \"\"\"Return the task object identified by the given dag_id and task_id.\"\"\"",
          "28:     dag = check_and_get_dag(dag_id, task_id)",
          "",
          "[Removed Lines]",
          "25: @deprecated(reason=\"Use DAG().get_task\", version=\"2.2.3\")",
          "",
          "[Added Lines]",
          "25: @deprecated(reason=\"Use DAG().get_task\", version=\"2.2.4\")",
          "",
          "---------------"
        ],
        "airflow/api/common/experimental/get_task_instance.py||airflow/api/common/experimental/get_task_instance.py": [
          "File: airflow/api/common/experimental/get_task_instance.py -> airflow/api/common/experimental/get_task_instance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: from airflow.models import TaskInstance",
          "29: def get_task_instance(dag_id: str, task_id: str, execution_date: datetime) -> TaskInstance:",
          "30:     \"\"\"Return the task instance identified by the given dag_id, task_id and execution_date.\"\"\"",
          "31:     dag = check_and_get_dag(dag_id, task_id)",
          "",
          "[Removed Lines]",
          "28: @deprecated(version=\"2.2.3\", reason=\"Use DagRun.get_task_instance instead\")",
          "",
          "[Added Lines]",
          "28: @deprecated(version=\"2.2.4\", reason=\"Use DagRun.get_task_instance instead\")",
          "",
          "---------------"
        ],
        "airflow/api/common/experimental/pool.py||airflow/api/common/experimental/pool.py": [
          "File: airflow/api/common/experimental/pool.py -> airflow/api/common/experimental/pool.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: from airflow.utils.session import provide_session",
          "27: @provide_session",
          "28: def get_pool(name, session=None):",
          "29:     \"\"\"Get pool by a given name.\"\"\"",
          "",
          "[Removed Lines]",
          "26: @deprecated(reason=\"Use Pool.get_pool() instead\", version=\"2.2.3\")",
          "",
          "[Added Lines]",
          "26: @deprecated(reason=\"Use Pool.get_pool() instead\", version=\"2.2.4\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "37:     return pool",
          "41: @provide_session",
          "42: def get_pools(session=None):",
          "43:     \"\"\"Get all pools.\"\"\"",
          "44:     return session.query(Pool).all()",
          "48: @provide_session",
          "49: def create_pool(name, slots, description, session=None):",
          "50:     \"\"\"Create a pool with a given parameters.\"\"\"",
          "",
          "[Removed Lines]",
          "40: @deprecated(reason=\"Use Pool.get_pools() instead\", version=\"2.2.3\")",
          "47: @deprecated(reason=\"Use Pool.create_pool() instead\", version=\"2.2.3\")",
          "",
          "[Added Lines]",
          "40: @deprecated(reason=\"Use Pool.get_pools() instead\", version=\"2.2.4\")",
          "47: @deprecated(reason=\"Use Pool.create_pool() instead\", version=\"2.2.4\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "75:     return pool",
          "79: @provide_session",
          "80: def delete_pool(name, session=None):",
          "81:     \"\"\"Delete pool by a given name.\"\"\"",
          "",
          "[Removed Lines]",
          "78: @deprecated(reason=\"Use Pool.delete_pool() instead\", version=\"2.2.3\")",
          "",
          "[Added Lines]",
          "78: @deprecated(reason=\"Use Pool.delete_pool() instead\", version=\"2.2.4\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "dda8f4356525041c5200c42d00e5dc05fd79c54b",
      "candidate_info": {
        "commit_hash": "dda8f4356525041c5200c42d00e5dc05fd79c54b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/dda8f4356525041c5200c42d00e5dc05fd79c54b",
        "files": [
          "airflow/cli/commands/dag_command.py"
        ],
        "message": "Fix 'airflow dags backfill --reset-dagruns' errors when run twice (#21062)\n\nCo-authored-by: uplsh <uplsh@linecorp.com>\n(cherry picked from commit d97e2bac854f9891eb47f0c06c261e89723038ca)",
        "before_after_code_files": [
          "airflow/cli/commands/dag_command.py||airflow/cli/commands/dag_command.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/commands/dag_command.py||airflow/cli/commands/dag_command.py": [
          "File: airflow/cli/commands/dag_command.py -> airflow/cli/commands/dag_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "47: )",
          "48: from airflow.utils.dot_renderer import render_dag",
          "49: from airflow.utils.session import create_session, provide_session",
          "53: @cli_utils.action_logging",
          "",
          "[Removed Lines]",
          "50: from airflow.utils.state import State",
          "",
          "[Added Lines]",
          "50: from airflow.utils.state import DagRunState",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "105:                 end_date=args.end_date,",
          "106:                 confirm_prompt=not args.yes,",
          "107:                 include_subdags=True,",
          "109:             )",
          "111:         try:",
          "",
          "[Removed Lines]",
          "108:                 dag_run_state=State.NONE,",
          "",
          "[Added Lines]",
          "108:                 dag_run_state=DagRunState.QUEUED,",
          "",
          "---------------"
        ]
      }
    }
  ]
}