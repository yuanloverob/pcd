{
  "cve_id": "CVE-2022-40754",
  "cve_desc": "In Apache Airflow 2.3.0 through 2.3.4, there was an open redirect in the webserver's `/confirm` endpoint.",
  "repo": "apache/airflow",
  "patch_hash": "3871f00230b7064e6234aa65b8a00ee2b6d33e4c",
  "patch_info": {
    "commit_hash": "3871f00230b7064e6234aa65b8a00ee2b6d33e4c",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/3871f00230b7064e6234aa65b8a00ee2b6d33e4c",
    "files": [
      "airflow/www/views.py"
    ],
    "message": "Fix UI redirect (#26409)\n\nCo-authored-by: Konstantin Weddige <konstantin.weddige@lutrasecurity.com>\n(cherry picked from commit 56e7555c42f013f789a4b718676ff09b4a9d5135)",
    "before_after_code_files": [
      "airflow/www/views.py||airflow/www/views.py"
    ]
  },
  "patch_diff": {
    "airflow/www/views.py||airflow/www/views.py": [
      "File: airflow/www/views.py -> airflow/www/views.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "2329:         task_id = args.get('task_id')",
      "2330:         dag_run_id = args.get('dag_run_id')",
      "2331:         state = args.get('state')",
      "2334:         if 'map_index' not in args:",
      "2335:             map_indexes: list[int] | None = None",
      "",
      "[Removed Lines]",
      "2332:         origin = args.get('origin')",
      "",
      "[Added Lines]",
      "2332:         origin = get_safe_url(args.get('origin'))",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "56e7555c42f013f789a4b718676ff09b4a9d5135",
      "candidate_info": {
        "commit_hash": "56e7555c42f013f789a4b718676ff09b4a9d5135",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/56e7555c42f013f789a4b718676ff09b4a9d5135",
        "files": [
          "airflow/www/views.py"
        ],
        "message": "Fix UI redirect (#26409)\n\nCo-authored-by: Konstantin Weddige <konstantin.weddige@lutrasecurity.com>",
        "before_after_code_files": [
          "airflow/www/views.py||airflow/www/views.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "diff_branch_cherry_pick": 1,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "airflow/www/views.py||airflow/www/views.py"
          ],
          "candidate": [
            "airflow/www/views.py||airflow/www/views.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2329:         task_id = args.get('task_id')",
          "2330:         dag_run_id = args.get('dag_run_id')",
          "2331:         state = args.get('state')",
          "2334:         if 'map_index' not in args:",
          "2335:             map_indexes: list[int] | None = None",
          "",
          "[Removed Lines]",
          "2332:         origin = args.get('origin')",
          "",
          "[Added Lines]",
          "2332:         origin = get_safe_url(args.get('origin'))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0967259373dcbb2f10d9b8cc06f44009e92a503c",
      "candidate_info": {
        "commit_hash": "0967259373dcbb2f10d9b8cc06f44009e92a503c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0967259373dcbb2f10d9b8cc06f44009e92a503c",
        "files": [
          "airflow/models/dagbag.py",
          "tests/dags/test_invalid_dup_task.py",
          "tests/jobs/test_scheduler_job.py",
          "tests/models/test_dagbag.py"
        ],
        "message": "Clear autoregistered DAGs if there are any import errors (#26398)\n\nWe need to clear any autoregistered DAGs that may have been already\nregistered if we encounter any import errors while parsing a given DAG\nfile.\n\nThis maintains the behavior before we autoregistered DAGs.\n\n(cherry picked from commit 01e3fb7eac6cceb1fa4cc68ee1e4fe3682edbf0a)",
        "before_after_code_files": [
          "airflow/models/dagbag.py||airflow/models/dagbag.py",
          "tests/dags/test_invalid_dup_task.py||tests/dags/test_invalid_dup_task.py",
          "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py",
          "tests/models/test_dagbag.py||tests/models/test_dagbag.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dagbag.py||airflow/models/dagbag.py": [
          "File: airflow/models/dagbag.py -> airflow/models/dagbag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "326:                 loader.exec_module(new_module)",
          "327:                 return [new_module]",
          "328:             except Exception as e:",
          "329:                 self.log.exception(\"Failed to import: %s\", filepath)",
          "330:                 if self.dagbag_import_error_tracebacks:",
          "331:                     self.import_errors[filepath] = traceback.format_exc(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "329:                 DagContext.autoregistered_dags.clear()",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "391:                     current_module = importlib.import_module(mod_name)",
          "392:                     mods.append(current_module)",
          "393:                 except Exception as e:",
          "394:                     fileloc = os.path.join(filepath, zip_info.filename)",
          "395:                     self.log.exception(\"Failed to import: %s\", fileloc)",
          "396:                     if self.dagbag_import_error_tracebacks:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "395:                     DagContext.autoregistered_dags.clear()",
          "",
          "---------------"
        ],
        "tests/dags/test_invalid_dup_task.py||tests/dags/test_invalid_dup_task.py": [
          "File: tests/dags/test_invalid_dup_task.py -> tests/dags/test_invalid_dup_task.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "19: from datetime import datetime",
          "21: from airflow import DAG",
          "22: from airflow.operators.empty import EmptyOperator",
          "24: with DAG(",
          "25:     \"test_invalid_dup_task\",",
          "26:     start_date=datetime(2021, 1, 1),",
          "27:     schedule=\"@once\",",
          "28: ):",
          "29:     EmptyOperator(task_id=\"hi\")",
          "30:     EmptyOperator(task_id=\"hi\")",
          "",
          "---------------"
        ],
        "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py": [
          "File: tests/jobs/test_scheduler_job.py -> tests/jobs/test_scheduler_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2766:         ignored_files = {",
          "2767:             'no_dags.py',",
          "2768:             'test_invalid_cron.py',",
          "2769:             'test_ignore_this.py',",
          "2770:             'test_invalid_param.py',",
          "2771:             'test_nested_dag.py',",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2769:             'test_invalid_dup_task.py',",
          "",
          "---------------"
        ],
        "tests/models/test_dagbag.py||tests/models/test_dagbag.py": [
          "File: tests/models/test_dagbag.py -> tests/models/test_dagbag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "357:             assert dag, f\"{dag_id} was bagged\"",
          "358:             assert dag.fileloc.endswith(path)",
          "360:     @patch.object(DagModel, \"get_current\")",
          "361:     def test_refresh_py_dag(self, mock_dagmodel):",
          "362:         \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "360:     def test_dag_registration_with_failure(self):",
          "361:         dagbag = models.DagBag(dag_folder=os.devnull, include_examples=False)",
          "362:         found = dagbag.process_file(str(TEST_DAGS_FOLDER / 'test_invalid_dup_task.py'))",
          "363:         assert [] == found",
          "365:     @pytest.fixture()",
          "366:     def zip_with_valid_dag_and_dup_tasks(self, tmp_path: pathlib.Path) -> Iterator[str]:",
          "367:         failing_dag_file = TEST_DAGS_FOLDER / 'test_invalid_dup_task.py'",
          "368:         working_dag_file = TEST_DAGS_FOLDER / 'test_example_bash_operator.py'",
          "369:         zipped = os.path.join(tmp_path, \"test_zip_invalid_dup_task.zip\")",
          "370:         with zipfile.ZipFile(zipped, \"w\") as zf:",
          "371:             zf.write(failing_dag_file, os.path.basename(failing_dag_file))",
          "372:             zf.write(working_dag_file, os.path.basename(working_dag_file))",
          "373:         yield zipped",
          "374:         os.unlink(zipped)",
          "376:     def test_dag_registration_with_failure_zipped(self, zip_with_valid_dag_and_dup_tasks):",
          "377:         dagbag = models.DagBag(dag_folder=os.devnull, include_examples=False)",
          "378:         found = dagbag.process_file(zip_with_valid_dag_and_dup_tasks)",
          "379:         assert 1 == len(found)",
          "380:         assert ['test_example_bash_operator'] == [dag.dag_id for dag in found]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0bcdba0e4e8d109abf8f98618a3784457364e442",
      "candidate_info": {
        "commit_hash": "0bcdba0e4e8d109abf8f98618a3784457364e442",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0bcdba0e4e8d109abf8f98618a3784457364e442",
        "files": [
          "RELEASE_NOTES.rst",
          "scripts/ci/pre_commit/pre_commit_version_heads_map.py"
        ],
        "message": "Update Release Note for 2.4.0",
        "before_after_code_files": [
          "scripts/ci/pre_commit/pre_commit_version_heads_map.py||scripts/ci/pre_commit/pre_commit_version_heads_map.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/pre_commit/pre_commit_version_heads_map.py||scripts/ci/pre_commit/pre_commit_version_heads_map.py": [
          "File: scripts/ci/pre_commit/pre_commit_version_heads_map.py -> scripts/ci/pre_commit/pre_commit_version_heads_map.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "60:     if airflow_version.is_devrelease or 'b' in (airflow_version.pre or ()):",
          "61:         exit(0)",
          "62:     versions = read_revision_heads_map()",
          "64:         print(\"Current airflow version is not in the REVISION_HEADS_MAP\")",
          "65:         print(\"Current airflow version:\", airflow_version)",
          "66:         print(\"Please add the version to the REVISION_HEADS_MAP at:\", DB_FILE)",
          "",
          "[Removed Lines]",
          "63:     if airflow_version not in versions:",
          "",
          "[Added Lines]",
          "63:     if airflow_version.base_version not in versions:",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b013ce9a951dac95f691adb5164d7e6140fedaea",
      "candidate_info": {
        "commit_hash": "b013ce9a951dac95f691adb5164d7e6140fedaea",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b013ce9a951dac95f691adb5164d7e6140fedaea",
        "files": [
          "airflow/models/dataset.py",
          "airflow/www/static/js/datasetUtils.js",
          "airflow/www/templates/airflow/dataset_next_run_modal.html",
          "airflow/www/views.py",
          "tests/www/views/test_views_grid.py"
        ],
        "message": "Fix Dataset bugs in grid view (#26356)\n\n* Fix bugs in grid view\n\n* Add DatasetModel.uri to group_by\n\n(cherry picked from commit 9a9ed5b1f6c3caf1525eb73c4d1440752f737f5e)",
        "before_after_code_files": [
          "airflow/models/dataset.py||airflow/models/dataset.py",
          "airflow/www/static/js/datasetUtils.js||airflow/www/static/js/datasetUtils.js",
          "airflow/www/templates/airflow/dataset_next_run_modal.html||airflow/www/templates/airflow/dataset_next_run_modal.html",
          "airflow/www/views.py||airflow/www/views.py",
          "tests/www/views/test_views_grid.py||tests/www/views/test_views_grid.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [
            "airflow/www/views.py||airflow/www/views.py"
          ],
          "candidate": [
            "airflow/www/views.py||airflow/www/views.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/models/dataset.py||airflow/models/dataset.py": [
          "File: airflow/models/dataset.py -> airflow/models/dataset.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "160:     created_at = Column(UtcDateTime, default=timezone.utcnow, nullable=False)",
          "161:     updated_at = Column(UtcDateTime, default=timezone.utcnow, onupdate=timezone.utcnow, nullable=False)",
          "165:     __tablename__ = \"task_outlet_dataset_reference\"",
          "166:     __table_args__ = (",
          "",
          "[Removed Lines]",
          "163:     dataset = relationship(\"DatasetModel\")",
          "",
          "[Added Lines]",
          "163:     dataset = relationship(\"DatasetModel\", back_populates=\"producing_tasks\")",
          "",
          "---------------"
        ],
        "airflow/www/static/js/datasetUtils.js||airflow/www/static/js/datasetUtils.js": [
          "File: airflow/www/static/js/datasetUtils.js -> airflow/www/static/js/datasetUtils.js",
          "--- Hunk 1 ---",
          "[Context before]",
          "38:     uriCell.append(datasetLink);",
          "40:     const timeCell = document.createElement('td');",
          "43:     row.append(uriCell);",
          "44:     row.append(timeCell);",
          "",
          "[Removed Lines]",
          "41:     if (d.created_at) timeCell.append(isoDateToTimeEl(d.created_at));",
          "",
          "[Added Lines]",
          "41:     if (d.lastUpdate) timeCell.append(isoDateToTimeEl(d.lastUpdate));",
          "",
          "---------------"
        ],
        "airflow/www/templates/airflow/dataset_next_run_modal.html||airflow/www/templates/airflow/dataset_next_run_modal.html": [
          "File: airflow/www/templates/airflow/dataset_next_run_modal.html -> airflow/www/templates/airflow/dataset_next_run_modal.html",
          "--- Hunk 1 ---",
          "[Context before]",
          "40:             <thead>",
          "41:               <tr>",
          "42:                 <th>Dataset URI</th>",
          "44:               </tr>",
          "45:             </thead>",
          "46:             <tbody id=\"datasets_tbody\">",
          "",
          "[Removed Lines]",
          "43:                 <th>Timestamp</th>",
          "",
          "[Added Lines]",
          "43:                 <th>Latest Update</th>",
          "",
          "---------------"
        ],
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "96: from airflow.models.dag import DAG, get_dataset_triggered_next_run_info",
          "97: from airflow.models.dagcode import DagCode",
          "98: from airflow.models.dagrun import DagRun, DagRunType",
          "100: from airflow.models.operator import Operator",
          "101: from airflow.models.serialized_dag import SerializedDagModel",
          "102: from airflow.models.taskinstance import TaskInstance",
          "",
          "[Removed Lines]",
          "99: from airflow.models.dataset import DagScheduleDatasetReference, DatasetDagRunQueue, DatasetModel",
          "",
          "[Added Lines]",
          "99: from airflow.models.dataset import DagScheduleDatasetReference, DatasetDagRunQueue, DatasetEvent, DatasetModel",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3481:                 for info in session.query(",
          "3482:                     DatasetModel.id,",
          "3483:                     DatasetModel.uri,",
          "3485:                 )",
          "3487:                 .join(",
          "3488:                     DatasetDagRunQueue,",
          "3489:                     and_(",
          "3491:                         DatasetDagRunQueue.target_dag_id == DagScheduleDatasetReference.dag_id,",
          "3492:                     ),",
          "3493:                     isouter=True,",
          "3494:                 )",
          "3495:                 .filter(DagScheduleDatasetReference.dag_id == dag_id)",
          "3497:                 .all()",
          "3498:             ]",
          "3499:         return (",
          "",
          "[Removed Lines]",
          "3484:                     DatasetDagRunQueue.created_at,",
          "3486:                 .join(DagScheduleDatasetReference, DatasetModel.id == DagScheduleDatasetReference.dataset_id)",
          "3490:                         DatasetDagRunQueue.dataset_id == DagScheduleDatasetReference.dataset_id,",
          "3496:                 .order_by(DatasetModel.id)",
          "",
          "[Added Lines]",
          "3484:                     func.max(DatasetEvent.timestamp).label(\"lastUpdate\"),",
          "3486:                 .join(DagScheduleDatasetReference, DagScheduleDatasetReference.dataset_id == DatasetModel.id)",
          "3490:                         DatasetDagRunQueue.dataset_id == DatasetModel.id,",
          "3495:                 .join(",
          "3496:                     DatasetEvent,",
          "3497:                     and_(",
          "3498:                         DatasetEvent.dataset_id == DatasetModel.id,",
          "3499:                         DatasetEvent.timestamp > DatasetDagRunQueue.created_at,",
          "3500:                     ),",
          "3501:                     isouter=True,",
          "3502:                 )",
          "3504:                 .group_by(DatasetModel.id, DatasetModel.uri)",
          "3505:                 .order_by(DatasetModel.uri)",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_grid.py||tests/www/views/test_views_grid.py": [
          "File: tests/www/views/test_views_grid.py -> tests/www/views/test_views_grid.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: from airflow.lineage.entities import File",
          "27: from airflow.models import DagBag",
          "28: from airflow.models.dagrun import DagRun",
          "30: from airflow.operators.empty import EmptyOperator",
          "31: from airflow.utils.state import DagRunState, TaskInstanceState",
          "32: from airflow.utils.task_group import TaskGroup",
          "",
          "[Removed Lines]",
          "29: from airflow.models.dataset import DatasetDagRunQueue, DatasetModel",
          "",
          "[Added Lines]",
          "29: from airflow.models.dataset import DatasetDagRunQueue, DatasetEvent, DatasetModel",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "342:         ds1_id = session.query(DatasetModel.id).filter_by(uri=datasets[0].uri).scalar()",
          "343:         ds2_id = session.query(DatasetModel.id).filter_by(uri=datasets[1].uri).scalar()",
          "344:         ddrq = DatasetDagRunQueue(",
          "346:         )",
          "347:         session.add(ddrq)",
          "348:         session.commit()",
          "350:         resp = admin_client.get(f'/object/next_run_datasets/{DAG_ID}', follow_redirects=True)",
          "352:     assert resp.status_code == 200, resp.json",
          "353:     assert resp.json == [",
          "356:     ]",
          "",
          "[Removed Lines]",
          "345:             target_dag_id=DAG_ID, dataset_id=ds1_id, created_at=pendulum.DateTime(2022, 8, 1, tzinfo=UTC)",
          "354:         {'id': ds1_id, 'uri': 's3://bucket/key/1', 'created_at': \"2022-08-01T00:00:00+00:00\"},",
          "355:         {'id': ds2_id, 'uri': 's3://bucket/key/2', 'created_at': None},",
          "",
          "[Added Lines]",
          "345:             target_dag_id=DAG_ID, dataset_id=ds1_id, created_at=pendulum.DateTime(2022, 8, 2, tzinfo=UTC)",
          "348:         dataset_events = [",
          "349:             DatasetEvent(",
          "350:                 dataset_id=ds1_id,",
          "351:                 extra={},",
          "352:                 timestamp=pendulum.DateTime(2022, 8, 1, 1, tzinfo=UTC),",
          "353:             ),",
          "354:             DatasetEvent(",
          "355:                 dataset_id=ds1_id,",
          "356:                 extra={},",
          "357:                 timestamp=pendulum.DateTime(2022, 8, 2, 1, tzinfo=UTC),",
          "358:             ),",
          "359:             DatasetEvent(",
          "360:                 dataset_id=ds1_id,",
          "361:                 extra={},",
          "362:                 timestamp=pendulum.DateTime(2022, 8, 2, 2, tzinfo=UTC),",
          "363:             ),",
          "364:         ]",
          "365:         session.add_all(dataset_events)",
          "372:         {'id': ds1_id, 'uri': 's3://bucket/key/1', 'lastUpdate': \"2022-08-02T02:00:00+00:00\"},",
          "373:         {'id': ds2_id, 'uri': 's3://bucket/key/2', 'lastUpdate': None},",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "62322ef5189bad4c6a4e58880a967b4a714df4d1",
      "candidate_info": {
        "commit_hash": "62322ef5189bad4c6a4e58880a967b4a714df4d1",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/62322ef5189bad4c6a4e58880a967b4a714df4d1",
        "files": [
          "setup.cfg"
        ],
        "message": "Add min attrs version (#26408)\n\nI'm not sure when we started using attrs directly, but we do and we need\n>=22.1.0 as we use the min_length validator.\n\n(cherry picked from commit fdecf12051308a4e064f5e4bf5464ffc9b183dad)",
        "before_after_code_files": [
          "setup.cfg||setup.cfg"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.cfg||setup.cfg": [
          "File: setup.cfg -> setup.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "85:     # together with SQLAlchemy. Our experience with Alembic is that it very stable in minor version",
          "86:     alembic>=1.5.1, <2.0",
          "87:     argcomplete>=1.10",
          "88:     blinker",
          "89:     cached_property>=1.5.0;python_version<=\"3.7\"",
          "90:     cattrs>=22.1.0",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "88:     attrs>=22.1.0",
          "",
          "---------------"
        ]
      }
    }
  ]
}