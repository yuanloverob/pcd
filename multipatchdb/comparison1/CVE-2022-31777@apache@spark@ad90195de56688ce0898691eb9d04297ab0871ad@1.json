{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "24a3fa95a384159004d45cf2c01a699f1c2e55f7",
      "candidate_info": {
        "commit_hash": "24a3fa95a384159004d45cf2c01a699f1c2e55f7",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/24a3fa95a384159004d45cf2c01a699f1c2e55f7",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala"
        ],
        "message": "[SPARK-39233][SQL] Remove the check for TimestampNTZ output in Analyzer\n\n### What changes were proposed in this pull request?\n\nIn [#36094](https://github.com/apache/spark/pull/36094), a check for failing TimestampNTZ type output(since we are disabling TimestampNTZ in 3.3) is added:\n```\n      case operator: LogicalPlan\n        if !Utils.isTesting && operator.output.exists(attr =>\n          attr.resolved && attr.dataType.isInstanceOf[TimestampNTZType]) =>\n        operator.failAnalysis(\"TimestampNTZ type is not supported in Spark 3.3.\")\n```\n\nHowever, the check can cause misleading error messages.\n\nIn 3.3:\n```\n> sql( \"select date '2018-11-17' > 1\").show()\norg.apache.spark.sql.AnalysisException: Invalid call to toAttribute on unresolved object;\n'Project [unresolvedalias((2018-11-17 > 1), None)]\n+- OneRowRelation\n\n  at org.apache.spark.sql.catalyst.analysis.UnresolvedAlias.toAttribute(unresolved.scala:510)\n  at org.apache.spark.sql.catalyst.plans.logical.Project.$anonfun$output$1(basicLogicalOperators.scala:70)\n```\nIn master or 3.2\n```\n> sql( \"select date '2018-11-17' > 1\").show()\norg.apache.spark.sql.AnalysisException: cannot resolve '(DATE '2018-11-17' > 1)' due to data type mismatch: differing types in '(DATE '2018-11-17' > 1)' (date and int).; line 1 pos 7;\n'Project [unresolvedalias((2018-11-17 > 1), None)]\n+- OneRowRelation\n\n  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n```\n\nWe should just remove the check to avoid such regression. It's not necessary for disabling TimestampNTZ anyway.\n\n### Why are the changes needed?\n\nFix regression in the error output of analysis check.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, it is not released yet.\n\n### How was this patch tested?\n\nBuild and try on `spark-shell`\n\nCloses #36609 from gengliangwang/fixRegression.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "32: import org.apache.spark.sql.internal.SQLConf",
          "33: import org.apache.spark.sql.types._",
          "34: import org.apache.spark.sql.util.SchemaUtils",
          "",
          "[Removed Lines]",
          "35: import org.apache.spark.util.Utils",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "160:       case _: ShowTableExtended =>",
          "161:         throw QueryCompilationErrors.commandUnsupportedInV2TableError(\"SHOW TABLE EXTENDED\")",
          "168:       case operator: LogicalPlan =>",
          "",
          "[Removed Lines]",
          "163:       case operator: LogicalPlan",
          "164:         if !Utils.isTesting && operator.output.exists(attr =>",
          "165:           attr.resolved && attr.dataType.isInstanceOf[TimestampNTZType]) =>",
          "166:         operator.failAnalysis(\"TimestampNTZ type is not supported in Spark 3.3.\")",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "17552d5ff90e6421b2699726468c5798a12970b9",
      "candidate_info": {
        "commit_hash": "17552d5ff90e6421b2699726468c5798a12970b9",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/17552d5ff90e6421b2699726468c5798a12970b9",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/connector/SupportsPushDownCatalystFilters.scala"
        ],
        "message": "[SPARK-38950][SQL][FOLLOWUP] Fix java doc\n\n### What changes were proposed in this pull request?\n`{link #pushFilters(Predicate[])}` ->  `{link #pushFilters(Seq[Expression])}`\n\n### Why are the changes needed?\nFixed java doc\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\n\nCloses #36302 from huaxingao/fix.\n\nAuthored-by: huaxingao <huaxin_gao@apple.com>\nSigned-off-by: huaxingao <huaxin_gao@apple.com>\n(cherry picked from commit 0b543e7480b6e414b23e02e6c805a33abc535c89)\nSigned-off-by: huaxingao <huaxin_gao@apple.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/connector/SupportsPushDownCatalystFilters.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/connector/SupportsPushDownCatalystFilters.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/connector/SupportsPushDownCatalystFilters.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/connector/SupportsPushDownCatalystFilters.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/internal/connector/SupportsPushDownCatalystFilters.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/internal/connector/SupportsPushDownCatalystFilters.scala"
        ]
      }
    },
    {
      "candidate_hash": "a0242eabaeef39ec4d74d2bdd0bcac78c71a63e6",
      "candidate_info": {
        "commit_hash": "a0242eabaeef39ec4d74d2bdd0bcac78c71a63e6",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/a0242eabaeef39ec4d74d2bdd0bcac78c71a63e6",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetPartitionReaderFactory.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaPruningSuite.scala"
        ],
        "message": "[SPARK-39951][SQL] Update Parquet V2 columnar check for nested fields\n\n### What changes were proposed in this pull request?\n\nUpdate the `supportsColumnarReads` check for Parquet V2 to take into account support for nested fields. Also fixed a typo I saw in one of the tests.\n\n### Why are the changes needed?\n\nMatch Parquet V1 in returning columnar batches if nested field vectorization is enabled.\n\n### Does this PR introduce _any_ user-facing change?\n\nParquet V2 scans will return columnar batches with nested fields if the config is enabled.\n\n### How was this patch tested?\n\nAdded new UTs checking both V1 and V2 return columnar batches for nested fields when the config is enabled.\n\nCloses #37379 from Kimahriman/parquet-v2-columnar.\n\nAuthored-by: Adam Binford <adamq43@gmail.com>\nSigned-off-by: Chao Sun <sunchao@apple.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetPartitionReaderFactory.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetPartitionReaderFactory.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaPruningSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaPruningSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "174:   override def supportBatch(sparkSession: SparkSession, schema: StructType): Boolean = {",
          "175:     val conf = sparkSession.sessionState.conf",
          "179:   }",
          "181:   override def vectorTypes(",
          "",
          "[Removed Lines]",
          "176:     conf.parquetVectorizedReaderEnabled && conf.wholeStageEnabled &&",
          "177:       ParquetUtils.isBatchReadSupportedForSchema(conf, schema) &&",
          "178:         !WholeStageCodegenExec.isTooManyFields(conf, schema)",
          "",
          "[Added Lines]",
          "176:     ParquetUtils.isBatchReadSupportedForSchema(conf, schema) && conf.wholeStageEnabled &&",
          "177:       !WholeStageCodegenExec.isTooManyFields(conf, schema)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetPartitionReaderFactory.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetPartitionReaderFactory.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetPartitionReaderFactory.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/parquet/ParquetPartitionReaderFactory.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "37: import org.apache.spark.sql.catalyst.util.RebaseDateTime.RebaseSpec",
          "38: import org.apache.spark.sql.connector.expressions.aggregate.Aggregation",
          "39: import org.apache.spark.sql.connector.read.{InputPartition, PartitionReader}",
          "40: import org.apache.spark.sql.execution.datasources.{AggregatePushDownUtils, DataSourceUtils, PartitionedFile, RecordReaderIterator}",
          "41: import org.apache.spark.sql.execution.datasources.parquet._",
          "42: import org.apache.spark.sql.execution.datasources.v2._",
          "43: import org.apache.spark.sql.internal.SQLConf",
          "44: import org.apache.spark.sql.sources.Filter",
          "46: import org.apache.spark.sql.vectorized.ColumnarBatch",
          "47: import org.apache.spark.util.SerializableConfiguration",
          "",
          "[Removed Lines]",
          "45: import org.apache.spark.sql.types.{AtomicType, StructType}",
          "",
          "[Added Lines]",
          "40: import org.apache.spark.sql.execution.WholeStageCodegenExec",
          "46: import org.apache.spark.sql.types.StructType",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "72:   private val enableOffHeapColumnVector = sqlConf.offHeapColumnVectorEnabled",
          "73:   private val enableVectorizedReader: Boolean =",
          "74:     ParquetUtils.isBatchReadSupportedForSchema(sqlConf, resultSchema)",
          "75:   private val enableRecordFilter: Boolean = sqlConf.parquetRecordFilterEnabled",
          "76:   private val timestampConversion: Boolean = sqlConf.isParquetINT96TimestampConversion",
          "77:   private val capacity = sqlConf.parquetVectorizedReaderBatchSize",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "76:   private val supportsColumnar = enableVectorizedReader && sqlConf.wholeStageEnabled &&",
          "77:     !WholeStageCodegenExec.isTooManyFields(sqlConf, resultSchema)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "104:   }",
          "106:   override def supportColumnarReads(partition: InputPartition): Boolean = {",
          "110:   }",
          "112:   override def buildReader(file: PartitionedFile): PartitionReader[InternalRow] = {",
          "",
          "[Removed Lines]",
          "107:     sqlConf.parquetVectorizedReaderEnabled && sqlConf.wholeStageEnabled &&",
          "108:       resultSchema.length <= sqlConf.wholeStageMaxNumFields &&",
          "109:       resultSchema.forall(_.dataType.isInstanceOf[AtomicType])",
          "",
          "[Added Lines]",
          "110:     supportsColumnar",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: import org.apache.spark.sql.execution.datasources.parquet.TestingUDT.{NestedStruct, NestedStructUDT, SingleElement}",
          "35: import org.apache.spark.sql.execution.datasources.v2.BatchScanExec",
          "36: import org.apache.spark.sql.execution.datasources.v2.parquet.ParquetScan",
          "37: import org.apache.spark.sql.internal.SQLConf",
          "38: import org.apache.spark.sql.test.SharedSparkSession",
          "39: import org.apache.spark.sql.types._",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "37: import org.apache.spark.sql.functions.struct",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1042:         val fileScan3 = df3.queryExecution.sparkPlan.find(_.isInstanceOf[FileSourceScanExec]).get",
          "1043:         assert(fileScan3.asInstanceOf[FileSourceScanExec].supportsColumnar)",
          "1044:         checkAnswer(df3, df.selectExpr(columns : _*))",
          "1045:       }",
          "1046:     }",
          "1047:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1047:         withSQLConf(SQLConf.PARQUET_VECTORIZED_READER_NESTED_COLUMN_ENABLED.key -> \"true\") {",
          "1048:           val df4 = spark.range(10).select(struct(",
          "1049:             Seq.tabulate(11) {i => ($\"id\" + i).as(s\"c$i\")} : _*).as(\"nested\"))",
          "1050:           df4.write.mode(SaveMode.Overwrite).parquet(path)",
          "1053:           val df5 = spark.read.parquet(path)",
          "1054:           val fileScan5 = df5.queryExecution.sparkPlan.find(_.isInstanceOf[FileSourceScanExec]).get",
          "1055:           assert(!fileScan5.asInstanceOf[FileSourceScanExec].supportsColumnar)",
          "1056:           checkAnswer(df5, df4)",
          "1059:           val columns2 = Seq.tabulate(9) {i => s\"nested.c$i\"}",
          "1060:           val df6 = df5.selectExpr(columns2 : _*)",
          "1061:           val fileScan6 = df6.queryExecution.sparkPlan.find(_.isInstanceOf[FileSourceScanExec]).get",
          "1062:           assert(fileScan6.asInstanceOf[FileSourceScanExec].supportsColumnar)",
          "1063:           checkAnswer(df6, df4.selectExpr(columns2 : _*))",
          "1064:         }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1079:         val parquetScan3 = fileScan3.asInstanceOf[BatchScanExec].scan.asInstanceOf[ParquetScan]",
          "1080:         assert(parquetScan3.createReaderFactory().supportColumnarReads(null))",
          "1081:         checkAnswer(df3, df.selectExpr(columns : _*))",
          "1082:       }",
          "1083:     }",
          "1084:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1103:         withSQLConf(SQLConf.PARQUET_VECTORIZED_READER_NESTED_COLUMN_ENABLED.key -> \"true\") {",
          "1104:           val df4 = spark.range(10).select(struct(",
          "1105:             Seq.tabulate(11) {i => ($\"id\" + i).as(s\"c$i\")} : _*).as(\"nested\"))",
          "1106:           df4.write.mode(SaveMode.Overwrite).parquet(path)",
          "1109:           val df5 = spark.read.parquet(path)",
          "1110:           val fileScan5 = df5.queryExecution.sparkPlan.find(_.isInstanceOf[BatchScanExec]).get",
          "1111:           val parquetScan5 = fileScan5.asInstanceOf[BatchScanExec].scan.asInstanceOf[ParquetScan]",
          "1114:           assert(!parquetScan5.createReaderFactory().supportColumnarReads(null))",
          "1115:           checkAnswer(df5, df4)",
          "1118:           val columns2 = Seq.tabulate(9) {i => s\"nested.c$i\"}",
          "1119:           val df6 = df5.selectExpr(columns2 : _*)",
          "1120:           val fileScan6 = df6.queryExecution.sparkPlan.find(_.isInstanceOf[BatchScanExec]).get",
          "1121:           val parquetScan6 = fileScan6.asInstanceOf[BatchScanExec].scan.asInstanceOf[ParquetScan]",
          "1122:           assert(parquetScan6.createReaderFactory().supportColumnarReads(null))",
          "1123:           checkAnswer(df6, df4.selectExpr(columns2 : _*))",
          "1124:         }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaPruningSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaPruningSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaPruningSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaPruningSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "32:   override protected val vectorizedReaderEnabledKey: String =",
          "33:     SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key",
          "34:   override protected val vectorizedReaderNestedEnabledKey: String =",
          "37: }",
          "",
          "[Removed Lines]",
          "35:     SQLConf.ORC_VECTORIZED_READER_NESTED_COLUMN_ENABLED.key",
          "",
          "[Added Lines]",
          "35:     SQLConf.PARQUET_VECTORIZED_READER_NESTED_COLUMN_ENABLED.key",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c25624b4d0c2d77f0a6db7e70ecf750e9a1143f2",
      "candidate_info": {
        "commit_hash": "c25624b4d0c2d77f0a6db7e70ecf750e9a1143f2",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/c25624b4d0c2d77f0a6db7e70ecf750e9a1143f2",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CollapseProjectSuite.scala"
        ],
        "message": "[SPARK-36718][SQL][FOLLOWUP] Improve the extract-only check in CollapseProject\n\n### What changes were proposed in this pull request?\n\nThis is a followup of https://github.com/apache/spark/pull/36510 , to fix a corner case: if the `CreateStruct` is only referenced once in non-extract expressions, we should still allow collapsing the projects.\n\n### Why are the changes needed?\n\ncompletely fix the perf regression\n\n### Does this PR introduce _any_ user-facing change?\n\nno\n\n### How was this patch tested?\n\na new test\n\nCloses #36572 from cloud-fan/regression.\n\nAuthored-by: Wenchen Fan <wenchen@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 98fad57221d4dffc6f1fe28d9aca1093172ecf72)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CollapseProjectSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CollapseProjectSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "991:           val producer = producerMap.getOrElse(reference, reference)",
          "992:           producer.deterministic && (count == 1 || alwaysInline || {",
          "993:             val relatedConsumers = consumers.filter(_.references.contains(reference))",
          "995:             shouldInline(producer, extractOnly)",
          "996:           })",
          "997:       }",
          "998:   }",
          "1006:     }",
          "1008:   }",
          "",
          "[Removed Lines]",
          "994:             val extractOnly = relatedConsumers.forall(isExtractOnly(_, reference))",
          "1000:   private def isExtractOnly(expr: Expression, ref: Attribute): Boolean = {",
          "1001:     def hasRefInNonExtractValue(e: Expression): Boolean = e match {",
          "1002:       case a: Attribute => a.semanticEquals(ref)",
          "1004:       case e: ExtractValue if e.children.head.semanticEquals(ref) => false",
          "1005:       case _ => e.children.exists(hasRefInNonExtractValue)",
          "1007:     !hasRefInNonExtractValue(expr)",
          "",
          "[Added Lines]",
          "996:             val extractOnly = relatedConsumers.map(refCountInNonExtract(_, reference)).sum <= 1",
          "1002:   private def refCountInNonExtract(expr: Expression, ref: Attribute): Int = {",
          "1003:     def refCount(e: Expression): Int = e match {",
          "1004:       case a: Attribute if a.semanticEquals(ref) => 1",
          "1006:       case e: ExtractValue if e.children.head.semanticEquals(ref) => 0",
          "1007:       case _ => e.children.map(refCount).sum",
          "1009:     refCount(expr)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CollapseProjectSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CollapseProjectSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CollapseProjectSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/CollapseProjectSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "141:       .select(($\"a\" + ($\"a\" + 1)).as(\"add\"))",
          "142:       .analyze",
          "143:     comparePlans(optimized2, expected2)",
          "144:   }",
          "146:   test(\"preserve top-level alias metadata while collapsing projects\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "146:     val query3 = testRelation",
          "147:       .select(namedStruct(\"a\", $\"a\", \"a_plus_1\", $\"a\" + 1).as(\"struct\"))",
          "148:       .select($\"struct\", $\"struct\".getField(\"a\"))",
          "149:       .analyze",
          "150:     val optimized3 = Optimize.execute(query3)",
          "151:     val expected3 = testRelation",
          "152:       .select(namedStruct(\"a\", $\"a\", \"a_plus_1\", $\"a\" + 1).as(\"struct\"), $\"a\".as(\"struct.a\"))",
          "153:       .analyze",
          "154:     comparePlans(optimized3, expected3)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "fae6a1c5e08ef75bd480f7ce2569b4b3959259ea",
      "candidate_info": {
        "commit_hash": "fae6a1c5e08ef75bd480f7ce2569b4b3959259ea",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/fae6a1c5e08ef75bd480f7ce2569b4b3959259ea",
        "files": [
          "common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala",
          "sql/core/src/test/resources/sql-tests/results/ansi/cast.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/date.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/string-functions.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/float4.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/float8.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/int8.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/text.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part2.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part4.sql.out",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala"
        ],
        "message": "[SPARK-38791][SQL][3.3] Output parameter values of error classes in the SQL style\n\n### What changes were proposed in this pull request?\nIn the PR, I propose new trait `QueryErrorsBase` which is supposed to be used by `Query.*Errors`, and new method `toSQLValue()`. The method converts a parameter value of error classes to its SQL representation.\n\n### Why are the changes needed?\nTo improve user experience with Spark SQL. Users should see values in error messages in unified SQL style.\n\n### Does this PR introduce _any_ user-facing change?\nYes.\n\n### How was this patch tested?\nBy running the modified test suites:\n```\n$ build/sbt \"test:testOnly *QueryExecutionErrorsSuite\"\n```\n\nAuthored-by: Max Gekk <max.gekkgmail.com>\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n(cherry picked from commit bc8c264851457d8ef59f5b332c79296651ec5d1e)\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n\nCloses #36143 from MaxGekk/cleanup-error-classes-3.3.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java||common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java||common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java": [
          "File: common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java -> common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "1315:     if (toLong(result, false)) {",
          "1316:       return result.value;",
          "1317:     }",
          "1319:   }",
          "",
          "[Removed Lines]",
          "1318:     throw new NumberFormatException(\"invalid input syntax for type numeric: \" + this);",
          "",
          "[Added Lines]",
          "1318:     throw new NumberFormatException(\"invalid input syntax for type numeric: '\" + this + \"'\");",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1329:     if (toInt(result, false)) {",
          "1330:       return result.value;",
          "1331:     }",
          "1333:   }",
          "1335:   public short toShortExact() {",
          "",
          "[Removed Lines]",
          "1332:     throw new NumberFormatException(\"invalid input syntax for type numeric: \" + this);",
          "",
          "[Added Lines]",
          "1332:     throw new NumberFormatException(\"invalid input syntax for type numeric: '\" + this + \"'\");",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1338:     if (result == value) {",
          "1339:       return result;",
          "1340:     }",
          "1342:   }",
          "1344:   public byte toByteExact() {",
          "",
          "[Removed Lines]",
          "1341:     throw new NumberFormatException(\"invalid input syntax for type numeric: \" + this);",
          "",
          "[Added Lines]",
          "1341:     throw new NumberFormatException(\"invalid input syntax for type numeric: '\" + this + \"'\");",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1347:     if (result == value) {",
          "1348:       return result;",
          "1349:     }",
          "1351:   }",
          "1353:   @Override",
          "",
          "[Removed Lines]",
          "1350:     throw new NumberFormatException(\"invalid input syntax for type numeric: \" + this);",
          "",
          "[Added Lines]",
          "1350:     throw new NumberFormatException(\"invalid input syntax for type numeric: '\" + this + \"'\");",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/IntervalUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1289:     val vShort = vInt.toShort",
          "1290:     if (vInt != vShort) {",
          "1291:       throw QueryExecutionErrors.castingCauseOverflowError(",
          "1293:     }",
          "1294:     vShort",
          "1295:   }",
          "",
          "[Removed Lines]",
          "1292:         toYearMonthIntervalString(v, ANSI_STYLE, startField, endField), ShortType)",
          "",
          "[Added Lines]",
          "1292:         Literal(v, YearMonthIntervalType(startField, endField)),",
          "1293:         ShortType)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1299:     val vByte = vInt.toByte",
          "1300:     if (vInt != vByte) {",
          "1301:       throw QueryExecutionErrors.castingCauseOverflowError(",
          "1303:     }",
          "1304:     vByte",
          "1305:   }",
          "",
          "[Removed Lines]",
          "1302:         toYearMonthIntervalString(v, ANSI_STYLE, startField, endField), ByteType)",
          "",
          "[Added Lines]",
          "1303:         Literal(v, YearMonthIntervalType(startField, endField)),",
          "1304:         ByteType)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1347:     val vInt = vLong.toInt",
          "1348:     if (vLong != vInt) {",
          "1349:       throw QueryExecutionErrors.castingCauseOverflowError(",
          "1351:     }",
          "1352:     vInt",
          "1353:   }",
          "",
          "[Removed Lines]",
          "1350:         toDayTimeIntervalString(v, ANSI_STYLE, startField, endField), IntegerType)",
          "",
          "[Added Lines]",
          "1352:         Literal(v, DayTimeIntervalType(startField, endField)),",
          "1353:         IntegerType)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1357:     val vShort = vLong.toShort",
          "1358:     if (vLong != vShort) {",
          "1359:       throw QueryExecutionErrors.castingCauseOverflowError(",
          "1361:     }",
          "1362:     vShort",
          "1363:   }",
          "",
          "[Removed Lines]",
          "1360:         toDayTimeIntervalString(v, ANSI_STYLE, startField, endField), ShortType)",
          "",
          "[Added Lines]",
          "1363:         Literal(v, DayTimeIntervalType(startField, endField)),",
          "1364:         ShortType)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1367:     val vByte = vLong.toByte",
          "1368:     if (vLong != vByte) {",
          "1369:       throw QueryExecutionErrors.castingCauseOverflowError(",
          "1371:     }",
          "1372:     vByte",
          "1373:   }",
          "",
          "[Removed Lines]",
          "1370:         toDayTimeIntervalString(v, ANSI_STYLE, startField, endField), ByteType)",
          "",
          "[Added Lines]",
          "1374:         Literal(v, DayTimeIntervalType(startField, endField)),",
          "1375:         ByteType)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: package org.apache.spark.sql.errors",
          "20: import org.apache.spark.sql.catalyst.expressions.Literal",
          "21: import org.apache.spark.sql.types.{DataType, DoubleType, FloatType}",
          "23: trait QueryErrorsBase {",
          "24:   private def litToErrorValue(l: Literal): String = l match {",
          "25:     case Literal(null, _) => \"NULL\"",
          "26:     case Literal(v: Float, FloatType) =>",
          "27:       if (v.isNaN) \"NaN\"",
          "28:       else if (v.isPosInfinity) \"Infinity\"",
          "29:       else if (v.isNegInfinity) \"-Infinity\"",
          "30:       else v.toString",
          "31:     case Literal(v: Double, DoubleType) =>",
          "32:       if (v.isNaN) \"NaN\"",
          "33:       else if (v.isPosInfinity) \"Infinity\"",
          "34:       else if (v.isNegInfinity) \"-Infinity\"",
          "35:       else l.sql",
          "36:     case l => l.sql",
          "37:   }",
          "40:   def toSQLValue(v: Any): String = {",
          "41:     litToErrorValue(Literal(v))",
          "42:   }",
          "44:   def toSQLValue(v: Any, t: DataType): String = {",
          "45:     litToErrorValue(Literal.create(v, t))",
          "46:   }",
          "47: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "71:   def logicalHintOperatorNotRemovedDuringAnalysisError(): Throwable = {",
          "72:     new SparkIllegalStateException(errorClass = \"INTERNAL_ERROR\",",
          "",
          "[Removed Lines]",
          "69: object QueryExecutionErrors {",
          "",
          "[Added Lines]",
          "69: object QueryExecutionErrors extends QueryErrorsBase {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "92:   def castingCauseOverflowError(t: Any, dataType: DataType): ArithmeticException = {",
          "93:     new SparkArithmeticException(errorClass = \"CAST_CAUSES_OVERFLOW\",",
          "95:   }",
          "97:   def cannotChangeDecimalPrecisionError(",
          "",
          "[Removed Lines]",
          "94:       messageParameters = Array(t.toString, dataType.catalogString, SQLConf.ANSI_ENABLED.key))",
          "",
          "[Added Lines]",
          "94:       messageParameters = Array(toSQLValue(t), dataType.catalogString, SQLConf.ANSI_ENABLED.key))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "112:   def invalidInputSyntaxForNumericError(s: UTF8String): NumberFormatException = {",
          "113:     new SparkNumberFormatException(errorClass = \"INVALID_INPUT_SYNTAX_FOR_NUMERIC_TYPE\",",
          "115:   }",
          "117:   def cannotCastFromNullTypeError(to: DataType): Throwable = {",
          "",
          "[Removed Lines]",
          "114:       messageParameters = Array(s.toString, SQLConf.ANSI_ENABLED.key))",
          "",
          "[Added Lines]",
          "114:       messageParameters = Array(toSQLValue(s, StringType), SQLConf.ANSI_ENABLED.key))",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "158:       numElements: Int,",
          "159:       key: String): ArrayIndexOutOfBoundsException = {",
          "160:     new SparkArrayIndexOutOfBoundsException(errorClass = \"INVALID_ARRAY_INDEX\",",
          "162:   }",
          "164:   def invalidElementAtIndexError(",
          "165:        index: Int,",
          "166:        numElements: Int): ArrayIndexOutOfBoundsException = {",
          "169:   }",
          "171:   def mapKeyNotExistError(",
          "",
          "[Removed Lines]",
          "161:       messageParameters = Array(index.toString, numElements.toString, key))",
          "167:     new SparkArrayIndexOutOfBoundsException(errorClass = \"INVALID_ARRAY_INDEX_IN_ELEMENT_AT\",",
          "168:       messageParameters = Array(index.toString, numElements.toString, SQLConf.ANSI_ENABLED.key))",
          "",
          "[Added Lines]",
          "161:       messageParameters = Array(toSQLValue(index), toSQLValue(numElements), key))",
          "167:     new SparkArrayIndexOutOfBoundsException(",
          "168:       errorClass = \"INVALID_ARRAY_INDEX_IN_ELEMENT_AT\",",
          "169:       messageParameters =",
          "170:         Array(toSQLValue(index), toSQLValue(numElements), SQLConf.ANSI_ENABLED.key))",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "174:       context: String): NoSuchElementException = {",
          "175:     if (isElementAtFunction) {",
          "176:       new SparkNoSuchElementException(errorClass = \"MAP_KEY_DOES_NOT_EXIST_IN_ELEMENT_AT\",",
          "178:     } else {",
          "179:       new SparkNoSuchElementException(errorClass = \"MAP_KEY_DOES_NOT_EXIST\",",
          "181:     }",
          "182:   }",
          "",
          "[Removed Lines]",
          "177:         messageParameters = Array(key.toString, SQLConf.ANSI_ENABLED.key, context))",
          "180:         messageParameters = Array(key.toString, SQLConf.ANSI_STRICT_INDEX_OPERATOR.key, context))",
          "",
          "[Added Lines]",
          "179:         messageParameters = Array(toSQLValue(key), SQLConf.ANSI_ENABLED.key, context))",
          "182:         messageParameters = Array(toSQLValue(key), SQLConf.ANSI_STRICT_INDEX_OPERATOR.key, context))",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "457:   }",
          "459:   def unaryMinusCauseOverflowError(originValue: AnyVal): ArithmeticException = {",
          "461:   }",
          "463:   def binaryArithmeticCauseOverflowError(",
          "464:       eval1: Short, symbol: String, eval2: Short): ArithmeticException = {",
          "466:   }",
          "468:   def failedSplitSubExpressionMsg(length: Int): String = {",
          "",
          "[Removed Lines]",
          "460:     arithmeticOverflowError(s\"- $originValue caused overflow\")",
          "465:     arithmeticOverflowError(s\"$eval1 $symbol $eval2 caused overflow\")",
          "",
          "[Added Lines]",
          "462:     arithmeticOverflowError(s\"- ${toSQLValue(originValue)} caused overflow\")",
          "467:     arithmeticOverflowError(s\"${toSQLValue(eval1)} $symbol ${toSQLValue(eval2)} caused overflow\")",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "1070:   def cannotParseStringAsDataTypeError(pattern: String, value: String, dataType: DataType)",
          "1071:   : Throwable = {",
          "1072:     new RuntimeException(",
          "1074:         s\"as target spark data type [$dataType].\")",
          "1075:   }",
          "",
          "[Removed Lines]",
          "1073:       s\"Cannot parse field value ${value} for pattern ${pattern} \" +",
          "",
          "[Added Lines]",
          "1075:       s\"Cannot parse field value ${toSQLValue(value)} for pattern ${toSQLValue(pattern)} \" +",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "1135:   }",
          "1137:   def paramIsNotIntegerError(paramName: String, value: String): Throwable = {",
          "1139:   }",
          "1141:   def paramIsNotBooleanValueError(paramName: String): Throwable = {",
          "",
          "[Removed Lines]",
          "1138:     new RuntimeException(s\"$paramName should be an integer. Found $value\")",
          "",
          "[Added Lines]",
          "1140:     new RuntimeException(s\"$paramName should be an integer. Found ${toSQLValue(value)}\")",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "1341:   }",
          "1343:   def indexOutOfBoundsOfArrayDataError(idx: Int): Throwable = {",
          "1345:   }",
          "1347:   def malformedRecordsDetectedInRecordParsingError(e: BadRecordException): Throwable = {",
          "",
          "[Removed Lines]",
          "1344:     new SparkIndexOutOfBoundsException(errorClass = \"INDEX_OUT_OF_BOUNDS\", Array(idx.toString))",
          "",
          "[Added Lines]",
          "1346:     new SparkIndexOutOfBoundsException(errorClass = \"INDEX_OUT_OF_BOUNDS\", Array(toSQLValue(idx)))",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1378:   }",
          "1380:   def dynamicPartitionKeyNotAmongWrittenPartitionPathsError(key: String): Throwable = {",
          "1382:   }",
          "1384:   def cannotRemovePartitionDirError(partitionPath: Path): Throwable = {",
          "",
          "[Removed Lines]",
          "1381:     new SparkException(s\"Dynamic partition key $key is not among written partition paths.\")",
          "",
          "[Added Lines]",
          "1383:     new SparkException(",
          "1384:       s\"Dynamic partition key ${toSQLValue(key)} is not among written partition paths.\")",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "1661:   }",
          "1663:   def valueIsNullError(index: Int): Throwable = {",
          "1665:   }",
          "1667:   def onlySupportDataSourcesProvidingFileFormatError(providingClass: String): Throwable = {",
          "",
          "[Removed Lines]",
          "1664:     new NullPointerException(s\"Value at index $index is null\")",
          "",
          "[Added Lines]",
          "1667:     new NullPointerException(s\"Value at index ${toSQLValue(index)} is null\")",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "2005:   def timestampAddOverflowError(micros: Long, amount: Int, unit: String): ArithmeticException = {",
          "2006:     new SparkArithmeticException(",
          "2007:       errorClass = \"DATETIME_OVERFLOW\",",
          "2009:   }",
          "2010: }",
          "",
          "[Removed Lines]",
          "2008:       messageParameters = Array(s\"add $amount $unit to '${DateTimeUtils.microsToInstant(micros)}'\"))",
          "",
          "[Added Lines]",
          "2011:       messageParameters = Array(",
          "2012:         s\"add ${toSQLValue(amount)} $unit to ${toSQLValue(DateTimeUtils.microsToInstant(micros))}\"))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "176:     Seq(IntegerType, ShortType, ByteType, LongType).foreach { dataType =>",
          "177:       checkExceptionInExpression[NumberFormatException](",
          "179:       checkExceptionInExpression[NumberFormatException](",
          "181:       checkExceptionInExpression[NumberFormatException](",
          "183:       checkExceptionInExpression[NumberFormatException](",
          "185:     }",
          "187:     Seq(DoubleType, FloatType, DecimalType.USER_DEFAULT).foreach { dataType =>",
          "188:       checkExceptionInExpression[NumberFormatException](",
          "190:       checkExceptionInExpression[NumberFormatException](",
          "192:       checkExceptionInExpression[NumberFormatException](",
          "194:     }",
          "195:   }",
          "197:   protected def checkCastToNumericError(l: Literal, to: DataType, tryCastResult: Any): Unit = {",
          "198:     checkExceptionInExpression[NumberFormatException](",
          "200:   }",
          "202:   test(\"cast from invalid string array to numeric array should throw NumberFormatException\") {",
          "",
          "[Removed Lines]",
          "178:         cast(\"string\", dataType), \"invalid input syntax for type numeric: string\")",
          "180:         cast(\"123-string\", dataType), \"invalid input syntax for type numeric: 123-string\")",
          "182:         cast(\"2020-07-19\", dataType), \"invalid input syntax for type numeric: 2020-07-19\")",
          "184:         cast(\"1.23\", dataType), \"invalid input syntax for type numeric: 1.23\")",
          "189:         cast(\"string\", dataType), \"invalid input syntax for type numeric: string\")",
          "191:         cast(\"123.000.00\", dataType), \"invalid input syntax for type numeric: 123.000.00\")",
          "193:         cast(\"abc.com\", dataType), \"invalid input syntax for type numeric: abc.com\")",
          "199:       cast(l, to), \"invalid input syntax for type numeric: true\")",
          "",
          "[Added Lines]",
          "178:         cast(\"string\", dataType), \"invalid input syntax for type numeric: 'string'\")",
          "180:         cast(\"123-string\", dataType), \"invalid input syntax for type numeric: '123-string'\")",
          "182:         cast(\"2020-07-19\", dataType), \"invalid input syntax for type numeric: '2020-07-19'\")",
          "184:         cast(\"1.23\", dataType), \"invalid input syntax for type numeric: '1.23'\")",
          "189:         cast(\"string\", dataType), \"invalid input syntax for type numeric: 'string'\")",
          "191:         cast(\"123.000.00\", dataType), \"invalid input syntax for type numeric: '123.000.00'\")",
          "193:         cast(\"abc.com\", dataType), \"invalid input syntax for type numeric: 'abc.com'\")",
          "199:       cast(l, to), \"invalid input syntax for type numeric: 'true'\")",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CastSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "600:       val e3 = intercept[ArithmeticException] {",
          "601:         Cast(Literal(Int.MaxValue + 1L), IntegerType).eval()",
          "602:       }.getMessage",
          "604:     }",
          "605:   }",
          "",
          "[Removed Lines]",
          "603:       assert(e3.contains(\"Casting 2147483648 to int causes overflow\"))",
          "",
          "[Added Lines]",
          "603:       assert(e3.contains(\"Casting 2147483648L to int causes overflow\"))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "774:     Seq(",
          "775:       (Int.MaxValue, DayTimeIntervalType(DAY)),",
          "777:       (Long.MaxValue, DayTimeIntervalType(DAY)),",
          "778:       (Long.MinValue, DayTimeIntervalType(DAY)),",
          "779:       (Long.MaxValue, DayTimeIntervalType(HOUR)),",
          "",
          "[Removed Lines]",
          "776:       (Int.MinValue, DayTimeIntervalType(DAY)),",
          "",
          "[Added Lines]",
          "776:       (Int.MinValue, DayTimeIntervalType(DAY))",
          "777:     ).foreach {",
          "778:       case (v, toType) =>",
          "779:         checkExceptionInExpression[ArithmeticException](cast(v, toType),",
          "780:           s\"Casting $v to ${toType.catalogString} causes overflow\")",
          "781:     }",
          "783:     Seq(",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "785:     ).foreach {",
          "786:       case (v, toType) =>",
          "787:         checkExceptionInExpression[ArithmeticException](cast(v, toType),",
          "789:     }",
          "790:   }",
          "",
          "[Removed Lines]",
          "788:           s\"Casting $v to ${toType.catalogString} causes overflow\")",
          "",
          "[Added Lines]",
          "795:           s\"Casting ${v}L to ${toType.catalogString} causes overflow\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "877:     Seq(",
          "878:       (Int.MaxValue, YearMonthIntervalType(YEAR)),",
          "880:       (Long.MaxValue, YearMonthIntervalType(YEAR)),",
          "881:       (Long.MinValue, YearMonthIntervalType(YEAR)),",
          "882:       (Long.MaxValue, YearMonthIntervalType(MONTH)),",
          "",
          "[Removed Lines]",
          "879:       (Int.MinValue, YearMonthIntervalType(YEAR)),",
          "",
          "[Added Lines]",
          "886:       (Int.MinValue, YearMonthIntervalType(YEAR))",
          "887:     ).foreach {",
          "888:       case (v, toType) =>",
          "889:         checkExceptionInExpression[ArithmeticException](cast(v, toType),",
          "890:           s\"Casting $v to ${toType.catalogString} causes overflow\")",
          "891:     }",
          "893:     Seq(",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "884:     ).foreach {",
          "885:       case (v, toType) =>",
          "886:         checkExceptionInExpression[ArithmeticException](cast(v, toType),",
          "888:     }",
          "889:   }",
          "890: }",
          "",
          "[Removed Lines]",
          "887:           s\"Casting $v to ${toType.catalogString} causes overflow\")",
          "",
          "[Added Lines]",
          "901:           s\"Casting ${v}L to ${toType.catalogString} causes overflow\")",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "302:             val errorMsg = intercept[NumberFormatException] {",
          "303:               sql(\"insert into t partition(a='ansi') values('ansi')\")",
          "304:             }.getMessage",
          "306:           } else {",
          "307:             sql(\"insert into t partition(a='ansi') values('ansi')\")",
          "308:             checkAnswer(sql(\"select * from t\"), Row(\"ansi\", null) :: Nil)",
          "",
          "[Removed Lines]",
          "305:             assert(errorMsg.contains(\"invalid input syntax for type numeric: ansi\"))",
          "",
          "[Added Lines]",
          "305:             assert(errorMsg.contains(\"invalid input syntax for type numeric: 'ansi'\"))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "276:     assert(e.getErrorClass === \"DATETIME_OVERFLOW\")",
          "277:     assert(e.getSqlState === \"22008\")",
          "278:     assert(e.getMessage ===",
          "280:   }",
          "281: }",
          "",
          "[Removed Lines]",
          "279:       \"Datetime operation overflow: add 1000000 YEAR to '2022-03-09T09:02:03Z'.\")",
          "",
          "[Added Lines]",
          "279:       \"Datetime operation overflow: add 1000000 YEAR to TIMESTAMP '2022-03-09 01:02:03'.\")",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "713:         var msg = intercept[SparkException] {",
          "714:           sql(s\"insert into t values($outOfRangeValue1)\")",
          "715:         }.getCause.getMessage",
          "718:         val outOfRangeValue2 = (Int.MinValue - 1L).toString",
          "719:         msg = intercept[SparkException] {",
          "720:           sql(s\"insert into t values($outOfRangeValue2)\")",
          "721:         }.getCause.getMessage",
          "723:       }",
          "724:     }",
          "725:   }",
          "",
          "[Removed Lines]",
          "716:         assert(msg.contains(s\"Casting $outOfRangeValue1 to int causes overflow\"))",
          "722:         assert(msg.contains(s\"Casting $outOfRangeValue2 to int causes overflow\"))",
          "",
          "[Added Lines]",
          "716:         assert(msg.contains(s\"Casting ${outOfRangeValue1}L to int causes overflow\"))",
          "722:         assert(msg.contains(s\"Casting ${outOfRangeValue2}L to int causes overflow\"))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "733:         var msg = intercept[SparkException] {",
          "734:           sql(s\"insert into t values(${outOfRangeValue1}D)\")",
          "735:         }.getCause.getMessage",
          "738:         val outOfRangeValue2 = Math.nextDown(Long.MinValue)",
          "739:         msg = intercept[SparkException] {",
          "740:           sql(s\"insert into t values(${outOfRangeValue2}D)\")",
          "741:         }.getCause.getMessage",
          "743:       }",
          "744:     }",
          "745:   }",
          "",
          "[Removed Lines]",
          "736:         assert(msg.contains(s\"Casting $outOfRangeValue1 to bigint causes overflow\"))",
          "742:         assert(msg.contains(s\"Casting $outOfRangeValue2 to bigint causes overflow\"))",
          "",
          "[Added Lines]",
          "736:         assert(msg.contains(s\"Casting ${outOfRangeValue1}D to bigint causes overflow\"))",
          "742:         assert(msg.contains(s\"Casting ${outOfRangeValue2}D to bigint causes overflow\"))",
          "",
          "---------------"
        ]
      }
    }
  ]
}