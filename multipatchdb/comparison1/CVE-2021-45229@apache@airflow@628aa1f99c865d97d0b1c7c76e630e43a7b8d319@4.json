{
  "cve_id": "CVE-2021-45229",
  "cve_desc": "It was discovered that the \"Trigger DAG with config\" screen was susceptible to XSS attacks via the `origin` query argument. This issue affects Apache Airflow versions 2.2.3 and below.",
  "repo": "apache/airflow",
  "patch_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
  "patch_info": {
    "commit_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "files": [
      "airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py"
    ],
    "message": "Simplify trigger cancel button (#21591)\n\nCo-authored-by: Jed Cunningham <jedcunningham@apache.org>\n(cherry picked from commit 65297673a318660fba76797e50d0c06804dfcafc)",
    "before_after_code_files": [
      "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
    ]
  },
  "patch_diff": {
    "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html": [
      "File: airflow/www/templates/airflow/trigger.html -> airflow/www/templates/airflow/trigger.html",
      "--- Hunk 1 ---",
      "[Context before]",
      "63:       </label>",
      "64:     </div>",
      "65:     <button type=\"submit\" class=\"btn btn-primary\">Trigger</button>",
      "67:   </form>",
      "68: {% endblock %}",
      "",
      "[Removed Lines]",
      "66:     <button type=\"button\" class=\"btn\" onclick=\"location.href = '{{ origin }}'; return false\">Cancel</button>",
      "",
      "[Added Lines]",
      "66:     <a class=\"btn\" href=\"{{ origin }}\">Cancel</a>",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
      "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "133:         (\"javascript:alert(1)\", \"/home\"),",
      "134:         (\"http://google.com\", \"/home\"),",
      "135:         (\"36539'%3balert(1)%2f%2f166\", \"/home\"),",
      "136:         (",
      "137:             \"%2Ftree%3Fdag_id%3Dexample_bash_operator';alert(33)//\",",
      "138:             \"/home\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "136:         (",
      "137:             '\"><script>alert(99)</script><a href=\"',",
      "138:             \"&#34;&gt;&lt;script&gt;alert(99)&lt;/script&gt;&lt;a href=&#34;\",",
      "139:         ),",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "145:     test_dag_id = \"example_bash_operator\"",
      "147:     resp = admin_client.get(f'trigger?dag_id={test_dag_id}&origin={test_origin}')",
      "156: @pytest.mark.parametrize(",
      "",
      "[Removed Lines]",
      "148:     check_content_in_response(",
      "149:         '<button type=\"button\" class=\"btn\" onclick=\"location.href = \\'{}\\'; return false\">'.format(",
      "150:             expected_origin",
      "151:         ),",
      "152:         resp,",
      "153:     )",
      "",
      "[Added Lines]",
      "152:     check_content_in_response(f'<a class=\"btn\" href=\"{expected_origin}\">Cancel</a>', resp)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "23b2e7bf2eeee658e755726111e00d1535d38ef4",
      "candidate_info": {
        "commit_hash": "23b2e7bf2eeee658e755726111e00d1535d38ef4",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/23b2e7bf2eeee658e755726111e00d1535d38ef4",
        "files": [
          "dev/check_files.py"
        ],
        "message": "Fix release check script (#19238)\n\nThere have been some changes to the filename conventions over time  and the release check script was not updated to reflect this.  This PR fixes the script and tries to simplify it a little bit.  In particular, the regex approach used previously was broken by the removal of the `-bin` identifier.  It is easy enough to simply compute all the expected files exactly and look for them, so that is what we do here\n\n(cherry picked from commit b49b81ac8466922cb704f989910e1811b7cb4fa9)",
        "before_after_code_files": [
          "dev/check_files.py||dev/check_files.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/check_files.py||dev/check_files.py": [
          "File: dev/check_files.py -> dev/check_files.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: import os",
          "19: import re",
          "20: from typing import List",
          "22: import click as click",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: from itertools import product",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "46: \"\"\"",
          "49: DOCKER_CMD = \"\"\"",
          "50: docker build --tag local/airflow .",
          "51: docker local/airflow info",
          "52: \"\"\"",
          "55: AIRFLOW = \"AIRFLOW\"",
          "56: PROVIDERS = \"PROVIDERS\"",
          "57: UPGRADE_CHECK = \"UPGRADE_CHECK\"",
          "64: def get_packages() -> List[str]:",
          "68:     if not content:",
          "69:         raise SystemExit(\"List of packages to check is empty. Please add packages to `packages.txt`\")",
          "",
          "[Removed Lines]",
          "59: ASC = re.compile(r\".*\\.asc$\")",
          "60: SHA = re.compile(r\".*\\.sha512$\")",
          "61: NORM = re.compile(r\".*\\.(whl|gz)$\")",
          "65:     with open(\"packages.txt\") as file:",
          "66:         content = file.read()",
          "",
          "[Added Lines]",
          "60:     try:",
          "61:         with open(\"packages.txt\") as file:",
          "62:             content = file.read()",
          "63:     except FileNotFoundError:",
          "64:         content = ''",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "86:     )",
          "138: def check_upgrade_check(files: List[str], version: str):",
          "151: @click.command()",
          "",
          "[Removed Lines]",
          "89: def check_all_present(prefix: str, files: List[str]):",
          "90:     all_present = True",
          "91:     for ext in [ASC, SHA, NORM]:",
          "92:         if any(re.match(ext, f) for f in files):",
          "93:             print(f\"    - {prefix} {ext.pattern}: [green]OK[/green]\")",
          "94:         else:",
          "95:             print(f\"    - {prefix} {ext.pattern}: [red]MISSING[/red]\")",
          "96:             all_present = False",
          "97:     return all_present",
          "100: def filter_files(files: List[str], prefix: str):",
          "101:     return [f for f in files if f.startswith(prefix)]",
          "104: def check_providers(files: List[str], version: str):",
          "105:     name_tpl = \"apache_airflow_providers_{}-{}\"",
          "106:     pip_packages = []",
          "107:     for p in get_packages():",
          "108:         print(p)",
          "110:         name = name_tpl.format(p.replace(\".\", \"_\"), version)",
          "111:         # Check sources",
          "112:         check_all_present(\"sources\", filter_files(files, name))",
          "114:         # Check wheels",
          "115:         name = name.replace(\"_\", \"-\")",
          "116:         if check_all_present(\"wheel\", filter_files(files, name)):",
          "117:             pip_packages.append(f\"{name.rpartition('-')[0]}=={version}\")",
          "119:     return pip_packages",
          "122: def check_release(files: List[str], version: str):",
          "123:     print(f\"apache_airflow-{version}\")",
          "125:     # Check bin",
          "126:     name = f\"apache-airflow-{version}-bin\"",
          "127:     check_all_present(\"binaries\", filter_files(files, name))",
          "129:     # Check sources",
          "130:     name = f\"apache-airflow-{version}-source\"",
          "131:     check_all_present(\"sources\", filter_files(files, name))",
          "133:     # Check wheels",
          "134:     name = f\"apache_airflow-{version}-py\"",
          "135:     check_all_present(\"wheel\", filter_files(files, name))",
          "139:     print(f\"apache_airflow-upgrade-check-{version}\")",
          "141:     name = f\"apache-airflow-upgrade-check-{version}-bin\"",
          "142:     check_all_present(\"binaries\", filter_files(files, name))",
          "144:     name = f\"apache-airflow-upgrade-check-{version}-source\"",
          "145:     check_all_present(\"sources\", filter_files(files, name))",
          "147:     name = f\"apache_airflow_upgrade_check-{version}-py\"",
          "148:     check_all_present(\"wheel\", filter_files(files, name))",
          "",
          "[Added Lines]",
          "86: def check_providers(files: List[str], version: str):",
          "87:     print(f\"Checking providers for version {version}:\\n\")",
          "88:     version = strip_rc_suffix(version)",
          "89:     missing_list = []",
          "90:     for p in get_packages():",
          "91:         print(p)",
          "92:         expected_files = expand_name_variations(",
          "93:             [",
          "94:                 f\"{p}-{version}.tar.gz\",",
          "95:                 f\"{p.replace('-', '_')}-{version}-py3-none-any.whl\",",
          "96:             ]",
          "97:         )",
          "99:         missing_list.extend(check_all_files(expected_files=expected_files, actual_files=files))",
          "101:     return missing_list",
          "104: def strip_rc_suffix(version):",
          "105:     return re.sub(r'rc\\d+$', '', version)",
          "108: def print_status(file, is_found: bool):",
          "109:     color, status = ('green', 'OK') if is_found else ('red', 'MISSING')",
          "110:     print(f\"    - {file}: [{color}]{status}[/{color}]\")",
          "113: def check_all_files(actual_files, expected_files):",
          "114:     missing_list = []",
          "115:     for file in expected_files:",
          "116:         is_found = file in actual_files",
          "117:         if not is_found:",
          "118:             missing_list.append(file)",
          "119:         print_status(file=file, is_found=is_found)",
          "120:     return missing_list",
          "123: def check_release(files: List[str], version: str):",
          "124:     print(f\"Checking airflow release for version {version}:\\n\")",
          "125:     version = strip_rc_suffix(version)",
          "127:     expected_files = expand_name_variations(",
          "128:         [",
          "129:             f\"apache-airflow-{version}.tar.gz\",",
          "130:             f\"apache-airflow-{version}-source.tar.gz\",",
          "131:             f\"apache_airflow-{version}-py3-none-any.whl\",",
          "132:         ]",
          "133:     )",
          "134:     return check_all_files(expected_files=expected_files, actual_files=files)",
          "137: def expand_name_variations(files):",
          "138:     return list(sorted(base + suffix for base, suffix in product(files, ['', '.asc', '.sha512'])))",
          "142:     print(f\"Checking upgrade_check for version {version}:\\n\")",
          "143:     version = strip_rc_suffix(version)",
          "145:     expected_files = expand_name_variations(",
          "146:         [",
          "147:             f\"apache-airflow-upgrade-check-{version}-bin.tar.gz\",",
          "148:             f\"apache-airflow-upgrade-check-{version}-source.tar.gz\",",
          "149:             f\"apache_airflow_upgrade_check-{version}-py2.py3-none-any.whl\",",
          "150:         ]",
          "151:     )",
          "152:     return check_all_files(expected_files=expected_files, actual_files=files)",
          "155: def warn_of_missing_files(files):",
          "156:     print(\"[red]Check failed. Here are the files we expected but did not find:[/red]\\n\")",
          "158:     for file in files:",
          "159:         print(f\"    - [red]{file}[/red]\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "189:     if check_type.upper() == PROVIDERS:",
          "190:         files = os.listdir(os.path.join(path, \"providers\"))",
          "192:         create_docker(PROVIDERS_DOCKER.format(\"\\n\".join(f\"RUN pip install '{p}'\" for p in pips)))",
          "193:         return",
          "195:     if check_type.upper() == AIRFLOW:",
          "196:         files = os.listdir(os.path.join(path, version))",
          "199:         base_version = version.split(\"rc\")[0]",
          "200:         prev_version = base_version[:-1] + str(int(base_version[-1]) - 1)",
          "201:         create_docker(AIRFLOW_DOCKER.format(prev_version, version))",
          "202:         return",
          "204:     if check_type.upper() == UPGRADE_CHECK:",
          "205:         files = os.listdir(os.path.join(path, \"upgrade-check\", version))",
          "208:         create_docker(DOCKER_UPGRADE.format(version))",
          "209:         return",
          "211:     raise SystemExit(f\"Unknown check type: {check_type}\")",
          "",
          "[Removed Lines]",
          "191:         pips = check_providers(files, version)",
          "197:         check_release(files, version)",
          "206:         check_upgrade_check(files, version)",
          "",
          "[Added Lines]",
          "202:         pips = [f\"{p}=={version}\" for p in get_packages()]",
          "203:         missing_files = check_providers(files, version)",
          "205:         if missing_files:",
          "206:             warn_of_missing_files(missing_files)",
          "211:         missing_files = check_release(files, version)",
          "216:         if missing_files:",
          "217:             warn_of_missing_files(missing_files)",
          "222:         missing_files = check_upgrade_check(files, version)",
          "225:         if missing_files:",
          "226:             warn_of_missing_files(missing_files)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "214: if __name__ == \"__main__\":",
          "215:     main()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "236: def test_check_release_pass():",
          "237:     \"\"\"Passes if all present\"\"\"",
          "238:     files = [",
          "239:         'apache_airflow-2.2.1-py3-none-any.whl',",
          "240:         'apache_airflow-2.2.1-py3-none-any.whl.asc',",
          "241:         'apache_airflow-2.2.1-py3-none-any.whl.sha512',",
          "242:         'apache-airflow-2.2.1-source.tar.gz',",
          "243:         'apache-airflow-2.2.1-source.tar.gz.asc',",
          "244:         'apache-airflow-2.2.1-source.tar.gz.sha512',",
          "245:         'apache-airflow-2.2.1.tar.gz',",
          "246:         'apache-airflow-2.2.1.tar.gz.asc',",
          "247:         'apache-airflow-2.2.1.tar.gz.sha512',",
          "248:     ]",
          "249:     assert check_release(files, version='2.2.1rc2') == []",
          "252: def test_check_release_fail():",
          "253:     \"\"\"Fails if missing one\"\"\"",
          "254:     files = [",
          "255:         'apache_airflow-2.2.1-py3-none-any.whl',",
          "256:         'apache_airflow-2.2.1-py3-none-any.whl.asc',",
          "257:         'apache_airflow-2.2.1-py3-none-any.whl.sha512',",
          "258:         'apache-airflow-2.2.1-source.tar.gz',",
          "259:         'apache-airflow-2.2.1-source.tar.gz.asc',",
          "260:         'apache-airflow-2.2.1-source.tar.gz.sha512',",
          "261:         'apache-airflow-2.2.1.tar.gz.asc',",
          "262:         'apache-airflow-2.2.1.tar.gz.sha512',",
          "263:     ]",
          "265:     missing_files = check_release(files, version='2.2.1rc2')",
          "266:     assert missing_files == ['apache-airflow-2.2.1.tar.gz']",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "3087dd283d98b81b96bc227b9b1b375a35c1d0f2",
      "candidate_info": {
        "commit_hash": "3087dd283d98b81b96bc227b9b1b375a35c1d0f2",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/3087dd283d98b81b96bc227b9b1b375a35c1d0f2",
        "files": [
          "scripts/in_container/check_junitxml_result.py"
        ],
        "message": "Fix MyPy errors in `scripts/in_container` (#20280)\n\n(cherry picked from commit 96212cb8f5e7e1e8caba18d92f58755a33b07a67)",
        "before_after_code_files": [
          "scripts/in_container/check_junitxml_result.py||scripts/in_container/check_junitxml_result.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/in_container/check_junitxml_result.py||scripts/in_container/check_junitxml_result.py": [
          "File: scripts/in_container/check_junitxml_result.py -> scripts/in_container/check_junitxml_result.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "19: import sys",
          "20: import xml.etree.ElementTree as ET",
          "22: TEXT_RED = '\\033[31m'",
          "23: TEXT_GREEN = '\\033[32m'",
          "24: TEXT_RESET = '\\033[0m'",
          "26: if __name__ == '__main__':",
          "27:     fname = sys.argv[1]",
          "28:     try:",
          "29:         with open(fname) as fh:",
          "30:             root = ET.parse(fh)",
          "31:         testsuite = root.find('.//testsuite')",
          "32:         if testsuite:",
          "33:             num_failures = testsuite.get('failures')",
          "34:             num_errors = testsuite.get('errors')",
          "35:             if num_failures == \"0\" and num_errors == \"0\":",
          "36:                 print(f'\\n{TEXT_GREEN}==== No errors, no failures. Good to go! ===={TEXT_RESET}\\n')",
          "37:                 sys.exit(0)",
          "38:             else:",
          "39:                 print(",
          "40:                     f'\\n{TEXT_RED}==== Errors: {num_errors}, Failures: {num_failures}. '",
          "41:                     f'Failing the test! ===={TEXT_RESET}\\n'",
          "42:                 )",
          "43:                 sys.exit(1)",
          "44:         else:",
          "45:             print(",
          "46:                 f'\\n{TEXT_RED}==== The testsuite element does not exist in file {fname!r}. '",
          "47:                 f'Cannot evaluate status of the test! ===={TEXT_RESET}\\n'",
          "48:             )",
          "49:             sys.exit(1)",
          "50:     except Exception as e:",
          "51:         print(",
          "52:             f'\\n{TEXT_RED}==== There was an error when parsing the junitxml file.'",
          "53:             f' Likely the file was corrupted ===={TEXT_RESET}\\n'",
          "54:         )",
          "55:         print(f'\\n{TEXT_RED}==== Error: {e} {TEXT_RESET}\\n')",
          "56:         sys.exit(2)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "581fcfdd58705a44c24cacbe364786b0422a0d1d",
      "candidate_info": {
        "commit_hash": "581fcfdd58705a44c24cacbe364786b0422a0d1d",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/581fcfdd58705a44c24cacbe364786b0422a0d1d",
        "files": [
          "airflow/www/api/experimental/endpoints.py"
        ],
        "message": "Remove unnecssary logging in experimental API (#20356)\n\nThe `execution_data` does not need to be passed to log. We send enough details to the API user in the response.\n\n(cherry picked from commit 790bc784435646c043d8def7096917a4ce0a62f7)",
        "before_after_code_files": [
          "airflow/www/api/experimental/endpoints.py||airflow/www/api/experimental/endpoints.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/api/experimental/endpoints.py||airflow/www/api/experimental/endpoints.py": [
          "File: airflow/www/api/experimental/endpoints.py -> airflow/www/api/experimental/endpoints.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "103:         try:",
          "104:             execution_date = timezone.parse(execution_date)",
          "105:         except ValueError:",
          "106:             error_message = (",
          "107:                 'Given execution date, {}, could not be identified '",
          "108:                 'as a date. Example date format: 2015-11-16T14:34:15+00:00'.format(execution_date)",
          "109:             )",
          "111:             response = jsonify({'error': error_message})",
          "112:             response.status_code = 400",
          "",
          "[Removed Lines]",
          "110:             log.error(error_message)",
          "",
          "[Added Lines]",
          "106:             log.error(\"Given execution date could not be identified as a date.\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "253:     try:",
          "254:         execution_date = timezone.parse(execution_date)",
          "255:     except ValueError:",
          "256:         error_message = (",
          "257:             'Given execution date, {}, could not be identified '",
          "258:             'as a date. Example date format: 2015-11-16T14:34:15+00:00'.format(execution_date)",
          "259:         )",
          "261:         response = jsonify({'error': error_message})",
          "262:         response.status_code = 400",
          "",
          "[Removed Lines]",
          "260:         log.error(error_message)",
          "",
          "[Added Lines]",
          "256:         log.error(\"Given execution date could not be identified as a date.\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "289:     try:",
          "290:         execution_date = timezone.parse(execution_date)",
          "291:     except ValueError:",
          "292:         error_message = (",
          "293:             'Given execution date, {}, could not be identified '",
          "294:             'as a date. Example date format: 2015-11-16T14:34:15+00:00'.format(execution_date)",
          "295:         )",
          "297:         response = jsonify({'error': error_message})",
          "298:         response.status_code = 400",
          "",
          "[Removed Lines]",
          "296:         log.error(error_message)",
          "",
          "[Added Lines]",
          "292:         log.error(\"Given execution date could not be identified as a date.\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "402:     try:",
          "403:         execution_dt = timezone.parse(execution_date)",
          "404:     except ValueError:",
          "405:         error_message = (",
          "406:             'Given execution date, {}, could not be identified '",
          "407:             'as a date. Example date format: 2015-11-16T14:34:15+00:00'.format(execution_date)",
          "408:         )",
          "410:         response = jsonify({'error': error_message})",
          "411:         response.status_code = 400",
          "",
          "[Removed Lines]",
          "409:         log.error(error_message)",
          "",
          "[Added Lines]",
          "405:         log.error(\"Given execution date could not be identified as a date.\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "871271065b67c39aa6ae1d94148ac522984502d9",
      "candidate_info": {
        "commit_hash": "871271065b67c39aa6ae1d94148ac522984502d9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/871271065b67c39aa6ae1d94148ac522984502d9",
        "files": [
          "scripts/in_container/run_resource_check.sh"
        ],
        "message": "Lower the recommended disk space requirements (#19775)\n\nThe recommended disk space requirements for Breeze were set to\n40GB which is way to high (and our Public Runners do not have that\nmuch of a disk space - this generated false warnings).\n\nLowering it to 20GB should be quite enough for most \"casual\" users.\n\n(cherry picked from commit 5901f79bb61d3e56f1350d43448edb7e2c28961e)",
        "before_after_code_files": [
          "scripts/in_container/run_resource_check.sh||scripts/in_container/run_resource_check.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/in_container/run_resource_check.sh||scripts/in_container/run_resource_check.sh": [
          "File: scripts/in_container/run_resource_check.sh -> scripts/in_container/run_resource_check.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "46:     else",
          "47:         echo \"* CPUs available ${cpus_available}. ${COLOR_GREEN}OK.${COLOR_RESET}\"",
          "48:     fi",
          "50:         echo \"${COLOR_YELLOW}WARNING!!!: Not enough Disk space available for Docker.${COLOR_RESET}\"",
          "52:         warning_resources=\"true\"",
          "53:     else",
          "54:         echo \"* Disk available ${human_readable_disk}. ${COLOR_GREEN}OK.${COLOR_RESET}\"",
          "",
          "[Removed Lines]",
          "49:     if (( disk_available < one_meg*40 )); then",
          "51:         echo \"At least 40 GBs recommended. You have ${human_readable_disk}\"",
          "",
          "[Added Lines]",
          "49:     if (( disk_available < one_meg*20 )); then",
          "51:         echo \"At least 20 GBs recommended. You have ${human_readable_disk}\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "07102e96dfb3c9794882f562548b37738ee4a37a",
      "candidate_info": {
        "commit_hash": "07102e96dfb3c9794882f562548b37738ee4a37a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/07102e96dfb3c9794882f562548b37738ee4a37a",
        "files": [
          "airflow/models/taskinstance.py",
          "tests/models/test_taskinstance.py"
        ],
        "message": "Do not set `TaskInstance.max_tries` in `refresh_from_task` (#21018)\n\n(cherry picked from commit e3832a77a3e0d374dfdbe14f34a941d22c9c459d)",
        "before_after_code_files": [
          "airflow/models/taskinstance.py||airflow/models/taskinstance.py",
          "tests/models/test_taskinstance.py||tests/models/test_taskinstance.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/taskinstance.py||airflow/models/taskinstance.py": [
          "File: airflow/models/taskinstance.py -> airflow/models/taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "447:         self.run_id = run_id",
          "449:         self.try_number = 0",
          "450:         self.unixname = getuser()",
          "451:         if state:",
          "452:             self.state = state",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "450:         self.max_tries = self.task.retries",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "775:         self.pool_slots = task.pool_slots",
          "776:         self.priority_weight = task.priority_weight_total",
          "777:         self.run_as_user = task.run_as_user",
          "779:         self.executor_config = task.executor_config",
          "780:         self.operator = task.task_type",
          "",
          "[Removed Lines]",
          "778:         self.max_tries = task.retries",
          "",
          "[Added Lines]",
          "779:         # Do not set max_tries to task.retries here because max_tries is a cumulative",
          "780:         # value that needs to be stored in the db.",
          "",
          "---------------"
        ],
        "tests/models/test_taskinstance.py||tests/models/test_taskinstance.py": [
          "File: tests/models/test_taskinstance.py -> tests/models/test_taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2143:     assert ti.executor_config == task.executor_config",
          "2144:     assert ti.operator == DummyOperator.__name__",
          "2147: class TestRunRawTaskQueriesCount:",
          "2148:     \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2146:     # Test that refresh_from_task does not reset ti.max_tries",
          "2147:     expected_max_tries = task.retries + 10",
          "2148:     ti.max_tries = expected_max_tries",
          "2149:     ti.refresh_from_task(task)",
          "2150:     assert ti.max_tries == expected_max_tries",
          "",
          "---------------"
        ]
      }
    }
  ]
}