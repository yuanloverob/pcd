{
  "cve_id": "CVE-2023-45348",
  "cve_desc": "Apache Airflow, versions 2.7.0 and 2.7.1, is affected by a vulnerability that allows an authenticated user to retrieve sensitive configuration information when the \"expose_config\" option is set to \"non-sensitive-only\". The `expose_config` option is False by default.\nIt is recommended to upgrade to a version that is not affected.",
  "repo": "apache/airflow",
  "patch_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
  "patch_info": {
    "commit_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
    "files": [
      "airflow/api_connexion/endpoints/config_endpoint.py",
      "airflow/configuration.py",
      "tests/api_connexion/endpoints/test_config_endpoint.py"
    ],
    "message": "Check if the lower of provided values are sensitives in config endpoint (#34712)\n\n* Check if the lower of provided values are sensitives in config endpoint\n\n* update unit test\n\n* ensure that all values in sensitive dict are in lower characters\n\n(cherry picked from commit f044589b685855a8fce8f5376bea2564c5a001f7)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py",
      "airflow/configuration.py||airflow/configuration.py",
      "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py": [
      "File: airflow/api_connexion/endpoints/config_endpoint.py -> airflow/api_connexion/endpoints/config_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "123:                 \"Config not found.\", detail=f\"The option [{section}/{option}] is not found in config.\"",
      "124:             )",
      "127:             value = \"< hidden >\"",
      "128:         else:",
      "129:             value = conf.get(section, option)",
      "",
      "[Removed Lines]",
      "126:         if (section, option) in conf.sensitive_config_values:",
      "",
      "[Added Lines]",
      "126:         if (section.lower(), option.lower()) in conf.sensitive_config_values:",
      "",
      "---------------"
    ],
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "311:             for s, s_c in self.configuration_description.items()",
      "312:             for k, item in s_c.get(\"options\").items()  # type: ignore[union-attr]",
      "313:         }",
      "315:         depr_option = {self.deprecated_options[x][:-1] for x in sensitive if x in self.deprecated_options}",
      "316:         depr_section = {",
      "317:             (self.deprecated_sections[s][0], k) for s, k in sensitive if s in self.deprecated_sections",
      "",
      "[Removed Lines]",
      "314:         sensitive = {(section, key) for (section, key), v in flattened.items() if v.get(\"sensitive\") is True}",
      "",
      "[Added Lines]",
      "314:         sensitive = {",
      "315:             (section.lower(), key.lower())",
      "316:             for (section, key), v in flattened.items()",
      "317:             if v.get(\"sensitive\") is True",
      "318:         }",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_config_endpoint.py -> tests/api_connexion/endpoints/test_config_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "247:         return_value=MOCK_CONF_WITH_SENSITIVE_VALUE,",
      "248:     )",
      "249:     @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "251:         response = self.client.get(",
      "253:             headers={\"Accept\": \"text/plain\"},",
      "254:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "255:         )",
      "256:         assert response.status_code == 200",
      "257:         expected = textwrap.dedent(",
      "261:         \"\"\"",
      "262:         )",
      "263:         assert expected == response.data.decode()",
      "",
      "[Removed Lines]",
      "250:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict):",
      "252:             \"/api/v1/config/section/core/option/sql_alchemy_conn\",",
      "258:             \"\"\"\\",
      "259:         [core]",
      "260:         sql_alchemy_conn = < hidden >",
      "",
      "[Added Lines]",
      "250:     @pytest.mark.parametrize(",
      "251:         \"section, option\",",
      "252:         [",
      "253:             (\"core\", \"sql_alchemy_conn\"),",
      "254:             (\"core\", \"SQL_ALCHEMY_CONN\"),",
      "255:             (\"corE\", \"sql_alchemy_conn\"),",
      "256:             (\"CORE\", \"sql_alchemy_conn\"),",
      "257:         ],",
      "258:     )",
      "259:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict, section, option):",
      "261:             f\"/api/v1/config/section/{section}/option/{option}\",",
      "267:             f\"\"\"\\",
      "268:         [{section}]",
      "269:         {option} = < hidden >",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "eada688f87ba7f11cb8b8f27dd4f908f5ceb6db3",
      "candidate_info": {
        "commit_hash": "eada688f87ba7f11cb8b8f27dd4f908f5ceb6db3",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/eada688f87ba7f11cb8b8f27dd4f908f5ceb6db3",
        "files": [
          "airflow/cli/cli_config.py"
        ],
        "message": "fix(cli): remove \"to backfill\" from --task-regex help message (#34598)\n\nthis arg is used by both \"airflow tasks clear\" and \"airflow tasks backfill\"\nand it does not make sense for \"airflow tasks clear\" to have the description \"to backfile\"\n\n(cherry picked from commit c019cf18dd1be3b20baf7503326a53002c236b45)",
        "before_after_code_files": [
          "airflow/cli/cli_config.py||airflow/cli/cli_config.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/cli_config.py||airflow/cli/cli_config.py": [
          "File: airflow/cli/cli_config.py -> airflow/cli/cli_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "156:     nargs=\"?\",",
          "157:     help=\"The execution_date of the DAG or run_id of the DAGRun (optional)\",",
          "158: )",
          "162: ARG_SUBDIR = Arg(",
          "163:     (\"-S\", \"--subdir\"),",
          "164:     help=(",
          "",
          "[Removed Lines]",
          "159: ARG_TASK_REGEX = Arg(",
          "160:     (\"-t\", \"--task-regex\"), help=\"The regex to filter specific task_ids to backfill (optional)\"",
          "161: )",
          "",
          "[Added Lines]",
          "159: ARG_TASK_REGEX = Arg((\"-t\", \"--task-regex\"), help=\"The regex to filter specific task_ids (optional)\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b38c37bae6956d5b624f2970e141e5b2ed6ed278",
      "candidate_info": {
        "commit_hash": "b38c37bae6956d5b624f2970e141e5b2ed6ed278",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b38c37bae6956d5b624f2970e141e5b2ed6ed278",
        "files": [
          "airflow/jobs/scheduler_job_runner.py",
          "tests/jobs/test_scheduler_job.py"
        ],
        "message": "Fix scheduler logic to plan new dag runs by ignoring manual runs (#34027)\n\n* Fix manual task triggering scheduled tasks\n\nFixes #33949\n\n* fix static checks\n\n* static checks\n\n* add unit test\n\n* static check\n\n* Undo renaming\n\n* Update airflow/jobs/scheduler_job_runner.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n* use keyword-only arguments for last_dag_run and total_active_runs\n\n---------\n\nCo-authored-by: daniel.dylag <danieldylag1990@gmail.com>\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 20d81428699db240b65f72a92183255c24e8c19b)",
        "before_after_code_files": [
          "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py",
          "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py": [
          "File: airflow/jobs/scheduler_job_runner.py -> airflow/jobs/scheduler_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1183:                 )",
          "1184:                 active_runs_of_dags[dag.dag_id] += 1",
          "1185:             if self._should_update_dag_next_dagruns(",
          "1187:             ):",
          "1188:                 dag_model.calculate_dagrun_date_fields(dag, data_interval)",
          "1189:         # TODO[HA]: Should we do a session.flush() so we don't have to keep lots of state/object in",
          "",
          "[Removed Lines]",
          "1186:                 dag, dag_model, active_runs_of_dags[dag.dag_id], session=session",
          "",
          "[Added Lines]",
          "1186:                 dag,",
          "1187:                 dag_model,",
          "1188:                 last_dag_run=None,",
          "1189:                 total_active_runs=active_runs_of_dags[dag.dag_id],",
          "1190:                 session=session,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1291:                 )",
          "1293:     def _should_update_dag_next_dagruns(",
          "1295:     ) -> bool:",
          "1296:         \"\"\"Check if the dag's next_dagruns_create_after should be updated.\"\"\"",
          "1297:         # If the DAG never schedules skip save runtime",
          "1298:         if not dag.timetable.can_be_scheduled:",
          "1299:             return False",
          "",
          "[Removed Lines]",
          "1294:         self, dag: DAG, dag_model: DagModel, total_active_runs: int | None = None, *, session: Session",
          "",
          "[Added Lines]",
          "1298:         self,",
          "1299:         dag: DAG,",
          "1300:         dag_model: DagModel,",
          "1302:         last_dag_run: DagRun | None = None,",
          "1303:         total_active_runs: int | None = None,",
          "1304:         session: Session,",
          "1307:         # If last_dag_run is defined, the update was triggered by a scheduling decision in this DAG run.",
          "1308:         # In such case, schedule next only if last_dag_run is finished and was an automated run.",
          "1309:         if last_dag_run and not (",
          "1310:             last_dag_run.state in State.finished_dr_states",
          "1311:             and last_dag_run.run_type in [DagRunType.SCHEDULED, DagRunType.BACKFILL_JOB]",
          "1312:         ):",
          "1313:             return False",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1428:                 session.merge(task_instance)",
          "1429:             session.flush()",
          "1430:             self.log.info(\"Run %s of %s has timed-out\", dag_run.run_id, dag_run.dag_id)",
          "1433:                 dag_model.calculate_dagrun_date_fields(dag, dag.get_run_data_interval(dag_run))",
          "1435:             callback_to_execute = DagCallbackRequest(",
          "",
          "[Removed Lines]",
          "1431:             # Work out if we should allow creating a new DagRun now?",
          "1432:             if self._should_update_dag_next_dagruns(dag, dag_model, session=session):",
          "",
          "[Added Lines]",
          "1449:             if self._should_update_dag_next_dagruns(dag, dag_model, last_dag_run=dag_run, session=session):",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1456:             return callback",
          "1457:         # TODO[HA]: Rename update_state -> schedule_dag_run, ?? something else?",
          "1458:         schedulable_tis, callback_to_run = dag_run.update_state(session=session, execute_callbacks=False)",
          "1464:         # This will do one query per dag run. We \"could\" build up a complex",
          "1465:         # query to update all the TIs across all the execution dates and dag",
          "1466:         # IDs in a single query, but it turns out that can be _very very slow_",
          "",
          "[Removed Lines]",
          "1459:         # Check if DAG not scheduled then skip interval calculation to same scheduler runtime",
          "1460:         if dag_run.state in State.finished_dr_states:",
          "1461:             # Work out if we should allow creating a new DagRun now?",
          "1462:             if self._should_update_dag_next_dagruns(dag, dag_model, session=session):",
          "1463:                 dag_model.calculate_dagrun_date_fields(dag, dag.get_run_data_interval(dag_run))",
          "",
          "[Added Lines]",
          "1477:         if self._should_update_dag_next_dagruns(dag, dag_model, last_dag_run=dag_run, session=session):",
          "1478:             dag_model.calculate_dagrun_date_fields(dag, dag.get_run_data_interval(dag_run))",
          "",
          "---------------"
        ],
        "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py": [
          "File: tests/jobs/test_scheduler_job.py -> tests/jobs/test_scheduler_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1755:             # Need to use something that doesn't immediately get marked as success by the scheduler",
          "1756:             BashOperator(task_id=\"task\", bash_command=\"true\")",
          "1763:         # Reach max_active_runs",
          "1764:         for _ in range(3):",
          "",
          "[Removed Lines]",
          "1758:         dag_run = dag_maker.create_dagrun(",
          "1759:             state=State.RUNNING,",
          "1760:             session=session,",
          "1761:         )",
          "",
          "[Added Lines]",
          "1758:         dag_run = dag_maker.create_dagrun(state=State.RUNNING, session=session, run_type=DagRunType.SCHEDULED)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3459:         self.job_runner = SchedulerJobRunner(job=scheduler_job)",
          "3461:         assert excepted is self.job_runner._should_update_dag_next_dagruns(",
          "3463:         )",
          "3465:     def test_create_dag_runs(self, dag_maker):",
          "",
          "[Removed Lines]",
          "3462:             dag, dag_model, number_running, session=session",
          "",
          "[Added Lines]",
          "3459:             dag, dag_model, total_active_runs=number_running, session=session",
          "3460:         )",
          "3462:     @pytest.mark.parametrize(",
          "3463:         \"run_type, should_update\",",
          "3464:         [",
          "3465:             (DagRunType.MANUAL, False),",
          "3466:             (DagRunType.SCHEDULED, True),",
          "3467:             (DagRunType.BACKFILL_JOB, True),",
          "3468:             (DagRunType.DATASET_TRIGGERED, False),",
          "3469:         ],",
          "3470:         ids=[",
          "3471:             DagRunType.MANUAL.name,",
          "3472:             DagRunType.SCHEDULED.name,",
          "3473:             DagRunType.BACKFILL_JOB.name,",
          "3474:             DagRunType.DATASET_TRIGGERED.name,",
          "3475:         ],",
          "3476:     )",
          "3477:     def test_should_update_dag_next_dagruns_after_run_type(self, run_type, should_update, session, dag_maker):",
          "3478:         \"\"\"Test that whether next dagrun is updated depends on run type\"\"\"",
          "3479:         with dag_maker(",
          "3480:             dag_id=\"test_should_update_dag_next_dagruns_after_run_type\",",
          "3481:             schedule=\"*/1 * * * *\",",
          "3482:             max_active_runs=10,",
          "3483:         ) as dag:",
          "3484:             EmptyOperator(task_id=\"dummy\")",
          "3486:         dag_model = dag_maker.dag_model",
          "3488:         run = dag_maker.create_dagrun(",
          "3489:             run_id=\"run\",",
          "3490:             run_type=run_type,",
          "3491:             execution_date=DEFAULT_DATE,",
          "3492:             start_date=timezone.utcnow(),",
          "3493:             state=State.SUCCESS,",
          "3494:             session=session,",
          "3495:         )",
          "3497:         session.flush()",
          "3498:         scheduler_job = Job(executor=self.null_exec)",
          "3499:         self.job_runner = SchedulerJobRunner(job=scheduler_job)",
          "3501:         assert should_update is self.job_runner._should_update_dag_next_dagruns(",
          "3502:             dag, dag_model, last_dag_run=run, total_active_runs=0, session=session",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a78ba98e6d82959cbd4570c0506d17dd2ea74a1f",
      "candidate_info": {
        "commit_hash": "a78ba98e6d82959cbd4570c0506d17dd2ea74a1f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a78ba98e6d82959cbd4570c0506d17dd2ea74a1f",
        "files": [
          "airflow/models/taskinstance.py",
          "airflow/providers/amazon/aws/hooks/sagemaker.py",
          "airflow/providers/elasticsearch/log/es_json_formatter.py",
          "airflow/providers/google/cloud/operators/dataproc.py",
          "airflow/providers/oracle/hooks/oracle.py",
          "airflow/task/task_runner/cgroup_task_runner.py",
          "airflow/utils/log/timezone_aware.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "tests/dags_corrupted/test_impersonation_custom.py",
          "tests/providers/amazon/aws/utils/eks_test_utils.py",
          "tests/providers/apache/kylin/operators/test_kylin_cube.py",
          "tests/providers/http/sensors/test_http.py"
        ],
        "message": "Replace strftime with f-strings where nicer (#33455)\n\n(cherry picked from commit 94f70d818482de7defa03c0aff3c213ca6b83e9e)",
        "before_after_code_files": [
          "airflow/models/taskinstance.py||airflow/models/taskinstance.py",
          "airflow/providers/amazon/aws/hooks/sagemaker.py||airflow/providers/amazon/aws/hooks/sagemaker.py",
          "airflow/providers/elasticsearch/log/es_json_formatter.py||airflow/providers/elasticsearch/log/es_json_formatter.py",
          "airflow/providers/google/cloud/operators/dataproc.py||airflow/providers/google/cloud/operators/dataproc.py",
          "airflow/providers/oracle/hooks/oracle.py||airflow/providers/oracle/hooks/oracle.py",
          "airflow/task/task_runner/cgroup_task_runner.py||airflow/task/task_runner/cgroup_task_runner.py",
          "airflow/utils/log/timezone_aware.py||airflow/utils/log/timezone_aware.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "tests/dags_corrupted/test_impersonation_custom.py||tests/dags_corrupted/test_impersonation_custom.py",
          "tests/providers/amazon/aws/utils/eks_test_utils.py||tests/providers/amazon/aws/utils/eks_test_utils.py",
          "tests/providers/apache/kylin/operators/test_kylin_cube.py||tests/providers/apache/kylin/operators/test_kylin_cube.py",
          "tests/providers/http/sensors/test_http.py||tests/providers/http/sensors/test_http.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/taskinstance.py||airflow/models/taskinstance.py": [
          "File: airflow/models/taskinstance.py -> airflow/models/taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2141:             execution_date = get_prev_execution_date()",
          "2142:             if execution_date is None:",
          "2143:                 return None",
          "2146:         def get_prev_ds_nodash() -> str | None:",
          "2147:             prev_ds = get_prev_ds()",
          "",
          "[Removed Lines]",
          "2144:             return execution_date.strftime(r\"%Y-%m-%d\")",
          "",
          "[Added Lines]",
          "2144:             return execution_date.strftime(\"%Y-%m-%d\")",
          "",
          "---------------"
        ],
        "airflow/providers/amazon/aws/hooks/sagemaker.py||airflow/providers/amazon/aws/hooks/sagemaker.py": [
          "File: airflow/providers/amazon/aws/hooks/sagemaker.py -> airflow/providers/amazon/aws/hooks/sagemaker.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "128:     status_strs = []",
          "129:     for transition in transitions_to_print:",
          "130:         message = transition[\"StatusMessage\"]",
          "136:     return \"\\n\".join(status_strs)",
          "",
          "[Removed Lines]",
          "131:         time_str = timezone.convert_to_utc(cast(datetime, job_description[\"LastModifiedTime\"])).strftime(",
          "132:             \"%Y-%m-%d %H:%M:%S\"",
          "133:         )",
          "134:         status_strs.append(f\"{time_str} {transition['Status']} - {message}\")",
          "",
          "[Added Lines]",
          "131:         time_utc = timezone.convert_to_utc(cast(datetime, job_description[\"LastModifiedTime\"]))",
          "132:         status_strs.append(f\"{time_utc:%Y-%m-%d %H:%M:%S} {transition['Status']} - {message}\")",
          "",
          "---------------"
        ],
        "airflow/providers/elasticsearch/log/es_json_formatter.py||airflow/providers/elasticsearch/log/es_json_formatter.py": [
          "File: airflow/providers/elasticsearch/log/es_json_formatter.py -> airflow/providers/elasticsearch/log/es_json_formatter.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31:     def formatTime(self, record, datefmt=None):",
          "32:         \"\"\"Return the creation time of the LogRecord in ISO 8601 date/time format in the local time zone.\"\"\"",
          "33:         dt = pendulum.from_timestamp(record.created, tz=pendulum.local_timezone())",
          "39:         if self.default_msec_format:",
          "40:             s = self.default_msec_format % (s, record.msecs)",
          "41:         if self.default_tz_format:",
          "",
          "[Removed Lines]",
          "34:         if datefmt:",
          "35:             s = dt.strftime(datefmt)",
          "36:         else:",
          "37:             s = dt.strftime(self.default_time_format)",
          "",
          "[Added Lines]",
          "34:         s = dt.strftime(datefmt or self.default_time_format)",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/operators/dataproc.py||airflow/providers/google/cloud/operators/dataproc.py": [
          "File: airflow/providers/google/cloud/operators/dataproc.py -> airflow/providers/google/cloud/operators/dataproc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1567:     @staticmethod",
          "1568:     def _generate_temp_filename(filename):",
          "1572:     def _upload_file_temp(self, bucket, local_file):",
          "1573:         \"\"\"Upload a local file to a Google Cloud Storage bucket.\"\"\"",
          "",
          "[Removed Lines]",
          "1569:         date = time.strftime(\"%Y%m%d%H%M%S\")",
          "1570:         return f\"{date}_{str(uuid.uuid4())[:8]}_{ntpath.basename(filename)}\"",
          "",
          "[Added Lines]",
          "1569:         return f\"{time:%Y%m%d%H%M%S}_{str(uuid.uuid4())[:8]}_{ntpath.basename(filename)}\"",
          "",
          "---------------"
        ],
        "airflow/providers/oracle/hooks/oracle.py||airflow/providers/oracle/hooks/oracle.py": [
          "File: airflow/providers/oracle/hooks/oracle.py -> airflow/providers/oracle/hooks/oracle.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "303:                 elif numpy and isinstance(cell, numpy.datetime64):",
          "304:                     lst.append(\"'\" + str(cell) + \"'\")",
          "305:                 elif isinstance(cell, datetime):",
          "309:                 else:",
          "310:                     lst.append(str(cell))",
          "311:             values = tuple(lst)",
          "",
          "[Removed Lines]",
          "306:                     lst.append(",
          "307:                         \"to_date('\" + cell.strftime(\"%Y-%m-%d %H:%M:%S\") + \"','YYYY-MM-DD HH24:MI:SS')\"",
          "308:                     )",
          "",
          "[Added Lines]",
          "306:                     lst.append(f\"to_date('{cell:%Y-%m-%d %H:%M:%S}','YYYY-MM-DD HH24:MI:SS')\")",
          "",
          "---------------"
        ],
        "airflow/task/task_runner/cgroup_task_runner.py||airflow/task/task_runner/cgroup_task_runner.py": [
          "File: airflow/task/task_runner/cgroup_task_runner.py -> airflow/task/task_runner/cgroup_task_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "134:             return",
          "136:         # Create a unique cgroup name",
          "139:         self.mem_cgroup_name = f\"memory/{cgroup_name}\"",
          "140:         self.cpu_cgroup_name = f\"cpu/{cgroup_name}\"",
          "",
          "[Removed Lines]",
          "137:         cgroup_name = f\"airflow/{datetime.datetime.utcnow().strftime('%Y-%m-%d')}/{str(uuid.uuid4())}\"",
          "",
          "[Added Lines]",
          "137:         cgroup_name = f\"airflow/{datetime.datetime.utcnow():%Y-%m-%d}/{uuid.uuid4()}\"",
          "",
          "---------------"
        ],
        "airflow/utils/log/timezone_aware.py||airflow/utils/log/timezone_aware.py": [
          "File: airflow/utils/log/timezone_aware.py -> airflow/utils/log/timezone_aware.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "40:         date and time format in the local time zone.",
          "41:         \"\"\"",
          "42:         dt = pendulum.from_timestamp(record.created, tz=pendulum.local_timezone())",
          "48:         if self.default_msec_format:",
          "49:             s = self.default_msec_format % (s, record.msecs)",
          "50:         if self.default_tz_format:",
          "",
          "[Removed Lines]",
          "43:         if datefmt:",
          "44:             s = dt.strftime(datefmt)",
          "45:         else:",
          "46:             s = dt.strftime(self.default_time_format)",
          "",
          "[Added Lines]",
          "43:         s = dt.strftime(datefmt or self.default_time_format)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1202:         get_console().print()",
          "1203:         get_console().print(",
          "1204:             \"Issue title: [yellow]Status of testing Providers that were \"",
          "1206:         )",
          "1207:         get_console().print()",
          "1208:         syntax = Syntax(issue_content, \"markdown\", theme=\"ansi_dark\")",
          "",
          "[Removed Lines]",
          "1205:             f\"prepared on {datetime.now().strftime('%B %d, %Y')}[/]\"",
          "",
          "[Added Lines]",
          "1205:             f\"prepared on {datetime.now():%B %d, %Y}[/]\"",
          "",
          "---------------"
        ],
        "tests/dags_corrupted/test_impersonation_custom.py||tests/dags_corrupted/test_impersonation_custom.py": [
          "File: tests/dags_corrupted/test_impersonation_custom.py -> tests/dags_corrupted/test_impersonation_custom.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "41: def print_today():",
          "42:     date_time = FakeDatetime.utcnow()",
          "46: def check_hive_conf():",
          "",
          "[Removed Lines]",
          "43:     print(f\"Today is {date_time.strftime('%Y-%m-%d')}\")",
          "",
          "[Added Lines]",
          "43:     print(f\"Today is {date_time:%Y-%m-%d}\")",
          "",
          "---------------"
        ],
        "tests/providers/amazon/aws/utils/eks_test_utils.py||tests/providers/amazon/aws/utils/eks_test_utils.py": [
          "File: tests/providers/amazon/aws/utils/eks_test_utils.py -> tests/providers/amazon/aws/utils/eks_test_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "237: def iso_date(input_datetime: datetime.datetime) -> str:",
          "241: def generate_dict(prefix, count) -> dict:",
          "",
          "[Removed Lines]",
          "238:     return input_datetime.strftime(\"%Y-%m-%dT%H:%M:%S\") + \"Z\"",
          "",
          "[Added Lines]",
          "238:     return f\"{input_datetime:%Y-%m-%dT%H:%M:%S}Z\"",
          "",
          "---------------"
        ],
        "tests/providers/apache/kylin/operators/test_kylin_cube.py||tests/providers/apache/kylin/operators/test_kylin_cube.py": [
          "File: tests/providers/apache/kylin/operators/test_kylin_cube.py -> tests/providers/apache/kylin/operators/test_kylin_cube.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "37:         \"project\": \"learn_kylin\",",
          "38:         \"cube\": \"kylin_sales_cube\",",
          "39:         \"command\": \"build\",",
          "42:     }",
          "43:     cube_command = [",
          "44:         \"fullbuild\",",
          "",
          "[Removed Lines]",
          "40:         \"start_time\": datetime(2012, 1, 2, 0, 0).strftime(\"%s\") + \"000\",",
          "41:         \"end_time\": datetime(2012, 1, 3, 0, 0).strftime(\"%s\") + \"000\",",
          "",
          "[Added Lines]",
          "40:         \"start_time\": str(int(datetime(2012, 1, 2, 0, 0).timestamp() * 1000)),",
          "41:         \"end_time\": str(int(datetime(2012, 1, 3, 0, 0).timestamp() * 1000)),",
          "",
          "---------------"
        ],
        "tests/providers/http/sensors/test_http.py||tests/providers/http/sensors/test_http.py": [
          "File: tests/providers/http/sensors/test_http.py -> tests/providers/http/sensors/test_http.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "244:             endpoint=\"/search\",",
          "245:             request_params={\"client\": \"ubuntu\", \"q\": \"airflow\", \"date\": \"{{ds}}\"},",
          "246:             headers={},",
          "250:             poke_interval=5,",
          "251:             timeout=15,",
          "252:             dag=self.dag,",
          "",
          "[Removed Lines]",
          "247:             response_check=lambda response: (",
          "248:                 \"apache/airflow/\" + DEFAULT_DATE.strftime(\"%Y-%m-%d\") in response.text",
          "249:             ),",
          "",
          "[Added Lines]",
          "247:             response_check=lambda response: f\"apache/airflow/{DEFAULT_DATE:%Y-%m-%d}\" in response.text,",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "830cd9847b6e04f03aa7f5b6b2f3e661a0422bf2",
      "candidate_info": {
        "commit_hash": "830cd9847b6e04f03aa7f5b6b2f3e661a0422bf2",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/830cd9847b6e04f03aa7f5b6b2f3e661a0422bf2",
        "files": [
          "airflow/models/baseoperator.py",
          "airflow/serialization/serialized_objects.py",
          "airflow/ti_deps/deps/trigger_rule_dep.py"
        ],
        "message": "Combine similar if logics in core (#33988)\n\n* Combine similar if logics in core\n\n* Update airflow/models/baseoperator.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n* replace in tuple by multiple or equalities\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit d800a0de5194bb1ef3cfad44c874abafcc78efd6)",
        "before_after_code_files": [
          "airflow/models/baseoperator.py||airflow/models/baseoperator.py",
          "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py",
          "airflow/ti_deps/deps/trigger_rule_dep.py||airflow/ti_deps/deps/trigger_rule_dep.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/baseoperator.py||airflow/models/baseoperator.py": [
          "File: airflow/models/baseoperator.py -> airflow/models/baseoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1103:         if self.__from_mapped:",
          "1104:             pass  # Don't add to DAG -- the mapped task takes the place.",
          "1108:             dag.add_task(self)",
          "1110:         self._dag = dag",
          "",
          "[Removed Lines]",
          "1105:         elif self.task_id not in dag.task_dict:",
          "1106:             dag.add_task(self)",
          "1107:         elif self.task_id in dag.task_dict and dag.task_dict[self.task_id] is not self:",
          "",
          "[Added Lines]",
          "1105:         elif dag.task_dict.get(self.task_id) is not self:",
          "",
          "---------------"
        ],
        "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py": [
          "File: airflow/serialization/serialized_objects.py -> airflow/serialization/serialized_objects.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "985:                 v = {arg: cls.deserialize(value) for arg, value in v.items()}",
          "986:             elif k in {\"expand_input\", \"op_kwargs_expand_input\"}:",
          "987:                 v = _ExpandInputRef(v[\"type\"], cls.deserialize(v[\"value\"]))",
          "991:                 v = cls.deserialize(v)",
          "992:             elif k == \"on_failure_fail_dagrun\":",
          "993:                 k = \"_on_failure_fail_dagrun\"",
          "",
          "[Removed Lines]",
          "988:             elif k in cls._decorated_fields or k not in op.get_serialized_fields():",
          "989:                 v = cls.deserialize(v)",
          "990:             elif k in (\"outlets\", \"inlets\"):",
          "",
          "[Added Lines]",
          "988:             elif (",
          "989:                 k in cls._decorated_fields",
          "990:                 or k not in op.get_serialized_fields()",
          "991:                 or k in (\"outlets\", \"inlets\")",
          "992:             ):",
          "",
          "---------------"
        ],
        "airflow/ti_deps/deps/trigger_rule_dep.py||airflow/ti_deps/deps/trigger_rule_dep.py": [
          "File: airflow/ti_deps/deps/trigger_rule_dep.py -> airflow/ti_deps/deps/trigger_rule_dep.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "470:                             f\"upstream_task_ids={task.upstream_task_ids}\"",
          "471:                         )",
          "472:                     )",
          "487:                 num_failures = upstream - success - skipped",
          "488:                 if ti.map_index > -1:",
          "489:                     num_failures -= removed",
          "",
          "[Removed Lines]",
          "473:             elif trigger_rule == TR.NONE_FAILED:",
          "474:                 num_failures = upstream - success - skipped",
          "475:                 if ti.map_index > -1:",
          "476:                     num_failures -= removed",
          "477:                 if num_failures > 0:",
          "478:                     yield self._failing_status(",
          "479:                         reason=(",
          "480:                             f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have \"",
          "481:                             f\"succeeded or been skipped, but found {num_failures} non-success(es). \"",
          "482:                             f\"upstream_states={upstream_states}, \"",
          "483:                             f\"upstream_task_ids={task.upstream_task_ids}\"",
          "484:                         )",
          "485:                     )",
          "486:             elif trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:",
          "",
          "[Added Lines]",
          "473:             elif trigger_rule == TR.NONE_FAILED or trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2bfbffe09c2ab82836e297bd4bf0b6d6e0c7d93b",
      "candidate_info": {
        "commit_hash": "2bfbffe09c2ab82836e297bd4bf0b6d6e0c7d93b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/2bfbffe09c2ab82836e297bd4bf0b6d6e0c7d93b",
        "files": [
          "airflow/api_connexion/openapi/v1.yaml",
          "airflow/api_connexion/schemas/task_instance_schema.py",
          "airflow/www/static/js/types/api-generated.ts",
          "tests/api_connexion/endpoints/test_task_instance_endpoint.py"
        ],
        "message": "Fix: make dry run optional for patch task instance  (#34568)\n\n* fix: Make dry_run optional per docs\n\nThis fixes an issue where dry_run is not actually and optional parameter\nin the patch task_instance api.\n\n* chore: remove formatting changes\n\n* fix: Make changes for api docs\n\nThis updates the docs and the code so that they are in alignment while\nalso being consistent with all other endpoints. All other Endpoints have\ndry run set to be True by default.\n\n* fix: Update static ts file for api change\n\n* fix: Remove dump_default\n\n(cherry picked from commit a4357ca25cc3d014e50968bac7858f533e6421e4)",
        "before_after_code_files": [
          "airflow/api_connexion/schemas/task_instance_schema.py||airflow/api_connexion/schemas/task_instance_schema.py",
          "airflow/www/static/js/types/api-generated.ts||airflow/www/static/js/types/api-generated.ts",
          "tests/api_connexion/endpoints/test_task_instance_endpoint.py||tests/api_connexion/endpoints/test_task_instance_endpoint.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api_connexion/schemas/task_instance_schema.py||airflow/api_connexion/schemas/task_instance_schema.py": [
          "File: airflow/api_connexion/schemas/task_instance_schema.py -> airflow/api_connexion/schemas/task_instance_schema.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "177: class SetSingleTaskInstanceStateFormSchema(Schema):",
          "178:     \"\"\"Schema for handling the request of updating state of a single task instance.\"\"\"",
          "181:     new_state = TaskInstanceStateField(",
          "182:         required=True,",
          "183:         validate=validate.OneOf(",
          "",
          "[Removed Lines]",
          "180:     dry_run = fields.Boolean(dump_default=True)",
          "",
          "[Added Lines]",
          "180:     dry_run = fields.Boolean(load_default=True)",
          "",
          "---------------"
        ],
        "airflow/www/static/js/types/api-generated.ts||airflow/www/static/js/types/api-generated.ts": [
          "File: airflow/www/static/js/types/api-generated.ts -> airflow/www/static/js/types/api-generated.ts"
        ],
        "tests/api_connexion/endpoints/test_task_instance_endpoint.py||tests/api_connexion/endpoints/test_task_instance_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_task_instance_endpoint.py -> tests/api_connexion/endpoints/test_task_instance_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1772:         assert response2.status_code == 200",
          "1773:         assert response2.json[\"state\"] == NEW_STATE",
          "1775:     def test_should_update_mapped_task_instance_state(self, session):",
          "1776:         NEW_STATE = \"failed\"",
          "1777:         map_index = 1",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1775:     def test_should_update_task_instance_state_default_dry_run_to_true(self, session):",
          "1776:         self.create_task_instances(session)",
          "1778:         NEW_STATE = \"running\"",
          "1780:         self.client.patch(",
          "1781:             self.ENDPOINT_URL,",
          "1782:             environ_overrides={\"REMOTE_USER\": \"test\"},",
          "1783:             json={",
          "1784:                 \"new_state\": NEW_STATE,",
          "1785:             },",
          "1786:         )",
          "1788:         response2 = self.client.get(",
          "1789:             self.ENDPOINT_URL,",
          "1790:             environ_overrides={\"REMOTE_USER\": \"test\"},",
          "1791:             json={},",
          "1792:         )",
          "1793:         assert response2.status_code == 200",
          "1794:         assert response2.json[\"state\"] == NEW_STATE",
          "",
          "---------------"
        ]
      }
    }
  ]
}