{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "c9b6b50ff640b531cc953249311a78f5b75ce349",
      "candidate_info": {
        "commit_hash": "c9b6b50ff640b531cc953249311a78f5b75ce349",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/c9b6b50ff640b531cc953249311a78f5b75ce349",
        "files": [
          "python/pyspark/ml/base.py",
          "python/pyspark/ml/evaluation.py",
          "python/pyspark/tests/test_worker.py"
        ],
        "message": "[SPARK-39049][PYTHON][CORE][ML] Remove unneeded `pass`\n\n### What changes were proposed in this pull request?\nRemove unneeded `pass`\n\n### Why are the changes needed?\nClass`s Estimator, Transformer and Evaluator are abstract classes. Which has functions.\n\nValueError in def run() has code.\n\nBy removing `pass` it will be easier to read, understand and reuse code.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nExisting tests passed.\n\nCloses #36383 from bjornjorgensen/remove-unneeded-pass.\n\nLead-authored-by: Bj\u00f8rn J\u00f8rgensen <bjornjorgensen@gmail.com>\nCo-authored-by: bjornjorgensen <bjornjorgensen@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 0e875875059c1cbf36de49205a4ce8dbc483d9d1)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/ml/base.py||python/pyspark/ml/base.py",
          "python/pyspark/ml/evaluation.py||python/pyspark/ml/evaluation.py",
          "python/pyspark/tests/test_worker.py||python/pyspark/tests/test_worker.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/ml/base.py||python/pyspark/ml/base.py": [
          "File: python/pyspark/ml/base.py -> python/pyspark/ml/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "110:     .. versionadded:: 1.3.0",
          "111:     \"\"\"",
          "115:     @abstractmethod",
          "116:     def _fit(self, dataset: DataFrame) -> M:",
          "117:         \"\"\"",
          "",
          "[Removed Lines]",
          "113:     pass",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "220:     .. versionadded:: 1.3.0",
          "221:     \"\"\"",
          "225:     @abstractmethod",
          "226:     def _transform(self, dataset: DataFrame) -> DataFrame:",
          "227:         \"\"\"",
          "",
          "[Removed Lines]",
          "223:     pass",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "python/pyspark/ml/evaluation.py||python/pyspark/ml/evaluation.py": [
          "File: python/pyspark/ml/evaluation.py -> python/pyspark/ml/evaluation.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "67:     .. versionadded:: 1.4.0",
          "68:     \"\"\"",
          "72:     @abstractmethod",
          "73:     def _evaluate(self, dataset: DataFrame) -> float:",
          "74:         \"\"\"",
          "",
          "[Removed Lines]",
          "70:     pass",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "python/pyspark/tests/test_worker.py||python/pyspark/tests/test_worker.py": [
          "File: python/pyspark/tests/test_worker.py -> python/pyspark/tests/test_worker.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "70:                 try:",
          "71:                     daemon_pid, worker_pid = map(int, data)",
          "72:                 except ValueError:",
          "74:                     # In case the value is not written yet.",
          "75:                     cnt += 1",
          "76:                     if cnt == 10:",
          "",
          "[Removed Lines]",
          "73:                     pass",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "44e90f37ad653d1ad3f8475926c261b9dba78a32",
      "candidate_info": {
        "commit_hash": "44e90f37ad653d1ad3f8475926c261b9dba78a32",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/44e90f37ad653d1ad3f8475926c261b9dba78a32",
        "files": [
          "core/src/main/resources/error/error-classes.json",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "sql/core/src/test/resources/sql-tests/results/ansi/array.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/map.sql.out"
        ],
        "message": "[SPARK-38967][SQL] Turn \"spark.sql.ansi.strictIndexOperator\" into an internal configuration\n\n### What changes were proposed in this pull request?\n\nCurrently, most the ANSI error message shows the hint \"If necessary set spark.sql.ansi.enabled to false to bypass this error.\"\n\nThere is only one special case: \"Map key not exist\" or \"array index out of bound\" from the `[]` operator. It shows the config spark.sql.ansi.strictIndexOperator instead.\n\nThis one special case can confuse users. To make it simple:\n- Turn \"spark.sql.ansi.strictIndexOperator\" into an internal configuration\n- Show the configuration `spark.sql.ansi.enabled` in error messages instead\n- If it is \"map key not exist\" error, show the hint for using `try_element_at`. Otherwise, we don't show it. For array, `[]` operator is using 0-based index while `try_element_at` is using 1-based index.\n\n### Why are the changes needed?\n\nMake the hints in ANSI runtime error message simple and consistent\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, the new configuration is not released yet.\n\n### How was this patch tested?\n\nExisting UT\n\nCloses #36282 from gengliangwang/updateErrorMsg.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 276bdbafe83a5c0b8425a20eb8101a630be8b752)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2116:     case MapType(_, valueType, _) => valueType",
          "2117:   }",
          "2121:   override def inputTypes: Seq[AbstractDataType] = {",
          "2122:     (left.dataType, right.dataType) match {",
          "2123:       case (arr: ArrayType, e2: IntegralType) if (e2 != LongType) =>",
          "",
          "[Removed Lines]",
          "2119:   override val isElementAtFunction: Boolean = true",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "342: trait GetMapValueUtil extends BinaryExpression with ImplicitCastInputTypes {",
          "347:   def getValueEval(",
          "348:       value: Any,",
          "",
          "[Removed Lines]",
          "344:   protected val isElementAtFunction: Boolean = false",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "368:     if (!found) {",
          "369:       if (failOnError) {",
          "371:       } else {",
          "372:         null",
          "373:       }",
          "",
          "[Removed Lines]",
          "370:         throw QueryExecutionErrors.mapKeyNotExistError(ordinal, isElementAtFunction, origin.context)",
          "",
          "[Added Lines]",
          "368:         throw QueryExecutionErrors.mapKeyNotExistError(ordinal, origin.context)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "403:     lazy val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "404:     nullSafeCodeGen(ctx, ev, (eval1, eval2) => {",
          "405:       val keyNotFoundBranch = if (failOnError) {",
          "408:       } else {",
          "409:         s\"${ev.isNull} = true;\"",
          "410:       }",
          "",
          "[Removed Lines]",
          "406:         s\"throw QueryExecutionErrors.mapKeyNotExistError(\" +",
          "407:           s\"$eval2, $isElementAtFunction, $errorContext);\"",
          "",
          "[Added Lines]",
          "404:         s\"throw QueryExecutionErrors.mapKeyNotExistError($eval2, $errorContext);\"",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "152:   }",
          "154:   def invalidArrayIndexError(index: Int, numElements: Int): ArrayIndexOutOfBoundsException = {",
          "156:   }",
          "158:   def invalidInputIndexError(index: Int, numElements: Int): ArrayIndexOutOfBoundsException = {",
          "",
          "[Removed Lines]",
          "155:     invalidArrayIndexErrorInternal(index, numElements, SQLConf.ANSI_STRICT_INDEX_OPERATOR.key)",
          "",
          "[Added Lines]",
          "155:     invalidArrayIndexErrorInternal(index, numElements, SQLConf.ANSI_ENABLED.key)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "176:         Array(toSQLValue(index), toSQLValue(numElements), SQLConf.ANSI_ENABLED.key))",
          "177:   }",
          "190:   }",
          "192:   def inputTypeUnsupportedError(dataType: DataType): Throwable = {",
          "",
          "[Removed Lines]",
          "179:   def mapKeyNotExistError(",
          "180:       key: Any,",
          "181:       isElementAtFunction: Boolean,",
          "182:       context: String): NoSuchElementException = {",
          "183:     if (isElementAtFunction) {",
          "184:       new SparkNoSuchElementException(errorClass = \"MAP_KEY_DOES_NOT_EXIST_IN_ELEMENT_AT\",",
          "185:         messageParameters = Array(toSQLValue(key), SQLConf.ANSI_ENABLED.key, context))",
          "186:     } else {",
          "187:       new SparkNoSuchElementException(errorClass = \"MAP_KEY_DOES_NOT_EXIST\",",
          "188:         messageParameters = Array(toSQLValue(key), SQLConf.ANSI_STRICT_INDEX_OPERATOR.key, context))",
          "189:     }",
          "",
          "[Added Lines]",
          "179:   def mapKeyNotExistError(key: Any, context: String): NoSuchElementException = {",
          "180:     new SparkNoSuchElementException(errorClass = \"MAP_KEY_DOES_NOT_EXIST\",",
          "181:       messageParameters = Array(toSQLValue(key), SQLConf.ANSI_ENABLED.key, context))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2855:     .createWithDefault(false)",
          "2857:   val ANSI_STRICT_INDEX_OPERATOR = buildConf(\"spark.sql.ansi.strictIndexOperator\")",
          "2858:     .doc(s\"When true and '${ANSI_ENABLED.key}' is true, accessing complex SQL types via [] \" +",
          "2859:       \"operator will throw an exception if array index is out of bound, or map key does not \" +",
          "2860:       \"exist. Otherwise, Spark will return a null result when accessing an invalid index.\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2858:     .internal()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "9d0650a1a7fd61611be525a7c263bcfc54b7ad25",
      "candidate_info": {
        "commit_hash": "9d0650a1a7fd61611be525a7c263bcfc54b7ad25",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/9d0650a1a7fd61611be525a7c263bcfc54b7ad25",
        "files": [
          "core/src/main/resources/error/error-classes.json",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/TryCastSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateFormatterSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/types/DecimalSuite.scala",
          "sql/core/src/test/resources/sql-tests/inputs/cast.sql",
          "sql/core/src/test/resources/sql-tests/results/ansi/cast.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/date.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/datetime-parsing-invalid.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/string-functions.sql.out",
          "sql/core/src/test/resources/sql-tests/results/cast.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/float4.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/float8.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/text.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part2.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part3.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part4.sql.out",
          "sql/core/src/test/resources/sql-tests/results/string-functions.sql.out",
          "sql/core/src/test/resources/sql-tests/results/timestampNTZ/timestamp-ansi.sql.out",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala"
        ],
        "message": "[SPARK-38929][SQL][3.3] Improve error messages for cast failures in ANSI\n\n### What changes were proposed in this pull request?\n\nImprove the error messages for cast failures in ANSI.\nAs mentioned in https://issues.apache.org/jira/browse/SPARK-38929, this PR targets two cast-to types: numeric types and date types.\n* For numeric(`int`, `smallint`, `double`, `float`, `decimal` ..) types, it embeds the cast-to types in the error message. For example,\n  ```\n  Invalid input value for type INT: '1.0'. To return NULL instead, use 'try_cast'. If necessary set %s to false to bypass this error.\n  ```\n  It uses the `toSQLType` and `toSQLValue` to wrap the corresponding types and literals.\n* For date types, it does similarly as above. For example,\n  ```\n  Invalid input value for type TIMESTAMP: 'a'. To return NULL instead, use 'try_cast'. If necessary set spark.sql.ansi.enabled to false to bypass this error.\n  ```\n\n### Why are the changes needed?\nTo improve the error message in general.\n\n### Does this PR introduce _any_ user-facing change?\nIt changes the error messages.\n\n### How was this patch tested?\nThe related unit tests are updated.\n\nAuthored-by: Xinyi Yu <xinyi.yudatabricks.com>\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n(cherry picked from commit f76b3e766f79b4c2d4f1ecffaad25aeb962336b7)\n\nCloses #36275 from anchovYu/ansi-error-improve-3.3.\n\nAuthored-by: Xinyi Yu <xinyi.yu@databricks.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/TryCastSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/TryCastSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateFormatterSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateFormatterSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/types/DecimalSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/types/DecimalSuite.scala",
          "sql/core/src/test/resources/sql-tests/inputs/cast.sql||sql/core/src/test/resources/sql-tests/inputs/cast.sql",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "816:       })",
          "817:     case StringType if ansiEnabled =>",
          "818:       buildCast[UTF8String](_,",
          "820:     case BooleanType =>",
          "821:       buildCast[Boolean](_, b => toPrecision(if (b) Decimal.ONE else Decimal.ZERO, target))",
          "822:     case DateType =>",
          "",
          "[Removed Lines]",
          "819:         s => changePrecision(Decimal.fromStringANSI(s, origin.context), target))",
          "",
          "[Added Lines]",
          "819:         s => changePrecision(Decimal.fromStringANSI(s, target, origin.context), target))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "845:           case _: NumberFormatException =>",
          "846:             val d = Cast.processFloatingPointSpecialLiterals(doubleStr, false)",
          "847:             if(ansiEnabled && d == null) {",
          "849:             } else {",
          "850:               d",
          "851:             }",
          "",
          "[Removed Lines]",
          "848:               throw QueryExecutionErrors.invalidInputSyntaxForNumericError(s, origin.context)",
          "",
          "[Added Lines]",
          "848:               throw QueryExecutionErrors.invalidInputSyntaxForNumericError(",
          "849:                 DoubleType, s, origin.context)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "870:           case _: NumberFormatException =>",
          "871:             val f = Cast.processFloatingPointSpecialLiterals(floatStr, true)",
          "872:             if (ansiEnabled && f == null) {",
          "874:             } else {",
          "875:               f",
          "876:             }",
          "",
          "[Removed Lines]",
          "873:               throw QueryExecutionErrors.invalidInputSyntaxForNumericError(s, origin.context)",
          "",
          "[Added Lines]",
          "874:               throw QueryExecutionErrors.invalidInputSyntaxForNumericError(",
          "875:                 FloatType, s, origin.context)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1376:           \"\"\"",
          "1377:       case StringType if ansiEnabled =>",
          "1378:         val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "1379:         (c, evPrim, evNull) =>",
          "1380:           code\"\"\"",
          "1382:               ${changePrecision(tmp, target, evPrim, evNull, canNullSafeCast, ctx)}",
          "1383:           \"\"\"",
          "1384:       case BooleanType =>",
          "",
          "[Removed Lines]",
          "1381:               Decimal $tmp = Decimal.fromStringANSI($c, $errorContext);",
          "",
          "[Added Lines]",
          "1381:         val toType = ctx.addReferenceObj(\"toType\", target)",
          "1384:               Decimal $tmp = Decimal.fromStringANSI($c, $toType, $errorContext);",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1899:         (c, evPrim, evNull) =>",
          "1900:           val handleNull = if (ansiEnabled) {",
          "1901:             val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "1903:           } else {",
          "1904:             s\"$evNull = true;\"",
          "1905:           }",
          "",
          "[Removed Lines]",
          "1902:             s\"throw QueryExecutionErrors.invalidInputSyntaxForNumericError($c, $errorContext);\"",
          "",
          "[Added Lines]",
          "1905:             s\"throw QueryExecutionErrors.invalidInputSyntaxForNumericError(\" +",
          "1906:               s\"org.apache.spark.sql.types.FloatType$$.MODULE$$,$c, $errorContext);\"",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1936:         (c, evPrim, evNull) =>",
          "1937:           val handleNull = if (ansiEnabled) {",
          "1938:             val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "1940:           } else {",
          "1941:             s\"$evNull = true;\"",
          "1942:           }",
          "",
          "[Removed Lines]",
          "1939:             s\"throw QueryExecutionErrors.invalidInputSyntaxForNumericError($c, $errorContext);\"",
          "",
          "[Added Lines]",
          "1943:             s\"throw QueryExecutionErrors.invalidInputSyntaxForNumericError(\" +",
          "1944:               s\"org.apache.spark.sql.types.DoubleType$$.MODULE$$, $c, $errorContext);\"",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/UTF8StringUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.catalyst.util",
          "20: import org.apache.spark.sql.errors.QueryExecutionErrors",
          "21: import org.apache.spark.unsafe.types.UTF8String",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import org.apache.spark.sql.types.{ByteType, DataType, IntegerType, LongType, ShortType}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "26: object UTF8StringUtils {",
          "28:   def toLongExact(s: UTF8String, errorContext: String): Long =",
          "31:   def toIntExact(s: UTF8String, errorContext: String): Int =",
          "34:   def toShortExact(s: UTF8String, errorContext: String): Short =",
          "37:   def toByteExact(s: UTF8String, errorContext: String): Byte =",
          "41:     try {",
          "42:       f",
          "43:     } catch {",
          "44:       case e: NumberFormatException =>",
          "46:     }",
          "47:   }",
          "48: }",
          "",
          "[Removed Lines]",
          "29:     withException(s.toLongExact, errorContext)",
          "32:     withException(s.toIntExact, errorContext)",
          "35:     withException(s.toShortExact, errorContext)",
          "38:     withException(s.toByteExact, errorContext)",
          "40:   private def withException[A](f: => A, errorContext: String): A = {",
          "45:         throw QueryExecutionErrors.invalidInputSyntaxForNumericError(e, errorContext)",
          "",
          "[Added Lines]",
          "30:     withException(s.toLongExact, errorContext, LongType, s)",
          "33:     withException(s.toIntExact, errorContext, IntegerType, s)",
          "36:     withException(s.toShortExact, errorContext, ShortType, s)",
          "39:     withException(s.toByteExact, errorContext, ByteType, s)",
          "41:   private def withException[A](f: => A, errorContext: String, to: DataType, s: UTF8String): A = {",
          "46:         throw QueryExecutionErrors.invalidInputSyntaxForNumericError(to, s, errorContext)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "112:   }",
          "114:   def invalidInputSyntaxForNumericError(",
          "115:       s: UTF8String,",
          "116:       errorContext: String): NumberFormatException = {",
          "119:   }",
          "121:   def cannotCastFromNullTypeError(to: DataType): Throwable = {",
          "",
          "[Removed Lines]",
          "117:     new SparkNumberFormatException(errorClass = \"INVALID_INPUT_SYNTAX_FOR_NUMERIC_TYPE\",",
          "118:       messageParameters = Array(toSQLValue(s, StringType), SQLConf.ANSI_ENABLED.key, errorContext))",
          "",
          "[Added Lines]",
          "115:       to: DataType,",
          "118:     new SparkNumberFormatException(errorClass = \"INVALID_SYNTAX_FOR_CAST\",",
          "119:       messageParameters = Array(toSQLType(to), toSQLValue(s, StringType),",
          "120:         SQLConf.ANSI_ENABLED.key, errorContext))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1048:   }",
          "1050:   def cannotCastToDateTimeError(value: Any, to: DataType, errorContext: String): Throwable = {",
          "1053:   }",
          "1055:   def registeringStreamingQueryListenerError(e: Exception): Throwable = {",
          "",
          "[Removed Lines]",
          "1051:     new DateTimeException(s\"Cannot cast $value to $to. To return NULL instead, use 'try_cast'. \" +",
          "1052:       s\"If necessary set ${SQLConf.ANSI_ENABLED.key} to false to bypass this error.\" + errorContext)",
          "",
          "[Added Lines]",
          "1053:     val valueString = if (value.isInstanceOf[UTF8String]) {",
          "1054:       toSQLValue(value, StringType)",
          "1055:     } else {",
          "1056:       toSQLValue(value)",
          "1057:     }",
          "1058:     new DateTimeException(s\"Invalid input syntax for type ${toSQLType(to)}: $valueString. \" +",
          "1059:       s\"To return NULL instead, use 'try_cast'. If necessary set ${SQLConf.ANSI_ENABLED.key} \" +",
          "1060:       s\"to false to bypass this error.\" + errorContext)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/types/Decimal.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "613:     }",
          "614:   }",
          "617:     try {",
          "618:       val bigDecimal = stringToJavaBigDecimal(str)",
          "",
          "[Removed Lines]",
          "616:   def fromStringANSI(str: UTF8String, errorContext: String = \"\"): Decimal = {",
          "",
          "[Added Lines]",
          "616:   def fromStringANSI(",
          "617:       str: UTF8String,",
          "618:       to: DecimalType = DecimalType.USER_DEFAULT,",
          "619:       errorContext: String = \"\"): Decimal = {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "626:       }",
          "627:     } catch {",
          "628:       case _: NumberFormatException =>",
          "630:     }",
          "631:   }",
          "",
          "[Removed Lines]",
          "629:         throw QueryExecutionErrors.invalidInputSyntaxForNumericError(str, errorContext)",
          "",
          "[Added Lines]",
          "632:         throw QueryExecutionErrors.invalidInputSyntaxForNumericError(to, str, errorContext)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/AnsiCastSuiteBase.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import org.apache.spark.sql.catalyst.util.DateTimeConstants.MILLIS_PER_SECOND",
          "27: import org.apache.spark.sql.catalyst.util.DateTimeTestUtils",
          "28: import org.apache.spark.sql.catalyst.util.DateTimeTestUtils.{withDefaultTimeZone, UTC}",
          "29: import org.apache.spark.sql.internal.SQLConf",
          "30: import org.apache.spark.sql.types._",
          "31: import org.apache.spark.unsafe.types.UTF8String",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "29: import org.apache.spark.sql.errors.QueryExecutionErrors.toSQLValue",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "174:   test(\"cast from invalid string to numeric should throw NumberFormatException\") {",
          "176:     Seq(IntegerType, ShortType, ByteType, LongType).foreach { dataType =>",
          "185:     }",
          "187:     Seq(DoubleType, FloatType, DecimalType.USER_DEFAULT).foreach { dataType =>",
          "194:     }",
          "195:   }",
          "198:     checkExceptionInExpression[NumberFormatException](",
          "200:   }",
          "202:   test(\"cast from invalid string array to numeric array should throw NumberFormatException\") {",
          "203:     val array = Literal.create(Seq(\"123\", \"true\", \"f\", null),",
          "204:       ArrayType(StringType, containsNull = true))",
          "207:       Seq(123.toByte, null, null, null))",
          "209:       Seq(123.toShort, null, null, null))",
          "211:       Seq(123, null, null, null))",
          "213:       Seq(123L, null, null, null))",
          "214:   }",
          "",
          "[Removed Lines]",
          "177:       checkExceptionInExpression[NumberFormatException](",
          "178:         cast(\"string\", dataType), \"invalid input syntax for type numeric: 'string'\")",
          "179:       checkExceptionInExpression[NumberFormatException](",
          "180:         cast(\"123-string\", dataType), \"invalid input syntax for type numeric: '123-string'\")",
          "181:       checkExceptionInExpression[NumberFormatException](",
          "182:         cast(\"2020-07-19\", dataType), \"invalid input syntax for type numeric: '2020-07-19'\")",
          "183:       checkExceptionInExpression[NumberFormatException](",
          "184:         cast(\"1.23\", dataType), \"invalid input syntax for type numeric: '1.23'\")",
          "188:       checkExceptionInExpression[NumberFormatException](",
          "189:         cast(\"string\", dataType), \"invalid input syntax for type numeric: 'string'\")",
          "190:       checkExceptionInExpression[NumberFormatException](",
          "191:         cast(\"123.000.00\", dataType), \"invalid input syntax for type numeric: '123.000.00'\")",
          "192:       checkExceptionInExpression[NumberFormatException](",
          "193:         cast(\"abc.com\", dataType), \"invalid input syntax for type numeric: 'abc.com'\")",
          "197:   protected def checkCastToNumericError(l: Literal, to: DataType, tryCastResult: Any): Unit = {",
          "199:       cast(l, to), \"invalid input syntax for type numeric: 'true'\")",
          "206:     checkCastToNumericError(array, ArrayType(ByteType, containsNull = true),",
          "208:     checkCastToNumericError(array, ArrayType(ShortType, containsNull = true),",
          "210:     checkCastToNumericError(array, ArrayType(IntegerType, containsNull = true),",
          "212:     checkCastToNumericError(array, ArrayType(LongType, containsNull = true),",
          "",
          "[Added Lines]",
          "178:       checkExceptionInExpression[NumberFormatException](cast(\"string\", dataType),",
          "179:         s\"Invalid input syntax for type ${dataType.sql}: 'string'\")",
          "180:       checkExceptionInExpression[NumberFormatException](cast(\"123-string\", dataType),",
          "181:         s\"Invalid input syntax for type ${dataType.sql}: '123-string'\")",
          "182:       checkExceptionInExpression[NumberFormatException](cast(\"2020-07-19\", dataType),",
          "183:         s\"Invalid input syntax for type ${dataType.sql}: '2020-07-19'\")",
          "184:       checkExceptionInExpression[NumberFormatException](cast(\"1.23\", dataType),",
          "185:         s\"Invalid input syntax for type ${dataType.sql}: '1.23'\")",
          "189:       checkExceptionInExpression[NumberFormatException](cast(\"string\", dataType),",
          "190:         s\"Invalid input syntax for type ${dataType.sql}: 'string'\")",
          "191:       checkExceptionInExpression[NumberFormatException](cast(\"123.000.00\", dataType),",
          "192:         s\"Invalid input syntax for type ${dataType.sql}: '123.000.00'\")",
          "193:       checkExceptionInExpression[NumberFormatException](cast(\"abc.com\", dataType),",
          "194:         s\"Invalid input syntax for type ${dataType.sql}: 'abc.com'\")",
          "198:   protected def checkCastToNumericError(l: Literal, to: DataType,",
          "199:       expectedDataTypeInErrorMsg: DataType, tryCastResult: Any): Unit = {",
          "201:       cast(l, to), s\"Invalid input syntax for type ${expectedDataTypeInErrorMsg.sql}: 'true'\")",
          "208:     checkCastToNumericError(array, ArrayType(ByteType, containsNull = true), ByteType,",
          "210:     checkCastToNumericError(array, ArrayType(ShortType, containsNull = true), ShortType,",
          "212:     checkCastToNumericError(array, ArrayType(IntegerType, containsNull = true), IntegerType,",
          "214:     checkCastToNumericError(array, ArrayType(LongType, containsNull = true), LongType,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "244:     checkExceptionInExpression[NumberFormatException](",
          "245:       cast(\"abcd\", DecimalType(38, 1)),",
          "247:   }",
          "249:   protected def checkCastToBooleanError(l: Literal, to: DataType, tryCastResult: Any): Unit = {",
          "",
          "[Removed Lines]",
          "246:       \"invalid input syntax for type numeric\")",
          "",
          "[Added Lines]",
          "248:       s\"Invalid input syntax for type ${DecimalType(38, 1).sql}: 'abcd'\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "259:   protected def checkCastToTimestampError(l: Literal, to: DataType): Unit = {",
          "260:     checkExceptionInExpression[DateTimeException](",
          "262:   }",
          "264:   test(\"cast from timestamp II\") {",
          "",
          "[Removed Lines]",
          "261:       cast(l, to), s\"Cannot cast $l to $to\")",
          "",
          "[Added Lines]",
          "263:       cast(l, to), s\"Invalid input syntax for type TIMESTAMP: ${toSQLValue(l)}\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "369:       assert(ret.resolved == !isTryCast)",
          "370:       if (!isTryCast) {",
          "371:         checkExceptionInExpression[NumberFormatException](",
          "373:       }",
          "374:     }",
          "",
          "[Removed Lines]",
          "372:           ret, \"invalid input syntax for type numeric\")",
          "",
          "[Added Lines]",
          "374:           ret, s\"Invalid input syntax for type ${IntegerType.sql}\")",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "387:       assert(ret.resolved == !isTryCast)",
          "388:       if (!isTryCast) {",
          "389:         checkExceptionInExpression[NumberFormatException](",
          "391:       }",
          "392:     }",
          "393:   }",
          "",
          "[Removed Lines]",
          "390:           ret, \"invalid input syntax for type numeric\")",
          "",
          "[Added Lines]",
          "392:           ret, s\"Invalid input syntax for type ${IntegerType.sql}\")",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "512:     assert(ret.resolved === !isTryCast)",
          "513:     if (!isTryCast) {",
          "514:       checkExceptionInExpression[NumberFormatException](",
          "516:     }",
          "517:   }",
          "",
          "[Removed Lines]",
          "515:         ret, \"invalid input syntax for type numeric\")",
          "",
          "[Added Lines]",
          "517:         ret, s\"Invalid input syntax for type ${IntegerType.sql}\")",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "521:       def checkCastWithParseError(str: String): Unit = {",
          "522:         checkExceptionInExpression[DateTimeException](",
          "523:           cast(Literal(str), TimestampType, Option(zid.getId)),",
          "525:       }",
          "527:       checkCastWithParseError(\"123\")",
          "",
          "[Removed Lines]",
          "524:           s\"Cannot cast $str to TimestampType.\")",
          "",
          "[Added Lines]",
          "526:           s\"Invalid input syntax for type TIMESTAMP: '$str'\")",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "542:       def checkCastWithParseError(str: String): Unit = {",
          "543:         checkExceptionInExpression[DateTimeException](",
          "544:           cast(Literal(str), DateType, Option(zid.getId)),",
          "546:       }",
          "548:       checkCastWithParseError(\"2015-13-18\")",
          "",
          "[Removed Lines]",
          "545:           s\"Cannot cast $str to DateType.\")",
          "",
          "[Added Lines]",
          "547:           s\"Invalid input syntax for type DATE: '$str'\")",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "570:       \"2021-06-17 00:00:00ABC\").foreach { invalidInput =>",
          "571:       checkExceptionInExpression[DateTimeException](",
          "572:         cast(invalidInput, TimestampNTZType),",
          "574:     }",
          "575:   }",
          "576: }",
          "",
          "[Removed Lines]",
          "573:         s\"Cannot cast $invalidInput to TimestampNTZType\")",
          "",
          "[Added Lines]",
          "575:         s\"Invalid input syntax for type TIMESTAMP_NTZ: '$invalidInput'\")",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/TryCastSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/TryCastSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/TryCastSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/TryCastSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "45:     checkEvaluation(cast(l, to), tryCastResult, InternalRow(l.value))",
          "46:   }",
          "49:     checkEvaluation(cast(l, to), tryCastResult, InternalRow(l.value))",
          "50:   }",
          "",
          "[Removed Lines]",
          "48:   override def checkCastToNumericError(l: Literal, to: DataType, tryCastResult: Any): Unit = {",
          "",
          "[Added Lines]",
          "48:   override def checkCastToNumericError(l: Literal, to: DataType,",
          "49:       expectedDataTypeInErrorMsg: DataType, tryCastResult: Any): Unit = {",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateFormatterSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateFormatterSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateFormatterSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/DateFormatterSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "208:     val errMsg = intercept[DateTimeException] {",
          "209:       formatter.parse(\"x123\")",
          "210:     }.getMessage",
          "212:   }",
          "213: }",
          "",
          "[Removed Lines]",
          "211:     assert(errMsg.contains(\"Cannot cast x123 to DateType\"))",
          "",
          "[Added Lines]",
          "211:     assert(errMsg.contains(\"Invalid input syntax for type DATE: 'x123'\"))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/util/TimestampFormatterSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "453:       val errMsg = intercept[DateTimeException] {",
          "454:         formatter.parse(\"x123\")",
          "455:       }.getMessage",
          "457:     }",
          "458:   }",
          "459: }",
          "",
          "[Removed Lines]",
          "456:       assert(errMsg.contains(\"Cannot cast x123 to TimestampType\"))",
          "",
          "[Added Lines]",
          "456:       assert(errMsg.contains(\"Invalid input syntax for type TIMESTAMP: 'x123'\"))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/types/DecimalSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/types/DecimalSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/types/DecimalSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/types/DecimalSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "285:     assert(Decimal.fromString(UTF8String.fromString(\"str\")) === null)",
          "286:     val e = intercept[NumberFormatException](Decimal.fromStringANSI(UTF8String.fromString(\"str\")))",
          "288:   }",
          "290:   test(\"SPARK-35841: Casting string to decimal type doesn't work \" +",
          "",
          "[Removed Lines]",
          "287:     assert(e.getMessage.contains(\"invalid input syntax for type numeric\"))",
          "",
          "[Added Lines]",
          "287:     assert(e.getMessage.contains(\"Invalid input syntax for type \" +",
          "288:       s\"${DecimalType.USER_DEFAULT.sql}: 'str'\"))",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/cast.sql||sql/core/src/test/resources/sql-tests/inputs/cast.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/cast.sql -> sql/core/src/test/resources/sql-tests/inputs/cast.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "4: SELECT CAST('-4.56' AS int);",
          "5: SELECT CAST('-4.56' AS long);",
          "8: SELECT CAST('abc' AS int);",
          "9: SELECT CAST('abc' AS long);",
          "11: -- cast string representing a very large number to integral should return null",
          "12: SELECT CAST('1234567890123' AS int);",
          "",
          "[Removed Lines]",
          "7: -- cast string which are not numbers to integral should return null",
          "",
          "[Added Lines]",
          "7: -- cast string which are not numbers to numeric types",
          "10: SELECT CAST('abc' AS float);",
          "11: SELECT CAST('abc' AS double);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "15: -- cast empty string to integral should return null",
          "16: SELECT CAST('' AS int);",
          "17: SELECT CAST('' AS long);",
          "19: -- cast null to integral should return null",
          "20: SELECT CAST(NULL AS int);",
          "21: SELECT CAST(NULL AS long);",
          "24: SELECT CAST('123.a' AS int);",
          "25: SELECT CAST('123.a' AS long);",
          "27: -- '-2147483648' is the smallest int value",
          "28: SELECT CAST('-2147483648' AS int);",
          "",
          "[Removed Lines]",
          "23: -- cast invalid decimal string to integral should return null",
          "",
          "[Added Lines]",
          "20: SELECT CAST('' AS float);",
          "21: SELECT CAST('' AS double);",
          "27: -- cast invalid decimal string to numeric types",
          "30: SELECT CAST('123.a' AS float);",
          "31: SELECT CAST('123.a' AS double);",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SQLInsertTestSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "302:             val errorMsg = intercept[NumberFormatException] {",
          "303:               sql(\"insert into t partition(a='ansi') values('ansi')\")",
          "304:             }.getMessage",
          "306:           } else {",
          "307:             sql(\"insert into t partition(a='ansi') values('ansi')\")",
          "308:             checkAnswer(sql(\"select * from t\"), Row(\"ansi\", null) :: Nil)",
          "",
          "[Removed Lines]",
          "305:             assert(errorMsg.contains(\"invalid input syntax for type numeric: 'ansi'\"))",
          "",
          "[Added Lines]",
          "305:             assert(errorMsg.contains(\"Invalid input syntax for type INT: 'ansi'\"))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "baeaaeb8cbb8a69b15fac1df7063186dfa81e6a8",
      "candidate_info": {
        "commit_hash": "baeaaeb8cbb8a69b15fac1df7063186dfa81e6a8",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/baeaaeb8cbb8a69b15fac1df7063186dfa81e6a8",
        "files": [
          "dev/deps/spark-deps-hadoop-2-hive-2.3",
          "dev/deps/spark-deps-hadoop-3-hive-2.3",
          "pom.xml"
        ],
        "message": "[SPARK-38784][CORE] Upgrade Jetty to 9.4.46\n\n### What changes were proposed in this pull request?\n\nUpgrade Jetty to 9.4.46\n\n### Why are the changes needed?\n\nThree CVEs, which don't necessarily appear to affect Spark, are fixed in this version. Just housekeeping.\nCVE-2021-28169\nCVE-2021-34428\nCVE-2021-34429\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36229 from srowen/SPARK-38784.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 619b7b4345013684e814499f8cec3b99ba9d88c2)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "dev/deps/spark-deps-hadoop-2-hive-2.3||dev/deps/spark-deps-hadoop-2-hive-2.3",
          "dev/deps/spark-deps-hadoop-3-hive-2.3||dev/deps/spark-deps-hadoop-3-hive-2.3"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/deps/spark-deps-hadoop-2-hive-2.3||dev/deps/spark-deps-hadoop-2-hive-2.3": [
          "File: dev/deps/spark-deps-hadoop-2-hive-2.3 -> dev/deps/spark-deps-hadoop-2-hive-2.3",
          "--- Hunk 1 ---",
          "[Context before]",
          "146: jersey-server/2.34//jersey-server-2.34.jar",
          "147: jetty-sslengine/6.1.26//jetty-sslengine-6.1.26.jar",
          "148: jetty-util/6.1.26//jetty-util-6.1.26.jar",
          "150: jetty/6.1.26//jetty-6.1.26.jar",
          "151: jline/2.14.6//jline-2.14.6.jar",
          "152: joda-time/2.10.13//joda-time-2.10.13.jar",
          "",
          "[Removed Lines]",
          "149: jetty-util/9.4.44.v20210927//jetty-util-9.4.44.v20210927.jar",
          "",
          "[Added Lines]",
          "149: jetty-util/9.4.46.v20220331//jetty-util-9.4.46.v20220331.jar",
          "",
          "---------------"
        ],
        "dev/deps/spark-deps-hadoop-3-hive-2.3||dev/deps/spark-deps-hadoop-3-hive-2.3": [
          "File: dev/deps/spark-deps-hadoop-3-hive-2.3 -> dev/deps/spark-deps-hadoop-3-hive-2.3",
          "--- Hunk 1 ---",
          "[Context before]",
          "133: jersey-hk2/2.34//jersey-hk2-2.34.jar",
          "134: jersey-server/2.34//jersey-server-2.34.jar",
          "135: jettison/1.1//jettison-1.1.jar",
          "138: jline/2.14.6//jline-2.14.6.jar",
          "139: joda-time/2.10.13//joda-time-2.10.13.jar",
          "140: jodd-core/3.5.2//jodd-core-3.5.2.jar",
          "",
          "[Removed Lines]",
          "136: jetty-util-ajax/9.4.44.v20210927//jetty-util-ajax-9.4.44.v20210927.jar",
          "137: jetty-util/9.4.44.v20210927//jetty-util-9.4.44.v20210927.jar",
          "",
          "[Added Lines]",
          "136: jetty-util-ajax/9.4.46.v20220331//jetty-util-ajax-9.4.46.v20220331.jar",
          "137: jetty-util/9.4.46.v20220331//jetty-util-9.4.46.v20220331.jar",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "fd998c8a6783c0c8aceed8dcde4017cd479e42c8",
      "candidate_info": {
        "commit_hash": "fd998c8a6783c0c8aceed8dcde4017cd479e42c8",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/fd998c8a6783c0c8aceed8dcde4017cd479e42c8",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/intervalExpressions.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala"
        ],
        "message": "[SPARK-39093][SQL] Avoid codegen compilation error when dividing year-month intervals or day-time intervals by an integral\n\n### What changes were proposed in this pull request?\n\nIn `DivideYMInterval#doGenCode` and `DivideDTInterval#doGenCode`, rely on the operand variable names provided by `nullSafeCodeGen` rather than calling `genCode` on the operands twice.\n\n### Why are the changes needed?\n\n`DivideYMInterval#doGenCode` and `DivideDTInterval#doGenCode` call `genCode` on the operands twice (once directly, and once indirectly via `nullSafeCodeGen`). However, if you call `genCode` on an operand twice, you might not get back the same variable name for both calls (e.g., when the operand is not a `BoundReference` or if whole-stage codegen is turned off). When that happens, `nullSafeCodeGen` generates initialization code for one set of variables, but the divide expression generates usage code for another set of variables, resulting in compilation errors like this:\n```\nspark-sql> create or replace temp view v1 as\n         > select * FROM VALUES\n         > (interval '10' months, interval '10' day, 2)\n         > as v1(period, duration, num);\nTime taken: 2.81 seconds\nspark-sql> cache table v1;\nTime taken: 2.184 seconds\nspark-sql> select period/(num + 3) from v1;\n22/05/03 08:56:37 ERROR CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 40, Column 44: Expression \"project_value_2\" is not an rvalue\n...\n22/05/03 08:56:37 WARN UnsafeProjection: Expr codegen error and falling back to interpreter mode\n...\n0-2\nTime taken: 0.149 seconds, Fetched 1 row(s)\nspark-sql> select duration/(num + 3) from v1;\n22/05/03 08:57:29 ERROR CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 40, Column 54: Expression \"project_value_2\" is not an rvalue\n...\n22/05/03 08:57:29 WARN UnsafeProjection: Expr codegen error and falling back to interpreter mode\n...\n2 00:00:00.000000000\nTime taken: 0.089 seconds, Fetched 1 row(s)\n```\nThe error is not fatal (unless you have `spark.sql.codegen.fallback` set to `false`), but it muddies the log and can slow the query (since the expression is interpreted).\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nNew unit tests (unit tests run with `spark.sql.codegen.fallback` set to `false`, so the new tests fail without the fix).\n\nCloses #36442 from bersprockets/interval_div_issue.\n\nAuthored-by: Bruce Robbins <bersprockets@gmail.com>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>\n(cherry picked from commit ca87bead23ca32a05c6a404a91cea47178f63e70)\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/intervalExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/intervalExpressions.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/intervalExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/intervalExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/intervalExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/intervalExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "670:           case _ => classOf[IntMath].getName",
          "671:         }",
          "672:         val javaType = CodeGenerator.javaType(dataType)",
          "683:           s\"\"\"",
          "684:              |${divideByZeroCheckCodegen(right.dataType, n, errorContext)}",
          "685:              |$checkIntegralDivideOverflow",
          "686:              |${ev.value} = ($javaType)$math.divide($m, $n, java.math.RoundingMode.HALF_UP);",
          "688:       case _: DecimalType =>",
          "689:         nullSafeCodeGen(ctx, ev, (m, n) =>",
          "690:           s\"\"\"",
          "",
          "[Removed Lines]",
          "673:         val months = left.genCode(ctx)",
          "674:         val num = right.genCode(ctx)",
          "675:         val checkIntegralDivideOverflow =",
          "676:           s\"\"\"",
          "677:              |if (${months.value} == ${Int.MinValue} && ${num.value} == -1)",
          "678:              |  throw QueryExecutionErrors.overflowInIntegralDivideError($errorContext);",
          "679:              |\"\"\".stripMargin",
          "680:         nullSafeCodeGen(ctx, ev, (m, n) =>",
          "687:           \"\"\".stripMargin)",
          "",
          "[Added Lines]",
          "673:         nullSafeCodeGen(ctx, ev, (m, n) => {",
          "674:           val checkIntegralDivideOverflow =",
          "675:             s\"\"\"",
          "676:                |if ($m == ${Int.MinValue} && $n == -1)",
          "677:                |  throw QueryExecutionErrors.overflowInIntegralDivideError($errorContext);",
          "678:                |\"\"\".stripMargin",
          "685:           \"\"\".stripMargin",
          "686:         })",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "744:     right.dataType match {",
          "745:       case _: IntegralType =>",
          "746:         val math = classOf[LongMath].getName",
          "755:           s\"\"\"",
          "756:              |${divideByZeroCheckCodegen(right.dataType, n, errorContext)}",
          "757:              |$checkIntegralDivideOverflow",
          "758:              |${ev.value} = $math.divide($m, $n, java.math.RoundingMode.HALF_UP);",
          "760:       case _: DecimalType =>",
          "761:         nullSafeCodeGen(ctx, ev, (m, n) =>",
          "762:           s\"\"\"",
          "",
          "[Removed Lines]",
          "747:         val micros = left.genCode(ctx)",
          "748:         val num = right.genCode(ctx)",
          "749:         val checkIntegralDivideOverflow =",
          "750:           s\"\"\"",
          "751:              |if (${micros.value} == ${Long.MinValue}L && ${num.value} == -1L)",
          "752:              |  throw QueryExecutionErrors.overflowInIntegralDivideError($errorContext);",
          "753:              |\"\"\".stripMargin",
          "754:         nullSafeCodeGen(ctx, ev, (m, n) =>",
          "759:           \"\"\".stripMargin)",
          "",
          "[Added Lines]",
          "746:         nullSafeCodeGen(ctx, ev, (m, n) => {",
          "747:           val checkIntegralDivideOverflow =",
          "748:             s\"\"\"",
          "749:                |if ($m == ${Long.MinValue}L && $n == -1L)",
          "750:                |  throw QueryExecutionErrors.overflowInIntegralDivideError($errorContext);",
          "751:                |\"\"\".stripMargin",
          "756:           \"\"\".stripMargin",
          "757:         })",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/ColumnExpressionSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2988:     checkAnswer(uncDf.filter($\"src\".ilike(\"\u0450\u0451\u0452\u047b\u03ce\u1ec1\")), Seq(\"\u0400\u0401\u0402\u047a\u038f\u1ec0\").toDF())",
          "2990:   }",
          "2991: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2992:   test(\"SPARK-39093: divide period by integral expression\") {",
          "2993:     val df = Seq(((Period.ofDays(10)), 2)).toDF(\"pd\", \"num\")",
          "2994:     checkAnswer(df.select($\"pd\" / ($\"num\" + 3)),",
          "2995:       Seq((Period.ofDays(2))).toDF)",
          "2996:   }",
          "2998:   test(\"SPARK-39093: divide duration by integral expression\") {",
          "2999:     val df = Seq(((Duration.ofDays(10)), 2)).toDF(\"dd\", \"num\")",
          "3000:     checkAnswer(df.select($\"dd\" / ($\"num\" + 3)),",
          "3001:       Seq((Duration.ofDays(2))).toDF)",
          "3002:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}