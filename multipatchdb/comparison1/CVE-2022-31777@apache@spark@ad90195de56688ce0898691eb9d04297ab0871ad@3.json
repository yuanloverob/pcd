{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "d7483b51f562d361be3dc995dfefcc4ac8d2e45f",
      "candidate_info": {
        "commit_hash": "d7483b51f562d361be3dc995dfefcc4ac8d2e45f",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/d7483b51f562d361be3dc995dfefcc4ac8d2e45f",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanPartitioning.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/connector/MetadataColumnSuite.scala"
        ],
        "message": "[SPARK-40429][SQL][3.3] Only set KeyGroupedPartitioning when the referenced column is in the output\n\n### What changes were proposed in this pull request?\nback porting [PR](https://github.com/apache/spark/pull/37886) to 3.3.\nOnly set `KeyGroupedPartitioning` when the referenced column is in the output\n\n### Why are the changes needed?\nbug fixing\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nNew test\n\nCloses #37901 from huaxingao/3.3.\n\nAuthored-by: huaxingao <huaxin_gao@apple.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanPartitioning.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanPartitioning.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/connector/MetadataColumnSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/connector/MetadataColumnSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanPartitioning.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanPartitioning.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanPartitioning.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanPartitioning.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "39:       }",
          "41:       val catalystPartitioning = scan.outputPartitioning() match {",
          "44:         case _: UnknownPartitioning => None",
          "45:         case p => throw new IllegalArgumentException(\"Unsupported data source V2 partitioning \" +",
          "46:             \"type: \" + p.getClass.getSimpleName)",
          "",
          "[Removed Lines]",
          "42:         case kgp: KeyGroupedPartitioning => sequenceToOption(kgp.keys().map(",
          "43:           V2ExpressionUtils.toCatalystOpt(_, relation, funCatalogOpt)))",
          "",
          "[Added Lines]",
          "42:         case kgp: KeyGroupedPartitioning =>",
          "43:           val partitioning = sequenceToOption(kgp.keys().map(",
          "44:             V2ExpressionUtils.toCatalystOpt(_, relation, funCatalogOpt)))",
          "45:           if (partitioning.isEmpty) {",
          "46:             None",
          "47:           } else {",
          "48:             if (partitioning.get.forall(p => p.references.subsetOf(d.outputSet))) {",
          "49:               partitioning",
          "50:             } else {",
          "51:               None",
          "52:             }",
          "53:           }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/connector/MetadataColumnSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/connector/MetadataColumnSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/connector/MetadataColumnSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/connector/MetadataColumnSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "216:       .withColumn(\"right_all\", struct($\"right.*\"))",
          "217:     checkAnswer(dfQuery, Row(1, \"a\", \"b\", Row(1, \"a\"), Row(1, \"b\")))",
          "218:   }",
          "219: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "220:   test(\"SPARK-40429: Only set KeyGroupedPartitioning when the referenced column is in the output\") {",
          "221:     withTable(tbl) {",
          "222:       sql(s\"CREATE TABLE $tbl (id bigint, data string) PARTITIONED BY (id)\")",
          "223:       sql(s\"INSERT INTO $tbl VALUES (1, 'a'), (2, 'b'), (3, 'c')\")",
          "224:       checkAnswer(",
          "225:         spark.table(tbl).select(\"index\", \"_partition\"),",
          "226:         Seq(Row(0, \"3\"), Row(0, \"2\"), Row(0, \"1\"))",
          "227:       )",
          "229:       checkAnswer(",
          "230:         spark.table(tbl).select(\"id\", \"index\", \"_partition\"),",
          "231:         Seq(Row(3, 0, \"3\"), Row(2, 0, \"2\"), Row(1, 0, \"1\"))",
          "232:       )",
          "233:     }",
          "234:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "fa9cbe21889a3d032687a152ec795ce1dd2db0ff",
      "candidate_info": {
        "commit_hash": "fa9cbe21889a3d032687a152ec795ce1dd2db0ff",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/fa9cbe21889a3d032687a152ec795ce1dd2db0ff",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala"
        ],
        "message": "[MINOR][SQL][SS][DOCS] Add varargs to Dataset.observe(String, ..) with a documentation fix\n\n### What changes were proposed in this pull request?\n\nThis PR proposes two minor changes:\n- Fixes the example at `Dataset.observe(String, ...)`\n- Adds `varargs` to be consistent with another overloaded version: `Dataset.observe(Observation, ..)`\n\n### Why are the changes needed?\n\nTo provide a correct example, support Java APIs properly with `varargs` and API consistency.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, the example is fixed in the documentation. Additionally Java users should be able to use `Dataset.observe(String, ..)` per `varargs`.\n\n### How was this patch tested?\n\nManually tested. CI should verify the changes too.\n\nCloses #36084 from HyukjinKwon/minor-docs.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit fb3f380b3834ca24947a82cb8d87efeae6487664)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala||sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala||sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala -> sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2004:   def observe(name: String, expr: Column, exprs: Column*): Dataset[T] = withTypedPlan {",
          "2005:     CollectMetrics(name, (expr +: exprs).map(_.named), logicalPlan)",
          "2006:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2004:   @varargs",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f5561b1237321ae13cb6f9f986421344b911e2c0",
      "candidate_info": {
        "commit_hash": "f5561b1237321ae13cb6f9f986421344b911e2c0",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/f5561b1237321ae13cb6f9f986421344b911e2c0",
        "files": [
          "core/src/test/scala/org/apache/spark/FileSuite.scala"
        ],
        "message": "[SPARK-36681][CORE][TESTS][FOLLOW-UP] Tests only when Snappy native library is available in low Hadoop versions\n\n### What changes were proposed in this pull request?\n\nThis PR is a minor followup to only test when Snappy native library is available in low Hadoop versions.\nFrom Hadoop 3.3.1 with `HADOOP-17125`, the tests should pass but it fails in lower versions of Hadoop when Snappy native library is unavailable (see also https://github.com/apache/spark/pull/35784#issuecomment-1081290978).\n\n### Why are the changes needed?\n\nTo make the tests robust.\n\n### Does this PR introduce _any_ user-facing change?\n\nNope, this is test-only.\n\n### How was this patch tested?\n\nShould monitor CI\n\nCloses #36136 from HyukjinKwon/SPARK-36681.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit bf19a5e1918bd2aec52a98428ccfe184102ea464)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "core/src/test/scala/org/apache/spark/FileSuite.scala||core/src/test/scala/org/apache/spark/FileSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/test/scala/org/apache/spark/FileSuite.scala||core/src/test/scala/org/apache/spark/FileSuite.scala": [
          "File: core/src/test/scala/org/apache/spark/FileSuite.scala -> core/src/test/scala/org/apache/spark/FileSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "136:   }",
          "142:   }",
          "143:   codecs.foreach { case (codec, codecName) =>",
          "144:     runSequenceFileCodecTest(codec, codecName)",
          "145:   }",
          "",
          "[Removed Lines]",
          "139:   val codecs = Seq((new DefaultCodec(), \"default\"), (new BZip2Codec(), \"bzip2\"),",
          "140:       (new SnappyCodec(), \"snappy\")) ++ {",
          "141:     if (VersionUtils.isHadoop3) Seq((new Lz4Codec(), \"lz4\")) else Seq()",
          "",
          "[Added Lines]",
          "139:   private val codecs = Seq((new DefaultCodec(), \"default\"), (new BZip2Codec(), \"bzip2\")) ++ {",
          "140:     scala.util.Try {",
          "144:       new SnappyCodec().getCompressorType",
          "145:       (new SnappyCodec(), \"snappy\")",
          "146:     }.toOption",
          "147:   } ++ {",
          "148:     if (VersionUtils.isHadoop3) Seq((new Lz4Codec(), \"lz4\")) else Seq.empty",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2078838adff6730bdb6db5337ee67f2efaf9153e",
      "candidate_info": {
        "commit_hash": "2078838adff6730bdb6db5337ee67f2efaf9153e",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/2078838adff6730bdb6db5337ee67f2efaf9153e",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/interfaces.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala"
        ],
        "message": "[SPARK-39355][SQL] Single column uses quoted to construct UnresolvedAttribute\n\n### What changes were proposed in this pull request?\nUse `UnresolvedAttribute.quoted` in `Alias.toAttribute` to avoid calling `UnresolvedAttribute.apply` causing `ParseException`.\n\n### Why are the changes needed?\n```sql\nSELECT *\nFROM (\n    SELECT '2022-06-01' AS c1\n) a\nWHERE c1 IN (\n    SELECT date_add('2022-06-01', 0)\n);\n```\n```\nError in query:\nmismatched input '(' expecting {<EOF>, '.', '-'}(line 1, pos 8)\n== SQL ==\ndate_add(2022-06-01, 0)\n--------^^^\n```\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nadd UT\n\nCloses #36740 from cxzl25/SPARK-39355.\n\nAuthored-by: sychen <sychen@ctrip.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 8731cb875d075b68e4e0cb1d1eb970725eab9cf9)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/interfaces.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/interfaces.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/interfaces.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/interfaces.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/interfaces.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/interfaces.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "121:   }",
          "123:   def filterAttributes: AttributeSet = filter.map(_.references).getOrElse(AttributeSet.empty)",
          "",
          "[Removed Lines]",
          "120:     UnresolvedAttribute(aggregateFunction.toString)",
          "",
          "[Added Lines]",
          "120:     UnresolvedAttribute.quoted(aggregateFunction.toString)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/namedExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "193:     if (resolved) {",
          "194:       AttributeReference(name, child.dataType, child.nullable, metadata)(exprId, qualifier)",
          "195:     } else {",
          "197:     }",
          "198:   }",
          "",
          "[Removed Lines]",
          "196:       UnresolvedAttribute(name)",
          "",
          "[Added Lines]",
          "196:       UnresolvedAttribute.quoted(name)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2185:       }",
          "2186:     }",
          "2187:   }",
          "2188: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2189:   test(\"SPARK-39355: Single column uses quoted to construct UnresolvedAttribute\") {",
          "2190:     checkAnswer(",
          "2191:       sql(\"\"\"",
          "2192:             |SELECT *",
          "2193:             |FROM (",
          "2194:             |    SELECT '2022-06-01' AS c1",
          "2195:             |) a",
          "2196:             |WHERE c1 IN (",
          "2197:             |     SELECT date_add('2022-06-01', 0)",
          "2198:             |)",
          "2199:             |\"\"\".stripMargin),",
          "2200:       Row(\"2022-06-01\"))",
          "2201:     checkAnswer(",
          "2202:       sql(\"\"\"",
          "2203:             |SELECT *",
          "2204:             |FROM (",
          "2205:             |    SELECT '2022-06-01' AS c1",
          "2206:             |) a",
          "2207:             |WHERE c1 IN (",
          "2208:             |    SELECT date_add(a.c1.k1, 0)",
          "2209:             |    FROM (",
          "2210:             |        SELECT named_struct('k1', '2022-06-01') AS c1",
          "2211:             |    ) a",
          "2212:             |)",
          "2213:             |\"\"\".stripMargin),",
          "2214:       Row(\"2022-06-01\"))",
          "2215:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ec6fc7419571114cfda94bfa15d4a40712b53fea",
      "candidate_info": {
        "commit_hash": "ec6fc7419571114cfda94bfa15d4a40712b53fea",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/ec6fc7419571114cfda94bfa15d4a40712b53fea",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Average.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ],
        "message": "[SPARK-39210][SQL] Provide query context of Decimal overflow in AVG when WSCG is off\n\n### What changes were proposed in this pull request?\n\nSimilar to https://github.com/apache/spark/pull/36525, this PR provides runtime error query context for the Average expression when WSCG is off.\n\n### Why are the changes needed?\n\nEnhance the runtime error query context of Average function. After changes, it works when the whole stage codegen is not available.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nNew UT\n\nCloses #36582 from gengliangwang/fixAvgContext.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>\n(cherry picked from commit 8b5b3e95f8761af97255cbcba35c3d836a419dba)\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Average.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Average.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Average.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Average.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Average.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Average.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "85:     case _: DecimalType =>",
          "86:       DecimalPrecision.decimalAndDecimal()(",
          "87:         Divide(",
          "89:           count.cast(DecimalType.LongDecimal), failOnError = false)).cast(resultType)",
          "90:     case _: YearMonthIntervalType =>",
          "91:       If(EqualTo(count, Literal(0L)),",
          "",
          "[Removed Lines]",
          "84:   protected def getEvaluateExpression = child.dataType match {",
          "88:           CheckOverflowInSum(sum, sumDataType.asInstanceOf[DecimalType], !useAnsiAdd),",
          "",
          "[Added Lines]",
          "84:   protected def getEvaluateExpression(queryContext: String) = child.dataType match {",
          "88:           CheckOverflowInSum(sum, sumDataType.asInstanceOf[DecimalType], !useAnsiAdd, queryContext),",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "123:   since = \"1.0.0\")",
          "124: case class Average(",
          "125:     child: Expression,",
          "127:   def this(child: Expression) = this(child, useAnsiAdd = SQLConf.get.ansiEnabled)",
          "129:   override protected def withNewChildInternal(newChild: Expression): Average =",
          "",
          "[Removed Lines]",
          "126:     useAnsiAdd: Boolean = SQLConf.get.ansiEnabled) extends AverageBase {",
          "",
          "[Added Lines]",
          "126:     useAnsiAdd: Boolean = SQLConf.get.ansiEnabled) extends AverageBase with SupportQueryContext {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "134:   override lazy val mergeExpressions: Seq[Expression] = getMergeExpressions",
          "137: }",
          "",
          "[Removed Lines]",
          "136:   override lazy val evaluateExpression: Expression = getEvaluateExpression",
          "",
          "[Added Lines]",
          "136:   override lazy val evaluateExpression: Expression = getEvaluateExpression(queryContext)",
          "138:   override def initQueryContext(): String = if (useAnsiAdd) {",
          "139:     origin.context",
          "140:   } else {",
          "141:     \"\"",
          "142:   }",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "192:   }",
          "194:   override lazy val evaluateExpression: Expression = {",
          "196:   }",
          "198:   override protected def withNewChildInternal(newChild: Expression): Expression =",
          "",
          "[Removed Lines]",
          "195:     addTryEvalIfNeeded(getEvaluateExpression)",
          "",
          "[Added Lines]",
          "201:     addTryEvalIfNeeded(getEvaluateExpression(\"\"))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "4423:     }",
          "4424:   }",
          "4428:     withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> \"false\",",
          "4429:       SQLConf.ANSI_ENABLED.key -> \"true\") {",
          "4430:       withTable(\"t\") {",
          "",
          "[Removed Lines]",
          "4426:   test(\"SPARK-39190, SPARK-39208: Query context of decimal overflow error should be serialized \" +",
          "4427:     \"to executors when WSCG is off\") {",
          "",
          "[Added Lines]",
          "4426:   test(\"SPARK-39190,SPARK-39208,SPARK-39210: Query context of decimal overflow error should \" +",
          "4427:     \"be serialized to executors when WSCG is off\") {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "4432:         sql(\"insert into t values (6e37BD),(6e37BD)\")",
          "4433:         Seq(",
          "4434:           \"select d / 0.1 from t\",",
          "4436:           val msg = intercept[SparkException] {",
          "4437:             sql(query).collect()",
          "4438:           }.getMessage",
          "",
          "[Removed Lines]",
          "4435:           \"select sum(d) from t\").foreach { query =>",
          "",
          "[Added Lines]",
          "4435:           \"select sum(d) from t\",",
          "4436:           \"select avg(d) from t\").foreach { query =>",
          "",
          "---------------"
        ]
      }
    }
  ]
}