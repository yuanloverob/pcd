{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "3cc47a178a68e957cde70fc1c3f10dbcca9bf84b",
      "candidate_info": {
        "commit_hash": "3cc47a178a68e957cde70fc1c3f10dbcca9bf84b",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/3cc47a178a68e957cde70fc1c3f10dbcca9bf84b",
        "files": [
          "external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaTestUtils.scala",
          "external/kafka-0-10/src/test/scala/org/apache/spark/streaming/kafka010/KafkaRDDSuite.scala",
          "external/kafka-0-10/src/test/scala/org/apache/spark/streaming/kafka010/KafkaTestUtils.scala",
          "pom.xml"
        ],
        "message": "Revert \"[SPARK-36837][BUILD] Upgrade Kafka to 3.1.0\"\n\n### What changes were proposed in this pull request?\n\nThis PR aims to revert commit 973ea0f06e72ab64574cbf00e095922a3415f864 from `branch-3.3` to exclude it from Apache Spark 3.3 scope.\n\n### Why are the changes needed?\n\nSPARK-36837 tried to use Apache Kafka 3.1.0 at Apache Spark 3.3.0 and initially wanted to upgrade to Apache Kafka 3.3.1 before the official release. However, we can use the stable Apache Kafka 2.8.1 at Spark 3.3.0 and wait for more proven versions, Apache Kafka 3.2.x or 3.3.x.\n\nApache Kafka 3.2.0 vote is already passed and will arrive.\n- https://lists.apache.org/thread/9k5sysvchg98lchv2rvvvq6xhpgk99cc\n\nApache Kafka 3.3.0 release discussion is started too.\n- https://lists.apache.org/thread/cmol5bcf011s1xl91rt4ylb1dgz2vb1r\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nPass the CIs.\n\nCloses #36517 from dongjoon-hyun/SPARK-36837-REVERT.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaTestUtils.scala||external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaTestUtils.scala",
          "external/kafka-0-10/src/test/scala/org/apache/spark/streaming/kafka010/KafkaRDDSuite.scala||external/kafka-0-10/src/test/scala/org/apache/spark/streaming/kafka010/KafkaRDDSuite.scala",
          "external/kafka-0-10/src/test/scala/org/apache/spark/streaming/kafka010/KafkaTestUtils.scala||external/kafka-0-10/src/test/scala/org/apache/spark/streaming/kafka010/KafkaTestUtils.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaTestUtils.scala||external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaTestUtils.scala": [
          "File: external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaTestUtils.scala -> external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaTestUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "44: import org.apache.kafka.common.security.auth.SecurityProtocol.{PLAINTEXT, SASL_PLAINTEXT}",
          "45: import org.apache.kafka.common.serialization.StringSerializer",
          "46: import org.apache.kafka.common.utils.SystemTime",
          "48: import org.apache.zookeeper.server.{NIOServerCnxnFactory, ZooKeeperServer}",
          "49: import org.apache.zookeeper.server.auth.SASLAuthenticationProvider",
          "50: import org.scalatest.Assertions._",
          "",
          "[Removed Lines]",
          "47: import org.apache.zookeeper.client.ZKClientConfig",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "268:     zkPort = zookeeper.actualPort",
          "269:     zkClient = KafkaZkClient(s\"$zkHost:$zkPort\", isSecure = false, zkSessionTimeout,",
          "271:     zkReady = true",
          "272:   }",
          "",
          "[Removed Lines]",
          "270:       zkConnectionTimeout, 1, new SystemTime(), \"test\", new ZKClientConfig)",
          "",
          "[Added Lines]",
          "269:       zkConnectionTimeout, 1, new SystemTime())",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "489:   protected def brokerConfiguration: Properties = {",
          "490:     val props = new Properties()",
          "491:     props.put(\"broker.id\", \"0\")",
          "493:     props.put(\"log.dir\", Utils.createTempDir().getAbsolutePath)",
          "494:     props.put(\"zookeeper.connect\", zkAddress)",
          "495:     props.put(\"zookeeper.connection.timeout.ms\", \"60000\")",
          "",
          "[Removed Lines]",
          "492:     props.put(\"listeners\", s\"PLAINTEXT://127.0.0.1:$brokerPort\")",
          "",
          "[Added Lines]",
          "491:     props.put(\"host.name\", \"127.0.0.1\")",
          "492:     props.put(\"advertised.host.name\", \"127.0.0.1\")",
          "493:     props.put(\"port\", brokerPort.toString)",
          "",
          "---------------"
        ],
        "external/kafka-0-10/src/test/scala/org/apache/spark/streaming/kafka010/KafkaRDDSuite.scala||external/kafka-0-10/src/test/scala/org/apache/spark/streaming/kafka010/KafkaRDDSuite.scala": [
          "File: external/kafka-0-10/src/test/scala/org/apache/spark/streaming/kafka010/KafkaRDDSuite.scala -> external/kafka-0-10/src/test/scala/org/apache/spark/streaming/kafka010/KafkaRDDSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import java.io.File",
          "23: import scala.collection.JavaConverters._",
          "25: import scala.util.Random",
          "28: import kafka.server.{BrokerTopicStats, LogDirFailureChannel}",
          "29: import kafka.utils.Pool",
          "30: import org.apache.kafka.common.TopicPartition",
          "31: import org.apache.kafka.common.record.{CompressionType, MemoryRecords, SimpleRecord}",
          "32: import org.apache.kafka.common.serialization.StringDeserializer",
          "33: import org.scalatest.BeforeAndAfterAll",
          "36: import org.apache.spark._",
          "37: import org.apache.spark.scheduler.ExecutorCacheTaskLocation",
          "",
          "[Removed Lines]",
          "24: import scala.concurrent.duration._",
          "27: import kafka.log.{CleanerConfig, LogCleaner, LogConfig, UnifiedLog}",
          "34: import org.scalatest.concurrent.Eventually.{eventually, interval, timeout}",
          "",
          "[Added Lines]",
          "26: import kafka.log.{CleanerConfig, Log, LogCleaner, LogConfig, ProducerStateManager}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "86:   private def compactLogs(topic: String, partition: Int,",
          "87:       messages: Array[(String, String)]): Unit = {",
          "88:     val mockTime = new MockTime()",
          "90:     val logDir = kafkaTestUtils.brokerLogDir",
          "91:     val dir = new File(logDir, topic + \"-\" + partition)",
          "92:     dir.mkdirs()",
          "",
          "[Removed Lines]",
          "89:     val logs = new Pool[TopicPartition, UnifiedLog]()",
          "",
          "[Added Lines]",
          "87:     val logs = new Pool[TopicPartition, Log]()",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "95:     logProps.put(LogConfig.MinCleanableDirtyRatioProp, java.lang.Float.valueOf(0.1f))",
          "96:     val logDirFailureChannel = new LogDirFailureChannel(1)",
          "97:     val topicPartition = new TopicPartition(topic, partition)",
          "99:       dir,",
          "100:       LogConfig(logProps),",
          "101:       0L,",
          "",
          "[Removed Lines]",
          "98:     val log = UnifiedLog(",
          "",
          "[Added Lines]",
          "96:     val log = new Log(",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "105:       mockTime,",
          "106:       Int.MaxValue,",
          "107:       Int.MaxValue,",
          "112:     )",
          "113:     messages.foreach { case (k, v) =>",
          "114:       val record = new SimpleRecord(k.getBytes, v.getBytes)",
          "",
          "[Removed Lines]",
          "108:       logDirFailureChannel,",
          "109:       lastShutdownClean = false,",
          "110:       topicId = None,",
          "111:       keepPartitionMetadataFile = false",
          "",
          "[Added Lines]",
          "106:       topicPartition,",
          "107:       new ProducerStateManager(topicPartition, dir),",
          "108:       logDirFailureChannel",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "204:       sc, kafkaParams, offsetRanges, preferredHosts",
          "205:     ).map(m => m.key -> m.value)",
          "212:     val received = rdd.collect.toSet",
          "213:     assert(received === compactedMessages.toSet)",
          "",
          "[Removed Lines]",
          "208:     eventually(timeout(20.second), interval(1.seconds)) {",
          "209:       val dir = new File(kafkaTestUtils.brokerLogDir, topic + \"-0\")",
          "210:       assert(dir.listFiles().exists(_.getName.endsWith(\".deleted\")))",
          "211:     }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "external/kafka-0-10/src/test/scala/org/apache/spark/streaming/kafka010/KafkaTestUtils.scala||external/kafka-0-10/src/test/scala/org/apache/spark/streaming/kafka010/KafkaTestUtils.scala": [
          "File: external/kafka-0-10/src/test/scala/org/apache/spark/streaming/kafka010/KafkaTestUtils.scala -> external/kafka-0-10/src/test/scala/org/apache/spark/streaming/kafka010/KafkaTestUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "35: import org.apache.kafka.common.network.ListenerName",
          "36: import org.apache.kafka.common.serialization.StringSerializer",
          "37: import org.apache.kafka.common.utils.{Time => KTime}",
          "39: import org.apache.zookeeper.server.{NIOServerCnxnFactory, ZooKeeperServer}",
          "41: import org.apache.spark.{SparkConf, SparkException}",
          "",
          "[Removed Lines]",
          "38: import org.apache.zookeeper.client.ZKClientConfig",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "108:     zkPort = zookeeper.actualPort",
          "109:     zkClient = KafkaZkClient(s\"$zkHost:$zkPort\", isSecure = false, zkSessionTimeout,",
          "111:     admClient = new AdminZkClient(zkClient)",
          "112:     zkReady = true",
          "113:   }",
          "",
          "[Removed Lines]",
          "110:       zkConnectionTimeout, 1, KTime.SYSTEM, \"test\", new ZKClientConfig)",
          "",
          "[Added Lines]",
          "109:       zkConnectionTimeout, 1, KTime.SYSTEM)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ebe4252e415e6afdf888e21d0b89ab744fd2dac7",
      "candidate_info": {
        "commit_hash": "ebe4252e415e6afdf888e21d0b89ab744fd2dac7",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/ebe4252e415e6afdf888e21d0b89ab744fd2dac7",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowNamespacesExec.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/command/ShowNamespacesSuiteBase.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/ShowNamespacesSuite.scala"
        ],
        "message": "[SPARK-39149][SQL] SHOW DATABASES command should not quote database names under legacy mode\n\n### What changes were proposed in this pull request?\n\nThis is a bug of the command legacy mode as it does not fully restore to the legacy behavior. The legacy v1 SHOW DATABASES command does not quote the database names. This PR fixes it.\n\n### Why are the changes needed?\n\nbug fix\n\n### Does this PR introduce _any_ user-facing change?\n\nno change by default, unless people turn on legacy mode, in which case SHOW DATABASES common won't quote the database names.\n\n### How was this patch tested?\n\nnew tests\n\nCloses #36508 from cloud-fan/regression.\n\nAuthored-by: Wenchen Fan <wenchen@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 3094e495095635f6c9e83f4646d3321c2a9311f4)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowNamespacesExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowNamespacesExec.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/command/ShowNamespacesSuiteBase.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/command/ShowNamespacesSuiteBase.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/ShowNamespacesSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/ShowNamespacesSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowNamespacesExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowNamespacesExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowNamespacesExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowNamespacesExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "42:       catalog.listNamespaces()",
          "43:     }",
          "45:     val rows = new ArrayBuffer[InternalRow]()",
          "47:       if (pattern.map(StringUtils.filterPattern(Seq(ns), _).nonEmpty).getOrElse(true)) {",
          "48:         rows += toCatalystRow(ns)",
          "49:       }",
          "",
          "[Removed Lines]",
          "46:     namespaces.map(_.quoted).map { ns =>",
          "",
          "[Added Lines]",
          "47:     val isLegacy = output.head.name == \"databaseName\"",
          "48:     val namespaceNames = if (isLegacy && namespaces.forall(_.length == 1)) {",
          "49:       namespaces.map(_.head)",
          "50:     } else {",
          "51:       namespaces.map(_.quoted)",
          "52:     }",
          "55:     namespaceNames.map { ns =>",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/command/ShowNamespacesSuiteBase.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/command/ShowNamespacesSuiteBase.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/command/ShowNamespacesSuiteBase.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/command/ShowNamespacesSuiteBase.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "43:   protected def builtinTopNamespaces: Seq[String] = Seq.empty",
          "44:   protected def isCasePreserving: Boolean = true",
          "46:   test(\"default namespace\") {",
          "47:     withSQLConf(SQLConf.DEFAULT_CATALOG.key -> catalog) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "45:   protected def createNamespaceWithSpecialName(ns: String): Unit = {",
          "46:     sql(s\"CREATE NAMESPACE $catalog.`$ns`\")",
          "47:   }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "124:     }",
          "125:   }",
          "127:   test(\"case sensitivity of the pattern string\") {",
          "128:     Seq(true, false).foreach { caseSensitive =>",
          "129:       withSQLConf(SQLConf.CASE_SENSITIVE.key -> caseSensitive.toString) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "130:   test(\"SPARK-39149: keep the legacy no-quote behavior\") {",
          "131:     Seq(true, false).foreach { legacy =>",
          "132:       withSQLConf(SQLConf.LEGACY_KEEP_COMMAND_OUTPUT_SCHEMA.key -> legacy.toString) {",
          "133:         withNamespace(s\"$catalog.`123`\") {",
          "134:           createNamespaceWithSpecialName(\"123\")",
          "135:           val res = if (legacy) \"123\" else \"`123`\"",
          "136:           checkAnswer(",
          "137:             sql(s\"SHOW NAMESPACES IN $catalog\"),",
          "138:             (res +: builtinTopNamespaces).map(Row(_)))",
          "139:         }",
          "140:       }",
          "141:     }",
          "142:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/ShowNamespacesSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/ShowNamespacesSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/ShowNamespacesSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/command/v1/ShowNamespacesSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.execution.command.v1",
          "20: import org.apache.spark.sql.AnalysisException",
          "21: import org.apache.spark.sql.execution.command",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import org.apache.spark.sql.catalyst.catalog.CatalogDatabase",
          "23: import org.apache.spark.util.Utils",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "31: trait ShowNamespacesSuiteBase extends command.ShowNamespacesSuiteBase {",
          "32:   override protected def builtinTopNamespaces: Seq[String] = Seq(\"default\")",
          "34:   test(\"IN namespace doesn't exist\") {",
          "35:     val errMsg = intercept[AnalysisException] {",
          "36:       sql(\"SHOW NAMESPACES in dummy\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "36:   override protected def createNamespaceWithSpecialName(ns: String): Unit = {",
          "38:     spark.sharedState.externalCatalog.createDatabase(",
          "39:       CatalogDatabase(",
          "40:         name = ns,",
          "41:         description = \"\",",
          "42:         locationUri = Utils.createTempDir().toURI,",
          "43:         properties = Map.empty),",
          "44:       ignoreIfExists = false)",
          "45:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "41d44d162c0b5057fd737bccf450010f6dcd4684",
      "candidate_info": {
        "commit_hash": "41d44d162c0b5057fd737bccf450010f6dcd4684",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/41d44d162c0b5057fd737bccf450010f6dcd4684",
        "files": [
          "core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java",
          "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala"
        ],
        "message": "Revert \"[SPARK-38354][SQL] Add hash probes metric for shuffled hash join\"\n\nThis reverts commit 158436655f30141bbd5afa8d95aec66282a5c4b4, as the original PR caused performance regression reported in https://github.com/apache/spark/pull/35686#issuecomment-1107807027 .\n\nCloses #36338 from c21/revert-metrics.\n\nAuthored-by: Cheng Su <chengsu@fb.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 6b5a1f9df28262fa90d28dc15af67e8a37a9efcf)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java||core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java",
          "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java||sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java||core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java": [
          "File: core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java -> core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "945:     return (1.0 * numProbes) / numKeyLookups;",
          "946:   }",
          "",
          "[Removed Lines]",
          "944:   public double getAvgHashProbesPerKey() {",
          "",
          "[Added Lines]",
          "944:   public double getAvgHashProbeBucketListIterations() {",
          "",
          "---------------"
        ],
        "sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java||sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java": [
          "File: sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java -> sql/core/src/main/java/org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "221:   }",
          "228:   }",
          "",
          "[Removed Lines]",
          "226:   public double getAvgHashProbesPerKey() {",
          "227:     return map.getAvgHashProbesPerKey();",
          "",
          "[Added Lines]",
          "226:   public double getAvgHashProbeBucketListIterations() {",
          "227:     return map.getAvgHashProbeBucketListIterations();",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "68:     \"spillSize\" -> SQLMetrics.createSizeMetric(sparkContext, \"spill size\"),",
          "69:     \"aggTime\" -> SQLMetrics.createTimingMetric(sparkContext, \"time in aggregation build\"),",
          "70:     \"avgHashProbe\" ->",
          "72:     \"numTasksFallBacked\" -> SQLMetrics.createMetric(sparkContext, \"number of sort fallback tasks\"))",
          "",
          "[Removed Lines]",
          "71:       SQLMetrics.createAverageMetric(sparkContext, \"avg hash probes per key\"),",
          "",
          "[Added Lines]",
          "71:       SQLMetrics.createAverageMetric(sparkContext, \"avg hash probe bucket list iters\"),",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "207:     metrics.incPeakExecutionMemory(maxMemory)",
          "212:     if (sorter == null) {",
          "",
          "[Removed Lines]",
          "210:     avgHashProbe.set(hashMap.getAvgHashProbesPerKey)",
          "",
          "[Added Lines]",
          "210:     avgHashProbe.set(hashMap.getAvgHashProbeBucketListIterations)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "389:     metrics.incPeakExecutionMemory(maxMemory)",
          "393:   })",
          "",
          "[Removed Lines]",
          "392:     avgHashProbe.set(hashMap.getAvgHashProbesPerKey)",
          "",
          "[Added Lines]",
          "392:     avgHashProbe.set(hashMap.getAvgHashProbeBucketListIterations)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "111:   def keys(): Iterator[InternalRow]",
          "",
          "[Removed Lines]",
          "116:   def getAvgHashProbesPerKey(): Double",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "227:   override def estimatedSize: Long = binaryMap.getTotalMemoryConsumption",
          "232:   var resultRow = new UnsafeRow(numFields)",
          "",
          "[Removed Lines]",
          "229:   override def getAvgHashProbesPerKey(): Double = binaryMap.getAvgHashProbesPerKey",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "576:   private var numKeys = 0L",
          "585:   def this() = {",
          "586:     this(",
          "",
          "[Removed Lines]",
          "579:   private var numProbes = 0L",
          "582:   private var numKeyLookups = 0L",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "630:   def getTotalMemoryConsumption: Long = array.length * 8L + page.length * 8L",
          "",
          "[Removed Lines]",
          "635:   def getAvgHashProbesPerKey: Double = (1.0 * numProbes) / numKeyLookups",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "670:   def getValue(key: Long, resultRow: UnsafeRow): UnsafeRow = {",
          "672:     if (isDense) {",
          "674:       if (key >= minKey && key <= maxKey) {",
          "675:         val value = array((key - minKey).toInt)",
          "676:         if (value > 0) {",
          "",
          "[Removed Lines]",
          "671:     numKeyLookups += 1",
          "673:       numProbes += 1",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "678:         }",
          "679:       }",
          "680:     } else {",
          "682:       var pos = firstSlot(key)",
          "683:       while (array(pos + 1) != 0) {",
          "684:         if (array(pos) == key) {",
          "685:           return getRow(array(pos + 1), resultRow)",
          "686:         }",
          "687:         pos = nextSlot(pos)",
          "689:       }",
          "690:     }",
          "691:     null",
          "",
          "[Removed Lines]",
          "681:       numProbes += 1",
          "688:         numProbes += 1",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "714:   def get(key: Long, resultRow: UnsafeRow): Iterator[UnsafeRow] = {",
          "716:     if (isDense) {",
          "718:       if (key >= minKey && key <= maxKey) {",
          "719:         val value = array((key - minKey).toInt)",
          "720:         if (value > 0) {",
          "",
          "[Removed Lines]",
          "715:     numKeyLookups += 1",
          "717:       numProbes += 1",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "722:         }",
          "723:       }",
          "724:     } else {",
          "726:       var pos = firstSlot(key)",
          "727:       while (array(pos + 1) != 0) {",
          "728:         if (array(pos) == key) {",
          "729:           return valueIter(array(pos + 1), resultRow)",
          "730:         }",
          "731:         pos = nextSlot(pos)",
          "733:       }",
          "734:     }",
          "735:     null",
          "",
          "[Removed Lines]",
          "725:       numProbes += 1",
          "732:         numProbes += 1",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "810:   private def updateIndex(key: Long, address: Long): Unit = {",
          "813:     var pos = firstSlot(key)",
          "814:     assert(numKeys < array.length / 2)",
          "815:     while (array(pos) != key && array(pos + 1) != 0) {",
          "816:       pos = nextSlot(pos)",
          "818:     }",
          "819:     if (array(pos + 1) == 0) {",
          "",
          "[Removed Lines]",
          "811:     numKeyLookups += 1",
          "812:     numProbes += 1",
          "817:       numProbes += 1",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1018:   override def estimatedSize: Long = map.getTotalMemoryConsumption",
          "1022:   override def get(key: InternalRow): Iterator[InternalRow] = {",
          "1023:     if (key.isNullAt(0)) {",
          "1024:       null",
          "",
          "[Removed Lines]",
          "1020:   override def getAvgHashProbesPerKey(): Double = map.getAvgHashProbesPerKey",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "1136:   override def close(): Unit = {}",
          "1138:   override def estimatedSize: Long = 0",
          "1141: }",
          "",
          "[Removed Lines]",
          "1140:   override def getAvgHashProbesPerKey(): Double = 0",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "1164:   override def close(): Unit = {}",
          "1166:   override def estimatedSize: Long = 0",
          "1169: }",
          "",
          "[Removed Lines]",
          "1168:   override def getAvgHashProbesPerKey(): Double = 0",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/joins/ShuffledHashJoinExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "49:   override lazy val metrics = Map(",
          "50:     \"numOutputRows\" -> SQLMetrics.createMetric(sparkContext, \"number of output rows\"),",
          "51:     \"buildDataSize\" -> SQLMetrics.createSizeMetric(sparkContext, \"data size of build side\"),",
          "55:   override def output: Seq[Attribute] = super[ShuffledJoin].output",
          "",
          "[Removed Lines]",
          "52:     \"buildTime\" -> SQLMetrics.createTimingMetric(sparkContext, \"time to build hash map\"),",
          "53:     \"avgHashProbe\" -> SQLMetrics.createAverageMetric(sparkContext, \"avg hash probes per key\"))",
          "",
          "[Added Lines]",
          "52:     \"buildTime\" -> SQLMetrics.createTimingMetric(sparkContext, \"time to build hash map\"))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "78:   def buildHashedRelation(iter: Iterator[InternalRow]): HashedRelation = {",
          "79:     val buildDataSize = longMetric(\"buildDataSize\")",
          "80:     val buildTime = longMetric(\"buildTime\")",
          "82:     val start = System.nanoTime()",
          "83:     val context = TaskContext.get()",
          "84:     val relation = HashedRelation(",
          "",
          "[Removed Lines]",
          "81:     val avgHashProbe = longMetric(\"avgHashProbe\")",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "91:     buildTime += NANOSECONDS.toMillis(System.nanoTime() - start)",
          "92:     buildDataSize += relation.estimatedSize",
          "99:     relation",
          "100:   }",
          "",
          "[Removed Lines]",
          "94:     context.addTaskCompletionListener[Unit](_ => {",
          "96:       avgHashProbe.set(relation.getAvgHashProbesPerKey())",
          "97:       relation.close()",
          "98:     })",
          "",
          "[Added Lines]",
          "92:     context.addTaskCompletionListener[Unit](_ => relation.close())",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/metric/SQLMetricsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "109:     val df = testData2.groupBy().count() // 2 partitions",
          "110:     val expected1 = Seq(",
          "111:       Map(\"number of output rows\" -> 2L,",
          "113:           aggregateMetricsPattern,",
          "114:         \"number of sort fallback tasks\" -> 0L),",
          "115:       Map(\"number of output rows\" -> 1L,",
          "117:           aggregateMetricsPattern,",
          "118:         \"number of sort fallback tasks\" -> 0L))",
          "119:     val shuffleExpected1 = Map(",
          "",
          "[Removed Lines]",
          "112:         \"avg hash probes per key\" ->",
          "116:         \"avg hash probes per key\" ->",
          "",
          "[Added Lines]",
          "112:         \"avg hash probe bucket list iters\" ->",
          "116:         \"avg hash probe bucket list iters\" ->",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "131:     val df2 = testData2.groupBy(Symbol(\"a\")).count()",
          "132:     val expected2 = Seq(",
          "133:       Map(\"number of output rows\" -> 4L,",
          "135:           aggregateMetricsPattern,",
          "136:         \"number of sort fallback tasks\" -> 0L),",
          "137:       Map(\"number of output rows\" -> 3L,",
          "139:           aggregateMetricsPattern,",
          "140:         \"number of sort fallback tasks\" -> 0L))",
          "",
          "[Removed Lines]",
          "134:         \"avg hash probes per key\" ->",
          "138:         \"avg hash probes per key\" ->",
          "",
          "[Added Lines]",
          "134:         \"avg hash probe bucket list iters\" ->",
          "138:         \"avg hash probe bucket list iters\" ->",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "184:       }",
          "185:       val metrics = getSparkPlanMetrics(df, 1, nodeIds, enableWholeStage).get",
          "186:       nodeIds.foreach { nodeId =>",
          "188:         if (!probes.contains(\"\\n\")) {",
          "190:           assert(probes.toDouble > 1.0)",
          "",
          "[Removed Lines]",
          "187:         val probes = metrics(nodeId)._2(\"avg hash probes per key\").toString",
          "",
          "[Added Lines]",
          "187:         val probes = metrics(nodeId)._2(\"avg hash probe bucket list iters\").toString",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "372:           val df = df1.join(df2, \"key\")",
          "373:           testSparkPlanMetrics(df, 1, Map(",
          "374:             nodeId1 -> ((\"ShuffledHashJoin\", Map(",
          "377:             nodeId2 -> ((\"Exchange\", Map(",
          "378:               \"shuffle records written\" -> 2L,",
          "379:               \"records read\" -> 2L))),",
          "",
          "[Removed Lines]",
          "375:               \"number of output rows\" -> 2L,",
          "376:               \"avg hash probes per key\" -> aggregateMetricsPattern))),",
          "",
          "[Added Lines]",
          "375:               \"number of output rows\" -> 2L))),",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "402:           rightDf.hint(\"shuffle_hash\"), $\"key\" === $\"key2\", joinType)",
          "403:         testSparkPlanMetrics(df, 1, Map(",
          "404:           nodeId -> ((\"ShuffledHashJoin\", Map(",
          "407:           enableWholeStage",
          "408:         )",
          "409:       }",
          "",
          "[Removed Lines]",
          "405:             \"number of output rows\" -> rows,",
          "406:             \"avg hash probes per key\" -> aggregateMetricsPattern)))),",
          "",
          "[Added Lines]",
          "404:             \"number of output rows\" -> rows)))),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "70becf290700e88c7be248e4277421dd17f3af4b",
      "candidate_info": {
        "commit_hash": "70becf290700e88c7be248e4277421dd17f3af4b",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/70becf290700e88c7be248e4277421dd17f3af4b",
        "files": [
          "python/pyspark/sql/types.py"
        ],
        "message": "[SPARK-39155][PYTHON] Access to JVM through passed-in GatewayClient during type conversion\n\n### What changes were proposed in this pull request?\nAccess to JVM through passed-in GatewayClient during type conversion.\n\n### Why are the changes needed?\nIn customized type converters, we may utilize the passed-in GatewayClient to access JVM, rather than rely on the `SparkContext._jvm`.\n\nThat's [how](https://github.com/py4j/py4j/blob/master/py4j-python/src/py4j/java_collections.py#L508) Py4J explicit converters access JVM.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nExisting tests.\n\nCloses #36504 from xinrong-databricks/gateway_client_jvm.\n\nAuthored-by: Xinrong Meng <xinrong.meng@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 92fcf214c107358c1a70566b644cec2d35c096c0)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/sql/types.py||python/pyspark/sql/types.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/sql/types.py||python/pyspark/sql/types.py": [
          "File: python/pyspark/sql/types.py -> python/pyspark/sql/types.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "44: )",
          "46: from py4j.protocol import register_input_converter",
          "49: from pyspark.serializers import CloudPickleSerializer",
          "",
          "[Removed Lines]",
          "47: from py4j.java_gateway import JavaClass, JavaGateway, JavaObject",
          "",
          "[Added Lines]",
          "47: from py4j.java_gateway import GatewayClient, JavaClass, JavaObject",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1929:     def can_convert(self, obj: Any) -> bool:",
          "1930:         return isinstance(obj, datetime.date)",
          "1933:         Date = JavaClass(\"java.sql.Date\", gateway_client)",
          "1934:         return Date.valueOf(obj.strftime(\"%Y-%m-%d\"))",
          "",
          "[Removed Lines]",
          "1932:     def convert(self, obj: datetime.date, gateway_client: JavaGateway) -> JavaObject:",
          "",
          "[Added Lines]",
          "1932:     def convert(self, obj: datetime.date, gateway_client: GatewayClient) -> JavaObject:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1938:     def can_convert(self, obj: Any) -> bool:",
          "1939:         return isinstance(obj, datetime.datetime)",
          "1942:         Timestamp = JavaClass(\"java.sql.Timestamp\", gateway_client)",
          "1943:         seconds = (",
          "1944:             calendar.timegm(obj.utctimetuple()) if obj.tzinfo else time.mktime(obj.timetuple())",
          "",
          "[Removed Lines]",
          "1941:     def convert(self, obj: datetime.datetime, gateway_client: JavaGateway) -> JavaObject:",
          "",
          "[Added Lines]",
          "1941:     def convert(self, obj: datetime.datetime, gateway_client: GatewayClient) -> JavaObject:",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1958:             and is_timestamp_ntz_preferred()",
          "1959:         )",
          "1964:         seconds = calendar.timegm(obj.utctimetuple())",
          "1969:         )",
          "1972: class DayTimeIntervalTypeConverter:",
          "1973:     def can_convert(self, obj: Any) -> bool:",
          "1974:         return isinstance(obj, datetime.timedelta)",
          "1982:             (math.floor(obj.total_seconds()) * 1000000) + obj.microseconds",
          "1983:         )",
          "",
          "[Removed Lines]",
          "1961:     def convert(self, obj: datetime.datetime, gateway_client: JavaGateway) -> JavaObject:",
          "1962:         from pyspark import SparkContext",
          "1965:         jvm = SparkContext._jvm",
          "1966:         assert jvm is not None",
          "1967:         return jvm.org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToLocalDateTime(",
          "1968:             int(seconds) * 1000000 + obj.microsecond",
          "1976:     def convert(self, obj: datetime.timedelta, gateway_client: JavaGateway) -> JavaObject:",
          "1977:         from pyspark import SparkContext",
          "1979:         jvm = SparkContext._jvm",
          "1980:         assert jvm is not None",
          "1981:         return jvm.org.apache.spark.sql.catalyst.util.IntervalUtils.microsToDuration(",
          "",
          "[Added Lines]",
          "1961:     def convert(self, obj: datetime.datetime, gateway_client: GatewayClient) -> JavaObject:",
          "1963:         DateTimeUtils = JavaClass(",
          "1964:             \"org.apache.spark.sql.catalyst.util.DateTimeUtils\",",
          "1965:             gateway_client,",
          "1967:         return DateTimeUtils.microsToLocalDateTime(int(seconds) * 1000000 + obj.microsecond)",
          "1974:     def convert(self, obj: datetime.timedelta, gateway_client: GatewayClient) -> JavaObject:",
          "1975:         IntervalUtils = JavaClass(",
          "1976:             \"org.apache.spark.sql.catalyst.util.IntervalUtils\",",
          "1977:             gateway_client,",
          "1978:         )",
          "1979:         return IntervalUtils.microsToDuration(",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8811e8caaa8540d1fa05fb77152043addc607b82",
      "candidate_info": {
        "commit_hash": "8811e8caaa8540d1fa05fb77152043addc607b82",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/8811e8caaa8540d1fa05fb77152043addc607b82",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBFileManager.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/RocksDBSuite.scala"
        ],
        "message": "[SPARK-38931][SS] Create root dfs directory for RocksDBFileManager with unknown number of keys on 1st checkpoint\n\n### What changes were proposed in this pull request?\nCreate root dfs directory for RocksDBFileManager with unknown number of keys on 1st checkpoint.\n\n### Why are the changes needed?\nIf this fix is not introduced, we might meet exception below:\n~~~java\nFile /private/var/folders/rk/wyr101_562ngn8lp7tbqt7_00000gp/T/spark-ce4a0607-b1d8-43b8-becd-638c6b030019/state/1/1 does not exist\njava.io.FileNotFoundException: File /private/var/folders/rk/wyr101_562ngn8lp7tbqt7_00000gp/T/spark-ce4a0607-b1d8-43b8-becd-638c6b030019/state/1/1 does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)\n\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)\n\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:327)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:333)\n\tat org.apache.spark.sql.execution.streaming.state.RocksDBFileManager.zipToDfsFile(RocksDBFileManager.scala:438)\n\tat org.apache.spark.sql.execution.streaming.state.RocksDBFileManager.saveCheckpointToDfs(RocksDBFileManager.scala:174)\n\tat org.apache.spark.sql.execution.streaming.state.RocksDBSuite.saveCheckpointFiles(RocksDBSuite.scala:566)\n\tat org.apache.spark.sql.execution.streaming.state.RocksDBSuite.$anonfun$new$35(RocksDBSuite.scala:179)\n        ........\n~~~\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nTested via RocksDBSuite.\n\nCloses #36242 from Myasuka/SPARK-38931.\n\nAuthored-by: Yun Tang <myasuka@live.com>\nSigned-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>\n(cherry picked from commit abb1df9d190e35a17b693f2b013b092af4f2528a)\nSigned-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBFileManager.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBFileManager.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/RocksDBSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/RocksDBSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBFileManager.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBFileManager.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBFileManager.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBFileManager.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "161:     metadata.writeToFile(metadataFile)",
          "162:     logInfo(s\"Written metadata for version $version:\\n${metadata.prettyJson}\")",
          "169:       val path = new Path(dfsRootDir)",
          "170:       if (!fm.exists(path)) fm.mkdirs(path)",
          "171:     }",
          "",
          "[Removed Lines]",
          "164:     if (version <= 1 && numKeys == 0) {",
          "",
          "[Added Lines]",
          "164:     if (version <= 1 && numKeys <= 0) {",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/RocksDBSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/RocksDBSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/RocksDBSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/RocksDBSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "170:     }",
          "171:   }",
          "173:   test(\"RocksDBFileManager: upload only new immutable files\") {",
          "174:     withTempDir { dir =>",
          "175:       val dfsRootDir = dir.getAbsolutePath",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "173:   test(\"RocksDBFileManager: create init dfs directory with unknown number of keys\") {",
          "174:     val dfsRootDir = new File(Utils.createTempDir().getAbsolutePath + \"/state/1/1\")",
          "175:     try {",
          "176:       val verificationDir = Utils.createTempDir().getAbsolutePath",
          "177:       val fileManager = new RocksDBFileManager(",
          "178:         dfsRootDir.getAbsolutePath, Utils.createTempDir(), new Configuration)",
          "180:       val cpFiles = Seq()",
          "181:       generateFiles(verificationDir, cpFiles)",
          "182:       assert(!dfsRootDir.exists())",
          "183:       saveCheckpointFiles(fileManager, cpFiles, version = 1, numKeys = -1)",
          "185:       assert(dfsRootDir.exists())",
          "186:       loadAndVerifyCheckpointFiles(fileManager, verificationDir, version = 1, Nil, -1)",
          "187:     } finally {",
          "188:       Utils.deleteRecursively(dfsRootDir)",
          "189:     }",
          "190:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}