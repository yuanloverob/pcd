{
  "cve_id": "CVE-2021-45456",
  "cve_desc": "Apache kylin checks the legitimacy of the project before executing some commands with the project name passed in by the user. There is a mismatch between what is being checked and what is being used as the shell command argument in DiagnosisService. This may cause an illegal project name to pass the check and perform the following steps, resulting in a command injection vulnerability. This issue affects Apache Kylin 4.0.0.",
  "repo": "apache/kylin",
  "patch_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
  "patch_info": {
    "commit_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
    "repo": "apache/kylin",
    "commit_url": "https://github.com/apache/kylin/commit/f4daf14dde99b934c92ce2c832509f24342bc845",
    "files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "server/src/main/webapp/WEB-INF/web.xml"
    ],
    "message": "test fix",
    "before_after_code_files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java"
    ]
  },
  "patch_diff": {
    "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "3403:     public String getKerberosPrincipal() {",
      "3404:         return getOptional(\"kylin.kerberos.principal\");",
      "3405:     }",
      "3406: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "3407:     public String getEncryptCipherIvSpec() {",
      "3408:         return getOptional(\"kylin.security.encrypt.cipher.ivSpec\", \"AAAAAAAAAAAAAAAA\");",
      "3409:     }",
      "",
      "---------------"
    ],
    "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java -> core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "25: import java.security.NoSuchAlgorithmException;",
      "27: import org.apache.commons.codec.binary.Base64;",
      "29: import javax.crypto.Cipher;",
      "30: import javax.crypto.NoSuchPaddingException;",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "28: import org.apache.kylin.common.KylinConfig;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "42:             InvalidKeyException, NoSuchPaddingException, NoSuchAlgorithmException, UnsupportedEncodingException {",
      "43:         Cipher cipher = Cipher.getInstance(\"AES/CFB/PKCS5Padding\");",
      "44:         final SecretKeySpec secretKey = new SecretKeySpec(key, \"AES\");",
      "46:         cipher.init(cipherMode, secretKey, ivSpec);",
      "47:         return cipher;",
      "48:     }",
      "",
      "[Removed Lines]",
      "45:         IvParameterSpec ivSpec = new IvParameterSpec(\"AAAAAAAAAAAAAAAA\".getBytes(\"UTF-8\"));",
      "",
      "[Added Lines]",
      "46:         IvParameterSpec ivSpec = new IvParameterSpec(KylinConfig.getInstanceFromEnv().getEncryptCipherIvSpec().getBytes(\"UTF-8\"));",
      "",
      "---------------"
    ],
    "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java": [
      "File: core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java -> core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: package org.apache.kylin.common.util;",
      "21: import org.junit.Assert;",
      "22: import org.junit.Test;",
      "26:     @Test",
      "27:     public void testAESEncrypt(){",
      "",
      "[Removed Lines]",
      "24: public class EncryptUtilTest {",
      "",
      "[Added Lines]",
      "21: import org.junit.After;",
      "23: import org.junit.Before;",
      "26: public class EncryptUtilTest extends LocalFileMetadataTestCase {",
      "27:     @Before",
      "28:     public void setUp() throws Exception {",
      "29:         this.createTestMetadata();",
      "30:     }",
      "32:     @After",
      "33:     public void after() throws Exception {",
      "34:         this.cleanupTestMetadata();",
      "35:     }",
      "",
      "---------------"
    ],
    "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java": [
      "File: server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java -> server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "87:     public String dumpProjectDiagnosisInfo(String project, File exportPath) throws IOException {",
      "88:         Message msg = MsgPicker.getMsg();",
      "89:         ProjectInstance projectInstance =",
      "90:                 ProjectManager.getInstance(KylinConfig.getInstanceFromEnv())",
      "92:         if (null == projectInstance) {",
      "93:             throw new BadRequestException(",
      "95:         }",
      "96:         aclEvaluate.checkProjectOperationPermission(projectInstance);",
      "98:         runDiagnosisCLI(args);",
      "99:         return getDiagnosisPackageName(exportPath);",
      "100:     }",
      "",
      "[Removed Lines]",
      "91:                         .getProject(ValidateUtil.convertStringToBeAlphanumericUnderscore(project));",
      "94:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), project));",
      "97:         String[] args = { project, exportPath.getAbsolutePath() };",
      "",
      "[Added Lines]",
      "89:         String projectName = ValidateUtil.convertStringToBeAlphanumericUnderscore(project);",
      "92:                         .getProject(projectName);",
      "95:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), projectName));",
      "98:         String[] args = { projectName, exportPath.getAbsolutePath() };",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "a261311a3150210f6d957726d6bf3749b78321b8",
      "candidate_info": {
        "commit_hash": "a261311a3150210f6d957726d6bf3749b78321b8",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/a261311a3150210f6d957726d6bf3749b78321b8",
        "files": [
          "server-base/src/main/java/org/apache/kylin/rest/job/MetadataCleanupJob.java"
        ],
        "message": "[KYLIN-5125] `MetadataCleanupJob.isJobComplete` should use `NSparkCubingJob` because kylin4 use spark engine to build cube (No more MR engine)",
        "before_after_code_files": [
          "server-base/src/main/java/org/apache/kylin/rest/job/MetadataCleanupJob.java||server-base/src/main/java/org/apache/kylin/rest/job/MetadataCleanupJob.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "server-base/src/main/java/org/apache/kylin/rest/job/MetadataCleanupJob.java||server-base/src/main/java/org/apache/kylin/rest/job/MetadataCleanupJob.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/job/MetadataCleanupJob.java -> server-base/src/main/java/org/apache/kylin/rest/job/MetadataCleanupJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: import org.apache.kylin.common.KylinConfig;",
          "28: import org.apache.kylin.common.persistence.ResourceStore;",
          "29: import org.apache.kylin.common.util.HadoopUtil;",
          "31: import org.apache.kylin.job.dao.ExecutableDao;",
          "32: import org.apache.kylin.job.dao.ExecutableOutputPO;",
          "33: import org.apache.kylin.job.dao.ExecutablePO;",
          "",
          "[Removed Lines]",
          "30: import org.apache.kylin.engine.mr.CubingJob;",
          "",
          "[Added Lines]",
          "30: import org.apache.kylin.engine.spark.job.NSparkCubingJob;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "106:             ExecutableOutputPO output = executableDao.getJobOutput(jobId);",
          "107:             String status = output.getStatus();",
          "108:             String jobType = job.getType();",
          "110:                     || jobType.equals(CheckpointExecutable.class.getName())) {",
          "111:                 if (StringUtils.equals(status, ExecutableState.SUCCEED.toString())",
          "112:                         || StringUtils.equals(status, ExecutableState.DISCARDED.toString())) {",
          "",
          "[Removed Lines]",
          "109:             if (jobType.equals(CubingJob.class.getName())",
          "",
          "[Added Lines]",
          "109:             if (jobType.equals(NSparkCubingJob.class.getName())",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d16e7f053116cf659a3998affba233320d3d1dca",
      "candidate_info": {
        "commit_hash": "d16e7f053116cf659a3998affba233320d3d1dca",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/d16e7f053116cf659a3998affba233320d3d1dca",
        "files": [
          "kylin-it/src/test/resources/query/sql_prune_segment/query02.sql",
          "kylin-it/src/test/resources/query/sql_prune_segment/query02.sql.expected/._SUCCESS.crc",
          "kylin-it/src/test/resources/query/sql_prune_segment/query02.sql.expected/.part-00000-45343019-e3fd-4ce5-b509-d744f9ccb327-c000.csv.crc",
          "kylin-it/src/test/resources/query/sql_prune_segment/query02.sql.expected/_SUCCESS",
          "kylin-it/src/test/resources/query/sql_prune_segment/query02.sql.expected/part-00000-45343019-e3fd-4ce5-b509-d744f9ccb327-c000.csv",
          "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala"
        ],
        "message": "KYLIN-4980 Support prunning segments from complex filter conditions",
        "before_after_code_files": [
          "kylin-it/src/test/resources/query/sql_prune_segment/query02.sql||kylin-it/src/test/resources/query/sql_prune_segment/query02.sql",
          "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala||kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "kylin-it/src/test/resources/query/sql_prune_segment/query02.sql||kylin-it/src/test/resources/query/sql_prune_segment/query02.sql": [
          "File: kylin-it/src/test/resources/query/sql_prune_segment/query02.sql -> kylin-it/src/test/resources/query/sql_prune_segment/query02.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: --",
          "2: -- Licensed to the Apache Software Foundation (ASF) under one",
          "3: -- or more contributor license agreements.  See the NOTICE file",
          "4: -- distributed with this work for additional information",
          "5: -- regarding copyright ownership.  The ASF licenses this file",
          "6: -- to you under the Apache License, Version 2.0 (the",
          "7: -- \"License\"); you may not use this file except in compliance",
          "8: -- with the License.  You may obtain a copy of the License at",
          "9: --",
          "10: --     http://www.apache.org/licenses/LICENSE-2.0",
          "11: --",
          "12: -- Unless required by applicable law or agreed to in writing, software",
          "13: -- distributed under the License is distributed on an \"AS IS\" BASIS,",
          "14: -- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
          "15: -- See the License for the specific language governing permissions and",
          "16: -- limitations under the License.",
          "17: --",
          "19: select  lo_orderdate, lo_quantity, sum(lo_revenue) from ssb.p_lineorder",
          "20: where",
          "21: (lo_orderdate = 19920906 and lo_quantity = 4) or",
          "22: (lo_orderdate = 19920905 and lo_quantity = 9) and",
          "23: (lo_orderdate = 19920904 and lo_quantity = 7) or",
          "24: (",
          "25:     lo_orderdate > 19920906 and lo_orderdate <= 19920907 and (lo_quantity = 6 or lo_quantity = 49)",
          "26: )",
          "27: group by lo_orderdate, lo_quantity",
          "28: ;{\"scanRowCount\":9,\"scanBytes\":0,\"scanFiles\":2,\"cuboidId\":[7],\"exactlyMatched\":[false]}",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala||kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala": [
          "File: kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala -> kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "238:     require(isResolved)",
          "239:     val startTime = System.nanoTime",
          "241:     logInfo(s\"Applying time partition filters: ${timePartitionFilters.mkString(\",\")}\")",
          "243:     val fsc = ShardFileStatusCache.getFileStatusCache(session)",
          "",
          "[Removed Lines]",
          "240:     val timePartitionFilters = getSpecFilter(dataFilters, timePartitionColumn)",
          "",
          "[Added Lines]",
          "240:     val timePartitionFilters = getSegmentFilter(dataFilters, timePartitionColumn)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "295:     }",
          "296:   }",
          "300:   }",
          "302:   private def pruneSegments(filters: Seq[Expression],",
          "",
          "[Removed Lines]",
          "298:   private def getSpecFilter(dataFilters: Seq[Expression], col: Attribute): Seq[Expression] = {",
          "299:     dataFilters.filter(_.references.subsetOf(AttributeSet(col)))",
          "",
          "[Added Lines]",
          "298:   private def getSegmentFilter(dataFilters: Seq[Expression], col: Attribute): Seq[Expression] = {",
          "299:     dataFilters.map(extractSegmentFilter(_, col)).filter(!_.equals(None)).map(_.get)",
          "300:   }",
          "302:   private def extractSegmentFilter(filter: Expression, col: Attribute): Option[Expression] = {",
          "303:     filter match {",
          "304:       case expressions.Or(left, right) =>",
          "305:         val leftChild = extractSegmentFilter(left, col)",
          "306:         val rightChild = extractSegmentFilter(right, col)",
          "311:         if (leftChild.eq(None) || rightChild.eq(None)) {",
          "312:           None",
          "313:         } else {",
          "314:           Some(expressions.Or(leftChild.get, rightChild.get))",
          "315:         }",
          "316:       case expressions.And(left, right) =>",
          "317:         val leftChild = extractSegmentFilter(left, col)",
          "318:         val rightChild = extractSegmentFilter(right, col)",
          "323:         if (!leftChild.eq(None) && !rightChild.eq(None)) {",
          "324:           Some(expressions.And(leftChild.get, rightChild.get))",
          "325:         } else if (!rightChild.eq(None)) {",
          "326:           rightChild",
          "327:         } else if (!leftChild.eq(None)) {",
          "328:           leftChild",
          "329:         } else {",
          "330:           None",
          "331:         }",
          "332:       case _ =>",
          "334:         if (filter.references.subsetOf(AttributeSet(col))) {",
          "335:           Some(filter)",
          "336:         } else {",
          "337:           None",
          "338:         }",
          "339:     }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a7b7edba90e8810bad3b9a76cbf64241da8de4b5",
      "candidate_info": {
        "commit_hash": "a7b7edba90e8810bad3b9a76cbf64241da8de4b5",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/a7b7edba90e8810bad3b9a76cbf64241da8de4b5",
        "files": [
          "build-engine/src/main/java/org/apache/kylin/engine/mr/common/CuboidRecommenderUtil.java",
          "build/conf/spark-driver-log4j.properties",
          "core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java",
          "core-common/src/main/resources/kylin-defaults.properties",
          "kylin-it/DEPRECATED_MODULE",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java",
          "server-base/src/main/java/org/apache/kylin/rest/job/StorageCleanupJob.java",
          "server-base/src/test/java/org/apache/kylin/rest/job/StorageCleanupJobTest.java"
        ],
        "message": "Add option to remove stale files under job_tmp for StorageCleanupJob (#1732)\n\n* Add option to remove stale files under job_tmp for StorageCleanupJob\n\n* fix code style\n\n* fix cleanupJobTmp\n\n* fix sample.sh\n\n* fix kylin.engine.build-base-cuboid-enabled conflict with cube planner\n\n* minor fix\n\n* Fix optimize job\n\n* fix typo\n\nCo-authored-by: yaqian.zhang <598593183@qq.com>",
        "before_after_code_files": [
          "build-engine/src/main/java/org/apache/kylin/engine/mr/common/CuboidRecommenderUtil.java||build-engine/src/main/java/org/apache/kylin/engine/mr/common/CuboidRecommenderUtil.java",
          "build/conf/spark-driver-log4j.properties||build/conf/spark-driver-log4j.properties",
          "core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java||core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java",
          "core-common/src/main/resources/kylin-defaults.properties||core-common/src/main/resources/kylin-defaults.properties",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java",
          "server-base/src/main/java/org/apache/kylin/rest/job/StorageCleanupJob.java||server-base/src/main/java/org/apache/kylin/rest/job/StorageCleanupJob.java",
          "server-base/src/test/java/org/apache/kylin/rest/job/StorageCleanupJobTest.java||server-base/src/test/java/org/apache/kylin/rest/job/StorageCleanupJobTest.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "build-engine/src/main/java/org/apache/kylin/engine/mr/common/CuboidRecommenderUtil.java||build-engine/src/main/java/org/apache/kylin/engine/mr/common/CuboidRecommenderUtil.java": [
          "File: build-engine/src/main/java/org/apache/kylin/engine/mr/common/CuboidRecommenderUtil.java -> build-engine/src/main/java/org/apache/kylin/engine/mr/common/CuboidRecommenderUtil.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "51:         }",
          "52:         CubeInstance cube = segment.getCubeInstance();",
          "53:         long baseCuboid = cube.getCuboidScheduler().getBaseCuboidId();",
          "56:             logger.info(BASE_CUBOID_COUNT_IN_CUBOID_STATISTICS_IS_ZERO);",
          "57:             return null;",
          "58:         }",
          "",
          "[Removed Lines]",
          "54:         if (cubeStatsReader.getCuboidRowEstimatesHLL().get(baseCuboid) == null",
          "55:                 || cubeStatsReader.getCuboidRowEstimatesHLL().get(baseCuboid) == 0L) {",
          "",
          "[Added Lines]",
          "54:         if ((cubeStatsReader.getCuboidRowEstimatesHLL().get(baseCuboid) == null",
          "55:                 || cubeStatsReader.getCuboidRowEstimatesHLL().get(baseCuboid) == 0L)",
          "56:                 && segment.getConfig().isBuildBaseCuboid()) {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "77:         Pair<Map<Long, Long>, Map<Long, Double>> statsPair = CuboidStatsReaderUtil",
          "78:                 .readCuboidStatsAndSizeFromCube(currentCuboids, cube);",
          "79:         long baseCuboid = cuboidScheduler.getBaseCuboidId();",
          "81:             logger.info(BASE_CUBOID_COUNT_IN_CUBOID_STATISTICS_IS_ZERO);",
          "82:             return null;",
          "83:         }",
          "",
          "[Removed Lines]",
          "80:         if (statsPair.getFirst().get(baseCuboid) == null || statsPair.getFirst().get(baseCuboid) == 0L) {",
          "",
          "[Added Lines]",
          "81:         if ((statsPair.getFirst().get(baseCuboid) == null || statsPair.getFirst().get(baseCuboid) == 0L)",
          "82:                 && cube.getConfig().isBuildBaseCuboid()) {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "121:         }",
          "122:         CubeInstance cube = segment.getCubeInstance();",
          "123:         long baseCuboid = cube.getCuboidScheduler().getBaseCuboidId();",
          "126:             logger.info(BASE_CUBOID_COUNT_IN_CUBOID_STATISTICS_IS_ZERO);",
          "127:             return null;",
          "128:         }",
          "",
          "[Removed Lines]",
          "124:         if (cubeStatsReader.getCuboidRowEstimatesHLL().get(baseCuboid) == null",
          "125:                 || cubeStatsReader.getCuboidRowEstimatesHLL().get(baseCuboid) == 0L) {",
          "",
          "[Added Lines]",
          "126:         if ((cubeStatsReader.getCuboidRowEstimatesHLL().get(baseCuboid) == null",
          "127:                 || cubeStatsReader.getCuboidRowEstimatesHLL().get(baseCuboid) == 0L)",
          "128:                 && segment.getConfig().isBuildBaseCuboid()) {",
          "",
          "---------------"
        ],
        "build/conf/spark-driver-log4j.properties||build/conf/spark-driver-log4j.properties": [
          "File: build/conf/spark-driver-log4j.properties -> build/conf/spark-driver-log4j.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: log4j.logger.org.springframework=WARN",
          "23: log4j.logger.org.apache.spark=WARN",
          "25: # hdfs file appender",
          "26: log4j.appender.hdfs=org.apache.kylin.engine.spark.common.logging.SparkDriverHdfsLogAppender",
          "27: log4j.appender.hdfs.kerberosEnable=${kylin.kerberos.enabled}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: log4j.appender.stderr=org.apache.log4j.ConsoleAppender",
          "26: log4j.appender.stderr.layout=org.apache.kylin.common.logging.SensitivePatternLayout",
          "27: log4j.appender.stderr.target=System.err",
          "28: log4j.appender.stderr.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2} : %m%n",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "36: #Don't add line number (%L) as it's too costly!",
          "37: log4j.appender.hdfs.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2} : %m%n",
          "39: log4j.appender.logFile=org.apache.log4j.FileAppender",
          "40: log4j.appender.logFile.Threshold=DEBUG",
          "41: log4j.appender.logFile.File=${spark.driver.local.logDir}/${spark.driver.param.taskId}.log",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "44: ## Log saved under $KYLIN_HOME/logs/spark",
          "",
          "---------------"
        ],
        "core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java||core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java -> core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "152:             }",
          "154:             if (Thread.interrupted()) {",
          "156:                 proc.destroy();",
          "157:                 try {",
          "158:                     Thread.sleep(1000);",
          "",
          "[Removed Lines]",
          "155:                 logger.info(\"CliCommandExecutor is interruppted by other, kill the sub process: \" + command);",
          "",
          "[Added Lines]",
          "155:                 logger.info(\"CliCommandExecutor is interrupted by other, kill the sub process: \" + command);",
          "",
          "---------------"
        ],
        "core-common/src/main/resources/kylin-defaults.properties||core-common/src/main/resources/kylin-defaults.properties": [
          "File: core-common/src/main/resources/kylin-defaults.properties -> core-common/src/main/resources/kylin-defaults.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "83: kylin.web.link-diagnostic=",
          "84: kylin.web.contact-mail=",
          "85: kylin.server.external-acl-provider=",
          "87: # Default time filter for job list, 0->current day, 1->last one day, 2->last one week, 3->last one year, 4->all",
          "88: kylin.web.default-time-filter=1",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "86: kylin.source.hive.database-for-flat-table=default",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "113:             CubeStatsReader origSegStatsReader = new CubeStatsReader(origSeg, config);",
          "114:             Map<Long, HLLCounter> cuboidHLLMap = Maps.newHashMap();",
          "115:             if (origSegStatsReader.getCuboidRowHLLCounters() == null) {",
          "117:                         \"Cuboid statistics of original segment do not exist. Please check the config of kylin.engine.segment-statistics-enabled.\");",
          "130:                 }",
          "131:             }",
          "140:             toUpdateSeg.setStatus(SegmentStatusEnum.READY_PENDING);",
          "141:         } else {",
          "142:             toUpdateSeg.setStatus(SegmentStatusEnum.READY);",
          "",
          "[Removed Lines]",
          "116:                 throw new IllegalArgumentException(",
          "118:             }",
          "119:             addFromCubeStatsReader(origSegStatsReader, cuboidHLLMap);",
          "120:             addFromCubeStatsReader(optSegStatsReader, cuboidHLLMap);",
          "122:             Set<Long> recommendCuboids = currentInstanceCopy.getCuboidsByMode(CuboidModeEnum.RECOMMEND);",
          "123:             Map<Long, HLLCounter> resultCuboidHLLMap = Maps.newHashMapWithExpectedSize(recommendCuboids.size());",
          "124:             for (long cuboid : recommendCuboids) {",
          "125:                 HLLCounter hll = cuboidHLLMap.get(cuboid);",
          "126:                 if (hll == null) {",
          "127:                     logger.warn(\"Cannot get the row count stats for cuboid \" + cuboid);",
          "128:                 } else {",
          "129:                     resultCuboidHLLMap.put(cuboid, hll);",
          "132:             if (fs.exists(statisticsFile)) {",
          "133:                 fs.delete(statisticsFile, false);",
          "134:             }",
          "135:             CubeStatsWriter.writeCuboidStatistics(HadoopUtil.getCurrentConfiguration(), new Path(statisticsDir),",
          "136:                     resultCuboidHLLMap, 1, origSegStatsReader.getSourceRowCount());",
          "137:             FSDataInputStream is = fs.open(statisticsFile);",
          "138:             ResourceStore.getStore(config).putBigResource(resKey, is, System.currentTimeMillis());",
          "",
          "[Added Lines]",
          "116:                 logger.warn(",
          "118:             } else {",
          "119:                 addFromCubeStatsReader(origSegStatsReader, cuboidHLLMap);",
          "120:                 addFromCubeStatsReader(optSegStatsReader, cuboidHLLMap);",
          "122:                 Set<Long> recommendCuboids = currentInstanceCopy.getCuboidsByMode(CuboidModeEnum.RECOMMEND);",
          "123:                 Map<Long, HLLCounter> resultCuboidHLLMap = Maps.newHashMapWithExpectedSize(recommendCuboids.size());",
          "124:                 for (long cuboid : recommendCuboids) {",
          "125:                     HLLCounter hll = cuboidHLLMap.get(cuboid);",
          "126:                     if (hll == null) {",
          "127:                         logger.warn(\"Cannot get the row count stats for cuboid \" + cuboid);",
          "128:                     } else {",
          "129:                         resultCuboidHLLMap.put(cuboid, hll);",
          "130:                     }",
          "132:                 if (fs.exists(statisticsFile)) {",
          "133:                     fs.delete(statisticsFile, false);",
          "134:                 }",
          "135:                 CubeStatsWriter.writeCuboidStatistics(HadoopUtil.getCurrentConfiguration(), new Path(statisticsDir),",
          "136:                         resultCuboidHLLMap, 1, origSegStatsReader.getSourceRowCount());",
          "137:                 FSDataInputStream is = fs.open(statisticsFile);",
          "138:                 ResourceStore.getStore(config).putBigResource(resKey, is, System.currentTimeMillis());",
          "",
          "---------------"
        ],
        "server-base/src/main/java/org/apache/kylin/rest/job/StorageCleanupJob.java||server-base/src/main/java/org/apache/kylin/rest/job/StorageCleanupJob.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/job/StorageCleanupJob.java -> server-base/src/main/java/org/apache/kylin/rest/job/StorageCleanupJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "43: import java.io.IOException;",
          "44: import java.util.ArrayList;",
          "45: import java.util.Arrays;",
          "46: import java.util.List;",
          "47: import java.util.stream.Collectors;",
          "49: public class StorageCleanupJob extends AbstractApplication {",
          "51:     private static final Logger logger = LoggerFactory.getLogger(StorageCleanupJob.class);",
          "53:     @SuppressWarnings(\"static-access\")",
          "56:     @SuppressWarnings(\"static-access\")",
          "57:     protected static final Option OPTION_CLEANUP_TABLE_SNAPSHOT = OptionBuilder.withArgName(\"cleanupTableSnapshot\")",
          "62:     @SuppressWarnings(\"static-access\")",
          "74:     final protected KylinConfig config;",
          "75:     final protected FileSystem fs;",
          "76:     final protected ExecutableManager executableManager;",
          "83:     protected long storageTimeCut;",
          "85:     protected static final List<String> protectedDir = Arrays.asList(\"cube_statistics\", \"resources-jdbc\", \"_sparder_logs\");",
          "",
          "[Removed Lines]",
          "54:     protected static final Option OPTION_DELETE = OptionBuilder.withArgName(\"delete\").hasArg().isRequired(false)",
          "55:             .withDescription(\"Delete the unused storage\").create(\"delete\");",
          "58:             .hasArg().isRequired(false).withDescription(\"Delete the unused storage\").create(\"cleanupTableSnapshot\");",
          "59:     @SuppressWarnings(\"static-access\")",
          "60:     protected static final Option OPTION_CLEANUP_GLOBAL_DICT = OptionBuilder.withArgName(\"cleanupGlobalDict\").hasArg()",
          "61:             .isRequired(false).withDescription(\"Delete the unused storage\").create(\"cleanupGlobalDict\");",
          "63:     protected static final Option OPTION_CLEANUP_THRESHOLD_HOUR = OptionBuilder.withArgName(\"cleanupThreshold\").hasArg()",
          "64:             .isRequired(false).withDescription(\"Delete unused storage that have not been modified in how many hours\")",
          "65:             .create(\"cleanupThreshold\");",
          "67:     private static final String GLOBAL_DICT_PREFIX = \"/dict/global_dict/\";",
          "68:     private static final String TABLE_SNAPSHOT_PREFIX = \"/table_snapshot/\";",
          "70:     private static final String TABLE_SNAPSHOT = \"table snapshot\";",
          "71:     private static final String GLOBAL_DICTIONARY = \"global dictionary\";",
          "72:     private static final String SEGMENT_PARQUET_FILE = \"segment parquet file\";",
          "78:     protected boolean delete = false;",
          "79:     protected boolean cleanupTableSnapshot = true;",
          "80:     protected boolean cleanupGlobalDict = true;",
          "81:     protected int cleanupThreshold = 12; // 12 hour",
          "",
          "[Added Lines]",
          "46: import java.util.Date;",
          "61:     public static final int DEFAULT_CLEANUP_HOUR_THRESHOLD = 24 * 7;",
          "62:     public static final boolean DEFAULT_CLEANUP_DICT = true;",
          "63:     public static final boolean DEFAULT_CLEANUP_SNAPSHOT = true;",
          "64:     public static final boolean DEFAULT_CLEANUP_JOB_TMP = false;",
          "65:     public static final boolean DEFAULT_CLEANUP = false;",
          "66:     private static final String GLOBAL_DICT_PREFIX = \"/dict/global_dict/\";",
          "67:     private static final String TABLE_SNAPSHOT_PREFIX = \"/table_snapshot/\";",
          "70:     protected static final Option OPTION_DELETE = OptionBuilder.withArgName(\"delete\")",
          "71:             .hasArg().isRequired(false)",
          "72:             .withType(Boolean.class.getName())",
          "73:             .withDescription(\"Boolean, whether or not to do real delete operation. Default value is \" + DEFAULT_CLEANUP + \", means a dry run.\")",
          "74:             .create(\"delete\");",
          "78:             .hasArg().isRequired(false)",
          "79:             .withType(Boolean.class.getName())",
          "80:             .withDescription(\"Boolean, whether or not to delete unreferenced snapshot files. Default value is \" + DEFAULT_CLEANUP_SNAPSHOT + \" .\")",
          "81:             .create(\"cleanupTableSnapshot\");",
          "84:     protected static final Option OPTION_CLEANUP_GLOBAL_DICT = OptionBuilder.withArgName(\"cleanupGlobalDict\")",
          "85:             .hasArg().isRequired(false)",
          "86:             .withType(Boolean.class.getName())",
          "87:             .withDescription(\"Boolean, whether or not to delete unreferenced global dict files. Default value is \" + DEFAULT_CLEANUP_DICT + \" .\")",
          "88:             .create(\"cleanupGlobalDict\");",
          "90:     @SuppressWarnings(\"static-access\")",
          "91:     protected static final Option OPTION_CLEANUP_JOB_TMP = OptionBuilder.withArgName(\"cleanupJobTmp\")",
          "92:             .hasArg().isRequired(false)",
          "93:             .withType(Boolean.class.getName())",
          "94:             .withDescription(\"Boolean, whether or not to delete job tmp files. Default value is \" + DEFAULT_CLEANUP_JOB_TMP + \" .\")",
          "95:             .create(\"cleanupJobTmp\");",
          "97:     @SuppressWarnings(\"static-access\")",
          "98:     protected static final Option OPTION_CLEANUP_THRESHOLD_HOUR = OptionBuilder.withArgName(\"cleanupThreshold\")",
          "99:             .hasArg().isRequired(false)",
          "100:             .withType(Integer.class.getName())",
          "101:             .withDescription(",
          "102:                     \"Integer, used to specific delete unreferenced storage that have not been modified before how many hours (recent files are protected). \" +",
          "103:                     \"Default value is \" + DEFAULT_CLEANUP_HOUR_THRESHOLD + \" hours.\")",
          "104:             .create(\"cleanupThreshold\");",
          "110:     protected boolean delete = DEFAULT_CLEANUP;",
          "111:     protected boolean cleanupTableSnapshot = DEFAULT_CLEANUP_SNAPSHOT;",
          "112:     protected boolean cleanupGlobalDict = DEFAULT_CLEANUP_DICT;",
          "113:     protected boolean cleanupJobTmp = DEFAULT_CLEANUP;",
          "114:     protected int cleanupThreshold = DEFAULT_CLEANUP_HOUR_THRESHOLD;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "101:         options.addOption(OPTION_DELETE);",
          "102:         options.addOption(OPTION_CLEANUP_GLOBAL_DICT);",
          "103:         options.addOption(OPTION_CLEANUP_TABLE_SNAPSHOT);",
          "104:         options.addOption(OPTION_CLEANUP_THRESHOLD_HOUR);",
          "105:         return options;",
          "106:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "136:         options.addOption(OPTION_CLEANUP_JOB_TMP);",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "108:     @Override",
          "109:     protected void execute(OptionsHelper optionsHelper) throws Exception {",
          "110:         logger.info(\"options: '\" + optionsHelper.getOptionsAsString() + \"'\");",
          "118:         delete = Boolean.parseBoolean(optionsHelper.getOptionValue(OPTION_DELETE));",
          "119:         if (optionsHelper.hasOption(OPTION_CLEANUP_TABLE_SNAPSHOT)) {",
          "120:             cleanupTableSnapshot = Boolean.parseBoolean(optionsHelper.getOptionValue(OPTION_CLEANUP_TABLE_SNAPSHOT));",
          "",
          "[Removed Lines]",
          "111:         logger.info(\"delete option value: '\" + optionsHelper.getOptionValue(OPTION_DELETE) + \"'\");",
          "112:         logger.info(\"cleanup table snapshot option value: '\"",
          "113:                 + optionsHelper.getOptionValue(OPTION_CLEANUP_TABLE_SNAPSHOT) + \"'\");",
          "114:         logger.info(",
          "115:                 \"delete global dict option value: '\" + optionsHelper.getOptionValue(OPTION_CLEANUP_GLOBAL_DICT) + \"'\");",
          "116:         logger.info(\"delete unused storage that have not been modified in how many hours option value: '\"",
          "117:                 + optionsHelper.getOptionValue(OPTION_CLEANUP_THRESHOLD_HOUR) + \"'\");",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "122:         if (optionsHelper.hasOption(OPTION_CLEANUP_GLOBAL_DICT)) {",
          "123:             cleanupGlobalDict = Boolean.parseBoolean(optionsHelper.getOptionValue(OPTION_CLEANUP_GLOBAL_DICT));",
          "124:         }",
          "125:         if (optionsHelper.hasOption(OPTION_CLEANUP_THRESHOLD_HOUR)) {",
          "126:             cleanupThreshold = Integer.parseInt(optionsHelper.getOptionValue(OPTION_CLEANUP_THRESHOLD_HOUR));",
          "127:         }",
          "129:         storageTimeCut = System.currentTimeMillis() - cleanupThreshold * 3600 * 1000L;",
          "130:         cleanup();",
          "131:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "151:         if (optionsHelper.hasOption(OPTION_CLEANUP_JOB_TMP)) {",
          "152:             cleanupJobTmp = Boolean.parseBoolean(optionsHelper.getOptionValue(OPTION_CLEANUP_JOB_TMP));",
          "153:         }",
          "159:         Date cleanBeforeDate = new Date(storageTimeCut);",
          "160:         logger.info(\"===================================================================\\n\" +",
          "161:                         \"delete : {}; cleanupTableSnapshot : {}; cleanupGlobalDict : {}; cleanupJobTmp : {}; cleanBeforeDate : {}.\"",
          "162:                 , delete, cleanupTableSnapshot, cleanupGlobalDict, cleanupJobTmp, cleanBeforeDate);",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "138:         List<String> projects = projectManager.listAllProjects().stream().map(ProjectInstance::getName)",
          "139:                 .collect(Collectors.toList());",
          "142:         List<CubeInstance> cubes = cubeManager.listAllCubes();",
          "143:         Path metadataPath = new Path(config.getHdfsWorkingDirectory());",
          "144:         if (fs.exists(metadataPath)) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "174:         logger.info(\"Start to clean up unreferenced projects and cubes ...\");",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "148:                     if (eligibleStorage(status)) {",
          "149:                         String projectName = status.getPath().getName();",
          "150:                         if (!projects.contains(projectName)) {",
          "152:                         } else {",
          "153:                             cleanupGlobalDict(projectName,",
          "154:                                     cubes.stream().filter(cube -> projectName.equals(cube.getProject()))",
          "",
          "[Removed Lines]",
          "151:                             cleanupStorage(status.getPath(), SEGMENT_PARQUET_FILE);",
          "",
          "[Added Lines]",
          "184:                             deleteOp(status.getPath(), StorageCleanType.PROJECT_DIR);",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "164:             }",
          "165:         }",
          "168:         for (CubeInstance cube : cubes) {",
          "169:             List<String> segments = cube.getSegments().stream().map(segment -> {",
          "170:                 return segment.getName() + \"_\" + segment.getStorageLocationIdentifier();",
          "171:             }).collect(Collectors.toList());",
          "172:             String project = cube.getProject();",
          "175:             Path cubePath = new Path(config.getHdfsWorkingDirectory(project) + \"/parquet/\" + cube.getName());",
          "176:             if (fs.exists(cubePath)) {",
          "177:                 FileStatus[] segmentStatus = fs.listStatus(cubePath);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "200:         logger.info(\"Start to clean up no unreferenced segments ...\");",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "180:                         if (eligibleStorage(status)) {",
          "181:                             String segment = status.getPath().getName();",
          "182:                             if (!segments.contains(segment)) {",
          "184:                             }",
          "185:                         }",
          "186:                     }",
          "187:                 }",
          "188:             } else {",
          "190:             }",
          "191:         }",
          "192:     }",
          "",
          "[Removed Lines]",
          "183:                                 cleanupStorage(status.getPath(), SEGMENT_PARQUET_FILE);",
          "189:                 logger.warn(\"Cube path doesn't exist! The path is \" + cubePath);",
          "",
          "[Added Lines]",
          "216:                                 deleteOp(status.getPath(), StorageCleanType.SEGMENT_DIR);",
          "222:                 logger.warn(\"Cube path doesn't exist! The path is {}\", cubePath);",
          "223:             }",
          "224:         }",
          "226:         if (cleanupJobTmp) {",
          "227:             logger.info(\"Start to clean up stale job_tmp ...\");",
          "228:             for (String prj : projects) {",
          "229:                 Path prjPath = new Path(config.getJobTmpDir(prj));",
          "230:                 FileStatus[] jobTmpPaths = fs.listStatus(prjPath);",
          "231:                 for (FileStatus status : jobTmpPaths) {",
          "232:                     if (eligibleStorage(status)) {",
          "233:                         deleteOp(status.getPath(), StorageCleanType.JOB_TMP);",
          "234:                     }",
          "235:                 }",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "201:                     if (eligibleStorage(status)) {",
          "202:                         String cubeName = status.getPath().getName();",
          "203:                         if (!cubes.contains(cubeName)) {",
          "205:                         }",
          "206:                     }",
          "207:                 }",
          "",
          "[Removed Lines]",
          "204:                             cleanupStorage(status.getPath(), SEGMENT_PARQUET_FILE);",
          "",
          "[Added Lines]",
          "250:                             deleteOp(status.getPath(), StorageCleanType.CUBE_DIR);",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "237:         }",
          "239:         for (Path path : toDeleteSnapshot) {",
          "241:         }",
          "242:     }",
          "",
          "[Removed Lines]",
          "240:             cleanupStorage(path, TABLE_SNAPSHOT);",
          "",
          "[Added Lines]",
          "286:             deleteOp(path, StorageCleanType.TABLE_SNAPSHOT);",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "279:         }",
          "281:         for (Path path : toDeleteDict) {",
          "283:         }",
          "284:     }",
          "287:         if (delete) {",
          "289:             fs.delete(path, true);",
          "290:         } else {",
          "292:         }",
          "293:     }",
          "",
          "[Removed Lines]",
          "282:             cleanupStorage(path, GLOBAL_DICTIONARY);",
          "286:     private void cleanupStorage(Path path, String storageType) throws IOException {",
          "288:             logger.info(\"Deleting unused {}, {}\", storageType, path);",
          "291:             logger.info(\"Dry run, pending delete unused {}, {}\", storageType, path);",
          "",
          "[Added Lines]",
          "328:             deleteOp(path, StorageCleanType.GLOBAL_DICTIONARY);",
          "332:     private void deleteOp(Path path, StorageCleanType type) throws IOException {",
          "334:             logger.info(\"Deleting unreferenced {}, {}\", type, path);",
          "337:             logger.info(\"Dry run, pending delete unreferenced path {}, {}\", type, path);",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "296:         return status != null && status.getModificationTime() < storageTimeCut;",
          "297:     }",
          "298: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "346: enum StorageCleanType {",
          "347:     PROJECT_DIR,",
          "348:     GLOBAL_DICTIONARY,",
          "349:     TABLE_SNAPSHOT,",
          "350:     CUBE_DIR,",
          "351:     SEGMENT_DIR,",
          "352:     JOB_TMP",
          "353: }",
          "",
          "---------------"
        ],
        "server-base/src/test/java/org/apache/kylin/rest/job/StorageCleanupJobTest.java||server-base/src/test/java/org/apache/kylin/rest/job/StorageCleanupJobTest.java": [
          "File: server-base/src/test/java/org/apache/kylin/rest/job/StorageCleanupJobTest.java -> server-base/src/test/java/org/apache/kylin/rest/job/StorageCleanupJobTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "64:         prepareHDFSFiles(basePath, mockFs);",
          "66:         StorageCleanupJob job = new StorageCleanupJob(kylinConfig, mockFs);",
          "69:         ArgumentCaptor<Path> pathCaptor = ArgumentCaptor.forClass(Path.class);",
          "70:         verify(mockFs, times(6)).delete(pathCaptor.capture(), eq(true));",
          "",
          "[Removed Lines]",
          "67:         job.execute(new String[] { \"--delete\", \"true\" });",
          "",
          "[Added Lines]",
          "67:         job.execute(new String[] { \"--delete\", \"true\", \"--cleanupThreshold\", \"12\" });",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e0de52432f3d9e253ac3a581f9f8e389e67afc86",
      "candidate_info": {
        "commit_hash": "e0de52432f3d9e253ac3a581f9f8e389e67afc86",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/e0de52432f3d9e253ac3a581f9f8e389e67afc86",
        "files": [
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CreateFlatTable.scala"
        ],
        "message": "KYLIN-5019 Avoid building global dictionary from all data of fact table every time (#1720)",
        "before_after_code_files": [
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CreateFlatTable.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CreateFlatTable.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CreateFlatTable.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CreateFlatTable.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CreateFlatTable.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/builder/CreateFlatTable.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "47:     var rootFactDataset = generateTableDataset(seg.factTable, ccCols.toSeq, ss, seg.project)",
          "49:     logInfo(s\"Create flattable need join lookup tables $needJoin, need encode cols $needEncode\")",
          "51:     (needJoin, needEncode) match {",
          "52:       case (true, true) =>",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "50:     rootFactDataset = applyPartitionCondition(seg, rootFactDataset)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "149:   private def applyFilterCondition(desc: SegmentInfo, ds: Dataset[Row]): Dataset[Row] = {",
          "150:     var afterFilter = ds",
          "152:     if (StringUtils.isNotBlank(desc.filterCondition)) {",
          "153:       val afterConvertCondition = desc.filterCondition",
          "154:       logInfo(s\"Filter condition is $afterConvertCondition\")",
          "155:       afterFilter = afterFilter.where(afterConvertCondition)",
          "156:     }",
          "157:     if (StringUtils.isNotBlank(desc.partitionExp)) {",
          "158:       afterFilter = afterFilter.where(desc.partitionExp)",
          "159:     }",
          "160:     afterFilter",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "157:     afterFilter",
          "158:   }",
          "160:   private def applyPartitionCondition(desc: SegmentInfo, ds: Dataset[Row]): Dataset[Row] = {",
          "161:     var afterFilter = ds",
          "163:       logInfo(s\"Partition Filter condition is ${desc.partitionExp}\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "39ee0b8a6a84dcf27404c838ead379b0342f7b65",
      "candidate_info": {
        "commit_hash": "39ee0b8a6a84dcf27404c838ead379b0342f7b65",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/39ee0b8a6a84dcf27404c838ead379b0342f7b65",
        "files": [
          "core-common/src/main/java/org/apache/kylin/common/QueryContext.java",
          "core-common/src/main/java/org/apache/kylin/common/debug/BackdoorToggles.java",
          "jdbc/src/main/java/org/apache/kylin/jdbc/IRemoteClient.java",
          "jdbc/src/main/java/org/apache/kylin/jdbc/KylinClient.java",
          "jdbc/src/main/java/org/apache/kylin/jdbc/KylinResultSet.java",
          "jdbc/src/main/java/org/apache/kylin/jdbc/json/SQLResponseStub.java",
          "server-base/src/main/java/org/apache/kylin/rest/response/SQLResponse.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java",
          "server-base/src/test/java/org/apache/kylin/rest/response/SQLResponseTest.java"
        ],
        "message": "KYLIN-5183 Customize or get query id for devops",
        "before_after_code_files": [
          "core-common/src/main/java/org/apache/kylin/common/QueryContext.java||core-common/src/main/java/org/apache/kylin/common/QueryContext.java",
          "core-common/src/main/java/org/apache/kylin/common/debug/BackdoorToggles.java||core-common/src/main/java/org/apache/kylin/common/debug/BackdoorToggles.java",
          "jdbc/src/main/java/org/apache/kylin/jdbc/IRemoteClient.java||jdbc/src/main/java/org/apache/kylin/jdbc/IRemoteClient.java",
          "jdbc/src/main/java/org/apache/kylin/jdbc/KylinClient.java||jdbc/src/main/java/org/apache/kylin/jdbc/KylinClient.java",
          "jdbc/src/main/java/org/apache/kylin/jdbc/KylinResultSet.java||jdbc/src/main/java/org/apache/kylin/jdbc/KylinResultSet.java",
          "jdbc/src/main/java/org/apache/kylin/jdbc/json/SQLResponseStub.java||jdbc/src/main/java/org/apache/kylin/jdbc/json/SQLResponseStub.java",
          "server-base/src/main/java/org/apache/kylin/rest/response/SQLResponse.java||server-base/src/main/java/org/apache/kylin/rest/response/SQLResponse.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java||server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java",
          "server-base/src/test/java/org/apache/kylin/rest/response/SQLResponseTest.java||server-base/src/test/java/org/apache/kylin/rest/response/SQLResponseTest.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core-common/src/main/java/org/apache/kylin/common/QueryContext.java||core-common/src/main/java/org/apache/kylin/common/QueryContext.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/QueryContext.java -> core-common/src/main/java/org/apache/kylin/common/QueryContext.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "53:     private long queryStartMillis;",
          "56:     private String username;",
          "57:     private Set<String> groups;",
          "58:     private String project;",
          "",
          "[Removed Lines]",
          "55:     private final String queryId;",
          "",
          "[Added Lines]",
          "55:     private String queryId;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "102:         }",
          "103:     }",
          "105:     public String getQueryId() {",
          "106:         return queryId == null ? \"\" : queryId;",
          "107:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "105:     public void setQueryId(String queryId) {",
          "106:         this.queryId = queryId;",
          "107:     }",
          "",
          "---------------"
        ],
        "core-common/src/main/java/org/apache/kylin/common/debug/BackdoorToggles.java||core-common/src/main/java/org/apache/kylin/common/debug/BackdoorToggles.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/debug/BackdoorToggles.java -> core-common/src/main/java/org/apache/kylin/common/debug/BackdoorToggles.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "401:     public final static String DEBUG_TOGGLE_HIT_CUBE = \"DEBUG_TOGGLE_HIT_CUBE\";",
          "403:     public final static String DEBUG_TOGGLE_SPARK_POOL = \"DEBUG_TOGGLE_SPARK_POOL\";",
          "404: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "408:     public final static String CUSTOMIZE_QUERY_ID = \"CUSTOMIZE_QUERY_ID\";",
          "",
          "---------------"
        ],
        "jdbc/src/main/java/org/apache/kylin/jdbc/IRemoteClient.java||jdbc/src/main/java/org/apache/kylin/jdbc/IRemoteClient.java": [
          "File: jdbc/src/main/java/org/apache/kylin/jdbc/IRemoteClient.java -> jdbc/src/main/java/org/apache/kylin/jdbc/IRemoteClient.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "31:     class QueryResult {",
          "32:         public final List<ColumnMetaData> columnMeta;",
          "33:         public final Iterable<Object> iterable;",
          "35:         public QueryResult(List<ColumnMetaData> columnMeta, Iterable<Object> iterable) {",
          "36:             this.columnMeta = columnMeta;",
          "37:             this.iterable = iterable;",
          "38:         }",
          "39:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "34:         public String queryId;",
          "36:         public String getQueryId() {",
          "37:             return queryId;",
          "38:         }",
          "45:         public QueryResult(List<ColumnMetaData> columnMeta, Iterable<Object> iterable, String queryId) {",
          "46:             this.columnMeta = columnMeta;",
          "47:             this.iterable = iterable;",
          "48:             this.queryId = queryId;",
          "49:         }",
          "",
          "---------------"
        ],
        "jdbc/src/main/java/org/apache/kylin/jdbc/KylinClient.java||jdbc/src/main/java/org/apache/kylin/jdbc/KylinClient.java": [
          "File: jdbc/src/main/java/org/apache/kylin/jdbc/KylinClient.java -> jdbc/src/main/java/org/apache/kylin/jdbc/KylinClient.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "314:             List<KMetaCatalog> catalogs = convertMetaCatalogs(schemas);",
          "315:             return new KMetaProject(project, catalogs);",
          "316:         } finally {",
          "318:         }",
          "319:     }",
          "",
          "[Removed Lines]",
          "317:            get.releaseConnection();",
          "",
          "[Added Lines]",
          "317:            get.releaseConnection();",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "406:         List<ColumnMetaData> metas = convertColumnMeta(queryResp);",
          "407:         List<Object> data = convertResultData(queryResp, metas);",
          "410:     }",
          "412:     private List<StatementParameter> convertParameters(List<Object> paramValues) {",
          "",
          "[Removed Lines]",
          "409:         return new QueryResult(metas, data);",
          "",
          "[Added Lines]",
          "409:         return new QueryResult(metas, data, queryResp.getQueryId());",
          "",
          "---------------"
        ],
        "jdbc/src/main/java/org/apache/kylin/jdbc/KylinResultSet.java||jdbc/src/main/java/org/apache/kylin/jdbc/KylinResultSet.java": [
          "File: jdbc/src/main/java/org/apache/kylin/jdbc/KylinResultSet.java -> jdbc/src/main/java/org/apache/kylin/jdbc/KylinResultSet.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "40: public class KylinResultSet extends AvaticaResultSet {",
          "42:     public KylinResultSet(AvaticaStatement statement, QueryState state, Signature signature, ResultSetMetaData resultSetMetaData, TimeZone timeZone, Frame firstFrame) throws SQLException {",
          "43:         super(statement, state, signature, resultSetMetaData, timeZone, firstFrame);",
          "44:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42:     private String queryId;",
          "44:     public String getQueryId() {",
          "45:         return queryId;",
          "46:     }",
          "48:     public void setQueryId(String queryId) {",
          "49:         this.queryId = queryId;",
          "50:     }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "71:         QueryResult result;",
          "72:         try {",
          "73:             result = client.executeQuery(sql, paramValues, queryToggles);",
          "74:         } catch (IOException e) {",
          "75:             throw new SQLException(e);",
          "76:         }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "84:             this.setQueryId(result.getQueryId());",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "92:             if (Driver.CLIENT_CALCITE_PROP_NAMES.contains(key)) {",
          "93:                 props.put(key, connProps.getProperty(key));",
          "94:             }",
          "95:         }",
          "97:         if (props.isEmpty()) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "106:             if (key.startsWith(\"CUSTOMIZE_\")) {",
          "107:                 queryToggles.put(key, connProps.getProperty(key));",
          "108:             }",
          "",
          "---------------"
        ],
        "jdbc/src/main/java/org/apache/kylin/jdbc/json/SQLResponseStub.java||jdbc/src/main/java/org/apache/kylin/jdbc/json/SQLResponseStub.java": [
          "File: jdbc/src/main/java/org/apache/kylin/jdbc/json/SQLResponseStub.java -> jdbc/src/main/java/org/apache/kylin/jdbc/json/SQLResponseStub.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "59:     private boolean storageCacheUsed = false;",
          "61:     public SQLResponseStub() {",
          "62:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "61:     private String queryId;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "153:         this.storageCacheUsed = storageCacheUsed;",
          "154:     }",
          "156:     @JsonIgnoreProperties(ignoreUnknown = true)",
          "157:     public static class ColumnMetaStub implements Serializable{",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "158:     public String getQueryId() {",
          "159:         return queryId;",
          "160:     }",
          "162:     public void setQueryId(String queryId) {",
          "163:         this.queryId = queryId;",
          "164:     }",
          "",
          "---------------"
        ],
        "server-base/src/main/java/org/apache/kylin/rest/response/SQLResponse.java||server-base/src/main/java/org/apache/kylin/rest/response/SQLResponse.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/response/SQLResponse.java -> server-base/src/main/java/org/apache/kylin/rest/response/SQLResponse.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "98:     private List<SQLResponseTrace> traces;",
          "100:     public SQLResponse() {",
          "101:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "100:     protected String queryId;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "306:         return traces;",
          "307:     }",
          "309:     @JsonIgnore",
          "310:     public List<QueryContext.CubeSegmentStatisticsResult> getCubeSegmentStatisticsList() {",
          "311:         try {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "311:     public String getQueryId() {",
          "312:         return queryId;",
          "313:     }",
          "315:     public void setQueryId(String queryId) {",
          "316:         this.queryId = queryId;",
          "317:     }",
          "",
          "---------------"
        ],
        "server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java||server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java -> server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "446:             BackdoorToggles.addToggles(sqlRequest.getBackdoorToggles());",
          "447:         }",
          "449:         try (SetThreadName ignored = new SetThreadName(\"Query %s\", queryContext.getQueryId())) {",
          "451:             OLAPContext.clearThreadLocalContexts();",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "449:         if (sqlRequest.getBackdoorToggles() != null && !StringUtil.isEmpty(sqlRequest.getBackdoorToggles().get(BackdoorToggles.CUSTOMIZE_QUERY_ID))) {",
          "450:             queryContext.setQueryId(sqlRequest.getBackdoorToggles().get(BackdoorToggles.CUSTOMIZE_QUERY_ID));",
          "451:         }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "484:                     sqlResponse = queryAndUpdateCache(sqlRequest, isQueryCacheEnabled);",
          "485:                 }",
          "486:             }",
          "488:             sqlResponse.setDuration(queryContext.getAccumulatedMillis());",
          "489:             if (QuerySparkMetrics.getInstance().getQueryExecutionMetrics(queryContext.getQueryId()) != null) {",
          "490:                 String sqlTraceUrl = SparderContext.appMasterTrackURL() + \"/SQL/execution/?id=\" +",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "491:             sqlResponse.setQueryId(queryContext.getQueryId());",
          "",
          "---------------"
        ],
        "server-base/src/test/java/org/apache/kylin/rest/response/SQLResponseTest.java||server-base/src/test/java/org/apache/kylin/rest/response/SQLResponseTest.java": [
          "File: server-base/src/test/java/org/apache/kylin/rest/response/SQLResponseTest.java -> server-base/src/test/java/org/apache/kylin/rest/response/SQLResponseTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "36:                 \"realizationTypes\", \"affectedRowCount\", \"isException\",",
          "37:                 \"exceptionMessage\", \"duration\", \"partial\", \"totalScanCount\", \"hitExceptionCache\",",
          "38:                 \"storageCacheUsed\", \"sparkPool\", \"pushDown\", \"traceUrl\", \"totalScanBytes\",",
          "41:         SQLResponse sqlResponse = new SQLResponse(null, null, \"learn_cube\", 100, false, null, false, false);",
          "42:         String jsonStr = JsonUtil.writeValueAsString(sqlResponse);",
          "",
          "[Removed Lines]",
          "39:                 \"totalScanFiles\", \"metadataTime\", \"totalSparkScanTime\", \"traces\"};",
          "",
          "[Added Lines]",
          "39:                 \"totalScanFiles\", \"metadataTime\", \"totalSparkScanTime\", \"traces\", \"queryId\"};",
          "",
          "---------------"
        ]
      }
    }
  ]
}