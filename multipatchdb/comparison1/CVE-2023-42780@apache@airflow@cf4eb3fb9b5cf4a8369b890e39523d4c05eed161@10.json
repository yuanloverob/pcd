{
  "cve_id": "CVE-2023-42780",
  "cve_desc": "Apache Airflow, versions prior to 2.7.2, contains a security vulnerability that allows authenticated users of Airflow to list warnings for all DAGs, even if the user had no permission to see those DAGs. It would reveal the dag_ids and the stack-traces of import errors for those DAGs with import errors.\nUsers of Apache Airflow are advised to upgrade to version 2.7.2 or newer to mitigate the risk associated with this vulnerability.\n\n",
  "repo": "apache/airflow",
  "patch_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
  "patch_info": {
    "commit_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ],
    "message": "Fix dag warning endpoint permissions (#34355)\n\n* Fix dag warning endpoint permissions\n\n* update the query to have an accurate result for total entries and pagination\n\n* add unit tests\n\n* Update test_dag_warning_endpoint.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 3570bbfbea69e2965f91b9964ce28bc268c68129)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_warning_endpoint.py -> airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "16: # under the License.",
      "17: from __future__ import annotations",
      "19: from sqlalchemy import select",
      "20: from sqlalchemy.orm import Session",
      "22: from airflow.api_connexion import security",
      "23: from airflow.api_connexion.parameters import apply_sorting, check_limit, format_parameters",
      "24: from airflow.api_connexion.schemas.dag_warning_schema import (",
      "25:     DagWarningCollection,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "19: from flask import g",
      "24: from airflow.api_connexion.exceptions import PermissionDenied",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "28: from airflow.api_connexion.types import APIResponse",
      "29: from airflow.models.dagwarning import DagWarning as DagWarningModel",
      "30: from airflow.security import permissions",
      "31: from airflow.utils.db import get_query_count",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from airflow.utils.airflow_flask_app import get_airflow_app",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "52:     allowed_filter_attrs = [\"dag_id\", \"warning_type\", \"message\", \"timestamp\"]",
      "53:     query = select(DagWarningModel)",
      "54:     if dag_id:",
      "55:         query = query.where(DagWarningModel.dag_id == dag_id)",
      "56:     if warning_type:",
      "57:         query = query.where(DagWarningModel.warning_type == warning_type)",
      "58:     total_entries = get_query_count(query, session=session)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "58:         if not get_airflow_app().appbuilder.sm.can_read_dag(dag_id, g.user):",
      "59:             raise PermissionDenied(detail=f\"User not allowed to access this DAG: {dag_id}\")",
      "61:     else:",
      "62:         readable_dags = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
      "63:         query = query.where(DagWarningModel.dag_id.in_(readable_dags))",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_warning_endpoint.py -> tests/api_connexion/endpoints/test_dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35:         app,  # type:ignore",
      "36:         username=\"test\",",
      "37:         role_name=\"Test\",",
      "39:     )",
      "40:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "42:     yield minimal_app_for_api",
      "44:     delete_user(app, username=\"test\")  # type: ignore",
      "45:     delete_user(app, username=\"test_no_permissions\")  # type: ignore",
      "48: class TestBaseDagWarning:",
      "",
      "[Removed Lines]",
      "38:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING)],  # type: ignore",
      "",
      "[Added Lines]",
      "38:         permissions=[",
      "39:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "40:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
      "41:         ],  # type: ignore",
      "44:     create_user(",
      "45:         app,  # type:ignore",
      "46:         username=\"test_with_dag2_read\",",
      "47:         role_name=\"TestWithDag2Read\",",
      "48:         permissions=[",
      "49:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "50:             (permissions.ACTION_CAN_READ, f\"{permissions.RESOURCE_DAG_PREFIX}dag2\"),",
      "51:         ],  # type: ignore",
      "52:     )",
      "58:     delete_user(app, username=\"test_with_dag2_read\")  # type: ignore",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "147:             \"/api/v1/dagWarnings\", environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"}",
      "148:         )",
      "149:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "164:     def test_should_raise_403_forbidden_when_user_has_no_dag_read_permission(self):",
      "165:         response = self.client.get(",
      "166:             \"/api/v1/dagWarnings\",",
      "167:             environ_overrides={\"REMOTE_USER\": \"test_with_dag2_read\"},",
      "168:             query_string={\"dag_id\": \"dag1\"},",
      "169:         )",
      "170:         assert response.status_code == 403",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "eada688f87ba7f11cb8b8f27dd4f908f5ceb6db3",
      "candidate_info": {
        "commit_hash": "eada688f87ba7f11cb8b8f27dd4f908f5ceb6db3",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/eada688f87ba7f11cb8b8f27dd4f908f5ceb6db3",
        "files": [
          "airflow/cli/cli_config.py"
        ],
        "message": "fix(cli): remove \"to backfill\" from --task-regex help message (#34598)\n\nthis arg is used by both \"airflow tasks clear\" and \"airflow tasks backfill\"\nand it does not make sense for \"airflow tasks clear\" to have the description \"to backfile\"\n\n(cherry picked from commit c019cf18dd1be3b20baf7503326a53002c236b45)",
        "before_after_code_files": [
          "airflow/cli/cli_config.py||airflow/cli/cli_config.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/cli_config.py||airflow/cli/cli_config.py": [
          "File: airflow/cli/cli_config.py -> airflow/cli/cli_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "156:     nargs=\"?\",",
          "157:     help=\"The execution_date of the DAG or run_id of the DAGRun (optional)\",",
          "158: )",
          "162: ARG_SUBDIR = Arg(",
          "163:     (\"-S\", \"--subdir\"),",
          "164:     help=(",
          "",
          "[Removed Lines]",
          "159: ARG_TASK_REGEX = Arg(",
          "160:     (\"-t\", \"--task-regex\"), help=\"The regex to filter specific task_ids to backfill (optional)\"",
          "161: )",
          "",
          "[Added Lines]",
          "159: ARG_TASK_REGEX = Arg((\"-t\", \"--task-regex\"), help=\"The regex to filter specific task_ids (optional)\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b38c37bae6956d5b624f2970e141e5b2ed6ed278",
      "candidate_info": {
        "commit_hash": "b38c37bae6956d5b624f2970e141e5b2ed6ed278",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b38c37bae6956d5b624f2970e141e5b2ed6ed278",
        "files": [
          "airflow/jobs/scheduler_job_runner.py",
          "tests/jobs/test_scheduler_job.py"
        ],
        "message": "Fix scheduler logic to plan new dag runs by ignoring manual runs (#34027)\n\n* Fix manual task triggering scheduled tasks\n\nFixes #33949\n\n* fix static checks\n\n* static checks\n\n* add unit test\n\n* static check\n\n* Undo renaming\n\n* Update airflow/jobs/scheduler_job_runner.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n* use keyword-only arguments for last_dag_run and total_active_runs\n\n---------\n\nCo-authored-by: daniel.dylag <danieldylag1990@gmail.com>\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 20d81428699db240b65f72a92183255c24e8c19b)",
        "before_after_code_files": [
          "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py",
          "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py": [
          "File: airflow/jobs/scheduler_job_runner.py -> airflow/jobs/scheduler_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1183:                 )",
          "1184:                 active_runs_of_dags[dag.dag_id] += 1",
          "1185:             if self._should_update_dag_next_dagruns(",
          "1187:             ):",
          "1188:                 dag_model.calculate_dagrun_date_fields(dag, data_interval)",
          "1189:         # TODO[HA]: Should we do a session.flush() so we don't have to keep lots of state/object in",
          "",
          "[Removed Lines]",
          "1186:                 dag, dag_model, active_runs_of_dags[dag.dag_id], session=session",
          "",
          "[Added Lines]",
          "1186:                 dag,",
          "1187:                 dag_model,",
          "1188:                 last_dag_run=None,",
          "1189:                 total_active_runs=active_runs_of_dags[dag.dag_id],",
          "1190:                 session=session,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1291:                 )",
          "1293:     def _should_update_dag_next_dagruns(",
          "1295:     ) -> bool:",
          "1296:         \"\"\"Check if the dag's next_dagruns_create_after should be updated.\"\"\"",
          "1297:         # If the DAG never schedules skip save runtime",
          "1298:         if not dag.timetable.can_be_scheduled:",
          "1299:             return False",
          "",
          "[Removed Lines]",
          "1294:         self, dag: DAG, dag_model: DagModel, total_active_runs: int | None = None, *, session: Session",
          "",
          "[Added Lines]",
          "1298:         self,",
          "1299:         dag: DAG,",
          "1300:         dag_model: DagModel,",
          "1302:         last_dag_run: DagRun | None = None,",
          "1303:         total_active_runs: int | None = None,",
          "1304:         session: Session,",
          "1307:         # If last_dag_run is defined, the update was triggered by a scheduling decision in this DAG run.",
          "1308:         # In such case, schedule next only if last_dag_run is finished and was an automated run.",
          "1309:         if last_dag_run and not (",
          "1310:             last_dag_run.state in State.finished_dr_states",
          "1311:             and last_dag_run.run_type in [DagRunType.SCHEDULED, DagRunType.BACKFILL_JOB]",
          "1312:         ):",
          "1313:             return False",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1428:                 session.merge(task_instance)",
          "1429:             session.flush()",
          "1430:             self.log.info(\"Run %s of %s has timed-out\", dag_run.run_id, dag_run.dag_id)",
          "1433:                 dag_model.calculate_dagrun_date_fields(dag, dag.get_run_data_interval(dag_run))",
          "1435:             callback_to_execute = DagCallbackRequest(",
          "",
          "[Removed Lines]",
          "1431:             # Work out if we should allow creating a new DagRun now?",
          "1432:             if self._should_update_dag_next_dagruns(dag, dag_model, session=session):",
          "",
          "[Added Lines]",
          "1449:             if self._should_update_dag_next_dagruns(dag, dag_model, last_dag_run=dag_run, session=session):",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1456:             return callback",
          "1457:         # TODO[HA]: Rename update_state -> schedule_dag_run, ?? something else?",
          "1458:         schedulable_tis, callback_to_run = dag_run.update_state(session=session, execute_callbacks=False)",
          "1464:         # This will do one query per dag run. We \"could\" build up a complex",
          "1465:         # query to update all the TIs across all the execution dates and dag",
          "1466:         # IDs in a single query, but it turns out that can be _very very slow_",
          "",
          "[Removed Lines]",
          "1459:         # Check if DAG not scheduled then skip interval calculation to same scheduler runtime",
          "1460:         if dag_run.state in State.finished_dr_states:",
          "1461:             # Work out if we should allow creating a new DagRun now?",
          "1462:             if self._should_update_dag_next_dagruns(dag, dag_model, session=session):",
          "1463:                 dag_model.calculate_dagrun_date_fields(dag, dag.get_run_data_interval(dag_run))",
          "",
          "[Added Lines]",
          "1477:         if self._should_update_dag_next_dagruns(dag, dag_model, last_dag_run=dag_run, session=session):",
          "1478:             dag_model.calculate_dagrun_date_fields(dag, dag.get_run_data_interval(dag_run))",
          "",
          "---------------"
        ],
        "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py": [
          "File: tests/jobs/test_scheduler_job.py -> tests/jobs/test_scheduler_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1755:             # Need to use something that doesn't immediately get marked as success by the scheduler",
          "1756:             BashOperator(task_id=\"task\", bash_command=\"true\")",
          "1763:         # Reach max_active_runs",
          "1764:         for _ in range(3):",
          "",
          "[Removed Lines]",
          "1758:         dag_run = dag_maker.create_dagrun(",
          "1759:             state=State.RUNNING,",
          "1760:             session=session,",
          "1761:         )",
          "",
          "[Added Lines]",
          "1758:         dag_run = dag_maker.create_dagrun(state=State.RUNNING, session=session, run_type=DagRunType.SCHEDULED)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3459:         self.job_runner = SchedulerJobRunner(job=scheduler_job)",
          "3461:         assert excepted is self.job_runner._should_update_dag_next_dagruns(",
          "3463:         )",
          "3465:     def test_create_dag_runs(self, dag_maker):",
          "",
          "[Removed Lines]",
          "3462:             dag, dag_model, number_running, session=session",
          "",
          "[Added Lines]",
          "3459:             dag, dag_model, total_active_runs=number_running, session=session",
          "3460:         )",
          "3462:     @pytest.mark.parametrize(",
          "3463:         \"run_type, should_update\",",
          "3464:         [",
          "3465:             (DagRunType.MANUAL, False),",
          "3466:             (DagRunType.SCHEDULED, True),",
          "3467:             (DagRunType.BACKFILL_JOB, True),",
          "3468:             (DagRunType.DATASET_TRIGGERED, False),",
          "3469:         ],",
          "3470:         ids=[",
          "3471:             DagRunType.MANUAL.name,",
          "3472:             DagRunType.SCHEDULED.name,",
          "3473:             DagRunType.BACKFILL_JOB.name,",
          "3474:             DagRunType.DATASET_TRIGGERED.name,",
          "3475:         ],",
          "3476:     )",
          "3477:     def test_should_update_dag_next_dagruns_after_run_type(self, run_type, should_update, session, dag_maker):",
          "3478:         \"\"\"Test that whether next dagrun is updated depends on run type\"\"\"",
          "3479:         with dag_maker(",
          "3480:             dag_id=\"test_should_update_dag_next_dagruns_after_run_type\",",
          "3481:             schedule=\"*/1 * * * *\",",
          "3482:             max_active_runs=10,",
          "3483:         ) as dag:",
          "3484:             EmptyOperator(task_id=\"dummy\")",
          "3486:         dag_model = dag_maker.dag_model",
          "3488:         run = dag_maker.create_dagrun(",
          "3489:             run_id=\"run\",",
          "3490:             run_type=run_type,",
          "3491:             execution_date=DEFAULT_DATE,",
          "3492:             start_date=timezone.utcnow(),",
          "3493:             state=State.SUCCESS,",
          "3494:             session=session,",
          "3495:         )",
          "3497:         session.flush()",
          "3498:         scheduler_job = Job(executor=self.null_exec)",
          "3499:         self.job_runner = SchedulerJobRunner(job=scheduler_job)",
          "3501:         assert should_update is self.job_runner._should_update_dag_next_dagruns(",
          "3502:             dag, dag_model, last_dag_run=run, total_active_runs=0, session=session",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a78ba98e6d82959cbd4570c0506d17dd2ea74a1f",
      "candidate_info": {
        "commit_hash": "a78ba98e6d82959cbd4570c0506d17dd2ea74a1f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a78ba98e6d82959cbd4570c0506d17dd2ea74a1f",
        "files": [
          "airflow/models/taskinstance.py",
          "airflow/providers/amazon/aws/hooks/sagemaker.py",
          "airflow/providers/elasticsearch/log/es_json_formatter.py",
          "airflow/providers/google/cloud/operators/dataproc.py",
          "airflow/providers/oracle/hooks/oracle.py",
          "airflow/task/task_runner/cgroup_task_runner.py",
          "airflow/utils/log/timezone_aware.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "tests/dags_corrupted/test_impersonation_custom.py",
          "tests/providers/amazon/aws/utils/eks_test_utils.py",
          "tests/providers/apache/kylin/operators/test_kylin_cube.py",
          "tests/providers/http/sensors/test_http.py"
        ],
        "message": "Replace strftime with f-strings where nicer (#33455)\n\n(cherry picked from commit 94f70d818482de7defa03c0aff3c213ca6b83e9e)",
        "before_after_code_files": [
          "airflow/models/taskinstance.py||airflow/models/taskinstance.py",
          "airflow/providers/amazon/aws/hooks/sagemaker.py||airflow/providers/amazon/aws/hooks/sagemaker.py",
          "airflow/providers/elasticsearch/log/es_json_formatter.py||airflow/providers/elasticsearch/log/es_json_formatter.py",
          "airflow/providers/google/cloud/operators/dataproc.py||airflow/providers/google/cloud/operators/dataproc.py",
          "airflow/providers/oracle/hooks/oracle.py||airflow/providers/oracle/hooks/oracle.py",
          "airflow/task/task_runner/cgroup_task_runner.py||airflow/task/task_runner/cgroup_task_runner.py",
          "airflow/utils/log/timezone_aware.py||airflow/utils/log/timezone_aware.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "tests/dags_corrupted/test_impersonation_custom.py||tests/dags_corrupted/test_impersonation_custom.py",
          "tests/providers/amazon/aws/utils/eks_test_utils.py||tests/providers/amazon/aws/utils/eks_test_utils.py",
          "tests/providers/apache/kylin/operators/test_kylin_cube.py||tests/providers/apache/kylin/operators/test_kylin_cube.py",
          "tests/providers/http/sensors/test_http.py||tests/providers/http/sensors/test_http.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/taskinstance.py||airflow/models/taskinstance.py": [
          "File: airflow/models/taskinstance.py -> airflow/models/taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2141:             execution_date = get_prev_execution_date()",
          "2142:             if execution_date is None:",
          "2143:                 return None",
          "2146:         def get_prev_ds_nodash() -> str | None:",
          "2147:             prev_ds = get_prev_ds()",
          "",
          "[Removed Lines]",
          "2144:             return execution_date.strftime(r\"%Y-%m-%d\")",
          "",
          "[Added Lines]",
          "2144:             return execution_date.strftime(\"%Y-%m-%d\")",
          "",
          "---------------"
        ],
        "airflow/providers/amazon/aws/hooks/sagemaker.py||airflow/providers/amazon/aws/hooks/sagemaker.py": [
          "File: airflow/providers/amazon/aws/hooks/sagemaker.py -> airflow/providers/amazon/aws/hooks/sagemaker.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "128:     status_strs = []",
          "129:     for transition in transitions_to_print:",
          "130:         message = transition[\"StatusMessage\"]",
          "136:     return \"\\n\".join(status_strs)",
          "",
          "[Removed Lines]",
          "131:         time_str = timezone.convert_to_utc(cast(datetime, job_description[\"LastModifiedTime\"])).strftime(",
          "132:             \"%Y-%m-%d %H:%M:%S\"",
          "133:         )",
          "134:         status_strs.append(f\"{time_str} {transition['Status']} - {message}\")",
          "",
          "[Added Lines]",
          "131:         time_utc = timezone.convert_to_utc(cast(datetime, job_description[\"LastModifiedTime\"]))",
          "132:         status_strs.append(f\"{time_utc:%Y-%m-%d %H:%M:%S} {transition['Status']} - {message}\")",
          "",
          "---------------"
        ],
        "airflow/providers/elasticsearch/log/es_json_formatter.py||airflow/providers/elasticsearch/log/es_json_formatter.py": [
          "File: airflow/providers/elasticsearch/log/es_json_formatter.py -> airflow/providers/elasticsearch/log/es_json_formatter.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31:     def formatTime(self, record, datefmt=None):",
          "32:         \"\"\"Return the creation time of the LogRecord in ISO 8601 date/time format in the local time zone.\"\"\"",
          "33:         dt = pendulum.from_timestamp(record.created, tz=pendulum.local_timezone())",
          "39:         if self.default_msec_format:",
          "40:             s = self.default_msec_format % (s, record.msecs)",
          "41:         if self.default_tz_format:",
          "",
          "[Removed Lines]",
          "34:         if datefmt:",
          "35:             s = dt.strftime(datefmt)",
          "36:         else:",
          "37:             s = dt.strftime(self.default_time_format)",
          "",
          "[Added Lines]",
          "34:         s = dt.strftime(datefmt or self.default_time_format)",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/operators/dataproc.py||airflow/providers/google/cloud/operators/dataproc.py": [
          "File: airflow/providers/google/cloud/operators/dataproc.py -> airflow/providers/google/cloud/operators/dataproc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1567:     @staticmethod",
          "1568:     def _generate_temp_filename(filename):",
          "1572:     def _upload_file_temp(self, bucket, local_file):",
          "1573:         \"\"\"Upload a local file to a Google Cloud Storage bucket.\"\"\"",
          "",
          "[Removed Lines]",
          "1569:         date = time.strftime(\"%Y%m%d%H%M%S\")",
          "1570:         return f\"{date}_{str(uuid.uuid4())[:8]}_{ntpath.basename(filename)}\"",
          "",
          "[Added Lines]",
          "1569:         return f\"{time:%Y%m%d%H%M%S}_{str(uuid.uuid4())[:8]}_{ntpath.basename(filename)}\"",
          "",
          "---------------"
        ],
        "airflow/providers/oracle/hooks/oracle.py||airflow/providers/oracle/hooks/oracle.py": [
          "File: airflow/providers/oracle/hooks/oracle.py -> airflow/providers/oracle/hooks/oracle.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "303:                 elif numpy and isinstance(cell, numpy.datetime64):",
          "304:                     lst.append(\"'\" + str(cell) + \"'\")",
          "305:                 elif isinstance(cell, datetime):",
          "309:                 else:",
          "310:                     lst.append(str(cell))",
          "311:             values = tuple(lst)",
          "",
          "[Removed Lines]",
          "306:                     lst.append(",
          "307:                         \"to_date('\" + cell.strftime(\"%Y-%m-%d %H:%M:%S\") + \"','YYYY-MM-DD HH24:MI:SS')\"",
          "308:                     )",
          "",
          "[Added Lines]",
          "306:                     lst.append(f\"to_date('{cell:%Y-%m-%d %H:%M:%S}','YYYY-MM-DD HH24:MI:SS')\")",
          "",
          "---------------"
        ],
        "airflow/task/task_runner/cgroup_task_runner.py||airflow/task/task_runner/cgroup_task_runner.py": [
          "File: airflow/task/task_runner/cgroup_task_runner.py -> airflow/task/task_runner/cgroup_task_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "134:             return",
          "136:         # Create a unique cgroup name",
          "139:         self.mem_cgroup_name = f\"memory/{cgroup_name}\"",
          "140:         self.cpu_cgroup_name = f\"cpu/{cgroup_name}\"",
          "",
          "[Removed Lines]",
          "137:         cgroup_name = f\"airflow/{datetime.datetime.utcnow().strftime('%Y-%m-%d')}/{str(uuid.uuid4())}\"",
          "",
          "[Added Lines]",
          "137:         cgroup_name = f\"airflow/{datetime.datetime.utcnow():%Y-%m-%d}/{uuid.uuid4()}\"",
          "",
          "---------------"
        ],
        "airflow/utils/log/timezone_aware.py||airflow/utils/log/timezone_aware.py": [
          "File: airflow/utils/log/timezone_aware.py -> airflow/utils/log/timezone_aware.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "40:         date and time format in the local time zone.",
          "41:         \"\"\"",
          "42:         dt = pendulum.from_timestamp(record.created, tz=pendulum.local_timezone())",
          "48:         if self.default_msec_format:",
          "49:             s = self.default_msec_format % (s, record.msecs)",
          "50:         if self.default_tz_format:",
          "",
          "[Removed Lines]",
          "43:         if datefmt:",
          "44:             s = dt.strftime(datefmt)",
          "45:         else:",
          "46:             s = dt.strftime(self.default_time_format)",
          "",
          "[Added Lines]",
          "43:         s = dt.strftime(datefmt or self.default_time_format)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1202:         get_console().print()",
          "1203:         get_console().print(",
          "1204:             \"Issue title: [yellow]Status of testing Providers that were \"",
          "1206:         )",
          "1207:         get_console().print()",
          "1208:         syntax = Syntax(issue_content, \"markdown\", theme=\"ansi_dark\")",
          "",
          "[Removed Lines]",
          "1205:             f\"prepared on {datetime.now().strftime('%B %d, %Y')}[/]\"",
          "",
          "[Added Lines]",
          "1205:             f\"prepared on {datetime.now():%B %d, %Y}[/]\"",
          "",
          "---------------"
        ],
        "tests/dags_corrupted/test_impersonation_custom.py||tests/dags_corrupted/test_impersonation_custom.py": [
          "File: tests/dags_corrupted/test_impersonation_custom.py -> tests/dags_corrupted/test_impersonation_custom.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "41: def print_today():",
          "42:     date_time = FakeDatetime.utcnow()",
          "46: def check_hive_conf():",
          "",
          "[Removed Lines]",
          "43:     print(f\"Today is {date_time.strftime('%Y-%m-%d')}\")",
          "",
          "[Added Lines]",
          "43:     print(f\"Today is {date_time:%Y-%m-%d}\")",
          "",
          "---------------"
        ],
        "tests/providers/amazon/aws/utils/eks_test_utils.py||tests/providers/amazon/aws/utils/eks_test_utils.py": [
          "File: tests/providers/amazon/aws/utils/eks_test_utils.py -> tests/providers/amazon/aws/utils/eks_test_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "237: def iso_date(input_datetime: datetime.datetime) -> str:",
          "241: def generate_dict(prefix, count) -> dict:",
          "",
          "[Removed Lines]",
          "238:     return input_datetime.strftime(\"%Y-%m-%dT%H:%M:%S\") + \"Z\"",
          "",
          "[Added Lines]",
          "238:     return f\"{input_datetime:%Y-%m-%dT%H:%M:%S}Z\"",
          "",
          "---------------"
        ],
        "tests/providers/apache/kylin/operators/test_kylin_cube.py||tests/providers/apache/kylin/operators/test_kylin_cube.py": [
          "File: tests/providers/apache/kylin/operators/test_kylin_cube.py -> tests/providers/apache/kylin/operators/test_kylin_cube.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "37:         \"project\": \"learn_kylin\",",
          "38:         \"cube\": \"kylin_sales_cube\",",
          "39:         \"command\": \"build\",",
          "42:     }",
          "43:     cube_command = [",
          "44:         \"fullbuild\",",
          "",
          "[Removed Lines]",
          "40:         \"start_time\": datetime(2012, 1, 2, 0, 0).strftime(\"%s\") + \"000\",",
          "41:         \"end_time\": datetime(2012, 1, 3, 0, 0).strftime(\"%s\") + \"000\",",
          "",
          "[Added Lines]",
          "40:         \"start_time\": str(int(datetime(2012, 1, 2, 0, 0).timestamp() * 1000)),",
          "41:         \"end_time\": str(int(datetime(2012, 1, 3, 0, 0).timestamp() * 1000)),",
          "",
          "---------------"
        ],
        "tests/providers/http/sensors/test_http.py||tests/providers/http/sensors/test_http.py": [
          "File: tests/providers/http/sensors/test_http.py -> tests/providers/http/sensors/test_http.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "244:             endpoint=\"/search\",",
          "245:             request_params={\"client\": \"ubuntu\", \"q\": \"airflow\", \"date\": \"{{ds}}\"},",
          "246:             headers={},",
          "250:             poke_interval=5,",
          "251:             timeout=15,",
          "252:             dag=self.dag,",
          "",
          "[Removed Lines]",
          "247:             response_check=lambda response: (",
          "248:                 \"apache/airflow/\" + DEFAULT_DATE.strftime(\"%Y-%m-%d\") in response.text",
          "249:             ),",
          "",
          "[Added Lines]",
          "247:             response_check=lambda response: f\"apache/airflow/{DEFAULT_DATE:%Y-%m-%d}\" in response.text,",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "830cd9847b6e04f03aa7f5b6b2f3e661a0422bf2",
      "candidate_info": {
        "commit_hash": "830cd9847b6e04f03aa7f5b6b2f3e661a0422bf2",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/830cd9847b6e04f03aa7f5b6b2f3e661a0422bf2",
        "files": [
          "airflow/models/baseoperator.py",
          "airflow/serialization/serialized_objects.py",
          "airflow/ti_deps/deps/trigger_rule_dep.py"
        ],
        "message": "Combine similar if logics in core (#33988)\n\n* Combine similar if logics in core\n\n* Update airflow/models/baseoperator.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n* replace in tuple by multiple or equalities\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit d800a0de5194bb1ef3cfad44c874abafcc78efd6)",
        "before_after_code_files": [
          "airflow/models/baseoperator.py||airflow/models/baseoperator.py",
          "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py",
          "airflow/ti_deps/deps/trigger_rule_dep.py||airflow/ti_deps/deps/trigger_rule_dep.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/baseoperator.py||airflow/models/baseoperator.py": [
          "File: airflow/models/baseoperator.py -> airflow/models/baseoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1103:         if self.__from_mapped:",
          "1104:             pass  # Don't add to DAG -- the mapped task takes the place.",
          "1108:             dag.add_task(self)",
          "1110:         self._dag = dag",
          "",
          "[Removed Lines]",
          "1105:         elif self.task_id not in dag.task_dict:",
          "1106:             dag.add_task(self)",
          "1107:         elif self.task_id in dag.task_dict and dag.task_dict[self.task_id] is not self:",
          "",
          "[Added Lines]",
          "1105:         elif dag.task_dict.get(self.task_id) is not self:",
          "",
          "---------------"
        ],
        "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py": [
          "File: airflow/serialization/serialized_objects.py -> airflow/serialization/serialized_objects.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "985:                 v = {arg: cls.deserialize(value) for arg, value in v.items()}",
          "986:             elif k in {\"expand_input\", \"op_kwargs_expand_input\"}:",
          "987:                 v = _ExpandInputRef(v[\"type\"], cls.deserialize(v[\"value\"]))",
          "991:                 v = cls.deserialize(v)",
          "992:             elif k == \"on_failure_fail_dagrun\":",
          "993:                 k = \"_on_failure_fail_dagrun\"",
          "",
          "[Removed Lines]",
          "988:             elif k in cls._decorated_fields or k not in op.get_serialized_fields():",
          "989:                 v = cls.deserialize(v)",
          "990:             elif k in (\"outlets\", \"inlets\"):",
          "",
          "[Added Lines]",
          "988:             elif (",
          "989:                 k in cls._decorated_fields",
          "990:                 or k not in op.get_serialized_fields()",
          "991:                 or k in (\"outlets\", \"inlets\")",
          "992:             ):",
          "",
          "---------------"
        ],
        "airflow/ti_deps/deps/trigger_rule_dep.py||airflow/ti_deps/deps/trigger_rule_dep.py": [
          "File: airflow/ti_deps/deps/trigger_rule_dep.py -> airflow/ti_deps/deps/trigger_rule_dep.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "470:                             f\"upstream_task_ids={task.upstream_task_ids}\"",
          "471:                         )",
          "472:                     )",
          "487:                 num_failures = upstream - success - skipped",
          "488:                 if ti.map_index > -1:",
          "489:                     num_failures -= removed",
          "",
          "[Removed Lines]",
          "473:             elif trigger_rule == TR.NONE_FAILED:",
          "474:                 num_failures = upstream - success - skipped",
          "475:                 if ti.map_index > -1:",
          "476:                     num_failures -= removed",
          "477:                 if num_failures > 0:",
          "478:                     yield self._failing_status(",
          "479:                         reason=(",
          "480:                             f\"Task's trigger rule '{trigger_rule}' requires all upstream tasks to have \"",
          "481:                             f\"succeeded or been skipped, but found {num_failures} non-success(es). \"",
          "482:                             f\"upstream_states={upstream_states}, \"",
          "483:                             f\"upstream_task_ids={task.upstream_task_ids}\"",
          "484:                         )",
          "485:                     )",
          "486:             elif trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:",
          "",
          "[Added Lines]",
          "473:             elif trigger_rule == TR.NONE_FAILED or trigger_rule == TR.NONE_FAILED_MIN_ONE_SUCCESS:",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2bfbffe09c2ab82836e297bd4bf0b6d6e0c7d93b",
      "candidate_info": {
        "commit_hash": "2bfbffe09c2ab82836e297bd4bf0b6d6e0c7d93b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/2bfbffe09c2ab82836e297bd4bf0b6d6e0c7d93b",
        "files": [
          "airflow/api_connexion/openapi/v1.yaml",
          "airflow/api_connexion/schemas/task_instance_schema.py",
          "airflow/www/static/js/types/api-generated.ts",
          "tests/api_connexion/endpoints/test_task_instance_endpoint.py"
        ],
        "message": "Fix: make dry run optional for patch task instance  (#34568)\n\n* fix: Make dry_run optional per docs\n\nThis fixes an issue where dry_run is not actually and optional parameter\nin the patch task_instance api.\n\n* chore: remove formatting changes\n\n* fix: Make changes for api docs\n\nThis updates the docs and the code so that they are in alignment while\nalso being consistent with all other endpoints. All other Endpoints have\ndry run set to be True by default.\n\n* fix: Update static ts file for api change\n\n* fix: Remove dump_default\n\n(cherry picked from commit a4357ca25cc3d014e50968bac7858f533e6421e4)",
        "before_after_code_files": [
          "airflow/api_connexion/schemas/task_instance_schema.py||airflow/api_connexion/schemas/task_instance_schema.py",
          "airflow/www/static/js/types/api-generated.ts||airflow/www/static/js/types/api-generated.ts",
          "tests/api_connexion/endpoints/test_task_instance_endpoint.py||tests/api_connexion/endpoints/test_task_instance_endpoint.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api_connexion/schemas/task_instance_schema.py||airflow/api_connexion/schemas/task_instance_schema.py": [
          "File: airflow/api_connexion/schemas/task_instance_schema.py -> airflow/api_connexion/schemas/task_instance_schema.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "177: class SetSingleTaskInstanceStateFormSchema(Schema):",
          "178:     \"\"\"Schema for handling the request of updating state of a single task instance.\"\"\"",
          "181:     new_state = TaskInstanceStateField(",
          "182:         required=True,",
          "183:         validate=validate.OneOf(",
          "",
          "[Removed Lines]",
          "180:     dry_run = fields.Boolean(dump_default=True)",
          "",
          "[Added Lines]",
          "180:     dry_run = fields.Boolean(load_default=True)",
          "",
          "---------------"
        ],
        "airflow/www/static/js/types/api-generated.ts||airflow/www/static/js/types/api-generated.ts": [
          "File: airflow/www/static/js/types/api-generated.ts -> airflow/www/static/js/types/api-generated.ts"
        ],
        "tests/api_connexion/endpoints/test_task_instance_endpoint.py||tests/api_connexion/endpoints/test_task_instance_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_task_instance_endpoint.py -> tests/api_connexion/endpoints/test_task_instance_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1772:         assert response2.status_code == 200",
          "1773:         assert response2.json[\"state\"] == NEW_STATE",
          "1775:     def test_should_update_mapped_task_instance_state(self, session):",
          "1776:         NEW_STATE = \"failed\"",
          "1777:         map_index = 1",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1775:     def test_should_update_task_instance_state_default_dry_run_to_true(self, session):",
          "1776:         self.create_task_instances(session)",
          "1778:         NEW_STATE = \"running\"",
          "1780:         self.client.patch(",
          "1781:             self.ENDPOINT_URL,",
          "1782:             environ_overrides={\"REMOTE_USER\": \"test\"},",
          "1783:             json={",
          "1784:                 \"new_state\": NEW_STATE,",
          "1785:             },",
          "1786:         )",
          "1788:         response2 = self.client.get(",
          "1789:             self.ENDPOINT_URL,",
          "1790:             environ_overrides={\"REMOTE_USER\": \"test\"},",
          "1791:             json={},",
          "1792:         )",
          "1793:         assert response2.status_code == 200",
          "1794:         assert response2.json[\"state\"] == NEW_STATE",
          "",
          "---------------"
        ]
      }
    }
  ]
}