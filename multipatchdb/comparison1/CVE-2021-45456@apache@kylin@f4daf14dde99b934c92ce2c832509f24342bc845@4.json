{
  "cve_id": "CVE-2021-45456",
  "cve_desc": "Apache kylin checks the legitimacy of the project before executing some commands with the project name passed in by the user. There is a mismatch between what is being checked and what is being used as the shell command argument in DiagnosisService. This may cause an illegal project name to pass the check and perform the following steps, resulting in a command injection vulnerability. This issue affects Apache Kylin 4.0.0.",
  "repo": "apache/kylin",
  "patch_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
  "patch_info": {
    "commit_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
    "repo": "apache/kylin",
    "commit_url": "https://github.com/apache/kylin/commit/f4daf14dde99b934c92ce2c832509f24342bc845",
    "files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "server/src/main/webapp/WEB-INF/web.xml"
    ],
    "message": "test fix",
    "before_after_code_files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java"
    ]
  },
  "patch_diff": {
    "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "3403:     public String getKerberosPrincipal() {",
      "3404:         return getOptional(\"kylin.kerberos.principal\");",
      "3405:     }",
      "3406: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "3407:     public String getEncryptCipherIvSpec() {",
      "3408:         return getOptional(\"kylin.security.encrypt.cipher.ivSpec\", \"AAAAAAAAAAAAAAAA\");",
      "3409:     }",
      "",
      "---------------"
    ],
    "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java -> core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "25: import java.security.NoSuchAlgorithmException;",
      "27: import org.apache.commons.codec.binary.Base64;",
      "29: import javax.crypto.Cipher;",
      "30: import javax.crypto.NoSuchPaddingException;",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "28: import org.apache.kylin.common.KylinConfig;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "42:             InvalidKeyException, NoSuchPaddingException, NoSuchAlgorithmException, UnsupportedEncodingException {",
      "43:         Cipher cipher = Cipher.getInstance(\"AES/CFB/PKCS5Padding\");",
      "44:         final SecretKeySpec secretKey = new SecretKeySpec(key, \"AES\");",
      "46:         cipher.init(cipherMode, secretKey, ivSpec);",
      "47:         return cipher;",
      "48:     }",
      "",
      "[Removed Lines]",
      "45:         IvParameterSpec ivSpec = new IvParameterSpec(\"AAAAAAAAAAAAAAAA\".getBytes(\"UTF-8\"));",
      "",
      "[Added Lines]",
      "46:         IvParameterSpec ivSpec = new IvParameterSpec(KylinConfig.getInstanceFromEnv().getEncryptCipherIvSpec().getBytes(\"UTF-8\"));",
      "",
      "---------------"
    ],
    "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java": [
      "File: core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java -> core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: package org.apache.kylin.common.util;",
      "21: import org.junit.Assert;",
      "22: import org.junit.Test;",
      "26:     @Test",
      "27:     public void testAESEncrypt(){",
      "",
      "[Removed Lines]",
      "24: public class EncryptUtilTest {",
      "",
      "[Added Lines]",
      "21: import org.junit.After;",
      "23: import org.junit.Before;",
      "26: public class EncryptUtilTest extends LocalFileMetadataTestCase {",
      "27:     @Before",
      "28:     public void setUp() throws Exception {",
      "29:         this.createTestMetadata();",
      "30:     }",
      "32:     @After",
      "33:     public void after() throws Exception {",
      "34:         this.cleanupTestMetadata();",
      "35:     }",
      "",
      "---------------"
    ],
    "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java": [
      "File: server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java -> server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "87:     public String dumpProjectDiagnosisInfo(String project, File exportPath) throws IOException {",
      "88:         Message msg = MsgPicker.getMsg();",
      "89:         ProjectInstance projectInstance =",
      "90:                 ProjectManager.getInstance(KylinConfig.getInstanceFromEnv())",
      "92:         if (null == projectInstance) {",
      "93:             throw new BadRequestException(",
      "95:         }",
      "96:         aclEvaluate.checkProjectOperationPermission(projectInstance);",
      "98:         runDiagnosisCLI(args);",
      "99:         return getDiagnosisPackageName(exportPath);",
      "100:     }",
      "",
      "[Removed Lines]",
      "91:                         .getProject(ValidateUtil.convertStringToBeAlphanumericUnderscore(project));",
      "94:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), project));",
      "97:         String[] args = { project, exportPath.getAbsolutePath() };",
      "",
      "[Added Lines]",
      "89:         String projectName = ValidateUtil.convertStringToBeAlphanumericUnderscore(project);",
      "92:                         .getProject(projectName);",
      "95:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), projectName));",
      "98:         String[] args = { projectName, exportPath.getAbsolutePath() };",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "5d552fe61e4a05b5f061805807f866ce53f4c03a",
      "candidate_info": {
        "commit_hash": "5d552fe61e4a05b5f061805807f866ce53f4c03a",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/5d552fe61e4a05b5f061805807f866ce53f4c03a",
        "files": [
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/spark/application/JobWorkSpace.scala"
        ],
        "message": "KYLIN-4895 change spark deploy mode of kylin4.0 engine from local to cluster",
        "before_after_code_files": [
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/spark/application/JobWorkSpace.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/spark/application/JobWorkSpace.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/application/SparkApplication.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import org.apache.commons.io.IOUtils;",
          "22: import org.apache.commons.lang.StringUtils;",
          "23: import org.apache.http.HttpHeaders;",
          "24: import org.apache.http.HttpResponse;",
          "25: import org.apache.http.HttpStatus;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "23: import org.apache.hadoop.fs.FSDataInputStream;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "33: import org.apache.kylin.engine.spark.job.UdfManager;",
          "34: import org.apache.kylin.engine.spark.utils.MetaDumpUtil;",
          "35: import org.apache.kylin.engine.spark.utils.SparkConfHelper;",
          "36: import java.io.IOException;",
          "37: import java.io.InputStream;",
          "38: import java.net.InetAddress;",
          "39: import java.net.URI;",
          "40: import java.net.UnknownHostException;",
          "41: import java.nio.charset.StandardCharsets;",
          "44: import java.util.HashMap;",
          "45: import java.util.Locale;",
          "46: import java.util.Map;",
          "",
          "[Removed Lines]",
          "42: import java.nio.file.Files;",
          "43: import java.nio.file.Paths;",
          "",
          "[Added Lines]",
          "38: import java.io.BufferedReader;",
          "41: import java.io.InputStreamReader;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "79:     protected BuildJobInfos infos;",
          "81:     public void execute(String[] args) {",
          "84:             if (argsLine.isEmpty()) {",
          "85:                 throw new RuntimeException(\"Args file is empty\");",
          "86:             }",
          "",
          "[Removed Lines]",
          "82:         try {",
          "83:             String argsLine = Files.readAllLines(Paths.get(args[0])).get(0);",
          "",
          "[Added Lines]",
          "84:         Path path = new Path(args[0]);",
          "85:         try (",
          "86:                 FileSystem fileSystem = FileSystem.get(path.toUri(), HadoopUtil.getCurrentConfiguration());",
          "87:                 FSDataInputStream inputStream = fileSystem.open(path);",
          "88:                 InputStreamReader inputStreamReader = new InputStreamReader(inputStream, StandardCharsets.UTF_8);",
          "89:                 BufferedReader bufferedReader = new BufferedReader(inputStreamReader);",
          "90:         ) {",
          "91:             String argsLine = bufferedReader.readLine();",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "312:             if (infos != null) {",
          "313:                 infos.jobEnd();",
          "314:             }",
          "319:         }",
          "320:     }",
          "",
          "[Removed Lines]",
          "315:             if (ss != null && !ss.conf().get(\"spark.master\").startsWith(\"local\")) {",
          "317:                 ss.stop();",
          "318:             }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: package org.apache.kylin.engine.spark.job;",
          "21: import java.io.File;",
          "22: import java.io.IOException;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import java.io.BufferedOutputStream;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "38: import org.apache.kylin.cube.CubeInstance;",
          "39: import org.apache.kylin.cube.CubeManager;",
          "40: import org.apache.kylin.engine.spark.utils.MetaDumpUtil;",
          "41: import org.apache.commons.collections.CollectionUtils;",
          "43: import org.apache.commons.lang.StringUtils;",
          "45: import org.apache.hadoop.conf.Configuration;",
          "",
          "[Removed Lines]",
          "42: import org.apache.commons.io.FileUtils;",
          "",
          "[Added Lines]",
          "42: import org.apache.hadoop.fs.FileSystem;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "169:     String dumpArgs() throws ExecuteException {",
          "170:         File tmpDir = null;",
          "171:         try {",
          "175:             logger.info(\"Spark job args json is : {}.\", JsonUtil.writeValueAsString(getParams()));",
          "177:         } catch (IOException e) {",
          "178:             if (tmpDir != null && tmpDir.exists()) {",
          "179:                 try {",
          "",
          "[Removed Lines]",
          "172:             tmpDir = File.createTempFile(MetadataConstants.P_SEGMENT_IDS, \"\");",
          "173:             FileUtils.writeByteArrayToFile(tmpDir, JsonUtil.writeValueAsBytes(getParams()));",
          "176:             return tmpDir.getCanonicalPath();",
          "",
          "[Added Lines]",
          "173:             String pathName = getId() + \"_\" + MetadataConstants.P_JOB_ID;",
          "174:             Path tgtPath = new Path(getConfig().getJobTmpDir(getParams().get(\"project\")), pathName);",
          "175:             FileSystem fileSystem = FileSystem.get(tgtPath.toUri(), HadoopUtil.getCurrentConfiguration());",
          "176:             try (BufferedOutputStream outputStream = new BufferedOutputStream(fileSystem.create(tgtPath, false))) {",
          "177:                 outputStream.write(JsonUtil.writeValueAsBytes(getParams()));",
          "178:             }",
          "181:             return tgtPath.toUri().toString();",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "382:         if (StringUtils.isNotBlank(sparkUploadFiles)) {",
          "383:             sb.append(\"--files \").append(sparkUploadFiles).append(\" \");",
          "384:         }",
          "385:         sb.append(\"--name job_step_%s \");",
          "386:         sb.append(\"--jars %s %s %s\");",
          "387:         String cmd = String.format(Locale.ROOT, sb.toString(), hadoopConf, sparkSubmitCmd, getId(), jars, kylinJobJar,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "390:         sb.append(\"--principal \").append(config.getKerberosPrincipal()).append(\" \");",
          "391:         sb.append(\"--keytab \").append(config.getKerberosKeytabPath()).append(\" \");",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/spark/application/JobWorkSpace.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/spark/application/JobWorkSpace.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/spark/application/JobWorkSpace.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/spark/application/JobWorkSpace.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "35:       val worker = new JobWorker(application, appArgs, eventLoop)",
          "36:       val monitor = new JobMonitor(eventLoop)",
          "37:       val workspace = new JobWorkSpace(eventLoop, monitor, worker)",
          "39:     } catch {",
          "40:       case throwable: Throwable =>",
          "41:         logError(\"Error occurred when init job workspace.\", throwable)",
          "",
          "[Removed Lines]",
          "38:       System.exit(workspace.run())",
          "",
          "[Added Lines]",
          "38:       if (System.getProperty(\"spark.master\").equals(\"yarn\") && System.getProperty(\"spark.submit.deployMode\").equals(\"cluster\")) {",
          "39:         workspace.run()",
          "40:       } else {",
          "41:         System.exit(workspace.run())",
          "42:       }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b996b7b8c3d4c6c81193962edda6540a94a3bd4f",
      "candidate_info": {
        "commit_hash": "b996b7b8c3d4c6c81193962edda6540a94a3bd4f",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/b996b7b8c3d4c6c81193962edda6540a94a3bd4f",
        "files": [
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "core-common/src/main/java/org/apache/kylin/common/util/CheckUtil.java",
          "core-metadata/src/main/java/org/apache/kylin/metadata/model/PartitionDesc.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/ModelSchemaUpdateChecker.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/ModelService.java",
          "webapp/app/js/controllers/models.js"
        ],
        "message": "KYLIN-4420 Add model compatibility check to allow more compatible update\n\n(cherry picked from commit 6aeaf2335adc97138075d177f9a7dd50536bae4e)",
        "before_after_code_files": [
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "core-common/src/main/java/org/apache/kylin/common/util/CheckUtil.java||core-common/src/main/java/org/apache/kylin/common/util/CheckUtil.java",
          "core-metadata/src/main/java/org/apache/kylin/metadata/model/PartitionDesc.java||core-metadata/src/main/java/org/apache/kylin/metadata/model/PartitionDesc.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/ModelSchemaUpdateChecker.java||server-base/src/main/java/org/apache/kylin/rest/service/ModelSchemaUpdateChecker.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/ModelService.java||server-base/src/main/java/org/apache/kylin/rest/service/ModelService.java",
          "webapp/app/js/controllers/models.js||webapp/app/js/controllers/models.js"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ],
          "candidate": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ]
        }
      },
      "candidate_diff": {
        "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "553:         return getOptional(\"kylin.metadata.hbase-client-retries-number\", \"1\");",
          "554:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "556:     public boolean isModelSchemaUpdaterCheckerEnabled() {",
          "557:         return Boolean.parseBoolean(getOptional(\"kylin.metadata.model-schema-updater-checker-enabled\", \"false\"));",
          "558:     }",
          "",
          "---------------"
        ],
        "core-common/src/main/java/org/apache/kylin/common/util/CheckUtil.java||core-common/src/main/java/org/apache/kylin/common/util/CheckUtil.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/util/CheckUtil.java -> core-common/src/main/java/org/apache/kylin/common/util/CheckUtil.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "71:         return false;",
          "72:     }",
          "73: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "74:     public static boolean equals(String s1, String s2) {",
          "75:         if (s1 != null && s2 != null) {",
          "76:             return s1.trim().equalsIgnoreCase(s2.trim());",
          "77:         }",
          "78:         return s1 == null && s2 == null;",
          "79:     }",
          "81:     public static <T> boolean equals(T o1, T o2) {",
          "82:         if (o1 != null && o2 != null) {",
          "83:             return o1.equals(o2);",
          "84:         }",
          "85:         return o1 == null && o2 == null;",
          "86:     }",
          "",
          "---------------"
        ],
        "core-metadata/src/main/java/org/apache/kylin/metadata/model/PartitionDesc.java||core-metadata/src/main/java/org/apache/kylin/metadata/model/PartitionDesc.java": [
          "File: core-metadata/src/main/java/org/apache/kylin/metadata/model/PartitionDesc.java -> core-metadata/src/main/java/org/apache/kylin/metadata/model/PartitionDesc.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: import org.apache.kylin.shaded.com.google.common.base.Preconditions;",
          "26: import org.apache.commons.lang3.StringUtils;",
          "27: import org.apache.kylin.common.util.ClassUtil;",
          "28: import org.apache.kylin.common.util.DateFormat;",
          "29: import org.apache.kylin.metadata.datatype.DataType;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: import org.apache.kylin.common.util.CheckUtil;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "189:         return partitionTimeColumnRef;",
          "190:     }",
          "194:     public static interface IPartitionConditionBuilder {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "193:     public boolean equalsRaw(Object obj) {",
          "194:         if (this == obj)",
          "195:             return true;",
          "196:         if (obj == null)",
          "197:             return false;",
          "198:         if (getClass() != obj.getClass())",
          "199:             return false;",
          "200:         PartitionDesc other = (PartitionDesc) obj;",
          "202:         if (!this.partitionType.equals(other.getCubePartitionType()))",
          "203:             return false;",
          "204:         if (!this.partitionConditionBuilderClz.equals(other.partitionConditionBuilderClz))",
          "205:             return false;",
          "206:         if (!CheckUtil.equals(this.partitionDateColumn, other.getPartitionDateColumn()))",
          "207:             return false;",
          "208:         if (!CheckUtil.equals(this.partitionDateFormat, other.getPartitionDateFormat()))",
          "209:             return false;",
          "210:         if (!CheckUtil.equals(this.partitionTimeColumn, other.getPartitionTimeColumn()))",
          "211:             return false;",
          "212:         if (!CheckUtil.equals(this.partitionTimeFormat, other.getPartitionTimeFormat()))",
          "213:             return false;",
          "214:         return true;",
          "215:     }",
          "217:     @Override",
          "218:     public boolean equals(Object obj) {",
          "219:         if (this == obj)",
          "220:             return true;",
          "221:         if (obj == null)",
          "222:             return false;",
          "223:         if (getClass() != obj.getClass())",
          "224:             return false;",
          "225:         PartitionDesc other = (PartitionDesc) obj;",
          "227:         if (!this.partitionType.equals(other.getCubePartitionType()))",
          "228:             return false;",
          "229:         if (!CheckUtil.equals(this.partitionDateColumn, other.getPartitionDateColumn()))",
          "230:             return false;",
          "231:         if (!CheckUtil.equals(this.partitionDateFormat, other.getPartitionDateFormat()))",
          "232:             return false;",
          "233:         if (this.partitionDateColumn != null) {",
          "234:             if (!this.partitionConditionBuilder.getClass().equals(other.getPartitionConditionBuilder().getClass()))",
          "235:                 return false;",
          "236:             if (this.partitionConditionBuilder instanceof DefaultPartitionConditionBuilder) {",
          "237:                 if (!CheckUtil.equals(this.partitionTimeColumn, other.getPartitionTimeColumn())) {",
          "238:                     return false;",
          "239:                 }",
          "240:                 if (!CheckUtil.equals(this.partitionTimeFormat, other.getPartitionTimeFormat())) {",
          "241:                     return false;",
          "242:                 }",
          "243:             }",
          "244:         }",
          "246:         return true;",
          "247:     }",
          "249:     @Override",
          "250:     public int hashCode() {",
          "251:         final int prime = 31;",
          "252:         int result = 1;",
          "253:         result = prime * result + partitionType.hashCode();",
          "254:         result = prime * result + partitionConditionBuilderClz.hashCode();",
          "255:         result = prime * result + ((partitionDateColumn == null) ? 0 : partitionDateColumn.hashCode());",
          "256:         result = prime * result + ((partitionDateFormat == null) ? 0 : partitionDateFormat.hashCode());",
          "257:         if (partitionConditionBuilder instanceof DefaultPartitionConditionBuilder) {",
          "258:             result = prime * result + ((partitionTimeColumn == null) ? 0 : partitionTimeColumn.hashCode());",
          "259:             result = prime * result + ((partitionTimeFormat == null) ? 0 : partitionTimeFormat.hashCode());",
          "260:         }",
          "261:         return result;",
          "262:     }",
          "",
          "---------------"
        ],
        "server-base/src/main/java/org/apache/kylin/rest/service/ModelSchemaUpdateChecker.java||server-base/src/main/java/org/apache/kylin/rest/service/ModelSchemaUpdateChecker.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/service/ModelSchemaUpdateChecker.java -> server-base/src/main/java/org/apache/kylin/rest/service/ModelSchemaUpdateChecker.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.rest.service;",
          "21: import static com.google.common.base.Preconditions.checkNotNull;",
          "22: import static java.lang.String.format;",
          "23: import static org.apache.kylin.measure.topn.TopNMeasureType.FUNC_TOP_N;",
          "25: import java.util.List;",
          "26: import java.util.Locale;",
          "27: import java.util.Map;",
          "28: import java.util.Set;",
          "30: import org.apache.kylin.common.util.CheckUtil;",
          "31: import org.apache.kylin.cube.CubeInstance;",
          "32: import org.apache.kylin.cube.CubeManager;",
          "33: import org.apache.kylin.metadata.TableMetadataManager;",
          "34: import org.apache.kylin.metadata.model.DataModelDesc;",
          "35: import org.apache.kylin.metadata.model.DataModelManager;",
          "36: import org.apache.kylin.metadata.model.FunctionDesc;",
          "37: import org.apache.kylin.metadata.model.JoinTableDesc;",
          "38: import org.apache.kylin.metadata.model.MeasureDesc;",
          "39: import org.apache.kylin.metadata.model.ModelDimensionDesc;",
          "40: import org.apache.kylin.metadata.model.TblColRef;",
          "42: import com.google.common.collect.ImmutableList;",
          "43: import com.google.common.collect.Iterables;",
          "44: import com.google.common.collect.Lists;",
          "45: import com.google.common.collect.Maps;",
          "46: import com.google.common.collect.Sets;",
          "48: public class ModelSchemaUpdateChecker {",
          "50:     private final TableMetadataManager metadataManager;",
          "51:     private final CubeManager cubeManager;",
          "52:     private final DataModelManager dataModelManager;",
          "54:     static class CheckResult {",
          "55:         private final boolean valid;",
          "56:         private final String reason;",
          "58:         private CheckResult(boolean valid, String reason) {",
          "59:             this.valid = valid;",
          "60:             this.reason = reason;",
          "61:         }",
          "63:         void raiseExceptionWhenInvalid() {",
          "64:             if (!valid) {",
          "65:                 throw new RuntimeException(reason);",
          "66:             }",
          "67:         }",
          "69:         static CheckResult validOnFirstCreate(String modelName) {",
          "70:             return new CheckResult(true, format(Locale.ROOT, \"Model '%s' hasn't been created before\", modelName));",
          "71:         }",
          "73:         static CheckResult validOnCompatibleSchema(String modelName) {",
          "74:             return new CheckResult(true,",
          "75:                     format(Locale.ROOT, \"Table '%s' is compatible with all existing cubes\", modelName));",
          "76:         }",
          "78:         static CheckResult invalidOnIncompatibleSchema(String modelName, List<String> reasons) {",
          "79:             StringBuilder buf = new StringBuilder();",
          "80:             for (String reason : reasons) {",
          "81:                 buf.append(\"- \").append(reason).append(\"\\n\");",
          "82:             }",
          "84:             return new CheckResult(false,",
          "85:                     format(Locale.ROOT,",
          "86:                             \"Found %d issue(s) with '%s':%n%s Please disable and purge related cube(s) first\",",
          "87:                             reasons.size(), modelName, buf.toString()));",
          "88:         }",
          "89:     }",
          "91:     ModelSchemaUpdateChecker(TableMetadataManager metadataManager, CubeManager cubeManager,",
          "92:             DataModelManager dataModelManager) {",
          "93:         this.metadataManager = checkNotNull(metadataManager, \"metadataManager is null\");",
          "94:         this.cubeManager = checkNotNull(cubeManager, \"cubeManager is null\");",
          "95:         this.dataModelManager = checkNotNull(dataModelManager, \"dataModelManager is null\");",
          "96:     }",
          "98:     private List<CubeInstance> findCubeByModel(final String modelName) {",
          "99:         Iterable<CubeInstance> relatedCubes = Iterables.filter(cubeManager.listAllCubes(), cube -> {",
          "100:             if (cube == null || cube.allowBrokenDescriptor()) {",
          "101:                 return false;",
          "102:             }",
          "103:             DataModelDesc model = cube.getModel();",
          "104:             if (model == null)",
          "105:                 return false;",
          "106:             return model.getName().equals(modelName);",
          "107:         });",
          "109:         return ImmutableList.copyOf(relatedCubes);",
          "110:     }",
          "120:     private static void checkDataModelCompatible(DataModelDesc existing, DataModelDesc newModel, List<String> issues) {",
          "122:         if (!existing.getRootFactTableName().equalsIgnoreCase(newModel.getRootFactTableName())) {",
          "123:             issues.add(format(Locale.ROOT,",
          "124:                     \"The fact table %s used in existing model is not the same as the updated one %s\",",
          "125:                     existing.getRootFactTableName(), newModel.getRootFactTableName()));",
          "126:         }",
          "128:         Map<String, JoinTableDesc> existingLookupMap = Maps.newHashMap();",
          "129:         for (JoinTableDesc joinTableDesc : existing.getJoinTables()) {",
          "130:             existingLookupMap.put(joinTableDesc.getAlias(), joinTableDesc);",
          "131:         }",
          "132:         for (JoinTableDesc joinTableDesc : newModel.getJoinTables()) {",
          "133:             if (existingLookupMap.get(joinTableDesc.getAlias()) == null) {",
          "134:                 issues.add(format(Locale.ROOT, \"The join table %s does not existing in existing model\",",
          "135:                         joinTableDesc.getTable()));",
          "136:                 continue;",
          "137:             }",
          "138:             JoinTableDesc existingLookup = existingLookupMap.remove(joinTableDesc.getAlias());",
          "139:             if (!existingLookup.getTable().equals(joinTableDesc.getTable())) {",
          "140:                 issues.add(format(Locale.ROOT,",
          "141:                         \"The join table %s used in existing model is not the same as the updated one %s\",",
          "142:                         existingLookup.getTable(), joinTableDesc.getTable()));",
          "143:                 continue;",
          "144:             }",
          "145:             if (!existingLookup.getKind().equals(joinTableDesc.getKind())) {",
          "146:                 issues.add(format(Locale.ROOT,",
          "147:                         \"The TableKind %s in existing model is not the same as the updated one %s for table %s\",",
          "148:                         existingLookup.getKind(), joinTableDesc.getKind(), existingLookup.getTable()));",
          "149:                 continue;",
          "150:             }",
          "151:             if (!existingLookup.getJoin().equals(joinTableDesc.getJoin())) {",
          "152:                 issues.add(format(Locale.ROOT, \"The join %s is not the same as the existing one %s\",",
          "153:                         joinTableDesc.getJoin(), existingLookup.getJoin()));",
          "154:             }",
          "155:         }",
          "156:         if (existingLookupMap.size() > 0) {",
          "157:             issues.add(format(Locale.ROOT, \"Missing lookup tables %s\", existingLookupMap.keySet()));",
          "158:         }",
          "160:         if (!CheckUtil.equals(existing.getPartitionDesc(), newModel.getPartitionDesc())) {",
          "161:             issues.add(format(Locale.ROOT, \"The partition desc %s is not the same as the existing one %s\",",
          "162:                     newModel.getPartitionDesc(), existing.getPartitionDesc()));",
          "163:         }",
          "165:         if (!CheckUtil.equals(existing.getFilterCondition(), newModel.getFilterCondition())) {",
          "166:             issues.add(format(Locale.ROOT, \"The filter %s is not the same as the existing one %s\",",
          "167:                     newModel.getFilterCondition(), existing.getFilterCondition()));",
          "168:         }",
          "169:     }",
          "171:     public CheckResult allowEdit(DataModelDesc modelDesc, String prj) {",
          "173:         final String modelName = modelDesc.getName();",
          "175:         DataModelDesc existing = dataModelManager.getDataModelDesc(modelName);",
          "176:         if (existing == null) {",
          "177:             return CheckResult.validOnFirstCreate(modelName);",
          "178:         }",
          "179:         modelDesc.init(metadataManager.getConfig(), metadataManager.getAllTablesMap(prj));",
          "182:         List<CubeInstance> cubes = findCubeByModel(modelName);",
          "183:         if (cubes.size() <= 0) {",
          "184:             return CheckResult.validOnCompatibleSchema(modelName);",
          "185:         }",
          "187:         existing = cubes.get(0).getModel();",
          "188:         List<String> issues = Lists.newArrayList();",
          "190:         checkDataModelCompatible(existing, modelDesc, issues);",
          "191:         if (!issues.isEmpty()) {",
          "192:             return CheckResult.invalidOnIncompatibleSchema(modelName, issues);",
          "193:         }",
          "196:         Set<String> dimensionColumns = Sets.newHashSet();",
          "197:         for (ModelDimensionDesc modelDimensionDesc : modelDesc.getDimensions()) {",
          "198:             for (String columnName : modelDimensionDesc.getColumns()) {",
          "199:                 dimensionColumns.add(modelDimensionDesc.getTable() + \".\" + columnName);",
          "200:             }",
          "201:         }",
          "203:         for (JoinTableDesc joinTableDesc : modelDesc.getJoinTables()) {",
          "204:             List<TblColRef> keyCols = Lists.newArrayList(joinTableDesc.getJoin().getForeignKeyColumns());",
          "205:             keyCols.addAll(Lists.newArrayList(joinTableDesc.getJoin().getPrimaryKeyColumns()));",
          "206:             dimensionColumns.addAll(Lists.transform(keyCols, entry -> entry.getIdentity()));",
          "207:         }",
          "208:         Set<String> measureColumns = Sets.newHashSet(modelDesc.getMetrics());",
          "209:         for (CubeInstance cube : cubes) {",
          "211:             List<String> cubeDimensionColumns = Lists.newLinkedList();",
          "212:             for (TblColRef entry : cube.getAllDimensions()) {",
          "213:                 cubeDimensionColumns.add(entry.getIdentity());",
          "214:             }",
          "215:             for (MeasureDesc input : cube.getMeasures()) {",
          "216:                 FunctionDesc funcDesc = input.getFunction();",
          "217:                 if (FUNC_TOP_N.equalsIgnoreCase(funcDesc.getExpression())) {",
          "218:                     List<TblColRef> ret = funcDesc.getParameter().getColRefs();",
          "219:                     cubeDimensionColumns",
          "220:                             .addAll(Lists.transform(ret.subList(1, ret.size()), entry -> entry.getIdentity()));",
          "221:                 }",
          "222:             }",
          "223:             if (!dimensionColumns.containsAll(cubeDimensionColumns)) {",
          "224:                 cubeDimensionColumns.removeAll(dimensionColumns);",
          "225:                 issues.add(format(Locale.ROOT, \"Missing some dimension columns %s for cube %s\", cubeDimensionColumns,",
          "226:                         cube.getName()));",
          "227:             }",
          "229:             List<List<TblColRef>> cubeMeasureTblColRefLists = Lists.transform(cube.getMeasures(), entry -> {",
          "230:                 FunctionDesc funcDesc = entry.getFunction();",
          "231:                 List<TblColRef> ret = funcDesc.getParameter().getColRefs();",
          "232:                 if (FUNC_TOP_N.equalsIgnoreCase(funcDesc.getExpression())) {",
          "233:                     return Lists.newArrayList(ret.get(0));",
          "234:                 } else {",
          "235:                     return funcDesc.getParameter().getColRefs();",
          "236:                 }",
          "237:             });",
          "238:             List<String> cubeMeasureColumns = Lists.transform(",
          "239:                     Lists.newArrayList(Iterables.concat(cubeMeasureTblColRefLists)), entry -> entry.getIdentity());",
          "240:             if (!measureColumns.containsAll(cubeMeasureColumns)) {",
          "241:                 cubeMeasureColumns.removeAll(measureColumns);",
          "242:                 issues.add(format(Locale.ROOT, \"Missing some measure columns %s for cube %s\", cubeMeasureColumns,",
          "243:                         cube.getName()));",
          "244:             }",
          "245:         }",
          "247:         if (issues.isEmpty()) {",
          "248:             return CheckResult.validOnCompatibleSchema(modelName);",
          "249:         }",
          "250:         return CheckResult.invalidOnIncompatibleSchema(modelName, issues);",
          "251:     }",
          "252: }",
          "",
          "---------------"
        ],
        "server-base/src/main/java/org/apache/kylin/rest/service/ModelService.java||server-base/src/main/java/org/apache/kylin/rest/service/ModelService.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/service/ModelService.java -> server-base/src/main/java/org/apache/kylin/rest/service/ModelService.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "41: import org.apache.kylin.metadata.model.TableDesc;",
          "42: import org.apache.kylin.metadata.model.TblColRef;",
          "43: import org.apache.kylin.metadata.util.ModelUtil;",
          "44: import org.apache.kylin.rest.exception.BadRequestException;",
          "45: import org.apache.kylin.rest.exception.ForbiddenException;",
          "46: import org.apache.kylin.rest.msg.Message;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "44: import org.apache.kylin.metadata.project.ProjectInstance;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "148:     public DataModelDesc updateModelAndDesc(String project, DataModelDesc desc) throws IOException {",
          "149:         aclEvaluate.checkProjectWritePermission(project);",
          "150:         validateModel(project, desc);",
          "151:         getDataModelManager().updateDataModelDesc(desc);",
          "152:         return desc;",
          "153:     }",
          "155:     public void validateModel(String project, DataModelDesc desc) throws IllegalArgumentException {",
          "156:         String factTableName = desc.getRootFactTableName();",
          "157:         TableDesc tableDesc = getTableManager().getTableDesc(factTableName, project);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "152:         checkModelCompatible(project, desc);",
          "157:     public void checkModelCompatible(String project, DataModelDesc dataModalDesc) {",
          "158:         ProjectInstance prjInstance = getProjectManager().getProject(project);",
          "159:         if (prjInstance == null) {",
          "160:             throw new BadRequestException(\"Project \" + project + \" does not exist\");",
          "161:         }",
          "162:         if (!prjInstance.getConfig().isModelSchemaUpdaterCheckerEnabled()) {",
          "163:             logger.info(\"Skip the check for model schema update\");",
          "164:             return;",
          "165:         }",
          "166:         ModelSchemaUpdateChecker checker = new ModelSchemaUpdateChecker(getTableManager(), getCubeManager(),",
          "167:                 getDataModelManager());",
          "168:         ModelSchemaUpdateChecker.CheckResult result = checker.allowEdit(dataModalDesc, project);",
          "169:         result.raiseExceptionWhenInvalid();",
          "170:     }",
          "",
          "---------------"
        ],
        "webapp/app/js/controllers/models.js||webapp/app/js/controllers/models.js": [
          "File: webapp/app/js/controllers/models.js -> webapp/app/js/controllers/models.js",
          "--- Hunk 1 ---",
          "[Context before]",
          "145:         })",
          "146:       }",
          "154:       } else {",
          "156:       }",
          "157:     })",
          "",
          "[Removed Lines]",
          "148:       if (modelstate==false){",
          "149:        if (isEditJson) {",
          "150:         $location.path(\"/models/edit/\" + model.name + \"/descriptionjson\");",
          "151:        } else {",
          "152:         $location.path(\"/models/edit/\" + model.name);",
          "153:        }",
          "155:         SweetAlert.swal('Sorry','This model is still used by '+ cubename.join(','));",
          "",
          "[Added Lines]",
          "148:       if (isEditJson) {",
          "149:         $location.path(\"/models/edit/\" + model.name + \"/descriptionjson\");",
          "151:         $location.path(\"/models/edit/\" + model.name);",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "9e33d93b2dc668c1309a1f4c1afb8f8f5a946c29",
      "candidate_info": {
        "commit_hash": "9e33d93b2dc668c1309a1f4c1afb8f8f5a946c29",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/9e33d93b2dc668c1309a1f4c1afb8f8f5a946c29",
        "files": [
          "build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java",
          "build-engine/src/main/java/org/apache/kylin/engine/mr/common/StatisticsDecisionUtil.java",
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala"
        ],
        "message": "KYLIN-4818 Persist metadata in SparkExecutable",
        "before_after_code_files": [
          "build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java||build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java",
          "build-engine/src/main/java/org/apache/kylin/engine/mr/common/StatisticsDecisionUtil.java||build-engine/src/main/java/org/apache/kylin/engine/mr/common/StatisticsDecisionUtil.java",
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ],
          "candidate": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ]
        }
      },
      "candidate_diff": {
        "build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java||build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java": [
          "File: build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java -> build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "100:         ResourceStore store = ResourceStore.getStore(kylinConfig);",
          "101:         String statsKey = cubeSegment.getStatisticsResourcePath();",
          "102:         RawResource resource = store.getResource(statsKey);",
          "106:         File tmpSeqFile = writeTmpSeqFile(resource.content());",
          "107:         Path path = new Path(HadoopUtil.fixWindowsPath(\"file://\" + tmpSeqFile.getAbsolutePath()));",
          "108:         logger.info(\"Reading statistics from {}\", path);",
          "",
          "[Removed Lines]",
          "103:         if (resource == null)",
          "104:             throw new IllegalStateException(\"Missing resource at \" + statsKey);",
          "",
          "[Added Lines]",
          "103:         if (resource == null) {",
          "105:             logger.warn(\"{} is not exists.\", statsKey);",
          "106:         }",
          "",
          "---------------"
        ],
        "build-engine/src/main/java/org/apache/kylin/engine/mr/common/StatisticsDecisionUtil.java||build-engine/src/main/java/org/apache/kylin/engine/mr/common/StatisticsDecisionUtil.java": [
          "File: build-engine/src/main/java/org/apache/kylin/engine/mr/common/StatisticsDecisionUtil.java -> build-engine/src/main/java/org/apache/kylin/engine/mr/common/StatisticsDecisionUtil.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: package org.apache.kylin.engine.mr.common;",
          "21: import java.io.IOException;",
          "22: import java.util.List;",
          "23: import java.util.Map;",
          "24: import java.util.Random;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: import java.util.HashMap;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "93:     }",
          "97:         if (isAbleToOptimizeCubingPlan(segment)) {",
          "98:             logger.info(\"It's able to trigger cuboid planner algorithm.\");",
          "99:         } else {",
          "101:         }",
          "103:         Map<Long, Long> recommendCuboidsWithStats = CuboidRecommenderUtil.getRecommendCuboidList(segment);",
          "104:         if (recommendCuboidsWithStats == null || recommendCuboidsWithStats.isEmpty()) {",
          "106:         }",
          "108:         CubeInstance cube = segment.getCubeInstance();",
          "109:         CubeUpdate update = new CubeUpdate(cube.latestCopyForWrite());",
          "110:         update.setCuboids(recommendCuboidsWithStats);",
          "111:         CubeManager.getInstance(cube.getConfig()).updateCube(update);",
          "112:     }",
          "114:     public static boolean isAbleToOptimizeCubingPlan(CubeSegment segment) {",
          "",
          "[Removed Lines]",
          "96:     public static void optimizeCubingPlan(CubeSegment segment) throws IOException {",
          "100:             return;",
          "105:             return;",
          "",
          "[Added Lines]",
          "97:     public static Map<Long, Long> optimizeCubingPlan(CubeSegment segment) throws IOException {",
          "101:             return new HashMap<>();",
          "106:             return new HashMap<>();",
          "113:         return recommendCuboidsWithStats;",
          "",
          "---------------"
        ],
        "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "2963:         return Integer.parseInt(this.getOptional(\"kylin.canary.sparder-context-period-min\", \"3\"));",
          "2964:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2969:     public boolean isSegmentStatisticsEnabled() {",
          "2970:         return Boolean.parseBoolean(this.getOptional(\"kylin.engine.segment-statistics-enabled\", \"false\"));",
          "2971:     }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/utils/UpdateMetadataUtil.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: import com.google.common.collect.Sets;",
          "30: import org.apache.commons.lang3.StringUtils;",
          "32: import org.apache.kylin.common.KylinConfig;",
          "33: import org.apache.kylin.cube.CubeInstance;",
          "34: import org.apache.kylin.cube.CubeManager;",
          "35: import org.apache.kylin.cube.CubeSegment;",
          "36: import org.apache.kylin.cube.CubeUpdate;",
          "37: import org.apache.kylin.cube.model.CubeBuildTypeEnum;",
          "38: import org.apache.kylin.engine.mr.steps.CubingExecutableUtil;",
          "39: import org.apache.kylin.engine.spark.job.NSparkExecutable;",
          "40: import org.apache.kylin.metadata.MetadataConstants;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32: import org.apache.hadoop.fs.FSDataInputStream;",
          "33: import org.apache.hadoop.fs.FileSystem;",
          "34: import org.apache.hadoop.fs.Path;",
          "36: import org.apache.kylin.common.persistence.ResourceStore;",
          "37: import org.apache.kylin.common.util.HadoopUtil;",
          "43: import org.apache.kylin.engine.mr.JobBuilderSupport;",
          "44: import org.apache.kylin.engine.mr.common.BatchConstants;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "43: import org.slf4j.Logger;",
          "44: import org.slf4j.LoggerFactory;",
          "46: public class UpdateMetadataUtil {",
          "48:     protected static final Logger logger = LoggerFactory.getLogger(UpdateMetadataUtil.class);",
          "50:     public static void syncLocalMetadataToRemote(KylinConfig config,",
          "52:         String cubeId = nsparkExecutable.getParam(MetadataConstants.P_CUBE_ID);",
          "53:         Set<String> segmentIds = Sets.newHashSet(StringUtils.split(",
          "54:                 nsparkExecutable.getParam(CubingExecutableUtil.SEGMENT_ID), \" \"));",
          "",
          "[Removed Lines]",
          "51:                                          NSparkExecutable nsparkExecutable) throws IOException {",
          "",
          "[Added Lines]",
          "53: import static org.apache.kylin.engine.mr.common.BatchConstants.CFG_OUTPUT_STATISTICS;",
          "60:                                                  NSparkExecutable nsparkExecutable) throws IOException {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "63:         KylinConfig kylinDistConfig = MetaDumpUtil.loadKylinConfigFromHdfs(remoteResourceStore);",
          "64:         CubeInstance distCube = CubeManager.getInstance(kylinDistConfig).getCubeByUuid(cubeId);",
          "69:             throw new IllegalStateException(",
          "70:                     String.format(Locale.ROOT, \"For cube %s, segment %s is expected but not in the tobe %s\",",
          "73:         CubeUpdate update = new CubeUpdate(currentInstanceCopy);",
          "74:         List<CubeSegment> toRemoveSegs = Lists.newArrayList();",
          "76:         if (String.valueOf(CubeBuildTypeEnum.MERGE).equals(jobType)) {",
          "79:             for (Map.Entry<String, String> entry :",
          "80:                     currentInstanceCopy.getLatestReadySegment().getSnapshots().entrySet()) {",
          "82:             }",
          "83:         } else {",
          "85:             for (CubeSegment segment : currentInstanceCopy.getSegments()) {",
          "86:                 if (!tobeSegments.contains(segment))",
          "87:                     toRemoveSegs.add(segment);",
          "",
          "[Removed Lines]",
          "65:         CubeSegment toUpdateSegs = distCube.getSegmentById(segmentId);",
          "67:         List<CubeSegment> tobeSegments = currentInstanceCopy.calculateToBeSegments(toUpdateSegs);",
          "68:         if (!tobeSegments.contains(toUpdateSegs))",
          "71:                             currentInstanceCopy.toString(), toUpdateSegs.toString(), tobeSegments.toString()));",
          "77:             toUpdateSegs.getSnapshots().clear();",
          "81:                 toUpdateSegs.putSnapshotResPath(entry.getKey(), entry.getValue());",
          "84:             toUpdateSegs.setStatus(SegmentStatusEnum.READY);",
          "",
          "[Added Lines]",
          "74:         CubeSegment toUpdateSeg = distCube.getSegmentById(segmentId);",
          "76:         List<CubeSegment> tobeSegments = currentInstanceCopy.calculateToBeSegments(toUpdateSeg);",
          "77:         if (!tobeSegments.contains(toUpdateSeg))",
          "80:                             currentInstanceCopy.toString(), toUpdateSeg.toString(), tobeSegments.toString()));",
          "82:         String resKey = toUpdateSeg.getStatisticsResourcePath();",
          "83:         String jobWorkingDirPath = JobBuilderSupport.getJobWorkingDir(currentInstanceCopy.getConfig().getHdfsWorkingDirectory(), nsparkExecutable.getParam(MetadataConstants.P_JOB_ID));",
          "84:         Path statisticsFile = new Path(jobWorkingDirPath + \"/\" + segmentId + \"/\" + CFG_OUTPUT_STATISTICS + \"/\" + BatchConstants.CFG_STATISTICS_CUBOID_ESTIMATION_FILENAME);",
          "85:         FileSystem fs = HadoopUtil.getWorkingFileSystem();",
          "86:         if (fs.exists(statisticsFile)) {",
          "87:             FSDataInputStream is = fs.open(statisticsFile);",
          "88:             ResourceStore.getStore(config).putResource(resKey, is, System.currentTimeMillis());",
          "89:         }",
          "92:         update.setCuboids(distCube.getCuboids());",
          "96:             toUpdateSeg.getSnapshots().clear();",
          "100:                 toUpdateSeg.putSnapshotResPath(entry.getKey(), entry.getValue());",
          "103:             toUpdateSeg.setStatus(SegmentStatusEnum.READY);",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "92:             }",
          "93:         }",
          "98:         update.setToRemoveSegs(toRemoveSegs.toArray(new CubeSegment[0]))",
          "100:         cubeManager.updateCube(update);",
          "101:     }",
          "",
          "[Removed Lines]",
          "95:         logger.info(\"Promoting cube {}, new segment {}, to remove segments {}\", currentInstanceCopy, toUpdateSegs, toRemoveSegs);",
          "97:         toUpdateSegs.setLastBuildTime(System.currentTimeMillis());",
          "99:                 .setToUpdateSegs(toUpdateSegs);",
          "",
          "[Added Lines]",
          "114:         logger.info(\"Promoting cube {}, new segment {}, to remove segments {}\", currentInstanceCopy, toUpdateSeg, toRemoveSegs);",
          "116:         toUpdateSeg.setLastBuildTime(System.currentTimeMillis());",
          "118:                 .setToUpdateSegs(toUpdateSeg);",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "94:     private BuildLayoutWithUpdate buildLayoutWithUpdate;",
          "95:     private Map<Long, Short> cuboidShardNum = Maps.newConcurrentMap();",
          "96:     private Map<Long, Long> cuboidsRowCount = Maps.newConcurrentMap();",
          "98:     public static void main(String[] args) {",
          "99:         CubeBuildJob cubeBuildJob = new CubeBuildJob();",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "97:     private Map<Long, Long> recommendCuboidMap = new HashMap<>();",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "119:         SpanningTree spanningTree ;",
          "120:         ParentSourceChooser sourceChooser;",
          "124:         if (needStatistics) {",
          "",
          "[Removed Lines]",
          "122:         boolean needStatistics = StatisticsDecisionUtil.isAbleToOptimizeCubingPlan(newSegment); // Cuboid Statistics is served for Cube Planner Phase One",
          "",
          "[Added Lines]",
          "124:         boolean needStatistics = StatisticsDecisionUtil.isAbleToOptimizeCubingPlan(newSegment)",
          "125:                 || config.isSegmentStatisticsEnabled();",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "138:             String jobWorkingDirPath = JobBuilderSupport.getJobWorkingDir(cubeInstance.getConfig().getHdfsWorkingDirectory(), jobId);",
          "141:             FileSystem fs = HadoopUtil.getWorkingFileSystem();",
          "142:             ResourceStore rs = ResourceStore.getStore(config);",
          "145:             FSDataInputStream is = fs.open(statisticsFile);",
          "152:         }",
          "154:         buildLayoutWithUpdate = new BuildLayoutWithUpdate();",
          "",
          "[Removed Lines]",
          "139:             CubeStatsWriter.writeCuboidStatistics(HadoopUtil.getCurrentConfiguration(), new Path(jobWorkingDirPath + \"/\" + firstSegmentId + \"/\" + CFG_OUTPUT_STATISTICS), hllMap, 1);",
          "143:             String resPath = newSegment.getStatisticsResourcePath();",
          "144:             Path statisticsFile = new Path(jobWorkingDirPath + \"/\" + firstSegmentId + \"/\" + CFG_OUTPUT_STATISTICS + \"/\" + BatchConstants.CFG_STATISTICS_CUBOID_ESTIMATION_FILENAME);",
          "146:             rs.putResource(resPath, is, System.currentTimeMillis());",
          "147:             logger.info(\"{} stats saved to resource {}\", newSegment, resPath);",
          "150:             logger.info(\"Trigger cube planner phase one .\");",
          "151:             StatisticsDecisionUtil.optimizeCubingPlan(newSegment);",
          "",
          "[Added Lines]",
          "142:             Path statisticsDir = new Path(jobWorkingDirPath + \"/\" + firstSegmentId + \"/\" + CFG_OUTPUT_STATISTICS);",
          "143:             CubeStatsWriter.writeCuboidStatistics(HadoopUtil.getCurrentConfiguration(), statisticsDir, hllMap, 1);",
          "147:             String metaKey = newSegment.getStatisticsResourcePath();",
          "148:             Path statisticsFile = new Path(statisticsDir, BatchConstants.CFG_STATISTICS_CUBOID_ESTIMATION_FILENAME);",
          "150:             rs.putResource(metaKey, is, System.currentTimeMillis()); // write to Job-Local metastore",
          "151:             logger.info(\"{}'s stats saved to resource key({}) with path({})\", newSegment, metaKey, statisticsFile);",
          "154:             recommendCuboidMap = StatisticsDecisionUtil.optimizeCubingPlan(newSegment);",
          "155:             if (!recommendCuboidMap.isEmpty())",
          "156:                 logger.info(\"Triggered cube planner phase one .\");",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "161:                 seg = ManagerHub.getSegmentInfo(config, cubeName, segId);",
          "162:                 spanningTree = new ForestSpanningTree(",
          "163:                         JavaConversions.asJavaCollection(seg.toBuildLayouts()));",
          "165:                         seg.toBuildLayouts().size(), seg.name());",
          "166:                 for (LayoutEntity cuboid : JavaConversions.asJavaCollection(seg.toBuildLayouts())) {",
          "167:                     logger.debug(\"Cuboid {} has row keys: {}\", cuboid.getId(),",
          "",
          "[Removed Lines]",
          "164:                 logger.debug(\"There are {} cuboids to be built in segment {}.\",",
          "",
          "[Added Lines]",
          "169:                 logger.info(\"There are {} cuboids to be built in segment {}.\",",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "192:                 assert buildFromFlatTable != null;",
          "193:                 updateSegmentInfo(getParam(MetadataConstants.P_CUBE_ID), seg, buildFromFlatTable.getFlatTableDS().count());",
          "194:             }",
          "197:         } finally {",
          "198:             FileSystem fs = HadoopUtil.getWorkingFileSystem();",
          "199:             for (String viewPath : persistedViewFactTable) {",
          "",
          "[Removed Lines]",
          "195:             updateSegmentSourceBytesSize(getParam(MetadataConstants.P_CUBE_ID),",
          "196:                     ResourceDetectUtils.getSegmentSourceSize(shareDir));",
          "",
          "[Added Lines]",
          "200:             updateCubeAndSegmentMeta(getParam(MetadataConstants.P_CUBE_ID),",
          "201:                     ResourceDetectUtils.getSegmentSourceSize(shareDir), recommendCuboidMap);",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "218:         segment.setSizeKB(segmentInfo.getAllLayoutSize() / 1024);",
          "219:         List<String> cuboidStatics = new LinkedList<>();",
          "222:         for (LayoutEntity layoutEntity : segmentInfo.getAllLayoutJava()) {",
          "224:         }",
          "233:         try {",
          "234:             cuboidStatRdd.saveAsTextFile(path);",
          "235:         } catch (Exception e) {",
          "237:         }",
          "239:         segment.setLastBuildTime(System.currentTimeMillis());",
          "",
          "[Removed Lines]",
          "221:         String template = \"{\\\"cuboid\\\":%d, \\\"rows\\\": %d, \\\"size\\\": %d}\";",
          "223:             cuboidStatics.add(String.format(Locale.getDefault(), template, layoutEntity.getId(), layoutEntity.getRows(), layoutEntity.getByteSize()));",
          "226:         JavaSparkContext jsc = JavaSparkContext.fromSparkContext(ss.sparkContext());",
          "227:         JavaRDD<String> cuboidStatRdd = jsc.parallelize(cuboidStatics);",
          "228:         for (String cuboid : cuboidStatics) {",
          "229:             logger.info(\"Statistics \\t: {}\", cuboid);",
          "230:         }",
          "231:         String path = config.getHdfsWorkingDirectory() + segment.getPreciseStatisticsResourcePath();",
          "232:         logger.info(\"Saving {} {}\", path, segmentInfo);",
          "236:             logger.error(\"Error\", e);",
          "",
          "[Added Lines]",
          "226:         String template = \"{\\\"cuboid\\\":%d, \\\"rows\\\": %d, \\\"size\\\": %d \\\"deviation\\\": %7f}\";",
          "228:             double deviation = 0.0d;",
          "229:             if (layoutEntity.getRows() > 0 && recommendCuboidMap != null && !recommendCuboidMap.isEmpty()) {",
          "230:                 long diff = (layoutEntity.getRows() - recommendCuboidMap.get(layoutEntity.getId()));",
          "231:                 deviation = diff / (layoutEntity.getRows() + 0.0d);",
          "232:             }",
          "233:             cuboidStatics.add(String.format(Locale.getDefault(), template, layoutEntity.getId(),",
          "234:                     layoutEntity.getRows(), layoutEntity.getByteSize(), deviation));",
          "238:             JavaSparkContext jsc = JavaSparkContext.fromSparkContext(ss.sparkContext());",
          "239:             JavaRDD<String> cuboidStatRdd = jsc.parallelize(cuboidStatics, 1);",
          "240:             for (String cuboid : cuboidStatics) {",
          "241:                 logger.info(\"Statistics \\t: {}\", cuboid);",
          "242:             }",
          "243:             String path = config.getHdfsWorkingDirectory() + segment.getPreciseStatisticsResourcePath();",
          "244:             logger.info(\"Saving {} {}\", path, segmentInfo);",
          "247:             logger.error(\"Write metrics failed.\", e);",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "256:         }",
          "257:     }",
          "261:         CubeInstance cubeInstance = cubeManager.getCubeByUuid(cubeId);",
          "262:         CubeInstance cubeCopy = cubeInstance.latestCopyForWrite();",
          "263:         CubeUpdate update = new CubeUpdate(cubeCopy);",
          "264:         List<CubeSegment> cubeSegments = Lists.newArrayList();",
          "265:         for (Map.Entry<String, Object> entry : toUpdateSegmentSourceSize.entrySet()) {",
          "266:             CubeSegment segment = cubeCopy.getSegmentById(entry.getKey());",
          "268:                 segment.setInputRecordsSize((Long) entry.getValue());",
          "269:                 segment.setLastBuildTime(System.currentTimeMillis());",
          "270:                 cubeSegments.add(segment);",
          "",
          "[Removed Lines]",
          "259:     private void updateSegmentSourceBytesSize(String cubeId, Map<String, Object> toUpdateSegmentSourceSize)",
          "260:             throws IOException {",
          "267:             if (segment.getInputRecords() > 0l) {",
          "",
          "[Added Lines]",
          "270:     private void updateCubeAndSegmentMeta(String cubeId, Map<String, Object> toUpdateSegmentSourceSize,",
          "271:                                           Map<Long, Long> recommendCuboidMap) throws IOException {",
          "276:         if (recommendCuboidMap != null && !recommendCuboidMap.isEmpty())",
          "277:             update.setCuboids(recommendCuboidMap);",
          "282:             if (segment.getInputRecords() > 0L) {",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "88:         logInfo(\"Sampling start ...\")",
          "89:         val coreDs = flatTableSource.getFlatTableDS.select(rowKeyColumns.head, rowKeyColumns.tail: _*)",
          "90:         aggInfo = CuboidStatisticsJob.statistics(coreDs, seg)",
          "92:         val statisticsStr = aggInfo.sortBy(x => x._1).map(x => x._1 + \":\" + x._2.cuboid.counter.getCountEstimate).mkString(\", \")",
          "93:         logInfo(\"Cuboid Statistics results : \\t\" + statisticsStr)",
          "94:       } else {",
          "",
          "[Removed Lines]",
          "91:         logInfo(\"Sampling finished and cost \" + (System.currentTimeMillis() - startMs) + \" s .\")",
          "",
          "[Added Lines]",
          "91:         logInfo(\"Sampling finished and cost \" + (System.currentTimeMillis() - startMs)/1000 + \" s .\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b4c857eda293a9a71e4f3e0841e62ef49cbe66c0",
      "candidate_info": {
        "commit_hash": "b4c857eda293a9a71e4f3e0841e62ef49cbe66c0",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/b4c857eda293a9a71e4f3e0841e62ef49cbe66c0",
        "files": [
          "docker/dockerfile/standalone/entrypoint.sh"
        ],
        "message": "KYLIN-5083, check hdfs service usability for kylin4 on docker",
        "before_after_code_files": [
          "docker/dockerfile/standalone/entrypoint.sh||docker/dockerfile/standalone/entrypoint.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "docker/dockerfile/standalone/entrypoint.sh||docker/dockerfile/standalone/entrypoint.sh": [
          "File: docker/dockerfile/standalone/entrypoint.sh -> docker/dockerfile/standalone/entrypoint.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "63: # check hive usability first, this operation will insert one version record into VERSION table",
          "64: $KYLIN_HOME/bin/check-hive-usability.sh > ${KYLIN_HOME}/logs/kylin-verbose.log 2>&1",
          "66: if [ ! -f \"/home/admin/first_run\" ]",
          "67: then",
          "68:     hdfs dfs -mkdir -p /kylin4/spark-history",
          "69:     hdfs dfs -mkdir -p /spark2_jars",
          "70:     hdfs dfs -put -f $SPARK_HOME/jars/* hdfs://localhost:9000/spark2_jars/",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "66: function check_hdfs_usability() {",
          "67:     echo \"Checking HDFS's service...\"",
          "68:     started_hdfs=",
          "69:     ((time_left = 60))",
          "70:     while ((time_left > 0)); do",
          "71:         hdfs dfs -test -d /tmp",
          "72:         started_hdfs=$?",
          "73:         if [[ $started_hdfs -eq 0 ]]; then",
          "74:             break",
          "75:         fi",
          "76:         sleep 5",
          "77:         ((timeLeft -= 5))",
          "78:     done",
          "79:     if [[ $started_hdfs -eq 0 ]]; then",
          "80:         echo \"HDFS's service started...\"",
          "81:     else",
          "82:         echo \"ERROR: Check HDFS's service failed, please check the status of your cluster\"",
          "83:     fi",
          "84: }",
          "88:     # check hdfs usability first if hdfs service was not started normally",
          "89:     check_hdfs_usability",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "68dd4073bf120dcb9ec47b8e7a6932c14be36f36",
      "candidate_info": {
        "commit_hash": "68dd4073bf120dcb9ec47b8e7a6932c14be36f36",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/68dd4073bf120dcb9ec47b8e7a6932c14be36f36",
        "files": [
          "kylin-it/src/test/resources/query/sql_casewhen/query58.sql"
        ],
        "message": "fix IT",
        "before_after_code_files": [
          "kylin-it/src/test/resources/query/sql_casewhen/query58.sql||kylin-it/src/test/resources/query/sql_casewhen/query58.sql"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "kylin-it/src/test/resources/query/sql_casewhen/query58.sql||kylin-it/src/test/resources/query/sql_casewhen/query58.sql": [
          "File: kylin-it/src/test/resources/query/sql_casewhen/query58.sql -> kylin-it/src/test/resources/query/sql_casewhen/query58.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: SELECT (CASE WHEN (\"TEST_KYLIN_FACT\".\"LSTG_FORMAT_NAME\" = 'Auction') THEN 'Auction2' ELSE 'Auction1' END) AS \"LSTG_FORMAT_NAME__group_\",",
          "20:   SUM(\"TEST_KYLIN_FACT\".\"PRICE\") AS \"sum_PRICE_ok\"",
          "21: FROM \"TEST_KYLIN_FACT\" \"TEST_KYLIN_FACT\"",
          "",
          "[Removed Lines]",
          "22: GROUP BY (CASE WHEN (\"TEST_KYLIN_FACT\".\"LSTG_FORMAT_NAME\" =  'Auction') THEN 'Auction2' ELSE 'Auction1' END)",
          "",
          "[Added Lines]",
          "23: ;{\"scanRowCount\":300,\"scanBytes\":190822,\"scanFiles\":1,\"cuboidId\":14336}",
          "",
          "---------------"
        ]
      }
    }
  ]
}