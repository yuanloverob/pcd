{
  "cve_id": "CVE-2023-42780",
  "cve_desc": "Apache Airflow, versions prior to 2.7.2, contains a security vulnerability that allows authenticated users of Airflow to list warnings for all DAGs, even if the user had no permission to see those DAGs. It would reveal the dag_ids and the stack-traces of import errors for those DAGs with import errors.\nUsers of Apache Airflow are advised to upgrade to version 2.7.2 or newer to mitigate the risk associated with this vulnerability.\n\n",
  "repo": "apache/airflow",
  "patch_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
  "patch_info": {
    "commit_hash": "cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/cf4eb3fb9b5cf4a8369b890e39523d4c05eed161",
    "files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ],
    "message": "Fix dag warning endpoint permissions (#34355)\n\n* Fix dag warning endpoint permissions\n\n* update the query to have an accurate result for total entries and pagination\n\n* add unit tests\n\n* Update test_dag_warning_endpoint.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 3570bbfbea69e2965f91b9964ce28bc268c68129)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_warning_endpoint.py||airflow/api_connexion/endpoints/dag_warning_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_warning_endpoint.py -> airflow/api_connexion/endpoints/dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "16: # under the License.",
      "17: from __future__ import annotations",
      "19: from sqlalchemy import select",
      "20: from sqlalchemy.orm import Session",
      "22: from airflow.api_connexion import security",
      "23: from airflow.api_connexion.parameters import apply_sorting, check_limit, format_parameters",
      "24: from airflow.api_connexion.schemas.dag_warning_schema import (",
      "25:     DagWarningCollection,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "19: from flask import g",
      "24: from airflow.api_connexion.exceptions import PermissionDenied",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "28: from airflow.api_connexion.types import APIResponse",
      "29: from airflow.models.dagwarning import DagWarning as DagWarningModel",
      "30: from airflow.security import permissions",
      "31: from airflow.utils.db import get_query_count",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from airflow.utils.airflow_flask_app import get_airflow_app",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "52:     allowed_filter_attrs = [\"dag_id\", \"warning_type\", \"message\", \"timestamp\"]",
      "53:     query = select(DagWarningModel)",
      "54:     if dag_id:",
      "55:         query = query.where(DagWarningModel.dag_id == dag_id)",
      "56:     if warning_type:",
      "57:         query = query.where(DagWarningModel.warning_type == warning_type)",
      "58:     total_entries = get_query_count(query, session=session)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "58:         if not get_airflow_app().appbuilder.sm.can_read_dag(dag_id, g.user):",
      "59:             raise PermissionDenied(detail=f\"User not allowed to access this DAG: {dag_id}\")",
      "61:     else:",
      "62:         readable_dags = get_airflow_app().appbuilder.sm.get_accessible_dag_ids(g.user)",
      "63:         query = query.where(DagWarningModel.dag_id.in_(readable_dags))",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_warning_endpoint.py||tests/api_connexion/endpoints/test_dag_warning_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_warning_endpoint.py -> tests/api_connexion/endpoints/test_dag_warning_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35:         app,  # type:ignore",
      "36:         username=\"test\",",
      "37:         role_name=\"Test\",",
      "39:     )",
      "40:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "42:     yield minimal_app_for_api",
      "44:     delete_user(app, username=\"test\")  # type: ignore",
      "45:     delete_user(app, username=\"test_no_permissions\")  # type: ignore",
      "48: class TestBaseDagWarning:",
      "",
      "[Removed Lines]",
      "38:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING)],  # type: ignore",
      "",
      "[Added Lines]",
      "38:         permissions=[",
      "39:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "40:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG),",
      "41:         ],  # type: ignore",
      "44:     create_user(",
      "45:         app,  # type:ignore",
      "46:         username=\"test_with_dag2_read\",",
      "47:         role_name=\"TestWithDag2Read\",",
      "48:         permissions=[",
      "49:             (permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_WARNING),",
      "50:             (permissions.ACTION_CAN_READ, f\"{permissions.RESOURCE_DAG_PREFIX}dag2\"),",
      "51:         ],  # type: ignore",
      "52:     )",
      "58:     delete_user(app, username=\"test_with_dag2_read\")  # type: ignore",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "147:             \"/api/v1/dagWarnings\", environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"}",
      "148:         )",
      "149:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "164:     def test_should_raise_403_forbidden_when_user_has_no_dag_read_permission(self):",
      "165:         response = self.client.get(",
      "166:             \"/api/v1/dagWarnings\",",
      "167:             environ_overrides={\"REMOTE_USER\": \"test_with_dag2_read\"},",
      "168:             query_string={\"dag_id\": \"dag1\"},",
      "169:         )",
      "170:         assert response.status_code == 403",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "05d11a34303d0fb20b08a0241661f4d10168ed74",
      "candidate_info": {
        "commit_hash": "05d11a34303d0fb20b08a0241661f4d10168ed74",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/05d11a34303d0fb20b08a0241661f4d10168ed74",
        "files": [
          "airflow/www/views.py"
        ],
        "message": "Fix typo in views (#33830)\n\n(cherry picked from commit b68beb930fa8d84dda041e27d9fe36c2d0d72f8b)",
        "before_after_code_files": [
          "airflow/www/views.py||airflow/www/views.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "4186: class AirflowModelView(ModelView):",
          "4187:     \"\"\"",
          "4190:     Overridden `__getattribute__` to wraps REST methods with action_logger",
          "4191:     \"\"\"",
          "",
          "[Removed Lines]",
          "4188:     Airflow Mode View.",
          "",
          "[Added Lines]",
          "4188:     Airflow Model View.",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f8fc3d69244427ff1096c93d3fa155e201c09459",
      "candidate_info": {
        "commit_hash": "f8fc3d69244427ff1096c93d3fa155e201c09459",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f8fc3d69244427ff1096c93d3fa155e201c09459",
        "files": [
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/tests/test_docker_command_utils.py"
        ],
        "message": "Parse 'docker context ls --format=json' correctly (#34711)\n\n(cherry picked from commit 19284981f88e45dca4c4003837e3cead1723caf1)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/tests/test_docker_command_utils.py||dev/breeze/tests/test_docker_command_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "826:     if result.returncode != 0:",
          "827:         get_console().print(\"[warning]Could not detect docker builder. Using default.[/]\")",
          "828:         return \"default\"",
          "834:     if not known_contexts:",
          "835:         get_console().print(\"[warning]Could not detect docker builder. Using default.[/]\")",
          "836:         return \"default\"",
          "",
          "[Removed Lines]",
          "829:     context_json = json.loads(result.stdout)",
          "830:     if isinstance(context_json, dict):",
          "831:         # In case there is one context it is returned as dict not array of dicts \u00af\\_(\u30c4)_/\u00af",
          "832:         context_json = [context_json]",
          "833:     known_contexts = {info[\"Name\"]: info for info in context_json}",
          "",
          "[Added Lines]",
          "829:     context_dicts = (json.loads(line) for line in result.stdout.splitlines() if line.strip())",
          "830:     known_contexts = {info[\"Name\"]: info for info in context_dicts}",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_docker_command_utils.py||dev/breeze/tests/test_docker_command_utils.py": [
          "File: dev/breeze/tests/test_docker_command_utils.py -> dev/breeze/tests/test_docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "195:     )",
          "205: @pytest.mark.parametrize(",
          "206:     \"context_output, selected_context, console_output\",",
          "207:     [",
          "208:         (",
          "210:             \"default\",",
          "211:             \"[info]Using default as context\",",
          "212:         ),",
          "214:         (",
          "216:             \"a\",",
          "217:             \"[warning]Could not use any of the preferred docker contexts\",",
          "218:         ),",
          "219:         (",
          "221:             \"desktop-linux\",",
          "222:             \"[info]Using desktop-linux as context\",",
          "223:         ),",
          "224:         (",
          "226:             \"default\",",
          "227:             \"[info]Using default as context\",",
          "228:         ),",
          "229:         (",
          "231:             \"desktop-linux\",",
          "232:             \"[info]Using desktop-linux as context\",",
          "233:         ),",
          "",
          "[Removed Lines]",
          "198: def _fake_ctx(name: str) -> dict[str, str]:",
          "199:     return {",
          "200:         \"Name\": name,",
          "201:         \"DockerEndpoint\": f\"unix://{name}\",",
          "202:     }",
          "209:             json.dumps([_fake_ctx(\"default\")]),",
          "213:         (\"[]\", \"default\", \"[warning]Could not detect docker builder\"),",
          "215:             json.dumps([_fake_ctx(\"a\"), _fake_ctx(\"b\")]),",
          "220:             json.dumps([_fake_ctx(\"a\"), _fake_ctx(\"desktop-linux\")]),",
          "225:             json.dumps([_fake_ctx(\"a\"), _fake_ctx(\"default\")]),",
          "230:             json.dumps([_fake_ctx(\"a\"), _fake_ctx(\"default\"), _fake_ctx(\"desktop-linux\")]),",
          "",
          "[Added Lines]",
          "198: def _fake_ctx_output(*names: str) -> str:",
          "199:     return \"\\n\".join(json.dumps({\"Name\": name, \"DockerEndpoint\": f\"unix://{name}\"}) for name in names)",
          "206:             _fake_ctx_output(\"default\"),",
          "210:         (\"\\n\", \"default\", \"[warning]Could not detect docker builder\"),",
          "212:             _fake_ctx_output(\"a\", \"b\"),",
          "217:             _fake_ctx_output(\"a\", \"desktop-linux\"),",
          "222:             _fake_ctx_output(\"a\", \"default\"),",
          "227:             _fake_ctx_output(\"a\", \"default\", \"desktop-linux\"),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8a120bdb2345db63969908df5b4e46dbd948032b",
      "candidate_info": {
        "commit_hash": "8a120bdb2345db63969908df5b4e46dbd948032b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8a120bdb2345db63969908df5b4e46dbd948032b",
        "files": [
          "airflow/www/extensions/init_appbuilder.py",
          "airflow/www/fab_security/sqla/manager.py"
        ],
        "message": "replace loop by any when looking for a positive value in core (#33985)\n\n(cherry picked from commit b8165941d007081f0b7b24458d461a8760909ac8)",
        "before_after_code_files": [
          "airflow/www/extensions/init_appbuilder.py||airflow/www/extensions/init_appbuilder.py",
          "airflow/www/fab_security/sqla/manager.py||airflow/www/fab_security/sqla/manager.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/extensions/init_appbuilder.py||airflow/www/extensions/init_appbuilder.py": [
          "File: airflow/www/extensions/init_appbuilder.py -> airflow/www/extensions/init_appbuilder.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "645:         )",
          "647:     def _view_exists(self, view):",
          "653:     def _process_inner_views(self):",
          "654:         for view in self.baseviews:",
          "",
          "[Removed Lines]",
          "648:         for baseview in self.baseviews:",
          "649:             if baseview.__class__ == view.__class__:",
          "650:                 return True",
          "651:         return False",
          "",
          "[Added Lines]",
          "648:         return any(baseview.__class__ == view.__class__ for baseview in self.baseviews)",
          "",
          "---------------"
        ],
        "airflow/www/fab_security/sqla/manager.py||airflow/www/fab_security/sqla/manager.py": [
          "File: airflow/www/fab_security/sqla/manager.py -> airflow/www/fab_security/sqla/manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "537:             self.get_session.rollback()",
          "539:     def perms_include_action(self, perms, action_name):",
          "545:     def add_permission_to_role(self, role: Role, permission: Permission | None) -> None:",
          "546:         \"\"\"",
          "",
          "[Removed Lines]",
          "540:         for perm in perms:",
          "541:             if perm.action and perm.action.name == action_name:",
          "542:                 return True",
          "543:         return False",
          "",
          "[Added Lines]",
          "540:         return any(perm.action and perm.action.name == action_name for perm in perms)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "25005a6e58b60f34db71667f0b075074101e3de4",
      "candidate_info": {
        "commit_hash": "25005a6e58b60f34db71667f0b075074101e3de4",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/25005a6e58b60f34db71667f0b075074101e3de4",
        "files": [
          "airflow/dag_processing/manager.py",
          "airflow/plugins_manager.py",
          "airflow/providers/amazon/aws/hooks/datasync.py",
          "airflow/providers/apache/hive/hooks/hive.py",
          "airflow/providers/google/cloud/operators/bigquery_dts.py",
          "airflow/providers/google/cloud/sensors/dataproc_metastore.py",
          "airflow/providers/google/cloud/transfers/gcs_to_gcs.py",
          "airflow/providers/snowflake/utils/sql_api_generate_jwt.py",
          "tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py"
        ],
        "message": "Refactor: Simplify comparisons (#34181)\n\n(cherry picked from commit 94d07908a2188eb650bfab21d89a49b287aee35c)",
        "before_after_code_files": [
          "airflow/dag_processing/manager.py||airflow/dag_processing/manager.py",
          "airflow/plugins_manager.py||airflow/plugins_manager.py",
          "airflow/providers/amazon/aws/hooks/datasync.py||airflow/providers/amazon/aws/hooks/datasync.py",
          "airflow/providers/apache/hive/hooks/hive.py||airflow/providers/apache/hive/hooks/hive.py",
          "airflow/providers/google/cloud/operators/bigquery_dts.py||airflow/providers/google/cloud/operators/bigquery_dts.py",
          "airflow/providers/google/cloud/sensors/dataproc_metastore.py||airflow/providers/google/cloud/sensors/dataproc_metastore.py",
          "airflow/providers/google/cloud/transfers/gcs_to_gcs.py||airflow/providers/google/cloud/transfers/gcs_to_gcs.py",
          "airflow/providers/snowflake/utils/sql_api_generate_jwt.py||airflow/providers/snowflake/utils/sql_api_generate_jwt.py",
          "tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py||tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/dag_processing/manager.py||airflow/dag_processing/manager.py": [
          "File: airflow/dag_processing/manager.py -> airflow/dag_processing/manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1078:         # needs to be done before this process is forked to create the DAG parsing processes.",
          "1079:         SecretCache.init()",
          "1082:             file_path = self._file_path_queue.popleft()",
          "1083:             # Stop creating duplicate processor i.e. processor with the same filepath",
          "1084:             if file_path in self._processors:",
          "",
          "[Removed Lines]",
          "1081:         while self._parallelism - len(self._processors) > 0 and self._file_path_queue:",
          "",
          "[Added Lines]",
          "1081:         while self._parallelism > len(self._processors) and self._file_path_queue:",
          "",
          "---------------"
        ],
        "airflow/plugins_manager.py||airflow/plugins_manager.py": [
          "File: airflow/plugins_manager.py -> airflow/plugins_manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "341:         for plugin in plugins:",
          "342:             registered_hooks.extend(plugin.hooks)",
          "349: def initialize_web_ui_plugins():",
          "",
          "[Removed Lines]",
          "344:     num_loaded = len(plugins)",
          "345:     if num_loaded > 0:",
          "346:         log.debug(\"Loading %d plugin(s) took %.2f seconds\", num_loaded, timer.duration)",
          "",
          "[Added Lines]",
          "344:     if plugins:",
          "345:         log.debug(\"Loading %d plugin(s) took %.2f seconds\", len(plugins), timer.duration)",
          "",
          "---------------"
        ],
        "airflow/providers/amazon/aws/hooks/datasync.py||airflow/providers/amazon/aws/hooks/datasync.py": [
          "File: airflow/providers/amazon/aws/hooks/datasync.py -> airflow/providers/amazon/aws/hooks/datasync.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "57:         self.locations: list = []",
          "58:         self.tasks: list = []",
          "59:         # wait_interval_seconds = 0 is used during unit tests",
          "61:             raise ValueError(f\"Invalid wait_interval_seconds {wait_interval_seconds}\")",
          "64:     def create_location(self, location_uri: str, **create_location_kwargs) -> str:",
          "65:         \"\"\"",
          "",
          "[Removed Lines]",
          "60:         if wait_interval_seconds < 0 or wait_interval_seconds > 15 * 60:",
          "62:         self.wait_interval_seconds = wait_interval_seconds",
          "",
          "[Added Lines]",
          "60:         if 0 <= wait_interval_seconds <= 15 * 60:",
          "61:             self.wait_interval_seconds = wait_interval_seconds",
          "62:         else:",
          "",
          "---------------"
        ],
        "airflow/providers/apache/hive/hooks/hive.py||airflow/providers/apache/hive/hooks/hive.py": [
          "File: airflow/providers/apache/hive/hooks/hive.py -> airflow/providers/apache/hive/hooks/hive.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "176:             )",
          "177:         try:",
          "178:             int_port = int(conn.port)",
          "180:                 raise Exception(f\"The port used in beeline command ({conn.port}) should be in range 0-65535)\")",
          "181:         except (ValueError, TypeError) as e:",
          "182:             raise Exception(f\"The port used in beeline command ({conn.port}) should be a valid integer: {e})\")",
          "",
          "[Removed Lines]",
          "179:             if int_port <= 0 or int_port > 65535:",
          "",
          "[Added Lines]",
          "179:             if not 0 < int_port <= 65535:",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/operators/bigquery_dts.py||airflow/providers/google/cloud/operators/bigquery_dts.py": [
          "File: airflow/providers/google/cloud/operators/bigquery_dts.py -> airflow/providers/google/cloud/operators/bigquery_dts.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "355:         )",
          "357:     def _wait_for_transfer_to_be_done(self, run_id: str, transfer_config_id: str, interval: int = 10):",
          "359:             raise ValueError(\"Interval must be > 0\")",
          "361:         while True:",
          "",
          "[Removed Lines]",
          "358:         if interval < 0:",
          "",
          "[Added Lines]",
          "358:         if interval <= 0:",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/sensors/dataproc_metastore.py||airflow/providers/google/cloud/sensors/dataproc_metastore.py": [
          "File: airflow/providers/google/cloud/sensors/dataproc_metastore.py -> airflow/providers/google/cloud/sensors/dataproc_metastore.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "114:         # Return True if we got all requested partitions.",
          "115:         # If no partitions were given in the request, then we expect to find at least one.",
          "",
          "[Removed Lines]",
          "116:         return found_partitions > 0 and found_partitions >= len(set(self.partitions))",
          "",
          "[Added Lines]",
          "116:         return found_partitions >= max(1, len(set(self.partitions)))",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/transfers/gcs_to_gcs.py||airflow/providers/google/cloud/transfers/gcs_to_gcs.py": [
          "File: airflow/providers/google/cloud/transfers/gcs_to_gcs.py -> airflow/providers/google/cloud/transfers/gcs_to_gcs.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "307:             ]",
          "309:         objects = set(objects) - set(existing_objects)",
          "311:             self.log.info(\"%s files are going to be synced: %s.\", len(objects), objects)",
          "312:         else:",
          "313:             self.log.info(\"There are no new files to sync. Have a nice day!\")",
          "",
          "[Removed Lines]",
          "310:         if len(objects) > 0:",
          "",
          "[Added Lines]",
          "310:         if objects:",
          "",
          "---------------"
        ],
        "airflow/providers/snowflake/utils/sql_api_generate_jwt.py||airflow/providers/snowflake/utils/sql_api_generate_jwt.py": [
          "File: airflow/providers/snowflake/utils/sql_api_generate_jwt.py -> airflow/providers/snowflake/utils/sql_api_generate_jwt.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "102:         account = raw_account",
          "103:         if \".global\" not in account:",
          "104:             # Handle the general case.",
          "108:         else:",
          "109:             # Handle the replication case.",
          "113:         # Use uppercase for the account identifier.",
          "114:         return account.upper()",
          "",
          "[Removed Lines]",
          "105:             idx = account.find(\".\")",
          "106:             if idx > 0:",
          "107:                 account = account[0:idx]",
          "110:             idx = account.find(\"-\")",
          "111:             if idx > 0:",
          "112:                 account = account[0:idx]  # pragma: no cover",
          "",
          "[Added Lines]",
          "105:             account = account.partition(\".\")[0]",
          "108:             account = account.partition(\"-\")[0]",
          "",
          "---------------"
        ],
        "tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py||tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py": [
          "File: tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py -> tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "92:     @staticmethod",
          "93:     def _is_valid_pod_id(name):",
          "94:         regex = r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$\"",
          "97:     @staticmethod",
          "98:     def _is_safe_label_value(value):",
          "",
          "[Removed Lines]",
          "95:         return len(name) <= 253 and all(ch.lower() == ch for ch in name) and re.match(regex, name)",
          "",
          "[Added Lines]",
          "95:         return len(name) <= 253 and name.islower() and re.match(regex, name)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2c1d4d691ff37bf1bd7c9da5f7d2c79fbe202565",
      "candidate_info": {
        "commit_hash": "2c1d4d691ff37bf1bd7c9da5f7d2c79fbe202565",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/2c1d4d691ff37bf1bd7c9da5f7d2c79fbe202565",
        "files": [
          "airflow/www/views.py"
        ],
        "message": "Add missing audit logs for Flask actions add, edit and delete (#34090)\n\n(cherry picked from commit 988632fd67abc10375ad9fe2cbd8c9edccc609a5)",
        "before_after_code_files": [
          "airflow/www/views.py||airflow/www/views.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "5228:     class_permission_name = permissions.RESOURCE_DAG_RUN",
          "5229:     method_permission_name = {",
          "5230:         \"list\": \"read\",",
          "5231:         \"action_clear\": \"edit\",",
          "5232:         \"action_muldelete\": \"delete\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "5230:         \"delete\": \"delete\",",
          "5231:         \"edit\": \"edit\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "5605:     class_permission_name = permissions.RESOURCE_TASK_INSTANCE",
          "5606:     method_permission_name = {",
          "5607:         \"list\": \"read\",",
          "5608:         \"action_clear\": \"edit\",",
          "5609:         \"action_muldelete\": \"delete\",",
          "5610:         \"action_set_running\": \"edit\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "5610:         \"delete\": \"delete\",",
          "",
          "---------------"
        ]
      }
    }
  ]
}