{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "1372f312052dd0361e371e2ed63436f3e299c617",
      "candidate_info": {
        "commit_hash": "1372f312052dd0361e371e2ed63436f3e299c617",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/1372f312052dd0361e371e2ed63436f3e299c617",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala"
        ],
        "message": "[SPARK-39164][SQL][3.3] Wrap asserts/illegal state exceptions by the INTERNAL_ERROR exception in actions\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to catch `java.lang.IllegalStateException` and `java.lang.AssertionError` (raised by asserts), and wrap them by Spark's exception w/ the `INTERNAL_ERROR` error class. The modification affects only actions so far.\n\nThis PR affects the case of missing bucket file. After the changes, Spark throws `SparkException` w/ `INTERNAL_ERROR` instead of `IllegalStateException`. Since this is not Spark's illegal state, the exception should be replaced by another runtime exception. Created the ticket SPARK-39163 to fix this.\n\nThis is a backport of https://github.com/apache/spark/pull/36500.\n\n### Why are the changes needed?\nTo improve user experience with Spark SQL and unify representation of internal errors by using error classes like for other errors. Usually, users shouldn't observe asserts and illegal states, but even if such situation happens, they should see errors in the same way as other errors (w/ error class `INTERNAL_ERROR`).\n\n### Does this PR introduce _any_ user-facing change?\nYes. At least, in one particular case, see the modified test suites and SPARK-39163.\n\n### How was this patch tested?\nBy running the affected test suites:\n```\n$ build/sbt \"test:testOnly *.BucketedReadWithoutHiveSupportSuite\"\n$ build/sbt \"test:testOnly *.AdaptiveQueryExecSuite\"\n$ build/sbt \"test:testOnly *.WholeStageCodegenSuite\"\n```\n\nAuthored-by: Max Gekk <max.gekkgmail.com>\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n(cherry picked from commit f5c3f0c228fef7808d1f927e134595ddd4d31723)\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n\nCloses #36533 from MaxGekk/class-internal-error-3.3.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala||sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala||sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala -> sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "28: import org.apache.commons.lang3.StringUtils",
          "31: import org.apache.spark.annotation.{DeveloperApi, Stable, Unstable}",
          "32: import org.apache.spark.api.java.JavaRDD",
          "33: import org.apache.spark.api.java.function._",
          "",
          "[Removed Lines]",
          "30: import org.apache.spark.TaskContext",
          "",
          "[Added Lines]",
          "30: import org.apache.spark.{SparkException, SparkThrowable, TaskContext}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3853:   private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = {",
          "3857:     }",
          "3858:   }",
          "",
          "[Removed Lines]",
          "3854:     SQLExecution.withNewExecutionId(qe, Some(name)) {",
          "3855:       qe.executedPlan.resetMetrics()",
          "3856:       action(qe.executedPlan)",
          "",
          "[Added Lines]",
          "3855:     try {",
          "3856:       SQLExecution.withNewExecutionId(qe, Some(name)) {",
          "3857:         qe.executedPlan.resetMetrics()",
          "3858:         action(qe.executedPlan)",
          "3859:       }",
          "3860:     } catch {",
          "3861:       case e: SparkThrowable => throw e",
          "3862:       case e @ (_: java.lang.IllegalStateException | _: java.lang.AssertionError) =>",
          "3863:         throw new SparkException(",
          "3864:           errorClass = \"INTERNAL_ERROR\",",
          "3865:           messageParameters = Array(s\"\"\"The \"$name\" action failed.\"\"\"),",
          "3866:           cause = e)",
          "3867:       case e: Throwable => throw e",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/DataSourceScanExec.scala"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/subquery.scala"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import scala.collection.mutable.ArrayBuffer",
          "22: import org.apache.spark.sql.catalyst.expressions.SubqueryExpression",
          "23: import org.apache.spark.sql.catalyst.plans.logical.{Join, LogicalPlan, Sort}",
          "24: import org.apache.spark.sql.execution.{ColumnarToRowExec, ExecSubqueryExpression, FileSourceScanExec, InputAdapter, ReusedSubqueryExec, ScalarSubquery, SubqueryExec, WholeStageCodegenExec}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: import org.apache.spark.SparkException",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "146:   }",
          "148:   test(\"runtime error when the number of rows is greater than 1\") {",
          "150:       sql(\"select (select a from (select 1 as a union all select 2 as a) t) as b\").collect()",
          "151:     }",
          "155:   }",
          "157:   test(\"uncorrelated scalar subquery on a DataFrame generated query\") {",
          "",
          "[Removed Lines]",
          "149:     val error2 = intercept[RuntimeException] {",
          "152:     assert(error2.getMessage.contains(",
          "153:       \"more than one row returned by a subquery used as an expression\")",
          "154:     )",
          "",
          "[Added Lines]",
          "150:     val e = intercept[SparkException] {",
          "154:     assert(e.getErrorClass ===  \"INTERNAL_ERROR\")",
          "155:     assert(e.getCause.getMessage.contains(",
          "156:       \"more than one row returned by a subquery used as an expression\"))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.execution",
          "20: import org.apache.spark.sql.{Dataset, QueryTest, Row, SaveMode}",
          "21: import org.apache.spark.sql.catalyst.expressions.codegen.{ByteCodeStats, CodeAndComment, CodeGenerator}",
          "22: import org.apache.spark.sql.execution.adaptive.DisableAdaptiveExecutionSuite",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import org.apache.spark.SparkException",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "762:           \"SELECT AVG(v) FROM VALUES(1) t(v)\",",
          "764:           \"SELECT k, AVG(v) FROM VALUES((1, 1)) t(k, v) GROUP BY k\").foreach { query =>",
          "766:             sql(query).collect",
          "769:         }",
          "770:       }",
          "771:     }",
          "",
          "[Removed Lines]",
          "765:           val errMsg = intercept[IllegalStateException] {",
          "767:           }.getMessage",
          "768:           assert(errMsg.contains(expectedErrMsg))",
          "",
          "[Added Lines]",
          "766:           val e = intercept[SparkException] {",
          "768:           }",
          "769:           assert(e.getErrorClass === \"INTERNAL_ERROR\")",
          "770:           assert(e.getCause.getMessage.contains(expectedErrMsg))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "785:           \"SELECT k, AVG(a + b), SUM(a + b + c) FROM VALUES((1, 1, 1, 1)) t(k, a, b, c) \" +",
          "786:             \"GROUP BY k\").foreach { query =>",
          "788:             sql(query).collect",
          "789:           }",
          "792:         }",
          "793:       }",
          "794:     }",
          "",
          "[Removed Lines]",
          "787:           val e = intercept[Exception] {",
          "790:           assert(e.isInstanceOf[IllegalStateException])",
          "791:           assert(e.getMessage.contains(expectedErrMsg))",
          "",
          "[Added Lines]",
          "789:           val e = intercept[SparkException] {",
          "792:           assert(e.getErrorClass === \"INTERNAL_ERROR\")",
          "793:           assert(e.getCause.getMessage.contains(expectedErrMsg))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/adaptive/AdaptiveQueryExecSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import org.apache.logging.log4j.Level",
          "24: import org.scalatest.PrivateMethodTester",
          "26: import org.apache.spark.scheduler.{SparkListener, SparkListenerEvent, SparkListenerJobStart}",
          "27: import org.apache.spark.sql.{Dataset, QueryTest, Row, SparkSession, Strategy}",
          "28: import org.apache.spark.sql.catalyst.optimizer.{BuildLeft, BuildRight}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: import org.apache.spark.SparkException",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "856:         df1.write.parquet(tableDir.getAbsolutePath)",
          "858:         val aggregated = spark.table(\"bucketed_table\").groupBy(\"i\").count()",
          "860:           aggregated.count()",
          "861:         }",
          "864:       }",
          "865:     }",
          "866:   }",
          "",
          "[Removed Lines]",
          "859:         val error = intercept[Exception] {",
          "862:         assert(error.toString contains \"Invalid bucket file\")",
          "863:         assert(error.getSuppressed.size === 0)",
          "",
          "[Added Lines]",
          "860:         val error = intercept[SparkException] {",
          "864:         assert(error.getErrorClass === \"INTERNAL_ERROR\")",
          "865:         assert(error.getCause.toString contains \"Invalid bucket file\")",
          "866:         assert(error.getCause.getSuppressed.size === 0)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/sources/BucketedReadSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import scala.util.Random",
          "25: import org.apache.spark.sql._",
          "26: import org.apache.spark.sql.catalyst.catalog.BucketSpec",
          "27: import org.apache.spark.sql.catalyst.expressions",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: import org.apache.spark.SparkException",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "841:       df1.write.parquet(tableDir.getAbsolutePath)",
          "843:       val aggregated = spark.table(\"bucketed_table\").groupBy(\"i\").count()",
          "845:         aggregated.count()",
          "846:       }",
          "849:     }",
          "850:   }",
          "",
          "[Removed Lines]",
          "844:       val error = intercept[Exception] {",
          "848:       assert(error.toString contains \"Invalid bucket file\")",
          "",
          "[Added Lines]",
          "845:       val e = intercept[SparkException] {",
          "849:       assert(e.getErrorClass === \"INTERNAL_ERROR\")",
          "850:       assert(e.getCause.toString contains \"Invalid bucket file\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "76fa565fac36f4fd94b181b213611fa716092a99",
      "candidate_info": {
        "commit_hash": "76fa565fac36f4fd94b181b213611fa716092a99",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/76fa565fac36f4fd94b181b213611fa716092a99",
        "files": [
          "python/pyspark/instrumentation_utils.py"
        ],
        "message": "[SPARK-38882][PYTHON] Fix usage logger attachment to handle static methods properly\n\n### What changes were proposed in this pull request?\n\nFixes usage logger attachment to handle static methods properly.\n\n### Why are the changes needed?\n\nThe usage logger attachment logic has an issue when handling static methods.\n\nFor example,\n\n```\n$ PYSPARK_PANDAS_USAGE_LOGGER=pyspark.pandas.usage_logging.usage_logger ./bin/pyspark\n```\n```py\n>>> import pyspark.pandas as ps\n>>> psdf = ps.DataFrame({\"a\": [1,2,3], \"b\": [4,5,6]})\n>>> psdf.from_records([(1, 2), (3, 4)])\nA function `DataFrame.from_records(data, index, exclude, columns, coerce_float, nrows)` was failed after 2007.430 ms: 0\nTraceback (most recent call last):\n...\n```\n\nwithout usage logger:\n\n```py\n>>> import pyspark.pandas as ps\n>>> psdf = ps.DataFrame({\"a\": [1,2,3], \"b\": [4,5,6]})\n>>> psdf.from_records([(1, 2), (3, 4)])\n   0  1\n0  1  2\n1  3  4\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, for a user attaches the usage logger, static methods will work as static methods.\n\n### How was this patch tested?\n\nManually tested.\n\n```py\n>>> import pyspark.pandas as ps\n>>> import logging\n>>> import sys\n>>> root = logging.getLogger()\n>>> root.setLevel(logging.INFO)\n>>> handler = logging.StreamHandler(sys.stdout)\n>>> handler.setLevel(logging.INFO)\n>>>\n>>> formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n>>> handler.setFormatter(formatter)\n>>> root.addHandler(handler)\n\n>>> psdf = ps.DataFrame({\"a\": [1,2,3], \"b\": [4,5,6]})\n2022-04-12 14:43:52,254 - pyspark.pandas.usage_logger - INFO - A function `DataFrame.__init__(self, data, index, columns, dtype, copy)` was successfully finished after 2714.896 ms.\n>>> psdf.from_records([(1, 2), (3, 4)])\n2022-04-12 14:43:59,765 - pyspark.pandas.usage_logger - INFO - A function `DataFrame.from_records(data, index, exclude, columns, coerce_float, nrows)` was successfully finished after 51.105 ms.\n2022-04-12 14:44:01,371 - pyspark.pandas.usage_logger - INFO - A function `DataFrame.__repr__(self)` was successfully finished after 1605.759 ms.\n   0  1\n0  1  2\n1  3  4\n\n>>> ps.DataFrame.from_records([(1, 2), (3, 4)])\n2022-04-12 14:44:25,301 - pyspark.pandas.usage_logger - INFO - A function `DataFrame.from_records(data, index, exclude, columns, coerce_float, nrows)` was successfully finished after 43.446 ms.\n2022-04-12 14:44:25,493 - pyspark.pandas.usage_logger - INFO - A function `DataFrame.__repr__(self)` was successfully finished after 192.053 ms.\n   0  1\n0  1  2\n1  3  4\n```\n\nCloses #36167 from ueshin/issues/SPARK-38882/staticmethod.\n\nAuthored-by: Takuya UESHIN <ueshin@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 1c1216f18f3008b410a601516b2fde49a9e27f7d)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/instrumentation_utils.py||python/pyspark/instrumentation_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/instrumentation_utils.py||python/pyspark/instrumentation_utils.py": [
          "File: python/pyspark/instrumentation_utils.py -> python/pyspark/instrumentation_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "163:         for name, func in inspect.getmembers(target_class, inspect.isfunction):",
          "164:             if name.startswith(\"_\") and name not in special_functions:",
          "165:                 continue",
          "168:         for name, prop in inspect.getmembers(target_class, lambda o: isinstance(o, property)):",
          "169:             if name.startswith(\"_\"):",
          "",
          "[Removed Lines]",
          "166:             setattr(target_class, name, _wrap_function(target_class.__name__, name, func, logger))",
          "",
          "[Added Lines]",
          "166:             try:",
          "167:                 isstatic = isinstance(inspect.getattr_static(target_class, name), staticmethod)",
          "168:             except AttributeError:",
          "169:                 isstatic = False",
          "170:             wrapped_function = _wrap_function(target_class.__name__, name, func, logger)",
          "171:             setattr(",
          "172:                 target_class, name, staticmethod(wrapped_function) if isstatic else wrapped_function",
          "173:             )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "233a54d0ab39944ec815bd86d2fc6200c03ca79a",
      "candidate_info": {
        "commit_hash": "233a54d0ab39944ec815bd86d2fc6200c03ca79a",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/233a54d0ab39944ec815bd86d2fc6200c03ca79a",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "sql/core/src/test/resources/sql-tests/inputs/string-functions.sql",
          "sql/core/src/test/resources/sql-tests/results/ansi/string-functions.sql.out",
          "sql/core/src/test/resources/sql-tests/results/string-functions.sql.out"
        ],
        "message": "[SPARK-40152][SQL] Fix split_part codegen compilation issue\n\n### What changes were proposed in this pull request?\n\nFix `split_part` codegen compilation issue:\n```sql\nSELECT split_part(str, delimiter, partNum) FROM VALUES ('11.12.13', '.', 3) AS v1(str, delimiter, partNum);\n```\n```\norg.codehaus.commons.compiler.CompileException: File 'generated.java', Line 42, Column 1: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 42, Column 1: Expression \"project_isNull_0 = false\" is not a type\n```\n\n### Why are the changes needed?\n\nFix bug.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nUnit test.\n\nCloses #37589 from wangyum/SPARK-40152.\n\nAuthored-by: Yuming Wang <yumwang@ebay.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n(cherry picked from commit cf1a80eeae8bf815270fb39568b1846c2bd8d437)\nSigned-off-by: Sean Owen <srowen@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "sql/core/src/test/resources/sql-tests/inputs/string-functions.sql||sql/core/src/test/resources/sql-tests/inputs/string-functions.sql"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2225:               case Some(value) =>",
          "2226:                 val defaultValueEval = value.genCode(ctx)",
          "2227:                 s\"\"\"",
          "2231:                 \"\"\".stripMargin",
          "2232:               case None => s\"${ev.isNull} = true;\"",
          "2233:             }",
          "",
          "[Removed Lines]",
          "2228:                   ${defaultValueEval.code}",
          "2229:                   ${ev.isNull} = ${defaultValueEval.isNull}",
          "2230:                   ${ev.value} = ${defaultValueEval.value}",
          "",
          "[Added Lines]",
          "2228:                   ${defaultValueEval.code};",
          "2229:                   ${ev.isNull} = ${defaultValueEval.isNull};",
          "2230:                   ${ev.value} = ${defaultValueEval.value};",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/string-functions.sql||sql/core/src/test/resources/sql-tests/inputs/string-functions.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/string-functions.sql -> sql/core/src/test/resources/sql-tests/inputs/string-functions.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "38: SELECT split_part('11.12.13', '.', 5);",
          "39: SELECT split_part('11.12.13', '.', -5);",
          "40: SELECT split_part(null, '.', 1);",
          "42: -- substring function",
          "43: SELECT substr('Spark SQL', 5);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "41: SELECT split_part(str, delimiter, partNum) FROM VALUES ('11.12.13', '.', 3) AS v1(str, delimiter, partNum);",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bb5e3aa0e6326305a788d189d2a9cc813000bd1c",
      "candidate_info": {
        "commit_hash": "bb5e3aa0e6326305a788d189d2a9cc813000bd1c",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/bb5e3aa0e6326305a788d189d2a9cc813000bd1c",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/streaming/FlatMapGroupsWithStateSuite.scala"
        ],
        "message": "[SPARK-38942][TESTS][SQL][3.3] Skip RocksDB-based test case in FlatMapGroupsWithStateSuite on Apple Silicon\n\n### What changes were proposed in this pull request?\nThis PR aims to skip RocksDB-based test case in FlatMapGroupsWithStateSuite on Apple Silicon.\n\n### Why are the changes needed?\nCurrently, it is broken on Apple Silicon.\n\n**BEFORE**\n```\n$ build/sbt \"sql/testOnly org.apache.spark.sql.streaming.FlatMapGroupsWithStateSuite\"\n...\n[info] *** 1 TEST FAILED ***\n[error] Failed tests:\n[error] \torg.apache.spark.sql.streaming.FlatMapGroupsWithStateSuite\n[error] (sql / Test / testOnly) sbt.TestsFailedException: Tests unsuccessful\n```\n\n**AFTER**\n```\n$ build/sbt \"sql/testOnly org.apache.spark.sql.streaming.FlatMapGroupsWithStateSuite\"\n...\n[info] Run completed in 32 seconds, 692 milliseconds.\n[info] Total number of tests run: 105\n[info] Suites: completed 1, aborted 0\n[info] Tests: succeeded 105, failed 0, canceled 1, ignored 0, pending 0\n[info] All tests passed.\n```\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nTest manually on Apple Silicon.\n\nCloses #36256 from williamhyun/SPARK-38942.\n\nAuthored-by: William Hyun <william@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/streaming/FlatMapGroupsWithStateSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/streaming/FlatMapGroupsWithStateSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/streaming/FlatMapGroupsWithStateSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/streaming/FlatMapGroupsWithStateSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/streaming/FlatMapGroupsWithStateSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/streaming/FlatMapGroupsWithStateSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1521:   }",
          "1523:   test(\"SPARK-38320 - flatMapGroupsWithState state with data should not timeout\") {",
          "1524:     withTempDir { dir =>",
          "1525:       withSQLConf(",
          "1526:         (SQLConf.STREAMING_NO_DATA_MICRO_BATCHES_ENABLED.key -> \"false\"),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1524:     assume(!Utils.isMacOnAppleSilicon)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "19991047d5b5316412d8b1763807c5945a705bff",
      "candidate_info": {
        "commit_hash": "19991047d5b5316412d8b1763807c5945a705bff",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/19991047d5b5316412d8b1763807c5945a705bff",
        "files": [
          "core/src/main/resources/error/error-classes.json",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TableOutputResolver.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala"
        ],
        "message": "[SPARK-39865][SQL][3.3] Show proper error messages on the overflow errors of table insert\n\n### What changes were proposed in this pull request?\n\nIn Spark 3.3, the error message of ANSI CAST is improved. However, the table insertion is using the same CAST expression:\n```\n> create table tiny(i tinyint);\n> insert into tiny values (1000);\n\norg.apache.spark.SparkArithmeticException[CAST_OVERFLOW]: The value 1000 of the type \"INT\" cannot be cast to \"TINYINT\" due to an overflow. Use `try_cast` to tolerate overflow and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error.\n```\n\nShowing the hint of `If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error` doesn't help at all. This PR is to fix the error message. After changes, the error message of this example will become:\n```\norg.apache.spark.SparkArithmeticException: [CAST_OVERFLOW_IN_TABLE_INSERT] Fail to insert a value of \"INT\" type into the \"TINYINT\" type column `i` due to an overflow. Use `try_cast` on the input value to tolerate overflow and return NULL instead.\n```\n### Why are the changes needed?\n\nShow proper error messages on the overflow errors of table insert. The current message is super confusing.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, after changes it show proper error messages on the overflow errors of table insert.\n\n### How was this patch tested?\n\nUnit test\n\nCloses #37311 from gengliangwang/PR_TOOL_PICK_PR_37283_BRANCH-3.3.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TableOutputResolver.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TableOutputResolver.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TableOutputResolver.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TableOutputResolver.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TableOutputResolver.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/TableOutputResolver.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import org.apache.spark.sql.errors.QueryCompilationErrors",
          "27: import org.apache.spark.sql.internal.SQLConf",
          "28: import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy",
          "31: object TableOutputResolver {",
          "32:   def resolveOutputColumns(",
          "",
          "[Removed Lines]",
          "29: import org.apache.spark.sql.types.{ArrayType, DataType, MapType, StructType}",
          "",
          "[Added Lines]",
          "29: import org.apache.spark.sql.types.{ArrayType, DataType, DecimalType, IntegralType, MapType, StructType}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "220:     }",
          "221:   }",
          "223:   private def checkField(",
          "224:       tableAttr: Attribute,",
          "225:       queryExpr: NamedExpression,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "223:   private def containsIntegralOrDecimalType(dt: DataType): Boolean = dt match {",
          "224:     case _: IntegralType | _: DecimalType => true",
          "225:     case a: ArrayType => containsIntegralOrDecimalType(a.elementType)",
          "226:     case m: MapType =>",
          "227:       containsIntegralOrDecimalType(m.keyType) || containsIntegralOrDecimalType(m.valueType)",
          "228:     case s: StructType =>",
          "229:       s.fields.exists(sf => containsIntegralOrDecimalType(sf.dataType))",
          "230:     case _ => false",
          "231:   }",
          "233:   private def canCauseCastOverflow(cast: AnsiCast): Boolean = {",
          "234:     containsIntegralOrDecimalType(cast.dataType) &&",
          "235:       !Cast.canUpCast(cast.child.dataType, cast.dataType)",
          "236:   }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "235:     } else {",
          "236:       val casted = storeAssignmentPolicy match {",
          "237:         case StoreAssignmentPolicy.ANSI =>",
          "239:         case StoreAssignmentPolicy.LEGACY =>",
          "240:           Cast(queryExpr, tableAttr.dataType, Option(conf.sessionLocalTimeZone),",
          "241:             ansiEnabled = false)",
          "",
          "[Removed Lines]",
          "238:           AnsiCast(queryExpr, tableAttr.dataType, Option(conf.sessionLocalTimeZone))",
          "",
          "[Added Lines]",
          "253:           val cast = AnsiCast(queryExpr, tableAttr.dataType, Option(conf.sessionLocalTimeZone))",
          "254:           if (canCauseCastOverflow(cast)) {",
          "255:             CheckOverflowInTableInsert(cast, tableAttr.name)",
          "256:           } else {",
          "257:             cast",
          "258:           }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import java.util.Locale",
          "22: import java.util.concurrent.TimeUnit._",
          "24: import org.apache.spark.sql.catalyst.InternalRow",
          "25: import org.apache.spark.sql.catalyst.analysis.{TypeCheckResult, TypeCoercion}",
          "26: import org.apache.spark.sql.catalyst.expressions.Cast.resolvableNullability",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24: import org.apache.spark.SparkArithmeticException",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2352:   override protected def withNewChildInternal(newChild: Expression): UpCast = copy(child = newChild)",
          "2353: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2360: case class CheckOverflowInTableInsert(child: AnsiCast, columnName: String) extends UnaryExpression {",
          "2361:   override protected def withNewChildInternal(newChild: Expression): Expression =",
          "2362:     copy(child = newChild.asInstanceOf[AnsiCast])",
          "2364:   override def eval(input: InternalRow): Any = try {",
          "2365:     child.eval(input)",
          "2366:   } catch {",
          "2367:     case e: SparkArithmeticException =>",
          "2368:       QueryExecutionErrors.castingCauseOverflowErrorInTableInsert(",
          "2369:         child.child.dataType,",
          "2370:         child.dataType,",
          "2371:         columnName)",
          "2372:   }",
          "2374:   override protected def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {",
          "2375:     val childGen = child.genCode(ctx)",
          "2376:     val exceptionClass = classOf[SparkArithmeticException].getCanonicalName",
          "2377:     val fromDt =",
          "2378:       ctx.addReferenceObj(\"from\", child.child.dataType, child.child.dataType.getClass.getName)",
          "2379:     val toDt = ctx.addReferenceObj(\"to\", child.dataType, child.dataType.getClass.getName)",
          "2380:     val col = ctx.addReferenceObj(\"colName\", columnName, \"java.lang.String\")",
          "2382:     ev.copy(code = code\"\"\"",
          "2383:       boolean ${ev.isNull} = true;",
          "2384:       ${CodeGenerator.javaType(dataType)} ${ev.value} = ${CodeGenerator.defaultValue(dataType)};",
          "2385:       try {",
          "2386:         ${childGen.code}",
          "2387:         ${ev.isNull} = ${childGen.isNull};",
          "2388:         ${ev.value} = ${childGen.value};",
          "2389:       } catch ($exceptionClass e) {",
          "2390:         throw QueryExecutionErrors.castingCauseOverflowErrorInTableInsert($fromDt, $toDt, $col);",
          "2391:       }\"\"\"",
          "2392:     )",
          "2394:   }",
          "2396:   override def dataType: DataType = child.dataType",
          "2397: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "93:         toSQLConf(SQLConf.ANSI_ENABLED.key)))",
          "94:   }",
          "96:   def cannotChangeDecimalPrecisionError(",
          "97:       value: Decimal,",
          "98:       decimalPrecision: Int,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "96:   def castingCauseOverflowErrorInTableInsert(",
          "97:       from: DataType,",
          "98:       to: DataType,",
          "99:       columnName: String): ArithmeticException = {",
          "100:     new SparkArithmeticException(",
          "101:       errorClass = \"CAST_OVERFLOW_IN_TABLE_INSERT\",",
          "102:       messageParameters = Array(",
          "103:         toSQLType(from),",
          "104:         toSQLType(to),",
          "105:         toSQLId(columnName))",
          "106:     )",
          "107:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/sources/InsertSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "710:       withTable(\"t\") {",
          "711:         sql(\"create table t(b int) using parquet\")",
          "712:         val outOfRangeValue1 = (Int.MaxValue + 1L).toString",
          "713:         var msg = intercept[SparkException] {",
          "714:           sql(s\"insert into t values($outOfRangeValue1)\")",
          "715:         }.getCause.getMessage",
          "719:         val outOfRangeValue2 = (Int.MinValue - 1L).toString",
          "720:         msg = intercept[SparkException] {",
          "721:           sql(s\"insert into t values($outOfRangeValue2)\")",
          "722:         }.getCause.getMessage",
          "725:       }",
          "726:     }",
          "727:   }",
          "",
          "[Removed Lines]",
          "716:         assert(msg.contains(",
          "717:           s\"\"\"The value ${outOfRangeValue1}L of the type \"BIGINT\" cannot be cast to \"INT\"\"\"\"))",
          "723:         assert(msg.contains(",
          "724:           s\"\"\"The value ${outOfRangeValue2}L of the type \"BIGINT\" cannot be cast to \"INT\"\"\"\"))",
          "",
          "[Added Lines]",
          "713:         val expectedMsg = \"Fail to insert a value of \\\"BIGINT\\\" type into the \\\"INT\\\" type column\" +",
          "714:           \" `b` due to an overflow.\"",
          "718:         assert(msg.contains(expectedMsg))",
          "724:         assert(msg.contains(expectedMsg))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "732:       withTable(\"t\") {",
          "733:         sql(\"create table t(b long) using parquet\")",
          "734:         val outOfRangeValue1 = Math.nextUp(Long.MaxValue)",
          "735:         var msg = intercept[SparkException] {",
          "736:           sql(s\"insert into t values(${outOfRangeValue1}D)\")",
          "737:         }.getCause.getMessage",
          "741:         val outOfRangeValue2 = Math.nextDown(Long.MinValue)",
          "742:         msg = intercept[SparkException] {",
          "743:           sql(s\"insert into t values(${outOfRangeValue2}D)\")",
          "744:         }.getCause.getMessage",
          "747:       }",
          "748:     }",
          "749:   }",
          "",
          "[Removed Lines]",
          "738:         assert(msg.contains(",
          "739:           s\"\"\"The value ${outOfRangeValue1}D of the type \"DOUBLE\" cannot be cast to \"BIGINT\"\"\"\"))",
          "745:         assert(msg.contains(",
          "746:           s\"\"\"The value ${outOfRangeValue2}D of the type \"DOUBLE\" cannot be cast to \"BIGINT\"\"\"\"))",
          "",
          "[Added Lines]",
          "735:         val expectedMsg = \"Fail to insert a value of \\\"DOUBLE\\\" type into the \\\"BIGINT\\\" type \" +",
          "736:           \"column `b` due to an overflow.\"",
          "740:         assert(msg.contains(expectedMsg))",
          "746:         assert(msg.contains(expectedMsg))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "754:       withTable(\"t\") {",
          "755:         sql(\"create table t(b decimal(3,2)) using parquet\")",
          "756:         val outOfRangeValue = \"123.45\"",
          "757:         val msg = intercept[SparkException] {",
          "758:           sql(s\"insert into t values(${outOfRangeValue})\")",
          "759:         }.getCause.getMessage",
          "761:       }",
          "762:     }",
          "763:   }",
          "",
          "[Removed Lines]",
          "760:         assert(msg.contains(\"cannot be represented as Decimal(3, 2)\"))",
          "",
          "[Added Lines]",
          "757:         val expectedMsg = \"Fail to insert a value of \\\"DECIMAL(5,2)\\\" type into the \" +",
          "758:           \"\\\"DECIMAL(3,2)\\\" type column `b` due to an overflow.\"",
          "762:         assert(msg.contains(expectedMsg))",
          "",
          "---------------"
        ]
      }
    }
  ]
}