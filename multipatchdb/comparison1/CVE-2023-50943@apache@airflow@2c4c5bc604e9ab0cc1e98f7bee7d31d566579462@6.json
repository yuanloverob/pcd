{
  "cve_id": "CVE-2023-50943",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows a potential attacker to poison the XCom data by bypassing the protection of \"enable_xcom_pickling=False\" configuration setting resulting in poisoned data after XCom deserialization. This vulnerability is considered low since it requires a DAG author to exploit it. Users are recommended to upgrade to version 2.8.1 or later, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
  "patch_info": {
    "commit_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "files": [
      "airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py"
    ],
    "message": "Stop deserializing pickle when enable_xcom_pickling is False (#36255)\n\n* Stop deserializing pickle when enable_xcom_pickling is False\n\n* Fix unit tests\n\n(cherry picked from commit 63e97abec5d56bc62a293c93f5227f364561e51c)",
    "before_after_code_files": [
      "airflow/models/xcom.py||airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py||tests/models/test_xcom.py"
    ]
  },
  "patch_diff": {
    "airflow/models/xcom.py||airflow/models/xcom.py": [
      "File: airflow/models/xcom.py -> airflow/models/xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "685:             except pickle.UnpicklingError:",
      "686:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "687:         else:",
      "693:     @staticmethod",
      "694:     def deserialize_value(result: XCom) -> Any:",
      "",
      "[Removed Lines]",
      "688:             try:",
      "689:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "690:             except (json.JSONDecodeError, UnicodeDecodeError):",
      "691:                 return pickle.loads(result.value)",
      "",
      "[Added Lines]",
      "688:             # Since xcom_pickling is disabled, we should only try to deserialize with JSON",
      "689:             return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "",
      "---------------"
    ],
    "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py": [
      "File: tests/api_connexion/schemas/test_xcom_schema.py -> tests/api_connexion/schemas/test_xcom_schema.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from airflow.models import DagRun, XCom",
      "31: from airflow.utils.dates import parse_execution_date",
      "32: from airflow.utils.session import create_session",
      "34: pytestmark = pytest.mark.db_test",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "188:     default_time = \"2016-04-02T21:00:00+00:00\"",
      "189:     default_time_parsed = parse_execution_date(default_time)",
      "191:     def test_serialize(self, create_xcom, session):",
      "192:         create_xcom(",
      "193:             dag_id=\"test_dag\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "192:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "208:             \"map_index\": -1,",
      "209:         }",
      "211:     def test_deserialize(self):",
      "212:         xcom_dump = {",
      "213:             \"key\": \"test_key\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "213:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------"
    ],
    "tests/models/test_xcom.py||tests/models/test_xcom.py": [
      "File: tests/models/test_xcom.py -> tests/models/test_xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "140:             ret_value = XCom.get_value(key=\"xcom_test3\", ti_key=ti_key, session=session)",
      "141:         assert ret_value == {\"key\": \"value\"}",
      "144:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"}):",
      "145:             XCom.set(",
      "146:                 key=\"xcom_test3\",",
      "",
      "[Removed Lines]",
      "143:     def test_xcom_deserialize_with_pickle_to_json_switch(self, task_instance, session):",
      "",
      "[Added Lines]",
      "143:     def test_xcom_deserialize_pickle_when_xcom_pickling_is_disabled(self, task_instance, session):",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "151:                 session=session,",
      "152:             )",
      "153:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"False\"}):",
      "163:     @conf_vars({(\"core\", \"xcom_enable_pickling\"): \"False\"})",
      "164:     def test_xcom_disable_pickle_type_fail_on_non_json(self, task_instance, session):",
      "",
      "[Removed Lines]",
      "154:             ret_value = XCom.get_one(",
      "155:                 key=\"xcom_test3\",",
      "156:                 dag_id=task_instance.dag_id,",
      "157:                 task_id=task_instance.task_id,",
      "158:                 run_id=task_instance.run_id,",
      "159:                 session=session,",
      "160:             )",
      "161:         assert ret_value == {\"key\": \"value\"}",
      "",
      "[Added Lines]",
      "154:             with pytest.raises(UnicodeDecodeError):",
      "155:                 XCom.get_one(",
      "156:                     key=\"xcom_test3\",",
      "157:                     dag_id=task_instance.dag_id,",
      "158:                     task_id=task_instance.task_id,",
      "159:                     run_id=task_instance.run_id,",
      "160:                     session=session,",
      "161:                 )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "8d76538d6e105947272b000581c6fabec20146b1",
      "candidate_info": {
        "commit_hash": "8d76538d6e105947272b000581c6fabec20146b1",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8d76538d6e105947272b000581c6fabec20146b1",
        "files": [
          "airflow/api_connexion/endpoints/dag_source_endpoint.py",
          "airflow/models/dagcode.py",
          "tests/api_connexion/endpoints/test_dag_source_endpoint.py"
        ],
        "message": "Check DAG read permission before accessing DAG code (#36257)\n\n(cherry picked from commit 30ea37e0d247ce54c2d25b115e807fdb0074d795)",
        "before_after_code_files": [
          "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
          "airflow/models/dagcode.py||airflow/models/dagcode.py",
          "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py": [
          "File: airflow/api_connexion/endpoints/dag_source_endpoint.py -> airflow/api_connexion/endpoints/dag_source_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: from __future__ import annotations",
          "19: from http import HTTPStatus",
          "21: from flask import Response, current_app, request",
          "22: from itsdangerous import BadSignature, URLSafeSerializer",
          "24: from airflow.api_connexion import security",
          "26: from airflow.api_connexion.schemas.dag_source_schema import dag_source_schema",
          "27: from airflow.auth.managers.models.resource_details import DagAccessEntity",
          "28: from airflow.models.dagcode import DagCode",
          "31: @security.requires_access_dag(\"GET\", DagAccessEntity.CODE)",
          "33:     \"\"\"Get source code using file token.\"\"\"",
          "34:     secret_key = current_app.config[\"SECRET_KEY\"]",
          "35:     auth_s = URLSafeSerializer(secret_key)",
          "36:     try:",
          "37:         path = auth_s.loads(file_token)",
          "39:     except (BadSignature, FileNotFoundError):",
          "40:         raise NotFound(\"Dag source not found\")",
          "",
          "[Removed Lines]",
          "25: from airflow.api_connexion.exceptions import NotFound",
          "32: def get_dag_source(*, file_token: str) -> Response:",
          "38:         dag_source = DagCode.code(path)",
          "",
          "[Added Lines]",
          "20: from typing import TYPE_CHECKING",
          "26: from airflow.api_connexion.exceptions import NotFound, PermissionDenied",
          "28: from airflow.api_connexion.security import get_readable_dags",
          "30: from airflow.models.dag import DagModel",
          "32: from airflow.utils.session import NEW_SESSION, provide_session",
          "34: if TYPE_CHECKING:",
          "35:     from sqlalchemy.orm import Session",
          "39: @provide_session",
          "40: def get_dag_source(*, file_token: str, session: Session = NEW_SESSION) -> Response:",
          "46:         dag_ids = session.query(DagModel.dag_id).filter(DagModel.fileloc == path).all()",
          "47:         readable_dags = get_readable_dags()",
          "48:         # Check if user has read access to all the DAGs defined in the file",
          "49:         if any(dag_id[0] not in readable_dags for dag_id in dag_ids):",
          "50:             raise PermissionDenied()",
          "51:         dag_source = DagCode.code(path, session=session)",
          "",
          "---------------"
        ],
        "airflow/models/dagcode.py||airflow/models/dagcode.py": [
          "File: airflow/models/dagcode.py -> airflow/models/dagcode.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "177:         return cls.code(fileloc)",
          "179:     @classmethod",
          "181:         \"\"\"Return source code for this DagCode object.",
          "183:         :return: source code as string",
          "184:         \"\"\"",
          "187:     @staticmethod",
          "188:     def _get_code_from_file(fileloc):",
          "",
          "[Removed Lines]",
          "180:     def code(cls, fileloc) -> str:",
          "185:         return cls._get_code_from_db(fileloc)",
          "",
          "[Added Lines]",
          "180:     @provide_session",
          "181:     def code(cls, fileloc, session: Session = NEW_SESSION) -> str:",
          "186:         return cls._get_code_from_db(fileloc, session)",
          "",
          "---------------"
        ],
        "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_dag_source_endpoint.py -> tests/api_connexion/endpoints/test_dag_source_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "35: ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))",
          "36: EXAMPLE_DAG_FILE = os.path.join(\"airflow\", \"example_dags\", \"example_bash_operator.py\")",
          "39: @pytest.fixture(scope=\"module\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "37: EXAMPLE_DAG_ID = \"example_bash_operator\"",
          "38: TEST_DAG_ID = \"latest_only\"",
          "39: NOT_READABLE_DAG_ID = \"latest_only_with_trigger\"",
          "40: TEST_MULTIPLE_DAGS_ID = \"dataset_produces_1\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "45:         role_name=\"Test\",",
          "46:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_CODE)],  # type: ignore",
          "47:     )",
          "48:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
          "50:     yield app",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "52:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
          "53:         TEST_DAG_ID,",
          "54:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
          "55:     )",
          "56:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
          "57:         EXAMPLE_DAG_ID,",
          "58:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
          "59:     )",
          "60:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
          "61:         TEST_MULTIPLE_DAGS_ID,",
          "62:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
          "63:     )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "80:     def test_should_respond_200_text(self, url_safe_serializer):",
          "81:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
          "82:         dagbag.sync_to_db()",
          "87:         response = self.client.get(",
          "88:             url, headers={\"Accept\": \"text/plain\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
          "89:         )",
          "",
          "[Removed Lines]",
          "83:         first_dag: DAG = next(iter(dagbag.dags.values()))",
          "84:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
          "86:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
          "",
          "[Added Lines]",
          "99:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
          "100:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
          "102:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "95:     def test_should_respond_200_json(self, url_safe_serializer):",
          "96:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
          "97:         dagbag.sync_to_db()",
          "102:         response = self.client.get(",
          "103:             url, headers={\"Accept\": \"application/json\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
          "104:         )",
          "",
          "[Removed Lines]",
          "98:         first_dag: DAG = next(iter(dagbag.dags.values()))",
          "99:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
          "101:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
          "",
          "[Added Lines]",
          "114:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
          "115:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
          "117:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "110:     def test_should_respond_406(self, url_safe_serializer):",
          "111:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
          "112:         dagbag.sync_to_db()",
          "116:         response = self.client.get(",
          "117:             url, headers={\"Accept\": \"image/webp\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
          "118:         )",
          "",
          "[Removed Lines]",
          "113:         first_dag: DAG = next(iter(dagbag.dags.values()))",
          "115:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
          "",
          "[Added Lines]",
          "129:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
          "131:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "151:             environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"},",
          "152:         )",
          "153:         assert response.status_code == 403",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "171:     def test_should_respond_403_not_readable(self, url_safe_serializer):",
          "172:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
          "173:         dagbag.sync_to_db()",
          "174:         dag: DAG = dagbag.dags[NOT_READABLE_DAG_ID]",
          "176:         response = self.client.get(",
          "177:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
          "178:             headers={\"Accept\": \"text/plain\"},",
          "179:             environ_overrides={\"REMOTE_USER\": \"test\"},",
          "180:         )",
          "181:         read_dag = self.client.get(",
          "182:             f\"/api/v1/dags/{NOT_READABLE_DAG_ID}\",",
          "183:             environ_overrides={\"REMOTE_USER\": \"test\"},",
          "184:         )",
          "185:         assert response.status_code == 403",
          "186:         assert read_dag.status_code == 403",
          "188:     def test_should_respond_403_some_dags_not_readable_in_the_file(self, url_safe_serializer):",
          "189:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
          "190:         dagbag.sync_to_db()",
          "191:         dag: DAG = dagbag.dags[TEST_MULTIPLE_DAGS_ID]",
          "193:         response = self.client.get(",
          "194:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
          "195:             headers={\"Accept\": \"text/plain\"},",
          "196:             environ_overrides={\"REMOTE_USER\": \"test\"},",
          "197:         )",
          "199:         read_dag = self.client.get(",
          "200:             f\"/api/v1/dags/{TEST_MULTIPLE_DAGS_ID}\",",
          "201:             environ_overrides={\"REMOTE_USER\": \"test\"},",
          "202:         )",
          "203:         assert response.status_code == 403",
          "204:         assert read_dag.status_code == 200",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0dba06d66064edab2144bfcb70cd253a31ce6bc7",
      "candidate_info": {
        "commit_hash": "0dba06d66064edab2144bfcb70cd253a31ce6bc7",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0dba06d66064edab2144bfcb70cd253a31ce6bc7",
        "files": [
          "airflow/models/dag.py"
        ],
        "message": "fix datetime reference in DAG.is_fixed_time_schedule (#36370)\n\n(cherry picked from commit 547ddf6317b3fc93e766b61daf11308b552e6d6b)",
        "before_after_code_files": [
          "airflow/models/dag.py||airflow/models/dag.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "874:         from croniter import croniter",
          "876:         cron = croniter(self.timetable._expression)",
          "879:         return next_b.minute == next_a.minute and next_b.hour == next_a.hour",
          "881:     def following_schedule(self, dttm):",
          "",
          "[Removed Lines]",
          "877:         next_a = cron.get_next(datetime.datetime)",
          "878:         next_b = cron.get_next(datetime.datetime)",
          "",
          "[Added Lines]",
          "877:         next_a = cron.get_next(datetime)",
          "878:         next_b = cron.get_next(datetime)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "171981fce86d8448a880914040680475b2e5edaa",
      "candidate_info": {
        "commit_hash": "171981fce86d8448a880914040680475b2e5edaa",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/171981fce86d8448a880914040680475b2e5edaa",
        "files": [
          "airflow/jobs/scheduler_job_runner.py"
        ],
        "message": "Fix the type hint for tis_query in _process_executor_events (#36655)\n\n(cherry picked from commit 98f5ce269acf7165b4c620f7a018d5ba34a7606e)",
        "before_after_code_files": [
          "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py": [
          "File: airflow/jobs/scheduler_job_runner.py -> airflow/jobs/scheduler_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "706:         query = select(TI).where(filter_for_tis).options(selectinload(TI.dag_model))",
          "707:         # row lock this entire set of taskinstances to make sure the scheduler doesn't fail when we have",
          "708:         # multi-schedulers",
          "710:             query,",
          "711:             of=TI,",
          "712:             session=session,",
          "714:         )",
          "716:         for ti in tis:",
          "717:             try_number = ti_primary_key_to_try_number_map[ti.key.primary]",
          "718:             buffer_key = ti.key.with_try_number(try_number)",
          "",
          "[Removed Lines]",
          "709:         tis: Iterator[TI] = with_row_locks(",
          "715:         tis = session.scalars(tis)",
          "",
          "[Added Lines]",
          "709:         tis_query: Query = with_row_locks(",
          "715:         tis: Iterator[TI] = session.scalars(tis_query)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b71d3882e944eecd64b5c4bf5fb42a3707496387",
      "candidate_info": {
        "commit_hash": "b71d3882e944eecd64b5c4bf5fb42a3707496387",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b71d3882e944eecd64b5c4bf5fb42a3707496387",
        "files": [
          "airflow/utils/event_scheduler.py"
        ],
        "message": "Avoid using dict as default value in call_regular_interval (#36608)\n\n(cherry picked from commit 9f275cf8900baac4e66eab3c5e8a1462a22f5ccc)",
        "before_after_code_files": [
          "airflow/utils/event_scheduler.py||airflow/utils/event_scheduler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/event_scheduler.py||airflow/utils/event_scheduler.py": [
          "File: airflow/utils/event_scheduler.py -> airflow/utils/event_scheduler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31:         delay: float,",
          "32:         action: Callable,",
          "33:         arguments=(),",
          "35:     ):",
          "36:         \"\"\"Call a function at (roughly) a given interval.\"\"\"",
          "",
          "[Removed Lines]",
          "34:         kwargs={},",
          "",
          "[Added Lines]",
          "34:         kwargs=None,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "43:             # Good enough for now",
          "44:             self.enter(delay, 1, repeat, args, kwargs)",
          "",
          "[Removed Lines]",
          "46:         self.enter(delay, 1, repeat, arguments, kwargs)",
          "",
          "[Added Lines]",
          "46:         self.enter(delay, 1, repeat, arguments, kwargs or {})",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "6d47a857436257ef9a478ddc83b578ccba05c9d6",
      "candidate_info": {
        "commit_hash": "6d47a857436257ef9a478ddc83b578ccba05c9d6",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/6d47a857436257ef9a478ddc83b578ccba05c9d6",
        "files": [
          "CONTRIBUTING.rst",
          "Dockerfile",
          "INSTALL",
          "airflow/utils/dot_renderer.py",
          "dev/breeze/src/airflow_breeze/global_constants.py",
          "docs/apache-airflow/extra-packages-ref.rst",
          "docs/docker-stack/build-arg-ref.rst",
          "docs/spelling_wordlist.txt",
          "images/breeze/output_prod-image_build.txt",
          "newsfragments/36647.significant.rst",
          "setup.cfg",
          "setup.py"
        ],
        "message": "Make `graphviz` dependency optional (#36647)\n\nThe `graphviz` dependency has been problematic as Airflow required\ndependency - especially for ARM-based installations. Graphviz\npackages require binary graphviz libraries - which is already a\nlimitation, but they also require to install graphviz Python\nbindings to be build and installed. This does not work for older\nLinux installation but - more importantly - when you try\nto install Graphviz libraries for Python 3.8, 3.9 for ARM M1\nMacBooks, the packages fail to install because Python bindings\ncompilation for M1 can only work for Python 3.10+.\n\nThere is not an easy solution for that except commenting out\ngraphviz dependency from setup.py, when you want to install Airflow\nfor Python 3.8, 3.9 for MacBook M1.\n\nHowever Graphviz is really used in two places:\n\n* when you want to render DAGs wia airflow CLI - either to an image\n  or directly to terminal (for terminals/systems supporting imgcat)\n\n* when you want to render ER diagram after you modified Airflow\n  models\n\nThe latter is a development-only feature, the former is production\nfeature, however it is a very niche one.\n\nThis PR turns rendering of the images in Airflow in optional feature\n(only working when graphviz python bindings are installed) and\neffectively turns graphviz into an optional extra (and removes it\nfrom requirements).\n\nThis is not a breaking change technically - the CLIs to render the\nDAGs is still there and IF you already have graphviz installed, it\nwill continue working as it did before. The only problem when it\ndoes not work is where you do not have graphviz installed for\nfresh installation and it will raise an error and inform that you need it.\n\nGraphviz will remain to be installed for most users:\n\n* the Airflow Image will still contain graphviz library, because\n  it is added there as extra\n* when previous version of Airflow has been installed already, then\n  graphviz library is already installed there and Airflow will\n  continue working as it did\n\nThe only change will be a new installation of new version of Airflow\nfrom the scratch, where graphviz will need to be specified as extra\nor installed separately in order to enable DAG rendering option.\n\nTaking into account this behaviour (which only requires to install\na graphviz package), this should not be considered as a breaking\nchange.\n\nExtracted from: #36537\n\n(cherry picked from commit 89f1737afb27f6e708c2e83e3d8e751d9a36f91e)",
        "before_after_code_files": [
          "airflow/utils/dot_renderer.py||airflow/utils/dot_renderer.py",
          "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py",
          "setup.cfg||setup.cfg",
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/dot_renderer.py||airflow/utils/dot_renderer.py": [
          "File: airflow/utils/dot_renderer.py -> airflow/utils/dot_renderer.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: \"\"\"Renderer DAG (tasks and dependencies) to the graphviz object.\"\"\"",
          "20: from __future__ import annotations",
          "22: from typing import TYPE_CHECKING, Any",
          "26: from airflow.exceptions import AirflowException",
          "27: from airflow.models.baseoperator import BaseOperator",
          "",
          "[Removed Lines]",
          "24: import graphviz",
          "",
          "[Added Lines]",
          "22: import warnings",
          "25: try:",
          "26:     import graphviz",
          "27: except ImportError:",
          "28:     warnings.warn(\"Could not import graphviz. Rendering graph to the graphical format will not be possible.\")",
          "29:     graphviz = None",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "151:     :param deps: List of DAG dependencies",
          "152:     :return: Graphviz object",
          "153:     \"\"\"",
          "154:     dot = graphviz.Digraph(graph_attr={\"rankdir\": \"LR\"})",
          "156:     for dag, dependencies in deps.items():",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "159:     if not graphviz:",
          "160:         raise AirflowException(",
          "161:             \"Could not import graphviz. Install the graphviz python package to fix this error.\"",
          "162:         )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "179:     :param tis: List of task instances",
          "180:     :return: Graphviz object",
          "181:     \"\"\"",
          "182:     dot = graphviz.Digraph(",
          "183:         dag.dag_id,",
          "184:         graph_attr={",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "191:     if not graphviz:",
          "192:         raise AirflowException(",
          "193:             \"Could not import graphviz. Install the graphviz python package to fix this error.\"",
          "194:         )",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/global_constants.py||dev/breeze/src/airflow_breeze/global_constants.py": [
          "File: dev/breeze/src/airflow_breeze/global_constants.py -> dev/breeze/src/airflow_breeze/global_constants.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "437:     \"ftp\",",
          "438:     \"google\",",
          "439:     \"google_auth\",",
          "440:     \"grpc\",",
          "441:     \"hashicorp\",",
          "442:     \"http\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "440:     \"graphviz\",",
          "",
          "---------------"
        ],
        "setup.cfg||setup.cfg": [
          "File: setup.cfg -> setup.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "108:     flask-wtf>=0.15",
          "109:     fsspec>=2023.10.0",
          "110:     google-re2>=1.0",
          "112:     gunicorn>=20.1.0",
          "113:     httpx",
          "114:     importlib_metadata>=1.7;python_version<\"3.9\"",
          "",
          "[Removed Lines]",
          "111:     graphviz>=0.12",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "318: ]",
          "319: doc_gen = [",
          "320:     \"eralchemy2\",",
          "321: ]",
          "322: flask_appbuilder_oauth = [",
          "323:     \"authlib>=1.0.0\",",
          "324:     # The version here should be upgraded at the same time as flask-appbuilder in setup.cfg",
          "325:     \"flask-appbuilder[oauth]==4.3.10\",",
          "326: ]",
          "327: kerberos = [",
          "328:     \"pykerberos>=1.1.13\",",
          "329:     \"requests_kerberos>=0.10.0\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "321:     \"graphviz>=0.12\",",
          "328: graphviz = [\"graphviz>=0.12\"]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "589:     \"deprecated_api\": deprecated_api,",
          "590:     \"github_enterprise\": flask_appbuilder_oauth,",
          "591:     \"google_auth\": flask_appbuilder_oauth,",
          "592:     \"kerberos\": kerberos,",
          "593:     \"ldap\": ldap,",
          "594:     \"leveldb\": leveldb,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "594:     \"graphviz\": graphviz,",
          "",
          "---------------"
        ]
      }
    }
  ]
}