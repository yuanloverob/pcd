{
  "cve_id": "CVE-2020-1936",
  "cve_desc": "A cross-site scripting issue was found in Apache Ambari Views. This was addressed in Apache Ambari 2.7.4.",
  "repo": "apache/ambari",
  "patch_hash": "a9cfdb9dcce63a3494c07c81ebb2cf8da218a210",
  "patch_info": {
    "commit_hash": "a9cfdb9dcce63a3494c07c81ebb2cf8da218a210",
    "repo": "apache/ambari",
    "commit_url": "https://github.com/apache/ambari/pull/3040/commits/a9cfdb9dcce63a3494c07c81ebb2cf8da218a210",
    "files": [
      "ambari-web/app/views/common/breadcrumbs_view.js"
    ],
    "message": "AMBARI-25329. Ambari breadcrumbs xss vulnerability",
    "before_after_code_files": [
      "ambari-web/app/views/common/breadcrumbs_view.js||ambari-web/app/views/common/breadcrumbs_view.js"
    ]
  },
  "patch_diff": {
    "ambari-web/app/views/common/breadcrumbs_view.js||ambari-web/app/views/common/breadcrumbs_view.js": [
      "File: ambari-web/app/views/common/breadcrumbs_view.js -> ambari-web/app/views/common/breadcrumbs_view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "149:   createLabel() {",
      "150:     let label = this.get('label');",
      "151:     let labelBindingPath = this.get('labelBindingPath');",
      "154:     this.set('formattedLabel', this.labelPostFormat(formattedLabel));",
      "155:   },",
      "",
      "[Removed Lines]",
      "153:     let formattedLabel = labelBindingPath ? App.get(_getLabelPathWithoutApp(labelBindingPath)) : label;",
      "",
      "[Added Lines]",
      "152:     let formattedLabel;",
      "154:     if (labelBindingPath) {",
      "155:       formattedLabel = Ember.Handlebars.Utils.escapeExpression(App.get(_getLabelPathWithoutApp(labelBindingPath)));",
      "156:     } else{",
      "157:       formattedLabel = label;",
      "158:     }",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "216:       }",
      "217:       currentState = currentState.get('parentState');",
      "218:     }",
      "220:     if (items.length) {",
      "221:       items.get('lastObject').setProperties({",
      "222:         disabled: true,",
      "",
      "[Removed Lines]",
      "219:     items = items.reverse().map(item => App.BreadcrumbItem.extend(item).create());",
      "",
      "[Added Lines]",
      "227:     items.reverse();",
      "228:     items.slice(1).forEach(item => item.label = Ember.Handlebars.Utils.escapeExpression(item.label));",
      "229:     items = items.map(item => App.BreadcrumbItem.extend(item).create());",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "b05961104ea272a6885d03ce6978eb03422479d4",
      "candidate_info": {
        "commit_hash": "b05961104ea272a6885d03ce6978eb03422479d4",
        "repo": "apache/ambari",
        "commit_url": "https://github.com/apache/ambari/commit/b05961104ea272a6885d03ce6978eb03422479d4",
        "files": [
          "ambari-agent/src/main/python/ambari_agent/HostCleanup.py"
        ],
        "message": "AMBARI-23469 HostCleanup.py script is failing with AttributeError: 'NoneType' object has no attribute 'get' (#3061)\n\nChange-Id: Id07397049da792bfa2f8e9c36e19871baf385fe5",
        "before_after_code_files": [
          "ambari-agent/src/main/python/ambari_agent/HostCleanup.py||ambari-agent/src/main/python/ambari_agent/HostCleanup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/ambari/pull/3633",
          "https://github.com/apache/ambari/pull/3631",
          "https://github.com/apache/ambari/pull/3637",
          "https://github.com/apache/ambari/pull/3632",
          "https://github.com/apache/ambari/pull/3634",
          "https://github.com/apache/ambari/pull/3635"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "ambari-agent/src/main/python/ambari_agent/HostCleanup.py||ambari-agent/src/main/python/ambari_agent/HostCleanup.py": [
          "File: ambari-agent/src/main/python/ambari_agent/HostCleanup.py -> ambari-agent/src/main/python/ambari_agent/HostCleanup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "141:       dirList = argMap.get(DIR_SECTION)",
          "142:       repoList = argMap.get(REPO_SECTION)",
          "143:       proc_map = argMap.get(PROCESS_SECTION)",
          "147:       alt_map = argMap.get(ALT_SECTION)",
          "148:       additionalDirList = self.get_additional_dirs()",
          "",
          "[Removed Lines]",
          "144:       procList = proc_map.get(PROCESS_KEY)",
          "145:       procUserList = proc_map.get(PROCESS_OWNER_KEY)",
          "146:       procIdentifierList = proc_map.get(PROCESS_IDENTIFIER_KEY)",
          "",
          "[Added Lines]",
          "144:       if proc_map:",
          "145:         procList = proc_map.get(PROCESS_KEY)",
          "146:         procUserList = proc_map.get(PROCESS_OWNER_KEY)",
          "147:         procIdentifierList = proc_map.get(PROCESS_IDENTIFIER_KEY)",
          "148:       else:",
          "149:         procList = []",
          "150:         procUserList = []",
          "151:         procIdentifierList = []",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a75ae27740a42b7ec08215024a35def87bccedb0",
      "candidate_info": {
        "commit_hash": "a75ae27740a42b7ec08215024a35def87bccedb0",
        "repo": "apache/ambari",
        "commit_url": "https://github.com/apache/ambari/commit/a75ae27740a42b7ec08215024a35def87bccedb0",
        "files": [
          "contrib/utils/perf/deploy-gce-perf-cluster.py"
        ],
        "message": "AMBARI-24614. deploy-gce-perf-cluster.py script does not work (aonishuk)",
        "before_after_code_files": [
          "contrib/utils/perf/deploy-gce-perf-cluster.py||contrib/utils/perf/deploy-gce-perf-cluster.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/ambari/pull/3633",
          "https://github.com/apache/ambari/pull/3631",
          "https://github.com/apache/ambari/pull/3637",
          "https://github.com/apache/ambari/pull/3632",
          "https://github.com/apache/ambari/pull/3634",
          "https://github.com/apache/ambari/pull/3635"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "contrib/utils/perf/deploy-gce-perf-cluster.py||contrib/utils/perf/deploy-gce-perf-cluster.py": [
          "File: contrib/utils/perf/deploy-gce-perf-cluster.py -> contrib/utils/perf/deploy-gce-perf-cluster.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "28: import socket",
          "30: cluster_prefix = \"perf\"",
          "33: public_hostname_script = \"foo\"",
          "34: hostname_script = \"foo\"",
          "",
          "[Removed Lines]",
          "31: ambari_repo_file_url = \"http://s3.amazonaws.com/dev.hortonworks.com/ambari/centos6/2.x/updates/2.5.0.0/ambaribn.repo\"",
          "",
          "[Added Lines]",
          "31: ambari_repo_file_url = \"http://s3.amazonaws.com/dev.hortonworks.com/ambari/centos7/2.x/updates/2.6.2.0/ambaribn.repo\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "279:   :param args: Command line args",
          "280:   :param number_of_nodes: Number of VMs to request.",
          "281:   \"\"\"",
          "284:                   \"Failed to create server, probably not enough resources!\", \"-tt\")",
          "285:   time.sleep(10)",
          "287:   # trying to create cluster with needed params",
          "290:                   \"Failed to create cluster VMs, probably not enough resources!\", \"-tt\")",
          "292:   # VMs are not accessible immediately",
          "",
          "[Removed Lines]",
          "282:   print \"Creating server VM {0}-server-{1} with xxlarge nodes on centos6...\".format(cluster_prefix, args.cluster_suffix)",
          "283:   execute_command(args, args.controller, \"/usr/sbin/gce up {0}-server-{1} 1 --centos6 --xxlarge --ex --disk-xxlarge --ssd\".format(cluster_prefix, args.cluster_suffix),",
          "288:   print \"Creating agent VMs {0}-agent-{1} with {2} xlarge nodes on centos6...\".format(cluster_prefix, args.cluster_suffix, str(number_of_nodes))",
          "289:   execute_command(args, args.controller, \"/usr/sbin/gce up {0}-agent-{1} {2} --centos6 --xlarge --ex --disk-xlarge\".format(cluster_prefix, args.cluster_suffix, str(number_of_nodes)),",
          "",
          "[Added Lines]",
          "282:   print \"Creating server VM {0}-server-{1} with xxlarge nodes on centos7...\".format(cluster_prefix, args.cluster_suffix)",
          "283:   execute_command(args, args.controller, \"/opt/gce-utils/gce up {0}-server-{1} 1 --centos7 --xlarge --ex --disk-xxlarge --ssd\".format(cluster_prefix, args.cluster_suffix),",
          "288:   print \"Creating agent VMs {0}-agent-{1} with {2} xlarge nodes on centos7...\".format(cluster_prefix, args.cluster_suffix, str(number_of_nodes))",
          "289:   execute_command(args, args.controller, \"/opt/gce-utils/gce up {0}-agent-{1} {2} --centos7 --xlarge --ex --disk-xlarge\".format(cluster_prefix, args.cluster_suffix, str(number_of_nodes)),",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "329:   # echo \"arg=value\" >> .../ambari.properties",
          "331:   contents = \"#!/bin/bash\\n\" + \\",
          "332:   \"wget -O /etc/yum.repos.d/ambari.repo {0}\\n\".format(ambari_repo_file_url) + \\",
          "333:   \"yum clean all; yum install git ambari-server -y\\n\" + \\",
          "334:   \"mkdir /home ; cd /home ; git clone https://github.com/apache/ambari.git ; cd ambari ; git checkout branch-2.5\\n\" + \\",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "332:   \"yum install wget -y\\n\" + \\",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "400:   # TODO, instead of cloning Ambari repo on each VM, do it on the server once and distribute to all of the agents.",
          "401:   contents = \"#!/bin/bash\\n\" + \\",
          "402:   \"wget -O /etc/yum.repos.d/ambari.repo {0}\\n\".format(ambari_repo_file_url) + \\",
          "403:   \"yum clean all; yum install krb5-workstation git ambari-agent -y\\n\" + \\",
          "404:   \"mkdir /home ; cd /home; git clone https://github.com/apache/ambari.git ; cd ambari ; git checkout branch-2.5\\n\" + \\",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "403:   \"yum install wget -y\\n\" + \\",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "476:   :param args: Command line args",
          "477:   :return: Mapping of VM host name to ip.",
          "478:   \"\"\"",
          "480:   out = execute_command(args, args.controller, gce_fqdb_cmd, \"Failed to get VMs list!\", \"-tt\")",
          "481:   lines = out.split('\\n')",
          "482:   #print \"LINES=\" + str(lines)",
          "",
          "[Removed Lines]",
          "479:   gce_fqdb_cmd = '/usr/sbin/gce fqdn {0}'.format(cluster_name)",
          "",
          "[Added Lines]",
          "481:   gce_fqdb_cmd = '/opt/gce-utils/gce fqdn {0}'.format(cluster_name)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "485:     for s in lines[2:]:  # Ignore non-meaningful lines",
          "486:       if not s:",
          "487:         continue",
          "489:       if match:",
          "490:         result[match.group(2)] = match.group(1)",
          "491:       else:",
          "",
          "[Removed Lines]",
          "488:       match = re.match(r'^([\\d\\.]*)\\s+([\\w\\.-]*)\\s+([\\w\\.-]*)\\s$', s, re.M)",
          "",
          "[Added Lines]",
          "490:       match = re.match(r'^([\\d\\.]*)\\s+([\\w\\.-]*)\\s+([\\w\\.-]*)\\s+$', s, re.M)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "193359448c30833d12c6344f7d4e3611e91670be",
      "candidate_info": {
        "commit_hash": "193359448c30833d12c6344f7d4e3611e91670be",
        "repo": "apache/ambari",
        "commit_url": "https://github.com/apache/ambari/commit/193359448c30833d12c6344f7d4e3611e91670be",
        "files": [
          "ambari-common/src/main/python/resource_management/libraries/providers/__init__.py",
          "ambari-common/src/main/python/resource_management/libraries/providers/execute_hdfs.py",
          "ambari-common/src/main/python/resource_management/libraries/resources/__init__.py",
          "ambari-common/src/main/python/resource_management/libraries/resources/execute_hdfs.py",
          "ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_namenode.py",
          "ambari-server/src/main/resources/stacks/BIGTOP/0.8/services/HDFS/package/scripts/hdfs_namenode.py",
          "ambari-server/src/test/python/stacks/2.0.6/HDFS/test_namenode.py"
        ],
        "message": "[AMBARI-25231] : Replace deprecated hadoop commands (#2962)",
        "before_after_code_files": [
          "ambari-common/src/main/python/resource_management/libraries/providers/__init__.py||ambari-common/src/main/python/resource_management/libraries/providers/__init__.py",
          "ambari-common/src/main/python/resource_management/libraries/providers/execute_hdfs.py||ambari-common/src/main/python/resource_management/libraries/providers/execute_hdfs.py",
          "ambari-common/src/main/python/resource_management/libraries/resources/__init__.py||ambari-common/src/main/python/resource_management/libraries/resources/__init__.py",
          "ambari-common/src/main/python/resource_management/libraries/resources/execute_hdfs.py||ambari-common/src/main/python/resource_management/libraries/resources/execute_hdfs.py",
          "ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_namenode.py||ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_namenode.py",
          "ambari-server/src/main/resources/stacks/BIGTOP/0.8/services/HDFS/package/scripts/hdfs_namenode.py||ambari-server/src/main/resources/stacks/BIGTOP/0.8/services/HDFS/package/scripts/hdfs_namenode.py",
          "ambari-server/src/test/python/stacks/2.0.6/HDFS/test_namenode.py||ambari-server/src/test/python/stacks/2.0.6/HDFS/test_namenode.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/ambari/pull/3633",
          "https://github.com/apache/ambari/pull/3631",
          "https://github.com/apache/ambari/pull/3637",
          "https://github.com/apache/ambari/pull/3632",
          "https://github.com/apache/ambari/pull/3634",
          "https://github.com/apache/ambari/pull/3635"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "ambari-common/src/main/python/resource_management/libraries/providers/__init__.py||ambari-common/src/main/python/resource_management/libraries/providers/__init__.py": [
          "File: ambari-common/src/main/python/resource_management/libraries/providers/__init__.py -> ambari-common/src/main/python/resource_management/libraries/providers/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "38:   ),",
          "39:   default=dict(",
          "40:     ExecuteHadoop=\"resource_management.libraries.providers.execute_hadoop.ExecuteHadoopProvider\",",
          "41:     TemplateConfig=\"resource_management.libraries.providers.template_config.TemplateConfigProvider\",",
          "42:     XmlConfig=\"resource_management.libraries.providers.xml_config.XmlConfigProvider\",",
          "43:     PropertiesFile=\"resource_management.libraries.providers.properties_file.PropertiesFileProvider\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "41:     ExecuteHDFS=\"resource_management.libraries.providers.execute_hdfs.ExecuteHDFSProvider\",",
          "",
          "---------------"
        ],
        "ambari-common/src/main/python/resource_management/libraries/providers/execute_hdfs.py||ambari-common/src/main/python/resource_management/libraries/providers/execute_hdfs.py": [
          "File: ambari-common/src/main/python/resource_management/libraries/providers/execute_hdfs.py -> ambari-common/src/main/python/resource_management/libraries/providers/execute_hdfs.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python",
          "2: \"\"\"",
          "3: Licensed to the Apache Software Foundation (ASF) under one",
          "4: or more contributor license agreements.  See the NOTICE file",
          "5: distributed with this work for additional information",
          "6: regarding copyright ownership.  The ASF licenses this file",
          "7: to you under the Apache License, Version 2.0 (the",
          "8: \"License\"); you may not use this file except in compliance",
          "9: with the License.  You may obtain a copy of the License at",
          "11:     http://www.apache.org/licenses/LICENSE-2.0",
          "13: Unless required by applicable law or agreed to in writing, software",
          "14: distributed under the License is distributed on an \"AS IS\" BASIS,",
          "15: WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
          "16: See the License for the specific language governing permissions and",
          "17: limitations under the License.",
          "19: Ambari Agent",
          "21: \"\"\"",
          "23: from resource_management.core.providers import Provider",
          "24: from resource_management.core.resources import Execute",
          "25: from resource_management.core.shell import quote_bash_args",
          "26: from resource_management.libraries.functions.format import format",
          "29: class ExecuteHDFSProvider(Provider):",
          "30:     def action_run(self):",
          "31:         conf_dir = self.resource.conf_dir",
          "32:         command = self.resource.command",
          "33:         if isinstance(command, (list, tuple)):",
          "34:             command = ' '.join(quote_bash_args(x) for x in command)",
          "35:         Execute(format(\"hdfs --config {conf_dir} {command}\"),",
          "36:                 user=self.resource.user,",
          "37:                 tries=self.resource.tries,",
          "38:                 try_sleep=self.resource.try_sleep,",
          "39:                 logoutput=self.resource.logoutput,",
          "40:                 path=self.resource.bin_dir,",
          "41:                 environment=self.resource.environment,",
          "42:                 )",
          "",
          "---------------"
        ],
        "ambari-common/src/main/python/resource_management/libraries/resources/__init__.py||ambari-common/src/main/python/resource_management/libraries/resources/__init__.py": [
          "File: ambari-common/src/main/python/resource_management/libraries/resources/__init__.py -> ambari-common/src/main/python/resource_management/libraries/resources/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: \"\"\"",
          "23: from resource_management.libraries.resources.execute_hadoop import *",
          "24: from resource_management.libraries.resources.template_config import *",
          "25: from resource_management.libraries.resources.xml_config import *",
          "26: from resource_management.libraries.resources.properties_file import *",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24: from resource_management.libraries.resources.execute_hdfs import *",
          "",
          "---------------"
        ],
        "ambari-common/src/main/python/resource_management/libraries/resources/execute_hdfs.py||ambari-common/src/main/python/resource_management/libraries/resources/execute_hdfs.py": [
          "File: ambari-common/src/main/python/resource_management/libraries/resources/execute_hdfs.py -> ambari-common/src/main/python/resource_management/libraries/resources/execute_hdfs.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python",
          "2: \"\"\"",
          "3: Licensed to the Apache Software Foundation (ASF) under one",
          "4: or more contributor license agreements.  See the NOTICE file",
          "5: distributed with this work for additional information",
          "6: regarding copyright ownership.  The ASF licenses this file",
          "7: to you under the Apache License, Version 2.0 (the",
          "8: \"License\"); you may not use this file except in compliance",
          "9: with the License.  You may obtain a copy of the License at",
          "11:     http://www.apache.org/licenses/LICENSE-2.0",
          "13: Unless required by applicable law or agreed to in writing, software",
          "14: distributed under the License is distributed on an \"AS IS\" BASIS,",
          "15: WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
          "16: See the License for the specific language governing permissions and",
          "17: limitations under the License.",
          "19: Ambari Agent",
          "21: \"\"\"",
          "23: _all__ = [\"ExecuteHDFS\"]",
          "24: from resource_management.core.base import Resource, ForcedListArgument, ResourceArgument",
          "27: class ExecuteHDFS(Resource):",
          "28:     action = ForcedListArgument(default=\"run\")",
          "29:     command = ResourceArgument(default=lambda obj: obj.name)",
          "30:     tries = ResourceArgument(default=1)",
          "31:     try_sleep = ResourceArgument(default=0)",
          "32:     user = ResourceArgument()",
          "33:     logoutput = ResourceArgument()",
          "34:     bin_dir = ResourceArgument(default=[])",
          "35:     environment = ResourceArgument(default={})",
          "36:     conf_dir = ResourceArgument()",
          "37:     actions = Resource.actions + [\"run\"]",
          "",
          "---------------"
        ],
        "ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_namenode.py||ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_namenode.py": [
          "File: ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_namenode.py -> ambari-server/src/main/resources/common-services/HDFS/2.1.0.2.0/package/scripts/hdfs_namenode.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: from resource_management.libraries.functions.check_process_status import check_process_status",
          "32: from resource_management.libraries.functions.namenode_ha_utils import get_name_service_by_hostname",
          "33: from resource_management.libraries.resources.execute_hadoop import ExecuteHadoop",
          "34: from resource_management.libraries.functions import Direction",
          "35: from ambari_commons import OSCheck, OSConst",
          "36: from ambari_commons.os_family_impl import OsFamilyImpl, OsFamilyFuncImpl",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "34: from resource_management.libraries.resources.execute_hdfs import ExecuteHDFS",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "452:     nn_refresh_cmd = format('dfsadmin -fs hdfs://{namenode_rpc} -refreshSuperUserGroupsConfiguration')",
          "453:   else:",
          "454:     nn_refresh_cmd = format('dfsadmin -fs {namenode_address} -refreshSuperUserGroupsConfiguration')",
          "460: @OsFamilyFuncImpl(os_family=OsFamilyImpl.DEFAULT)",
          "461: def decommission():",
          "",
          "[Removed Lines]",
          "455:   ExecuteHadoop(nn_refresh_cmd,",
          "456:                 user=params.hdfs_user,",
          "457:                 conf_dir=params.hadoop_conf_dir,",
          "458:                 bin_dir=params.hadoop_bin_dir)",
          "",
          "[Added Lines]",
          "456:   ExecuteHDFS(nn_refresh_cmd,",
          "457:               user=params.hdfs_user,",
          "458:               conf_dir=params.hadoop_conf_dir,",
          "459:               bin_dir=params.hadoop_bin_dir)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "491:       nn_refresh_cmd = format('dfsadmin -fs hdfs://{namenode_rpc} -refreshNodes')",
          "492:     else:",
          "493:       nn_refresh_cmd = format('dfsadmin -fs {namenode_address} -refreshNodes')",
          "499: @OsFamilyFuncImpl(os_family=OSConst.WINSRV_FAMILY)",
          "500: def decommission():",
          "",
          "[Removed Lines]",
          "494:     ExecuteHadoop(nn_refresh_cmd,",
          "495:                   user=hdfs_user,",
          "496:                   conf_dir=conf_dir,",
          "497:                   bin_dir=params.hadoop_bin_dir)",
          "",
          "[Added Lines]",
          "496:     ExecuteHDFS(nn_refresh_cmd,",
          "497:                 user=hdfs_user,",
          "498:                 conf_dir=conf_dir,",
          "499:                 bin_dir=params.hadoop_bin_dir)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "517:   if params.dfs_ha_enabled:",
          "518:     # due to a bug in hdfs, refreshNodes will not run on both namenodes so we",
          "519:     # need to execute each command scoped to a particular namenode",
          "521:   else:",
          "523:   Execute(nn_refresh_cmd, user=hdfs_user)",
          "",
          "[Removed Lines]",
          "520:     nn_refresh_cmd = format('cmd /c hadoop dfsadmin -fs hdfs://{namenode_rpc} -refreshNodes')",
          "522:     nn_refresh_cmd = format('cmd /c hadoop dfsadmin -fs {namenode_address} -refreshNodes')",
          "",
          "[Added Lines]",
          "523:     nn_refresh_cmd = format('cmd /c hdfs dfsadmin -fs hdfs://{namenode_rpc} -refreshNodes')",
          "525:     nn_refresh_cmd = format('cmd /c hdfs dfsadmin -fs {namenode_address} -refreshNodes')",
          "",
          "---------------"
        ],
        "ambari-server/src/main/resources/stacks/BIGTOP/0.8/services/HDFS/package/scripts/hdfs_namenode.py||ambari-server/src/main/resources/stacks/BIGTOP/0.8/services/HDFS/package/scripts/hdfs_namenode.py": [
          "File: ambari-server/src/main/resources/stacks/BIGTOP/0.8/services/HDFS/package/scripts/hdfs_namenode.py -> ambari-server/src/main/resources/stacks/BIGTOP/0.8/services/HDFS/package/scripts/hdfs_namenode.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "163:       nn_refresh_cmd = format('dfsadmin -fs hdfs://{namenode_rpc} -refreshNodes')",
          "164:     else:",
          "165:       nn_refresh_cmd = format('dfsadmin -refreshNodes')",
          "",
          "[Removed Lines]",
          "166:     ExecuteHadoop(nn_refresh_cmd,",
          "167:                   user=hdfs_user,",
          "168:                   conf_dir=conf_dir,",
          "169:                   kinit_override=True,",
          "170:                   bin_dir=params.hadoop_bin_dir)",
          "",
          "[Added Lines]",
          "166:     ExecuteHDFS(nn_refresh_cmd,",
          "167:                 user=hdfs_user,",
          "168:                 conf_dir=conf_dir,",
          "169:                 kinit_override=True,",
          "170:                 bin_dir=params.hadoop_bin_dir)",
          "",
          "---------------"
        ],
        "ambari-server/src/test/python/stacks/2.0.6/HDFS/test_namenode.py||ambari-server/src/test/python/stacks/2.0.6/HDFS/test_namenode.py": [
          "File: ambari-server/src/test/python/stacks/2.0.6/HDFS/test_namenode.py -> ambari-server/src/test/python/stacks/2.0.6/HDFS/test_namenode.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1052:                               group = 'hadoop',",
          "1053:                               )",
          "1054:     self.assertResourceCalled('Execute', '', user = 'hdfs')",
          "1059:     self.assertNoMoreResources()",
          "1061:   def test_decommission_update_files_only(self):",
          "",
          "[Removed Lines]",
          "1055:     self.assertResourceCalled('ExecuteHadoop', 'dfsadmin -fs hdfs://c6401.ambari.apache.org:8020 -refreshNodes',",
          "1056:                               user = 'hdfs',",
          "1057:                               conf_dir = '/etc/hadoop/conf',",
          "1058:                               bin_dir = '/usr/bin')",
          "",
          "[Added Lines]",
          "1055:     self.assertResourceCalled('ExecuteHDFS', 'dfsadmin -fs hdfs://c6401.ambari.apache.org:8020 -refreshNodes',",
          "1056:                               user='hdfs',",
          "1057:                               conf_dir='/etc/hadoop/conf',",
          "1058:                               bin_dir='/usr/bin')",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1088:                               group = 'hadoop',",
          "1089:                               )",
          "1090:     self.assertResourceCalled('Execute', '', user = 'hdfs')",
          "1095:     self.assertNoMoreResources()",
          "",
          "[Removed Lines]",
          "1091:     self.assertResourceCalled('ExecuteHadoop', 'dfsadmin -fs hdfs://c6401.ambari.apache.org:8020 -refreshNodes',",
          "1092:                               user = 'hdfs',",
          "1093:                               conf_dir = '/etc/hadoop/conf',",
          "1094:                               bin_dir = '/usr/bin')",
          "",
          "[Added Lines]",
          "1091:     self.assertResourceCalled('ExecuteHDFS', 'dfsadmin -fs hdfs://c6401.ambari.apache.org:8020 -refreshNodes',",
          "1092:                               user='hdfs',",
          "1093:                               conf_dir='/etc/hadoop/conf',",
          "1094:                               bin_dir='/usr/bin')",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1111:     self.assertResourceCalled('Execute', '/usr/bin/kinit -kt /etc/security/keytabs/nn.service.keytab nn/c6401.ambari.apache.org@EXAMPLE.COM;',",
          "1112:         user = 'hdfs',",
          "1113:     )",
          "1119:     self.assertNoMoreResources()",
          "1121:   def assert_configure_default(self):",
          "",
          "[Removed Lines]",
          "1114:     self.assertResourceCalled('ExecuteHadoop', 'dfsadmin -fs hdfs://c6401.ambari.apache.org:8020 -refreshNodes',",
          "1115:         bin_dir = '/usr/bin',",
          "1116:         conf_dir = '/etc/hadoop/conf',",
          "1117:         user = 'hdfs',",
          "1118:     )",
          "",
          "[Added Lines]",
          "1114:     self.assertResourceCalled('ExecuteHDFS', 'dfsadmin -fs hdfs://c6401.ambari.apache.org:8020 -refreshNodes',",
          "1115:                               bin_dir='/usr/bin',",
          "1116:                               conf_dir='/etc/hadoop/conf',",
          "1117:                               user='hdfs',",
          "1118:                               )",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1763:                          stack_version = self.STACK_VERSION,",
          "1764:                          target = RMFTestCase.TARGET_COMMON_SERVICES",
          "1765:                          )",
          "1771:       self.assertNoMoreResources()",
          "1773:   def test_reload_configs(self):",
          "",
          "[Removed Lines]",
          "1767:       self.assertResourceCalled('ExecuteHadoop', 'dfsadmin -fs hdfs://c6401.ambari.apache.org:8020 -refreshSuperUserGroupsConfiguration',",
          "1768:                                 user = 'hdfs',",
          "1769:                                 conf_dir = '/etc/hadoop/conf',",
          "1770:                                 bin_dir = '/usr/bin')",
          "",
          "[Added Lines]",
          "1766:       self.assertResourceCalled('ExecuteHDFS',",
          "1767:                                 'dfsadmin -fs hdfs://c6401.ambari.apache.org:8020 -refreshSuperUserGroupsConfiguration',",
          "1768:                                 user='hdfs',",
          "1769:                                 conf_dir='/etc/hadoop/conf',",
          "1770:                                 bin_dir='/usr/bin')",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ae5fe7aad56e6ba0a65a2ffacf15edddd7e32906",
      "candidate_info": {
        "commit_hash": "ae5fe7aad56e6ba0a65a2ffacf15edddd7e32906",
        "repo": "apache/ambari",
        "commit_url": "https://github.com/apache/ambari/commit/ae5fe7aad56e6ba0a65a2ffacf15edddd7e32906",
        "files": [
          "ambari-agent/src/main/python/ambari_agent/alerts/base_alert.py"
        ],
        "message": "AMBARI-25469. Bad UTF encoding on Alert listener receiver. (dvitiiuk via dgrinenko) (#3175)",
        "before_after_code_files": [
          "ambari-agent/src/main/python/ambari_agent/alerts/base_alert.py||ambari-agent/src/main/python/ambari_agent/alerts/base_alert.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/ambari/pull/3633",
          "https://github.com/apache/ambari/pull/3631",
          "https://github.com/apache/ambari/pull/3637",
          "https://github.com/apache/ambari/pull/3632",
          "https://github.com/apache/ambari/pull/3634",
          "https://github.com/apache/ambari/pull/3635"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "ambari-agent/src/main/python/ambari_agent/alerts/base_alert.py||ambari-agent/src/main/python/ambari_agent/alerts/base_alert.py": [
          "File: ambari-agent/src/main/python/ambari_agent/alerts/base_alert.py -> ambari-agent/src/main/python/ambari_agent/alerts/base_alert.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "185:       data['text'] = \"There is a problem with the alert definition: {0}\".format(str(exception))",
          "186:     finally:",
          "187:       # put the alert into the collector so it can be collected on the next run",
          "188:       self.collector.put(self.cluster_name, data)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "188:       data['text'] = data['text'].replace('\\x00', '')",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ac98aab0ce0ffeb178b60bc51e20c6331e86422c",
      "candidate_info": {
        "commit_hash": "ac98aab0ce0ffeb178b60bc51e20c6331e86422c",
        "repo": "apache/ambari",
        "commit_url": "https://github.com/apache/ambari/commit/ac98aab0ce0ffeb178b60bc51e20c6331e86422c",
        "files": [
          "ambari-web/app/controllers/wizard/step7_controller.js"
        ],
        "message": "AMBARI-24359 Back and forth on All Configurations Page has multiple tabs marked as active",
        "before_after_code_files": [
          "ambari-web/app/controllers/wizard/step7_controller.js||ambari-web/app/controllers/wizard/step7_controller.js"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/ambari/pull/3633",
          "https://github.com/apache/ambari/pull/3631",
          "https://github.com/apache/ambari/pull/3637",
          "https://github.com/apache/ambari/pull/3632",
          "https://github.com/apache/ambari/pull/3634",
          "https://github.com/apache/ambari/pull/3635"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "ambari-web/app/controllers/wizard/step7_controller.js||ambari-web/app/controllers/wizard/step7_controller.js": [
          "File: ambari-web/app/controllers/wizard/step7_controller.js -> ambari-web/app/controllers/wizard/step7_controller.js",
          "--- Hunk 1 ---",
          "[Context before]",
          "1904:   },",
          "1906:   selectService: function (event) {",
          "1908:     this.set('selectedService', event.context);",
          "1909:     var activeTabs = this.get('tabs').findProperty('isActive', true);",
          "1910:     if (activeTabs) {",
          "",
          "[Removed Lines]",
          "1907:     event.context.set('isActive', true);",
          "",
          "[Added Lines]",
          "1907:     this.get('stepConfigs').forEach((service) => {",
          "1908:       service.set('isActive', service.get('serviceName') === event.context.serviceName);",
          "1909:     });",
          "",
          "---------------"
        ]
      }
    }
  ]
}