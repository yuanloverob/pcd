{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "d622f3133c1acd3f00aba0108838fdba826cef9b",
      "candidate_info": {
        "commit_hash": "d622f3133c1acd3f00aba0108838fdba826cef9b",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/d622f3133c1acd3f00aba0108838fdba826cef9b",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/percentiles.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/aggregate/PercentileSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/PercentileQuerySuite.scala"
        ],
        "message": "[SPARK-39427][SQL] Disable ANSI intervals in the percentile functions\n\nIn the PR, I propose to don't support ANSI intervals by the percentile functions, and remove the YearMonthIntervalType and DayTimeIntervalType types from the list of input types. I propose to properly support ANSI intervals and enable them back after that.\n\nTo don't confuse users by results of the percentile functions when inputs are ANSI intervals. At the moment, the functions return DOUBLE (or ARRAY OF DAUBLE) type independently from inputs. In the case of ANSI intervals, the functions should return ANSI interval too.\n\nNo, since the functions haven't released yet.\n\nBy running affected test suites:\n```\n$ build/sbt \"sql/test:testOnly org.apache.spark.sql.expressions.ExpressionInfoSuite\"\n$ build/sbt \"sql/testOnly *ExpressionsSchemaSuite\"\n$ build/sbt \"sql/testOnly org.apache.spark.sql.SQLQueryTestSuite\"\n$ build/sbt \"test:testOnly *PercentileSuite\"\n$ build/sbt \"test:testOnly *PercentileQuerySuite\"\n```\nand checked manually that ANSI intervals are not supported as input types:\n```sql\nspark-sql> SELECT percentile(col, 0.5) FROM VALUES (INTERVAL '0' MONTH), (INTERVAL '10' MONTH) AS tab(col);\nError in query: cannot resolve 'percentile(tab.col, CAST(0.5BD AS DOUBLE), 1L)' due to data type mismatch: argument 1 requires numeric type, however, 'tab.col' is of interval month type.; line 1 pos 7;\n```\n\nCloses #36817 from MaxGekk/percentile-disable-ansi-interval.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>\n(cherry picked from commit ee24847ad100139628a9bffe45f711bdebaa0170)\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/percentiles.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/percentiles.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/aggregate/PercentileSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/aggregate/PercentileSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/PercentileQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/PercentileQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/percentiles.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/percentiles.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/percentiles.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/percentiles.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "67:       case _: ArrayType => ArrayType(DoubleType, false)",
          "68:       case _ => DoubleType",
          "69:     }",
          "72:   }",
          "",
          "[Removed Lines]",
          "70:     Seq(TypeCollection(NumericType, YearMonthIntervalType, DayTimeIntervalType),",
          "71:       percentageExpType, IntegralType)",
          "",
          "[Added Lines]",
          "70:     Seq(NumericType, percentageExpType, IntegralType)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "288:   usage =",
          "289:     \"\"\"",
          "290:       _FUNC_(col, percentage [, frequency]) - Returns the exact percentile value of numeric",
          "292:        between 0.0 and 1.0. The value of frequency should be positive integral",
          "293:       _FUNC_(col, array(percentage1 [, percentage2]...) [, frequency]) - Returns the exact",
          "294:       percentile value array of numeric column `col` at the given percentage(s). Each value",
          "",
          "[Removed Lines]",
          "291:        or ansi interval column `col` at the given percentage. The value of percentage must be",
          "",
          "[Added Lines]",
          "290:        column `col` at the given percentage. The value of percentage must be",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "301:        3.0",
          "302:       > SELECT _FUNC_(col, array(0.25, 0.75)) FROM VALUES (0), (10) AS tab(col);",
          "303:        [2.5,7.5]",
          "308:   \"\"\",",
          "309:   group = \"agg_funcs\",",
          "310:   since = \"2.1.0\")",
          "",
          "[Removed Lines]",
          "304:       > SELECT _FUNC_(col, 0.5) FROM VALUES (INTERVAL '0' MONTH), (INTERVAL '10' MONTH) AS tab(col);",
          "305:        5.0",
          "306:       > SELECT _FUNC_(col, array(0.2, 0.5)) FROM VALUES (INTERVAL '0' SECOND), (INTERVAL '10' SECOND) AS tab(col);",
          "307:        [2000000.0,5000000.0]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/aggregate/PercentileSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/aggregate/PercentileSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/aggregate/PercentileSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/aggregate/PercentileSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "170:       val child = AttributeReference(\"a\", dataType)()",
          "171:       val percentile = new Percentile(child, percentage)",
          "172:       assertEqual(percentile.checkInputDataTypes(),",
          "175:     }",
          "177:     val invalidFrequencyDataTypes = Seq(FloatType, DoubleType, BooleanType,",
          "",
          "[Removed Lines]",
          "173:         TypeCheckFailure(s\"argument 1 requires (numeric or interval year to month or \" +",
          "174:           s\"interval day to second) type, however, 'a' is of ${dataType.simpleString} type.\"))",
          "",
          "[Added Lines]",
          "173:         TypeCheckFailure(s\"argument 1 requires numeric type,\" +",
          "174:           s\" however, 'a' is of ${dataType.simpleString} type.\"))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "184:       val frq = AttributeReference(\"frq\", frequencyType)()",
          "185:       val percentile = new Percentile(child, percentage, frq)",
          "186:       assertEqual(percentile.checkInputDataTypes(),",
          "189:     }",
          "191:     for(dataType <- validDataTypes;",
          "",
          "[Removed Lines]",
          "187:         TypeCheckFailure(s\"argument 1 requires (numeric or interval year to month or \" +",
          "188:           s\"interval day to second) type, however, 'a' is of ${dataType.simpleString} type.\"))",
          "",
          "[Added Lines]",
          "187:         TypeCheckFailure(s\"argument 1 requires numeric type,\" +",
          "188:           s\" however, 'a' is of ${dataType.simpleString} type.\"))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/PercentileQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/PercentileQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/PercentileQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/PercentileQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "30:   private val table = \"percentile_test\"",
          "33:     withTempView(table) {",
          "34:       Seq((Period.ofMonths(100), Duration.ofSeconds(100L)),",
          "35:         (Period.ofMonths(200), Duration.ofSeconds(200L)),",
          "36:         (Period.ofMonths(300), Duration.ofSeconds(300L)))",
          "37:         .toDF(\"col1\", \"col2\").createOrReplaceTempView(table)",
          "39:         spark.sql(",
          "40:           s\"\"\"SELECT",
          "47:     }",
          "48:   }",
          "49: }",
          "",
          "[Removed Lines]",
          "32:   test(\"SPARK-37138: Support Ansi Interval type in Percentile\") {",
          "38:       checkAnswer(",
          "41:              |  CAST(percentile(col1, 0.5) AS STRING),",
          "42:              |  SUM(null),",
          "43:              |  CAST(percentile(col2, 0.5) AS STRING)",
          "44:              |FROM $table",
          "45:            \"\"\".stripMargin),",
          "46:         Row(\"200.0\", null, \"2.0E8\"))",
          "",
          "[Added Lines]",
          "32:   test(\"SPARK-37138, SPARK-39427: Disable Ansi Interval type in Percentile\") {",
          "38:       val e = intercept[AnalysisException] {",
          "41:             |  CAST(percentile(col1, 0.5) AS STRING),",
          "42:             |  SUM(null),",
          "43:             |  CAST(percentile(col2, 0.5) AS STRING)",
          "44:             |FROM $table\"\"\".stripMargin).collect()",
          "45:       }",
          "46:       assert(e.getMessage.contains(\"data type mismatch\"))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "88f8ac6b55e9a68161aa275dc379bd8167ef29c1",
      "candidate_info": {
        "commit_hash": "88f8ac6b55e9a68161aa275dc379bd8167ef29c1",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/88f8ac6b55e9a68161aa275dc379bd8167ef29c1",
        "files": [
          "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala",
          "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala"
        ],
        "message": "[SPARK-40065][K8S] Mount ConfigMap on executors with non-default profile as well\n\n### What changes were proposed in this pull request?\n\nThis fixes a bug where ConfigMap is not mounted on executors if they are under a non-default resource profile.\n\n### Why are the changes needed?\n\nWhen `spark.kubernetes.executor.disableConfigMap` is `false`, expected behavior is that the ConfigMap is mounted regardless of executor's resource profile. However, it is not mounted if the resource profile is non-default.\n\n### Does this PR introduce _any_ user-facing change?\n\nExecutors with non-default resource profile will have the ConfigMap mounted that was missing before if `spark.kubernetes.executor.disableConfigMap` is `false` or default. If certain users need to keep that behavior for some reason, they would need to explicitly set `spark.kubernetes.executor.disableConfigMap` to `true`.\n\n### How was this patch tested?\n\nA new test case is added just below the existing ConfigMap test case.\n\nCloses #37504 from nsuke/SPARK-40065.\n\nAuthored-by: Aki Sukegawa <nsuke@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 41ca6299eff4155aa3ac28656fe96501a7573fb0)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala||resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala",
          "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala||resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala||resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala": [
          "File: resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala -> resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "246:           .build()",
          "247:       }.getOrElse(executorContainerWithConfVolume)",
          "248:     } else {",
          "250:     }",
          "251:     val containerWithLifecycle =",
          "252:       if (!kubernetesConf.workerDecommissioning) {",
          "",
          "[Removed Lines]",
          "249:       executorContainer",
          "",
          "[Added Lines]",
          "249:       executorContainerWithConfVolume",
          "",
          "---------------"
        ],
        "resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala||resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala": [
          "File: resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala -> resource-managers/kubernetes/core/src/test/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStepSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "372:     assert(!SecretVolumeUtils.podHasVolume(podConfigured.pod, SPARK_CONF_VOLUME_EXEC))",
          "373:   }",
          "375:   test(\"SPARK-35482: user correct block manager port for executor pods\") {",
          "376:     try {",
          "377:       val initPod = SparkPod.initialPod()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "375:   test(\"SPARK-40065 Mount configmap on executors with non-default profile as well\") {",
          "376:     val baseDriverPod = SparkPod.initialPod()",
          "377:     val rp = new ResourceProfileBuilder().build()",
          "378:     val step = new BasicExecutorFeatureStep(newExecutorConf(), new SecurityManager(baseConf), rp)",
          "379:     val podConfigured = step.configurePod(baseDriverPod)",
          "380:     assert(SecretVolumeUtils.containerHasVolume(podConfigured.container,",
          "381:       SPARK_CONF_VOLUME_EXEC, SPARK_CONF_DIR_INTERNAL))",
          "382:     assert(SecretVolumeUtils.podHasVolume(podConfigured.pod, SPARK_CONF_VOLUME_EXEC))",
          "383:   }",
          "385:   test(\"SPARK-40065 Disable configmap volume on executor pod's container (non-default profile)\") {",
          "386:     baseConf.set(KUBERNETES_EXECUTOR_DISABLE_CONFIGMAP, true)",
          "387:     val baseDriverPod = SparkPod.initialPod()",
          "388:     val rp = new ResourceProfileBuilder().build()",
          "389:     val step = new BasicExecutorFeatureStep(newExecutorConf(), new SecurityManager(baseConf), rp)",
          "390:     val podConfigured = step.configurePod(baseDriverPod)",
          "391:     assert(!SecretVolumeUtils.containerHasVolume(podConfigured.container,",
          "392:       SPARK_CONF_VOLUME_EXEC, SPARK_CONF_DIR_INTERNAL))",
          "393:     assert(!SecretVolumeUtils.podHasVolume(podConfigured.pod, SPARK_CONF_VOLUME_EXEC))",
          "394:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4c09616f72d563f6c731654c8d69a8b36e40983a",
      "candidate_info": {
        "commit_hash": "4c09616f72d563f6c731654c8d69a8b36e40983a",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/4c09616f72d563f6c731654c8d69a8b36e40983a",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala"
        ],
        "message": "[SPARK-38085][SQL][FOLLOWUP] Do not fail too early for DeleteFromTable\n\n### What changes were proposed in this pull request?\n\n`DeleteFromTable` has been in Spark for a long time and there are existing Spark extensions to compile `DeleteFromTable` to physical plans. However, the new analyzer rule `RewriteDeleteFromTable` fails very early if the v2 table does not support delete. This breaks certain Spark extensions which can still execute `DeleteFromTable` for certain v2 tables.\n\nThis PR simply removes the error throwing in `RewriteDeleteFromTable`. It's a safe change because:\n1. the new delete-related rules only match v2 table with `SupportsRowLevelOperations`, so won't be affected by this change\n2. the planner rule will fail eventually if the v2 table doesn't support deletion. Spark eagerly executes commands so Spark users can still see this error immediately.\n\n### Why are the changes needed?\n\nTo not break existing Spark extesions.\n\n### Does this PR introduce _any_ user-facing change?\n\nno\n\n### How was this patch tested?\n\nexisting tests\n\nCloses #36402 from cloud-fan/follow.\n\nLead-authored-by: Wenchen Fan <wenchen@databricks.com>\nCo-authored-by: Wenchen Fan <cloud0fan@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 5630f700768432396a948376f5b46b00d4186e1b)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/RewriteDeleteFromTable.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import org.apache.spark.sql.connector.catalog.{SupportsDelete, SupportsRowLevelOperations, TruncatableTable}",
          "24: import org.apache.spark.sql.connector.write.RowLevelOperation.Command.DELETE",
          "25: import org.apache.spark.sql.connector.write.RowLevelOperationTable",
          "27: import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation",
          "28: import org.apache.spark.sql.util.CaseInsensitiveStringMap",
          "",
          "[Removed Lines]",
          "26: import org.apache.spark.sql.errors.QueryCompilationErrors",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "53:           d",
          "58:         case _ =>",
          "59:           d",
          "60:       }",
          "",
          "[Removed Lines]",
          "55:         case DataSourceV2Relation(t, _, _, _, _) =>",
          "56:           throw QueryCompilationErrors.tableDoesNotSupportDeletesError(t)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "167f3ff4d752c6f51b71a38378deb47c97f745f0",
      "candidate_info": {
        "commit_hash": "167f3ff4d752c6f51b71a38378deb47c97f745f0",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/167f3ff4d752c6f51b71a38378deb47c97f745f0",
        "files": [
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala"
        ],
        "message": "[SPARK-40152][SQL][TESTS] Move tests from SplitPart to elementAt\n\nMove tests from SplitPart to elementAt in CollectionExpressionsSuite.\n\nSimplify test.\n\nNo.\n\nN/A.\n\nCloses #37637 from wangyum/SPARK-40152-3.\n\nAuthored-by: Yuming Wang <yumwang@ebay.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 06997d6eb73f271aede5b159d86d1db80a73b89f)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1535:     }",
          "1536:     checkEvaluation(ElementAt(mb0, Literal(Array[Byte](2, 1), BinaryType)), \"2\")",
          "1537:     checkEvaluation(ElementAt(mb0, Literal(Array[Byte](3, 4))), null)",
          "1538:   }",
          "1540:   test(\"correctly handles ElementAt nullability for arrays\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1540:     val delimiter = Literal.create(\".\", StringType)",
          "1541:     val str = StringSplitSQL(Literal.create(\"11.12.13\", StringType), delimiter)",
          "1542:     val outOfBoundValue = Some(Literal.create(\"\", StringType))",
          "1544:     checkEvaluation(ElementAt(str, Literal(3), outOfBoundValue), UTF8String.fromString(\"13\"))",
          "1545:     checkEvaluation(ElementAt(str, Literal(1), outOfBoundValue), UTF8String.fromString(\"11\"))",
          "1546:     checkEvaluation(ElementAt(str, Literal(10), outOfBoundValue), UTF8String.fromString(\"\"))",
          "1547:     checkEvaluation(ElementAt(str, Literal(-10), outOfBoundValue), UTF8String.fromString(\"\"))",
          "1549:     checkEvaluation(ElementAt(StringSplitSQL(Literal.create(null, StringType), delimiter),",
          "1550:       Literal(1), outOfBoundValue), null)",
          "1551:     checkEvaluation(ElementAt(StringSplitSQL(Literal.create(\"11.12.13\", StringType),",
          "1552:       Literal.create(null, StringType)), Literal(1), outOfBoundValue), null)",
          "1554:     checkExceptionInExpression[Exception](",
          "1555:       ElementAt(str, Literal(0), outOfBoundValue), \"The index 0 is invalid\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2532:           Date.valueOf(\"2017-02-12\")))",
          "2533:     }",
          "2534:   }",
          "2555: }",
          "",
          "[Removed Lines]",
          "2536:   test(\"SplitPart\") {",
          "2537:     val delimiter = Literal.create(\".\", StringType)",
          "2538:     val str = StringSplitSQL(Literal.create(\"11.12.13\", StringType), delimiter)",
          "2539:     val outOfBoundValue = Some(Literal.create(\"\", StringType))",
          "2541:     checkEvaluation(ElementAt(str, Literal(3), outOfBoundValue), UTF8String.fromString(\"13\"))",
          "2542:     checkEvaluation(ElementAt(str, Literal(1), outOfBoundValue), UTF8String.fromString(\"11\"))",
          "2543:     checkEvaluation(ElementAt(str, Literal(10), outOfBoundValue), UTF8String.fromString(\"\"))",
          "2544:     checkEvaluation(ElementAt(str, Literal(-10), outOfBoundValue), UTF8String.fromString(\"\"))",
          "2546:     checkEvaluation(ElementAt(StringSplitSQL(Literal.create(null, StringType), delimiter),",
          "2547:       Literal(1), outOfBoundValue), null)",
          "2548:     checkEvaluation(ElementAt(StringSplitSQL(Literal.create(\"11.12.13\", StringType),",
          "2549:       Literal.create(null, StringType)), Literal(1), outOfBoundValue), null)",
          "2551:     intercept[Exception] {",
          "2552:       checkEvaluation(ElementAt(str, Literal(0), outOfBoundValue), null)",
          "2553:     }.getMessage.contains(\"The index 0 is invalid\")",
          "2554:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "92e82fdf8e2faec5add61e2448f11272dfb19c6e",
      "candidate_info": {
        "commit_hash": "92e82fdf8e2faec5add61e2448f11272dfb19c6e",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/92e82fdf8e2faec5add61e2448f11272dfb19c6e",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala"
        ],
        "message": "[SPARK-39293][SQL] Fix the accumulator of ArrayAggregate to handle complex types properly\n\n### What changes were proposed in this pull request?\n\nFix the accumulator of `ArrayAggregate` to handle complex types properly.\n\nThe accumulator of `ArrayAggregate` should copy the intermediate result if string, struct, array, or map.\n\n### Why are the changes needed?\n\nIf the intermediate data of `ArrayAggregate` holds reusable data, the result will be duplicated.\n\n```scala\nimport org.apache.spark.sql.functions._\n\nval reverse = udf((s: String) => s.reverse)\n\nval df = Seq(Array(\"abc\", \"def\")).toDF(\"array\")\nval testArray = df.withColumn(\n  \"agg\",\n  aggregate(\n    col(\"array\"),\n    array().cast(\"array<string>\"),\n    (acc, s) => concat(acc, array(reverse(s)))))\n\naggArray.show(truncate=false)\n```\n\nshould be:\n\n```\n+----------+----------+\n|array     |agg       |\n+----------+----------+\n|[abc, def]|[cba, fed]|\n+----------+----------+\n```\n\nbut:\n\n```\n+----------+----------+\n|array     |agg       |\n+----------+----------+\n|[abc, def]|[fed, fed]|\n+----------+----------+\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, this fixes the correctness issue.\n\n### How was this patch tested?\n\nAdded a test.\n\nCloses #36674 from ueshin/issues/SPARK-39293/array_aggregate.\n\nAuthored-by: Takuya UESHIN <ueshin@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit d6a11cb4b411c8136eb241aac167bc96990f5421)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "826:       var i = 0",
          "827:       while (i < arr.numElements()) {",
          "828:         elementVar.value.set(arr.get(i, elementVar.dataType))",
          "830:         i += 1",
          "831:       }",
          "832:       accForFinishVar.value.set(accForMergeVar.value.get)",
          "",
          "[Removed Lines]",
          "829:         accForMergeVar.value.set(mergeForEval.eval(input))",
          "",
          "[Added Lines]",
          "829:         accForMergeVar.value.set(InternalRow.copyValue(mergeForEval.eval(input)))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2933:     checkAnswer(test10, Row(Array(Row(\"cbaihg\"), Row(\"fedlkj\"))) :: Nil)",
          "2934:   }",
          "2936:   test(\"SPARK-34882: Aggregate with multiple distinct null sensitive aggregators\") {",
          "2937:     withUserDefinedFunction((\"countNulls\", true)) {",
          "2938:       spark.udf.register(\"countNulls\", udaf(new Aggregator[JLong, JLong, JLong] {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2936:   test(\"SPARK-39293: The accumulator of ArrayAggregate to handle complex types properly\") {",
          "2937:     val reverse = udf((s: String) => s.reverse)",
          "2939:     val df = Seq(Array(\"abc\", \"def\")).toDF(\"array\")",
          "2940:     val testArray = df.select(",
          "2941:       aggregate(",
          "2942:         col(\"array\"),",
          "2943:         array().cast(\"array<string>\"),",
          "2944:         (acc, s) => concat(acc, array(reverse(s)))))",
          "2945:     checkAnswer(testArray, Row(Array(\"cba\", \"fed\")) :: Nil)",
          "2947:     val testMap = df.select(",
          "2948:       aggregate(",
          "2949:         col(\"array\"),",
          "2950:         map().cast(\"map<string, string>\"),",
          "2951:         (acc, s) => map_concat(acc, map(s, reverse(s)))))",
          "2952:     checkAnswer(testMap, Row(Map(\"abc\" -> \"cba\", \"def\" -> \"fed\")) :: Nil)",
          "2953:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}