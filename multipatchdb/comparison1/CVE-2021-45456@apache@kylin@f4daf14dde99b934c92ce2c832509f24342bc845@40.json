{
  "cve_id": "CVE-2021-45456",
  "cve_desc": "Apache kylin checks the legitimacy of the project before executing some commands with the project name passed in by the user. There is a mismatch between what is being checked and what is being used as the shell command argument in DiagnosisService. This may cause an illegal project name to pass the check and perform the following steps, resulting in a command injection vulnerability. This issue affects Apache Kylin 4.0.0.",
  "repo": "apache/kylin",
  "patch_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
  "patch_info": {
    "commit_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
    "repo": "apache/kylin",
    "commit_url": "https://github.com/apache/kylin/commit/f4daf14dde99b934c92ce2c832509f24342bc845",
    "files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "server/src/main/webapp/WEB-INF/web.xml"
    ],
    "message": "test fix",
    "before_after_code_files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java"
    ]
  },
  "patch_diff": {
    "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "3403:     public String getKerberosPrincipal() {",
      "3404:         return getOptional(\"kylin.kerberos.principal\");",
      "3405:     }",
      "3406: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "3407:     public String getEncryptCipherIvSpec() {",
      "3408:         return getOptional(\"kylin.security.encrypt.cipher.ivSpec\", \"AAAAAAAAAAAAAAAA\");",
      "3409:     }",
      "",
      "---------------"
    ],
    "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java -> core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "25: import java.security.NoSuchAlgorithmException;",
      "27: import org.apache.commons.codec.binary.Base64;",
      "29: import javax.crypto.Cipher;",
      "30: import javax.crypto.NoSuchPaddingException;",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "28: import org.apache.kylin.common.KylinConfig;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "42:             InvalidKeyException, NoSuchPaddingException, NoSuchAlgorithmException, UnsupportedEncodingException {",
      "43:         Cipher cipher = Cipher.getInstance(\"AES/CFB/PKCS5Padding\");",
      "44:         final SecretKeySpec secretKey = new SecretKeySpec(key, \"AES\");",
      "46:         cipher.init(cipherMode, secretKey, ivSpec);",
      "47:         return cipher;",
      "48:     }",
      "",
      "[Removed Lines]",
      "45:         IvParameterSpec ivSpec = new IvParameterSpec(\"AAAAAAAAAAAAAAAA\".getBytes(\"UTF-8\"));",
      "",
      "[Added Lines]",
      "46:         IvParameterSpec ivSpec = new IvParameterSpec(KylinConfig.getInstanceFromEnv().getEncryptCipherIvSpec().getBytes(\"UTF-8\"));",
      "",
      "---------------"
    ],
    "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java": [
      "File: core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java -> core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: package org.apache.kylin.common.util;",
      "21: import org.junit.Assert;",
      "22: import org.junit.Test;",
      "26:     @Test",
      "27:     public void testAESEncrypt(){",
      "",
      "[Removed Lines]",
      "24: public class EncryptUtilTest {",
      "",
      "[Added Lines]",
      "21: import org.junit.After;",
      "23: import org.junit.Before;",
      "26: public class EncryptUtilTest extends LocalFileMetadataTestCase {",
      "27:     @Before",
      "28:     public void setUp() throws Exception {",
      "29:         this.createTestMetadata();",
      "30:     }",
      "32:     @After",
      "33:     public void after() throws Exception {",
      "34:         this.cleanupTestMetadata();",
      "35:     }",
      "",
      "---------------"
    ],
    "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java": [
      "File: server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java -> server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "87:     public String dumpProjectDiagnosisInfo(String project, File exportPath) throws IOException {",
      "88:         Message msg = MsgPicker.getMsg();",
      "89:         ProjectInstance projectInstance =",
      "90:                 ProjectManager.getInstance(KylinConfig.getInstanceFromEnv())",
      "92:         if (null == projectInstance) {",
      "93:             throw new BadRequestException(",
      "95:         }",
      "96:         aclEvaluate.checkProjectOperationPermission(projectInstance);",
      "98:         runDiagnosisCLI(args);",
      "99:         return getDiagnosisPackageName(exportPath);",
      "100:     }",
      "",
      "[Removed Lines]",
      "91:                         .getProject(ValidateUtil.convertStringToBeAlphanumericUnderscore(project));",
      "94:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), project));",
      "97:         String[] args = { project, exportPath.getAbsolutePath() };",
      "",
      "[Added Lines]",
      "89:         String projectName = ValidateUtil.convertStringToBeAlphanumericUnderscore(project);",
      "92:                         .getProject(projectName);",
      "95:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), projectName));",
      "98:         String[] args = { projectName, exportPath.getAbsolutePath() };",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "2edeaea625ce5a1e8bfbc73a72fbafccbfb420ff",
      "candidate_info": {
        "commit_hash": "2edeaea625ce5a1e8bfbc73a72fbafccbfb420ff",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/2edeaea625ce5a1e8bfbc73a72fbafccbfb420ff",
        "files": [
          "core-job/src/main/java/org/apache/kylin/job/constant/ExecutableConstants.java",
          "core-job/src/main/java/org/apache/kylin/job/execution/ExecutableManager.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepFactory.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepType.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkMergeStatisticsStep.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkMergingJob.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/JobService.java"
        ],
        "message": "KYLIN-4996 Merge cuboid statistics in merge job for kylin4",
        "before_after_code_files": [
          "core-job/src/main/java/org/apache/kylin/job/constant/ExecutableConstants.java||core-job/src/main/java/org/apache/kylin/job/constant/ExecutableConstants.java",
          "core-job/src/main/java/org/apache/kylin/job/execution/ExecutableManager.java||core-job/src/main/java/org/apache/kylin/job/execution/ExecutableManager.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepFactory.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepFactory.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepType.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepType.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkMergeStatisticsStep.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkMergeStatisticsStep.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkMergingJob.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkMergingJob.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/JobService.java||server-base/src/main/java/org/apache/kylin/rest/service/JobService.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core-job/src/main/java/org/apache/kylin/job/constant/ExecutableConstants.java||core-job/src/main/java/org/apache/kylin/job/constant/ExecutableConstants.java": [
          "File: core-job/src/main/java/org/apache/kylin/job/constant/ExecutableConstants.java -> core-job/src/main/java/org/apache/kylin/job/constant/ExecutableConstants.java"
        ],
        "core-job/src/main/java/org/apache/kylin/job/execution/ExecutableManager.java||core-job/src/main/java/org/apache/kylin/job/execution/ExecutableManager.java": [
          "File: core-job/src/main/java/org/apache/kylin/job/execution/ExecutableManager.java -> core-job/src/main/java/org/apache/kylin/job/execution/ExecutableManager.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "271:         AbstractExecutable jobInstance = getJob(jobId);",
          "272:         String outputStorePath = KylinConfig.getInstanceFromEnv().getJobOutputStorePath(jobInstance.getParam(MetadataConstants.P_PROJECT_NAME), stepId);",
          "273:         ExecutableOutputPO jobOutput = getJobOutputFromHDFS(outputStorePath);",
          "274:         assertOutputNotNull(jobOutput, outputStorePath);",
          "276:         if (Objects.nonNull(jobOutput.getLogPath())) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "274:         if (jobOutput == null) {",
          "275:             return null;",
          "276:         }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "291:             Path path = new Path(resPath);",
          "292:             FileSystem fs = HadoopUtil.getWorkingFileSystem();",
          "293:             if (!fs.exists(path)) {",
          "297:             }",
          "299:             din = fs.open(path);",
          "",
          "[Removed Lines]",
          "294:                 ExecutableOutputPO executableOutputPO = new ExecutableOutputPO();",
          "295:                 executableOutputPO.setContent(\"job output not found, please check kylin.log\");",
          "296:                 return executableOutputPO;",
          "",
          "[Added Lines]",
          "297:                 return null;",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepFactory.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepFactory.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepFactory.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepFactory.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "45:         case OPTIMIZING:",
          "46:             step = new NSparkOptimizingStep(OptimizeBuildJob.class.getName());",
          "47:             break;",
          "48:         case CLEAN_UP_AFTER_MERGE:",
          "49:             step = new NSparkUpdateMetaAndCleanupAfterMergeStep();",
          "50:             break;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "48:         case MERGE_STATISTICS:",
          "49:             step = new NSparkMergeStatisticsStep();",
          "50:             break;",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepType.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepType.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepType.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/JobStepType.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: public enum JobStepType {",
          "22:     RESOURCE_DETECT,",
          "26:     FILTER_RECOMMEND_CUBOID",
          "27: }",
          "",
          "[Removed Lines]",
          "24:     CLEAN_UP_AFTER_MERGE, CUBING, MERGING, OPTIMIZING,",
          "",
          "[Added Lines]",
          "24:     CLEAN_UP_AFTER_MERGE, CUBING, MERGING, MERGE_STATISTICS, OPTIMIZING,",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkMergeStatisticsStep.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkMergeStatisticsStep.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkMergeStatisticsStep.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkMergeStatisticsStep.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.engine.spark.job;",
          "21: import org.apache.hadoop.conf.Configuration;",
          "22: import org.apache.hadoop.fs.FSDataInputStream;",
          "23: import org.apache.hadoop.fs.FileSystem;",
          "24: import org.apache.hadoop.fs.Path;",
          "25: import org.apache.hadoop.io.BytesWritable;",
          "26: import org.apache.hadoop.io.IOUtils;",
          "27: import org.apache.hadoop.io.LongWritable;",
          "28: import org.apache.hadoop.io.SequenceFile;",
          "29: import org.apache.hadoop.util.ReflectionUtils;",
          "30: import org.apache.kylin.common.KylinConfig;",
          "31: import org.apache.kylin.common.persistence.ResourceStore;",
          "32: import org.apache.kylin.common.util.ByteArray;",
          "33: import org.apache.kylin.common.util.Bytes;",
          "34: import org.apache.kylin.common.util.HadoopUtil;",
          "35: import org.apache.kylin.cube.CubeInstance;",
          "36: import org.apache.kylin.cube.CubeManager;",
          "37: import org.apache.kylin.cube.CubeSegment;",
          "38: import org.apache.kylin.engine.mr.common.BatchConstants;",
          "39: import org.apache.kylin.engine.mr.common.CubeStatsWriter;",
          "40: import org.apache.kylin.job.constant.ExecutableConstants;",
          "41: import org.apache.kylin.job.exception.ExecuteException;",
          "42: import org.apache.kylin.job.execution.ExecutableContext;",
          "43: import org.apache.kylin.job.execution.ExecuteResult;",
          "44: import org.apache.kylin.measure.hllc.HLLCounter;",
          "45: import org.apache.kylin.metadata.MetadataConstants;",
          "46: import org.apache.kylin.shaded.com.google.common.collect.Lists;",
          "47: import org.apache.kylin.shaded.com.google.common.collect.Maps;",
          "48: import org.slf4j.Logger;",
          "49: import org.slf4j.LoggerFactory;",
          "51: import java.io.File;",
          "52: import java.io.FileOutputStream;",
          "53: import java.io.IOException;",
          "54: import java.io.InputStream;",
          "55: import java.util.List;",
          "56: import java.util.Map;",
          "58: public class NSparkMergeStatisticsStep extends NSparkExecutable {",
          "59:     private static final Logger logger = LoggerFactory.getLogger(NSparkMergeStatisticsStep.class);",
          "61:     private List<CubeSegment> mergingSegments = Lists.newArrayList();",
          "62:     protected Map<Long, HLLCounter> cuboidHLLMap = Maps.newHashMap();",
          "64:     public NSparkMergeStatisticsStep() {",
          "65:         this.setName(ExecutableConstants.STEP_NAME_MERGE_STATISTICS);",
          "66:     }",
          "68:     @Override",
          "69:     protected ExecuteResult doWork(ExecutableContext context) throws ExecuteException {",
          "70:         String jobId = getParam(MetadataConstants.P_JOB_ID);",
          "71:         String cubeId = getParam(MetadataConstants.P_CUBE_ID);",
          "73:         String mergedSegmentUuid = getParam(MetadataConstants.P_SEGMENT_IDS);",
          "74:         final KylinConfig kylinConfig = wrapConfig(context);",
          "75:         CubeInstance cube = CubeManager.getInstance(kylinConfig).getCubeByUuid(cubeId);",
          "76:         CubeSegment mergedSeg = cube.getSegmentById(mergedSegmentUuid);",
          "78:         String jobTmpDir = kylinConfig.getJobTmpDir(cube.getProject()) + \"/\" + jobId;",
          "79:         Path statisticsDir = new Path(jobTmpDir + \"/\" + ResourceStore.CUBE_STATISTICS_ROOT + \"/\"",
          "80:                 + cubeId + \"/\" + mergedSeg.getUuid() + \"/\");",
          "82:         mergingSegments = cube.getMergingSegments(mergedSeg);",
          "84:         Configuration conf = HadoopUtil.getCurrentConfiguration();",
          "85:         ResourceStore rs = ResourceStore.getStore(kylinConfig);",
          "86:         try {",
          "87:             int averageSamplingPercentage = 0;",
          "88:             long sourceRecordCount = 0;",
          "89:             for (CubeSegment segment : mergingSegments) {",
          "90:                 String segmentId = segment.getUuid();",
          "91:                 String fileKey = CubeSegment",
          "92:                         .getStatisticsResourcePath(cube.getName(), segmentId);",
          "93:                 InputStream is = rs.getResource(fileKey).content();",
          "94:                 File tempFile = null;",
          "95:                 FileOutputStream tempFileStream = null;",
          "96:                 try {",
          "97:                     tempFile = File.createTempFile(segmentId, \".seq\");",
          "98:                     tempFileStream = new FileOutputStream(tempFile);",
          "99:                     org.apache.commons.io.IOUtils.copy(is, tempFileStream);",
          "100:                 } finally {",
          "101:                     IOUtils.closeStream(is);",
          "102:                     IOUtils.closeStream(tempFileStream);",
          "103:                 }",
          "105:                 FileSystem fs = HadoopUtil.getFileSystem(\"file:///\" + tempFile.getAbsolutePath());",
          "106:                 SequenceFile.Reader reader = null;",
          "107:                 try {",
          "108:                     reader = new SequenceFile.Reader(fs, new Path(tempFile.getAbsolutePath()), conf);",
          "109:                     LongWritable key = (LongWritable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);",
          "110:                     BytesWritable value = (BytesWritable) ReflectionUtils.newInstance(reader.getValueClass(), conf);",
          "111:                     while (reader.next(key, value)) {",
          "112:                         if (key.get() == 0L) {",
          "114:                             averageSamplingPercentage += Bytes.toInt(value.getBytes());",
          "115:                         } else if (key.get() == -3) {",
          "116:                             long perSourceRecordCount = Bytes.toLong(value.getBytes());",
          "117:                             if (perSourceRecordCount > 0) {",
          "118:                                 sourceRecordCount += perSourceRecordCount;",
          "119:                             }",
          "120:                         } else if (key.get() > 0) {",
          "121:                             HLLCounter hll = new HLLCounter(kylinConfig.getCubeStatsHLLPrecision());",
          "122:                             ByteArray byteArray = new ByteArray(value.getBytes());",
          "123:                             hll.readRegisters(byteArray.asBuffer());",
          "125:                             if (cuboidHLLMap.get(key.get()) != null) {",
          "126:                                 cuboidHLLMap.get(key.get()).merge(hll);",
          "127:                             } else {",
          "128:                                 cuboidHLLMap.put(key.get(), hll);",
          "129:                             }",
          "130:                         }",
          "131:                     }",
          "132:                 } catch (Exception e) {",
          "133:                     e.printStackTrace();",
          "134:                     throw e;",
          "135:                 } finally {",
          "136:                     IOUtils.closeStream(reader);",
          "137:                     if (tempFile != null)",
          "138:                         tempFile.delete();",
          "139:                 }",
          "140:             }",
          "141:             averageSamplingPercentage = averageSamplingPercentage / mergingSegments.size();",
          "142:             CubeStatsWriter.writeCuboidStatistics(conf, statisticsDir, cuboidHLLMap,",
          "143:                     averageSamplingPercentage, sourceRecordCount);",
          "144:             Path statisticsFilePath = new Path(statisticsDir, BatchConstants.CFG_STATISTICS_CUBOID_ESTIMATION_FILENAME);",
          "145:             FileSystem fs = HadoopUtil.getFileSystem(statisticsFilePath, conf);",
          "146:             FSDataInputStream is = fs.open(statisticsFilePath);",
          "147:             try {",
          "149:                 String statisticsFileName = mergedSeg.getStatisticsResourcePath();",
          "150:                 rs.putResource(statisticsFileName, is, System.currentTimeMillis());",
          "151:             } finally {",
          "152:                 IOUtils.closeStream(is);",
          "153:             }",
          "155:             return ExecuteResult.createSucceed();",
          "156:         } catch (IOException e) {",
          "157:             logger.error(\"fail to merge cuboid statistics\", e);",
          "158:             return ExecuteResult.createError(e);",
          "159:         }",
          "160:     }",
          "162: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkMergingJob.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkMergingJob.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkMergingJob.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkMergingJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "88:         JobStepFactory.addStep(job, JobStepType.RESOURCE_DETECT, cube);",
          "89:         JobStepFactory.addStep(job, JobStepType.MERGING, cube);",
          "90:         JobStepFactory.addStep(job, JobStepType.CLEAN_UP_AFTER_MERGE, cube);",
          "92:         return job;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "90:         if (KylinConfig.getInstanceFromEnv().isSegmentStatisticsEnabled()) {",
          "91:             JobStepFactory.addStep(job, JobStepType.MERGE_STATISTICS, cube);",
          "92:         }",
          "",
          "---------------"
        ],
        "server-base/src/main/java/org/apache/kylin/rest/service/JobService.java||server-base/src/main/java/org/apache/kylin/rest/service/JobService.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/service/JobService.java -> server-base/src/main/java/org/apache/kylin/rest/service/JobService.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "502:     public String getJobStepOutput(String jobId, String stepId) {",
          "503:         ExecutableManager executableManager = getExecutableManager();",
          "506:             return executableManager.getOutput(stepId).getVerboseMsg();",
          "507:         }",
          "508:         return executableManager.getOutputFromHDFSByJobId(jobId, stepId).getVerboseMsg();",
          "",
          "[Removed Lines]",
          "504:         AbstractExecutable job = executableManager.getJob(jobId);",
          "505:         if (job instanceof CheckpointExecutable) {",
          "",
          "[Added Lines]",
          "504:         if (executableManager.getOutputFromHDFSByJobId(jobId, stepId) == null) {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "511:     public String getAllJobStepOutput(String jobId, String stepId) {",
          "512:         ExecutableManager executableManager = getExecutableManager();",
          "515:             return executableManager.getOutput(stepId).getVerboseMsg();",
          "516:         }",
          "517:         return executableManager.getOutputFromHDFSByJobId(jobId, stepId, Integer.MAX_VALUE).getVerboseMsg();",
          "",
          "[Removed Lines]",
          "513:         AbstractExecutable job = executableManager.getJob(jobId);",
          "514:         if (job instanceof CheckpointExecutable) {",
          "",
          "[Added Lines]",
          "512:         if (executableManager.getOutputFromHDFSByJobId(jobId, stepId, Integer.MAX_VALUE) == null) {",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e769f1cc19a03fba9d3792c5f1d302457c4038a3",
      "candidate_info": {
        "commit_hash": "e769f1cc19a03fba9d3792c5f1d302457c4038a3",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/e769f1cc19a03fba9d3792c5f1d302457c4038a3",
        "files": [
          "core-job/src/main/java/org/apache/kylin/job/common/PatternedLogger.java"
        ],
        "message": "KYLIN-4611 modify PATTERN_SPARK_APP_URL to Tracking URL, ignore case\n\n(cherry picked from commit f49f7df8b883df11c6af9b40bee00d0141f58168)",
        "before_after_code_files": [
          "core-job/src/main/java/org/apache/kylin/job/common/PatternedLogger.java||core-job/src/main/java/org/apache/kylin/job/common/PatternedLogger.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core-job/src/main/java/org/apache/kylin/job/common/PatternedLogger.java||core-job/src/main/java/org/apache/kylin/job/common/PatternedLogger.java": [
          "File: core-job/src/main/java/org/apache/kylin/job/common/PatternedLogger.java -> core-job/src/main/java/org/apache/kylin/job/common/PatternedLogger.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "53:     private static final Pattern PATTERN_SPARK_APP_ID = Pattern.compile(\"Submitted application (.*)\");",
          "55:     private static final Pattern PATTERN_JOB_STATE = Pattern.compile(\"Final-State : (.*?)$\");",
          "",
          "[Removed Lines]",
          "54:     private static final Pattern PATTERN_SPARK_APP_URL = Pattern.compile(\"tracking URL: (.*)\");",
          "",
          "[Added Lines]",
          "54:     private static final Pattern PATTERN_SPARK_APP_URL = Pattern.compile(\"(?i)Tracking URL: (.*)\");",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "47585640987fcc05babcc02b6455ee882b580c02",
      "candidate_info": {
        "commit_hash": "47585640987fcc05babcc02b6455ee882b580c02",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/47585640987fcc05babcc02b6455ee882b580c02",
        "files": [
          "cache/src/main/java/net/spy/memcached/protocol/TCPMemcachedNodeImpl.java"
        ],
        "message": "KYLIN-4507 Add hack file TCPMemcachedNodeImpl.java\n\n(cherry picked from commit 03e67bc1cf8fbb63ad59e868ae0287679bde4e5d)",
        "before_after_code_files": [
          "cache/src/main/java/net/spy/memcached/protocol/TCPMemcachedNodeImpl.java||cache/src/main/java/net/spy/memcached/protocol/TCPMemcachedNodeImpl.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "cache/src/main/java/net/spy/memcached/protocol/TCPMemcachedNodeImpl.java||cache/src/main/java/net/spy/memcached/protocol/TCPMemcachedNodeImpl.java": [
          "File: cache/src/main/java/net/spy/memcached/protocol/TCPMemcachedNodeImpl.java -> cache/src/main/java/net/spy/memcached/protocol/TCPMemcachedNodeImpl.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package net.spy.memcached.protocol;",
          "21: import java.io.IOException;",
          "22: import java.net.SocketAddress;",
          "23: import java.nio.ByteBuffer;",
          "24: import java.nio.channels.SelectionKey;",
          "25: import java.nio.channels.SocketChannel;",
          "26: import java.util.ArrayList;",
          "27: import java.util.Collection;",
          "28: import java.util.concurrent.BlockingQueue;",
          "29: import java.util.concurrent.CountDownLatch;",
          "30: import java.util.concurrent.TimeUnit;",
          "31: import java.util.concurrent.atomic.AtomicInteger;",
          "33: import net.spy.memcached.ConnectionFactory;",
          "34: import net.spy.memcached.FailureMode;",
          "35: import net.spy.memcached.MemcachedConnection;",
          "36: import net.spy.memcached.MemcachedNode;",
          "37: import net.spy.memcached.compat.SpyObject;",
          "38: import net.spy.memcached.ops.Operation;",
          "39: import net.spy.memcached.ops.OperationState;",
          "40: import net.spy.memcached.protocol.binary.TapAckOperationImpl;",
          "46: public abstract class TCPMemcachedNodeImpl extends SpyObject implements MemcachedNode {",
          "48:     private final SocketAddress socketAddress;",
          "49:     private final ByteBuffer rbuf;",
          "50:     private final ByteBuffer wbuf;",
          "51:     protected final BlockingQueue<Operation> writeQ;",
          "52:     private final BlockingQueue<Operation> readQ;",
          "53:     private final BlockingQueue<Operation> inputQueue;",
          "54:     private final long opQueueMaxBlockTime;",
          "55:     private final long authWaitTime;",
          "56:     private final ConnectionFactory connectionFactory;",
          "57:     private AtomicInteger reconnectAttempt = new AtomicInteger(1);",
          "58:     private SocketChannel channel;",
          "59:     private int toWrite = 0;",
          "60:     protected Operation optimizedOp = null;",
          "61:     private volatile SelectionKey sk = null;",
          "62:     private boolean shouldAuth = false;",
          "63:     private CountDownLatch authLatch;",
          "64:     private ArrayList<Operation> reconnectBlocked;",
          "65:     private long defaultOpTimeout;",
          "66:     private volatile long lastReadTimestamp = System.nanoTime();",
          "67:     private MemcachedConnection connection;",
          "70:     private final AtomicInteger continuousTimeout = new AtomicInteger(0);",
          "72:     public TCPMemcachedNodeImpl(SocketAddress sa, SocketChannel c, int bufSize, BlockingQueue<Operation> rq,",
          "73:             BlockingQueue<Operation> wq, BlockingQueue<Operation> iq, long opQueueMaxBlockTime, boolean waitForAuth,",
          "74:             long dt, long authWaitTime, ConnectionFactory fact) {",
          "75:         super();",
          "76:         assert sa != null : \"No SocketAddress\";",
          "77:         assert c != null : \"No SocketChannel\";",
          "78:         assert bufSize > 0 : \"Invalid buffer size: \" + bufSize;",
          "79:         assert rq != null : \"No operation read queue\";",
          "80:         assert wq != null : \"No operation write queue\";",
          "81:         assert iq != null : \"No input queue\";",
          "82:         socketAddress = sa;",
          "83:         connectionFactory = fact;",
          "84:         this.authWaitTime = authWaitTime;",
          "85:         setChannel(c);",
          "90:         rbuf = ByteBuffer.allocateDirect(bufSize);",
          "91:         wbuf = ByteBuffer.allocateDirect(bufSize);",
          "92:         getWbuf().clear();",
          "93:         readQ = rq;",
          "94:         writeQ = wq;",
          "95:         inputQueue = iq;",
          "96:         this.opQueueMaxBlockTime = opQueueMaxBlockTime;",
          "97:         shouldAuth = waitForAuth;",
          "98:         defaultOpTimeout = dt;",
          "99:         setupForAuth();",
          "100:     }",
          "107:     public final void copyInputQueue() {",
          "108:         Collection<Operation> tmp = new ArrayList<Operation>();",
          "111:         inputQueue.drainTo(tmp, writeQ.remainingCapacity());",
          "112:         writeQ.addAll(tmp);",
          "113:     }",
          "120:     public Collection<Operation> destroyInputQueue() {",
          "121:         Collection<Operation> rv = new ArrayList<Operation>();",
          "122:         inputQueue.drainTo(rv);",
          "123:         return rv;",
          "124:     }",
          "131:     public final void setupResend() {",
          "134:         Operation op = getCurrentWriteOp();",
          "135:         if (shouldAuth && op != null) {",
          "136:             op.cancel();",
          "137:         } else if (op != null) {",
          "138:             ByteBuffer buf = op.getBuffer();",
          "139:             if (buf != null) {",
          "140:                 buf.reset();",
          "141:             } else {",
          "142:                 getLogger().info(\"No buffer for current write op, removing\");",
          "143:                 removeCurrentWriteOp();",
          "144:             }",
          "145:         }",
          "148:         while (hasReadOp()) {",
          "149:             op = removeCurrentReadOp();",
          "150:             if (op != getCurrentWriteOp()) {",
          "151:                 getLogger().warn(\"Discarding partially completed op: %s\", op);",
          "152:                 op.cancel();",
          "153:             }",
          "154:         }",
          "156:         while (shouldAuth && hasWriteOp()) {",
          "157:             op = removeCurrentWriteOp();",
          "158:             getLogger().warn(\"Discarding partially completed op: %s\", op);",
          "159:             op.cancel();",
          "160:         }",
          "162:         getWbuf().clear();",
          "163:         getRbuf().clear();",
          "164:         toWrite = 0;",
          "165:     }",
          "169:     private boolean preparePending() {",
          "171:         copyInputQueue();",
          "174:         Operation nextOp = getCurrentWriteOp();",
          "175:         while (nextOp != null && nextOp.isCancelled()) {",
          "176:             getLogger().info(\"Removing cancelled operation: %s\", nextOp);",
          "177:             removeCurrentWriteOp();",
          "178:             nextOp = getCurrentWriteOp();",
          "179:         }",
          "180:         return nextOp != null;",
          "181:     }",
          "188:     public final void fillWriteBuffer(boolean shouldOptimize) {",
          "189:         if (toWrite == 0 && readQ.remainingCapacity() > 0) {",
          "190:             getWbuf().clear();",
          "191:             Operation o = getNextWritableOp();",
          "193:             while (o != null && toWrite < getWbuf().capacity()) {",
          "194:                 synchronized (o) {",
          "195:                     assert o.getState() == OperationState.WRITING;",
          "197:                     ByteBuffer obuf = o.getBuffer();",
          "198:                     assert obuf != null : \"Didn't get a write buffer from \" + o;",
          "199:                     int bytesToCopy = Math.min(getWbuf().remaining(), obuf.remaining());",
          "200:                     byte[] b = new byte[bytesToCopy];",
          "201:                     obuf.get(b);",
          "202:                     getWbuf().put(b);",
          "203:                     getLogger().debug(\"After copying stuff from %s: %s\", o, getWbuf());",
          "204:                     if (!o.getBuffer().hasRemaining()) {",
          "205:                         o.writeComplete();",
          "206:                         transitionWriteItem();",
          "208:                         preparePending();",
          "209:                         if (shouldOptimize) {",
          "210:                             optimize();",
          "211:                         }",
          "213:                         o = getNextWritableOp();",
          "214:                     }",
          "215:                     toWrite += bytesToCopy;",
          "216:                 }",
          "217:             }",
          "218:             getWbuf().flip();",
          "219:             assert toWrite <= getWbuf().capacity() : \"toWrite exceeded capacity: \" + this;",
          "220:             assert toWrite == getWbuf().remaining() : \"Expected \" + toWrite + \" remaining, got \"",
          "221:                     + getWbuf().remaining();",
          "222:         } else {",
          "223:             getLogger().debug(\"Buffer is full, skipping\");",
          "224:         }",
          "225:     }",
          "227:     private Operation getNextWritableOp() {",
          "228:         Operation o = getCurrentWriteOp();",
          "229:         while (o != null && o.getState() == OperationState.WRITE_QUEUED) {",
          "230:             synchronized (o) {",
          "231:                 if (o.isCancelled()) {",
          "232:                     getLogger().debug(\"Not writing cancelled op.\");",
          "233:                     Operation cancelledOp = removeCurrentWriteOp();",
          "234:                     assert o == cancelledOp;",
          "235:                 } else if (o.isTimedOut(defaultOpTimeout)) {",
          "236:                     getLogger().debug(\"Not writing timed out op.\");",
          "237:                     Operation timedOutOp = removeCurrentWriteOp();",
          "238:                     assert o == timedOutOp;",
          "239:                 } else {",
          "240:                     o.writing();",
          "241:                     if (!(o instanceof TapAckOperationImpl)) {",
          "242:                         readQ.add(o);",
          "243:                     }",
          "244:                     return o;",
          "245:                 }",
          "246:                 o = getCurrentWriteOp();",
          "247:             }",
          "248:         }",
          "249:         return o;",
          "250:     }",
          "255:     public final void transitionWriteItem() {",
          "256:         Operation op = removeCurrentWriteOp();",
          "257:         assert op != null : \"There is no write item to transition\";",
          "258:         getLogger().debug(\"Finished writing %s\", op);",
          "259:     }",
          "266:     protected abstract void optimize();",
          "273:     public final Operation getCurrentReadOp() {",
          "274:         return readQ.peek();",
          "275:     }",
          "282:     public final Operation removeCurrentReadOp() {",
          "283:         return readQ.remove();",
          "284:     }",
          "291:     public final Operation getCurrentWriteOp() {",
          "292:         return optimizedOp == null ? writeQ.peek() : optimizedOp;",
          "293:     }",
          "300:     public final Operation removeCurrentWriteOp() {",
          "301:         Operation rv = optimizedOp;",
          "302:         if (rv == null) {",
          "303:             rv = writeQ.remove();",
          "304:         } else {",
          "305:             optimizedOp = null;",
          "306:         }",
          "307:         return rv;",
          "308:     }",
          "315:     public final boolean hasReadOp() {",
          "316:         return !readQ.isEmpty();",
          "317:     }",
          "324:     public final boolean hasWriteOp() {",
          "325:         return !(optimizedOp == null && writeQ.isEmpty());",
          "326:     }",
          "333:     public final void addOp(Operation op) {",
          "334:         try {",
          "335:             if (!authLatch.await(authWaitTime, TimeUnit.MILLISECONDS)) {",
          "336:                 FailureMode mode = connectionFactory.getFailureMode();",
          "337:                 if (mode == FailureMode.Redistribute || mode == FailureMode.Retry) {",
          "338:                     getLogger().debug(\"Redistributing Operation \" + op + \" because auth \" + \"latch taken longer than \"",
          "339:                             + authWaitTime + \" milliseconds to \" + \"complete on node \" + getSocketAddress());",
          "340:                     connection.retryOperation(op);",
          "341:                 } else {",
          "342:                     op.cancel();",
          "343:                     getLogger().warn(\"Operation canceled because authentication \"",
          "344:                             + \"or reconnection and authentication has \" + \"taken more than \" + authWaitTime",
          "345:                             + \" milliseconds to \" + \"complete on node \" + this);",
          "346:                     getLogger().debug(\"Canceled operation %s\", op.toString());",
          "347:                 }",
          "348:                 return;",
          "349:             }",
          "350:             if (!inputQueue.offer(op, opQueueMaxBlockTime, TimeUnit.MILLISECONDS)) {",
          "351:                 throw new IllegalStateException(",
          "352:                         \"Timed out waiting to add \" + op + \"(max wait=\" + opQueueMaxBlockTime + \"ms)\");",
          "353:             }",
          "354:         } catch (InterruptedException e) {",
          "356:             Thread.currentThread().interrupt();",
          "357:             throw new IllegalStateException(\"Interrupted while waiting to add \" + op);",
          "358:         }",
          "359:     }",
          "367:     public final void insertOp(Operation op) {",
          "368:         ArrayList<Operation> tmp = new ArrayList<Operation>(inputQueue.size() + 1);",
          "369:         tmp.add(op);",
          "370:         inputQueue.drainTo(tmp);",
          "371:         inputQueue.addAll(tmp);",
          "372:     }",
          "379:     public final int getSelectionOps() {",
          "380:         int rv = 0;",
          "381:         if (getChannel().isConnected()) {",
          "382:             if (hasReadOp()) {",
          "383:                 rv |= SelectionKey.OP_READ;",
          "384:             }",
          "385:             if (toWrite > 0 || hasWriteOp()) {",
          "386:                 rv |= SelectionKey.OP_WRITE;",
          "387:             }",
          "388:         } else {",
          "389:             rv = SelectionKey.OP_CONNECT;",
          "390:         }",
          "391:         return rv;",
          "392:     }",
          "399:     public final ByteBuffer getRbuf() {",
          "400:         return rbuf;",
          "401:     }",
          "408:     public final ByteBuffer getWbuf() {",
          "409:         return wbuf;",
          "410:     }",
          "417:     public final SocketAddress getSocketAddress() {",
          "418:         return socketAddress;",
          "419:     }",
          "426:     public final boolean isActive() {",
          "427:         return reconnectAttempt.get() == 0 && getChannel() != null && getChannel().isConnected();",
          "428:     }",
          "435:     public boolean isAuthenticated() {",
          "436:         return (0 == authLatch.getCount());",
          "437:     }",
          "444:     public final void reconnecting() {",
          "445:         reconnectAttempt.incrementAndGet();",
          "446:         continuousTimeout.set(0);",
          "447:     }",
          "454:     public final void connected() {",
          "455:         reconnectAttempt.set(0);",
          "456:         continuousTimeout.set(0);",
          "457:     }",
          "464:     public final int getReconnectCount() {",
          "465:         return reconnectAttempt.get();",
          "466:     }",
          "473:     @Override",
          "474:     public final String toString() {",
          "475:         int sops = 0;",
          "476:         if (getSk() != null && getSk().isValid()) {",
          "477:             sops = getSk().interestOps();",
          "478:         }",
          "479:         int rsize = readQ.size() + (optimizedOp == null ? 0 : 1);",
          "480:         int wsize = writeQ.size();",
          "481:         int isize = inputQueue.size();",
          "482:         return \"{QA sa=\" + getSocketAddress() + \", #Rops=\" + rsize + \", #Wops=\" + wsize + \", #iq=\" + isize + \", topRop=\"",
          "483:                 + getCurrentReadOp() + \", topWop=\" + getCurrentWriteOp() + \", toWrite=\" + toWrite + \", interested=\"",
          "484:                 + sops + \"}\";",
          "485:     }",
          "494:     public final void registerChannel(SocketChannel ch, SelectionKey skey) {",
          "495:         setChannel(ch);",
          "496:         setSk(skey);",
          "497:     }",
          "505:     public final void setChannel(SocketChannel to) {",
          "506:         assert channel == null || !channel.isOpen() : \"Attempting to overwrite channel\";",
          "507:         channel = to;",
          "508:     }",
          "515:     public final SocketChannel getChannel() {",
          "516:         return channel;",
          "517:     }",
          "524:     public final void setSk(SelectionKey to) {",
          "525:         sk = to;",
          "526:     }",
          "533:     public final SelectionKey getSk() {",
          "534:         return sk;",
          "535:     }",
          "542:     public final int getBytesRemainingToWrite() {",
          "543:         return toWrite;",
          "544:     }",
          "551:     public final int writeSome() throws IOException {",
          "552:         int wrote = channel.write(wbuf);",
          "553:         assert wrote >= 0 : \"Wrote negative bytes?\";",
          "554:         toWrite -= wrote;",
          "555:         assert toWrite >= 0 : \"toWrite went negative after writing \" + wrote + \" bytes for \" + this;",
          "556:         getLogger().debug(\"Wrote %d bytes\", wrote);",
          "557:         return wrote;",
          "558:     }",
          "565:     public void setContinuousTimeout(boolean timedOut) {",
          "566:         if (timedOut && isActive()) {",
          "567:             continuousTimeout.incrementAndGet();",
          "568:         } else {",
          "569:             continuousTimeout.set(0);",
          "570:         }",
          "571:     }",
          "578:     public int getContinuousTimeout() {",
          "579:         return continuousTimeout.get();",
          "580:     }",
          "582:     public final void fixupOps() {",
          "585:         SelectionKey s = sk;",
          "586:         if (s != null && s.isValid()) {",
          "587:             int iops = getSelectionOps();",
          "588:             getLogger().debug(\"Setting interested opts to %d\", iops);",
          "589:             s.interestOps(iops);",
          "590:         } else {",
          "591:             getLogger().debug(\"Selection key is not valid.\");",
          "592:         }",
          "593:     }",
          "595:     public final void authComplete() {",
          "596:         if (reconnectBlocked != null && reconnectBlocked.size() > 0) {",
          "597:             inputQueue.addAll(reconnectBlocked);",
          "598:         }",
          "599:         authLatch.countDown();",
          "600:     }",
          "602:     public final void setupForAuth() {",
          "603:         if (shouldAuth) {",
          "604:             authLatch = new CountDownLatch(1);",
          "605:             if (inputQueue.size() > 0) {",
          "606:                 reconnectBlocked = new ArrayList<Operation>(inputQueue.size() + 1);",
          "607:                 inputQueue.drainTo(reconnectBlocked);",
          "608:             }",
          "609:             assert (inputQueue.size() == 0);",
          "610:             setupResend();",
          "611:         } else {",
          "612:             authLatch = new CountDownLatch(0);",
          "613:         }",
          "614:     }",
          "621:     public long lastReadDelta() {",
          "622:         return TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - lastReadTimestamp);",
          "623:     }",
          "628:     public void completedRead() {",
          "629:         lastReadTimestamp = System.nanoTime();",
          "630:     }",
          "632:     @Override",
          "633:     public MemcachedConnection getConnection() {",
          "634:         return connection;",
          "635:     }",
          "637:     @Override",
          "638:     public void setConnection(MemcachedConnection connection) {",
          "639:         this.connection = connection;",
          "640:     }",
          "641: }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2ad58c712ea1cb8ca7a3494eb30a7d1a69f212a2",
      "candidate_info": {
        "commit_hash": "2ad58c712ea1cb8ca7a3494eb30a7d1a69f212a2",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/2ad58c712ea1cb8ca7a3494eb30a7d1a69f212a2",
        "files": [
          "build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java",
          "webapp/app/js/controllers/cube.js"
        ],
        "message": "KYLIN-4872 Fix NPE when there are more than one segment if cube planner is open",
        "before_after_code_files": [
          "build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java||build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java",
          "webapp/app/js/controllers/cube.js||webapp/app/js/controllers/cube.js"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java||build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java": [
          "File: build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java -> build-engine/src/main/java/org/apache/kylin/engine/mr/common/CubeStatsReader.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "97:     public CubeStatsReader(CubeSegment cubeSegment, CuboidScheduler cuboidScheduler, KylinConfig kylinConfig)",
          "98:             throws IOException {",
          "99:         ResourceStore store = ResourceStore.getStore(kylinConfig);",
          "100:         String statsKey = cubeSegment.getStatisticsResourcePath();",
          "101:         RawResource resource = store.getResource(statsKey);",
          "104:             logger.warn(\"{} is not exists.\", statsKey);",
          "105:         }",
          "119:     }",
          "",
          "[Removed Lines]",
          "102:         if (resource == null) {",
          "106:         File tmpSeqFile = writeTmpSeqFile(resource.content());",
          "107:         Path path = new Path(HadoopUtil.fixWindowsPath(\"file://\" + tmpSeqFile.getAbsolutePath()));",
          "108:         logger.info(\"Reading statistics from {}\", path);",
          "109:         CubeStatsResult cubeStatsResult = new CubeStatsResult(path, kylinConfig.getCubeStatsHLLPrecision());",
          "110:         tmpSeqFile.delete();",
          "112:         this.seg = cubeSegment;",
          "113:         this.cuboidScheduler = cuboidScheduler;",
          "114:         this.samplingPercentage = cubeStatsResult.getPercentage();",
          "115:         this.mapperNumberOfFirstBuild = cubeStatsResult.getMapperNumber();",
          "116:         this.mapperOverlapRatioOfFirstBuild = cubeStatsResult.getMapperOverlapRatio();",
          "117:         this.cuboidRowEstimatesHLL = cubeStatsResult.getCounterMap();",
          "118:         this.sourceRowCount = cubeStatsResult.getSourceRecordCount();",
          "",
          "[Added Lines]",
          "99:         this.seg = cubeSegment;",
          "100:         this.cuboidScheduler = cuboidScheduler;",
          "104:         if (resource != null) {",
          "105:             File tmpSeqFile = writeTmpSeqFile(resource.content());",
          "106:             Path path = new Path(HadoopUtil.fixWindowsPath(\"file://\" + tmpSeqFile.getAbsolutePath()));",
          "107:             logger.info(\"Reading statistics from {}\", path);",
          "108:             CubeStatsResult cubeStatsResult = new CubeStatsResult(path, kylinConfig.getCubeStatsHLLPrecision());",
          "109:             tmpSeqFile.delete();",
          "111:             this.samplingPercentage = cubeStatsResult.getPercentage();",
          "112:             this.mapperNumberOfFirstBuild = cubeStatsResult.getMapperNumber();",
          "113:             this.mapperOverlapRatioOfFirstBuild = cubeStatsResult.getMapperOverlapRatio();",
          "114:             this.cuboidRowEstimatesHLL = cubeStatsResult.getCounterMap();",
          "115:             this.sourceRowCount = cubeStatsResult.getSourceRecordCount();",
          "116:         } else {",
          "119:             this.samplingPercentage = -1;",
          "120:             this.mapperNumberOfFirstBuild = -1;",
          "121:             this.mapperOverlapRatioOfFirstBuild = -1.0;",
          "122:             this.cuboidRowEstimatesHLL = null;",
          "123:             this.sourceRowCount = -1L;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "167:     }",
          "169:     public Map<Long, Long> getCuboidRowEstimatesHLL() {",
          "170:         return getCuboidRowCountMapFromSampling(cuboidRowEstimatesHLL, samplingPercentage);",
          "171:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "176:         if (cuboidRowEstimatesHLL == null) {",
          "177:             return null;",
          "178:         }",
          "",
          "---------------"
        ],
        "webapp/app/js/controllers/cube.js||webapp/app/js/controllers/cube.js": [
          "File: webapp/app/js/controllers/cube.js -> webapp/app/js/controllers/cube.js",
          "--- Hunk 1 ---",
          "[Context before]",
          "239:             };",
          "240:             $scope.currentOptions.chart.sunburst = getSunburstDispatch();",
          "241:             $scope.currentOptions.title.text = 'Current Cuboid Distribution';",
          "243:         } else if ('recommend' === type) {",
          "244:             $scope.recommendData = [chartData];",
          "245:             $scope.recommendOptions = angular.copy(cubeConfig.baseChartOptions);",
          "",
          "[Removed Lines]",
          "242:             $scope.currentOptions.subtitle.text = '[Cuboid Count: ' + data.nodeInfos.length + '] [Row Count: ' + data.totalRowCount + ']';",
          "",
          "[Added Lines]",
          "242:             $scope.currentOptions.subtitle.text = '[Cuboid Count: ' + data.nodeInfos.length + '] [The Row Count of The First Segment: ' + data.totalRowCount + ']';",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "258:             };",
          "259:             $scope.recommendOptions.chart.sunburst = getSunburstDispatch();",
          "260:             $scope.recommendOptions.title.text = 'Recommend Cuboid Distribution';",
          "262:         }",
          "263:     };",
          "",
          "[Removed Lines]",
          "261:             $scope.recommendOptions.subtitle.text = '[Cuboid Count: ' + data.nodeInfos.length + '] [Row Count: ' + data.totalRowCount + ']';",
          "",
          "[Added Lines]",
          "261:             $scope.recommendOptions.subtitle.text = '[Cuboid Count: ' + data.nodeInfos.length + '] [The Row Count of The First Segment: ' + data.totalRowCount + ']';",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5650a4e98d1ca02f1a1163c74d8756835a75c443",
      "candidate_info": {
        "commit_hash": "5650a4e98d1ca02f1a1163c74d8756835a75c443",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/5650a4e98d1ca02f1a1163c74d8756835a75c443",
        "files": [
          "build/bin/find-hadoop-conf-dir.sh",
          "metrics-reporter-hive/src/main/java/org/apache/kylin/metrics/lib/impl/hive/HiveProducer.java"
        ],
        "message": "HOTFIX Fix the issue of closing FileSystem",
        "before_after_code_files": [
          "build/bin/find-hadoop-conf-dir.sh||build/bin/find-hadoop-conf-dir.sh",
          "metrics-reporter-hive/src/main/java/org/apache/kylin/metrics/lib/impl/hive/HiveProducer.java||metrics-reporter-hive/src/main/java/org/apache/kylin/metrics/lib/impl/hive/HiveProducer.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "build/bin/find-hadoop-conf-dir.sh||build/bin/find-hadoop-conf-dir.sh": [
          "File: build/bin/find-hadoop-conf-dir.sh -> build/bin/find-hadoop-conf-dir.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "89:             checkAndLinkFile $hadoop_conf_dir/yarn-site.xml $kylin_hadoop_conf_dir/yarn-site.xml",
          "90:             checkAndLinkFile $hadoop_conf_dir/mapred-site.xml $kylin_hadoop_conf_dir/mapred-site.xml",
          "92:             checkAndLinkFile $hadoop_conf_dir/topology.py $kylin_hadoop_conf_dir/topology.py",
          "93:             checkAndLinkFile $hadoop_conf_dir/topology.map $kylin_hadoop_conf_dir/topology.map",
          "94:             checkAndLinkFile $hadoop_conf_dir/ssl-client.xml $kylin_hadoop_conf_dir/ssl-client.xml",
          "95:             checkAndLinkFile $hadoop_conf_dir/hadoop-env.sh $kylin_hadoop_conf_dir/hadoop-env.sh",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "92:             # For CDH",
          "95:             # For HDP",
          "96:             checkAndLinkFile $hadoop_conf_dir/topology_script.py $kylin_hadoop_conf_dir/topology_script.py",
          "97:             checkAndLinkFile $hadoop_conf_dir/topology_mappings.data $kylin_hadoop_conf_dir/topology_mappings.data",
          "",
          "---------------"
        ],
        "metrics-reporter-hive/src/main/java/org/apache/kylin/metrics/lib/impl/hive/HiveProducer.java||metrics-reporter-hive/src/main/java/org/apache/kylin/metrics/lib/impl/hive/HiveProducer.java": [
          "File: metrics-reporter-hive/src/main/java/org/apache/kylin/metrics/lib/impl/hive/HiveProducer.java -> metrics-reporter-hive/src/main/java/org/apache/kylin/metrics/lib/impl/hive/HiveProducer.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "176:         Path partitionPath = new Path(sb.toString());",
          "178:         if (partitionPath.toUri().getScheme() != null && !partitionPath.toUri().toString().startsWith(fs.getUri().toString())) {",
          "180:             fs = partitionPath.getFileSystem(hiveConf);",
          "181:         }",
          "",
          "[Removed Lines]",
          "179:             fs.close();",
          "",
          "[Added Lines]",
          "179:             logger.info(\"Change HDFS scheme from {} to {}.\", fs.getUri().toString(),",
          "180:                     partitionPath.toUri().toString());",
          "",
          "---------------"
        ]
      }
    }
  ]
}