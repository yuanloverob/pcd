{
  "cve_id": "CVE-2021-45229",
  "cve_desc": "It was discovered that the \"Trigger DAG with config\" screen was susceptible to XSS attacks via the `origin` query argument. This issue affects Apache Airflow versions 2.2.3 and below.",
  "repo": "apache/airflow",
  "patch_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
  "patch_info": {
    "commit_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "files": [
      "airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py"
    ],
    "message": "Simplify trigger cancel button (#21591)\n\nCo-authored-by: Jed Cunningham <jedcunningham@apache.org>\n(cherry picked from commit 65297673a318660fba76797e50d0c06804dfcafc)",
    "before_after_code_files": [
      "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
    ]
  },
  "patch_diff": {
    "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html": [
      "File: airflow/www/templates/airflow/trigger.html -> airflow/www/templates/airflow/trigger.html",
      "--- Hunk 1 ---",
      "[Context before]",
      "63:       </label>",
      "64:     </div>",
      "65:     <button type=\"submit\" class=\"btn btn-primary\">Trigger</button>",
      "67:   </form>",
      "68: {% endblock %}",
      "",
      "[Removed Lines]",
      "66:     <button type=\"button\" class=\"btn\" onclick=\"location.href = '{{ origin }}'; return false\">Cancel</button>",
      "",
      "[Added Lines]",
      "66:     <a class=\"btn\" href=\"{{ origin }}\">Cancel</a>",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
      "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "133:         (\"javascript:alert(1)\", \"/home\"),",
      "134:         (\"http://google.com\", \"/home\"),",
      "135:         (\"36539'%3balert(1)%2f%2f166\", \"/home\"),",
      "136:         (",
      "137:             \"%2Ftree%3Fdag_id%3Dexample_bash_operator';alert(33)//\",",
      "138:             \"/home\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "136:         (",
      "137:             '\"><script>alert(99)</script><a href=\"',",
      "138:             \"&#34;&gt;&lt;script&gt;alert(99)&lt;/script&gt;&lt;a href=&#34;\",",
      "139:         ),",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "145:     test_dag_id = \"example_bash_operator\"",
      "147:     resp = admin_client.get(f'trigger?dag_id={test_dag_id}&origin={test_origin}')",
      "156: @pytest.mark.parametrize(",
      "",
      "[Removed Lines]",
      "148:     check_content_in_response(",
      "149:         '<button type=\"button\" class=\"btn\" onclick=\"location.href = \\'{}\\'; return false\">'.format(",
      "150:             expected_origin",
      "151:         ),",
      "152:         resp,",
      "153:     )",
      "",
      "[Added Lines]",
      "152:     check_content_in_response(f'<a class=\"btn\" href=\"{expected_origin}\">Cancel</a>', resp)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "82964ef3be722cb6898dd0226af75fb821166b19",
      "candidate_info": {
        "commit_hash": "82964ef3be722cb6898dd0226af75fb821166b19",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/82964ef3be722cb6898dd0226af75fb821166b19",
        "files": [
          "airflow/www/ask_for_recompile_assets_if_needed.sh",
          "scripts/in_container/bin/run_tmux",
          "scripts/in_container/run_tmux_welcome.sh"
        ],
        "message": "Disable yarn-dev in start-airflow command (#19626)\n\nWhen you run start-airflow, by default it also run `yarn dev`\ncommand, however if you've never built assets before, yarn dev\nis very slow first time and the webserver started before the dist\nfolder was even created which caused asset-less airflow experience.\n\nThere was another race condition even if you did build the\nassets before. If you run start-airflow on MacOS or Windows when\nthe filesystem was slow, there could be a case that yarn dev\ncleaned up the dist folder while webserver was starting and\nit could lead again to asset-less experience if you were unlucky.\n\nAlso running `yarn dev` has the side effect of removing the checksum\nfile which is used to see if any of the assets changed and whether\nthey need recompilation. As the result after running `start-airflow`\nyou always got the warning that the assets need recompilation.\n\nThis PR disables automated start of `yarn dev` and suggests to run\nit manually instead if there is a need for dynamic asset\nrecompilation. Also when `start-airflow` is run and we are starting\nairflow from sources rather than PyPI, asset compilation is\nexecuted if the checksum is missing or does not match the source\nfiles.\n\nRelated to: #19566\n\n(cherry picked from commit 510ff6277585d8de411f83b7f0ec0b5d52ce685c)",
        "before_after_code_files": [
          "airflow/www/ask_for_recompile_assets_if_needed.sh||airflow/www/ask_for_recompile_assets_if_needed.sh",
          "scripts/in_container/run_tmux_welcome.sh||scripts/in_container/run_tmux_welcome.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/ask_for_recompile_assets_if_needed.sh||airflow/www/ask_for_recompile_assets_if_needed.sh": [
          "File: airflow/www/ask_for_recompile_assets_if_needed.sh -> airflow/www/ask_for_recompile_assets_if_needed.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: md5sum=$(find package.json yarn.lock static/css static/js -type f | sort | xargs md5sum)",
          "31: old_md5sum=$(cat \"${MD5SUM_FILE}\" 2>/dev/null || true)",
          "32: if [[ ${old_md5sum} != \"${md5sum}\" ]]; then",
          "39: else",
          "40:     echo",
          "41:     echo -e \"${GREEN}No need for www assets recompilation.${NO_COLOR}\"",
          "",
          "[Removed Lines]",
          "33:     echo",
          "34:     echo -e \"${YELLOW}WARNING: It seems that the generated assets files do not match the content of the sources.${NO_COLOR}\"",
          "35:     echo \"To recompile assets, run:\"",
          "36:     echo \"\"",
          "37:     echo \"   ./airflow/www/compile_assets.sh\"",
          "38:     echo \"\"",
          "",
          "[Added Lines]",
          "33:     if [[ ${START_AIRFLOW} == \"true\" && ${USE_AIRFLOW_VERSION} == \"\" ]]; then",
          "34:         echo",
          "35:         echo -e \"${YELLOW}Recompiling assets as they have changed and you need them for 'start_airflow' command${NO_COLOR}\"",
          "36:         echo",
          "37:         ./compile_assets.sh",
          "38:     else",
          "39:         echo",
          "40:         echo -e \"${YELLOW}WARNING: It seems that the generated assets files do not match the content of the sources.${NO_COLOR}\"",
          "41:         echo \"To recompile assets, run:\"",
          "42:         echo \"\"",
          "43:         echo \"   ./airflow/www/compile_assets.sh\"",
          "44:         echo \"\"",
          "45:     fi",
          "",
          "---------------"
        ],
        "scripts/in_container/run_tmux_welcome.sh||scripts/in_container/run_tmux_welcome.sh": [
          "File: scripts/in_container/run_tmux_welcome.sh -> scripts/in_container/run_tmux_welcome.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: clear",
          "20: echo \"Welcome to your tmux based running Airflow environment (courtesy of Breeze).\"",
          "21: echo",
          "23: echo",
          "",
          "[Removed Lines]",
          "22: echo \"     To stop Airflow and exit tmux just type 'stop_airflow'.\"",
          "",
          "[Added Lines]",
          "22: echo \"     To stop Airflow and exit tmux, just type 'stop_airflow'.\"",
          "23: echo",
          "24: echo \"     If you want to rebuild webserver assets dynamically, run 'cd airflow/www; yarn && yarn dev' and restart airflow webserver with '-d' flag.\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0072200713f520e4ecbccd3ffeaa4b6f31e48db8",
      "candidate_info": {
        "commit_hash": "0072200713f520e4ecbccd3ffeaa4b6f31e48db8",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0072200713f520e4ecbccd3ffeaa4b6f31e48db8",
        "files": [
          "airflow/example_dags/example_subdag_operator.py",
          "airflow/example_dags/tutorial.py",
          "airflow/example_dags/tutorial_etl_dag.py",
          "airflow/providers/google/cloud/example_dags/example_functions.py",
          "docs/apache-airflow/best-practices.rst",
          "docs/apache-airflow/concepts/dags.rst",
          "docs/apache-airflow/dag-run.rst",
          "docs/apache-airflow/faq.rst",
          "docs/apache-airflow/lineage.rst",
          "docs/apache-airflow/timezone.rst",
          "docs/apache-airflow/tutorial.rst"
        ],
        "message": "Clean up ``default_args`` usage in docs (#19803)\n\nThis PR aligns `default_args` usage within docs to updates that have been made to example DAGs across the board. The main types of updates include:\n- Removing `start_date` from being declared in `default_args`.\n- Removing the pattern of declaring `default_args` separately from the `DAG()` object.\n- Updating `default_args` values to more relevant examples.\n- Replace `DummyOperator` with another operator to make some other `default_args` updates relevant and applicable.\n\n(cherry picked from commit 744d11bdb2acd52794a959572695943df8729a37)",
        "before_after_code_files": [
          "airflow/example_dags/example_subdag_operator.py||airflow/example_dags/example_subdag_operator.py",
          "airflow/example_dags/tutorial.py||airflow/example_dags/tutorial.py",
          "airflow/example_dags/tutorial_etl_dag.py||airflow/example_dags/tutorial_etl_dag.py",
          "airflow/providers/google/cloud/example_dags/example_functions.py||airflow/providers/google/cloud/example_dags/example_functions.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/example_dags/example_subdag_operator.py||airflow/example_dags/example_subdag_operator.py": [
          "File: airflow/example_dags/example_subdag_operator.py -> airflow/example_dags/example_subdag_operator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "28: DAG_NAME = 'example_subdag_operator'",
          "34: with DAG(",
          "36: ) as dag:",
          "38:     start = DummyOperator(",
          "",
          "[Removed Lines]",
          "30: args = {",
          "31:     'owner': 'airflow',",
          "32: }",
          "35:     dag_id=DAG_NAME, default_args=args, start_date=days_ago(2), schedule_interval=\"@once\", tags=['example']",
          "",
          "[Added Lines]",
          "31:     dag_id=DAG_NAME,",
          "32:     default_args={\"retries\": 2},",
          "33:     start_date=days_ago(2),",
          "34:     schedule_interval=\"@once\",",
          "35:     tags=['example'],",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "42:     section_1 = SubDagOperator(",
          "43:         task_id='section-1',",
          "45:     )",
          "47:     some_other_task = DummyOperator(",
          "",
          "[Removed Lines]",
          "44:         subdag=subdag(DAG_NAME, 'section-1', args),",
          "",
          "[Added Lines]",
          "44:         subdag=subdag(DAG_NAME, 'section-1', dag.default_args),",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "51:     section_2 = SubDagOperator(",
          "52:         task_id='section-2',",
          "54:     )",
          "56:     end = DummyOperator(",
          "",
          "[Removed Lines]",
          "53:         subdag=subdag(DAG_NAME, 'section-2', args),",
          "",
          "[Added Lines]",
          "53:         subdag=subdag(DAG_NAME, 'section-2', dag.default_args),",
          "",
          "---------------"
        ],
        "airflow/example_dags/tutorial.py||airflow/example_dags/tutorial.py": [
          "File: airflow/example_dags/tutorial.py -> airflow/example_dags/tutorial.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "35: # [END import_module]",
          "64: # [START instantiate_dag]",
          "65: with DAG(",
          "66:     'tutorial',",
          "68:     description='A simple tutorial DAG',",
          "69:     schedule_interval=timedelta(days=1),",
          "70:     start_date=datetime(2021, 1, 1),",
          "",
          "[Removed Lines]",
          "37: # [START default_args]",
          "38: # These args will get passed on to each operator",
          "39: # You can override them on a per-task basis during operator initialization",
          "40: default_args = {",
          "41:     'owner': 'airflow',",
          "42:     'depends_on_past': False,",
          "43:     'email': ['airflow@example.com'],",
          "44:     'email_on_failure': False,",
          "45:     'email_on_retry': False,",
          "46:     'retries': 1,",
          "47:     'retry_delay': timedelta(minutes=5),",
          "48:     # 'queue': 'bash_queue',",
          "49:     # 'pool': 'backfill',",
          "50:     # 'priority_weight': 10,",
          "51:     # 'end_date': datetime(2016, 1, 1),",
          "52:     # 'wait_for_downstream': False,",
          "53:     # 'dag': dag,",
          "54:     # 'sla': timedelta(hours=2),",
          "55:     # 'execution_timeout': timedelta(seconds=300),",
          "56:     # 'on_failure_callback': some_function,",
          "57:     # 'on_success_callback': some_other_function,",
          "58:     # 'on_retry_callback': another_function,",
          "59:     # 'sla_miss_callback': yet_another_function,",
          "60:     # 'trigger_rule': 'all_success'",
          "61: }",
          "62: # [END default_args]",
          "67:     default_args=default_args,",
          "",
          "[Added Lines]",
          "41:     # [START default_args]",
          "42:     # These args will get passed on to each operator",
          "43:     # You can override them on a per-task basis during operator initialization",
          "44:     default_args={",
          "45:         'depends_on_past': False,",
          "46:         'email': ['airflow@example.com'],",
          "47:         'email_on_failure': False,",
          "48:         'email_on_retry': False,",
          "49:         'retries': 1,",
          "50:         'retry_delay': timedelta(minutes=5),",
          "51:         # 'queue': 'bash_queue',",
          "52:         # 'pool': 'backfill',",
          "53:         # 'priority_weight': 10,",
          "54:         # 'end_date': datetime(2016, 1, 1),",
          "55:         # 'wait_for_downstream': False,",
          "56:         # 'sla': timedelta(hours=2),",
          "57:         # 'execution_timeout': timedelta(seconds=300),",
          "58:         # 'on_failure_callback': some_function,",
          "59:         # 'on_success_callback': some_other_function,",
          "60:         # 'on_retry_callback': another_function,",
          "61:         # 'sla_miss_callback': yet_another_function,",
          "62:         # 'trigger_rule': 'all_success'",
          "63:     },",
          "64:     # [END default_args]",
          "",
          "---------------"
        ],
        "airflow/example_dags/tutorial_etl_dag.py||airflow/example_dags/tutorial_etl_dag.py": [
          "File: airflow/example_dags/tutorial_etl_dag.py -> airflow/example_dags/tutorial_etl_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "38: # [END import_module]",
          "48: # [START instantiate_dag]",
          "49: with DAG(",
          "50:     'tutorial_etl_dag',",
          "52:     description='ETL DAG tutorial',",
          "53:     schedule_interval=None,",
          "54:     start_date=datetime(2021, 1, 1),",
          "",
          "[Removed Lines]",
          "40: # [START default_args]",
          "41: # These args will get passed on to each operator",
          "42: # You can override them on a per-task basis during operator initialization",
          "43: default_args = {",
          "44:     'owner': 'airflow',",
          "45: }",
          "46: # [END default_args]",
          "51:     default_args=default_args,",
          "",
          "[Added Lines]",
          "43:     # [START default_args]",
          "44:     # These args will get passed on to each operator",
          "45:     # You can override them on a per-task basis during operator initialization",
          "46:     default_args={'retries': 2},",
          "47:     # [END default_args]",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/example_dags/example_functions.py||airflow/providers/google/cloud/example_dags/example_functions.py": [
          "File: airflow/providers/google/cloud/example_dags/example_functions.py -> airflow/providers/google/cloud/example_dags/example_functions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "75: # [END howto_operator_gcf_deploy_body]",
          "77: # [START howto_operator_gcf_default_args]",
          "79: # [END howto_operator_gcf_default_args]",
          "81: # [START howto_operator_gcf_deploy_variants]",
          "",
          "[Removed Lines]",
          "78: default_args = {'owner': 'airflow'}",
          "",
          "[Added Lines]",
          "78: default_args = {'retries': '3'}",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "fa83fa4c4ef80220fd95a8e0a9282ba65fb625af",
      "candidate_info": {
        "commit_hash": "fa83fa4c4ef80220fd95a8e0a9282ba65fb625af",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/fa83fa4c4ef80220fd95a8e0a9282ba65fb625af",
        "files": [
          "Dockerfile",
          "Dockerfile.ci",
          "IMAGES.rst",
          "docs/docker-stack/build-arg-ref.rst",
          "docs/docker-stack/docker-examples/customizing/add-build-essential-custom.sh",
          "docs/docker-stack/docker-examples/customizing/pypi-dev-runtime-deps.sh"
        ],
        "message": "Update base python image to be Python 3.7 by default (#20978)\n\nDefault base python image should be set to Python 3.7 as we\ndropped Python 3.6 support.\n\n(cherry picked from commit 7ee4295dd3f7dba4fcd763286c7823bb1707fe99)",
        "before_after_code_files": [
          "Dockerfile.ci||Dockerfile.ci"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "Dockerfile.ci||Dockerfile.ci": [
          "File: Dockerfile.ci -> Dockerfile.ci",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: #",
          "16: # WARNING: THIS DOCKERFILE IS NOT INTENDED FOR PRODUCTION USE OR DEPLOYMENT.",
          "17: #",
          "19: FROM ${PYTHON_BASE_IMAGE} as main",
          "21: # Nolog bash flag is currently ignored - but you can replace it with other flags (for example",
          "22: # xtrace - to show commands executed)",
          "23: SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-o\", \"errexit\", \"-o\", \"nounset\", \"-o\", \"nolog\", \"-c\"]",
          "26: ARG AIRFLOW_VERSION=\"2.2.0.dev0\"",
          "27: ARG AIRFLOW_IMAGE_REPOSITORY=\"https://github.com/apache/airflow\"",
          "",
          "[Removed Lines]",
          "18: ARG PYTHON_BASE_IMAGE=\"python:3.6-slim-buster\"",
          "25: ARG PYTHON_BASE_IMAGE=\"python:3.6-slim-buster\"",
          "",
          "[Added Lines]",
          "18: ARG PYTHON_BASE_IMAGE=\"python:3.7-slim-buster\"",
          "25: ARG PYTHON_BASE_IMAGE=\"python:3.7-slim-buster\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cf9051107865d6e3f4d2baae3bc02491a27f0fcc",
      "candidate_info": {
        "commit_hash": "cf9051107865d6e3f4d2baae3bc02491a27f0fcc",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cf9051107865d6e3f4d2baae3bc02491a27f0fcc",
        "files": [
          "Dockerfile",
          "docs/docker-stack/README.md",
          "docs/docker-stack/index.rst",
          "scripts/ci/libraries/_build_images.sh",
          "scripts/ci/pre_commit/pre_commit_update_versions.py"
        ],
        "message": "Add image labels required by ArtifactHub (#21040)\n\n(cherry picked from commit 7f02b4718bc9e282c1d59b0aab5dd46972b6f79c)",
        "before_after_code_files": [
          "scripts/ci/libraries/_build_images.sh||scripts/ci/libraries/_build_images.sh",
          "scripts/ci/pre_commit/pre_commit_update_versions.py||scripts/ci/pre_commit/pre_commit_update_versions.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/libraries/_build_images.sh||scripts/ci/libraries/_build_images.sh": [
          "File: scripts/ci/libraries/_build_images.sh -> scripts/ci/libraries/_build_images.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "673:         --build-arg AIRFLOW_CONSTRAINTS=\"${AIRFLOW_CONSTRAINTS}\" \\",
          "674:         --build-arg AIRFLOW_IMAGE_REPOSITORY=\"https://github.com/${GITHUB_REPOSITORY}\" \\",
          "675:         --build-arg AIRFLOW_IMAGE_DATE_CREATED=\"$(date -u +'%Y-%m-%dT%H:%M:%SZ')\" \\",
          "676:         \"${additional_dev_args[@]}\" \\",
          "677:         \"${additional_runtime_args[@]}\" \\",
          "678:         \"${docker_cache_prod_directive[@]}\" \\",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "676:         --build-arg AIRFLOW_IMAGE_README_URL=\"https://raw.githubusercontent.com/apache/airflow/${COMMIT_SHA}/docs/docker-stack/README.md\" \\",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_update_versions.py||scripts/ci/pre_commit/pre_commit_update_versions.py": [
          "File: scripts/ci/pre_commit/pre_commit_update_versions.py -> scripts/ci/pre_commit/pre_commit_update_versions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "32: def update_version(pattern: re.Pattern, v: str, file_path: str):",
          "34:     with open(file_path, \"r+\") as f:",
          "35:         file_content = f.read()",
          "36:         if not pattern.search(file_content):",
          "",
          "[Removed Lines]",
          "33:     print(f\"Replacing {pattern} to {version} in {file_path}\")",
          "",
          "[Added Lines]",
          "33:     print(f\"Checking {pattern} in {file_path}\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "38:         new_content = pattern.sub(fr'\\g<1>{v}\\g<2>', file_content)",
          "39:         if file_content == new_content:",
          "40:             return",
          "41:         f.seek(0)",
          "42:         f.truncate()",
          "43:         f.write(new_content)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "41:         print(\"    Updated.\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "46: REPLACEMENTS = {",
          "47:     r'^(FROM apache\\/airflow:).*($)': \"docs/docker-stack/docker-examples/extending/*/Dockerfile\",",
          "48:     r'(apache\\/airflow:)[^-]*(\\-)': \"docs/docker-stack/entrypoint.rst\",",
          "49: }",
          "51: if __name__ == '__main__':",
          "52:     for regexp, p in REPLACEMENTS.items():",
          "53:         text_pattern = re.compile(regexp, flags=re.MULTILINE)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "50:     r'(`apache/airflow:)[0-9].*?((?:-pythonX.Y)?`)': \"docs/docker-stack/README.md\",",
          "51:     r'(\\(Assuming Airflow version `).*(`\\))': \"docs/docker-stack/README.md\",",
          "54: print(f\"Current version: {version}\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ad00e8e6aa23496d9e14108702d5702121b63827",
      "candidate_info": {
        "commit_hash": "ad00e8e6aa23496d9e14108702d5702121b63827",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ad00e8e6aa23496d9e14108702d5702121b63827",
        "files": [
          "tests/jobs/test_scheduler_job.py",
          "tests/test_utils/asserts.py"
        ],
        "message": "Restore stability and unquarantine all test_scheduler_job tests (#19860)\n\n* Restore stability and unquarantine all test_scheduler_job tests\n\nThe scheduler job tests were pretty flaky and some of them were\nquarantined already (especially the query count). This PR improves\nthe stability in the following ways:\n\n* clean the database between tests for TestSchedulerJob to avoid\n  side effects\n* forces UTC timezone in tests where date missed timezone specs\n* updates number of queries expected in the query count tests\n* stabilizes the sequence of retrieval of tasks in case tests\n  depended on it\n* adds more stack trace levels (5) to compare where extra\n  methods were called.\n* increase number of scheduler runs where it was needed\n* add session.flush() where it was missing\n* add requirement to have serialized dags ready when needed\n* increase dagruns number to process where we could have\n  some \"too slow\" tests comparing to fast processing of\n  dag runs.\n\nHopefully:\n\n* Fixes: #18777\n* Fixes: #17291\n* Fixes: #17224\n* Fixes: #15255\n* Fixes: #15085\n\n* Update tests/jobs/test_scheduler_job.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit 9b277dbb9b77c74a9799d64e01e0b86b7c1d1542)",
        "before_after_code_files": [
          "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py",
          "tests/test_utils/asserts.py||tests/test_utils/asserts.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py": [
          "File: tests/jobs/test_scheduler_job.py -> tests/jobs/test_scheduler_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import shutil",
          "23: from datetime import timedelta",
          "24: from tempfile import mkdtemp",
          "25: from unittest import mock",
          "26: from unittest.mock import MagicMock, patch",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: from typing import Generator, Optional",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "66: )",
          "67: from tests.test_utils.mock_executor import MockExecutor",
          "68: from tests.test_utils.mock_operators import CustomOperator",
          "70: ROOT_FOLDER = os.path.realpath(",
          "71:     os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir, os.pardir)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "70: from tests.utils.test_timezone import UTC",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "110:         # The tests expect DAGs to be fully loaded here via setUpClass method below",
          "112:     @pytest.fixture(autouse=True)",
          "115:         self.clean_db()",
          "116:         self.scheduler_job = None",
          "117:         # Speed up some tests by not running the tasks, just look at what we",
          "118:         # enqueue!",
          "121:         # Since we don't want to store the code for the DAG defined in this file",
          "122:         with patch('airflow.dag_processing.manager.SerializedDagModel.remove_deleted_dags'), patch(",
          "",
          "[Removed Lines]",
          "113:     def set_instance_attrs(self, dagbag):",
          "114:         self.dagbag = dagbag",
          "119:         self.null_exec = MockExecutor()",
          "",
          "[Added Lines]",
          "115:     def per_test(self) -> Generator:",
          "119:         yield",
          "121:         if self.scheduler_job and self.scheduler_job.processor_agent:",
          "122:             self.scheduler_job.processor_agent.end()",
          "123:             self.scheduler_job = None",
          "124:         self.clean_db()",
          "126:     @pytest.fixture(autouse=True)",
          "127:     def set_instance_attrs(self, dagbag) -> Generator:",
          "128:         self.dagbag = dagbag",
          "131:         self.null_exec: Optional[MockExecutor] = MockExecutor()",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "124:         ):",
          "125:             yield",
          "132:     def test_is_alive(self):",
          "133:         self.scheduler_job = SchedulerJob(None, heartrate=10, state=State.RUNNING)",
          "",
          "[Removed Lines]",
          "127:         if self.scheduler_job and self.scheduler_job.processor_agent:",
          "128:             self.scheduler_job.processor_agent.end()",
          "129:             self.scheduler_job = None",
          "130:         self.clean_db()",
          "",
          "[Added Lines]",
          "139:         self.null_exec = None",
          "140:         self.dagbag = None",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "166:         self.scheduler_job.heartrate = 0",
          "167:         self.scheduler_job.run()",
          "170:     def test_no_orphan_process_will_be_left(self):",
          "171:         empty_dir = mkdtemp()",
          "172:         current_process = psutil.Process()",
          "",
          "[Removed Lines]",
          "169:     @pytest.mark.quarantined",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "443:         task_id_2 = 'dummydummy'",
          "444:         session = settings.Session()",
          "445:         with dag_maker(dag_id=dag_id, max_active_tasks=16, session=session):",
          "449:         self.scheduler_job = SchedulerJob(subdir=os.devnull)",
          "451:         dr1 = dag_maker.create_dagrun(run_type=DagRunType.SCHEDULED)",
          "452:         dr2 = dag_maker.create_dagrun_after(dr1, run_type=DagRunType.SCHEDULED)",
          "455:         for ti in tis:",
          "456:             ti.state = State.SCHEDULED",
          "457:             session.merge(ti)",
          "",
          "[Removed Lines]",
          "446:             DummyOperator(task_id=task_id_1, pool='a')",
          "447:             DummyOperator(task_id=task_id_2, pool='b')",
          "454:         tis = dr1.task_instances + dr2.task_instances",
          "",
          "[Added Lines]",
          "455:             DummyOperator(task_id=task_id_1, pool='a', priority_weight=2)",
          "456:             DummyOperator(task_id=task_id_2, pool='b', priority_weight=1)",
          "463:         tis = [",
          "464:             dr1.get_task_instance(task_id_1, session=session),",
          "465:             dr1.get_task_instance(task_id_2, session=session),",
          "466:             dr2.get_task_instance(task_id_1, session=session),",
          "467:             dr2.get_task_instance(task_id_2, session=session),",
          "468:         ]",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "1705:             session.commit()",
          "1706:             assert [] == self.null_exec.sorted_tasks",
          "1709:     def test_scheduler_task_start_date(self):",
          "1710:         \"\"\"",
          "1711:         Test that the scheduler respects task start dates that are different from DAG start dates",
          "1712:         \"\"\"",
          "1715:         dag_id = 'test_task_start_date_scheduling'",
          "1716:         dag = self.dagbag.get_dag(dag_id)",
          "1717:         dag.is_paused_upon_creation = False",
          "",
          "[Removed Lines]",
          "1708:     @pytest.mark.quarantined",
          "1714:         dagbag = DagBag(dag_folder=os.path.join(settings.DAGS_FOLDER, \"no_dags.py\"), include_examples=False)",
          "",
          "[Added Lines]",
          "1727:         dagbag = DagBag(",
          "1728:             dag_folder=os.path.join(settings.DAGS_FOLDER, \"test_scheduler_dags.py\"), include_examples=False",
          "1729:         )",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "1725:         dagbag.sync_to_db()",
          "1728:         self.scheduler_job.run()",
          "1730:         session = settings.Session()",
          "",
          "[Removed Lines]",
          "1727:         self.scheduler_job = SchedulerJob(executor=self.null_exec, subdir=dag.fileloc, num_runs=2)",
          "",
          "[Added Lines]",
          "1742:         self.scheduler_job = SchedulerJob(executor=self.null_exec, subdir=dag.fileloc, num_runs=3)",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "1732:         ti1s = tiq.filter(TaskInstance.task_id == 'dummy1').all()",
          "1733:         ti2s = tiq.filter(TaskInstance.task_id == 'dummy2').all()",
          "1734:         assert len(ti1s) == 0",
          "1736:         for task in ti2s:",
          "1737:             assert task.state == State.SUCCESS",
          "",
          "[Removed Lines]",
          "1735:         assert len(ti2s) == 2",
          "",
          "[Added Lines]",
          "1750:         assert len(ti2s) >= 2",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1757:         session = settings.Session()",
          "1758:         assert len(session.query(TaskInstance).filter(TaskInstance.dag_id == dag_id).all()) == 0",
          "1785:     def test_scheduler_verify_pool_full(self, dag_maker):",
          "1786:         \"\"\"",
          "1787:         Test task instances not queued when pool is full",
          "",
          "[Removed Lines]",
          "1760:     @conf_vars({(\"core\", \"mp_start_method\"): \"spawn\"})",
          "1761:     def test_scheduler_multiprocessing_with_spawn_method(self):",
          "1762:         \"\"\"",
          "1763:         Test that the scheduler can successfully queue multiple dags in parallel",
          "1764:         when using \"spawn\" mode of multiprocessing. (Fork is default on Linux and older OSX)",
          "1765:         \"\"\"",
          "1766:         dag_ids = ['test_start_date_scheduling', 'test_dagrun_states_success']",
          "1767:         for dag_id in dag_ids:",
          "1768:             dag = self.dagbag.get_dag(dag_id)",
          "1769:             dag.clear()",
          "1771:         self.scheduler_job = SchedulerJob(",
          "1772:             executor=self.null_exec,",
          "1773:             subdir=os.path.join(TEST_DAG_FOLDER, 'test_scheduler_dags.py'),",
          "1774:             num_runs=1,",
          "1775:         )",
          "1777:         self.scheduler_job.run()",
          "1779:         # zero tasks ran",
          "1780:         dag_id = 'test_start_date_scheduling'",
          "1781:         with create_session() as session:",
          "1782:             assert session.query(TaskInstance).filter(TaskInstance.dag_id == dag_id).count() == 0",
          "1784:     @pytest.mark.quarantined",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "1808:         self.scheduler_job._schedule_dag_run(dr, session)",
          "1809:         dr = dag_maker.create_dagrun_after(dr, run_type=DagRunType.SCHEDULED, state=State.RUNNING)",
          "1810:         self.scheduler_job._schedule_dag_run(dr, session)",
          "1811:         task_instances_list = self.scheduler_job._executable_task_instances_to_queued(",
          "1812:             max_tis=32, session=session",
          "1813:         )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1801:         session.flush()",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "1858:         # As tasks require 2 slots, only 3 can fit into 6 available",
          "1859:         assert len(task_instances_list) == 3",
          "1862:     def test_scheduler_keeps_scheduling_pool_full(self, dag_maker):",
          "1863:         \"\"\"",
          "1864:         Test task instances in a pool that isn't full keep getting scheduled even when a pool is full.",
          "",
          "[Removed Lines]",
          "1861:     @pytest.mark.quarantined",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "1898:         def _create_dagruns(dag: DAG):",
          "1899:             next_info = dag.next_dagrun_info(None)",
          "1901:                 yield dag.create_dagrun(",
          "1902:                     run_type=DagRunType.SCHEDULED,",
          "1903:                     execution_date=next_info.logical_date,",
          "1904:                     data_interval=next_info.data_interval,",
          "1906:                 )",
          "1907:                 next_info = dag.next_dagrun_info(next_info.data_interval)",
          "1910:         # To increase the chances the TIs from the \"full\" pool will get retrieved first, we schedule all",
          "1911:         # TIs from the first dag first.",
          "1912:         for dr in _create_dagruns(dag_d1):",
          "",
          "[Removed Lines]",
          "1900:             for _ in range(5):",
          "1905:                     state=State.RUNNING,",
          "1909:         # Create 5 dagruns for each DAG.",
          "",
          "[Added Lines]",
          "1890:             assert next_info is not None",
          "1891:             for _ in range(30):",
          "1896:                     state=DagRunState.RUNNING,",
          "1900:         # Create 30 dagruns for each DAG.",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "2048:         session.rollback()",
          "2049:         session.close()",
          "2052:     def test_verify_integrity_if_dag_changed(self, dag_maker):",
          "2053:         # CleanUp",
          "2054:         with create_session() as session:",
          "",
          "[Removed Lines]",
          "2051:     @pytest.mark.quarantined",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "2113:         session.rollback()",
          "2114:         session.close()",
          "2117:     @pytest.mark.need_serialized_dag",
          "2118:     def test_retry_still_in_executor(self, dag_maker):",
          "2119:         \"\"\"",
          "",
          "[Removed Lines]",
          "2116:     @pytest.mark.quarantined",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "2889:                 ti.state = State.SUCCESS",
          "2890:                 session.flush()",
          "2892:         with dag_maker(max_active_runs=3, session=session) as dag:",
          "2893:             # Need to use something that doesn't immediately get marked as success by the scheduler",
          "2894:             BashOperator(task_id='task', bash_command='true')",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2881:         self.clean_db()",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "2906:         # Pre-condition",
          "2907:         assert DagRun.active_runs_of_dags(session=session) == {'test_dag': 3}",
          "2916:         assert model.next_dagrun_create_after is None",
          "2918:         complete_one_dagrun()",
          "",
          "[Removed Lines]",
          "2909:         assert model.next_dagrun == timezone.convert_to_utc(",
          "2910:             timezone.DateTime(",
          "2911:                 2016,",
          "2912:                 1,",
          "2913:                 3,",
          "2914:             )",
          "2915:         )",
          "",
          "[Added Lines]",
          "2900:         assert model.next_dagrun == timezone.DateTime(2016, 1, 3, tzinfo=UTC)",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "3423:         assert tis[dummy3.task_id].state == State.SKIPPED",
          "3428: class TestSchedulerJobQueriesCount:",
          "3429:     \"\"\"",
          "3430:     These tests are designed to detect changes in the number of queries for",
          "",
          "[Removed Lines]",
          "3426: # TODO(potiuk): unquarantine me where we get rid of those pesky 195 -> 196 problem!",
          "3427: @pytest.mark.quarantined",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "3456:     @pytest.mark.parametrize(",
          "3457:         \"expected_query_count, dag_count, task_count\",",
          "3458:         [",
          "3462:         ],",
          "3463:     )",
          "3464:     def test_execute_queries_count_with_harvested_dags(self, expected_query_count, dag_count, task_count):",
          "",
          "[Removed Lines]",
          "3459:             (20, 1, 1),  # One DAG with one task per DAG file.",
          "3460:             (20, 1, 5),  # One DAG with five tasks per DAG file.",
          "3461:             (83, 10, 10),  # 10 DAGs with 10 tasks per DAG file.",
          "",
          "[Added Lines]",
          "3442:             (21, 1, 1),  # One DAG with one task per DAG file.",
          "3443:             (21, 1, 5),  # One DAG with five tasks per DAG file.",
          "3444:             (93, 10, 10),  # 10 DAGs with 10 tasks per DAG file.",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "3519:             # One DAG with one task per DAG file.",
          "3520:             ([10, 10, 10, 10], 1, 1, \"1d\", \"None\", \"no_structure\"),",
          "3521:             ([10, 10, 10, 10], 1, 1, \"1d\", \"None\", \"linear\"),",
          "3529:             # One DAG with five tasks per DAG file.",
          "3530:             ([10, 10, 10, 10], 1, 5, \"1d\", \"None\", \"no_structure\"),",
          "3531:             ([10, 10, 10, 10], 1, 5, \"1d\", \"None\", \"linear\"),",
          "3539:             # 10 DAGs with 10 tasks per DAG file.",
          "3540:             ([10, 10, 10, 10], 10, 10, \"1d\", \"None\", \"no_structure\"),",
          "3541:             ([10, 10, 10, 10], 10, 10, \"1d\", \"None\", \"linear\"),",
          "3549:         ],",
          "3550:     )",
          "3551:     def test_process_dags_queries_count(",
          "",
          "[Removed Lines]",
          "3522:             ([23, 13, 13, 13], 1, 1, \"1d\", \"@once\", \"no_structure\"),",
          "3523:             ([23, 13, 13, 13], 1, 1, \"1d\", \"@once\", \"linear\"),",
          "3524:             ([23, 24, 26, 28], 1, 1, \"1d\", \"30m\", \"no_structure\"),",
          "3525:             ([23, 24, 26, 28], 1, 1, \"1d\", \"30m\", \"linear\"),",
          "3526:             ([23, 24, 26, 28], 1, 1, \"1d\", \"30m\", \"binary_tree\"),",
          "3527:             ([23, 24, 26, 28], 1, 1, \"1d\", \"30m\", \"star\"),",
          "3528:             ([23, 24, 26, 28], 1, 1, \"1d\", \"30m\", \"grid\"),",
          "3532:             ([23, 13, 13, 13], 1, 5, \"1d\", \"@once\", \"no_structure\"),",
          "3533:             ([24, 14, 14, 14], 1, 5, \"1d\", \"@once\", \"linear\"),",
          "3534:             ([23, 24, 26, 28], 1, 5, \"1d\", \"30m\", \"no_structure\"),",
          "3535:             ([24, 26, 29, 32], 1, 5, \"1d\", \"30m\", \"linear\"),",
          "3536:             ([24, 26, 29, 32], 1, 5, \"1d\", \"30m\", \"binary_tree\"),",
          "3537:             ([24, 26, 29, 32], 1, 5, \"1d\", \"30m\", \"star\"),",
          "3538:             ([24, 26, 29, 32], 1, 5, \"1d\", \"30m\", \"grid\"),",
          "3542:             ([95, 28, 28, 28], 10, 10, \"1d\", \"@once\", \"no_structure\"),",
          "3543:             ([105, 41, 41, 41], 10, 10, \"1d\", \"@once\", \"linear\"),",
          "3544:             ([95, 99, 99, 99], 10, 10, \"1d\", \"30m\", \"no_structure\"),",
          "3545:             ([105, 125, 125, 125], 10, 10, \"1d\", \"30m\", \"linear\"),",
          "3546:             ([105, 119, 119, 119], 10, 10, \"1d\", \"30m\", \"binary_tree\"),",
          "3547:             ([105, 119, 119, 119], 10, 10, \"1d\", \"30m\", \"star\"),",
          "3548:             ([105, 119, 119, 119], 10, 10, \"1d\", \"30m\", \"grid\"),",
          "",
          "[Added Lines]",
          "3505:             ([24, 14, 14, 14], 1, 1, \"1d\", \"@once\", \"no_structure\"),",
          "3506:             ([24, 14, 14, 14], 1, 1, \"1d\", \"@once\", \"linear\"),",
          "3507:             ([24, 26, 29, 32], 1, 1, \"1d\", \"30m\", \"no_structure\"),",
          "3508:             ([24, 26, 29, 32], 1, 1, \"1d\", \"30m\", \"linear\"),",
          "3509:             ([24, 26, 29, 32], 1, 1, \"1d\", \"30m\", \"binary_tree\"),",
          "3510:             ([24, 26, 29, 32], 1, 1, \"1d\", \"30m\", \"star\"),",
          "3511:             ([24, 26, 29, 32], 1, 1, \"1d\", \"30m\", \"grid\"),",
          "3515:             ([24, 14, 14, 14], 1, 5, \"1d\", \"@once\", \"no_structure\"),",
          "3516:             ([25, 15, 15, 15], 1, 5, \"1d\", \"@once\", \"linear\"),",
          "3517:             ([24, 26, 29, 32], 1, 5, \"1d\", \"30m\", \"no_structure\"),",
          "3518:             ([25, 28, 32, 36], 1, 5, \"1d\", \"30m\", \"linear\"),",
          "3519:             ([25, 28, 32, 36], 1, 5, \"1d\", \"30m\", \"binary_tree\"),",
          "3520:             ([25, 28, 32, 36], 1, 5, \"1d\", \"30m\", \"star\"),",
          "3521:             ([25, 28, 32, 36], 1, 5, \"1d\", \"30m\", \"grid\"),",
          "3525:             ([105, 38, 38, 38], 10, 10, \"1d\", \"@once\", \"no_structure\"),",
          "3526:             ([115, 51, 51, 51], 10, 10, \"1d\", \"@once\", \"linear\"),",
          "3527:             ([105, 119, 119, 119], 10, 10, \"1d\", \"30m\", \"no_structure\"),",
          "3528:             ([115, 145, 145, 145], 10, 10, \"1d\", \"30m\", \"linear\"),",
          "3529:             ([115, 139, 139, 139], 10, 10, \"1d\", \"30m\", \"binary_tree\"),",
          "3530:             ([115, 139, 139, 139], 10, 10, \"1d\", \"30m\", \"star\"),",
          "3531:             ([115, 139, 139, 139], 10, 10, \"1d\", \"30m\", \"grid\"),",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "3669:                 assert end_date is None",
          "3670:                 assert duration is None",
          "3672:     def test_catchup_works_correctly(self, dag_maker):",
          "3673:         \"\"\"Test that catchup works correctly\"\"\"",
          "3674:         session = settings.Session()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3655:     @pytest.mark.need_serialized_dag",
          "",
          "---------------"
        ],
        "tests/test_utils/asserts.py||tests/test_utils/asserts.py": [
          "File: tests/test_utils/asserts.py -> tests/test_utils/asserts.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "63:             and __file__ != f.filename",
          "64:             and ('session.py' not in f.filename and f.name != 'wrapper')",
          "65:         ]",
          "67:         self.result[f\"{stack_info}\"] += 1",
          "",
          "[Removed Lines]",
          "66:         stack_info = \">\".join([f\"{f.filename.rpartition('/')[-1]}:{f.name}:{f.lineno}\" for f in stack][-3:])",
          "",
          "[Added Lines]",
          "66:         stack_info = \">\".join([f\"{f.filename.rpartition('/')[-1]}:{f.name}:{f.lineno}\" for f in stack][-5:])",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "75:     with count_queries() as result:",
          "76:         yield None",
          "78:     count = sum(result.values())",
          "80:         message_fmt = (",
          "81:             message_fmt",
          "83:             \"The current number is {current_count}.\\n\\n\"",
          "84:             \"Recorded query locations:\"",
          "85:         )",
          "88:         for location, count in result.items():",
          "89:             message += f'\\n\\t{location}:\\t{count}'",
          "",
          "[Removed Lines]",
          "79:     if expected_count != count:",
          "82:             or \"The expected number of db queries is {expected_count}. \"",
          "86:         message = message_fmt.format(current_count=count, expected_count=expected_count)",
          "",
          "[Added Lines]",
          "78:     # This is a margin we have for queries - we do not want to change it every time we",
          "79:     # changed queries, but we want to catch cases where we spin out of control",
          "80:     margin = 15",
          "83:     if count > expected_count + margin:",
          "86:             or \"The expected number of db queries is {expected_count} with extra margin: {margin}. \"",
          "90:         message = message_fmt.format(current_count=count, expected_count=expected_count, margin=margin)",
          "",
          "---------------"
        ]
      }
    }
  ]
}