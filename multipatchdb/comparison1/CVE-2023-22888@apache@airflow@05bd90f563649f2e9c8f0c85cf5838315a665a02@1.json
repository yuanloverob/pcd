{
  "cve_id": "CVE-2023-22888",
  "cve_desc": "Apache Airflow, versions before 2.6.3, is affected by a vulnerability that allows an attacker to cause a service disruption by manipulating the run_id parameter. This vulnerability is considered low since it requires an authenticated user to exploit it. It is recommended to upgrade to a version that is not affected",
  "repo": "apache/airflow",
  "patch_hash": "05bd90f563649f2e9c8f0c85cf5838315a665a02",
  "patch_info": {
    "commit_hash": "05bd90f563649f2e9c8f0c85cf5838315a665a02",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/05bd90f563649f2e9c8f0c85cf5838315a665a02",
    "files": [
      "airflow/config_templates/config.yml",
      "airflow/config_templates/default_airflow.cfg",
      "airflow/models/dag.py",
      "airflow/models/dagrun.py",
      "airflow/www/views.py",
      "tests/models/test_dagrun.py",
      "tests/www/views/test_views_trigger_dag.py"
    ],
    "message": "Sanitize `DagRun.run_id` and allow flexibility (#32293)\n\nThis commit sanitizes the DagRun.run_id parameter by introducing a configurable option.\nUsers now have the ability to select a specific run_id pattern for their runs,\nensuring stricter control over the values used. This update does not impact the default run_id\ngeneration performed by the scheduler for scheduled DAG runs or for Dag runs triggered without\nmodifying the run_id parameter in the run configuration page.\nThe configuration flexibility empowers users to align the run_id pattern with their specific requirements.",
    "before_after_code_files": [
      "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg",
      "airflow/models/dag.py||airflow/models/dag.py",
      "airflow/models/dagrun.py||airflow/models/dagrun.py",
      "airflow/www/views.py||airflow/www/views.py",
      "tests/models/test_dagrun.py||tests/models/test_dagrun.py",
      "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
    ]
  },
  "patch_diff": {
    "airflow/config_templates/default_airflow.cfg||airflow/config_templates/default_airflow.cfg": [
      "File: airflow/config_templates/default_airflow.cfg -> airflow/config_templates/default_airflow.cfg",
      "--- Hunk 1 ---",
      "[Context before]",
      "1325: # longer than `[scheduler] task_queued_timeout`.",
      "1326: task_queued_timeout_check_interval = 120.0",
      "1328: [triggerer]",
      "1329: # How many triggers a single Triggerer will run at once, by default.",
      "1330: default_capacity = 1000",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1328: # The run_id pattern used to verify the validity of user input to the run_id parameter when",
      "1329: # triggering a DAG. This pattern cannot change the pattern used by scheduler to generate run_id",
      "1330: # for scheduled DAG runs or DAG runs triggered without changing the run_id parameter.",
      "1331: allowed_run_id_pattern = ^[A-Za-z0-9_.~:+-]+$",
      "",
      "---------------"
    ],
    "airflow/models/dag.py||airflow/models/dag.py": [
      "File: airflow/models/dag.py -> airflow/models/dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "80: import airflow.templates",
      "81: from airflow import settings, utils",
      "82: from airflow.api_internal.internal_api_call import internal_api_call",
      "84: from airflow.exceptions import (",
      "85:     AirflowDagInconsistent,",
      "86:     AirflowException,",
      "",
      "[Removed Lines]",
      "83: from airflow.configuration import conf, secrets_backend_list",
      "",
      "[Added Lines]",
      "83: from airflow.configuration import conf as airflow_conf, secrets_backend_list",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "96: from airflow.models.baseoperator import BaseOperator",
      "97: from airflow.models.dagcode import DagCode",
      "98: from airflow.models.dagpickle import DagPickle",
      "100: from airflow.models.operator import Operator",
      "101: from airflow.models.param import DagParam, ParamsDict",
      "102: from airflow.models.taskinstance import Context, TaskInstance, TaskInstanceKey, clear_task_instances",
      "",
      "[Removed Lines]",
      "99: from airflow.models.dagrun import DagRun",
      "",
      "[Added Lines]",
      "99: from airflow.models.dagrun import RUN_ID_REGEX, DagRun",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "422:         user_defined_filters: dict | None = None,",
      "423:         default_args: dict | None = None,",
      "424:         concurrency: int | None = None,",
      "427:         dagrun_timeout: timedelta | None = None,",
      "428:         sla_miss_callback: None | SLAMissCallback | list[SLAMissCallback] = None,",
      "432:         on_success_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,",
      "433:         on_failure_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,",
      "434:         doc_md: str | None = None,",
      "",
      "[Removed Lines]",
      "425:         max_active_tasks: int = conf.getint(\"core\", \"max_active_tasks_per_dag\"),",
      "426:         max_active_runs: int = conf.getint(\"core\", \"max_active_runs_per_dag\"),",
      "429:         default_view: str = conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower(),",
      "430:         orientation: str = conf.get_mandatory_value(\"webserver\", \"dag_orientation\"),",
      "431:         catchup: bool = conf.getboolean(\"scheduler\", \"catchup_by_default\"),",
      "",
      "[Added Lines]",
      "425:         max_active_tasks: int = airflow_conf.getint(\"core\", \"max_active_tasks_per_dag\"),",
      "426:         max_active_runs: int = airflow_conf.getint(\"core\", \"max_active_runs_per_dag\"),",
      "429:         default_view: str = airflow_conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower(),",
      "430:         orientation: str = airflow_conf.get_mandatory_value(\"webserver\", \"dag_orientation\"),",
      "431:         catchup: bool = airflow_conf.getboolean(\"scheduler\", \"catchup_by_default\"),",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "2588:         mark_success=False,",
      "2589:         local=False,",
      "2590:         executor=None,",
      "2592:         ignore_task_deps=False,",
      "2593:         ignore_first_depends_on_past=True,",
      "2594:         pool=None,",
      "",
      "[Removed Lines]",
      "2591:         donot_pickle=conf.getboolean(\"core\", \"donot_pickle\"),",
      "",
      "[Added Lines]",
      "2591:         donot_pickle=airflow_conf.getboolean(\"core\", \"donot_pickle\"),",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "2826:                 \"Creating DagRun needs either `run_id` or both `run_type` and `execution_date`\"",
      "2827:             )",
      "2837:         # create a copy of params before validating",
      "2838:         copied_params = copy.deepcopy(self.params)",
      "",
      "[Removed Lines]",
      "2829:         if run_id and \"/\" in run_id:",
      "2830:             warnings.warn(",
      "2831:                 \"Using forward slash ('/') in a DAG run ID is deprecated. Note that this character \"",
      "2832:                 \"also makes the run impossible to retrieve via Airflow's REST API.\",",
      "2833:                 RemovedInAirflow3Warning,",
      "2834:                 stacklevel=3,",
      "2835:             )",
      "",
      "[Added Lines]",
      "2829:         regex = airflow_conf.get(\"scheduler\", \"allowed_run_id_pattern\")",
      "2831:         if run_id and not re.match(RUN_ID_REGEX, run_id):",
      "2832:             if not regex.strip() or not re.match(regex.strip(), run_id):",
      "2833:                 raise AirflowException(",
      "2834:                     f\"The provided run ID '{run_id}' is invalid. It does not match either \"",
      "2835:                     f\"the configured pattern: '{regex}' or the built-in pattern: '{RUN_ID_REGEX}'\"",
      "2836:                 )",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "3125:     def get_default_view(self):",
      "3126:         \"\"\"This is only there for backward compatible jinja2 templates.\"\"\"",
      "3127:         if self.default_view is None:",
      "3129:         else:",
      "3130:             return self.default_view",
      "",
      "[Removed Lines]",
      "3128:             return conf.get(\"webserver\", \"dag_default_view\").lower()",
      "",
      "[Added Lines]",
      "3129:             return airflow_conf.get(\"webserver\", \"dag_default_view\").lower()",
      "",
      "---------------",
      "--- Hunk 7 ---",
      "[Context before]",
      "3342:     root_dag_id = Column(StringID())",
      "3343:     # A DAG can be paused from the UI / DB",
      "3344:     # Set this default value of is_paused based on a configuration value!",
      "3346:     is_paused = Column(Boolean, default=is_paused_at_creation)",
      "3347:     # Whether the DAG is a subdag",
      "3348:     is_subdag = Column(Boolean, default=False)",
      "",
      "[Removed Lines]",
      "3345:     is_paused_at_creation = conf.getboolean(\"core\", \"dags_are_paused_at_creation\")",
      "",
      "[Added Lines]",
      "3346:     is_paused_at_creation = airflow_conf.getboolean(\"core\", \"dags_are_paused_at_creation\")",
      "",
      "---------------",
      "--- Hunk 8 ---",
      "[Context before]",
      "3416:         \"TaskOutletDatasetReference\",",
      "3417:         cascade=\"all, delete, delete-orphan\",",
      "3418:     )",
      "3421:     def __init__(self, concurrency=None, **kwargs):",
      "3422:         super().__init__(**kwargs)",
      "",
      "[Removed Lines]",
      "3419:     NUM_DAGS_PER_DAGRUN_QUERY = conf.getint(\"scheduler\", \"max_dagruns_to_create_per_loop\", fallback=10)",
      "",
      "[Added Lines]",
      "3420:     NUM_DAGS_PER_DAGRUN_QUERY = airflow_conf.getint(",
      "3421:         \"scheduler\", \"max_dagruns_to_create_per_loop\", fallback=10",
      "3422:     )",
      "",
      "---------------",
      "--- Hunk 9 ---",
      "[Context before]",
      "3429:                 )",
      "3430:                 self.max_active_tasks = concurrency",
      "3431:             else:",
      "3434:         if self.max_active_runs is None:",
      "3437:         if self.has_task_concurrency_limits is None:",
      "3438:             # Be safe -- this will be updated later once the DAG is parsed",
      "",
      "[Removed Lines]",
      "3432:                 self.max_active_tasks = conf.getint(\"core\", \"max_active_tasks_per_dag\")",
      "3435:             self.max_active_runs = conf.getint(\"core\", \"max_active_runs_per_dag\")",
      "",
      "[Added Lines]",
      "3435:                 self.max_active_tasks = airflow_conf.getint(\"core\", \"max_active_tasks_per_dag\")",
      "3438:             self.max_active_runs = airflow_conf.getint(\"core\", \"max_active_runs_per_dag\")",
      "",
      "---------------",
      "--- Hunk 10 ---",
      "[Context before]",
      "3510:         have a value.",
      "3511:         \"\"\"",
      "3512:         # This is for backwards-compatibility with old dags that don't have None as default_view",
      "3515:     @property",
      "3516:     def safe_dag_id(self):",
      "",
      "[Removed Lines]",
      "3513:         return self.default_view or conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower()",
      "",
      "[Added Lines]",
      "3516:         return self.default_view or airflow_conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower()",
      "",
      "---------------",
      "--- Hunk 11 ---",
      "[Context before]",
      "3699:     user_defined_filters: dict | None = None,",
      "3700:     default_args: dict | None = None,",
      "3701:     concurrency: int | None = None,",
      "3704:     dagrun_timeout: timedelta | None = None,",
      "3705:     sla_miss_callback: None | SLAMissCallback | list[SLAMissCallback] = None,",
      "3709:     on_success_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,",
      "3710:     on_failure_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,",
      "3711:     doc_md: str | None = None,",
      "",
      "[Removed Lines]",
      "3702:     max_active_tasks: int = conf.getint(\"core\", \"max_active_tasks_per_dag\"),",
      "3703:     max_active_runs: int = conf.getint(\"core\", \"max_active_runs_per_dag\"),",
      "3706:     default_view: str = conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower(),",
      "3707:     orientation: str = conf.get_mandatory_value(\"webserver\", \"dag_orientation\"),",
      "3708:     catchup: bool = conf.getboolean(\"scheduler\", \"catchup_by_default\"),",
      "",
      "[Added Lines]",
      "3705:     max_active_tasks: int = airflow_conf.getint(\"core\", \"max_active_tasks_per_dag\"),",
      "3706:     max_active_runs: int = airflow_conf.getint(\"core\", \"max_active_runs_per_dag\"),",
      "3709:     default_view: str = airflow_conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower(),",
      "3710:     orientation: str = airflow_conf.get_mandatory_value(\"webserver\", \"dag_orientation\"),",
      "3711:     catchup: bool = airflow_conf.getboolean(\"scheduler\", \"catchup_by_default\"),",
      "",
      "---------------"
    ],
    "airflow/models/dagrun.py||airflow/models/dagrun.py": [
      "File: airflow/models/dagrun.py -> airflow/models/dagrun.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "24: from datetime import datetime",
      "25: from typing import TYPE_CHECKING, Any, Callable, Iterable, Iterator, NamedTuple, Sequence, TypeVar, overload",
      "27: from sqlalchemy import (",
      "28:     Boolean,",
      "29:     Column,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "27: import re2 as re",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "44: )",
      "45: from sqlalchemy.exc import IntegrityError",
      "46: from sqlalchemy.ext.associationproxy import association_proxy",
      "48: from sqlalchemy.sql.expression import false, select, true",
      "50: from airflow import settings",
      "",
      "[Removed Lines]",
      "47: from sqlalchemy.orm import Query, Session, declared_attr, joinedload, relationship, synonym",
      "",
      "[Added Lines]",
      "48: from sqlalchemy.orm import Query, Session, declared_attr, joinedload, relationship, synonym, validates",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "76:     CreatedTasks = TypeVar(\"CreatedTasks\", Iterator[\"dict[str, Any]\"], Iterator[TI])",
      "77:     TaskCreator = Callable[[Operator, Iterable[int]], CreatedTasks]",
      "80: class TISchedulingDecision(NamedTuple):",
      "81:     \"\"\"Type of return for DagRun.task_instance_scheduling_decisions.\"\"\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "80: RUN_ID_REGEX = r\"^(?:manual|scheduled|dataset_triggered)__(?:\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\+00:00)$\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "240:             external_trigger=self.external_trigger,",
      "241:         )",
      "243:     @property",
      "244:     def stats_tags(self) -> dict[str, str]:",
      "245:         return prune_dict({\"dag_id\": self.dag_id, \"run_type\": self.run_type})",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "246:     @validates(\"run_id\")",
      "247:     def validate_run_id(self, key: str, run_id: str) -> str | None:",
      "248:         if not run_id:",
      "249:             return None",
      "250:         regex = airflow_conf.get(\"scheduler\", \"allowed_run_id_pattern\")",
      "251:         if not re.match(regex, run_id) and not re.match(RUN_ID_REGEX, run_id):",
      "252:             raise ValueError(",
      "253:                 f\"The run_id provided '{run_id}' does not match the pattern '{regex}' or '{RUN_ID_REGEX}'\"",
      "254:             )",
      "255:         return run_id",
      "",
      "---------------"
    ],
    "airflow/www/views.py||airflow/www/views.py": [
      "File: airflow/www/views.py -> airflow/www/views.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "99: from airflow.models.abstractoperator import AbstractOperator",
      "100: from airflow.models.dag import DAG, get_dataset_triggered_next_run_info",
      "101: from airflow.models.dagcode import DagCode",
      "103: from airflow.models.dataset import DagScheduleDatasetReference, DatasetDagRunQueue, DatasetEvent, DatasetModel",
      "104: from airflow.models.mappedoperator import MappedOperator",
      "105: from airflow.models.operator import Operator",
      "",
      "[Removed Lines]",
      "102: from airflow.models.dagrun import DagRun, DagRunType",
      "",
      "[Added Lines]",
      "102: from airflow.models.dagrun import RUN_ID_REGEX, DagRun, DagRunType",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "1975:     @provide_session",
      "1976:     def trigger(self, dag_id: str, session: Session = NEW_SESSION):",
      "1977:         \"\"\"Triggers DAG Run.\"\"\"",
      "1979:         origin = get_safe_url(request.values.get(\"origin\"))",
      "1980:         unpause = request.values.get(\"unpause\")",
      "1981:         request_conf = request.values.get(\"conf\")",
      "",
      "[Removed Lines]",
      "1978:         run_id = request.values.get(\"run_id\", \"\")",
      "",
      "[Added Lines]",
      "1978:         run_id = request.values.get(\"run_id\", \"\").replace(\" \", \"+\")",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "2096:             flash(message, \"error\")",
      "2097:             return redirect(origin)",
      "2107:         run_conf = {}",
      "2108:         if request_conf:",
      "",
      "[Removed Lines]",
      "2099:         # Flash a warning when slash is used, but still allow it to continue on.",
      "2100:         if run_id and \"/\" in run_id:",
      "2101:             flash(",
      "2102:                 \"Using forward slash ('/') in a DAG run ID is deprecated. Note that this character \"",
      "2103:                 \"also makes the run impossible to retrieve via Airflow's REST API.\",",
      "2104:                 \"warning\",",
      "2105:             )",
      "",
      "[Added Lines]",
      "2099:         regex = conf.get(\"scheduler\", \"allowed_run_id_pattern\")",
      "2100:         if run_id and not re.match(RUN_ID_REGEX, run_id):",
      "2101:             if not regex.strip() or not re.match(regex.strip(), run_id):",
      "2102:                 flash(",
      "2103:                     f\"The provided run ID '{run_id}' is invalid. It does not match either \"",
      "2104:                     f\"the configured pattern: '{regex}' or the built-in pattern: '{RUN_ID_REGEX}'\",",
      "2105:                     \"error\",",
      "2106:                 )",
      "2108:                 form = DateTimeForm(data={\"execution_date\": execution_date})",
      "2109:                 return self.render_template(",
      "2110:                     \"airflow/trigger.html\",",
      "2111:                     form_fields=form_fields,",
      "2112:                     dag=dag,",
      "2113:                     dag_id=dag_id,",
      "2114:                     origin=origin,",
      "2115:                     conf=request_conf,",
      "2116:                     form=form,",
      "2117:                     is_dag_run_conf_overrides_params=is_dag_run_conf_overrides_params,",
      "2118:                     recent_confs=recent_confs,",
      "2119:                 )",
      "",
      "---------------"
    ],
    "tests/models/test_dagrun.py||tests/models/test_dagrun.py": [
      "File: tests/models/test_dagrun.py -> tests/models/test_dagrun.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from airflow import settings",
      "31: from airflow.callbacks.callback_requests import DagCallbackRequest",
      "32: from airflow.decorators import setup, task, task_group, teardown",
      "33: from airflow.models import (",
      "34:     DAG,",
      "35:     DagBag,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from airflow.exceptions import AirflowException",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "54: from airflow.utils.types import DagRunType",
      "55: from tests.models import DEFAULT_DATE as _DEFAULT_DATE",
      "56: from tests.test_utils import db",
      "57: from tests.test_utils.mock_operators import MockOperator",
      "59: DEFAULT_DATE = pendulum.instance(_DEFAULT_DATE)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "58: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "2541:     tis = dr.task_instance_scheduling_decisions(session).tis",
      "2542:     tis_for_state = {x.task_id for x in dr._tis_for_dagrun_state(dag=dag, tis=tis)}",
      "2543:     assert tis_for_state == expected",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "2548: @pytest.mark.parametrize(",
      "2549:     \"pattern, run_id, result\",",
      "2550:     [",
      "2551:         [\"^[A-Z]\", \"ABC\", True],",
      "2552:         [\"^[A-Z]\", \"abc\", False],",
      "2553:         [\"^[0-9]\", \"123\", True],",
      "2554:         # The below params tests that user configuration does not affect internally generated",
      "2555:         # run_ids",
      "2556:         [\"\", \"scheduled__2023-01-01T00:00:00+00:00\", True],",
      "2557:         [\"\", \"manual__2023-01-01T00:00:00+00:00\", True],",
      "2558:         [\"\", \"dataset_triggered__2023-01-01T00:00:00+00:00\", True],",
      "2559:         [\"\", \"scheduled_2023-01-01T00\", False],",
      "2560:         [\"\", \"manual_2023-01-01T00\", False],",
      "2561:         [\"\", \"dataset_triggered_2023-01-01T00\", False],",
      "2562:         [\"^[0-9]\", \"scheduled__2023-01-01T00:00:00+00:00\", True],",
      "2563:         [\"^[0-9]\", \"manual__2023-01-01T00:00:00+00:00\", True],",
      "2564:         [\"^[a-z]\", \"dataset_triggered__2023-01-01T00:00:00+00:00\", True],",
      "2565:     ],",
      "2566: )",
      "2567: def test_dag_run_id_config(session, dag_maker, pattern, run_id, result):",
      "2568:     with conf_vars({(\"scheduler\", \"allowed_run_id_pattern\"): pattern}):",
      "2569:         with dag_maker():",
      "2570:             ...",
      "2571:         if result:",
      "2572:             dag_maker.create_dagrun(run_id=run_id)",
      "2573:         else:",
      "2574:             with pytest.raises(AirflowException):",
      "2575:                 dag_maker.create_dagrun(run_id=run_id)",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
      "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from airflow.utils.session import create_session",
      "31: from airflow.utils.types import DagRunType",
      "32: from tests.test_utils.api_connexion_utils import create_test_client",
      "33: from tests.test_utils.www import check_content_in_response",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "287:         f'<textarea style=\"display: none;\" id=\"json_start\" name=\"json_start\">{expected_dag_conf}</textarea>',",
      "288:         resp,",
      "289:     )",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "293: @pytest.mark.parametrize(",
      "294:     \"pattern, run_id, result\",",
      "295:     [",
      "296:         [\"^[A-Z]\", \"ABC\", True],",
      "297:         [\"^[A-Z]\", \"abc\", False],",
      "298:         [\"^[0-9]\", \"123\", True],",
      "299:         # The below params tests that user configuration does not affect internally generated",
      "300:         # run_ids. We use manual__ as a prefix for manually triggered DAGs due to a restriction",
      "301:         # in manually triggered DAGs that the run_id must not start with scheduled__.",
      "302:         [\"\", \"manual__2023-01-01T00:00:00+00:00\", True],",
      "303:         [\"\", \"scheduled_2023-01-01T00\", False],",
      "304:         [\"\", \"manual_2023-01-01T00\", False],",
      "305:         [\"\", \"dataset_triggered_2023-01-01T00\", False],",
      "306:         [\"^[0-9]\", \"manual__2023-01-01T00:00:00+00:00\", True],",
      "307:         [\"^[a-z]\", \"manual__2023-01-01T00:00:00+00:00\", True],",
      "308:     ],",
      "309: )",
      "310: def test_dag_run_id_pattern(session, admin_client, pattern, run_id, result):",
      "311:     with conf_vars({(\"scheduler\", \"allowed_run_id_pattern\"): pattern}):",
      "312:         test_dag_id = \"example_bash_operator\"",
      "313:         admin_client.post(f\"dags/{test_dag_id}/trigger?&run_id={run_id}\")",
      "314:         run = session.query(DagRun).filter(DagRun.dag_id == test_dag_id).first()",
      "315:         if result:",
      "316:             assert run is not None",
      "317:             assert run.run_type == DagRunType.MANUAL",
      "318:         else:",
      "319:             assert run is None",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "71c64de96248694017897fdb3d9d241e7c980825",
      "candidate_info": {
        "commit_hash": "71c64de96248694017897fdb3d9d241e7c980825",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/71c64de96248694017897fdb3d9d241e7c980825",
        "files": [
          "airflow/cli/cli_parser.py",
          "airflow/cli/commands/dag_command.py",
          "airflow/example_dags/example_bash_operator.py",
          "airflow/models/dag.py",
          "docs/apache-airflow/executor/debug.rst",
          "newsfragments/26400.significant.rst",
          "tests/cli/commands/test_dag_command.py",
          "tests/models/test_dag.py"
        ],
        "message": "Create a more efficient  airflow dag test command that also has better local logging (#26400)\n\nThe current `airflow dag test` command is insufficient for an iterative local workflow when developing dags. The current system relies on running a backfill job which a) is extremely slow and b) only shows scheduler-level logs, so the user would need to search for log files to see how their tasks actually ran!\n\nThis new system \n\n\n## Speed\n\nWith this new system, we see a notable speedup in between-task speed. The new airflow dag test command can load tasks immediately and not wait for a scheduler heartbeat or other irrelevant airflow processes.\n\n### Before:\n\nexample bash operator: 13 seconds\nexample python operator: 26 seconds\n\n### After\n\nexample bash operator: 6 seconds\nexample python operator: 16 seconds\n\n\n## Logging\n\nPreviously, the `airflow dag test` command would pass on scheduler-level logs. This gives essentially 0 useful information for a DAG writer, and would force them to seek out task.log files in their filesystem every time they run a test dag. To improve this, we now surface task-level logs to the command line. We also no longer have any scheduler-level logs as we are no longer using a scheduler!\n\n### Before\n\n```\n[2022-09-14 14:37:48,310] {dagbag.py:522} INFO - Filling up the DagBag from /files/dags\n[2022-09-14 14:37:48,967] {base_executor.py:93} INFO - Adding to queue: ['<TaskInstance: example_bash_operator.runme_0 backfill__2022-01-02T00:00:00+00:00 [queued]>']\n[2022-09-14 14:37:48,973] {base_executor.py:93} INFO - Adding to queue: ['<TaskInstance: example_bash_operator.runme_1 backfill__2022-01-02T00:00:00+00:00 [queued]>']\n[2022-09-14 14:37:48,979] {base_executor.py:93} INFO - Adding to queue: ['<TaskInstance: example_bash_operator.runme_2 backfill__2022-01-02T00:00:00+00:00 [queued]>']\n[2022-09-14 14:37:48,985] {base_executor.py:93} INFO - Adding to queue: ['<TaskInstance: example_bash_operator.also_run_this backfill__2022-01-02T00:00:00+00:00 [queued]>']\n[2022-09-14 14:37:48,991] {base_executor.py:93} INFO - Adding to queue: ['<TaskInstance: example_bash_operator.this_will_skip backfill__2022-01-02T00:00:00+00:00 [queued]>']\n[2022-09-14 14:37:54,045] {subprocess.py:62} INFO - Tmp dir root location:\n /tmp\n[2022-09-14 14:37:54,045] {subprocess.py:74} INFO - Running command: ['/bin/bash', '-c', 'echo \"example_bash_operator__runme_0__20220102\" && sleep 1']\n[2022-09-14 14:37:54,053] {subprocess.py:85} INFO - Output:\n[2022-09-14 14:37:54,055] {subprocess.py:92} INFO - example_bash_operator__runme_0__20220102\n[2022-09-14 14:37:55,061] {subprocess.py:96} INFO - Command exited with return code 0\n[2022-09-14 14:37:55,117] {subprocess.py:62} INFO - Tmp dir root location:\n /tmp\n[2022-09-14 14:37:55,117] {subprocess.py:74} INFO - Running command: ['/bin/bash', '-c', 'echo \"example_bash_operator__runme_1__20220102\" && sleep 1']\n[2022-09-14 14:37:55,124] {subprocess.py:85} INFO - Output:\n[2022-09-14 14:37:55,125] {subprocess.py:92} INFO - example_bash_operator__runme_1__20220102\n[2022-09-14 14:37:56,131] {subprocess.py:96} INFO - Command exited with return code 0\n[2022-09-14 14:37:56,186] {subprocess.py:62} INFO - Tmp dir root location:\n /tmp\n[2022-09-14 14:37:56,186] {subprocess.py:74} INFO - Running command: ['/bin/bash', '-c', 'echo \"example_bash_operator__runme_2__20220102\" && sleep 1']\n[2022-09-14 14:37:56,193] {subprocess.py:85} INFO - Output:\n[2022-09-14 14:37:56,194] {subprocess.py:92} INFO - example_bash_operator__runme_2__20220102\n[2022-09-14 14:37:57,199] {subprocess.py:96} INFO - Command exited with return code 0\n[2022-09-14 14:37:57,257] {subprocess.py:62} INFO - Tmp dir root location:\n /tmp\n[2022-09-14 14:37:57,257] {subprocess.py:74} INFO - Running command: ['/bin/bash', '-c', 'echo \"ti_key=example_bash_operator__also_run_this__20220102\"']\n[2022-09-14 14:37:57,265] {subprocess.py:85} INFO - Output:\n[2022-09-14 14:37:57,266] {subprocess.py:92} INFO - ti_key=example_bash_operator__also_run_this__20220102\n[2022-09-14 14:37:57,266] {subprocess.py:96} INFO - Command exited with return code 0\n[2022-09-14 14:37:57,307] {subprocess.py:62} INFO - Tmp dir root location:\n /tmp\n[2022-09-14 14:37:57,308] {subprocess.py:74} INFO - Running command: ['/bin/bash', '-c', 'echo \"hello world\"; exit 99;']\n[2022-09-14 14:37:57,316] {subprocess.py:85} INFO - Output:\n[2022-09-14 14:37:57,317] {subprocess.py:92} INFO - hello world\n[2022-09-14 14:37:57,317] {subprocess.py:96} INFO - Command exited with return code 99\n[2022-09-14 14:37:57,340] {backfill_job.py:381} INFO - [backfill progress] | finished run 0 of 1 | tasks waiting: 2 | succeeded: 4 | running: 0 | failed: 0 | skipped: 1 | deadlocked: 0 | not ready: 2\n[2022-09-14 14:37:57,351] {base_executor.py:93} INFO - Adding to queue: ['<TaskInstance: example_bash_operator.run_after_loop backfill__2022-01-02T00:00:00+00:00 [queued]>']\n[2022-09-14 14:37:58,918] {subprocess.py:62} INFO - Tmp dir root location:\n /tmp\n[2022-09-14 14:37:58,918] {subprocess.py:74} INFO - Running command: ['/bin/bash', '-c', 'echo 1']\n[2022-09-14 14:37:58,925] {subprocess.py:85} INFO - Output:\n[2022-09-14 14:37:58,926] {subprocess.py:92} INFO - 1\n[2022-09-14 14:37:58,927] {subprocess.py:96} INFO - Command exited with return code 0\n[2022-09-14 14:37:58,945] {dagrun.py:595} INFO - Marking run <DagRun example_bash_operator @ 2022-01-02T00:00:00+00:00: backfill__2022-01-02T00:00:00+00:00, state:running, queued_at: None. externally triggered: False> successful\n[2022-09-14 14:37:58,946] {dagrun.py:657} INFO - DagRun Finished: dag_id=example_bash_operator, execution_date=2022-01-02T00:00:00+00:00, run_id=backfill__2022-01-02T00:00:00+00:00, run_start_date=2022-09-14 14:37:48.865989+00:00, run_end_date=2022-09-14 14:37:58.946180+00:00, run_duration=10.080191, state=success, external_trigger=False, run_type=backfill, data_interval_start=2022-01-02T00:00:00+00:00, data_interval_end=2022-01-03T00:00:00+00:00, dag_hash=None\n[2022-09-14 14:37:58,946] {backfill_job.py:381} INFO - [backfill progress] | finished run 1 of 1 | tasks waiting: 0 | succeeded: 5 | running: 0 | failed: 0 | skipped: 2 | deadlocked: 0 | not ready: 0\n[2022-09-14 14:37:58,949] {backfill_job.py:875} INFO - Backfill done for DAG <DAG: example_bash_operator>. Exiting.\n```\n\n\n### After:\n\n```\nroot@613d334d9c9d:/opt/airflow# airflow dags test example_bash_operator  2022-01-03\n[2022-09-14 14:43:45,232] {dagbag.py:522} INFO - Filling up the DagBag from /files/dags\n[2022-09-14 14:43:45,997] {dag_command.py:556} INFO - dagrun id:example_bash_operator\n/opt/airflow/airflow/cli/commands/dag_command.py:573 RemovedInAirflow3Warning: Calling `DAG.create_dagrun()` without an explicit data interval is deprecated\ncreated dagrun <DagRun example_bash_operator @ 2022-01-03T00:00:00+00:00: manual__2022-01-03T00:00:00+00:00, state:running, queued_at: None. externally triggered: False>\n[2022-09-14 14:43:46,013] {dag_command.py:496} INFO - starting dagrun\n[2022-09-14 14:43:46,138] {dag_command.py:542} INFO - *****************************************************\n[2022-09-14 14:43:46,138] {dag_command.py:543} INFO - Running task runme_0\n[2022-09-14 14:43:46,144] {subprocess.py:62} INFO - Tmp dir root location:\n /tmp\n[2022-09-14 14:43:46,145] {subprocess.py:74} INFO - Running command: ['/bin/bash', '-c', 'echo \"example_bash_operator__runme_0__20220103\" && sleep 1']\n[2022-09-14 14:43:46,152] {subprocess.py:85} INFO - Output:\n[2022-09-14 14:43:46,154] {subprocess.py:92} INFO - example_bash_operator__runme_0__20220103\n[2022-09-14 14:43:47,158] {subprocess.py:96} INFO - Command exited with return code 0\n[2022-09-14 14:43:47,179] {dag_command.py:547} INFO - runme_0 ran successfully!\n[2022-09-14 14:43:47,180] {dag_command.py:550} INFO - *****************************************************\n[2022-09-14 14:43:47,197] {dag_command.py:542} INFO - *****************************************************\n[2022-09-14 14:43:47,197] {dag_command.py:543} INFO - Running task runme_1\n[2022-09-14 14:43:47,203] {subprocess.py:62} INFO - Tmp dir root location:\n /tmp\n[2022-09-14 14:43:47,204] {subprocess.py:74} INFO - Running command: ['/bin/bash', '-c', 'echo \"example_bash_operator__runme_1__20220103\" && sleep 1']\n[2022-09-14 14:43:47,211] {subprocess.py:85} INFO - Output:\n[2022-09-14 14:43:47,213] {subprocess.py:92} INFO - example_bash_operator__runme_1__20220103\n[2022-09-14 14:43:48,219] {subprocess.py:96} INFO - Command exited with return code 0\n[2022-09-14 14:43:48,239] {dag_command.py:547} INFO - runme_1 ran successfully!\n[2022-09-14 14:43:48,239] {dag_command.py:550} INFO - *****************************************************\n[2022-09-14 14:43:48,253] {dag_command.py:542} INFO - *****************************************************\n[2022-09-14 14:43:48,253] {dag_command.py:543} INFO - Running task runme_2\n[2022-09-14 14:43:48,259] {subprocess.py:62} INFO - Tmp dir root location:\n /tmp\n[2022-09-14 14:43:48,259] {subprocess.py:74} INFO - Running command: ['/bin/bash', '-c', 'echo \"example_bash_operator__runme_2__20220103\" && sleep 1']\n[2022-09-14 14:43:48,266] {subprocess.py:85} INFO - Output:\n[2022-09-14 14:43:48,267] {subprocess.py:92} INFO - example_bash_operator__runme_2__20220103\n[2022-09-14 14:43:49,273] {subprocess.py:96} INFO - Command exited with return code 0\n[2022-09-14 14:43:49,294] {dag_command.py:547} INFO - runme_2 ran successfully!\n[2022-09-14 14:43:49,295] {dag_command.py:550} INFO - *****************************************************\n[2022-09-14 14:43:49,374] {dag_command.py:542} INFO - *****************************************************\n[2022-09-14 14:43:49,374] {dag_command.py:543} INFO - Running task also_run_this\n[2022-09-14 14:43:49,380] {subprocess.py:62} INFO - Tmp dir root location:\n /tmp\n[2022-09-14 14:43:49,380] {subprocess.py:74} INFO - Running command: ['/bin/bash', '-c', 'echo \"ti_key=example_bash_operator__also_run_this__20220103\"']\n[2022-09-14 14:43:49,389] {subprocess.py:85} INFO - Output:\n[2022-09-14 14:43:49,390] {subprocess.py:92} INFO - ti_key=example_bash_operator__also_run_this__20220103\n[2022-09-14 14:43:49,390] {subprocess.py:96} INFO - Command exited with return code 0\n[2022-09-14 14:43:49,395] {dag_command.py:547} INFO - also_run_this ran successfully!\n[2022-09-14 14:43:49,395] {dag_command.py:550} INFO - *****************************************************\n[2022-09-14 14:43:49,406] {dag_command.py:542} INFO - *****************************************************\n[2022-09-14 14:43:49,407] {dag_command.py:543} INFO - Running task this_will_skip\n[2022-09-14 14:43:49,412] {subprocess.py:62} INFO - Tmp dir root location:\n /tmp\n[2022-09-14 14:43:49,413] {subprocess.py:74} INFO - Running command: ['/bin/bash', '-c', 'echo \"hello world\"; exit 99;']\n[2022-09-14 14:43:49,420] {subprocess.py:85} INFO - Output:\n[2022-09-14 14:43:49,421] {subprocess.py:92} INFO - hello world\n[2022-09-14 14:43:49,422] {subprocess.py:96} INFO - Command exited with return code 99\n[2022-09-14 14:43:49,422] {dag_command.py:549} INFO - Task Skipped, continuing\n[2022-09-14 14:43:49,422] {dag_command.py:550} INFO - *****************************************************\n[2022-09-14 14:43:49,440] {dag_command.py:542} INFO - *****************************************************\n[2022-09-14 14:43:49,440] {dag_command.py:543} INFO - Running task run_after_loop\n[2022-09-14 14:43:49,445] {subprocess.py:62} INFO - Tmp dir root location:\n /tmp\n[2022-09-14 14:43:49,446] {subprocess.py:74} INFO - Running command: ['/bin/bash', '-c', 'echo 1']\n[2022-09-14 14:43:49,453] {subprocess.py:85} INFO - Output:\n[2022-09-14 14:43:49,454] {subprocess.py:92} INFO - 1\n[2022-09-14 14:43:49,454] {subprocess.py:96} INFO - Command exited with return code 0\n[2022-09-14 14:43:49,458] {dag_command.py:547} INFO - run_after_loop ran successfully!\n[2022-09-14 14:43:49,458] {dag_command.py:550} INFO - *****************************************************\n[2022-09-14 14:43:49,471] {dag_command.py:542} INFO - *****************************************************\n[2022-09-14 14:43:49,471] {dag_command.py:543} INFO - Running task run_this_last\n[2022-09-14 14:43:49,475] {dag_command.py:547} INFO - run_this_last ran successfully!\n[2022-09-14 14:43:49,475] {dag_command.py:550} INFO - *****************************************************\n[2022-09-14 14:43:49,480] {dagrun.py:595} INFO - Marking run <DagRun example_bash_operator @ 2022-01-03T00:00:00+00:00: manual__2022-01-03T00:00:00+00:00, state:running, queued_at: None. externally triggered: False> successful\n[2022-09-14 14:43:49,480] {dagrun.py:657} INFO - DagRun Finished: dag_id=example_bash_operator, execution_date=2022-01-03T00:00:00+00:00, run_id=manual__2022-01-03T00:00:00+00:00, run_start_date=2022-01-03T00:00:00+00:00, run_end_date=2022-09-14 14:43:49.480895+00:00, run_duration=21998629.480895, state=success, external_trigger=False, run_type=manual, data_interval_start=2022-01-03T00:00:00+00:00, data_interval_end=2022-01-04T00:00:00+00:00, dag_hash=None\n```\n\n---\n**^ Add meaningful description above**\n\nRead the **[Pull Request Guidelines](https://github.com/apache/airflow/blob/main/CONTRIBUTING.rst#pull-request-guidelines)** for more information.\nIn case of fundamental code changes, an Airflow Improvement Proposal ([AIP](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals)) is needed.\nIn case of a new dependency, check compliance with the [ASF 3rd Party License Policy](https://www.apache.org/legal/resolved.html#category-x).\nIn case of backwards incompatible changes please leave a note in a newsfragment file, named `{pr_number}.significant.rst` or `{issue_number}.significant.rst`, in [newsfragments](https://github.com/apache/airflow/tree/main/newsfragments).",
        "before_after_code_files": [
          "airflow/cli/cli_parser.py||airflow/cli/cli_parser.py",
          "airflow/cli/commands/dag_command.py||airflow/cli/commands/dag_command.py",
          "airflow/example_dags/example_bash_operator.py||airflow/example_dags/example_bash_operator.py",
          "airflow/models/dag.py||airflow/models/dag.py",
          "tests/cli/commands/test_dag_command.py||tests/cli/commands/test_dag_command.py",
          "tests/models/test_dag.py||tests/models/test_dag.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/models/dag.py||airflow/models/dag.py"
          ],
          "candidate": [
            "airflow/models/dag.py||airflow/models/dag.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/cli/cli_parser.py||airflow/cli/cli_parser.py": [
          "File: airflow/cli/cli_parser.py -> airflow/cli/cli_parser.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2035:         subcommands=[",
          "2036:             _remove_dag_id_opt(sp)",
          "2037:             for sp in DAGS_COMMANDS",
          "2039:         ],",
          "2040:     ),",
          "2041:     GroupCommand(",
          "",
          "[Removed Lines]",
          "2038:             if sp.name in ['backfill', 'list-runs', 'pause', 'unpause']",
          "",
          "[Added Lines]",
          "2038:             if sp.name in ['backfill', 'list-runs', 'pause', 'unpause', 'test']",
          "",
          "---------------"
        ],
        "airflow/cli/commands/dag_command.py||airflow/cli/commands/dag_command.py": [
          "File: airflow/cli/commands/dag_command.py -> airflow/cli/commands/dag_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "33: from airflow.api.client import get_current_api_client",
          "34: from airflow.cli.simple_table import AirflowConsole",
          "35: from airflow.configuration import conf",
          "38: from airflow.jobs.base_job import BaseJob",
          "39: from airflow.models import DagBag, DagModel, DagRun, TaskInstance",
          "40: from airflow.models.dag import DAG",
          "",
          "[Removed Lines]",
          "36: from airflow.exceptions import AirflowException, BackfillUnfinished, RemovedInAirflow3Warning",
          "37: from airflow.executors.debug_executor import DebugExecutor",
          "",
          "[Added Lines]",
          "36: from airflow.exceptions import AirflowException, RemovedInAirflow3Warning",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "450: @provide_session",
          "451: @cli_utils.action_cli",
          "453:     \"\"\"Execute one single DagRun for a given DAG and execution date, using the DebugExecutor.\"\"\"",
          "454:     run_conf = None",
          "455:     if args.conf:",
          "",
          "[Removed Lines]",
          "452: def dag_test(args, session=None):",
          "",
          "[Added Lines]",
          "451: def dag_test(args, dag=None, session=None):",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "458:         except ValueError as e:",
          "459:             raise SystemExit(f\"Configuration {args.conf!r} is not valid JSON. Error: {e}\")",
          "460:     execution_date = args.execution_date or timezone.utcnow()",
          "477:     show_dagrun = args.show_dagrun",
          "478:     imgcat = args.imgcat_dagrun",
          "479:     filename = args.save_dagrun",
          "",
          "[Removed Lines]",
          "461:     dag = get_dag(subdir=args.subdir, dag_id=args.dag_id)",
          "462:     dag.clear(start_date=execution_date, end_date=execution_date, dag_run_state=False)",
          "463:     try:",
          "464:         dag.run(",
          "465:             executor=DebugExecutor(),",
          "466:             start_date=execution_date,",
          "467:             end_date=execution_date,",
          "468:             conf=run_conf,",
          "469:             # Always run the DAG at least once even if no logical runs are",
          "470:             # available. This does not make a lot of sense, but Airflow has",
          "471:             # been doing this prior to 2.2 so we keep compatibility.",
          "472:             run_at_least_once=True,",
          "473:         )",
          "474:     except BackfillUnfinished as e:",
          "475:         print(str(e))",
          "",
          "[Added Lines]",
          "460:     dag = dag or get_dag(subdir=args.subdir, dag_id=args.dag_id)",
          "461:     dag.test(execution_date=execution_date, run_conf=run_conf, session=session)",
          "",
          "---------------"
        ],
        "airflow/example_dags/example_bash_operator.py||airflow/example_dags/example_bash_operator.py": [
          "File: airflow/example_dags/example_bash_operator.py -> airflow/example_dags/example_bash_operator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "73: this_will_skip >> run_this_last",
          "75: if __name__ == \"__main__\":",
          "",
          "[Removed Lines]",
          "76:     dag.cli()",
          "",
          "[Added Lines]",
          "76:     dag.test()",
          "",
          "---------------"
        ],
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "63: import airflow.templates",
          "64: from airflow import settings, utils",
          "65: from airflow.compat.functools import cached_property",
          "67: from airflow.exceptions import (",
          "68:     AirflowDagInconsistent,",
          "69:     AirflowException,",
          "70:     DuplicateTaskIdFound,",
          "71:     RemovedInAirflow3Warning,",
          "72:     TaskNotFound,",
          "",
          "[Removed Lines]",
          "66: from airflow.configuration import conf",
          "",
          "[Added Lines]",
          "66: from airflow.configuration import conf, secrets_backend_list",
          "70:     AirflowSkipException,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "80: from airflow.models.operator import Operator",
          "81: from airflow.models.param import DagParam, ParamsDict",
          "82: from airflow.models.taskinstance import Context, TaskInstance, TaskInstanceKey, clear_task_instances",
          "83: from airflow.security import permissions",
          "84: from airflow.stats import Stats",
          "85: from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction, Timetable",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "84: from airflow.secrets.local_filesystem import LocalFilesystemBackend",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2435:         args = parser.parse_args()",
          "2436:         args.func(args, self)",
          "2438:     @provide_session",
          "2439:     def create_dagrun(",
          "2440:         self,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2440:     @provide_session",
          "2441:     def test(",
          "2442:         self,",
          "2443:         execution_date: datetime | None = None,",
          "2444:         run_conf: dict[str, Any] | None = None,",
          "2445:         conn_file_path: str | None = None,",
          "2446:         variable_file_path: str | None = None,",
          "2447:         session: Session = NEW_SESSION,",
          "2448:     ) -> None:",
          "2449:         \"\"\"Execute one single DagRun for a given DAG and execution date.\"\"\"",
          "2451:         def add_logger_if_needed(ti: TaskInstance):",
          "2452:             \"\"\"",
          "2453:             Add a formatted logger to the taskinstance so all logs are surfaced to the command line instead",
          "2454:             of into a task file. Since this is a local test run, it is much better for the user to see logs",
          "2455:             in the command line, rather than needing to search for a log file.",
          "2456:             Args:",
          "2457:                 ti: The taskinstance that will receive a logger",
          "2459:             \"\"\"",
          "2460:             format = logging.Formatter(\"[%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s\")",
          "2461:             handler = logging.StreamHandler(sys.stdout)",
          "2462:             handler.level = logging.INFO",
          "2463:             handler.setFormatter(format)",
          "2464:             # only add log handler once",
          "2465:             if not any(isinstance(h, logging.StreamHandler) for h in ti.log.handlers):",
          "2466:                 self.log.debug(\"Adding Streamhandler to taskinstance %s\", ti.task_id)",
          "2467:                 ti.log.addHandler(handler)",
          "2469:         if conn_file_path or variable_file_path:",
          "2470:             local_secrets = LocalFilesystemBackend(",
          "2471:                 variables_file_path=variable_file_path, connections_file_path=conn_file_path",
          "2472:             )",
          "2473:             secrets_backend_list.insert(0, local_secrets)",
          "2475:         execution_date = execution_date or timezone.utcnow()",
          "2476:         self.log.debug(\"Clearing existing task instances for execution date %s\", execution_date)",
          "2477:         self.clear(",
          "2478:             start_date=execution_date,",
          "2479:             end_date=execution_date,",
          "2480:             dag_run_state=False,  # type: ignore",
          "2481:             session=session,",
          "2482:         )",
          "2483:         self.log.debug(\"Getting dagrun for dag %s\", self.dag_id)",
          "2484:         dr: DagRun = _get_or_create_dagrun(",
          "2485:             dag=self,",
          "2486:             start_date=execution_date,",
          "2487:             execution_date=execution_date,",
          "2488:             run_id=DagRun.generate_run_id(DagRunType.MANUAL, execution_date),",
          "2489:             session=session,",
          "2490:             conf=run_conf,",
          "2491:         )",
          "2493:         tasks = self.task_dict",
          "2494:         self.log.debug(\"starting dagrun\")",
          "2495:         # Instead of starting a scheduler, we run the minimal loop possible to check",
          "2496:         # for task readiness and dependency management. This is notably faster",
          "2497:         # than creating a BackfillJob and allows us to surface logs to the user",
          "2498:         while dr.state == State.RUNNING:",
          "2499:             schedulable_tis, _ = dr.update_state(session=session)",
          "2500:             for ti in schedulable_tis:",
          "2501:                 add_logger_if_needed(ti)",
          "2502:                 ti.task = tasks[ti.task_id]",
          "2503:                 _run_task(ti, session=session)",
          "2504:         if conn_file_path or variable_file_path:",
          "2505:             # Remove the local variables we have added to the secrets_backend_list",
          "2506:             secrets_backend_list.pop(0)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "3505:             return cls._context_managed_dags[0]",
          "3506:         except IndexError:",
          "3507:             return None",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3580: def _run_task(ti: TaskInstance, session):",
          "3581:     \"\"\"",
          "3582:     Run a single task instance, and push result to Xcom for downstream tasks. Bypasses a lot of",
          "3583:     extra steps used in `task.run` to keep our local running as fast as possible",
          "3584:     This function is only meant for the `dag.test` function as a helper function.",
          "3586:     Args:",
          "3587:         ti: TaskInstance to run",
          "3588:     \"\"\"",
          "3589:     log.info(\"*****************************************************\")",
          "3590:     if ti.map_index > 0:",
          "3591:         log.info(\"Running task %s index %d\", ti.task_id, ti.map_index)",
          "3592:     else:",
          "3593:         log.info(\"Running task %s\", ti.task_id)",
          "3594:     try:",
          "3595:         ti._run_raw_task(session=session)",
          "3596:         session.flush()",
          "3597:         log.info(\"%s ran successfully!\", ti.task_id)",
          "3598:     except AirflowSkipException:",
          "3599:         log.info(\"Task Skipped, continuing\")",
          "3600:     log.info(\"*****************************************************\")",
          "3603: def _get_or_create_dagrun(",
          "3604:     dag: DAG,",
          "3605:     conf: dict[Any, Any] | None,",
          "3606:     start_date: datetime,",
          "3607:     execution_date: datetime,",
          "3608:     run_id: str,",
          "3609:     session: Session,",
          "3610: ) -> DagRun:",
          "3611:     \"\"\"",
          "3612:     Create a DAGRun, but only after clearing the previous instance of said dagrun to prevent collisions.",
          "3613:     This function is only meant for the `dag.test` function as a helper function.",
          "3614:     :param dag: Dag to be used to find dagrun",
          "3615:     :param conf: configuration to pass to newly created dagrun",
          "3616:     :param start_date: start date of new dagrun, defaults to execution_date",
          "3617:     :param execution_date: execution_date for finding the dagrun",
          "3618:     :param run_id: run_id to pass to new dagrun",
          "3619:     :param session: sqlalchemy session",
          "3620:     :return:",
          "3621:     \"\"\"",
          "3622:     log.info(\"dagrun id: %s\", dag.dag_id)",
          "3623:     dr: DagRun = (",
          "3624:         session.query(DagRun)",
          "3625:         .filter(DagRun.dag_id == dag.dag_id, DagRun.execution_date == execution_date)",
          "3626:         .first()",
          "3627:     )",
          "3628:     if dr:",
          "3629:         session.delete(dr)",
          "3630:         session.commit()",
          "3631:     dr = dag.create_dagrun(",
          "3632:         state=DagRunState.RUNNING,",
          "3633:         execution_date=execution_date,",
          "3634:         run_id=run_id,",
          "3635:         start_date=start_date or execution_date,",
          "3636:         session=session,",
          "3637:         conf=conf,  # type: ignore",
          "3638:     )",
          "3639:     log.info(\"created dagrun \" + str(dr))",
          "3640:     return dr",
          "",
          "---------------"
        ],
        "tests/cli/commands/test_dag_command.py||tests/cli/commands/test_dag_command.py": [
          "File: tests/cli/commands/test_dag_command.py -> tests/cli/commands/test_dag_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import contextlib",
          "21: import io",
          "23: import os",
          "24: import tempfile",
          "25: import unittest",
          "",
          "[Removed Lines]",
          "22: import json",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "618:             is None",
          "619:         )",
          "622:     @mock.patch(\"airflow.cli.commands.dag_command.get_dag\")",
          "624:         cli_args = self.parser.parse_args(['dags', 'test', 'example_bash_operator', DEFAULT_DATE.isoformat()])",
          "625:         dag_command.dag_test(cli_args)",
          "627:         mock_get_dag.assert_has_calls(",
          "628:             [",
          "629:                 mock.call(subdir=cli_args.subdir, dag_id='example_bash_operator'),",
          "641:                 ),",
          "642:             ]",
          "643:         )",
          "646:     @mock.patch(\"airflow.cli.commands.dag_command.get_dag\")",
          "647:     @mock.patch(\"airflow.utils.timezone.utcnow\")",
          "649:         now = pendulum.now()",
          "650:         mock_utcnow.return_value = now",
          "651:         cli_args = self.parser.parse_args(['dags', 'test', 'example_bash_operator'])",
          "",
          "[Removed Lines]",
          "621:     @mock.patch(\"airflow.cli.commands.dag_command.DebugExecutor\")",
          "623:     def test_dag_test(self, mock_get_dag, mock_executor):",
          "630:                 mock.call().clear(",
          "631:                     start_date=cli_args.execution_date,",
          "632:                     end_date=cli_args.execution_date,",
          "633:                     dag_run_state=False,",
          "634:                 ),",
          "635:                 mock.call().run(",
          "636:                     executor=mock_executor.return_value,",
          "637:                     start_date=cli_args.execution_date,",
          "638:                     end_date=cli_args.execution_date,",
          "639:                     conf=None,",
          "640:                     run_at_least_once=True,",
          "645:     @mock.patch(\"airflow.cli.commands.dag_command.DebugExecutor\")",
          "648:     def test_dag_test_no_execution_date(self, mock_utcnow, mock_get_dag, mock_executor):",
          "",
          "[Added Lines]",
          "621:     def test_dag_test(self, mock_get_dag):",
          "628:                 mock.call().test(",
          "629:                     execution_date=timezone.parse(DEFAULT_DATE.isoformat()), run_conf=None, session=mock.ANY",
          "636:     def test_dag_test_no_execution_date(self, mock_utcnow, mock_get_dag):",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "657:         mock_get_dag.assert_has_calls(",
          "658:             [",
          "659:                 mock.call(subdir=cli_args.subdir, dag_id='example_bash_operator'),",
          "672:             ]",
          "673:         )",
          "676:     @mock.patch(\"airflow.cli.commands.dag_command.get_dag\")",
          "678:         cli_args = self.parser.parse_args(",
          "679:             [",
          "680:                 'dags',",
          "",
          "[Removed Lines]",
          "660:                 mock.call().clear(",
          "661:                     start_date=now,",
          "662:                     end_date=now,",
          "663:                     dag_run_state=False,",
          "664:                 ),",
          "665:                 mock.call().run(",
          "666:                     executor=mock_executor.return_value,",
          "667:                     start_date=now,",
          "668:                     end_date=now,",
          "669:                     conf=None,",
          "670:                     run_at_least_once=True,",
          "671:                 ),",
          "675:     @mock.patch(\"airflow.cli.commands.dag_command.DebugExecutor\")",
          "677:     def test_dag_test_conf(self, mock_get_dag, mock_executor):",
          "",
          "[Added Lines]",
          "648:                 mock.call().test(execution_date=mock.ANY, run_conf=None, session=mock.ANY),",
          "653:     def test_dag_test_conf(self, mock_get_dag):",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "690:         mock_get_dag.assert_has_calls(",
          "691:             [",
          "692:                 mock.call(subdir=cli_args.subdir, dag_id='example_bash_operator'),",
          "704:                 ),",
          "705:             ]",
          "706:         )",
          "708:     @mock.patch(\"airflow.cli.commands.dag_command.render_dag\", return_value=MagicMock(source=\"SOURCE\"))",
          "710:     @mock.patch(\"airflow.cli.commands.dag_command.get_dag\")",
          "712:         cli_args = self.parser.parse_args(",
          "713:             ['dags', 'test', 'example_bash_operator', DEFAULT_DATE.isoformat(), '--show-dagrun']",
          "714:         )",
          "",
          "[Removed Lines]",
          "693:                 mock.call().clear(",
          "694:                     start_date=cli_args.execution_date,",
          "695:                     end_date=cli_args.execution_date,",
          "696:                     dag_run_state=False,",
          "697:                 ),",
          "698:                 mock.call().run(",
          "699:                     executor=mock_executor.return_value,",
          "700:                     start_date=cli_args.execution_date,",
          "701:                     end_date=cli_args.execution_date,",
          "702:                     conf=json.loads(cli_args.conf),",
          "703:                     run_at_least_once=True,",
          "709:     @mock.patch(\"airflow.cli.commands.dag_command.DebugExecutor\")",
          "711:     def test_dag_test_show_dag(self, mock_get_dag, mock_executor, mock_render_dag):",
          "",
          "[Added Lines]",
          "669:                 mock.call().test(",
          "670:                     execution_date=timezone.parse(DEFAULT_DATE.isoformat()),",
          "671:                     run_conf={\"dag_run_conf_param\": \"param_value\"},",
          "672:                     session=mock.ANY,",
          "679:     def test_dag_test_show_dag(self, mock_get_dag, mock_render_dag):",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "720:         mock_get_dag.assert_has_calls(",
          "721:             [",
          "722:                 mock.call(subdir=cli_args.subdir, dag_id='example_bash_operator'),",
          "734:                 ),",
          "735:             ]",
          "736:         )",
          "",
          "[Removed Lines]",
          "723:                 mock.call().clear(",
          "724:                     start_date=cli_args.execution_date,",
          "725:                     end_date=cli_args.execution_date,",
          "726:                     dag_run_state=False,",
          "727:                 ),",
          "728:                 mock.call().run(",
          "729:                     executor=mock_executor.return_value,",
          "730:                     start_date=cli_args.execution_date,",
          "731:                     end_date=cli_args.execution_date,",
          "732:                     conf=None,",
          "733:                     run_at_least_once=True,",
          "",
          "[Added Lines]",
          "691:                 mock.call().test(",
          "692:                     execution_date=timezone.parse(DEFAULT_DATE.isoformat()), run_conf=None, session=mock.ANY",
          "",
          "---------------"
        ],
        "tests/models/test_dag.py||tests/models/test_dag.py": [
          "File: tests/models/test_dag.py -> tests/models/test_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1604:         dagrun = dagruns[0]  # type: DagRun",
          "1605:         assert dagrun.state == dag_run_state",
          "1607:     def _make_test_subdag(self, session):",
          "1608:         dag_id = 'test_subdag'",
          "1609:         self._clean_up(dag_id)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1607:     def test_dag_test_basic(self):",
          "1608:         dag = DAG(dag_id=\"test_local_testing_conn_file\", start_date=DEFAULT_DATE)",
          "1609:         mock_object = mock.MagicMock()",
          "1611:         @task_decorator",
          "1612:         def check_task():",
          "1613:             # we call a mock object to ensure that this task actually ran.",
          "1614:             mock_object()",
          "1616:         with dag:",
          "1617:             check_task()",
          "1619:         dag.test()",
          "1620:         mock_object.assert_called_once()",
          "1622:     def test_dag_test_with_dependencies(self):",
          "1623:         dag = DAG(dag_id=\"test_local_testing_conn_file\", start_date=DEFAULT_DATE)",
          "1624:         mock_object = mock.MagicMock()",
          "1626:         @task_decorator",
          "1627:         def check_task():",
          "1628:             return \"output of first task\"",
          "1630:         @task_decorator",
          "1631:         def check_task_2(my_input):",
          "1632:             # we call a mock object to ensure that this task actually ran.",
          "1633:             mock_object(my_input)",
          "1635:         with dag:",
          "1636:             check_task_2(check_task())",
          "1638:         dag.test()",
          "1639:         mock_object.assert_called_with(\"output of first task\")",
          "1641:     def test_dag_test_with_task_mapping(self):",
          "1642:         dag = DAG(dag_id=\"test_local_testing_conn_file\", start_date=DEFAULT_DATE)",
          "1643:         mock_object = mock.MagicMock()",
          "1645:         @task_decorator()",
          "1646:         def get_index(current_val, ti=None):",
          "1647:             return ti.map_index",
          "1649:         @task_decorator",
          "1650:         def check_task(my_input):",
          "1651:             # we call a mock object with the combined map to ensure all expected indexes are called",
          "1652:             mock_object(list(my_input))",
          "1654:         with dag:",
          "1655:             mapped_task = get_index.expand(current_val=[1, 1, 1, 1, 1])",
          "1656:             check_task(mapped_task)",
          "1658:         dag.test()",
          "1659:         mock_object.assert_called_with([0, 1, 2, 3, 4])",
          "1661:     def test_dag_connection_file(self):",
          "1662:         test_connections_string = \"\"\"",
          "1663: ---",
          "1664: my_postgres_conn:",
          "1665:   - conn_id: my_postgres_conn",
          "1666:     conn_type: postgres",
          "1667:         \"\"\"",
          "1668:         dag = DAG(dag_id=\"test_local_testing_conn_file\", start_date=DEFAULT_DATE)",
          "1670:         @task_decorator",
          "1671:         def check_task():",
          "1672:             from airflow.configuration import secrets_backend_list",
          "1673:             from airflow.secrets.local_filesystem import LocalFilesystemBackend",
          "1675:             assert isinstance(secrets_backend_list[0], LocalFilesystemBackend)",
          "1676:             local_secrets: LocalFilesystemBackend = secrets_backend_list[0]",
          "1677:             assert local_secrets.get_connection(\"my_postgres_conn\").conn_id == \"my_postgres_conn\"",
          "1679:         with dag:",
          "1680:             check_task()",
          "1681:         with NamedTemporaryFile(suffix=\".yaml\") as tmp:",
          "1682:             with open(tmp.name, 'w') as f:",
          "1683:                 f.write(test_connections_string)",
          "1684:             dag.test(conn_file_path=tmp.name)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d7bd6d780aa3db825e6baae7fc1a7d8ae0f8b14b",
      "candidate_info": {
        "commit_hash": "d7bd6d780aa3db825e6baae7fc1a7d8ae0f8b14b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/d7bd6d780aa3db825e6baae7fc1a7d8ae0f8b14b",
        "files": [
          "airflow/dag_processing/processor.py",
          "airflow/example_dags/tutorial.py",
          "airflow/models/baseoperator.py",
          "airflow/models/dag.py",
          "airflow/models/mappedoperator.py",
          "airflow/models/taskinstance.py",
          "docs/apache-airflow/administration-and-deployment/logging-monitoring/callbacks.rst",
          "tests/dag_processing/test_processor.py",
          "tests/models/test_taskinstance.py"
        ],
        "message": "Support using a list of callbacks in `on_*_callback/sla_miss_callback`s (#28469)\n\n* Support using a list of callbacks in `on_*_callback/sla_miss_callback`s\n\nPreviously, it was only possible to specify a single callback function when defining a DAG/task callbacks.\nThis change allows users to specify a list of callback functions, which will be invoked in the order they are provided.\n\nThis will not affect DAG/task that use a single callback function.\n\n* Apply suggestion from code review",
        "before_after_code_files": [
          "airflow/dag_processing/processor.py||airflow/dag_processing/processor.py",
          "airflow/example_dags/tutorial.py||airflow/example_dags/tutorial.py",
          "airflow/models/baseoperator.py||airflow/models/baseoperator.py",
          "airflow/models/dag.py||airflow/models/dag.py",
          "airflow/models/mappedoperator.py||airflow/models/mappedoperator.py",
          "airflow/models/taskinstance.py||airflow/models/taskinstance.py",
          "tests/dag_processing/test_processor.py||tests/dag_processing/test_processor.py",
          "tests/models/test_taskinstance.py||tests/models/test_taskinstance.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/models/dag.py||airflow/models/dag.py"
          ],
          "candidate": [
            "airflow/models/dag.py||airflow/models/dag.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/dag_processing/processor.py||airflow/dag_processing/processor.py": [
          "File: airflow/dag_processing/processor.py -> airflow/dag_processing/processor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "473:             notification_sent = False",
          "474:             if dag.sla_miss_callback:",
          "475:                 # Execute the alert callback",
          "483:             email_content = f\"\"\"\\",
          "484:             Here's a list of tasks that missed their SLAs:",
          "485:             <pre><code>{task_list}\\n<code></pre>",
          "",
          "[Removed Lines]",
          "476:                 self.log.info(\"Calling SLA miss callback\")",
          "477:                 try:",
          "478:                     dag.sla_miss_callback(dag, task_list, blocking_task_list, slas, blocking_tis)",
          "479:                     notification_sent = True",
          "480:                 except Exception:",
          "481:                     Stats.incr(\"sla_callback_notification_failure\")",
          "482:                     self.log.exception(\"Could not call sla_miss_callback for DAG %s\", dag.dag_id)",
          "",
          "[Added Lines]",
          "476:                 callbacks = (",
          "477:                     dag.sla_miss_callback",
          "478:                     if isinstance(dag.sla_miss_callback, list)",
          "479:                     else [dag.sla_miss_callback]",
          "480:                 )",
          "481:                 for callback in callbacks:",
          "482:                     self.log.info(\"Calling SLA miss callback %s\", callback)",
          "483:                     try:",
          "484:                         callback(dag, task_list, blocking_task_list, slas, blocking_tis)",
          "485:                         notification_sent = True",
          "486:                     except Exception:",
          "487:                         Stats.incr(\"sla_callback_notification_failure\")",
          "488:                         self.log.exception(",
          "489:                             \"Could not call sla_miss_callback(%s) for DAG %s\",",
          "490:                             callback.func_name,  # type: ignore[attr-defined]",
          "491:                             dag.dag_id,",
          "492:                         )",
          "",
          "---------------"
        ],
        "airflow/example_dags/tutorial.py||airflow/example_dags/tutorial.py": [
          "File: airflow/example_dags/tutorial.py -> airflow/example_dags/tutorial.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "56:         # 'wait_for_downstream': False,",
          "57:         # 'sla': timedelta(hours=2),",
          "58:         # 'execution_timeout': timedelta(seconds=300),",
          "63:         # 'trigger_rule': 'all_success'",
          "64:     },",
          "65:     # [END default_args]",
          "",
          "[Removed Lines]",
          "59:         # 'on_failure_callback': some_function,",
          "60:         # 'on_success_callback': some_other_function,",
          "61:         # 'on_retry_callback': another_function,",
          "62:         # 'sla_miss_callback': yet_another_function,",
          "",
          "[Added Lines]",
          "59:         # 'on_failure_callback': some_function, # or list of functions",
          "60:         # 'on_success_callback': some_other_function, # or list of functions",
          "61:         # 'on_retry_callback': another_function, # or list of functions",
          "62:         # 'sla_miss_callback': yet_another_function, # or list of functions",
          "",
          "---------------"
        ],
        "airflow/models/baseoperator.py||airflow/models/baseoperator.py": [
          "File: airflow/models/baseoperator.py -> airflow/models/baseoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "211:     weight_rule: str = DEFAULT_WEIGHT_RULE,",
          "212:     sla: timedelta | None = None,",
          "213:     max_active_tis_per_dag: int | None = None,",
          "218:     run_as_user: str | None = None,",
          "219:     executor_config: dict | None = None,",
          "220:     inlets: Any | None = None,",
          "",
          "[Removed Lines]",
          "214:     on_execute_callback: TaskStateChangeCallback | None = None,",
          "215:     on_failure_callback: TaskStateChangeCallback | None = None,",
          "216:     on_success_callback: TaskStateChangeCallback | None = None,",
          "217:     on_retry_callback: TaskStateChangeCallback | None = None,",
          "",
          "[Added Lines]",
          "214:     on_execute_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,",
          "215:     on_failure_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,",
          "216:     on_success_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,",
          "217:     on_retry_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "538:         notification are sent once and only once for each task instance.",
          "539:     :param execution_timeout: max time allowed for the execution of",
          "540:         this task instance, if it goes beyond it will raise and fail.",
          "542:         of this task fails. a context dictionary is passed as a single",
          "543:         parameter to this function. Context contains references to related",
          "544:         objects to the task instance and is documented under the macros",
          "",
          "[Removed Lines]",
          "541:     :param on_failure_callback: a function to be called when a task instance",
          "",
          "[Added Lines]",
          "541:     :param on_failure_callback: a function or list of functions to be called when a task instance",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "706:         pool_slots: int = DEFAULT_POOL_SLOTS,",
          "707:         sla: timedelta | None = None,",
          "708:         execution_timeout: timedelta | None = DEFAULT_TASK_EXECUTION_TIMEOUT,",
          "713:         pre_execute: TaskPreExecuteHook | None = None,",
          "714:         post_execute: TaskPostExecuteHook | None = None,",
          "715:         trigger_rule: str = DEFAULT_TRIGGER_RULE,",
          "",
          "[Removed Lines]",
          "709:         on_execute_callback: TaskStateChangeCallback | None = None,",
          "710:         on_failure_callback: TaskStateChangeCallback | None = None,",
          "711:         on_success_callback: TaskStateChangeCallback | None = None,",
          "712:         on_retry_callback: TaskStateChangeCallback | None = None,",
          "",
          "[Added Lines]",
          "709:         on_execute_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,",
          "710:         on_failure_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,",
          "711:         on_success_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,",
          "712:         on_retry_callback: None | TaskStateChangeCallback | list[TaskStateChangeCallback] = None,",
          "",
          "---------------"
        ],
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "309:     :param dagrun_timeout: specify how long a DagRun should be up before",
          "310:         timing out / failing, so that new DagRuns can be created. The timeout",
          "311:         is only enforced for scheduled DagRuns.",
          "313:         timeouts. See :ref:`sla_miss_callback<concepts:sla_miss_callback>` for",
          "314:         more information about the function signature and parameters that are",
          "315:         passed to the callback.",
          "",
          "[Removed Lines]",
          "312:     :param sla_miss_callback: specify a function to call when reporting SLA",
          "",
          "[Added Lines]",
          "312:     :param sla_miss_callback: specify a function or list of functions to call when reporting SLA",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "317:                                                    gantt, landing_times), default grid",
          "318:     :param orientation: Specify DAG orientation in graph view (LR, TB, RL, BT), default LR",
          "319:     :param catchup: Perform scheduler catchup (or only run latest)? Defaults to True",
          "321:         A context dictionary is passed as a single parameter to this function.",
          "322:     :param on_success_callback: Much like the ``on_failure_callback`` except",
          "323:         that it is executed when the dag succeeds.",
          "",
          "[Removed Lines]",
          "320:     :param on_failure_callback: A function to be called when a DagRun of this dag fails.",
          "",
          "[Added Lines]",
          "320:     :param on_failure_callback: A function or list of functions to be called when a DagRun of this dag fails.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "396:         max_active_tasks: int = conf.getint(\"core\", \"max_active_tasks_per_dag\"),",
          "397:         max_active_runs: int = conf.getint(\"core\", \"max_active_runs_per_dag\"),",
          "398:         dagrun_timeout: timedelta | None = None,",
          "400:         default_view: str = conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower(),",
          "401:         orientation: str = conf.get_mandatory_value(\"webserver\", \"dag_orientation\"),",
          "402:         catchup: bool = conf.getboolean(\"scheduler\", \"catchup_by_default\"),",
          "405:         doc_md: str | None = None,",
          "406:         params: dict | None = None,",
          "407:         access_control: dict | None = None,",
          "",
          "[Removed Lines]",
          "399:         sla_miss_callback: SLAMissCallback | None = None,",
          "403:         on_success_callback: DagStateChangeCallback | None = None,",
          "404:         on_failure_callback: DagStateChangeCallback | None = None,",
          "",
          "[Added Lines]",
          "399:         sla_miss_callback: None | SLAMissCallback | list[SLAMissCallback] = None,",
          "403:         on_success_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,",
          "404:         on_failure_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1313:         :param reason: Completion reason",
          "1314:         :param session: Database session",
          "1315:         \"\"\"",
          "1319:             tis = dagrun.get_task_instances(session=session)",
          "1320:             ti = tis[-1]  # get first TaskInstance of DagRun",
          "1321:             ti.task = self.get_task(ti.task_id)",
          "1322:             context = ti.get_template_context(session=session)",
          "1323:             context.update({\"reason\": reason})",
          "1330:     def get_active_runs(self):",
          "1331:         \"\"\"",
          "",
          "[Removed Lines]",
          "1316:         callback = self.on_success_callback if success else self.on_failure_callback",
          "1317:         if callback:",
          "1318:             self.log.info(\"Executing dag callback function: %s\", callback)",
          "1324:             try:",
          "1325:                 callback(context)",
          "1326:             except Exception:",
          "1327:                 self.log.exception(\"failed to invoke dag state update callback\")",
          "1328:                 Stats.incr(\"dag.callback_exceptions\")",
          "",
          "[Added Lines]",
          "1316:         callbacks = self.on_success_callback if success else self.on_failure_callback",
          "1317:         if callbacks:",
          "1318:             callbacks = callbacks if isinstance(callbacks, list) else [callbacks]",
          "1324:             for callback in callbacks:",
          "1325:                 self.log.info(\"Executing dag callback function: %s\", callback)",
          "1326:                 try:",
          "1327:                     callback(context)",
          "1328:                 except Exception:",
          "1329:                     self.log.exception(\"failed to invoke dag state update callback\")",
          "1330:                     Stats.incr(\"dag.callback_exceptions\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "3468:     max_active_tasks: int = conf.getint(\"core\", \"max_active_tasks_per_dag\"),",
          "3469:     max_active_runs: int = conf.getint(\"core\", \"max_active_runs_per_dag\"),",
          "3470:     dagrun_timeout: timedelta | None = None,",
          "3472:     default_view: str = conf.get_mandatory_value(\"webserver\", \"dag_default_view\").lower(),",
          "3473:     orientation: str = conf.get_mandatory_value(\"webserver\", \"dag_orientation\"),",
          "3474:     catchup: bool = conf.getboolean(\"scheduler\", \"catchup_by_default\"),",
          "3477:     doc_md: str | None = None,",
          "3478:     params: dict | None = None,",
          "3479:     access_control: dict | None = None,",
          "",
          "[Removed Lines]",
          "3471:     sla_miss_callback: SLAMissCallback | None = None,",
          "3475:     on_success_callback: DagStateChangeCallback | None = None,",
          "3476:     on_failure_callback: DagStateChangeCallback | None = None,",
          "",
          "[Added Lines]",
          "3473:     sla_miss_callback: None | SLAMissCallback | list[SLAMissCallback] = None,",
          "3477:     on_success_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,",
          "3478:     on_failure_callback: None | DagStateChangeCallback | list[DagStateChangeCallback] = None,",
          "",
          "---------------"
        ],
        "airflow/models/mappedoperator.py||airflow/models/mappedoperator.py": [
          "File: airflow/models/mappedoperator.py -> airflow/models/mappedoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "447:         return self.partial_kwargs.get(\"resources\")",
          "449:     @property",
          "451:         return self.partial_kwargs.get(\"on_execute_callback\")",
          "453:     @on_execute_callback.setter",
          "",
          "[Removed Lines]",
          "450:     def on_execute_callback(self) -> TaskStateChangeCallback | None:",
          "",
          "[Added Lines]",
          "450:     def on_execute_callback(self) -> None | TaskStateChangeCallback | list[TaskStateChangeCallback]:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "455:         self.partial_kwargs[\"on_execute_callback\"] = value",
          "457:     @property",
          "459:         return self.partial_kwargs.get(\"on_failure_callback\")",
          "461:     @on_failure_callback.setter",
          "",
          "[Removed Lines]",
          "458:     def on_failure_callback(self) -> TaskStateChangeCallback | None:",
          "",
          "[Added Lines]",
          "458:     def on_failure_callback(self) -> None | TaskStateChangeCallback | list[TaskStateChangeCallback]:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "463:         self.partial_kwargs[\"on_failure_callback\"] = value",
          "465:     @property",
          "467:         return self.partial_kwargs.get(\"on_retry_callback\")",
          "469:     @on_retry_callback.setter",
          "",
          "[Removed Lines]",
          "466:     def on_retry_callback(self) -> TaskStateChangeCallback | None:",
          "",
          "[Added Lines]",
          "466:     def on_retry_callback(self) -> None | TaskStateChangeCallback | list[TaskStateChangeCallback]:",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "471:         self.partial_kwargs[\"on_retry_callback\"] = value",
          "473:     @property",
          "475:         return self.partial_kwargs.get(\"on_success_callback\")",
          "477:     @on_success_callback.setter",
          "",
          "[Removed Lines]",
          "474:     def on_success_callback(self) -> TaskStateChangeCallback | None:",
          "",
          "[Added Lines]",
          "474:     def on_success_callback(self) -> None | TaskStateChangeCallback | list[TaskStateChangeCallback]:",
          "",
          "---------------"
        ],
        "airflow/models/taskinstance.py||airflow/models/taskinstance.py": [
          "File: airflow/models/taskinstance.py -> airflow/models/taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "104: from airflow.utils.email import send_email",
          "105: from airflow.utils.helpers import render_template_to_string",
          "106: from airflow.utils.log.logging_mixin import LoggingMixin",
          "107: from airflow.utils.net import get_hostname",
          "108: from airflow.utils.operator_helpers import context_to_airflow_vars",
          "109: from airflow.utils.platform import getuser",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "107: from airflow.utils.module_loading import qualname",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1528:         Stats.incr(\"ti_successes\")",
          "1530:     def _run_finished_callback(",
          "1532:     ) -> None:",
          "1533:         \"\"\"Run callback after task finishes\"\"\"",
          "1540:     def _execute_task(self, context, task_orig):",
          "1541:         \"\"\"Executes Task (optionally with a Timeout) and pushes Xcom results\"\"\"",
          "",
          "[Removed Lines]",
          "1531:         self, callback: TaskStateChangeCallback | None, context: Context, callback_type: str",
          "1534:         try:",
          "1535:             if callback:",
          "1536:                 callback(context)",
          "1537:         except Exception:  # pylint: disable=broad-except",
          "1538:             self.log.exception(f\"Error when executing {callback_type} callback\")",
          "",
          "[Added Lines]",
          "1532:         self,",
          "1533:         callbacks: None | TaskStateChangeCallback | list[TaskStateChangeCallback],",
          "1534:         context: Context,",
          "1535:         callback_type: str,",
          "1538:         if callbacks:",
          "1539:             callbacks = callbacks if isinstance(callbacks, list) else [callbacks]",
          "1540:             for callback in callbacks:",
          "1541:                 try:",
          "1542:                     callback(context)",
          "1543:                 except Exception:  # pylint: disable=broad-except",
          "1544:                     callback_name = qualname(callback).split(\".\")[-1]",
          "1545:                     self.log.exception(",
          "1546:                         f\"Error when executing {callback_name} callback\"  # type: ignore[attr-defined]",
          "1547:                     )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1633:     def _run_execute_callback(self, context: Context, task: Operator) -> None:",
          "1634:         \"\"\"Functions that need to be run before a Task is executed\"\"\"",
          "1641:     @provide_session",
          "1642:     def run(",
          "",
          "[Removed Lines]",
          "1635:         try:",
          "1636:             if task.on_execute_callback:",
          "1637:                 task.on_execute_callback(context)",
          "1638:         except Exception:",
          "1639:             self.log.exception(\"Failed when executing execute callback\")",
          "",
          "[Added Lines]",
          "1644:         callbacks = task.on_execute_callback",
          "1645:         if callbacks:",
          "1646:             callbacks = callbacks if isinstance(callbacks, list) else [callbacks]",
          "1647:             for callback in callbacks:",
          "1648:                 try:",
          "1649:                     callback(context)",
          "1650:                 except Exception:",
          "1651:                     self.log.exception(\"Failed when executing execute callback\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1814:         if force_fail or not self.is_eligible_to_retry():",
          "1815:             self.state = State.FAILED",
          "1816:             email_for_state = operator.attrgetter(\"email_on_failure\")",
          "1818:             callback_type = \"on_failure\"",
          "1819:         else:",
          "1820:             if self.state == State.QUEUED:",
          "",
          "[Removed Lines]",
          "1817:             callback = task.on_failure_callback if task else None",
          "",
          "[Added Lines]",
          "1829:             callbacks = task.on_failure_callback if task else None",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1822:                 self._try_number += 1",
          "1823:             self.state = State.UP_FOR_RETRY",
          "1824:             email_for_state = operator.attrgetter(\"email_on_retry\")",
          "1826:             callback_type = \"on_retry\"",
          "1828:         self._log_state(\"Immediate failure requested. \" if force_fail else \"\")",
          "",
          "[Removed Lines]",
          "1825:             callback = task.on_retry_callback if task else None",
          "",
          "[Added Lines]",
          "1837:             callbacks = task.on_retry_callback if task else None",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1832:             except Exception:",
          "1833:                 self.log.exception(\"Failed to send email to: %s\", task.email)",
          "1838:         if not test_mode:",
          "1839:             session.merge(self)",
          "",
          "[Removed Lines]",
          "1835:         if callback and context:",
          "1836:             self._run_finished_callback(callback, context, callback_type)",
          "",
          "[Added Lines]",
          "1847:         if callbacks and context:",
          "1848:             self._run_finished_callback(callbacks, context, callback_type)",
          "",
          "---------------"
        ],
        "tests/dag_processing/test_processor.py||tests/dag_processing/test_processor.py": [
          "File: tests/dag_processing/test_processor.py -> tests/dag_processing/test_processor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "303:         sla_callback = MagicMock(side_effect=RuntimeError(\"Could not call function\"))",
          "305:         test_start_date = timezone.utcnow() - datetime.timedelta(days=1)",
          "329:     @mock.patch(\"airflow.dag_processing.processor.send_email\")",
          "330:     def test_dag_file_processor_only_collect_emails_from_sla_missed_tasks(",
          "",
          "[Removed Lines]",
          "306:         dag, task = create_dummy_dag(",
          "307:             dag_id=\"test_sla_miss\",",
          "308:             task_id=\"dummy\",",
          "309:             sla_miss_callback=sla_callback,",
          "310:             default_args={\"start_date\": test_start_date, \"sla\": datetime.timedelta(hours=1)},",
          "311:         )",
          "312:         mock_stats_incr.reset_mock()",
          "314:         session.merge(TaskInstance(task=task, execution_date=test_start_date, state=\"Success\"))",
          "316:         # Create an SlaMiss where notification was sent, but email was not",
          "317:         session.merge(SlaMiss(task_id=\"dummy\", dag_id=\"test_sla_miss\", execution_date=test_start_date))",
          "319:         # Now call manage_slas and see if the sla_miss callback gets called",
          "320:         mock_log = mock.MagicMock()",
          "321:         dag_file_processor = DagFileProcessor(dag_ids=[], dag_directory=TEST_DAGS_FOLDER, log=mock_log)",
          "322:         dag_file_processor.manage_slas(dag=dag, session=session)",
          "323:         assert sla_callback.called",
          "324:         mock_log.exception.assert_called_once_with(",
          "325:             \"Could not call sla_miss_callback for DAG %s\", \"test_sla_miss\"",
          "326:         )",
          "327:         mock_stats_incr.assert_called_once_with(\"sla_callback_notification_failure\")",
          "",
          "[Added Lines]",
          "307:         for i, callback in enumerate([[sla_callback], sla_callback]):",
          "308:             dag, task = create_dummy_dag(",
          "309:                 dag_id=f\"test_sla_miss_{i}\",",
          "310:                 task_id=\"dummy\",",
          "311:                 sla_miss_callback=callback,",
          "312:                 default_args={\"start_date\": test_start_date, \"sla\": datetime.timedelta(hours=1)},",
          "313:             )",
          "314:             mock_stats_incr.reset_mock()",
          "316:             session.merge(TaskInstance(task=task, execution_date=test_start_date, state=\"Success\"))",
          "318:             # Create an SlaMiss where notification was sent, but email was not",
          "319:             session.merge(",
          "320:                 SlaMiss(task_id=\"dummy\", dag_id=f\"test_sla_miss_{i}\", execution_date=test_start_date)",
          "321:             )",
          "323:             # Now call manage_slas and see if the sla_miss callback gets called",
          "324:             mock_log = mock.MagicMock()",
          "325:             dag_file_processor = DagFileProcessor(dag_ids=[], dag_directory=TEST_DAGS_FOLDER, log=mock_log)",
          "326:             dag_file_processor.manage_slas(dag=dag, session=session)",
          "327:             assert sla_callback.called",
          "328:             mock_log.exception.assert_called_once_with(",
          "329:                 \"Could not call sla_miss_callback(%s) for DAG %s\",",
          "330:                 sla_callback.func_name,  # type: ignore[attr-defined]",
          "331:                 f\"test_sla_miss_{i}\",",
          "332:             )",
          "333:             mock_stats_incr.assert_called_once_with(\"sla_callback_notification_failure\")",
          "",
          "---------------"
        ],
        "tests/models/test_taskinstance.py||tests/models/test_taskinstance.py": [
          "File: tests/models/test_taskinstance.py -> tests/models/test_taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "78: from airflow.ti_deps.deps.trigger_rule_dep import TriggerRuleDep, _UpstreamTIStates",
          "79: from airflow.utils import timezone",
          "80: from airflow.utils.db import merge_conn",
          "81: from airflow.utils.session import create_session, provide_session",
          "82: from airflow.utils.state import State, TaskInstanceState",
          "83: from airflow.utils.task_group import TaskGroup",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "81: from airflow.utils.module_loading import qualname",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2328:             called = True",
          "2329:             assert context[\"dag_run\"].dag_id == \"test_dagrun_execute_callback\"",
          "2347:     @pytest.mark.parametrize(",
          "2348:         \"finished_state, callback_type\",",
          "",
          "[Removed Lines]",
          "2331:         ti = create_task_instance(",
          "2332:             dag_id=\"test_execute_callback\",",
          "2333:             on_execute_callback=on_execute_callable,",
          "2334:             state=State.RUNNING,",
          "2335:         )",
          "2337:         session = settings.Session()",
          "2339:         session.merge(ti)",
          "2340:         session.commit()",
          "2342:         ti._run_raw_task()",
          "2343:         assert called",
          "2344:         ti.refresh_from_db()",
          "2345:         assert ti.state == State.SUCCESS",
          "",
          "[Added Lines]",
          "2332:         for i, callback_input in enumerate([[on_execute_callable], on_execute_callable]):",
          "2334:             ti = create_task_instance(",
          "2335:                 dag_id=f\"test_execute_callback_{i}\",",
          "2336:                 on_execute_callback=callback_input,",
          "2337:                 state=State.RUNNING,",
          "2338:             )",
          "2340:             session = settings.Session()",
          "2342:             session.merge(ti)",
          "2343:             session.commit()",
          "2345:             ti._run_raw_task()",
          "2346:             assert called",
          "2347:             ti.refresh_from_db()",
          "2348:             assert ti.state == State.SUCCESS",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2363:             raise KeyError",
          "2364:             completed = True",
          "2381:     @provide_session",
          "2382:     def test_handle_failure(self, create_dummy_dag, session=None):",
          "",
          "[Removed Lines]",
          "2366:         ti = create_task_instance(",
          "2367:             end_date=timezone.utcnow() + datetime.timedelta(days=10),",
          "2368:             on_success_callback=on_finish_callable,",
          "2369:             on_retry_callback=on_finish_callable,",
          "2370:             on_failure_callback=on_finish_callable,",
          "2371:             state=finished_state,",
          "2372:         )",
          "2373:         ti._log = mock.Mock()",
          "2374:         ti._run_finished_callback(on_finish_callable, {}, callback_type)",
          "2376:         assert called",
          "2377:         assert not completed",
          "2378:         expected_message = f\"Error when executing {callback_type} callback\"",
          "2379:         ti.log.exception.assert_called_once_with(expected_message)",
          "",
          "[Added Lines]",
          "2369:         for i, callback_input in enumerate([[on_finish_callable], on_finish_callable]):",
          "2371:             ti = create_task_instance(",
          "2372:                 dag_id=f\"test_finish_callback_{i}\",",
          "2373:                 end_date=timezone.utcnow() + datetime.timedelta(days=10),",
          "2374:                 on_success_callback=callback_input,",
          "2375:                 on_retry_callback=callback_input,",
          "2376:                 on_failure_callback=callback_input,",
          "2377:                 state=finished_state,",
          "2378:             )",
          "2379:             ti._log = mock.Mock()",
          "2380:             ti._run_finished_callback(callback_input, {}, callback_type)",
          "2382:             assert called",
          "2383:             assert not completed",
          "2384:             callback_name = callback_input[0] if isinstance(callback_input, list) else callback_input",
          "2385:             callback_name = qualname(callback_name).split(\".\")[-1]",
          "2386:             expected_message = f\"Error when executing {callback_name} callback\"",
          "2387:             ti.log.exception.assert_called_once_with(expected_message)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ee38382efa54565c4b389eaeb536f0d45e12d498",
      "candidate_info": {
        "commit_hash": "ee38382efa54565c4b389eaeb536f0d45e12d498",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ee38382efa54565c4b389eaeb536f0d45e12d498",
        "files": [
          ".pre-commit-config.yaml",
          "STATIC_CODE_CHECKS.rst",
          "airflow/api_connexion/endpoints/provider_endpoint.py",
          "airflow/cli/commands/provider_command.py",
          "airflow/cli/commands/user_command.py",
          "airflow/config_templates/default_celery.py",
          "airflow/configuration.py",
          "airflow/decorators/base.py",
          "airflow/kubernetes/pod_generator.py",
          "airflow/kubernetes/pod_generator_deprecated.py",
          "airflow/metrics/validators.py",
          "airflow/models/dag.py",
          "airflow/models/dagrun.py",
          "airflow/security/utils.py",
          "airflow/serialization/serde.py",
          "airflow/utils/cli.py",
          "airflow/utils/db_cleanup.py",
          "airflow/utils/email.py",
          "airflow/utils/file.py",
          "airflow/utils/log/colored_log.py",
          "airflow/utils/log/logging_mixin.py",
          "airflow/utils/log/secrets_masker.py",
          "airflow/utils/task_group.py",
          "airflow/www/fab_security/manager.py",
          "airflow/www/views.py",
          "dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "images/breeze/output-commands-hash.txt",
          "images/breeze/output_static-checks.svg",
          "kubernetes_tests/test_base.py",
          "tests/utils/log/test_secrets_masker.py"
        ],
        "message": "Use linear time regular expressions (#32303)\n\nThe standard regexp library can consume > O(n) in certain circumstances.\nThe re2 library does not have this issue.",
        "before_after_code_files": [
          "airflow/api_connexion/endpoints/provider_endpoint.py||airflow/api_connexion/endpoints/provider_endpoint.py",
          "airflow/cli/commands/provider_command.py||airflow/cli/commands/provider_command.py",
          "airflow/cli/commands/user_command.py||airflow/cli/commands/user_command.py",
          "airflow/config_templates/default_celery.py||airflow/config_templates/default_celery.py",
          "airflow/configuration.py||airflow/configuration.py",
          "airflow/decorators/base.py||airflow/decorators/base.py",
          "airflow/kubernetes/pod_generator.py||airflow/kubernetes/pod_generator.py",
          "airflow/kubernetes/pod_generator_deprecated.py||airflow/kubernetes/pod_generator_deprecated.py",
          "airflow/metrics/validators.py||airflow/metrics/validators.py",
          "airflow/models/dag.py||airflow/models/dag.py",
          "airflow/models/dagrun.py||airflow/models/dagrun.py",
          "airflow/security/utils.py||airflow/security/utils.py",
          "airflow/serialization/serde.py||airflow/serialization/serde.py",
          "airflow/utils/cli.py||airflow/utils/cli.py",
          "airflow/utils/db_cleanup.py||airflow/utils/db_cleanup.py",
          "airflow/utils/email.py||airflow/utils/email.py",
          "airflow/utils/file.py||airflow/utils/file.py",
          "airflow/utils/log/colored_log.py||airflow/utils/log/colored_log.py",
          "airflow/utils/log/logging_mixin.py||airflow/utils/log/logging_mixin.py",
          "airflow/utils/log/secrets_masker.py||airflow/utils/log/secrets_masker.py",
          "airflow/utils/task_group.py||airflow/utils/task_group.py",
          "airflow/www/fab_security/manager.py||airflow/www/fab_security/manager.py",
          "airflow/www/views.py||airflow/www/views.py",
          "dev/breeze/src/airflow_breeze/pre_commit_ids.py||dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "kubernetes_tests/test_base.py||kubernetes_tests/test_base.py",
          "tests/utils/log/test_secrets_masker.py||tests/utils/log/test_secrets_masker.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/models/dag.py||airflow/models/dag.py",
            "airflow/models/dagrun.py||airflow/models/dagrun.py",
            "airflow/www/views.py||airflow/www/views.py"
          ],
          "candidate": [
            "airflow/models/dag.py||airflow/models/dag.py",
            "airflow/models/dagrun.py||airflow/models/dagrun.py",
            "airflow/www/views.py||airflow/www/views.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/api_connexion/endpoints/provider_endpoint.py||airflow/api_connexion/endpoints/provider_endpoint.py": [
          "File: airflow/api_connexion/endpoints/provider_endpoint.py -> airflow/api_connexion/endpoints/provider_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "21: from airflow.api_connexion import security",
          "22: from airflow.api_connexion.schemas.provider_schema import (",
          "",
          "[Removed Lines]",
          "19: import re",
          "",
          "[Added Lines]",
          "19: import re2",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "32: def _remove_rst_syntax(value: str) -> str:",
          "36: def _provider_mapper(provider: ProviderInfo) -> Provider:",
          "",
          "[Removed Lines]",
          "33:     return re.sub(\"[`_<>]\", \"\", value.strip(\" \\n.\"))",
          "",
          "[Added Lines]",
          "33:     return re2.sub(\"[`_<>]\", \"\", value.strip(\" \\n.\"))",
          "",
          "---------------"
        ],
        "airflow/cli/commands/provider_command.py||airflow/cli/commands/provider_command.py": [
          "File: airflow/cli/commands/provider_command.py -> airflow/cli/commands/provider_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: \"\"\"Providers sub-commands.\"\"\"",
          "18: from __future__ import annotations",
          "22: from airflow.cli.simple_table import AirflowConsole",
          "23: from airflow.providers_manager import ProvidersManager",
          "",
          "[Removed Lines]",
          "20: import re",
          "",
          "[Added Lines]",
          "20: import re2",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "29: def _remove_rst_syntax(value: str) -> str:",
          "33: @suppress_logs_and_warning",
          "",
          "[Removed Lines]",
          "30:     return re.sub(\"[`_<>]\", \"\", value.strip(\" \\n.\"))",
          "",
          "[Added Lines]",
          "30:     return re2.sub(\"[`_<>]\", \"\", value.strip(\" \\n.\"))",
          "",
          "---------------"
        ],
        "airflow/cli/commands/user_command.py||airflow/cli/commands/user_command.py": [
          "File: airflow/cli/commands/user_command.py -> airflow/cli/commands/user_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import json",
          "23: import os",
          "24: import random",
          "26: import string",
          "27: from typing import Any",
          "29: from marshmallow import Schema, fields, validate",
          "30: from marshmallow.exceptions import ValidationError",
          "",
          "[Removed Lines]",
          "25: import re",
          "",
          "[Added Lines]",
          "28: import re2",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "164:         # In the User model the first and last name fields have underscores,",
          "165:         # but the corresponding parameters in the CLI don't",
          "166:         def remove_underscores(s):",
          "169:         users = [",
          "170:             {",
          "",
          "[Removed Lines]",
          "167:             return re.sub(\"_\", \"\", s)",
          "",
          "[Added Lines]",
          "167:             return re2.sub(\"_\", \"\", s)",
          "",
          "---------------"
        ],
        "airflow/config_templates/default_celery.py||airflow/config_templates/default_celery.py": [
          "File: airflow/config_templates/default_celery.py -> airflow/config_templates/default_celery.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: from __future__ import annotations",
          "21: import logging",
          "23: import ssl",
          "25: from airflow.configuration import conf",
          "26: from airflow.exceptions import AirflowConfigException, AirflowException",
          "",
          "[Removed Lines]",
          "22: import re",
          "",
          "[Added Lines]",
          "24: import re2",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "88:                 \"ca_certs\": conf.get(\"celery\", \"SSL_CACERT\"),",
          "89:                 \"cert_reqs\": ssl.CERT_REQUIRED,",
          "90:             }",
          "92:             broker_use_ssl = {",
          "93:                 \"ssl_keyfile\": conf.get(\"celery\", \"SSL_KEY\"),",
          "94:                 \"ssl_certfile\": conf.get(\"celery\", \"SSL_CERT\"),",
          "",
          "[Removed Lines]",
          "91:         elif broker_url and re.search(\"rediss?://|sentinel://\", broker_url):",
          "",
          "[Added Lines]",
          "92:         elif broker_url and re2.search(\"rediss?://|sentinel://\", broker_url):",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "114:         f\"all necessary certs and key ({e}).\"",
          "115:     )",
          "118:     log.warning(",
          "119:         \"You have configured a result_backend of %s, it is highly recommended \"",
          "120:         \"to use an alternative result_backend (i.e. a database).\",",
          "",
          "[Removed Lines]",
          "117: if re.search(\"rediss?://|amqp://|rpc://\", result_backend):",
          "",
          "[Added Lines]",
          "118: if re2.search(\"rediss?://|amqp://|rpc://\", result_backend):",
          "",
          "---------------"
        ],
        "airflow/configuration.py||airflow/configuration.py": [
          "File: airflow/configuration.py -> airflow/configuration.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import multiprocessing",
          "24: import os",
          "25: import pathlib",
          "27: import shlex",
          "28: import stat",
          "29: import subprocess",
          "",
          "[Removed Lines]",
          "26: import re",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "36: from configparser import _UNSET, ConfigParser, NoOptionError, NoSectionError  # type: ignore",
          "37: from contextlib import contextmanager, suppress",
          "38: from json.decoder import JSONDecodeError",
          "41: from urllib.parse import urlsplit",
          "43: from typing_extensions import overload",
          "45: from airflow.auth.managers.base_auth_manager import BaseAuthManager",
          "",
          "[Removed Lines]",
          "39: from re import Pattern",
          "40: from typing import IO, Any, Dict, Iterable, Set, Tuple, Union",
          "",
          "[Added Lines]",
          "38: from typing import IO, Any, Dict, Iterable, Pattern, Set, Tuple, Union",
          "41: import re2",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "56:     warnings.filterwarnings(action=\"default\", category=DeprecationWarning, module=\"airflow\")",
          "57:     warnings.filterwarnings(action=\"default\", category=PendingDeprecationWarning, module=\"airflow\")",
          "61: ConfigType = Union[str, int, float, bool]",
          "62: ConfigOptionsDictType = Dict[str, ConfigType]",
          "",
          "[Removed Lines]",
          "59: _SQLITE3_VERSION_PATTERN = re.compile(r\"(?P<version>^\\d+(?:\\.\\d+)*)\\D?.*$\")",
          "",
          "[Added Lines]",
          "58: _SQLITE3_VERSION_PATTERN = re2.compile(r\"(?P<version>^\\d+(?:\\.\\d+)*)\\D?.*$\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "270:     # about. Mapping of section -> setting -> { old, replace, by_version }",
          "271:     deprecated_values: dict[str, dict[str, tuple[Pattern, str, str]]] = {",
          "272:         \"core\": {",
          "274:         },",
          "275:         \"webserver\": {",
          "278:         },",
          "279:         \"email\": {",
          "280:             \"email_backend\": (",
          "282:                 r\"airflow.providers.sendgrid.utils.emailer.send_email\",",
          "283:                 \"2.1\",",
          "284:             ),",
          "285:         },",
          "286:         \"logging\": {",
          "287:             \"log_filename_template\": (",
          "289:                 \"XX-set-after-default-config-loaded-XX\",",
          "290:                 \"3.0\",",
          "291:             ),",
          "292:         },",
          "293:         \"api\": {",
          "294:             \"auth_backends\": (",
          "296:                 \"airflow.api.auth.backend.session\",",
          "297:                 \"3.0\",",
          "298:             ),",
          "299:         },",
          "300:         \"elasticsearch\": {",
          "301:             \"log_id_template\": (",
          "303:                 \"{dag_id}-{task_id}-{run_id}-{map_index}-{try_number}\",",
          "304:                 \"3.0\",",
          "305:             )",
          "",
          "[Removed Lines]",
          "273:             \"hostname_callable\": (re.compile(r\":\"), r\".\", \"2.1\"),",
          "276:             \"navbar_color\": (re.compile(r\"\\A#007A87\\Z\", re.IGNORECASE), \"#fff\", \"2.1\"),",
          "277:             \"dag_default_view\": (re.compile(r\"^tree$\"), \"grid\", \"3.0\"),",
          "281:                 re.compile(r\"^airflow\\.contrib\\.utils\\.sendgrid\\.send_email$\"),",
          "288:                 re.compile(re.escape(\"{{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log\")),",
          "295:                 re.compile(r\"^airflow\\.api\\.auth\\.backend\\.deny_all$|^$\"),",
          "302:                 re.compile(\"^\" + re.escape(\"{dag_id}-{task_id}-{execution_date}-{try_number}\") + \"$\"),",
          "",
          "[Added Lines]",
          "272:             \"hostname_callable\": (re2.compile(r\":\"), r\".\", \"2.1\"),",
          "275:             \"navbar_color\": (re2.compile(r\"(?i)\\A#007A87\\z\"), \"#fff\", \"2.1\"),",
          "276:             \"dag_default_view\": (re2.compile(r\"^tree$\"), \"grid\", \"3.0\"),",
          "280:                 re2.compile(r\"^airflow\\.contrib\\.utils\\.sendgrid\\.send_email$\"),",
          "287:                 re2.compile(re2.escape(\"{{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log\")),",
          "294:                 re2.compile(r\"^airflow\\.api\\.auth\\.backend\\.deny_all$|^$\"),",
          "301:                 re2.compile(\"^\" + re2.escape(\"{dag_id}-{task_id}-{execution_date}-{try_number}\") + \"$\"),",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "426:                 FutureWarning,",
          "427:             )",
          "428:             self.upgraded_values[(section, key)] = old_value",
          "430:             self._update_env_var(section=section, name=key, new_value=new_value)",
          "432:             # if the old value is set via env var, we need to wipe it",
          "",
          "[Removed Lines]",
          "429:             new_value = re.sub(\"^\" + re.escape(f\"{parsed.scheme}://\"), f\"{good_scheme}://\", old_value)",
          "",
          "[Added Lines]",
          "428:             new_value = re2.sub(\"^\" + re2.escape(f\"{parsed.scheme}://\"), f\"{good_scheme}://\", old_value)",
          "",
          "---------------"
        ],
        "airflow/decorators/base.py||airflow/decorators/base.py": [
          "File: airflow/decorators/base.py -> airflow/decorators/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: from __future__ import annotations",
          "19: import inspect",
          "21: import warnings",
          "22: from functools import cached_property",
          "23: from itertools import chain",
          "",
          "[Removed Lines]",
          "20: import re",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "38: )",
          "40: import attr",
          "41: import typing_extensions",
          "42: from sqlalchemy.orm import Session",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "40: import re2",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "144:         return task_id",
          "146:     def _find_id_suffixes(dag: DAG) -> Iterator[int]:",
          "148:         for task_id in dag.task_ids:",
          "150:             if match is None:",
          "151:                 continue",
          "152:             yield int(match.group(1))",
          "153:         yield 0  # Default if there's no matching task ID.",
          "156:     return f\"{core}__{max(_find_id_suffixes(dag)) + 1}\"",
          "",
          "[Removed Lines]",
          "147:         prefix = re.split(r\"__\\d+$\", tg_task_id)[0]",
          "149:             match = re.match(rf\"^{prefix}__(\\d+)$\", task_id)",
          "155:     core = re.split(r\"__\\d+$\", task_id)[0]",
          "",
          "[Added Lines]",
          "147:         prefix = re2.split(r\"__\\d+$\", tg_task_id)[0]",
          "149:             match = re2.match(rf\"^{prefix}__(\\d+)$\", task_id)",
          "155:     core = re2.split(r\"__\\d+$\", task_id)[0]",
          "",
          "---------------"
        ],
        "airflow/kubernetes/pod_generator.py||airflow/kubernetes/pod_generator.py": [
          "File: airflow/kubernetes/pod_generator.py -> airflow/kubernetes/pod_generator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "28: import datetime",
          "29: import logging",
          "30: import os",
          "32: import warnings",
          "33: from functools import reduce",
          "35: from dateutil import parser",
          "36: from kubernetes.client import models as k8s",
          "37: from kubernetes.client.api_client import ApiClient",
          "",
          "[Removed Lines]",
          "31: import re",
          "",
          "[Added Lines]",
          "34: import re2",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "65:     way from the original value sent to this function, then we need to truncate to",
          "66:     53 chars, and append it with a unique hash.",
          "67:     \"\"\"",
          "70:     if len(safe_label) > MAX_LABEL_LEN or string != safe_label:",
          "71:         safe_hash = md5(string.encode()).hexdigest()[:9]",
          "",
          "[Removed Lines]",
          "68:     safe_label = re.sub(r\"^[^a-z0-9A-Z]*|[^a-zA-Z0-9_\\-\\.]|[^a-z0-9A-Z]*$\", \"\", string)",
          "",
          "[Added Lines]",
          "68:     safe_label = re2.sub(r\"^[^a-z0-9A-Z]*|[^a-zA-Z0-9_\\-\\.]|[^a-z0-9A-Z]*$\", \"\", string)",
          "",
          "---------------"
        ],
        "airflow/kubernetes/pod_generator_deprecated.py||airflow/kubernetes/pod_generator_deprecated.py": [
          "File: airflow/kubernetes/pod_generator_deprecated.py -> airflow/kubernetes/pod_generator_deprecated.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: from __future__ import annotations",
          "27: import copy",
          "29: import uuid",
          "31: from kubernetes.client import models as k8s",
          "33: from airflow.utils.hashlib_wrapper import md5",
          "",
          "[Removed Lines]",
          "28: import re",
          "",
          "[Added Lines]",
          "30: import re2",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "70:     way from the original value sent to this function, then we need to truncate to",
          "71:     53 chars, and append it with a unique hash.",
          "72:     \"\"\"",
          "75:     if len(safe_label) > MAX_LABEL_LEN or string != safe_label:",
          "76:         safe_hash = md5(string.encode()).hexdigest()[:9]",
          "",
          "[Removed Lines]",
          "73:     safe_label = re.sub(r\"^[^a-z0-9A-Z]*|[^a-zA-Z0-9_\\-\\.]|[^a-z0-9A-Z]*$\", \"\", string)",
          "",
          "[Added Lines]",
          "73:     safe_label = re2.sub(r\"^[^a-z0-9A-Z]*|[^a-zA-Z0-9_\\-\\.]|[^a-z0-9A-Z]*$\", \"\", string)",
          "",
          "---------------"
        ],
        "airflow/metrics/validators.py||airflow/metrics/validators.py": [
          "File: airflow/metrics/validators.py -> airflow/metrics/validators.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import abc",
          "23: import logging",
          "25: import string",
          "26: import warnings",
          "27: from functools import partial, wraps",
          "28: from typing import Callable, Iterable, Pattern, cast",
          "30: from airflow.configuration import conf",
          "31: from airflow.exceptions import InvalidStatsNameException",
          "",
          "[Removed Lines]",
          "24: import re",
          "",
          "[Added Lines]",
          "29: import re2",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "78:     r\"^dagrun\\.schedule_delay\\.(?P<dag_id>.*)$\",",
          "79:     r\"^dagrun\\.(?P<dag_id>.*)\\.first_task_scheduling_delay$\",",
          "80: }",
          "83: OTEL_NAME_MAX_LENGTH = 63",
          "",
          "[Removed Lines]",
          "81: BACK_COMPAT_METRIC_NAMES: set[Pattern[str]] = {re.compile(name) for name in BACK_COMPAT_METRIC_NAME_PATTERNS}",
          "",
          "[Added Lines]",
          "82: BACK_COMPAT_METRIC_NAMES: set[Pattern[str]] = {re2.compile(name) for name in BACK_COMPAT_METRIC_NAME_PATTERNS}",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "132:         # If the name is in the exceptions list, do not fail it for being too long.",
          "133:         # It may still be deemed invalid for other reasons below.",
          "134:         for exemption in BACK_COMPAT_METRIC_NAMES:",
          "136:                 # There is a back-compat exception for this name; proceed",
          "137:                 name_length_exemption = True",
          "138:                 matched_exemption = exemption.pattern",
          "",
          "[Removed Lines]",
          "135:             if re.match(exemption, stat_name):",
          "",
          "[Added Lines]",
          "136:             if re2.match(exemption, stat_name):",
          "",
          "---------------"
        ],
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "53: import jinja2",
          "54: import pendulum",
          "56: from dateutil.relativedelta import relativedelta",
          "57: from pendulum.tz.timezone import Timezone",
          "58: from sqlalchemy import (",
          "",
          "[Removed Lines]",
          "55: import re2 as re",
          "",
          "[Added Lines]",
          "55: import re2",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2361:         dag = copy.deepcopy(self, memo)  # type: ignore",
          "2363:         if isinstance(task_ids_or_regex, (str, Pattern)):",
          "2365:         else:",
          "2366:             matched_tasks = [t for t in self.tasks if t.task_id in task_ids_or_regex]",
          "",
          "[Removed Lines]",
          "2364:             matched_tasks = [t for t in self.tasks if re.findall(task_ids_or_regex, t.task_id)]",
          "",
          "[Added Lines]",
          "2364:             matched_tasks = [t for t in self.tasks if re2.findall(task_ids_or_regex, t.task_id)]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2829:         regex = airflow_conf.get(\"scheduler\", \"allowed_run_id_pattern\")",
          "2833:                 raise AirflowException(",
          "2834:                     f\"The provided run ID '{run_id}' is invalid. It does not match either \"",
          "2835:                     f\"the configured pattern: '{regex}' or the built-in pattern: '{RUN_ID_REGEX}'\"",
          "",
          "[Removed Lines]",
          "2831:         if run_id and not re.match(RUN_ID_REGEX, run_id):",
          "2832:             if not regex.strip() or not re.match(regex.strip(), run_id):",
          "",
          "[Added Lines]",
          "2831:         if run_id and not re2.match(RUN_ID_REGEX, run_id):",
          "2832:             if not regex.strip() or not re2.match(regex.strip(), run_id):",
          "",
          "---------------"
        ],
        "airflow/models/dagrun.py||airflow/models/dagrun.py": [
          "File: airflow/models/dagrun.py -> airflow/models/dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: from datetime import datetime",
          "25: from typing import TYPE_CHECKING, Any, Callable, Iterable, Iterator, NamedTuple, Sequence, TypeVar, overload",
          "28: from sqlalchemy import (",
          "29:     Boolean,",
          "30:     Column,",
          "",
          "[Removed Lines]",
          "27: import re2 as re",
          "",
          "[Added Lines]",
          "27: import re2",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "248:         if not run_id:",
          "249:             return None",
          "250:         regex = airflow_conf.get(\"scheduler\", \"allowed_run_id_pattern\")",
          "252:             raise ValueError(",
          "253:                 f\"The run_id provided '{run_id}' does not match the pattern '{regex}' or '{RUN_ID_REGEX}'\"",
          "254:             )",
          "",
          "[Removed Lines]",
          "251:         if not re.match(regex, run_id) and not re.match(RUN_ID_REGEX, run_id):",
          "",
          "[Added Lines]",
          "251:         if not re2.match(regex, run_id) and not re2.match(RUN_ID_REGEX, run_id):",
          "",
          "---------------"
        ],
        "airflow/security/utils.py||airflow/security/utils.py": [
          "File: airflow/security/utils.py -> airflow/security/utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: # limitations under the License.",
          "35: #",
          "36: \"\"\"Various security-related utils.\"\"\"",
          "38: import socket",
          "40: from airflow.utils.net import get_hostname",
          "",
          "[Removed Lines]",
          "37: import re",
          "",
          "[Added Lines]",
          "39: import re2",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "49:     \"\"\"",
          "50:     if not principal:",
          "51:         return None",
          "55: def replace_hostname_pattern(components, host=None):",
          "",
          "[Removed Lines]",
          "52:     return re.split(r\"[/@]\", str(principal))",
          "",
          "[Added Lines]",
          "53:     return re2.split(r\"[/@]\", str(principal))",
          "",
          "---------------"
        ],
        "airflow/serialization/serde.py||airflow/serialization/serde.py": [
          "File: airflow/serialization/serde.py -> airflow/serialization/serde.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import enum",
          "22: import functools",
          "23: import logging",
          "25: import sys",
          "26: from importlib import import_module",
          "27: from types import ModuleType",
          "30: import attr",
          "32: import airflow.serialization.serializers",
          "33: from airflow.configuration import conf",
          "",
          "[Removed Lines]",
          "24: import re",
          "28: from typing import Any, TypeVar, Union, cast",
          "",
          "[Added Lines]",
          "27: from typing import Any, Pattern, TypeVar, Union, cast",
          "30: import re2",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "359: @functools.lru_cache(maxsize=None)",
          "361:     patterns = conf.get(\"core\", \"allowed_deserialization_classes\").split()",
          "365: _register()",
          "",
          "[Removed Lines]",
          "360: def _get_patterns() -> list[re.Pattern]:",
          "362:     return [re.compile(re.sub(r\"(\\w)\\.\", r\"\\1\\..\", p)) for p in patterns]",
          "",
          "[Added Lines]",
          "360: def _get_patterns() -> list[Pattern]:",
          "362:     return [re2.compile(re2.sub(r\"(\\w)\\.\", r\"\\1\\..\", p)) for p in patterns]",
          "",
          "---------------"
        ],
        "airflow/utils/cli.py||airflow/utils/cli.py": [
          "File: airflow/utils/cli.py -> airflow/utils/cli.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import functools",
          "22: import logging",
          "23: import os",
          "25: import socket",
          "26: import sys",
          "27: import threading",
          "",
          "[Removed Lines]",
          "24: import re",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "32: from pathlib import Path",
          "33: from typing import TYPE_CHECKING, Callable, TypeVar, cast",
          "35: from sqlalchemy.orm import Session",
          "37: from airflow import settings",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "34: import re2",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "252:     if not use_regex:",
          "253:         return [get_dag(subdir, dag_id)]",
          "254:     dagbag = DagBag(process_subdir(subdir))",
          "256:     if not matched_dags:",
          "257:         raise AirflowException(",
          "258:             f\"dag_id could not be found with regex: {dag_id}. Either the dag did not exist or \"",
          "",
          "[Removed Lines]",
          "255:     matched_dags = [dag for dag in dagbag.dags.values() if re.search(dag_id, dag.dag_id)]",
          "",
          "[Added Lines]",
          "255:     matched_dags = [dag for dag in dagbag.dags.values() if re2.search(dag_id, dag.dag_id)]",
          "",
          "---------------"
        ],
        "airflow/utils/db_cleanup.py||airflow/utils/db_cleanup.py": [
          "File: airflow/utils/db_cleanup.py -> airflow/utils/db_cleanup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "143: def _do_delete(*, query, orm_model, skip_archive, session):",
          "145:     from datetime import datetime",
          "147:     print(\"Performing Delete...\")",
          "148:     # using bulk delete",
          "149:     # create a new table and copy the rows there",
          "151:     target_table_name = f\"{ARCHIVE_TABLE_PREFIX}{orm_model.name}__{timestamp_str}\"",
          "152:     print(f\"Moving data to table {target_table_name}\")",
          "153:     bind = session.get_bind()",
          "",
          "[Removed Lines]",
          "144:     import re",
          "150:     timestamp_str = re.sub(r\"[^\\d]\", \"\", datetime.utcnow().isoformat())[:14]",
          "",
          "[Added Lines]",
          "146:     import re2",
          "151:     timestamp_str = re2.sub(r\"[^\\d]\", \"\", datetime.utcnow().isoformat())[:14]",
          "",
          "---------------"
        ],
        "airflow/utils/email.py||airflow/utils/email.py": [
          "File: airflow/utils/email.py -> airflow/utils/email.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import collections.abc",
          "21: import logging",
          "22: import os",
          "24: import smtplib",
          "25: import warnings",
          "26: from email.mime.application import MIMEApplication",
          "",
          "[Removed Lines]",
          "23: import re",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "29: from email.utils import formatdate",
          "30: from typing import Any, Iterable",
          "32: from airflow.configuration import conf",
          "33: from airflow.exceptions import AirflowConfigException, AirflowException, RemovedInAirflow3Warning",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "31: import re2",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "328:     :return: A list of email addresses.",
          "329:     \"\"\"",
          "330:     pattern = r\"\\s*[,;]\\s*\"",
          "",
          "[Removed Lines]",
          "331:     return [address for address in re.split(pattern, addresses)]",
          "",
          "[Added Lines]",
          "332:     return [address for address in re2.split(pattern, addresses)]",
          "",
          "---------------"
        ],
        "airflow/utils/file.py||airflow/utils/file.py": [
          "File: airflow/utils/file.py -> airflow/utils/file.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import io",
          "22: import logging",
          "23: import os",
          "25: import zipfile",
          "26: from collections import OrderedDict",
          "27: from pathlib import Path",
          "30: from pathspec.patterns import GitWildMatchPattern",
          "32: from airflow.configuration import conf",
          "",
          "[Removed Lines]",
          "24: import re",
          "28: from typing import Generator, NamedTuple, Protocol, overload",
          "",
          "[Added Lines]",
          "27: from typing import Generator, NamedTuple, Pattern, Protocol, overload",
          "29: import re2",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "53: class _RegexpIgnoreRule(NamedTuple):",
          "54:     \"\"\"Typed namedtuple with utility functions for regexp ignore rules.\"\"\"",
          "57:     base_dir: Path",
          "59:     @staticmethod",
          "60:     def compile(pattern: str, base_dir: Path, definition_file: Path) -> _IgnoreRule | None:",
          "61:         \"\"\"Build an ignore rule from the supplied regexp pattern and log a useful warning if it is invalid.\"\"\"",
          "62:         try:",
          "65:             log.warning(\"Ignoring invalid regex '%s' from %s: %s\", pattern, definition_file, e)",
          "66:             return None",
          "",
          "[Removed Lines]",
          "56:     pattern: re.Pattern",
          "63:             return _RegexpIgnoreRule(re.compile(pattern), base_dir)",
          "64:         except re.error as e:",
          "",
          "[Added Lines]",
          "56:     pattern: Pattern",
          "63:             return _RegexpIgnoreRule(re2.compile(pattern), base_dir)",
          "64:         except re2.error as e:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "79: class _GlobIgnoreRule(NamedTuple):",
          "80:     \"\"\"Typed namedtuple with utility functions for glob ignore rules.\"\"\"",
          "83:     raw_pattern: str",
          "84:     include: bool | None = None",
          "85:     relative_to: Path | None = None",
          "",
          "[Removed Lines]",
          "82:     pattern: re.Pattern",
          "",
          "[Added Lines]",
          "82:     pattern: Pattern",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "150:     Path(path).mkdir(mode=mode, parents=True, exist_ok=True)",
          "156: @overload",
          "",
          "[Removed Lines]",
          "153: ZIP_REGEX = re.compile(rf\"((.*\\.zip){re.escape(os.sep)})?(.*)\")",
          "",
          "[Added Lines]",
          "153: ZIP_REGEX = re2.compile(rf\"((.*\\.zip){re2.escape(os.sep)})?(.*)\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "217:         ignore_file_path = Path(root) / ignore_file_name",
          "218:         if ignore_file_path.is_file():",
          "219:             with open(ignore_file_path) as ifile:",
          "221:                 # append new patterns and filter out \"None\" objects, which are invalid patterns",
          "222:                 patterns += [",
          "223:                     p",
          "",
          "[Removed Lines]",
          "220:                 lines_no_comments = [re.sub(r\"\\s*#.*\", \"\", line) for line in ifile.read().split(\"\\n\")]",
          "",
          "[Added Lines]",
          "219:                 lines_no_comments = [re2.sub(r\"\\s*#.*\", \"\", line) for line in ifile.read().split(\"\\n\")]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "327:     return file_paths",
          "333: def might_contain_dag(file_path: str, safe_mode: bool, zip_file: zipfile.ZipFile | None = None) -> bool:",
          "",
          "[Removed Lines]",
          "330: COMMENT_PATTERN = re.compile(r\"\\s*#.*\")",
          "",
          "[Added Lines]",
          "329: COMMENT_PATTERN = re2.compile(r\"\\s*#.*\")",
          "",
          "---------------"
        ],
        "airflow/utils/log/colored_log.py||airflow/utils/log/colored_log.py": [
          "File: airflow/utils/log/colored_log.py -> airflow/utils/log/colored_log.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: \"\"\"Class responsible for colouring logs based on log level.\"\"\"",
          "19: from __future__ import annotations",
          "22: import sys",
          "23: from logging import LogRecord",
          "24: from typing import Any",
          "26: from colorlog import TTYColoredFormatter",
          "27: from colorlog.escape_codes import esc, escape_codes",
          "",
          "[Removed Lines]",
          "21: import re",
          "",
          "[Added Lines]",
          "25: import re2",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "62:     @staticmethod",
          "63:     def _count_number_of_arguments_in_message(record: LogRecord) -> int:",
          "65:         return len(matches) if matches else 0",
          "67:     def _color_record_args(self, record: LogRecord) -> LogRecord:",
          "",
          "[Removed Lines]",
          "64:         matches = re.findall(r\"%.\", record.msg)",
          "",
          "[Added Lines]",
          "64:         matches = re2.findall(r\"%.\", record.msg)",
          "",
          "---------------"
        ],
        "airflow/utils/log/logging_mixin.py||airflow/utils/log/logging_mixin.py": [
          "File: airflow/utils/log/logging_mixin.py -> airflow/utils/log/logging_mixin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import abc",
          "21: import enum",
          "22: import logging",
          "24: import sys",
          "25: from io import IOBase",
          "26: from logging import Handler, Logger, StreamHandler",
          "27: from typing import IO, Any, TypeVar, cast",
          "29: # 7-bit C1 ANSI escape sequences",
          "33: # Private: A sentinel objects",
          "",
          "[Removed Lines]",
          "23: import re",
          "30: ANSI_ESCAPE = re.compile(r\"\\x1B[@-_][0-?]*[ -/]*[@-~]\")",
          "",
          "[Added Lines]",
          "28: import re2",
          "31: ANSI_ESCAPE = re2.compile(r\"\\x1B[@-_][0-?]*[ -/]*[@-~]\")",
          "",
          "---------------"
        ],
        "airflow/utils/log/secrets_masker.py||airflow/utils/log/secrets_masker.py": [
          "File: airflow/utils/log/secrets_masker.py -> airflow/utils/log/secrets_masker.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import collections.abc",
          "21: import logging",
          "23: import sys",
          "24: from functools import cached_property",
          "25: from typing import (",
          "",
          "[Removed Lines]",
          "22: import re",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "31:     Iterable,",
          "32:     Iterator,",
          "33:     List,",
          "34:     TextIO,",
          "35:     Tuple,",
          "36:     TypeVar,",
          "37:     Union,",
          "38: )",
          "40: from airflow import settings",
          "41: from airflow.compat.functools import cache",
          "42: from airflow.typing_compat import TypeGuard",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33:     Pattern,",
          "40: import re2",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "144: class SecretsMasker(logging.Filter):",
          "145:     \"\"\"Redact secrets from logs.\"\"\"",
          "148:     patterns: set[str]",
          "150:     ALREADY_FILTERED_FLAG = \"__SecretsMasker_filtered\"",
          "",
          "[Removed Lines]",
          "147:     replacer: re.Pattern | None = None",
          "",
          "[Added Lines]",
          "149:     replacer: Pattern | None = None",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "332:             new_mask = False",
          "333:             for s in self._adaptations(secret):",
          "334:                 if s:",
          "336:                     if pattern not in self.patterns and (not name or should_hide_value_for_key(name)):",
          "337:                         self.patterns.add(pattern)",
          "338:                         new_mask = True",
          "340:             if new_mask:",
          "343:         elif isinstance(secret, collections.abc.Iterable):",
          "344:             for v in secret:",
          "",
          "[Removed Lines]",
          "335:                     pattern = re.escape(s)",
          "341:                 self.replacer = re.compile(\"|\".join(self.patterns))",
          "",
          "[Added Lines]",
          "337:                     pattern = re2.escape(s)",
          "343:                 self.replacer = re2.compile(\"|\".join(self.patterns))",
          "",
          "---------------"
        ],
        "airflow/utils/task_group.py||airflow/utils/task_group.py": [
          "File: airflow/utils/task_group.py -> airflow/utils/task_group.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import copy",
          "25: import functools",
          "26: import operator",
          "28: import weakref",
          "29: from typing import TYPE_CHECKING, Any, Generator, Iterator, Sequence",
          "31: from airflow.compat.functools import cache",
          "32: from airflow.exceptions import (",
          "33:     AirflowDagCycleException,",
          "",
          "[Removed Lines]",
          "27: import re",
          "",
          "[Added Lines]",
          "30: import re2",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "166:         if self._group_id in self.used_group_ids:",
          "167:             if not add_suffix_on_collision:",
          "168:                 raise DuplicateTaskIdFound(f\"group_id '{self._group_id}' has already been added to the DAG\")",
          "170:             suffixes = sorted(",
          "172:                 for used_group_id in self.used_group_ids",
          "174:             )",
          "175:             if not suffixes:",
          "176:                 self._group_id += \"__1\"",
          "",
          "[Removed Lines]",
          "169:             base = re.split(r\"__\\d+$\", self._group_id)[0]",
          "171:                 int(re.split(r\"^.+__\", used_group_id)[1])",
          "173:                 if used_group_id is not None and re.match(rf\"^{base}__\\d+$\", used_group_id)",
          "",
          "[Added Lines]",
          "170:             base = re2.split(r\"__\\d+$\", self._group_id)[0]",
          "172:                 int(re2.split(r\"^.+__\", used_group_id)[1])",
          "174:                 if used_group_id is not None and re2.match(rf\"^{base}__\\d+$\", used_group_id)",
          "",
          "---------------"
        ],
        "airflow/www/fab_security/manager.py||airflow/www/fab_security/manager.py": [
          "File: airflow/www/fab_security/manager.py -> airflow/www/fab_security/manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import datetime",
          "23: import json",
          "24: import logging",
          "26: from functools import cached_property",
          "27: from typing import Any",
          "28: from uuid import uuid4",
          "30: from flask import Flask, current_app, g, session, url_for",
          "31: from flask_appbuilder import AppBuilder",
          "32: from flask_appbuilder.const import (",
          "",
          "[Removed Lines]",
          "25: import re",
          "",
          "[Added Lines]",
          "29: import re2",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "704:     def _azure_parse_jwt(self, id_token):",
          "705:         jwt_token_parts = r\"^([^\\.\\s]*)\\.([^\\.\\s]+)\\.([^\\.\\s]*)$\"",
          "707:         if not matches or len(matches.groups()) < 3:",
          "708:             log.error(\"Unable to parse token.\")",
          "709:             return {}",
          "",
          "[Removed Lines]",
          "706:         matches = re.search(jwt_token_parts, id_token)",
          "",
          "[Added Lines]",
          "706:         matches = re2.search(jwt_token_parts, id_token)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1375:     def _has_access_builtin_roles(self, role, action_name: str, resource_name: str) -> bool:",
          "1376:         \"\"\"Checks permission on builtin role.\"\"\"",
          "1377:         perms = self.builtin_roles.get(role.name, [])",
          "1380:                 return True",
          "1381:         return False",
          "",
          "[Removed Lines]",
          "1378:         for (_resource_name, _action_name) in perms:",
          "1379:             if re.match(_resource_name, resource_name) and re.match(_action_name, action_name):",
          "",
          "[Added Lines]",
          "1378:         for _resource_name, _action_name in perms:",
          "1379:             if re2.match(_resource_name, resource_name) and re2.match(_action_name, action_name):",
          "",
          "---------------"
        ],
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import json",
          "25: import logging",
          "26: import math",
          "28: import sys",
          "29: import traceback",
          "30: import warnings",
          "",
          "[Removed Lines]",
          "27: import re",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39: import flask.json",
          "40: import lazy_object_proxy",
          "41: import nvd3",
          "42: import sqlalchemy as sqla",
          "43: from croniter import croniter",
          "44: from flask import (",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "41: import re2",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2105:             return redirect(origin)",
          "2107:         regex = conf.get(\"scheduler\", \"allowed_run_id_pattern\")",
          "2110:                 flash(",
          "2111:                     f\"The provided run ID '{run_id}' is invalid. It does not match either \"",
          "2112:                     f\"the configured pattern: '{regex}' or the built-in pattern: '{RUN_ID_REGEX}'\",",
          "",
          "[Removed Lines]",
          "2108:         if run_id and not re.match(RUN_ID_REGEX, run_id):",
          "2109:             if not regex.strip() or not re.match(regex.strip(), run_id):",
          "",
          "[Added Lines]",
          "2108:         if run_id and not re2.match(RUN_ID_REGEX, run_id):",
          "2109:             if not regex.strip() or not re2.match(regex.strip(), run_id):",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "4743:         \"\"\"Duplicate Multiple connections.\"\"\"",
          "4744:         for selected_conn in connections:",
          "4745:             new_conn_id = selected_conn.conn_id",
          "4748:             base_conn_id = selected_conn.conn_id",
          "4749:             if match:",
          "",
          "[Removed Lines]",
          "4746:             match = re.search(r\"_copy(\\d+)$\", selected_conn.conn_id)",
          "",
          "[Added Lines]",
          "4746:             match = re2.search(r\"_copy(\\d+)$\", selected_conn.conn_id)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "5002:             return Markup(f'<a href=\"{url}\">{text}</a>')",
          "5004:         cd = escape(description)",
          "5007:         return Markup(cd)",
          "",
          "[Removed Lines]",
          "5005:         cd = re.sub(r\"`(.*)[\\s+]+&lt;(.*)&gt;`__\", _build_link, cd)",
          "5006:         cd = re.sub(r\"\\n\", r\"<br>\", cd)",
          "",
          "[Added Lines]",
          "5005:         cd = re2.sub(r\"`(.*)[\\s+]+&lt;(.*)&gt;`__\", _build_link, cd)",
          "5006:         cd = re2.sub(r\"\\n\", r\"<br>\", cd)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/pre_commit_ids.py||dev/breeze/src/airflow_breeze/pre_commit_ids.py": [
          "File: dev/breeze/src/airflow_breeze/pre_commit_ids.py -> dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "71:     \"check-system-tests-tocs\",",
          "72:     \"check-tests-unittest-testcase\",",
          "73:     \"check-urlparse-usage-in-code\",",
          "74:     \"check-xml\",",
          "75:     \"codespell\",",
          "76:     \"compile-www-assets\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "74:     \"check-usage-of-re2-over-re\",",
          "",
          "---------------"
        ],
        "kubernetes_tests/test_base.py||kubernetes_tests/test_base.py": [
          "File: kubernetes_tests/test_base.py -> kubernetes_tests/test_base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: from __future__ import annotations",
          "19: import os",
          "21: import subprocess",
          "22: import tempfile",
          "23: import time",
          "",
          "[Removed Lines]",
          "20: import re",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "26: from subprocess import check_call, check_output",
          "28: import pytest",
          "29: import requests",
          "30: import requests.exceptions",
          "31: from requests.adapters import HTTPAdapter",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "28: import re2",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "103:     def _num_pods_in_namespace(namespace):",
          "104:         air_pod = check_output([\"kubectl\", \"get\", \"pods\", \"-n\", namespace]).decode()",
          "105:         air_pod = air_pod.split(\"\\n\")",
          "107:         return len(names)",
          "109:     @staticmethod",
          "",
          "[Removed Lines]",
          "106:         names = [re.compile(r\"\\s+\").split(x)[0] for x in air_pod if \"airflow\" in x]",
          "",
          "[Added Lines]",
          "106:         names = [re2.compile(r\"\\s+\").split(x)[0] for x in air_pod if \"airflow\" in x]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "111:         suffix = \"-\" + name if name else \"\"",
          "112:         air_pod = check_output([\"kubectl\", \"get\", \"pods\"]).decode()",
          "113:         air_pod = air_pod.split(\"\\n\")",
          "115:         if names:",
          "116:             check_call([\"kubectl\", \"delete\", \"pod\", names[0]])",
          "",
          "[Removed Lines]",
          "114:         names = [re.compile(r\"\\s+\").split(x)[0] for x in air_pod if \"airflow\" + suffix in x]",
          "",
          "[Added Lines]",
          "114:         names = [re2.compile(r\"\\s+\").split(x)[0] for x in air_pod if \"airflow\" + suffix in x]",
          "",
          "---------------"
        ],
        "tests/utils/log/test_secrets_masker.py||tests/utils/log/test_secrets_masker.py": [
          "File: tests/utils/log/test_secrets_masker.py -> tests/utils/log/test_secrets_masker.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "393:     def reset_secrets_masker_and_skip_escape(self):",
          "394:         self.secrets_masker = SecretsMasker()",
          "395:         with patch(\"airflow.utils.log.secrets_masker._secrets_masker\", return_value=self.secrets_masker):",
          "397:                 yield",
          "399:     def test_calling_mask_secret_adds_adaptations_for_returned_str(self):",
          "",
          "[Removed Lines]",
          "396:             with patch(\"airflow.utils.log.secrets_masker.re.escape\", lambda x: x):",
          "",
          "[Added Lines]",
          "396:             with patch(\"airflow.utils.log.secrets_masker.re2.escape\", lambda x: x):",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4358511fce2eab6d3f97edb1c95d09f4f5a49376",
      "candidate_info": {
        "commit_hash": "4358511fce2eab6d3f97edb1c95d09f4f5a49376",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4358511fce2eab6d3f97edb1c95d09f4f5a49376",
        "files": [
          "airflow/exceptions.py",
          "airflow/settings.py",
          "tests/models/test_dagrun.py",
          "tests/providers/amazon/aws/auth_manager/avp/test_facade.py",
          "tests/providers/amazon/aws/fs/test_s3.py",
          "tests/providers/amazon/aws/hooks/test_base_aws.py",
          "tests/providers/amazon/aws/hooks/test_emr.py",
          "tests/providers/amazon/aws/operators/test_ecs.py",
          "tests/providers/amazon/conftest.py",
          "tests/providers/apache/hive/transfers/test_s3_to_hive.py",
          "tests/providers/cncf/kubernetes/operators/test_job.py",
          "tests/providers/cncf/kubernetes/operators/test_resource.py",
          "tests/providers/google/cloud/operators/test_datapipeline.py",
          "tests/providers/google/cloud/operators/test_kubernetes_engine.py",
          "tests/providers/google/cloud/operators/test_vertex_ai.py",
          "tests/providers/mysql/hooks/test_mysql.py",
          "tests/providers/samba/hooks/test_samba.py",
          "tests/providers/slack/hooks/test_slack_webhook.py",
          "tests/serialization/test_dag_serialization.py",
          "tests/utils/test_cli_util.py",
          "tests/utils/test_email.py"
        ],
        "message": "Add `AirflowInternalRuntimeError` for raise \"non catchable\" errors (#38778)\n\n* Add `AirflowInternalRuntimeError` for raise non catchable errors\n\n* Fixup non-db tests or move it as db tests\n\n* Fixup mysql test",
        "before_after_code_files": [
          "airflow/exceptions.py||airflow/exceptions.py",
          "airflow/settings.py||airflow/settings.py",
          "tests/models/test_dagrun.py||tests/models/test_dagrun.py",
          "tests/providers/amazon/aws/auth_manager/avp/test_facade.py||tests/providers/amazon/aws/auth_manager/avp/test_facade.py",
          "tests/providers/amazon/aws/fs/test_s3.py||tests/providers/amazon/aws/fs/test_s3.py",
          "tests/providers/amazon/aws/hooks/test_base_aws.py||tests/providers/amazon/aws/hooks/test_base_aws.py",
          "tests/providers/amazon/aws/hooks/test_emr.py||tests/providers/amazon/aws/hooks/test_emr.py",
          "tests/providers/amazon/aws/operators/test_ecs.py||tests/providers/amazon/aws/operators/test_ecs.py",
          "tests/providers/amazon/conftest.py||tests/providers/amazon/conftest.py",
          "tests/providers/apache/hive/transfers/test_s3_to_hive.py||tests/providers/apache/hive/transfers/test_s3_to_hive.py",
          "tests/providers/cncf/kubernetes/operators/test_job.py||tests/providers/cncf/kubernetes/operators/test_job.py",
          "tests/providers/cncf/kubernetes/operators/test_resource.py||tests/providers/cncf/kubernetes/operators/test_resource.py",
          "tests/providers/google/cloud/operators/test_datapipeline.py||tests/providers/google/cloud/operators/test_datapipeline.py",
          "tests/providers/google/cloud/operators/test_kubernetes_engine.py||tests/providers/google/cloud/operators/test_kubernetes_engine.py",
          "tests/providers/google/cloud/operators/test_vertex_ai.py||tests/providers/google/cloud/operators/test_vertex_ai.py",
          "tests/providers/mysql/hooks/test_mysql.py||tests/providers/mysql/hooks/test_mysql.py",
          "tests/providers/sambhooks/test_samba.py||tests/providers/samba/hooks/test_samba.py",
          "tests/providers/slack/hooks/test_slack_webhook.py||tests/providers/slack/hooks/test_slack_webhook.py",
          "tests/serialization/test_dag_serialization.py||tests/serialization/test_dag_serialization.py",
          "tests/utils/test_cli_util.py||tests/utils/test_cli_util.py",
          "tests/utils/test_email.py||tests/utils/test_email.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "tests/models/test_dagrun.py||tests/models/test_dagrun.py"
          ],
          "candidate": [
            "tests/models/test_dagrun.py||tests/models/test_dagrun.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/exceptions.py||airflow/exceptions.py": [
          "File: airflow/exceptions.py -> airflow/exceptions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "107:     \"\"\"Raise by providers when imports are missing for optional provider features.\"\"\"",
          "110: class XComNotFound(AirflowException):",
          "111:     \"\"\"Raise when an XCom reference is being resolved against a non-existent XCom.\"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "110: class AirflowInternalRuntimeError(BaseException):",
          "111:     \"\"\"",
          "112:     Airflow Internal runtime error.",
          "114:     Indicates that something really terrible happens during the Airflow execution.",
          "116:     :meta private:",
          "117:     \"\"\"",
          "",
          "---------------"
        ],
        "airflow/settings.py||airflow/settings.py": [
          "File: airflow/settings.py -> airflow/settings.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: from airflow import policies",
          "35: from airflow.configuration import AIRFLOW_HOME, WEBSERVER_CONFIG, conf  # noqa: F401",
          "37: from airflow.executors import executor_constants",
          "38: from airflow.logging_config import configure_logging",
          "39: from airflow.utils.orm_event_handlers import setup_event_handlers",
          "",
          "[Removed Lines]",
          "36: from airflow.exceptions import RemovedInAirflow3Warning",
          "",
          "[Added Lines]",
          "36: from airflow.exceptions import AirflowInternalRuntimeError, RemovedInAirflow3Warning",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "210:     \"\"\"This fake session is used to skip DB tests when `_AIRFLOW_SKIP_DB_TESTS` is set.\"\"\"",
          "212:     def __init__(self):",
          "214:             \"Your test accessed the DB but `_AIRFLOW_SKIP_DB_TESTS` is set.\\n\"",
          "215:             \"Either make sure your test does not use database or mark the test with `@pytest.mark.db_test`\\n\"",
          "216:             \"See https://github.com/apache/airflow/blob/main/contributing-docs/testing/unit_tests.rst#\"",
          "",
          "[Removed Lines]",
          "213:         raise RuntimeError(",
          "",
          "[Added Lines]",
          "213:         raise AirflowInternalRuntimeError(",
          "",
          "---------------"
        ],
        "tests/models/test_dagrun.py||tests/models/test_dagrun.py": [
          "File: tests/models/test_dagrun.py -> tests/models/test_dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import datetime",
          "21: from functools import reduce",
          "22: from typing import TYPE_CHECKING, Mapping",
          "23: from unittest import mock",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import warnings",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "29: from airflow import settings",
          "30: from airflow.callbacks.callback_requests import DagCallbackRequest",
          "31: from airflow.decorators import setup, task, task_group, teardown",
          "33: from airflow.models.baseoperator import BaseOperator",
          "34: from airflow.models.dag import DAG, DagModel",
          "36: from airflow.models.dagrun import DagRun, DagRunNote",
          "37: from airflow.models.taskinstance import TaskInstance, TaskInstanceNote, clear_task_instances",
          "38: from airflow.models.taskmap import TaskMap",
          "",
          "[Removed Lines]",
          "32: from airflow.exceptions import AirflowException",
          "35: from airflow.models.dagbag import DagBag",
          "",
          "[Added Lines]",
          "33: from airflow.exceptions import AirflowException, RemovedInAirflow3Warning",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "60: DEFAULT_DATE = pendulum.instance(_DEFAULT_DATE)",
          "63: class TestDagRun:",
          "66:     @staticmethod",
          "68:         db.clear_db_runs()",
          "69:         db.clear_db_pools()",
          "70:         db.clear_db_dags()",
          "",
          "[Removed Lines]",
          "64:     dagbag = DagBag(include_examples=True)",
          "67:     def clean_db():",
          "",
          "[Added Lines]",
          "63: @pytest.fixture(scope=\"module\")",
          "64: def dagbag():",
          "65:     from airflow.models.dagbag import DagBag",
          "67:     with warnings.catch_warnings():",
          "68:         # Some dags use deprecated operators, e.g SubDagOperator",
          "69:         # if it is not imported, then it might have side effects for the other tests",
          "70:         warnings.simplefilter(\"ignore\", category=RemovedInAirflow3Warning)",
          "71:         # Ensure the DAGs we are looking at from the DB are up-to-date",
          "72:         dag_bag = DagBag(include_examples=True)",
          "73:     return dag_bag",
          "77:     @pytest.fixture(autouse=True)",
          "78:     def setup_test_cases(self):",
          "79:         self._clean_db()",
          "80:         yield",
          "81:         self._clean_db()",
          "84:     def _clean_db():",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "73:         db.clear_db_xcom()",
          "74:         db.clear_db_task_fail()",
          "82:     def create_dag_run(",
          "83:         self,",
          "84:         dag: DAG,",
          "",
          "[Removed Lines]",
          "76:     def setup_class(self) -> None:",
          "77:         self.clean_db()",
          "79:     def teardown_method(self) -> None:",
          "80:         self.clean_db()",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "741:             (None, False),",
          "742:         ],",
          "743:     )",
          "745:         dag_id = \"test_depends_on_past\"",
          "748:         task = dag.tasks[0]",
          "750:         dag_run_1 = self.create_dag_run(",
          "",
          "[Removed Lines]",
          "744:     def test_depends_on_past(self, session, prev_ti_state, is_ti_success):",
          "747:         dag = self.dagbag.get_dag(dag_id)",
          "",
          "[Added Lines]",
          "755:     def test_depends_on_past(self, dagbag, session, prev_ti_state, is_ti_success):",
          "758:         dag = dagbag.get_dag(dag_id)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "778:             (None, False),",
          "779:         ],",
          "780:     )",
          "782:         dag_id = \"test_wait_for_downstream\"",
          "784:         upstream, downstream = dag.tasks",
          "786:         # For ti.set_state() to work, the DagRun has to exist,",
          "",
          "[Removed Lines]",
          "781:     def test_wait_for_downstream(self, session, prev_ti_state, is_ti_success):",
          "783:         dag = self.dagbag.get_dag(dag_id)",
          "",
          "[Added Lines]",
          "792:     def test_wait_for_downstream(self, dagbag, session, prev_ti_state, is_ti_success):",
          "794:         dag = dagbag.get_dag(dag_id)",
          "",
          "---------------"
        ],
        "tests/providers/amazon/aws/auth_manager/avp/test_facade.py||tests/providers/amazon/aws/auth_manager/avp/test_facade.py": [
          "File: tests/providers/amazon/aws/auth_manager/avp/test_facade.py -> tests/providers/amazon/aws/auth_manager/avp/test_facade.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "44: def facade():",
          "45:     with conf_vars(",
          "46:         {",
          "47:             (\"aws_auth_manager\", \"region_name\"): REGION_NAME,",
          "48:             (\"aws_auth_manager\", \"avp_policy_store_id\"): AVP_POLICY_STORE_ID,",
          "49:         }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "47:             (\"aws_auth_manager\", \"conn_id\"): \"aws_default\",",
          "",
          "---------------"
        ],
        "tests/providers/amazon/aws/fs/test_s3.py||tests/providers/amazon/aws/fs/test_s3.py": [
          "File: tests/providers/amazon/aws/fs/test_s3.py -> tests/providers/amazon/aws/fs/test_s3.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "35: TEST_REQ_URI = \"s3://bucket/key\"",
          "38: class TestFilesystem:",
          "39:     def test_get_s3fs(self):",
          "40:         import s3fs",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "38: @pytest.fixture(scope=\"module\", autouse=True)",
          "39: def _setup_connections():",
          "40:     with pytest.MonkeyPatch.context() as mp_ctx:",
          "41:         mp_ctx.setenv(f\"AIRFLOW_CONN_{TEST_CONN}\".upper(), \"aws://\")",
          "42:         yield",
          "",
          "---------------"
        ],
        "tests/providers/amazon/aws/hooks/test_base_aws.py||tests/providers/amazon/aws/hooks/test_base_aws.py": [
          "File: tests/providers/amazon/aws/hooks/test_base_aws.py -> tests/providers/amazon/aws/hooks/test_base_aws.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "427:         assert mock_class_name.call_count == len(found_classes)",
          "428:         assert user_agent_tags[\"Caller\"] == found_classes[-1]",
          "430:     @mock.patch.object(AwsEcsExecutor, \"_load_run_kwargs\")",
          "431:     def test_user_agent_caller_target_executor_found(self, mock_load_run_kwargs):",
          "432:         with conf_vars(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "430:     @pytest.mark.db_test",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "450:         assert user_agent_tags[\"Caller\"] == default_caller_name",
          "452:     @pytest.mark.parametrize(\"env_var, expected_version\", [({\"AIRFLOW_CTX_DAG_ID\": \"banana\"}, 5), [{}, None]])",
          "453:     @mock.patch.object(AwsBaseHook, \"_get_caller\", return_value=\"Test\")",
          "454:     def test_user_agent_dag_run_key_is_hashed_correctly(self, _, env_var, expected_version):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "453:     @pytest.mark.db_test",
          "",
          "---------------"
        ],
        "tests/providers/amazon/aws/hooks/test_emr.py||tests/providers/amazon/aws/hooks/test_emr.py": [
          "File: tests/providers/amazon/aws/hooks/test_emr.py -> tests/providers/amazon/aws/hooks/test_emr.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "175:             if sys.version_info >= (3, 12):",
          "176:                 # Botocore generates deprecation warning on Python 3.12 connected with utcnow use",
          "177:                 warnings.filterwarnings(\"ignore\", message=r\".*datetime.utcnow.*\", category=DeprecationWarning)",
          "179:         cluster = client.describe_cluster(ClusterId=cluster[\"JobFlowId\"])[\"Cluster\"]",
          "181:         # The AmiVersion comes back as {Requested,Running}AmiVersion fields.",
          "",
          "[Removed Lines]",
          "178:             cluster = hook.create_job_flow({\"Name\": \"test_cluster\", \"ReleaseLabel\": \"\", \"AmiVersion\": \"3.2\"})",
          "",
          "[Added Lines]",
          "178:             cluster = hook.create_job_flow(",
          "179:                 {\"Name\": \"test_cluster\", \"ReleaseLabel\": \"\", \"AmiVersion\": \"3.2\", \"Instances\": {}}",
          "180:             )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "192:         hook.create_job_flow(job_flow_overrides)",
          "193:         mock_run_job_flow.assert_called_once_with(**job_flow_overrides)",
          "195:     @mock.patch(\"airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook.get_conn\")",
          "196:     def test_missing_emr_conn_id(self, mock_boto3_client):",
          "197:         \"\"\"Test not exists ``emr_conn_id``.\"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "197:     @pytest.mark.db_test",
          "",
          "---------------"
        ],
        "tests/providers/amazon/aws/operators/test_ecs.py||tests/providers/amazon/aws/operators/test_ecs.py": [
          "File: tests/providers/amazon/aws/operators/test_ecs.py -> tests/providers/amazon/aws/operators/test_ecs.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "99:     def setup_test_cases(self, monkeypatch):",
          "100:         self.client = boto3.client(\"ecs\", region_name=\"eu-west-3\")",
          "101:         monkeypatch.setattr(EcsHook, \"conn\", self.client)",
          "104: class TestEcsBaseOperator(EcsBaseTestCase):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "102:         monkeypatch.setenv(\"AIRFLOW_CONN_AWS_TEST_CONN\", '{\"conn_type\": \"aws\"}')",
          "",
          "---------------"
        ],
        "tests/providers/amazon/conftest.py||tests/providers/amazon/conftest.py": [
          "File: tests/providers/amazon/conftest.py -> tests/providers/amazon/conftest.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "102:             monkeypatch.delenv(env_name, raising=False)",
          "103:     for env_name, value in aws_testing_env_vars.items():",
          "104:         monkeypatch.setenv(env_name, value)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "107: @pytest.fixture(scope=\"package\", autouse=True)",
          "108: def setup_default_aws_connections():",
          "109:     with pytest.MonkeyPatch.context() as mp_ctx:",
          "110:         mp_ctx.setenv(\"AIRFLOW_CONN_AWS_DEFAULT\", '{\"conn_type\": \"aws\"}')",
          "111:         mp_ctx.setenv(\"AIRFLOW_CONN_EMR_DEFAULT\", '{\"conn_type\": \"emr\", \"extra\": {}}')",
          "112:         yield",
          "",
          "---------------"
        ],
        "tests/providers/apache/hive/transfers/test_s3_to_hive.py||tests/providers/apache/hive/transfers/test_s3_to_hive.py": [
          "File: tests/providers/apache/hive/transfers/test_s3_to_hive.py -> tests/providers/apache/hive/transfers/test_s3_to_hive.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "37: logger = logging.getLogger(__name__)",
          "40: class TestS3ToHiveTransfer:",
          "41:     @pytest.fixture(autouse=True)",
          "42:     def setup_attrs(self):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "40: @pytest.mark.db_test",
          "",
          "---------------"
        ],
        "tests/providers/cncf/kubernetes/operators/test_job.py||tests/providers/cncf/kubernetes/operators/test_job.py": [
          "File: tests/providers/cncf/kubernetes/operators/test_job.py -> tests/providers/cncf/kubernetes/operators/test_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "78:     }",
          "81: @pytest.mark.execution_timeout(300)",
          "82: class TestKubernetesJobOperator:",
          "83:     @pytest.fixture(autouse=True)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "81: @pytest.mark.db_test",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "90:         patch.stopall()",
          "93:     def test_templates(self, create_task_instance_of_operator):",
          "94:         dag_id = \"TestKubernetesJobOperator\"",
          "95:         ti = create_task_instance_of_operator(",
          "",
          "[Removed Lines]",
          "92:     @pytest.mark.db_test",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "465:         job = k.build_job_request_obj({})",
          "466:         assert re.match(r\"job-a-very-reasonable-task-name-[a-z0-9-]+\", job.metadata.name) is not None",
          "468:     @patch(JOB_OPERATORS_PATH.format(\"KubernetesJobOperator.build_job_request_obj\"))",
          "469:     @patch(JOB_OPERATORS_PATH.format(\"KubernetesJobOperator.create_job\"))",
          "470:     @patch(HOOK_CLASS)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "468:     @pytest.mark.non_db_test_override",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "496:         assert execute_result is None",
          "497:         assert not mock_hook.wait_until_job_complete.called",
          "499:     @patch(JOB_OPERATORS_PATH.format(\"KubernetesJobOperator.build_job_request_obj\"))",
          "500:     @patch(JOB_OPERATORS_PATH.format(\"KubernetesJobOperator.create_job\"))",
          "501:     @patch(JOB_OPERATORS_PATH.format(\"KubernetesJobOperator.execute_deferrable\"))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "500:     @pytest.mark.non_db_test_override",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "531:         assert actual_result is None",
          "532:         assert not mock_hook.wait_until_job_complete.called",
          "534:     @patch(JOB_OPERATORS_PATH.format(\"KubernetesJobOperator.build_job_request_obj\"))",
          "535:     @patch(JOB_OPERATORS_PATH.format(\"KubernetesJobOperator.create_job\"))",
          "536:     @patch(HOOK_CLASS)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "536:     @pytest.mark.non_db_test_override",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "545:         with pytest.raises(AirflowException):",
          "546:             op.execute(context=dict(ti=mock.MagicMock()))",
          "548:     @patch(JOB_OPERATORS_PATH.format(\"KubernetesJobOperator.defer\"))",
          "549:     @patch(JOB_OPERATORS_PATH.format(\"KubernetesJobTrigger\"))",
          "550:     def test_execute_deferrable(self, mock_trigger, mock_execute_deferrable):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "551:     @pytest.mark.non_db_test_override",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "587:         )",
          "588:         assert actual_result is None",
          "590:     @patch(JOB_OPERATORS_PATH.format(\"KubernetesJobOperator.build_job_request_obj\"))",
          "591:     @patch(JOB_OPERATORS_PATH.format(\"KubernetesJobOperator.create_job\"))",
          "592:     @patch(f\"{HOOK_CLASS}.wait_until_job_complete\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "594:     @pytest.mark.non_db_test_override",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "609:             job_poll_interval=POLL_INTERVAL,",
          "610:         )",
          "612:     def test_execute_complete(self):",
          "613:         mock_ti = mock.MagicMock()",
          "614:         context = {\"ti\": mock_ti}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "617:     @pytest.mark.non_db_test_override",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "620:         mock_ti.xcom_push.assert_called_once_with(key=\"job\", value=mock_job)",
          "622:     def test_execute_complete_fail(self):",
          "623:         mock_ti = mock.MagicMock()",
          "624:         context = {\"ti\": mock_ti}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "628:     @pytest.mark.non_db_test_override",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "631:         mock_ti.xcom_push.assert_called_once_with(key=\"job\", value=mock_job)",
          "633:     @patch(JOB_OPERATORS_PATH.format(\"KubernetesJobOperator.client\"))",
          "634:     @patch(HOOK_CLASS)",
          "635:     def test_on_kill(self, mock_hook, mock_client):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "640:     @pytest.mark.non_db_test_override",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "650:         )",
          "651:         mock_serialize.assert_called_once_with(mock_job)",
          "653:     @patch(JOB_OPERATORS_PATH.format(\"KubernetesJobOperator.client\"))",
          "654:     @patch(HOOK_CLASS)",
          "655:     def test_on_kill_termination_grace_period(self, mock_hook, mock_client):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "661:     @pytest.mark.non_db_test_override",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "674:         )",
          "675:         mock_serialize.assert_called_once_with(mock_job)",
          "677:     @patch(JOB_OPERATORS_PATH.format(\"KubernetesJobOperator.client\"))",
          "678:     @patch(HOOK_CLASS)",
          "679:     def test_on_kill_none_job(self, mock_hook, mock_client):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "686:     @pytest.mark.non_db_test_override",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "686:         mock_serialize.assert_not_called()",
          "689: @pytest.mark.execution_timeout(300)",
          "690: class TestKubernetesDeleteJobOperator:",
          "691:     @pytest.fixture(autouse=True)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "699: @pytest.mark.db_test",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "825:         patch.stopall()",
          "827:     @patch(\"kubernetes.config.load_kube_config\")",
          "828:     @patch(\"kubernetes.client.api.BatchV1Api.patch_namespaced_job\")",
          "829:     def test_update_execute(self, mock_patch_namespaced_job, mock_load_kube_config):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "838:     @pytest.mark.db_test",
          "",
          "---------------"
        ],
        "tests/providers/cncf/kubernetes/operators/test_resource.py||tests/providers/cncf/kubernetes/operators/test_resource.py": [
          "File: tests/providers/cncf/kubernetes/operators/test_resource.py -> tests/providers/cncf/kubernetes/operators/test_resource.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "76: HOOK_CLASS = \"airflow.providers.cncf.kubernetes.hooks.kubernetes.KubernetesHook\"",
          "79: @patch(\"airflow.utils.context.Context\")",
          "80: class TestKubernetesXResourceOperator:",
          "81:     @pytest.fixture(autouse=True)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "79: @pytest.mark.db_test",
          "",
          "---------------"
        ],
        "tests/providers/google/cloud/operators/test_datapipeline.py||tests/providers/google/cloud/operators/test_datapipeline.py": [
          "File: tests/providers/google/cloud/operators/test_datapipeline.py -> tests/providers/google/cloud/operators/test_datapipeline.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "82:             project_id=TEST_PROJECTID, body=TEST_BODY, location=TEST_LOCATION",
          "83:         )",
          "85:     def test_body_invalid(self):",
          "86:         \"\"\"",
          "87:         Test that if the operator is not passed a Request Body, an AirflowException is raised",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "85:     @pytest.mark.db_test",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "124:         with pytest.raises(AirflowException):",
          "125:             CreateDataPipelineOperator(**init_kwargs).execute(mock.MagicMock())",
          "127:     def test_response_invalid(self):",
          "128:         \"\"\"",
          "129:         Test that if the Response Body contains an error message, an AirflowException is raised",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "128:     @pytest.mark.db_test",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "139:             CreateDataPipelineOperator(**init_kwargs).execute(mock.MagicMock())",
          "142: class TestRunDataPipelineOperator:",
          "143:     @pytest.fixture",
          "144:     def run_operator(self):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "144: @pytest.mark.db_test",
          "",
          "---------------"
        ],
        "tests/providers/google/cloud/operators/test_kubernetes_engine.py||tests/providers/google/cloud/operators/test_kubernetes_engine.py": [
          "File: tests/providers/google/cloud/operators/test_kubernetes_engine.py -> tests/providers/google/cloud/operators/test_kubernetes_engine.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "493:         self.gke_op._cluster_url = CLUSTER_URL",
          "494:         self.gke_op._ssl_ca_cert = SSL_CA_CERT",
          "496:     @mock.patch.dict(os.environ, {})",
          "497:     @mock.patch(TEMP_FILE)",
          "498:     @mock.patch(f\"{GKE_CLUSTER_AUTH_DETAILS_PATH}.fetch_cluster_info\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "496:     @pytest.mark.db_test",
          "",
          "---------------"
        ],
        "tests/providers/google/cloud/operators/test_vertex_ai.py||tests/providers/google/cloud/operators/test_vertex_ai.py": [
          "File: tests/providers/google/cloud/operators/test_vertex_ai.py -> tests/providers/google/cloud/operators/test_vertex_ai.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1568:         op.execute(context={\"ti\": mock.MagicMock()})",
          "1569:         mock_defer.assert_called_once()",
          "1571:     def test_deferrable_sync_error(self):",
          "1572:         op = CreateHyperparameterTuningJobOperator(",
          "1573:             task_id=TASK_ID,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1571:     @pytest.mark.db_test",
          "",
          "---------------"
        ],
        "tests/providers/mysql/hooks/test_mysql.py||tests/providers/mysql/hooks/test_mysql.py": [
          "File: tests/providers/mysql/hooks/test_mysql.py -> tests/providers/mysql/hooks/test_mysql.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "169:     @mock.patch(\"MySQLdb.connect\")",
          "170:     @mock.patch(\"airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook.get_client_type\")",
          "173:         mock_client.return_value.generate_db_auth_token.return_value = \"aws_token\"",
          "174:         self.db_hook.get_conn()",
          "175:         mock_connect.assert_called_once_with(",
          "",
          "[Removed Lines]",
          "171:     def test_get_conn_rds_iam(self, mock_client, mock_connect):",
          "172:         self.connection.extra = '{\"iam\":true}'",
          "",
          "[Added Lines]",
          "171:     def test_get_conn_rds_iam(self, mock_client, mock_connect, monkeypatch):",
          "172:         monkeypatch.setenv(\"AIRFLOW_CONN_TEST_AWS_IAM_CONN\", '{\"conn_type\": \"aws\"}')",
          "173:         self.connection.extra = '{\"iam\":true, \"aws_conn_id\": \"test_aws_iam_conn\"}'",
          "",
          "---------------"
        ],
        "tests/providers/sambhooks/test_samba.py||tests/providers/samba/hooks/test_samba.py": [
          "File: tests/providers/sambhooks/test_samba.py -> tests/providers/samba/hooks/test_samba.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: from airflow.exceptions import AirflowNotFoundException",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "40:     @pytest.mark.db_test",
          "42:         with pytest.raises(AirflowNotFoundException):",
          "43:             SambaHook(\"non-existed-connection-id\")",
          "",
          "---------------"
        ],
        "tests/providers/slack/hooks/test_slack_webhook.py||tests/providers/slack/hooks/test_slack_webhook.py": [
          "File: tests/providers/slack/hooks/test_slack_webhook.py -> tests/providers/slack/hooks/test_slack_webhook.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "123:             schema=\"http\",",
          "124:             host=\"some.netloc\",",
          "125:         ),",
          "126:     ]",
          "127:     with pytest.MonkeyPatch.context() as mp:",
          "128:         for conn in connections:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "126:         # Not supported anymore",
          "127:         Connection(conn_id=\"conn_token_in_host_1\", conn_type=CONN_TYPE, host=TEST_WEBHOOK_URL),",
          "128:         Connection(",
          "129:             conn_id=\"conn_token_in_host_2\",",
          "130:             conn_type=CONN_TYPE,",
          "131:             schema=\"https\",",
          "132:             host=\"hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX\",",
          "133:         ),",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "197:     @pytest.mark.parametrize(\"conn_id\", [\"conn_token_in_host_1\", \"conn_token_in_host_2\"])",
          "198:     def test_wrong_connections(self, conn_id):",
          "200:         hook = SlackWebhookHook(slack_webhook_conn_id=conn_id)",
          "202:             hook._get_conn_params()",
          "204:     @pytest.mark.parametrize(",
          "",
          "[Removed Lines]",
          "199:         \"\"\"Test previously valid connections, but now it is dropped.\"\"\"",
          "201:         with pytest.raises(AirflowNotFoundException):",
          "",
          "[Added Lines]",
          "207:         \"\"\"Test previously valid connections, but now support of it is dropped.\"\"\"",
          "209:         with pytest.raises(AirflowNotFoundException, match=\"does not contain password\"):",
          "",
          "---------------"
        ],
        "tests/serialization/test_dag_serialization.py||tests/serialization/test_dag_serialization.py": [
          "File: tests/serialization/test_dag_serialization.py -> tests/serialization/test_dag_serialization.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "384: class TestStringifiedDAGs:",
          "385:     \"\"\"Unit tests for stringified DAGs.\"\"\"",
          "391:                 extra=(",
          "392:                     \"{\"",
          "393:                     '\"project_id\": \"mock\", '",
          "",
          "[Removed Lines]",
          "387:     def setup_method(self):",
          "388:         self.backup_base_hook_get_connection = BaseHook.get_connection",
          "389:         BaseHook.get_connection = mock.Mock(",
          "390:             return_value=Connection(",
          "",
          "[Added Lines]",
          "387:     @pytest.fixture(autouse=True)",
          "388:     def setup_test_cases(self):",
          "389:         with mock.patch.object(BaseHook, \"get_connection\") as m:",
          "390:             m.return_value = Connection(",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "399:                     \"}\"",
          "400:                 )",
          "401:             )",
          "408:     @pytest.mark.db_test",
          "409:     def test_serialization(self):",
          "",
          "[Removed Lines]",
          "402:         )",
          "403:         self.maxDiff = None",
          "405:     def teardown_method(self):",
          "406:         BaseHook.get_connection = self.backup_base_hook_get_connection",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "422:         )",
          "423:         assert actual == expected",
          "425:     @pytest.mark.parametrize(",
          "426:         \"timetable, serialized_timetable\",",
          "427:         [",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "420:     @pytest.mark.db_test",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "458:                 print(task[\"task_id\"], k, v)",
          "459:         assert actual == expected",
          "461:     def test_dag_serialization_preserves_empty_access_roles(self):",
          "462:         \"\"\"Verify that an explicitly empty access_control dict is preserved.\"\"\"",
          "463:         dag = collect_dags([\"airflow/example_dags\"])[\"simple_dag\"]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "457:     @pytest.mark.db_test",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "468:         assert serialized_dag[\"dag\"][\"_access_control\"] == {\"__type\": \"dict\", \"__var\": {}}",
          "470:     def test_dag_serialization_unregistered_custom_timetable(self):",
          "471:         \"\"\"Verify serialization fails without timetable registration.\"\"\"",
          "472:         dag = get_timetable_based_simple_dag(CustomSerializationTimetable(\"bar\"))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "467:     @pytest.mark.db_test",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "506:         expected = json.loads(json.dumps(sorted_serialized_dag(expected)))",
          "507:         return actual, expected",
          "509:     def test_deserialization_across_process(self):",
          "510:         \"\"\"A serialized DAG can be deserialized in another process.\"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "507:     @pytest.mark.db_test",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "533:         for dag_id in stringified_dags:",
          "534:             self.validate_deserialized_dag(stringified_dags[dag_id], dags[dag_id])",
          "536:     def test_roundtrip_provider_example_dags(self):",
          "537:         dags = collect_dags(",
          "538:             [",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "535:     @pytest.mark.db_test",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "546:             serialized_dag = SerializedDAG.from_json(SerializedDAG.to_json(dag))",
          "547:             self.validate_deserialized_dag(serialized_dag, dag)",
          "549:     @pytest.mark.parametrize(",
          "550:         \"timetable\",",
          "551:         [cron_timetable(\"0 0 * * *\"), CustomSerializationTimetable(\"foo\")],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "549:     @pytest.mark.db_test",
          "",
          "---------------"
        ],
        "tests/utils/test_cli_util.py||tests/utils/test_cli_util.py": [
          "File: tests/utils/test_cli_util.py -> tests/utils/test_cli_util.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "35: from airflow.utils import cli, cli_action_loggers, timezone",
          "36: from airflow.utils.cli import _search_for_dag_file, get_dag_by_pickle",
          "38: repo_root = Path(airflow.__file__).parent.parent",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "38: # Mark entire module as db_test because ``action_cli`` wrapper still could use DB on callbacks:",
          "39: # - ``cli_action_loggers.on_pre_execution``",
          "40: # - ``cli_action_loggers.on_post_execution``",
          "41: pytestmark = pytest.mark.db_test",
          "",
          "---------------"
        ],
        "tests/utils/test_email.py||tests/utils/test_email.py": [
          "File: tests/utils/test_email.py -> tests/utils/test_email.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "145:         assert msg[\"To\"] == \",\".join(recipients)",
          "148: class TestEmailSmtp:",
          "149:     @mock.patch(\"airflow.utils.email.send_mime_email\")",
          "150:     def test_send_smtp(self, mock_send_mime, tmp_path):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "148: @pytest.mark.db_test",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "87b08ad0840a11d8cd5c0b5043d3a341b1a8f258",
      "candidate_info": {
        "commit_hash": "87b08ad0840a11d8cd5c0b5043d3a341b1a8f258",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/87b08ad0840a11d8cd5c0b5043d3a341b1a8f258",
        "files": [
          "airflow/models/baseoperator.py",
          "airflow/models/dag.py",
          "airflow/models/dagbag.py",
          "airflow/models/dagcode.py",
          "airflow/models/dagrun.py",
          "airflow/models/dagwarning.py",
          "airflow/models/db_callback_request.py",
          "airflow/models/expandinput.py",
          "airflow/models/mappedoperator.py",
          "airflow/models/operator.py",
          "airflow/models/param.py",
          "airflow/models/pool.py",
          "airflow/models/renderedtifields.py",
          "airflow/models/serialized_dag.py",
          "airflow/models/skipmixin.py",
          "airflow/models/taskinstance.py",
          "airflow/models/taskmixin.py",
          "airflow/models/taskreschedule.py",
          "airflow/models/trigger.py",
          "airflow/models/variable.py",
          "airflow/models/xcom.py",
          "airflow/models/xcom_arg.py"
        ],
        "message": "Improve importing the module in Airflow models package (#33799)",
        "before_after_code_files": [
          "airflow/models/baseoperator.py||airflow/models/baseoperator.py",
          "airflow/models/dag.py||airflow/models/dag.py",
          "airflow/models/dagbag.py||airflow/models/dagbag.py",
          "airflow/models/dagcode.py||airflow/models/dagcode.py",
          "airflow/models/dagrun.py||airflow/models/dagrun.py",
          "airflow/models/dagwarning.py||airflow/models/dagwarning.py",
          "airflow/models/db_callback_request.py||airflow/models/db_callback_request.py",
          "airflow/models/expandinput.py||airflow/models/expandinput.py",
          "airflow/models/mappedoperator.py||airflow/models/mappedoperator.py",
          "airflow/models/operator.py||airflow/models/operator.py",
          "airflow/models/param.py||airflow/models/param.py",
          "airflow/models/pool.py||airflow/models/pool.py",
          "airflow/models/renderedtifields.py||airflow/models/renderedtifields.py",
          "airflow/models/serialized_dag.py||airflow/models/serialized_dag.py",
          "airflow/models/skipmixin.py||airflow/models/skipmixin.py",
          "airflow/models/taskinstance.py||airflow/models/taskinstance.py",
          "airflow/models/taskmixin.py||airflow/models/taskmixin.py",
          "airflow/models/taskreschedule.py||airflow/models/taskreschedule.py",
          "airflow/models/trigger.py||airflow/models/trigger.py",
          "airflow/models/variable.py||airflow/models/variable.py",
          "airflow/models/xcom.py||airflow/models/xcom.py",
          "airflow/models/xcom_arg.py||airflow/models/xcom_arg.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/models/dag.py||airflow/models/dag.py",
            "airflow/models/dagrun.py||airflow/models/dagrun.py"
          ],
          "candidate": [
            "airflow/models/dag.py||airflow/models/dag.py",
            "airflow/models/dagrun.py||airflow/models/dagrun.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/models/baseoperator.py||airflow/models/baseoperator.py": [
          "File: airflow/models/baseoperator.py -> airflow/models/baseoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: from abc import ABCMeta, abstractmethod",
          "31: from datetime import datetime, timedelta",
          "32: from inspect import signature",
          "34: from typing import (",
          "35:     TYPE_CHECKING,",
          "36:     Any,",
          "",
          "[Removed Lines]",
          "33: from types import ClassMethodDescriptorType, FunctionType",
          "",
          "[Added Lines]",
          "33: from types import FunctionType",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "50: import pendulum",
          "51: from dateutil.relativedelta import relativedelta",
          "52: from sqlalchemy import select",
          "54: from sqlalchemy.orm.exc import NoResultFound",
          "56: from airflow.configuration import conf",
          "",
          "[Removed Lines]",
          "53: from sqlalchemy.orm import Session",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "75:     DEFAULT_WAIT_FOR_PAST_DEPENDS_BEFORE_SKIPPING,",
          "76:     DEFAULT_WEIGHT_RULE,",
          "77:     AbstractOperator,",
          "79: )",
          "80: from airflow.models.mappedoperator import OperatorPartial, validate_mapping_kwargs",
          "81: from airflow.models.param import ParamsDict",
          "",
          "[Removed Lines]",
          "78:     TaskStateChangeCallback,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "83: from airflow.models.taskinstance import TaskInstance, clear_task_instances",
          "84: from airflow.models.taskmixin import DependencyMixin",
          "85: from airflow.serialization.enums import DagAttributeTypes",
          "87: from airflow.ti_deps.deps.not_in_retry_period_dep import NotInRetryPeriodDep",
          "88: from airflow.ti_deps.deps.not_previously_skipped_dep import NotPreviouslySkippedDep",
          "89: from airflow.ti_deps.deps.prev_dagrun_dep import PrevDagrunDep",
          "90: from airflow.ti_deps.deps.trigger_rule_dep import TriggerRuleDep",
          "92: from airflow.utils import timezone",
          "93: from airflow.utils.context import Context",
          "94: from airflow.utils.decorators import fixup_decorator_warning_stack",
          "",
          "[Removed Lines]",
          "86: from airflow.ti_deps.deps.base_ti_dep import BaseTIDep",
          "91: from airflow.triggers.base import BaseTrigger",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "98: from airflow.utils.session import NEW_SESSION, provide_session",
          "99: from airflow.utils.setup_teardown import SetupTeardownContext",
          "100: from airflow.utils.trigger_rule import TriggerRule",
          "102: from airflow.utils.weight_rule import WeightRule",
          "103: from airflow.utils.xcom import XCOM_RETURN_KEY",
          "105: if TYPE_CHECKING:",
          "106:     import jinja2  # Slow import.",
          "108:     from airflow.models.dag import DAG",
          "109:     from airflow.models.operator import Operator",
          "110:     from airflow.models.taskinstancekey import TaskInstanceKey",
          "111:     from airflow.models.xcom_arg import XComArg",
          "112:     from airflow.utils.task_group import TaskGroup",
          "114: ScheduleInterval = Union[str, timedelta, relativedelta]",
          "",
          "[Removed Lines]",
          "101: from airflow.utils.types import NOTSET, ArgNotSet",
          "",
          "[Added Lines]",
          "97: from airflow.utils.types import NOTSET",
          "102:     from types import ClassMethodDescriptorType",
          "105:     from sqlalchemy.orm import Session",
          "107:     from airflow.models.abstractoperator import (",
          "108:         TaskStateChangeCallback,",
          "109:     )",
          "114:     from airflow.ti_deps.deps.base_ti_dep import BaseTIDep",
          "115:     from airflow.triggers.base import BaseTrigger",
          "117:     from airflow.utils.types import ArgNotSet",
          "",
          "---------------"
        ],
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "54: import pendulum",
          "55: import re2",
          "56: from dateutil.relativedelta import relativedelta",
          "58: from sqlalchemy import (",
          "59:     Boolean,",
          "60:     Column,",
          "",
          "[Removed Lines]",
          "57: from pendulum.tz.timezone import Timezone",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "73: )",
          "74: from sqlalchemy.ext.associationproxy import association_proxy",
          "75: from sqlalchemy.orm import backref, joinedload, relationship",
          "78: from sqlalchemy.sql import Select, expression",
          "80: import airflow.templates",
          "",
          "[Removed Lines]",
          "76: from sqlalchemy.orm.query import Query",
          "77: from sqlalchemy.orm.session import Session",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "97: from airflow.models.dagcode import DagCode",
          "98: from airflow.models.dagpickle import DagPickle",
          "99: from airflow.models.dagrun import RUN_ID_REGEX, DagRun",
          "101: from airflow.models.param import DagParam, ParamsDict",
          "102: from airflow.models.taskinstance import Context, TaskInstance, TaskInstanceKey, clear_task_instances",
          "103: from airflow.secrets.local_filesystem import LocalFilesystemBackend",
          "",
          "[Removed Lines]",
          "100: from airflow.models.operator import Operator",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "111:     NullTimetable,",
          "112:     OnceTimetable,",
          "113: )",
          "115: from airflow.utils import timezone",
          "116: from airflow.utils.dag_cycle_tester import check_cycle",
          "117: from airflow.utils.dates import cron_presets, date_range as utils_date_range",
          "",
          "[Removed Lines]",
          "114: from airflow.typing_compat import Literal",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "133: if TYPE_CHECKING:",
          "134:     from types import ModuleType",
          "136:     from airflow.datasets import Dataset",
          "137:     from airflow.decorators import TaskDecoratorCollection",
          "138:     from airflow.models.dagbag import DagBag",
          "139:     from airflow.models.slamiss import SlaMiss",
          "140:     from airflow.utils.task_group import TaskGroup",
          "142: log = logging.getLogger(__name__)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "131:     from pendulum.tz.timezone import Timezone",
          "132:     from sqlalchemy.orm.query import Query",
          "133:     from sqlalchemy.orm.session import Session",
          "138:     from airflow.models.operator import Operator",
          "140:     from airflow.typing_compat import Literal",
          "",
          "---------------"
        ],
        "airflow/models/dagbag.py||airflow/models/dagbag.py": [
          "File: airflow/models/dagbag.py -> airflow/models/dagbag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: from typing import TYPE_CHECKING, NamedTuple",
          "33: from sqlalchemy.exc import OperationalError",
          "35: from tabulate import tabulate",
          "37: from airflow import settings",
          "",
          "[Removed Lines]",
          "34: from sqlalchemy.orm import Session",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "53: from airflow.utils.retries import MAX_DB_RETRIES, run_with_db_retries",
          "54: from airflow.utils.session import NEW_SESSION, provide_session",
          "55: from airflow.utils.timeout import timeout",
          "58: if TYPE_CHECKING:",
          "59:     import pathlib",
          "61:     from airflow.models.dag import DAG",
          "64: class FileLoadStat(NamedTuple):",
          "",
          "[Removed Lines]",
          "56: from airflow.utils.types import NOTSET, ArgNotSet",
          "",
          "[Added Lines]",
          "55: from airflow.utils.types import NOTSET",
          "60:     from sqlalchemy.orm import Session",
          "63:     from airflow.utils.types import ArgNotSet",
          "",
          "---------------"
        ],
        "airflow/models/dagcode.py||airflow/models/dagcode.py": [
          "File: airflow/models/dagcode.py -> airflow/models/dagcode.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import os",
          "21: import struct",
          "22: from datetime import datetime",
          "25: from sqlalchemy import BigInteger, Column, String, Text, delete, select",
          "26: from sqlalchemy.dialects.mysql import MEDIUMTEXT",
          "28: from sqlalchemy.sql.expression import literal",
          "30: from airflow.exceptions import AirflowException, DagCodeNotFound",
          "",
          "[Removed Lines]",
          "23: from typing import Collection, Iterable",
          "27: from sqlalchemy.orm import Session",
          "",
          "[Added Lines]",
          "23: from typing import TYPE_CHECKING, Collection, Iterable",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "34: from airflow.utils.session import NEW_SESSION, provide_session",
          "35: from airflow.utils.sqlalchemy import UtcDateTime",
          "37: log = logging.getLogger(__name__)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "36: if TYPE_CHECKING:",
          "37:     from sqlalchemy.orm import Session",
          "",
          "---------------"
        ],
        "airflow/models/dagrun.py||airflow/models/dagrun.py": [
          "File: airflow/models/dagrun.py -> airflow/models/dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import os",
          "22: import warnings",
          "23: from collections import defaultdict",
          "25: from typing import TYPE_CHECKING, Any, Callable, Iterable, Iterator, NamedTuple, Sequence, TypeVar, overload",
          "27: import re2",
          "",
          "[Removed Lines]",
          "24: from datetime import datetime",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "45: )",
          "46: from sqlalchemy.exc import IntegrityError",
          "47: from sqlalchemy.ext.associationproxy import association_proxy",
          "49: from sqlalchemy.sql.expression import false, select, true",
          "51: from airflow import settings",
          "",
          "[Removed Lines]",
          "48: from sqlalchemy.orm import Query, Session, declared_attr, joinedload, relationship, synonym, validates",
          "",
          "[Added Lines]",
          "47: from sqlalchemy.orm import declared_attr, joinedload, relationship, synonym, validates",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "61: from airflow.stats import Stats",
          "62: from airflow.ti_deps.dep_context import DepContext",
          "63: from airflow.ti_deps.dependencies_states import SCHEDULEABLE_STATES",
          "65: from airflow.utils import timezone",
          "66: from airflow.utils.helpers import chunks, is_container, prune_dict",
          "67: from airflow.utils.log.logging_mixin import LoggingMixin",
          "68: from airflow.utils.session import NEW_SESSION, provide_session",
          "69: from airflow.utils.sqlalchemy import UtcDateTime, nulls_first, skip_locked, tuple_in_condition, with_row_locks",
          "70: from airflow.utils.state import DagRunState, State, TaskInstanceState",
          "73: if TYPE_CHECKING:",
          "74:     from airflow.models.dag import DAG",
          "75:     from airflow.models.operator import Operator",
          "77:     CreatedTasks = TypeVar(\"CreatedTasks\", Iterator[\"dict[str, Any]\"], Iterator[TI])",
          "78:     TaskCreator = Callable[[Operator, Iterable[int]], CreatedTasks]",
          "",
          "[Removed Lines]",
          "64: from airflow.typing_compat import Literal",
          "71: from airflow.utils.types import NOTSET, ArgNotSet, DagRunType",
          "",
          "[Added Lines]",
          "69: from airflow.utils.types import NOTSET, DagRunType",
          "72:     from datetime import datetime",
          "74:     from sqlalchemy.orm import Query, Session",
          "78:     from airflow.typing_compat import Literal",
          "79:     from airflow.utils.types import ArgNotSet",
          "",
          "---------------"
        ],
        "airflow/models/dagwarning.py||airflow/models/dagwarning.py": [
          "File: airflow/models/dagwarning.py -> airflow/models/dagwarning.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: from enum import Enum",
          "22: from sqlalchemy import Column, ForeignKeyConstraint, String, Text, delete, false, select",
          "25: from airflow.api_internal.internal_api_call import internal_api_call",
          "26: from airflow.models.base import Base, StringID",
          "",
          "[Removed Lines]",
          "23: from sqlalchemy.orm import Session",
          "",
          "[Added Lines]",
          "21: from typing import TYPE_CHECKING",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "29: from airflow.utils.session import NEW_SESSION, provide_session",
          "30: from airflow.utils.sqlalchemy import UtcDateTime",
          "33: class DagWarning(Base):",
          "34:     \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32: if TYPE_CHECKING:",
          "33:     from sqlalchemy.orm import Session",
          "",
          "---------------"
        ],
        "airflow/models/db_callback_request.py||airflow/models/db_callback_request.py": [
          "File: airflow/models/db_callback_request.py -> airflow/models/db_callback_request.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: from importlib import import_module",
          "22: from sqlalchemy import Column, Integer, String",
          "25: from airflow.models.base import Base",
          "26: from airflow.utils import timezone",
          "27: from airflow.utils.sqlalchemy import ExtendedJSON, UtcDateTime",
          "30: class DbCallbackRequest(Base):",
          "31:     \"\"\"Used to handle callbacks through database.\"\"\"",
          "",
          "[Removed Lines]",
          "24: from airflow.callbacks.callback_requests import CallbackRequest",
          "",
          "[Added Lines]",
          "21: from typing import TYPE_CHECKING",
          "29: if TYPE_CHECKING:",
          "30:     from airflow.callbacks.callback_requests import CallbackRequest",
          "",
          "---------------"
        ],
        "airflow/models/expandinput.py||airflow/models/expandinput.py": [
          "File: airflow/models/expandinput.py -> airflow/models/expandinput.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: import attr",
          "29: from airflow.utils.mixins import ResolveMixin",
          "30: from airflow.utils.session import NEW_SESSION, provide_session",
          "",
          "[Removed Lines]",
          "27: from airflow.typing_compat import TypeGuard",
          "28: from airflow.utils.context import Context",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "35:     from airflow.models.operator import Operator",
          "36:     from airflow.models.xcom_arg import XComArg",
          "38: ExpandInput = Union[\"DictOfListsExpandInput\", \"ListOfDictsExpandInput\"]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "35:     from airflow.typing_compat import TypeGuard",
          "36:     from airflow.utils.context import Context",
          "",
          "---------------"
        ],
        "airflow/models/mappedoperator.py||airflow/models/mappedoperator.py": [
          "File: airflow/models/mappedoperator.py -> airflow/models/mappedoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import collections.abc",
          "22: import contextlib",
          "23: import copy",
          "25: import warnings",
          "26: from typing import TYPE_CHECKING, Any, ClassVar, Collection, Iterable, Iterator, Mapping, Sequence, Union",
          "28: import attr",
          "32: from airflow import settings",
          "33: from airflow.compat.functools import cache",
          "",
          "[Removed Lines]",
          "24: import datetime",
          "29: import pendulum",
          "30: from sqlalchemy.orm.session import Session",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "45:     DEFAULT_WEIGHT_RULE,",
          "46:     AbstractOperator,",
          "47:     NotMapped,",
          "49: )",
          "50: from airflow.models.expandinput import (",
          "51:     DictOfListsExpandInput,",
          "53:     ListOfDictsExpandInput,",
          "56:     is_mappable,",
          "57: )",
          "59: from airflow.models.pool import Pool",
          "60: from airflow.serialization.enums import DagAttributeTypes",
          "62: from airflow.ti_deps.deps.mapped_task_expanded import MappedTaskIsExpanded",
          "63: from airflow.typing_compat import Literal",
          "65: from airflow.utils.helpers import is_container, prevent_duplicates",
          "68: from airflow.utils.types import NOTSET",
          "69: from airflow.utils.xcom import XCOM_RETURN_KEY",
          "71: if TYPE_CHECKING:",
          "72:     import jinja2  # Slow import.",
          "74:     from airflow.models.baseoperator import BaseOperator, BaseOperatorLink",
          "75:     from airflow.models.dag import DAG",
          "76:     from airflow.models.operator import Operator",
          "77:     from airflow.models.xcom_arg import XComArg",
          "78:     from airflow.utils.task_group import TaskGroup",
          "80: ValidationSource = Union[Literal[\"expand\"], Literal[\"partial\"]]",
          "",
          "[Removed Lines]",
          "48:     TaskStateChangeCallback,",
          "52:     ExpandInput,",
          "54:     OperatorExpandArgument,",
          "55:     OperatorExpandKwargsArgument,",
          "58: from airflow.models.param import ParamsDict",
          "61: from airflow.ti_deps.deps.base_ti_dep import BaseTIDep",
          "64: from airflow.utils.context import Context, context_update_for_unmapped",
          "66: from airflow.utils.operator_resources import Resources",
          "67: from airflow.utils.trigger_rule import TriggerRule",
          "",
          "[Added Lines]",
          "55: from airflow.utils.context import context_update_for_unmapped",
          "61:     import datetime",
          "64:     import pendulum",
          "65:     from sqlalchemy.orm.session import Session",
          "67:     from airflow.models.abstractoperator import (",
          "68:         TaskStateChangeCallback,",
          "69:     )",
          "72:     from airflow.models.expandinput import (",
          "73:         ExpandInput,",
          "74:         OperatorExpandArgument,",
          "75:         OperatorExpandKwargsArgument,",
          "76:     )",
          "78:     from airflow.models.param import ParamsDict",
          "80:     from airflow.ti_deps.deps.base_ti_dep import BaseTIDep",
          "81:     from airflow.utils.context import Context",
          "82:     from airflow.utils.operator_resources import Resources",
          "84:     from airflow.utils.trigger_rule import TriggerRule",
          "",
          "---------------"
        ],
        "airflow/models/operator.py||airflow/models/operator.py": [
          "File: airflow/models/operator.py -> airflow/models/operator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "23: from airflow.models.baseoperator import BaseOperator",
          "24: from airflow.models.mappedoperator import MappedOperator",
          "27: Operator = Union[BaseOperator, MappedOperator]",
          "",
          "[Removed Lines]",
          "20: from typing import Union",
          "22: from airflow.models.abstractoperator import AbstractOperator",
          "25: from airflow.typing_compat import TypeGuard",
          "",
          "[Added Lines]",
          "20: from typing import TYPE_CHECKING, Union",
          "25: if TYPE_CHECKING:",
          "26:     from airflow.models.abstractoperator import AbstractOperator",
          "27:     from airflow.typing_compat import TypeGuard",
          "",
          "---------------"
        ],
        "airflow/models/param.py||airflow/models/param.py": [
          "File: airflow/models/param.py -> airflow/models/param.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: from airflow.exceptions import AirflowException, ParamValidationError, RemovedInAirflow3Warning",
          "30: from airflow.utils import timezone",
          "32: from airflow.utils.mixins import ResolveMixin",
          "33: from airflow.utils.types import NOTSET, ArgNotSet",
          "",
          "[Removed Lines]",
          "31: from airflow.utils.context import Context",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "36:     from airflow.models.dag import DAG",
          "37:     from airflow.models.dagrun import DagRun",
          "38:     from airflow.models.operator import Operator",
          "40: logger = logging.getLogger(__name__)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "38:     from airflow.utils.context import Context",
          "",
          "---------------"
        ],
        "airflow/models/pool.py||airflow/models/pool.py": [
          "File: airflow/models/pool.py -> airflow/models/pool.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "22: from sqlalchemy import Boolean, Column, Integer, String, Text, func, select",
          "25: from airflow.exceptions import AirflowException, PoolNotFound",
          "26: from airflow.models.base import Base",
          "",
          "[Removed Lines]",
          "20: from typing import Any",
          "23: from sqlalchemy.orm.session import Session",
          "",
          "[Added Lines]",
          "20: from typing import TYPE_CHECKING, Any",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "31: from airflow.utils.sqlalchemy import nowait, with_row_locks",
          "32: from airflow.utils.state import TaskInstanceState",
          "35: class PoolStats(TypedDict):",
          "36:     \"\"\"Dictionary containing Pool Stats.\"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33: if TYPE_CHECKING:",
          "34:     from sqlalchemy.orm.session import Session",
          "",
          "---------------"
        ],
        "airflow/models/renderedtifields.py||airflow/models/renderedtifields.py": [
          "File: airflow/models/renderedtifields.py -> airflow/models/renderedtifields.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "33:     text,",
          "34: )",
          "35: from sqlalchemy.ext.associationproxy import association_proxy",
          "38: from airflow.configuration import conf",
          "39: from airflow.models.base import Base, StringID",
          "41: from airflow.serialization.helpers import serialize_template_field",
          "42: from airflow.settings import json",
          "43: from airflow.utils.retries import retry_db_transaction",
          "44: from airflow.utils.session import NEW_SESSION, provide_session",
          "46: if TYPE_CHECKING:",
          "47:     from sqlalchemy.sql import FromClause",
          "50: class RenderedTaskInstanceFields(Base):",
          "51:     \"\"\"Save Rendered Template Fields.\"\"\"",
          "",
          "[Removed Lines]",
          "36: from sqlalchemy.orm import Session, relationship",
          "40: from airflow.models.taskinstance import TaskInstance",
          "",
          "[Added Lines]",
          "36: from sqlalchemy.orm import relationship",
          "46:     from sqlalchemy.orm import Session",
          "49:     from airflow.models.taskinstance import TaskInstance",
          "",
          "---------------"
        ],
        "airflow/models/serialized_dag.py||airflow/models/serialized_dag.py": [
          "File: airflow/models/serialized_dag.py -> airflow/models/serialized_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import logging",
          "22: import zlib",
          "26: import sqlalchemy_jsonfield",
          "27: from sqlalchemy import BigInteger, Column, Index, LargeBinary, String, and_, or_, select",
          "29: from sqlalchemy.sql.expression import func, literal",
          "31: from airflow.models.base import ID_LEN, Base",
          "33: from airflow.models.dagcode import DagCode",
          "34: from airflow.models.dagrun import DagRun",
          "35: from airflow.serialization.serialized_objects import DagDependency, SerializedDAG",
          "",
          "[Removed Lines]",
          "23: from datetime import datetime, timedelta",
          "24: from typing import Collection",
          "28: from sqlalchemy.orm import Session, backref, foreign, relationship",
          "32: from airflow.models.dag import DAG, DagModel",
          "",
          "[Added Lines]",
          "23: from datetime import timedelta",
          "24: from typing import TYPE_CHECKING, Collection",
          "28: from sqlalchemy.orm import backref, foreign, relationship",
          "32: from airflow.models.dag import DagModel",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39: from airflow.utils.session import NEW_SESSION, provide_session",
          "40: from airflow.utils.sqlalchemy import UtcDateTime",
          "42: log = logging.getLogger(__name__)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42: if TYPE_CHECKING:",
          "43:     from datetime import datetime",
          "45:     from sqlalchemy.orm import Session",
          "47:     from airflow.models.dag import DAG",
          "",
          "---------------"
        ],
        "airflow/models/skipmixin.py||airflow/models/skipmixin.py": [
          "File: airflow/models/skipmixin.py -> airflow/models/skipmixin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: from airflow.exceptions import AirflowException, RemovedInAirflow3Warning",
          "26: from airflow.models.dagrun import DagRun",
          "27: from airflow.models.taskinstance import TaskInstance",
          "29: from airflow.utils import timezone",
          "30: from airflow.utils.log.logging_mixin import LoggingMixin",
          "31: from airflow.utils.session import NEW_SESSION, create_session, provide_session",
          "",
          "[Removed Lines]",
          "28: from airflow.serialization.pydantic.dag_run import DagRunPydantic",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39:     from airflow.models.operator import Operator",
          "40:     from airflow.models.taskmixin import DAGNode",
          "41:     from airflow.serialization.pydantic.taskinstance import TaskInstancePydantic",
          "43: # The key used by SkipMixin to store XCom data.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "40:     from airflow.serialization.pydantic.dag_run import DagRunPydantic",
          "",
          "---------------"
        ],
        "airflow/models/taskinstance.py||airflow/models/taskinstance.py": [
          "File: airflow/models/taskinstance.py -> airflow/models/taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: import signal",
          "28: import warnings",
          "29: from collections import defaultdict",
          "31: from enum import Enum",
          "34: from typing import TYPE_CHECKING, Any, Callable, Collection, Generator, Iterable, Tuple",
          "35: from urllib.parse import quote",
          "",
          "[Removed Lines]",
          "30: from datetime import datetime, timedelta",
          "32: from pathlib import PurePath",
          "33: from types import TracebackType",
          "",
          "[Added Lines]",
          "30: from datetime import timedelta",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "62: from sqlalchemy.ext.mutable import MutableDict",
          "63: from sqlalchemy.orm import reconstructor, relationship",
          "64: from sqlalchemy.orm.attributes import NO_VALUE, set_committed_value",
          "69: from airflow import settings",
          "70: from airflow.compat.functools import cache",
          "",
          "[Removed Lines]",
          "65: from sqlalchemy.orm.session import Session",
          "66: from sqlalchemy.sql.elements import BooleanClauseList",
          "67: from sqlalchemy.sql.expression import ColumnOperators, case",
          "",
          "[Added Lines]",
          "63: from sqlalchemy.sql.expression import case",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "102: from airflow.templates import SandboxedEnvironment",
          "103: from airflow.ti_deps.dep_context import DepContext",
          "104: from airflow.ti_deps.dependencies_deps import REQUEUEABLE_DEPS, RUNNING_DEPS",
          "107: from airflow.utils import timezone",
          "108: from airflow.utils.context import ConnectionAccessor, Context, VariableAccessor, context_merge",
          "109: from airflow.utils.email import send_email",
          "",
          "[Removed Lines]",
          "105: from airflow.timetables.base import DataInterval",
          "106: from airflow.typing_compat import Literal, TypeGuard",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "136: if TYPE_CHECKING:",
          "137:     from airflow.models.abstractoperator import TaskStateChangeCallback",
          "138:     from airflow.models.baseoperator import BaseOperator",
          "139:     from airflow.models.dag import DAG, DagModel",
          "140:     from airflow.models.dagrun import DagRun",
          "141:     from airflow.models.dataset import DatasetEvent",
          "142:     from airflow.models.operator import Operator",
          "143:     from airflow.utils.task_group import TaskGroup",
          "145:     # This is a workaround because mypy doesn't work with hybrid_property",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "131:     from datetime import datetime",
          "132:     from pathlib import PurePath",
          "133:     from types import TracebackType",
          "135:     from sqlalchemy.orm.session import Session",
          "136:     from sqlalchemy.sql.elements import BooleanClauseList",
          "137:     from sqlalchemy.sql.expression import ColumnOperators",
          "145:     from airflow.timetables.base import DataInterval",
          "146:     from airflow.typing_compat import Literal, TypeGuard",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1836:     def dry_run(self) -> None:",
          "1837:         \"\"\"Only Renders Templates for the TI.\"\"\"",
          "1840:         self.task = self.task.prepare_for_execution()",
          "1841:         self.render_templates()",
          "1842:         if TYPE_CHECKING:",
          "",
          "[Removed Lines]",
          "1838:         from airflow.models.baseoperator import BaseOperator",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/models/taskmixin.py||airflow/models/taskmixin.py": [
          "File: airflow/models/taskmixin.py -> airflow/models/taskmixin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: from abc import ABCMeta, abstractmethod",
          "21: from typing import TYPE_CHECKING, Any, Iterable, Sequence",
          "25: from airflow.exceptions import AirflowException, RemovedInAirflow3Warning",
          "29: if TYPE_CHECKING:",
          "30:     from logging import Logger",
          "32:     from airflow.models.baseoperator import BaseOperator",
          "33:     from airflow.models.dag import DAG",
          "34:     from airflow.models.operator import Operator",
          "35:     from airflow.utils.edgemodifier import EdgeModifier",
          "36:     from airflow.utils.task_group import TaskGroup",
          "39: class DependencyMixin:",
          "",
          "[Removed Lines]",
          "23: import pendulum",
          "26: from airflow.serialization.enums import DagAttributeTypes",
          "27: from airflow.utils.types import NOTSET, ArgNotSet",
          "",
          "[Added Lines]",
          "24: from airflow.utils.types import NOTSET",
          "29:     import pendulum",
          "34:     from airflow.serialization.enums import DagAttributeTypes",
          "37:     from airflow.utils.types import ArgNotSet",
          "",
          "---------------"
        ],
        "airflow/models/taskreschedule.py||airflow/models/taskreschedule.py": [
          "File: airflow/models/taskreschedule.py -> airflow/models/taskreschedule.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: \"\"\"TaskReschedule tracks rescheduled task instances.\"\"\"",
          "19: from __future__ import annotations",
          "22: from typing import TYPE_CHECKING",
          "24: from sqlalchemy import Column, ForeignKeyConstraint, Index, Integer, String, asc, desc, event, text",
          "25: from sqlalchemy.ext.associationproxy import association_proxy",
          "28: from airflow.models.base import COLLATION_ARGS, ID_LEN, Base",
          "29: from airflow.utils.session import NEW_SESSION, provide_session",
          "30: from airflow.utils.sqlalchemy import UtcDateTime",
          "32: if TYPE_CHECKING:",
          "33:     from airflow.models.operator import Operator",
          "34:     from airflow.models.taskinstance import TaskInstance",
          "",
          "[Removed Lines]",
          "21: import datetime",
          "26: from sqlalchemy.orm import Query, Session, relationship",
          "",
          "[Added Lines]",
          "25: from sqlalchemy.orm import relationship",
          "32:     import datetime",
          "34:     from sqlalchemy.orm import Query, Session",
          "",
          "---------------"
        ],
        "airflow/models/trigger.py||airflow/models/trigger.py": [
          "File: airflow/models/trigger.py -> airflow/models/trigger.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: import datetime",
          "20: from traceback import format_exception",
          "23: from sqlalchemy import Column, Integer, String, delete, func, or_, select, update",
          "25: from sqlalchemy.sql.functions import coalesce",
          "27: from airflow.api_internal.internal_api_call import internal_api_call",
          "28: from airflow.models.base import Base",
          "29: from airflow.models.taskinstance import TaskInstance",
          "31: from airflow.utils import timezone",
          "32: from airflow.utils.retries import run_with_db_retries",
          "33: from airflow.utils.session import NEW_SESSION, provide_session",
          "34: from airflow.utils.sqlalchemy import ExtendedJSON, UtcDateTime, with_row_locks",
          "35: from airflow.utils.state import TaskInstanceState",
          "38: class Trigger(Base):",
          "39:     \"\"\"",
          "",
          "[Removed Lines]",
          "21: from typing import Any, Iterable",
          "24: from sqlalchemy.orm import Session, joinedload, relationship",
          "30: from airflow.triggers.base import BaseTrigger",
          "",
          "[Added Lines]",
          "21: from typing import TYPE_CHECKING, Any, Iterable",
          "24: from sqlalchemy.orm import joinedload, relationship",
          "36: if TYPE_CHECKING:",
          "37:     from sqlalchemy.orm import Session",
          "39:     from airflow.triggers.base import BaseTrigger",
          "",
          "---------------"
        ],
        "airflow/models/variable.py||airflow/models/variable.py": [
          "File: airflow/models/variable.py -> airflow/models/variable.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import json",
          "21: import logging",
          "24: from sqlalchemy import Boolean, Column, Integer, String, Text, delete, select",
          "25: from sqlalchemy.dialects.mysql import MEDIUMTEXT",
          "28: from airflow.api_internal.internal_api_call import internal_api_call",
          "29: from airflow.configuration import ensure_secrets_loaded",
          "",
          "[Removed Lines]",
          "22: from typing import Any",
          "26: from sqlalchemy.orm import Session, declared_attr, reconstructor, synonym",
          "",
          "[Added Lines]",
          "22: from typing import TYPE_CHECKING, Any",
          "26: from sqlalchemy.orm import declared_attr, reconstructor, synonym",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "35: from airflow.utils.log.secrets_masker import mask_secret",
          "36: from airflow.utils.session import provide_session",
          "38: log = logging.getLogger(__name__)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "38: if TYPE_CHECKING:",
          "39:     from sqlalchemy.orm import Session",
          "",
          "---------------"
        ],
        "airflow/models/xcom.py||airflow/models/xcom.py": [
          "File: airflow/models/xcom.py -> airflow/models/xcom.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import collections.abc",
          "21: import contextlib",
          "23: import inspect",
          "24: import itertools",
          "25: import json",
          "",
          "[Removed Lines]",
          "22: import datetime",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "30: from typing import TYPE_CHECKING, Any, Generator, Iterable, cast, overload",
          "32: import attr",
          "34: from sqlalchemy import (",
          "35:     Column,",
          "36:     ForeignKeyConstraint,",
          "",
          "[Removed Lines]",
          "33: import pendulum",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "43:     text,",
          "44: )",
          "45: from sqlalchemy.ext.associationproxy import association_proxy",
          "47: from sqlalchemy.orm.exc import NoResultFound",
          "49: from airflow import settings",
          "",
          "[Removed Lines]",
          "46: from sqlalchemy.orm import Query, Session, reconstructor, relationship",
          "",
          "[Added Lines]",
          "44: from sqlalchemy.orm import Query, reconstructor, relationship",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "68: log = logging.getLogger(__name__)",
          "70: if TYPE_CHECKING:",
          "71:     from airflow.models.taskinstancekey import TaskInstanceKey",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "69:     import datetime",
          "71:     import pendulum",
          "72:     from sqlalchemy.orm import Session",
          "",
          "---------------"
        ],
        "airflow/models/xcom_arg.py||airflow/models/xcom_arg.py": [
          "File: airflow/models/xcom_arg.py -> airflow/models/xcom_arg.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: from typing import TYPE_CHECKING, Any, Callable, Iterable, Iterator, Mapping, Sequence, Union, overload",
          "24: from sqlalchemy import func, or_",
          "27: from airflow.exceptions import AirflowException, XComNotFound",
          "28: from airflow.models.abstractoperator import AbstractOperator",
          "30: from airflow.models.mappedoperator import MappedOperator",
          "33: from airflow.utils.db import exists_query",
          "35: from airflow.utils.mixins import ResolveMixin",
          "36: from airflow.utils.session import NEW_SESSION, provide_session",
          "37: from airflow.utils.setup_teardown import SetupTeardownContext",
          "",
          "[Removed Lines]",
          "25: from sqlalchemy.orm import Session",
          "29: from airflow.models.baseoperator import BaseOperator",
          "31: from airflow.models.taskmixin import DAGNode, DependencyMixin",
          "32: from airflow.utils.context import Context",
          "34: from airflow.utils.edgemodifier import EdgeModifier",
          "",
          "[Added Lines]",
          "29: from airflow.models.taskmixin import DependencyMixin",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "41: from airflow.utils.xcom import XCOM_RETURN_KEY",
          "43: if TYPE_CHECKING:",
          "44:     from airflow.models.dag import DAG",
          "45:     from airflow.models.operator import Operator",
          "47: # Callable objects contained by MapXComArg. We only accept callables from",
          "48: # the user, but deserialize them into strings in a serialized XComArg for",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "40:     from sqlalchemy.orm import Session",
          "42:     from airflow.models.baseoperator import BaseOperator",
          "45:     from airflow.models.taskmixin import DAGNode",
          "46:     from airflow.utils.context import Context",
          "47:     from airflow.utils.edgemodifier import EdgeModifier",
          "",
          "---------------"
        ]
      }
    }
  ]
}