{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "30834b847e7577cf694558d43fb618fc0b1eb09e",
      "candidate_info": {
        "commit_hash": "30834b847e7577cf694558d43fb618fc0b1eb09e",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/30834b847e7577cf694558d43fb618fc0b1eb09e",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala"
        ],
        "message": "[SPARK-39157][SQL] H2Dialect should override getJDBCType so as make the data type is correct\n\n### What changes were proposed in this pull request?\nCurrently, `H2Dialect` not implement `getJDBCType` of `JdbcDialect`, so the DS V2 push-down will throw exception show below:\n```\nJob aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 13) (jiaan-gengdembp executor driver):\n org.h2.jdbc.JdbcSQLNonTransientException: Unknown data type: \"STRING\"; SQL statement:\nSELECT \"DEPT\",\"NAME\",\"SALARY\",\"BONUS\",\"IS_MANAGER\" FROM \"test\".\"employee\"  WHERE (\"BONUS\" IS NOT NULL) AND (\"DEPT\" IS NOT NULL) AND (CAST(\"BONUS\" AS string) LIKE '%30%') AND (CAST(\"DEPT\" AS byte) > 1) AND (CAST(\"DEPT\" AS short) > 1) AND (CAST(\"BONUS\" AS decimal(20,2)) > 1200.00)    [50004-210]\n```\nH2Dialect should implement `getJDBCType` of `JdbcDialect`.\n\n### Why are the changes needed?\n make the H2 data type is correct.\n\n### Does this PR introduce _any_ user-facing change?\n'Yes'.\nFix a bug for `H2Dialect`.\n\n### How was this patch tested?\nNew tests.\n\nCloses #36516 from beliefer/SPARK-39157.\n\nAuthored-by: Jiaan Geng <beliefer@163.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit fa3f096e02d408fbeab5f69af451ef8bc8f5b3db)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala||sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala||sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala -> sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.jdbc",
          "21: import java.util.Locale",
          "23: import scala.util.control.NonFatal",
          "",
          "[Removed Lines]",
          "20: import java.sql.SQLException",
          "",
          "[Added Lines]",
          "20: import java.sql.{SQLException, Types}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "27: import org.apache.spark.sql.connector.expressions.Expression",
          "28: import org.apache.spark.sql.connector.expressions.aggregate.{AggregateFunc, GeneralAggregateFunc}",
          "29: import org.apache.spark.sql.errors.QueryCompilationErrors",
          "31: private object H2Dialect extends JdbcDialect {",
          "32:   override def canHandle(url: String): Boolean =",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "30: import org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils",
          "31: import org.apache.spark.sql.types.{BooleanType, ByteType, DataType, DecimalType, ShortType, StringType}",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "90:     )",
          "91:   }",
          "93:   override def classifyException(message: String, e: Throwable): AnalysisException = {",
          "94:     e match {",
          "95:       case exception: SQLException =>",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "95:   override def getJDBCType(dt: DataType): Option[JdbcType] = dt match {",
          "96:     case StringType => Option(JdbcType(\"CLOB\", Types.CLOB))",
          "97:     case BooleanType => Some(JdbcType(\"BOOLEAN\", Types.BOOLEAN))",
          "98:     case ShortType | ByteType => Some(JdbcType(\"SMALLINT\", Types.SMALLINT))",
          "99:     case t: DecimalType => Some(",
          "100:       JdbcType(s\"NUMERIC(${t.precision},${t.scale})\", Types.NUMERIC))",
          "101:     case _ => JdbcUtils.getCommonJDBCType(dt)",
          "102:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala -> sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "466:         checkFiltersRemoved(df7, false)",
          "467:         checkPushedInfo(df7, \"PushedFilters: [DEPT IS NOT NULL]\")",
          "468:         checkAnswer(df7, Seq(Row(6, \"jen\", 12000, 1200, true)))",
          "469:       }",
          "470:     }",
          "471:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "470:         val df8 = sql(",
          "471:           \"\"\"",
          "472:             |SELECT * FROM h2.test.employee",
          "473:             |WHERE cast(bonus as string) like '%30%'",
          "474:             |AND cast(dept as byte) > 1",
          "475:             |AND cast(dept as short) > 1",
          "476:             |AND cast(bonus as decimal(20, 2)) > 1200\"\"\".stripMargin)",
          "477:         checkFiltersRemoved(df8, ansiMode)",
          "478:         val expectedPlanFragment8 = if (ansiMode) {",
          "479:           \"PushedFilters: [BONUS IS NOT NULL, DEPT IS NOT NULL, \" +",
          "480:             \"CAST(BONUS AS string) LIKE '%30%', CAST(DEPT AS byte) > 1, ...,\"",
          "481:         } else {",
          "482:           \"PushedFilters: [BONUS IS NOT NULL, DEPT IS NOT NULL],\"",
          "483:         }",
          "484:         checkPushedInfo(df8, expectedPlanFragment8)",
          "485:         checkAnswer(df8, Seq(Row(2, \"david\", 10000, 1300, true)))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "88c076de9d042d1e57b324634a913ab7dfdc1db3",
      "candidate_info": {
        "commit_hash": "88c076de9d042d1e57b324634a913ab7dfdc1db3",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/88c076de9d042d1e57b324634a913ab7dfdc1db3",
        "files": [
          "core/src/main/java/org/apache/spark/memory/SparkOutOfMemoryError.java",
          "core/src/main/resources/error/error-classes.json",
          "core/src/main/scala/org/apache/spark/ErrorInfo.scala",
          "core/src/main/scala/org/apache/spark/SparkException.scala",
          "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala"
        ],
        "message": "[SPARK-39229][SQL][3.3] Separate query contexts from error-classes.json\n\n### What changes were proposed in this pull request?\n\nSeparate query contexts for runtime errors from error-classes.json.\n\n### Why are the changes needed?\n\nThe message is JSON should only contain parameters explicitly thrown. It is more elegant to separate query contexts from error-classes.json.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting UT\n\nCloses #36607 from gengliangwang/SPARK-39229-3.3.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "core/src/main/java/org/apache/spark/memory/SparkOutOfMemoryError.java||core/src/main/java/org/apache/spark/memory/SparkOutOfMemoryError.java",
          "core/src/main/scala/org/apache/spark/ErrorInfo.scala||core/src/main/scala/org/apache/spark/ErrorInfo.scala",
          "core/src/main/scala/org/apache/spark/SparkException.scala||core/src/main/scala/org/apache/spark/SparkException.scala",
          "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala||core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/java/org/apache/spark/memory/SparkOutOfMemoryError.java||core/src/main/java/org/apache/spark/memory/SparkOutOfMemoryError.java": [
          "File: core/src/main/java/org/apache/spark/memory/SparkOutOfMemoryError.java -> core/src/main/java/org/apache/spark/memory/SparkOutOfMemoryError.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "39:     }",
          "41:     public SparkOutOfMemoryError(String errorClass, String[] messageParameters) {",
          "43:         this.errorClass = errorClass;",
          "44:         this.messageParameters = messageParameters;",
          "45:     }",
          "",
          "[Removed Lines]",
          "42:         super(SparkThrowableHelper.getMessage(errorClass, messageParameters));",
          "",
          "[Added Lines]",
          "42:         super(SparkThrowableHelper.getMessage(errorClass, messageParameters, \"\"));",
          "",
          "---------------"
        ],
        "core/src/main/scala/org/apache/spark/ErrorInfo.scala||core/src/main/scala/org/apache/spark/ErrorInfo.scala": [
          "File: core/src/main/scala/org/apache/spark/ErrorInfo.scala -> core/src/main/scala/org/apache/spark/ErrorInfo.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "55:     mapper.readValue(errorClassesUrl, new TypeReference[SortedMap[String, ErrorInfo]]() {})",
          "56:   }",
          "59:     val errorInfo = errorClassToInfoMap.getOrElse(errorClass,",
          "60:       throw new IllegalArgumentException(s\"Cannot find error class '$errorClass'\"))",
          "61:     String.format(errorInfo.messageFormat.replaceAll(\"<[a-zA-Z0-9_-]+>\", \"%s\"),",
          "63:   }",
          "65:   def getSqlState(errorClass: String): String = {",
          "",
          "[Removed Lines]",
          "58:   def getMessage(errorClass: String, messageParameters: Array[String]): String = {",
          "62:       messageParameters: _*)",
          "",
          "[Added Lines]",
          "58:   def getMessage(",
          "59:       errorClass: String,",
          "60:       messageParameters: Array[String],",
          "61:       queryContext: String = \"\"): String = {",
          "65:       messageParameters: _*) + queryContext",
          "",
          "---------------"
        ],
        "core/src/main/scala/org/apache/spark/SparkException.scala||core/src/main/scala/org/apache/spark/SparkException.scala": [
          "File: core/src/main/scala/org/apache/spark/SparkException.scala -> core/src/main/scala/org/apache/spark/SparkException.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "96:     with SparkThrowable {",
          "98:   override def getErrorClass: String = errorClass",
          "",
          "[Removed Lines]",
          "94: private[spark] class SparkArithmeticException(errorClass: String, messageParameters: Array[String])",
          "95:   extends ArithmeticException(SparkThrowableHelper.getMessage(errorClass, messageParameters))",
          "",
          "[Added Lines]",
          "94: private[spark] class SparkArithmeticException(",
          "95:     errorClass: String,",
          "96:     messageParameters: Array[String],",
          "97:     queryContext: String = \"\")",
          "98:   extends ArithmeticException(",
          "99:     SparkThrowableHelper.getMessage(errorClass, messageParameters, queryContext))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "143:   extends DateTimeException(",
          "146:   override def getErrorClass: String = errorClass",
          "147: }",
          "",
          "[Removed Lines]",
          "142: private[spark] class SparkDateTimeException(errorClass: String, messageParameters: Array[String])",
          "144:     SparkThrowableHelper.getMessage(errorClass, messageParameters)) with SparkThrowable {",
          "",
          "[Added Lines]",
          "146: private[spark] class SparkDateTimeException(",
          "147:     errorClass: String,",
          "148:     messageParameters: Array[String],",
          "149:     queryContext: String = \"\")",
          "151:     SparkThrowableHelper.getMessage(errorClass, messageParameters, queryContext))",
          "152:     with SparkThrowable {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "176: private[spark] class SparkNumberFormatException(",
          "177:     errorClass: String,",
          "179:   extends NumberFormatException(",
          "182:   override def getErrorClass: String = errorClass",
          "183: }",
          "",
          "[Removed Lines]",
          "178:     messageParameters: Array[String])",
          "180:     SparkThrowableHelper.getMessage(errorClass, messageParameters)) with SparkThrowable {",
          "",
          "[Added Lines]",
          "186:     messageParameters: Array[String],",
          "187:     queryContext: String)",
          "189:     SparkThrowableHelper.getMessage(errorClass, messageParameters, queryContext))",
          "190:     with SparkThrowable {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "233: private[spark] class SparkRuntimeException(",
          "234:     errorClass: String,",
          "235:     messageParameters: Array[String],",
          "237:   extends RuntimeException(",
          "240:   override def getErrorClass: String = errorClass",
          "241: }",
          "",
          "[Removed Lines]",
          "236:     cause: Throwable = null)",
          "238:     SparkThrowableHelper.getMessage(errorClass, messageParameters), cause) with SparkThrowable {",
          "",
          "[Added Lines]",
          "246:     cause: Throwable = null,",
          "247:     queryContext: String = \"\")",
          "249:     SparkThrowableHelper.getMessage(errorClass, messageParameters, queryContext), cause)",
          "250:     with SparkThrowable {",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "282: private[spark] class SparkNoSuchElementException(",
          "283:     errorClass: String,",
          "285:   extends NoSuchElementException(",
          "288:   override def getErrorClass: String = errorClass",
          "289: }",
          "",
          "[Removed Lines]",
          "284:     messageParameters: Array[String])",
          "286:     SparkThrowableHelper.getMessage(errorClass, messageParameters)) with SparkThrowable {",
          "",
          "[Added Lines]",
          "296:     messageParameters: Array[String],",
          "297:     queryContext: String)",
          "299:     SparkThrowableHelper.getMessage(errorClass, messageParameters, queryContext))",
          "300:     with SparkThrowable {",
          "",
          "---------------"
        ],
        "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala||core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala": [
          "File: core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala -> core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "127:     assert(getMessage(\"DIVIDE_BY_ZERO\", Array(\"foo\", \"bar\", \"baz\")) ==",
          "128:       \"Division by zero. To return NULL instead, use `try_divide`. If necessary set foo \" +",
          "130:   }",
          "132:   test(\"Error message is formatted\") {",
          "",
          "[Removed Lines]",
          "129:       \"to \\\"false\\\" (except for ANSI interval type) to bypass this error.bar\")",
          "",
          "[Added Lines]",
          "129:       \"to \\\"false\\\" (except for ANSI interval type) to bypass this error.\")",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "104:         value.toDebugString,",
          "105:         decimalPrecision.toString,",
          "106:         decimalScale.toString,",
          "109:   }",
          "111:   def invalidInputInCastToDatetimeError(",
          "",
          "[Removed Lines]",
          "107:         toSQLConf(SQLConf.ANSI_ENABLED.key),",
          "108:         context))",
          "",
          "[Added Lines]",
          "107:         toSQLConf(SQLConf.ANSI_ENABLED.key)),",
          "108:       queryContext = context)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "119:         toSQLValue(value, from),",
          "120:         toSQLType(from),",
          "121:         toSQLType(to),",
          "124:   }",
          "126:   def invalidInputSyntaxForBooleanError(",
          "",
          "[Removed Lines]",
          "122:         toSQLConf(SQLConf.ANSI_ENABLED.key),",
          "123:         errorContext))",
          "",
          "[Added Lines]",
          "122:         toSQLConf(SQLConf.ANSI_ENABLED.key)),",
          "123:       queryContext = errorContext)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "132:         toSQLValue(s, StringType),",
          "133:         toSQLType(StringType),",
          "134:         toSQLType(BooleanType),",
          "137:   }",
          "139:   def invalidInputInCastToNumberError(",
          "",
          "[Removed Lines]",
          "135:         toSQLConf(SQLConf.ANSI_ENABLED.key),",
          "136:         errorContext))",
          "",
          "[Added Lines]",
          "135:         toSQLConf(SQLConf.ANSI_ENABLED.key)),",
          "136:       queryContext = errorContext)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "146:         toSQLValue(s, StringType),",
          "147:         toSQLType(StringType),",
          "148:         toSQLType(to),",
          "151:   }",
          "153:   def cannotCastFromNullTypeError(to: DataType): Throwable = {",
          "",
          "[Removed Lines]",
          "149:         toSQLConf(SQLConf.ANSI_ENABLED.key),",
          "150:         errorContext))",
          "",
          "[Added Lines]",
          "149:         toSQLConf(SQLConf.ANSI_ENABLED.key)),",
          "150:       queryContext = errorContext)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "180:   def divideByZeroError(context: String): ArithmeticException = {",
          "181:     new SparkArithmeticException(",
          "182:       errorClass = \"DIVIDE_BY_ZERO\",",
          "184:   }",
          "186:   def invalidArrayIndexError(index: Int, numElements: Int): ArrayIndexOutOfBoundsException = {",
          "",
          "[Removed Lines]",
          "183:       messageParameters = Array(toSQLConf(SQLConf.ANSI_ENABLED.key), context))",
          "",
          "[Added Lines]",
          "183:       messageParameters = Array(toSQLConf(SQLConf.ANSI_ENABLED.key)),",
          "184:       queryContext = context)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "218:       errorClass = \"MAP_KEY_DOES_NOT_EXIST\",",
          "219:       messageParameters = Array(",
          "220:         toSQLValue(key, dataType),",
          "223:   }",
          "225:   def inputTypeUnsupportedError(dataType: DataType): Throwable = {",
          "",
          "[Removed Lines]",
          "221:         toSQLConf(SQLConf.ANSI_ENABLED.key),",
          "222:         context))",
          "",
          "[Added Lines]",
          "222:         toSQLConf(SQLConf.ANSI_ENABLED.key)),",
          "223:       queryContext = context)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "493:       hint: String = \"\",",
          "494:       errorContext: String = \"\"): ArithmeticException = {",
          "495:     val alternative = if (hint.nonEmpty) s\" To return NULL instead, use '$hint'.\" else \"\"",
          "498:   }",
          "500:   def unaryMinusCauseOverflowError(originValue: Int): ArithmeticException = {",
          "",
          "[Removed Lines]",
          "496:     new SparkArithmeticException(\"ARITHMETIC_OVERFLOW\",",
          "497:       Array(message, alternative, SQLConf.ANSI_ENABLED.key, errorContext))",
          "",
          "[Added Lines]",
          "497:     new SparkArithmeticException(",
          "498:       errorClass = \"ARITHMETIC_OVERFLOW\",",
          "499:       messageParameters = Array(message, alternative, SQLConf.ANSI_ENABLED.key),",
          "500:       queryContext = errorContext)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "369b01404c25f02458316fd99307a3f94a13cec5",
      "candidate_info": {
        "commit_hash": "369b01404c25f02458316fd99307a3f94a13cec5",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/369b01404c25f02458316fd99307a3f94a13cec5",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/TransposeWindowSuite.scala"
        ],
        "message": "[SPARK-38034][SQL] Optimize TransposeWindow rule\n\n### What changes were proposed in this pull request?\n\nOptimize the TransposeWindow rule to extend applicable cases and optimize time complexity.\nTransposeWindow rule will try to eliminate unnecessary shuffle:\n\nbut the function compatiblePartitions will only take the first n elements of the window2 partition sequence, for some cases, this will not take effect, like the case below:\u00a0\n\nval df = spark.range(10).selectExpr(\"id AS a\", \"id AS b\", \"id AS c\", \"id AS d\")\ndf.selectExpr(\n    \"sum(`d`) OVER(PARTITION BY `b`,`a`) as e\",\n    \"sum(`c`) OVER(PARTITION BY `a`) as f\"\n  ).explain\n\nCurrent plan\n\n== Physical Plan ==\n*(5) Project [e#10L, f#11L]\n+- Window [sum(c#4L) windowspecdefinition(a#2L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS f#11L], [a#2L]\n\u00a0 \u00a0+- *(4) Sort [a#2L ASC NULLS FIRST], false, 0\n\u00a0 \u00a0 \u00a0 +- Exchange hashpartitioning(a#2L, 200), true, [id=#41]\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0+- *(3) Project [a#2L, c#4L, e#10L]\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 +- Window [sum(d#5L) windowspecdefinition(b#3L, a#2L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS e#10L], [b#3L, a#2L]\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0+- *(2) Sort [b#3L ASC NULLS FIRST, a#2L ASC NULLS FIRST], false, 0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 +- Exchange hashpartitioning(b#3L, a#2L, 200), true, [id=#33]\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0+- *(1) Project [id#0L AS d#5L, id#0L AS b#3L, id#0L AS a#2L, id#0L AS c#4L]\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 +- *(1) Range (0, 10, step=1, splits=10)\n\nExpected plan:\n\n== Physical Plan ==\n*(4) Project [e#924L, f#925L]\n+- Window [sum(d#43L) windowspecdefinition(b#41L, a#40L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS e#924L], [b#41L, a#40L]\n  \u00a0+- *(3) Sort [b#41L ASC NULLS FIRST, a#40L ASC NULLS FIRST], false, 0\n\u00a0 \u00a0 \u00a0 +- *(3) Project [d#43L, b#41L, a#40L, f#925L]\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0+- Window [sum(c#42L) windowspecdefinition(a#40L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS f#925L], [a#40L]\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 +- *(2) Sort [a#40L ASC NULLS FIRST], false, 0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0+- Exchange hashpartitioning(a#40L, 200), true, [id=#282]\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 +- *(1) Project [id#38L AS d#43L, id#38L AS b#41L, id#38L AS a#40L, id#38L AS c#42L]\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0+- *(1) Range (0, 10, step=1, splits=10)\n\nAlso the permutations method has a O(n!) time complexity, which is very expensive when there are many partition columns, we could try to optimize it.\n\n### Why are the changes needed?\n\nWe could apply the rule for more cases, which could improve the execution performance by eliminate unnecessary shuffle, and by reducing the time complexity from O(n!) to O(n2), the performance for the rule itself could improve\n\n### Does this PR introduce _any_ user-facing change?\n\nno\n\n### How was this patch tested?\n\nUT\n\nCloses #35334 from constzhou/SPARK-38034_optimize_transpose_window_rule.\n\nAuthored-by: xzhou <15210830305@163.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 0cc331dc7e51e53000063052b0c8ace417eb281b)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/TransposeWindowSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/TransposeWindowSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1177: object TransposeWindow extends Rule[LogicalPlan] {",
          "1178:   private def compatiblePartitions(ps1 : Seq[Expression], ps2: Seq[Expression]): Boolean = {",
          "1182:   }",
          "1184:   private def windowsCompatible(w1: Window, w2: Window): Boolean = {",
          "",
          "[Removed Lines]",
          "1179:     ps1.length < ps2.length && ps2.take(ps1.length).permutations.exists(ps1.zip(_).forall {",
          "1180:       case (l, r) => l.semanticEquals(r)",
          "1181:     })",
          "",
          "[Added Lines]",
          "1179:     ps1.length < ps2.length && ps1.forall { expr1 =>",
          "1180:       ps2.exists(expr1.semanticEquals)",
          "1181:     }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/TransposeWindowSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/TransposeWindowSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/TransposeWindowSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/TransposeWindowSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "142:     comparePlans(optimized, analyzed)",
          "143:   }",
          "145: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "145:   test(\"SPARK-38034: transpose two adjacent windows with compatible partitions \" +",
          "146:     \"which is not a prefix\") {",
          "147:     val query = testRelation",
          "148:       .window(Seq(sum(c).as('sum_a_2)), partitionSpec4, orderSpec2)",
          "149:       .window(Seq(sum(c).as('sum_a_1)), partitionSpec3, orderSpec1)",
          "151:     val analyzed = query.analyze",
          "152:     val optimized = Optimize.execute(analyzed)",
          "154:     val correctAnswer = testRelation",
          "155:       .window(Seq(sum(c).as('sum_a_1)), partitionSpec3, orderSpec1)",
          "156:       .window(Seq(sum(c).as('sum_a_2)), partitionSpec4, orderSpec2)",
          "157:       .select('a, 'b, 'c, 'd, 'sum_a_2, 'sum_a_1)",
          "159:     comparePlans(optimized, correctAnswer.analyze)",
          "160:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "36c01df145072e0fee966b9b2fc6782f7636d862",
      "candidate_info": {
        "commit_hash": "36c01df145072e0fee966b9b2fc6782f7636d862",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/36c01df145072e0fee966b9b2fc6782f7636d862",
        "files": [
          "R/pkg/DESCRIPTION",
          "assembly/pom.xml",
          "common/kvstore/pom.xml",
          "common/network-common/pom.xml",
          "common/network-shuffle/pom.xml",
          "common/network-yarn/pom.xml",
          "common/sketch/pom.xml",
          "common/tags/pom.xml",
          "common/unsafe/pom.xml",
          "core/pom.xml",
          "docs/_config.yml",
          "examples/pom.xml",
          "external/avro/pom.xml",
          "external/docker-integration-tests/pom.xml",
          "external/kafka-0-10-assembly/pom.xml",
          "external/kafka-0-10-sql/pom.xml",
          "external/kafka-0-10-token-provider/pom.xml",
          "external/kafka-0-10/pom.xml",
          "external/kinesis-asl-assembly/pom.xml",
          "external/kinesis-asl/pom.xml",
          "external/spark-ganglia-lgpl/pom.xml",
          "graphx/pom.xml",
          "hadoop-cloud/pom.xml",
          "launcher/pom.xml",
          "mllib-local/pom.xml",
          "mllib/pom.xml",
          "pom.xml",
          "python/pyspark/version.py",
          "repl/pom.xml",
          "resource-managers/kubernetes/core/pom.xml",
          "resource-managers/kubernetes/integration-tests/pom.xml",
          "resource-managers/mesos/pom.xml",
          "resource-managers/yarn/pom.xml",
          "sql/catalyst/pom.xml",
          "sql/core/pom.xml",
          "sql/hive-thriftserver/pom.xml",
          "sql/hive/pom.xml",
          "streaming/pom.xml",
          "tools/pom.xml"
        ],
        "message": "Preparing development version 3.3.1-SNAPSHOT",
        "before_after_code_files": [
          "python/pyspark/version.py||python/pyspark/version.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/version.py||python/pyspark/version.py": [
          "File: python/pyspark/version.py -> python/pyspark/version.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # See the License for the specific language governing permissions and",
          "17: # limitations under the License.",
          "",
          "[Removed Lines]",
          "19: __version__: str = \"3.3.0\"",
          "",
          "[Added Lines]",
          "19: __version__: str = \"3.3.1.dev0\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cf7e3574efc1d4bb7233f18fcf344e94d26c2ac1",
      "candidate_info": {
        "commit_hash": "cf7e3574efc1d4bb7233f18fcf344e94d26c2ac1",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/cf7e3574efc1d4bb7233f18fcf344e94d26c2ac1",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala"
        ],
        "message": "[SPARK-38825][SQL][TEST] Add a test to cover parquet notIn filter\n\n### What changes were proposed in this pull request?\nCurrently we don't have a test for parquet `notIn` filter, so add a test for this\n\n### Why are the changes needed?\nto make tests more complete\n\n### Does this PR introduce _any_ user-facing change?\nno\n\n### How was this patch tested?\nnew test\n\nCloses #36109 from huaxingao/inFilter.\n\nAuthored-by: huaxingao <huaxin_gao@apple.com>\nSigned-off-by: huaxingao <huaxin_gao@apple.com>\n(cherry picked from commit d6fd0405b60875ac5e2c9daee1ec785f74e9b7a3)\nSigned-off-by: huaxingao <huaxin_gao@apple.com>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1901:       }",
          "1902:     }",
          "1903:   }",
          "1904: }",
          "1906: @ExtendedSQLTest",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1905:   test(\"SPARK-38825: in and notIn filters\") {",
          "1906:     import testImplicits._",
          "1907:     withTempPath { file =>",
          "1908:       Seq(1, 2, 0, -1, 99, 1000, 3, 7, 2).toDF(\"id\").coalesce(1).write.mode(\"overwrite\")",
          "1909:         .parquet(file.getCanonicalPath)",
          "1910:       var df = spark.read.parquet(file.getCanonicalPath)",
          "1911:       var in = df.filter(col(\"id\").isin(100, 3, 11, 12, 13))",
          "1912:       var notIn = df.filter(!col(\"id\").isin(100, 3, 11, 12, 13))",
          "1913:       checkAnswer(in, Seq(Row(3)))",
          "1914:       checkAnswer(notIn, Seq(Row(1), Row(2), Row(0), Row(-1), Row(99), Row(1000), Row(7), Row(2)))",
          "1916:       Seq(\"mary\", \"martin\", \"lucy\", \"alex\", \"mary\", \"dan\").toDF(\"name\").coalesce(1)",
          "1917:         .write.mode(\"overwrite\").parquet(file.getCanonicalPath)",
          "1918:       df = spark.read.parquet(file.getCanonicalPath)",
          "1919:       in = df.filter(col(\"name\").isin(\"mary\", \"victor\", \"leo\", \"alex\"))",
          "1920:       notIn = df.filter(!col(\"name\").isin(\"mary\", \"victor\", \"leo\", \"alex\"))",
          "1921:       checkAnswer(in, Seq(Row(\"mary\"), Row(\"alex\"), Row(\"mary\")))",
          "1922:       checkAnswer(notIn, Seq(Row(\"martin\"), Row(\"lucy\"), Row(\"dan\")))",
          "1923:     }",
          "1924:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}