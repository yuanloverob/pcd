{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "364a4f52610fdacdefc2d16af984900c55f8e31b",
      "candidate_info": {
        "commit_hash": "364a4f52610fdacdefc2d16af984900c55f8e31b",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/364a4f52610fdacdefc2d16af984900c55f8e31b",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala"
        ],
        "message": "[SPARK-39612][SQL][TESTS] DataFrame.exceptAll followed by count should work\n\n### What changes were proposed in this pull request?\n\nThis PR adds a test case broken by https://github.com/apache/spark/commit/4b9343593eca780ca30ffda45244a71413577884 which was reverted in https://github.com/apache/spark/commit/161c596cafea9c235b5c918d8999c085401d73a9.\n\n### Why are the changes needed?\n\nTo prevent a regression in the future. This was a regression in Apache Spark 3.3 that used to work in Apache Spark 3.2.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, it makes `DataFrame.exceptAll` followed by `count` working.\n\n### How was this patch tested?\n\nThe unit test was added.\n\nCloses #37084 from HyukjinKwon/SPARK-39612.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 947e271402f749f6f58b79fecd59279eaf86db57)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "3215:       }",
          "3216:     }",
          "3217:   }",
          "3218: }",
          "3220: case class GroupByKey(a: Int, b: Int)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3219:   test(\"SPARK-39612: exceptAll with following count should work\") {",
          "3220:     val d1 = Seq(\"a\").toDF",
          "3221:     assert(d1.exceptAll(d1).count() === 0)",
          "3222:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a4b420cacd4eef93ea661d31d886fe3b60d5fe64",
      "candidate_info": {
        "commit_hash": "a4b420cacd4eef93ea661d31d886fe3b60d5fe64",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/a4b420cacd4eef93ea661d31d886fe3b60d5fe64",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/conditionalExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/nullExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala",
          "sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql",
          "sql/core/src/test/resources/sql-tests/inputs/postgreSQL/case.sql",
          "sql/core/src/test/resources/sql-tests/inputs/udf/postgreSQL/udf-case.sql",
          "sql/core/src/test/resources/sql-tests/results/ansi/conditional-functions.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/case.sql.out",
          "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-case.sql.out"
        ],
        "message": "[SPARK-39106][SQL] Correct conditional expression constant folding\n\n- add try-catch when we fold children inside `ConditionalExpression` if it's not foldable\n- mark `CaseWhen` and `If` as foldable if it's children are foldable\n\nFor a conditional expression, we should add a try-catch to partially fold the constant inside it's children because some bracnhes may no be evaluated at runtime. For example if c1 or c2 is not null, the last branch should be never hit:\n```sql\nSELECT COALESCE(c1, c2, 1/0);\n```\nBesides, for CaseWhen and If, we should mark it as foldable if it's children are foldable. It is safe since the both non-codegen and codegen code path have already respected the evaluation order.\n\nyes, bug fix\n\nadd more test in sql file\n\nCloses #36468 from ulysses-you/SPARK-39106.\n\nAuthored-by: ulysses-you <ulyssesyou18@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 08a4ade8ba881589da0741b3ffacd3304dc1e9b5)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/conditionalExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/conditionalExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/nullExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/nullExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala",
          "sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql||sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql",
          "sql/core/src/test/resources/sql-tests/inputs/postgreSQL/case.sql||sql/core/src/test/resources/sql-tests/inputs/postgreSQL/case.sql",
          "sql/core/src/test/resources/sql-tests/inputs/udf/postgreSQL/udf-case.sql||sql/core/src/test/resources/sql-tests/inputs/udf/postgreSQL/udf-case.sql"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "461: trait ConditionalExpression extends Expression {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "462:   final override def foldable: Boolean = children.forall(_.foldable)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/conditionalExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/conditionalExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/conditionalExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/conditionalExpressions.scala"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/nullExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/nullExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/nullExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/nullExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "54:   override def nullable: Boolean = children.forall(_.nullable)",
          "59:   final override val nodePatterns: Seq[TreePattern] = Seq(COALESCE)",
          "61:   override def checkInputDataTypes(): TypeCheckResult = {",
          "",
          "[Removed Lines]",
          "57:   override def foldable: Boolean = children.forall(_.foldable)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import scala.collection.immutable.HashSet",
          "21: import scala.collection.mutable.{ArrayBuffer, Stack}",
          "23: import org.apache.spark.sql.catalyst.analysis._",
          "24: import org.apache.spark.sql.catalyst.expressions.{MultiLikeBase, _}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: import scala.util.control.NonFatal",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "28: import org.apache.spark.sql.catalyst.plans._",
          "29: import org.apache.spark.sql.catalyst.plans.logical._",
          "30: import org.apache.spark.sql.catalyst.rules._",
          "32: import org.apache.spark.sql.catalyst.trees.TreePattern._",
          "33: import org.apache.spark.sql.types._",
          "34: import org.apache.spark.unsafe.types.UTF8String",
          "",
          "[Removed Lines]",
          "31: import org.apache.spark.sql.catalyst.trees.AlwaysProcess",
          "",
          "[Added Lines]",
          "32: import org.apache.spark.sql.catalyst.trees.{AlwaysProcess, TreeNodeTag}",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "45: object ConstantFolding extends Rule[LogicalPlan] {",
          "47:   private def hasNoSideEffect(e: Expression): Boolean = e match {",
          "48:     case _: Attribute => true",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "49:   private[sql] val FAILED_TO_EVALUATE = TreeNodeTag[Unit](\"FAILED_TO_EVALUATE\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "52:     case _ => false",
          "53:   }",
          "71:   }",
          "72: }",
          "",
          "[Removed Lines]",
          "55:   def apply(plan: LogicalPlan): LogicalPlan = plan.transformWithPruning(AlwaysProcess.fn, ruleId) {",
          "56:     case q: LogicalPlan => q.transformExpressionsDownWithPruning(",
          "57:       AlwaysProcess.fn, ruleId) {",
          "61:       case l: Literal => l",
          "63:       case Size(c: CreateArray, _) if c.children.forall(hasNoSideEffect) =>",
          "64:         Literal(c.children.length)",
          "65:       case Size(c: CreateMap, _) if c.children.forall(hasNoSideEffect) =>",
          "66:         Literal(c.children.length / 2)",
          "69:       case e if e.foldable => Literal.create(e.eval(EmptyRow), e.dataType)",
          "70:     }",
          "",
          "[Added Lines]",
          "59:   private def constantFolding(",
          "60:       e: Expression,",
          "61:       isConditionalBranch: Boolean = false): Expression = e match {",
          "62:     case c: ConditionalExpression if !c.foldable =>",
          "63:       c.mapChildren(constantFolding(_, isConditionalBranch = true))",
          "68:     case l: Literal => l",
          "70:     case Size(c: CreateArray, _) if c.children.forall(hasNoSideEffect) =>",
          "71:       Literal(c.children.length)",
          "72:     case Size(c: CreateMap, _) if c.children.forall(hasNoSideEffect) =>",
          "73:       Literal(c.children.length / 2)",
          "75:     case e if e.getTagValue(FAILED_TO_EVALUATE).isDefined => e",
          "78:     case e if e.foldable =>",
          "79:       try {",
          "80:         Literal.create(e.eval(EmptyRow), e.dataType)",
          "81:       } catch {",
          "82:         case NonFatal(_) if isConditionalBranch =>",
          "86:           e.setTagValue(FAILED_TO_EVALUATE, ())",
          "87:           e",
          "88:       }",
          "90:     case other => other.mapChildren(constantFolding(_, isConditionalBranch))",
          "91:   }",
          "93:   def apply(plan: LogicalPlan): LogicalPlan = plan.transformWithPruning(AlwaysProcess.fn, ruleId) {",
          "94:     case q: LogicalPlan => q.mapExpressions(constantFolding(_))",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.catalyst.optimizer",
          "20: import org.apache.spark.sql.catalyst.analysis.{EliminateSubqueryAliases, UnresolvedExtractValue}",
          "21: import org.apache.spark.sql.catalyst.dsl.expressions._",
          "22: import org.apache.spark.sql.catalyst.dsl.plans._",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import org.apache.spark.SparkArithmeticException",
          "21: import org.apache.spark.sql.Row",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "25: import org.apache.spark.sql.catalyst.plans.PlanTest",
          "26: import org.apache.spark.sql.catalyst.plans.logical.{LocalRelation, LogicalPlan}",
          "27: import org.apache.spark.sql.catalyst.rules.RuleExecutor",
          "28: import org.apache.spark.sql.types._",
          "29: import org.apache.spark.unsafe.types.ByteArray",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "30: import org.apache.spark.sql.internal.SQLConf",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "331:     comparePlans(optimized, correctAnswer)",
          "332:   }",
          "333: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "337:   test(\"SPARK-39106: Correct conditional expression constant folding\") {",
          "338:     val t = LocalRelation.fromExternalRows(",
          "339:       $\"c\".double :: Nil,",
          "340:       Row(1d) :: Row(null) :: Row(Double.NaN) :: Nil)",
          "342:     withSQLConf(SQLConf.ANSI_ENABLED.key -> \"true\") {",
          "344:       Seq(",
          "345:         t.select(CaseWhen((Divide(1, 0) === 1, Add(1, 0)) :: Nil, Subtract(1, 0))),",
          "346:         t.select(If(Divide(1, 0) === 1, Add(1, 0), Add(1, 0))),",
          "347:         t.select(Coalesce(Divide(1, 0) :: Add(1, 0) :: Nil)),",
          "348:         t.select(NaNvl(Divide(1, 0), Add(1, 0)))",
          "349:       ).foreach { query =>",
          "350:         intercept[SparkArithmeticException] {",
          "351:           Optimize.execute(query.analyze)",
          "352:         }",
          "353:       }",
          "356:       Seq(",
          "357:         t.select(CaseWhen(($\"c\" === 1d, Divide(1, 0)) :: Nil, 1d)),",
          "358:         t.select(If($\"c\" === 1d, Divide(1, 0), 1d)),",
          "359:         t.select(Coalesce($\"c\" :: Divide(1, 0) :: Nil)),",
          "360:         t.select(NaNvl($\"c\", Divide(1, 0)))",
          "361:       ).foreach { query =>",
          "362:         val optimized = Optimize.execute(query.analyze)",
          "363:         val failedToEvaluated = optimized.expressions.flatMap(_.collect {",
          "364:           case e: Expression if e.getTagValue(ConstantFolding.FAILED_TO_EVALUATE).isDefined => e",
          "365:         })",
          "366:         assert(failedToEvaluated.size == 1)",
          "367:         comparePlans(query.analyze, optimized)",
          "368:       }",
          "369:     }",
          "370:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql||sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql -> sql/core/src/test/resources/sql-tests/inputs/ansi/conditional-functions.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: -- Tests for conditional functions",
          "6: DROP TABLE IF EXISTS t;",
          "",
          "[Removed Lines]",
          "2: CREATE TABLE t USING PARQUET AS SELECT c1, c2 FROM VALUES(1, 0),(2, 1) AS t(c1, c2);",
          "4: SELECT nanvl(c1, c1/c2 + c1/c2) FROM t;",
          "",
          "[Added Lines]",
          "3: CREATE TABLE t USING PARQUET AS SELECT c1, c2 FROM VALUES(1d, 0),(2d, 1),(null, 1),(CAST('NaN' AS DOUBLE), 0) AS t(c1, c2);",
          "5: SELECT nanvl(c2, c1/c2 + c1/c2) FROM t;",
          "6: SELECT nanvl(c2, 1/0) FROM t;",
          "7: SELECT nanvl(1-0, 1/0) FROM t;",
          "9: SELECT if(c2 >= 0, 1-0, 1/0) from t;",
          "10: SELECT if(1 == 1, 1, 1/0);",
          "11: SELECT if(1 != 1, 1/0, 1);",
          "13: SELECT coalesce(c2, 1/0) from t;",
          "14: SELECT coalesce(1, 1/0);",
          "15: SELECT coalesce(null, 1, 1/0);",
          "17: SELECT case when c2 >= 0 then 1 else 1/0 end from t;",
          "18: SELECT case when 1 < 2 then 1 else 1/0 end;",
          "19: SELECT case when 1 > 2 then 1/0 else 1 end;",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/postgreSQL/case.sql||sql/core/src/test/resources/sql-tests/inputs/postgreSQL/case.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/postgreSQL/case.sql -> sql/core/src/test/resources/sql-tests/inputs/postgreSQL/case.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "65:   CASE WHEN rand() < 0 THEN 1",
          "66:   END AS `NULL on no matches`;",
          "69: -- Constant-expression folding shouldn't evaluate unreachable subexpressions",
          "70: SELECT CASE WHEN 1=0 THEN 1/0 WHEN 1=1 THEN 1 ELSE 2/0 END;",
          "71: SELECT CASE 1 WHEN 0 THEN 1/0 WHEN 1 THEN 1 ELSE 2/0 END;",
          "",
          "[Removed Lines]",
          "68: -- [SPARK-33008] Spark SQL throws an exception",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/udf/postgreSQL/udf-case.sql||sql/core/src/test/resources/sql-tests/inputs/udf/postgreSQL/udf-case.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/udf/postgreSQL/udf-case.sql -> sql/core/src/test/resources/sql-tests/inputs/udf/postgreSQL/udf-case.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "67:   CASE WHEN rand() < udf(0) THEN 1",
          "68:   END AS `NULL on no matches`;",
          "71: -- Constant-expression folding shouldn't evaluate unreachable subexpressions",
          "72: SELECT CASE WHEN udf(1=0) THEN 1/0 WHEN 1=1 THEN 1 ELSE 2/0 END;",
          "73: SELECT CASE 1 WHEN 0 THEN 1/udf(0) WHEN 1 THEN 1 ELSE 2/0 END;",
          "79: -- Test for cases involving untyped literals in test expression",
          "80: SELECT CASE 'a' WHEN 'a' THEN udf(1) ELSE udf(2) END;",
          "",
          "[Removed Lines]",
          "70: -- [SPARK-33008] Spark SQL throws an exception",
          "75: -- However we do not currently suppress folding of potentially",
          "76: -- reachable subexpressions",
          "77: SELECT CASE WHEN i > 100 THEN udf(1/0) ELSE udf(0) END FROM case_tbl;",
          "",
          "[Added Lines]",
          "74: -- SPARK-39122: Python UDF does not follow the conditional expression evaluation order",
          "75: -- SELECT CASE WHEN i > 100 THEN udf(1/0) ELSE udf(0) END FROM case_tbl;",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8f9aa50f730a48c1697cbb4f4c9ba707963a37a1",
      "candidate_info": {
        "commit_hash": "8f9aa50f730a48c1697cbb4f4c9ba707963a37a1",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/8f9aa50f730a48c1697cbb4f4c9ba707963a37a1",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "sql/core/src/test/resources/sql-tests/results/explain-aqe.sql.out",
          "sql/core/src/test/resources/sql-tests/results/explain.sql.out"
        ],
        "message": "[SPARK-34863][SQL][FOLLOWUP] Disable `spark.sql.parquet.enableNestedColumnVectorizedReader` by default\n\n### What changes were proposed in this pull request?\n\nThis PR disables `spark.sql.parquet.enableNestedColumnVectorizedReader` by default.\n\n### Why are the changes needed?\n\nIn #34659 the config was turned mainly for testing reason. As the feature is new, we should turn it off by default.\n\n### Does this PR introduce _any_ user-facing change?\n\nThe config `spark.sql.parquet.enableNestedColumnVectorizedReader` is turned off by default now.\n\n### How was this patch tested?\n\nExisting tests.\n\nCloses #36055 from sunchao/disable.\n\nAuthored-by: Chao Sun <sunchao@apple.com>\nSigned-off-by: Liang-Chi Hsieh <viirya@gmail.com>\n(cherry picked from commit 1b08673a6d92e3e0fceb4a686a0ba77a87f1ebbc)\nSigned-off-by: Liang-Chi Hsieh <viirya@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1015:           s\"Requires ${PARQUET_VECTORIZED_READER_ENABLED.key} to be enabled.\")",
          "1016:       .version(\"3.3.0\")",
          "1017:       .booleanConf",
          "1020:   val PARQUET_RECORD_FILTER_ENABLED = buildConf(\"spark.sql.parquet.recordLevelFilter.enabled\")",
          "1021:     .doc(\"If true, enables Parquet's native record-level filtering using the pushed down \" +",
          "",
          "[Removed Lines]",
          "1018:       .createWithDefault(true)",
          "",
          "[Added Lines]",
          "1018:       .createWithDefault(false)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7101e88201d88cf24057187f360c828d1b376589",
      "candidate_info": {
        "commit_hash": "7101e88201d88cf24057187f360c828d1b376589",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/7101e88201d88cf24057187f360c828d1b376589",
        "files": [
          "python/pyspark/ml/feature.py"
        ],
        "message": "[SPARK-37405][FOLLOW-UP][PYTHON][ML] Move _input_kwargs hints to consistent positions\n\n### What changes were proposed in this pull request?\nThis PR moves `_input_kwargs` hints to beginning of the bodies of the annotated classes.\n\n### Why are the changes needed?\nConsistency with other modules.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nExisting tests.\n\nCloses #36203 from zero323/SPARK-37405-FOLLOW-UP.\n\nAuthored-by: zero323 <mszymkiewicz@gmail.com>\nSigned-off-by: zero323 <mszymkiewicz@gmail.com>\n(cherry picked from commit 797abc069348a2770742d5b57fd8c0fab0abe8d4)\nSigned-off-by: zero323 <mszymkiewicz@gmail.com>",
        "before_after_code_files": [
          "python/pyspark/ml/feature.py||python/pyspark/ml/feature.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/ml/feature.py||python/pyspark/ml/feature.py": [
          "File: python/pyspark/ml/feature.py -> python/pyspark/ml/feature.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "177:     ...",
          "178:     \"\"\"",
          "180:     threshold: Param[float] = Param(",
          "181:         Params._dummy(),",
          "182:         \"threshold\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "180:     _input_kwargs: Dict[str, Any]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "195:         typeConverter=TypeConverters.toListFloat,",
          "196:     )",
          "200:     @overload",
          "201:     def __init__(",
          "202:         self,",
          "",
          "[Removed Lines]",
          "198:     _input_kwargs: Dict[str, Any]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "721:     ...",
          "722:     \"\"\"",
          "724:     splits: Param[List[float]] = Param(",
          "725:         Params._dummy(),",
          "726:         \"splits\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "724:     _input_kwargs: Dict[str, Any]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "762:         typeConverter=TypeConverters.toListListFloat,",
          "763:     )",
          "767:     @overload",
          "768:     def __init__(",
          "769:         self,",
          "",
          "[Removed Lines]",
          "765:     _input_kwargs: Dict[str, Any]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1284:     False",
          "1285:     \"\"\"",
          "1287:     inverse: Param[bool] = Param(",
          "1288:         Params._dummy(),",
          "1289:         \"inverse\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1287:     _input_kwargs: Dict[str, Any]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1291:         typeConverter=TypeConverters.toBoolean,",
          "1292:     )",
          "1296:     @keyword_only",
          "1297:     def __init__(",
          "1298:         self,",
          "",
          "[Removed Lines]",
          "1294:     _input_kwargs: Dict[str, Any]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "1392:     True",
          "1393:     \"\"\"",
          "1395:     scalingVec: Param[Vector] = Param(",
          "1396:         Params._dummy(),",
          "1397:         \"scalingVec\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1395:     _input_kwargs: Dict[str, Any]",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "1399:         typeConverter=TypeConverters.toVector,",
          "1400:     )",
          "1404:     @keyword_only",
          "1405:     def __init__(",
          "1406:         self,",
          "",
          "[Removed Lines]",
          "1402:     _input_kwargs: Dict[str, Any]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "1528:     True",
          "1529:     \"\"\"",
          "1531:     categoricalCols: Param[List[str]] = Param(",
          "1532:         Params._dummy(),",
          "1533:         \"categoricalCols\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1531:     _input_kwargs: Dict[str, Any]",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1535:         typeConverter=TypeConverters.toListString,",
          "1536:     )",
          "1540:     @keyword_only",
          "1541:     def __init__(",
          "1542:         self,",
          "",
          "[Removed Lines]",
          "1538:     _input_kwargs: Dict[str, Any]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "1650:     5",
          "1651:     \"\"\"",
          "1653:     binary: Param[bool] = Param(",
          "1654:         Params._dummy(),",
          "1655:         \"binary\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1653:     _input_kwargs: Dict[str, Any]",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "1659:         typeConverter=TypeConverters.toBoolean,",
          "1660:     )",
          "1664:     @keyword_only",
          "1665:     def __init__(",
          "1666:         self,",
          "",
          "[Removed Lines]",
          "1662:     _input_kwargs: Dict[str, Any]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "2882:     True",
          "2883:     \"\"\"",
          "2885:     n: Param[int] = Param(",
          "2886:         Params._dummy(),",
          "2887:         \"n\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2885:     _input_kwargs: Dict[str, Any]",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "2889:         typeConverter=TypeConverters.toInt,",
          "2890:     )",
          "2894:     @keyword_only",
          "2895:     def __init__(",
          "2896:         self, *, n: int = 2, inputCol: Optional[str] = None, outputCol: Optional[str] = None",
          "",
          "[Removed Lines]",
          "2892:     _input_kwargs: Dict[str, Any]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "2982:     True",
          "2983:     \"\"\"",
          "2987:     _input_kwargs: Dict[str, Any]",
          "2989:     @keyword_only",
          "2990:     def __init__(",
          "2991:         self, *, p: float = 2.0, inputCol: Optional[str] = None, outputCol: Optional[str] = None",
          "",
          "[Removed Lines]",
          "2985:     p = Param(Params._dummy(), \"p\", \"the p norm value.\", typeConverter=TypeConverters.toFloat)",
          "",
          "[Added Lines]",
          "2987:     p = Param(Params._dummy(), \"p\", \"the p norm value.\", typeConverter=TypeConverters.toFloat)",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "3378:     True",
          "3379:     \"\"\"",
          "3381:     degree: Param[int] = Param(",
          "3382:         Params._dummy(),",
          "3383:         \"degree\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3381:     _input_kwargs: Dict[str, Any]",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "3385:         typeConverter=TypeConverters.toInt,",
          "3386:     )",
          "3390:     @keyword_only",
          "3391:     def __init__(",
          "3392:         self, *, degree: int = 2, inputCol: Optional[str] = None, outputCol: Optional[str] = None",
          "",
          "[Removed Lines]",
          "3388:     _input_kwargs: Dict[str, Any]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "3546:     ...",
          "3547:     \"\"\"",
          "3549:     numBuckets: Param[int] = Param(",
          "3550:         Params._dummy(),",
          "3551:         \"numBuckets\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3549:     _input_kwargs: Dict[str, Any]",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "3579:         typeConverter=TypeConverters.toListInt,",
          "3580:     )",
          "3584:     @overload",
          "3585:     def __init__(",
          "3586:         self,",
          "",
          "[Removed Lines]",
          "3582:     _input_kwargs: Dict[str, Any]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "4076:     True",
          "4077:     \"\"\"",
          "4079:     minTokenLength: Param[int] = Param(",
          "4080:         Params._dummy(),",
          "4081:         \"minTokenLength\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4079:     _input_kwargs: Dict[str, Any]",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "4100:         typeConverter=TypeConverters.toBoolean,",
          "4101:     )",
          "4105:     @keyword_only",
          "4106:     def __init__(",
          "4107:         self,",
          "",
          "[Removed Lines]",
          "4103:     _input_kwargs: Dict[str, Any]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "4237:     True",
          "4238:     \"\"\"",
          "4240:     statement = Param(",
          "4241:         Params._dummy(), \"statement\", \"SQL statement\", typeConverter=TypeConverters.toString",
          "4242:     )",
          "4246:     @keyword_only",
          "4247:     def __init__(self, *, statement: Optional[str] = None):",
          "4248:         \"\"\"",
          "",
          "[Removed Lines]",
          "4244:     _input_kwargs: Dict[str, Any]",
          "",
          "[Added Lines]",
          "4240:     _input_kwargs: Dict[str, Any]",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "4874:     StringIndexer : for converting categorical values into category indices",
          "4875:     \"\"\"",
          "4877:     labels: Param[List[str]] = Param(",
          "4878:         Params._dummy(),",
          "4879:         \"labels\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4877:     _input_kwargs: Dict[str, Any]",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "4882:         typeConverter=TypeConverters.toListString,",
          "4883:     )",
          "4887:     @keyword_only",
          "4888:     def __init__(",
          "4889:         self,",
          "",
          "[Removed Lines]",
          "4885:     _input_kwargs: Dict[str, Any]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "4996:     ...",
          "4997:     \"\"\"",
          "4999:     stopWords: Param[List[str]] = Param(",
          "5000:         Params._dummy(),",
          "5001:         \"stopWords\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4999:     _input_kwargs: Dict[str, Any]",
          "",
          "---------------",
          "--- Hunk 26 ---",
          "[Context before]",
          "5015:         typeConverter=TypeConverters.toString,",
          "5016:     )",
          "5020:     @overload",
          "5021:     def __init__(",
          "5022:         self,",
          "",
          "[Removed Lines]",
          "5018:     _input_kwargs: Dict[str, Any]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 27 ---",
          "[Context before]",
          "5327:     ...",
          "5328:     \"\"\"",
          "5330:     handleInvalid: Param[str] = Param(",
          "5331:         Params._dummy(),",
          "5332:         \"handleInvalid\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "5330:     _input_kwargs: Dict[str, Any]",
          "",
          "---------------",
          "--- Hunk 28 ---",
          "[Context before]",
          "5341:         typeConverter=TypeConverters.toString,",
          "5342:     )",
          "5346:     @keyword_only",
          "5347:     def __init__(",
          "5348:         self,",
          "",
          "[Removed Lines]",
          "5344:     _input_kwargs: Dict[str, Any]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 29 ---",
          "[Context before]",
          "5690:     True",
          "5691:     \"\"\"",
          "5693:     indices: Param[List[int]] = Param(",
          "5694:         Params._dummy(),",
          "5695:         \"indices\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "5693:     _input_kwargs: Dict[str, Any]",
          "",
          "---------------",
          "--- Hunk 30 ---",
          "[Context before]",
          "5707:         typeConverter=TypeConverters.toListString,",
          "5708:     )",
          "5712:     @keyword_only",
          "5713:     def __init__(",
          "5714:         self,",
          "",
          "[Removed Lines]",
          "5710:     _input_kwargs: Dict[str, Any]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 31 ---",
          "[Context before]",
          "6909:     True",
          "6910:     \"\"\"",
          "6912:     size: Param[int] = Param(",
          "6913:         Params._dummy(), \"size\", \"Size of vectors in column.\", typeConverter=TypeConverters.toInt",
          "6914:     )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "6912:     _input_kwargs: Dict[str, Any]",
          "",
          "---------------",
          "--- Hunk 32 ---",
          "[Context before]",
          "6924:         TypeConverters.toString,",
          "6925:     )",
          "6929:     @keyword_only",
          "6930:     def __init__(",
          "6931:         self,",
          "",
          "[Removed Lines]",
          "6927:     _input_kwargs: Dict[str, Any]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0e2758c9955c2ae102e37e0b49aa9446bbe6fecf",
      "candidate_info": {
        "commit_hash": "0e2758c9955c2ae102e37e0b49aa9446bbe6fecf",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/0e2758c9955c2ae102e37e0b49aa9446bbe6fecf",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/RegexpExpressionsSuite.scala",
          "sql/core/src/test/resources/sql-tests/inputs/regexp-functions.sql",
          "sql/core/src/test/resources/sql-tests/results/regexp-functions.sql.out"
        ],
        "message": "[SPARK-39758][SQL][3.3] Fix NPE from the regexp functions on invalid patterns\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to catch `PatternSyntaxException` while compiling the regexp pattern by the `regexp_extract`, `regexp_extract_all` and `regexp_instr`, and substitute the exception by Spark's exception w/ the error class `INVALID_PARAMETER_VALUE`. In this way, Spark SQL will output the error in the form:\n```sql\norg.apache.spark.SparkRuntimeException\n[INVALID_PARAMETER_VALUE] The value of parameter(s) 'regexp' in `regexp_instr` is invalid: ) ?\n```\ninstead of (on Spark 3.3.0):\n```java\njava.lang.NullPointerException: null\n```\nAlso I propose to set `lastRegex` only after the compilation of the regexp pattern completes successfully.\n\nThis is a backport of https://github.com/apache/spark/pull/37171.\n\n### Why are the changes needed?\nThe changes fix NPE portrayed by the code on Spark 3.3.0:\n```sql\nspark-sql> SELECT regexp_extract('1a 2b 14m', '(?l)');\n22/07/12 19:07:21 ERROR SparkSQLDriver: Failed in [SELECT regexp_extract('1a 2b 14m', '(?l)')]\njava.lang.NullPointerException: null\n\tat org.apache.spark.sql.catalyst.expressions.RegExpExtractBase.getLastMatcher(regexpExpressions.scala:768) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]\n```\nThis should improve user experience with Spark SQL.\n\n### Does this PR introduce _any_ user-facing change?\nNo. In regular cases, the behavior is the same but users will observe different exceptions (error messages) after the changes.\n\n### How was this patch tested?\nBy running new tests:\n```\n$ build/sbt \"sql/testOnly org.apache.spark.sql.SQLQueryTestSuite -- -z regexp-functions.sql\"\n$ build/sbt \"test:testOnly *.RegexpExpressionsSuite\"\n$ build/sbt \"sql/test:testOnly org.apache.spark.sql.expressions.ExpressionInfoSuite\"\n```\n\nAuthored-by: Max Gekk <max.gekkgmail.com>\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n(cherry picked from commit 5b96bd5cf8f44eee7a16cd027d37dec552ed5a6a)\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n\nCloses #37181 from MaxGekk/pattern-syntax-exception-3.3.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/RegexpExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/RegexpExpressionsSuite.scala",
          "sql/core/src/test/resources/sql-tests/inputs/regexp-functions.sql||sql/core/src/test/resources/sql-tests/inputs/regexp-functions.sql"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.catalyst.expressions",
          "20: import java.util.Locale",
          "23: import scala.collection.JavaConverters._",
          "24: import scala.collection.mutable.ArrayBuffer",
          "",
          "[Removed Lines]",
          "21: import java.util.regex.{Matcher, MatchResult, Pattern}",
          "",
          "[Added Lines]",
          "21: import java.util.regex.{Matcher, MatchResult, Pattern, PatternSyntaxException}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "752:   protected def getLastMatcher(s: Any, p: Any): Matcher = {",
          "753:     if (p != lastRegex) {",
          "757:     }",
          "758:     pattern.matcher(s.toString)",
          "759:   }",
          "760: }",
          "",
          "[Removed Lines]",
          "755:       lastRegex = p.asInstanceOf[UTF8String].clone()",
          "756:       pattern = Pattern.compile(lastRegex.toString)",
          "",
          "[Added Lines]",
          "755:       try {",
          "756:         val r = p.asInstanceOf[UTF8String].clone()",
          "757:         pattern = Pattern.compile(r.toString)",
          "758:         lastRegex = r",
          "759:       } catch {",
          "760:         case e: PatternSyntaxException =>",
          "761:           throw QueryExecutionErrors.invalidPatternError(prettyName, e.getPattern)",
          "763:       }",
          "768:   protected def initLastMatcherCode(",
          "769:       ctx: CodegenContext,",
          "770:       subject: String,",
          "771:       regexp: String,",
          "772:       matcher: String): String = {",
          "773:     val classNamePattern = classOf[Pattern].getCanonicalName",
          "774:     val termLastRegex = ctx.addMutableState(\"UTF8String\", \"lastRegex\")",
          "775:     val termPattern = ctx.addMutableState(classNamePattern, \"pattern\")",
          "777:     s\"\"\"",
          "778:       |if (!$regexp.equals($termLastRegex)) {",
          "779:       |  // regex value changed",
          "780:       |  try {",
          "781:       |    UTF8String r = $regexp.clone();",
          "782:       |    $termPattern = $classNamePattern.compile(r.toString());",
          "783:       |    $termLastRegex = r;",
          "784:       |  } catch (java.util.regex.PatternSyntaxException e) {",
          "785:       |    throw QueryExecutionErrors.invalidPatternError(\"$prettyName\", e.getPattern());",
          "786:       |  }",
          "787:       |}",
          "788:       |java.util.regex.Matcher $matcher = $termPattern.matcher($subject.toString());",
          "789:       |\"\"\".stripMargin",
          "790:   }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "818:   override def prettyName: String = \"regexp_extract\"",
          "820:   override protected def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {",
          "822:     val classNameRegExpExtractBase = classOf[RegExpExtractBase].getCanonicalName",
          "823:     val matcher = ctx.freshName(\"matcher\")",
          "824:     val matchResult = ctx.freshName(\"matchResult\")",
          "829:     val setEvNotNull = if (nullable) {",
          "830:       s\"${ev.isNull} = false;\"",
          "831:     } else {",
          "",
          "[Removed Lines]",
          "821:     val classNamePattern = classOf[Pattern].getCanonicalName",
          "826:     val termLastRegex = ctx.addMutableState(\"UTF8String\", \"lastRegex\")",
          "827:     val termPattern = ctx.addMutableState(classNamePattern, \"pattern\")",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "835:     nullSafeCodeGen(ctx, ev, (subject, regexp, idx) => {",
          "836:       s\"\"\"",
          "844:       if ($matcher.find()) {",
          "845:         java.util.regex.MatchResult $matchResult = $matcher.toMatchResult();",
          "846:         $classNameRegExpExtractBase.checkGroupIndex($matchResult.groupCount(), $idx);",
          "",
          "[Removed Lines]",
          "837:       if (!$regexp.equals($termLastRegex)) {",
          "839:         $termLastRegex = $regexp.clone();",
          "840:         $termPattern = $classNamePattern.compile($termLastRegex.toString());",
          "841:       }",
          "842:       java.util.regex.Matcher $matcher =",
          "843:         $termPattern.matcher($subject.toString());",
          "",
          "[Added Lines]",
          "863:       ${initLastMatcherCode(ctx, subject, regexp, matcher)}",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "922:   override def prettyName: String = \"regexp_extract_all\"",
          "924:   override protected def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {",
          "926:     val classNameRegExpExtractBase = classOf[RegExpExtractBase].getCanonicalName",
          "927:     val arrayClass = classOf[GenericArrayData].getName",
          "928:     val matcher = ctx.freshName(\"matcher\")",
          "929:     val matchResult = ctx.freshName(\"matchResult\")",
          "930:     val matchResults = ctx.freshName(\"matchResults\")",
          "935:     val setEvNotNull = if (nullable) {",
          "936:       s\"${ev.isNull} = false;\"",
          "937:     } else {",
          "",
          "[Removed Lines]",
          "925:     val classNamePattern = classOf[Pattern].getCanonicalName",
          "932:     val termLastRegex = ctx.addMutableState(\"UTF8String\", \"lastRegex\")",
          "933:     val termPattern = ctx.addMutableState(classNamePattern, \"pattern\")",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "939:     }",
          "940:     nullSafeCodeGen(ctx, ev, (subject, regexp, idx) => {",
          "941:       s\"\"\"",
          "948:          | java.util.ArrayList $matchResults = new java.util.ArrayList<UTF8String>();",
          "949:          | while ($matcher.find()) {",
          "950:          |   java.util.regex.MatchResult $matchResult = $matcher.toMatchResult();",
          "",
          "[Removed Lines]",
          "942:          | if (!$regexp.equals($termLastRegex)) {",
          "943:          |   // regex value changed",
          "944:          |   $termLastRegex = $regexp.clone();",
          "945:          |   $termPattern = $classNamePattern.compile($termLastRegex.toString());",
          "946:          | }",
          "947:          | java.util.regex.Matcher $matcher = $termPattern.matcher($subject.toString());",
          "",
          "[Added Lines]",
          "957:          | ${initLastMatcherCode(ctx, subject, regexp, matcher)}",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2080:     new SparkException(errorClass = \"INVALID_BUCKET_FILE\", messageParameters = Array(path),",
          "2081:       cause = null)",
          "2082:   }",
          "2083: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2084:   def invalidPatternError(funcName: String, pattern: String): RuntimeException = {",
          "2085:     new SparkRuntimeException(",
          "2086:       errorClass = \"INVALID_PARAMETER_VALUE\",",
          "2087:       messageParameters = Array(",
          "2088:         \"regexp\",",
          "2089:         toSQLId(funcName),",
          "2090:         toSQLValue(pattern, StringType)))",
          "2091:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/RegexpExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/RegexpExpressionsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/RegexpExpressionsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/RegexpExpressionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.catalyst.expressions",
          "21: import org.apache.spark.sql.AnalysisException",
          "22: import org.apache.spark.sql.catalyst.dsl.expressions._",
          "23: import org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext",
          "",
          "[Removed Lines]",
          "20: import org.apache.spark.SparkFunSuite",
          "",
          "[Added Lines]",
          "20: import org.apache.spark.{SparkFunSuite, SparkRuntimeException}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "483:         .likeAll(\"%foo%\", Literal.create(null, StringType)), null)",
          "484:     }",
          "485:   }",
          "486: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "487:   test(\"SPARK-39758: invalid regexp pattern\") {",
          "488:     val s = $\"s\".string.at(0)",
          "489:     val p = $\"p\".string.at(1)",
          "490:     val r = $\"r\".int.at(2)",
          "491:     val prefix = \"The value of parameter(s) 'regexp' in\"",
          "492:     checkExceptionInExpression[SparkRuntimeException](",
          "493:       RegExpExtract(s, p, r),",
          "494:       create_row(\"1a 2b 14m\", \"(?l)\", 0),",
          "495:       s\"$prefix `regexp_extract` is invalid: '(?l)'\")",
          "496:     checkExceptionInExpression[SparkRuntimeException](",
          "497:       RegExpExtractAll(s, p, r),",
          "498:       create_row(\"abc\", \"] [\", 0),",
          "499:       s\"$prefix `regexp_extract_all` is invalid: '] ['\")",
          "500:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/regexp-functions.sql||sql/core/src/test/resources/sql-tests/inputs/regexp-functions.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/regexp-functions.sql -> sql/core/src/test/resources/sql-tests/inputs/regexp-functions.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "14: SELECT regexp_extract('1a 2b 14m', '(\\\\d+)([a-z]+)', -1);",
          "15: SELECT regexp_extract('1a 2b 14m', '(\\\\d+)?([a-z]+)', 1);",
          "16: SELECT regexp_extract('a b m', '(\\\\d+)?([a-z]+)', 1);",
          "18: -- regexp_extract_all",
          "19: SELECT regexp_extract_all('1a 2b 14m', '\\\\d+');",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "17: SELECT regexp_extract('1a 2b 14m', '(?l)');",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "31: SELECT regexp_extract_all('1a 2b 14m', '(\\\\d+)([a-z]+)', -1);",
          "32: SELECT regexp_extract_all('1a 2b 14m', '(\\\\d+)?([a-z]+)', 1);",
          "33: SELECT regexp_extract_all('a 2b 14m', '(\\\\d+)?([a-z]+)', 1);",
          "35: -- regexp_replace",
          "36: SELECT regexp_replace('healthy, wealthy, and wise', '\\\\w+thy', 'something');",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "35: SELECT regexp_extract_all('abc', col0, 1) FROM VALUES('], [') AS t(col0);",
          "",
          "---------------"
        ]
      }
    }
  ]
}