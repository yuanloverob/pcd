{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "84c091845d46375b54014538035864744d7d4399",
      "candidate_info": {
        "commit_hash": "84c091845d46375b54014538035864744d7d4399",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/84c091845d46375b54014538035864744d7d4399",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala"
        ],
        "message": "[SPARK-40315][SQL] Add hashCode() for Literal of ArrayBasedMapData\n\n### What changes were proposed in this pull request?\nThere is no explicit `hashCode()` function override for `ArrayBasedMapData`. As a result, there is a non-deterministic error where the `hashCode()` computed for `Literal`s of `ArrayBasedMapData` can be different for two equal objects (`Literal`s of `ArrayBasedMapData` with equal keys and values).\n\nIn this PR, we add a `hashCode` function so that it works exactly as we expect.\n\n### Why are the changes needed?\nThis is a bug fix for a non-deterministic error. It is also more consistent with the rest of Spark if we implement the `hashCode` method instead of relying on defaults. We can't add the `hashCode` directly to `ArrayBasedMapData` because of SPARK-9415.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nA simple unit test was added.\n\nCloses #37807 from c27kwan/SPARK-40315-lit.\n\nAuthored-by: Carmen Kwan <carmen.kwan@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit e85a4ffbdfa063c8da91b23dfbde77e2f9ed62e9)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "374:     val valueHashCode = value match {",
          "375:       case null => 0",
          "376:       case binary: Array[Byte] => util.Arrays.hashCode(binary)",
          "377:       case other => other.hashCode()",
          "378:     }",
          "379:     31 * Objects.hashCode(dataType) + valueHashCode",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "378:       case arrayBasedMapData: ArrayBasedMapData =>",
          "379:         arrayBasedMapData.keyArray.hashCode() * 37 + arrayBasedMapData.valueArray.hashCode()",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ComplexTypeSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "527:     assert(m1.semanticEquals(m2))",
          "528:   }",
          "529: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "530:   test(\"SPARK-40315: Literals of ArrayBasedMapData should have deterministic hashCode.\") {",
          "531:     val keys = new Array[UTF8String](1)",
          "532:     val values1 = new Array[UTF8String](1)",
          "533:     val values2 = new Array[UTF8String](1)",
          "535:     keys(0) = UTF8String.fromString(\"key\")",
          "536:     values1(0) = UTF8String.fromString(\"value1\")",
          "537:     values2(0) = UTF8String.fromString(\"value2\")",
          "539:     val d1 = new ArrayBasedMapData(new GenericArrayData(keys), new GenericArrayData(values1))",
          "540:     val d2 = new ArrayBasedMapData(new GenericArrayData(keys), new GenericArrayData(values1))",
          "541:     val d3 = new ArrayBasedMapData(new GenericArrayData(keys), new GenericArrayData(values2))",
          "542:     val m1 = Literal.create(d1, MapType(StringType, StringType))",
          "543:     val m2 = Literal.create(d2, MapType(StringType, StringType))",
          "544:     val m3 = Literal.create(d3, MapType(StringType, StringType))",
          "548:     assert(m1 == m2)",
          "549:     assert(m1.hashCode() == m2.hashCode())",
          "552:     assert(m1 != m3)",
          "553:     assert(m1.hashCode() != m3.hashCode())",
          "554:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7603f8d0aeb72e1989643fc9911edca0744087ad",
      "candidate_info": {
        "commit_hash": "7603f8d0aeb72e1989643fc9911edca0744087ad",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/7603f8d0aeb72e1989643fc9911edca0744087ad",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala"
        ],
        "message": "[SPARK-39835][SQL] Fix EliminateSorts remove global sort below the local sort\n\n### What changes were proposed in this pull request?\n\n Correct the `EliminateSorts` follows:\n\n- If the upper sort is global then we can remove the global or local sort recursively.\n- If the upper sort is local then we can only remove the local sort recursively.\n\n### Why are the changes needed?\n\nIf a global sort below locol sort, we should not remove the global sort becuase the output partitioning can be affected.\n\nThis issue is going to worse since we pull out the V1 Write sort to logcial side.\n\n### Does this PR introduce _any_ user-facing change?\n\nyes, bug fix\n\n### How was this patch tested?\n\nadd test\n\nCloses #37250 from ulysses-you/remove-sort.\n\nAuthored-by: ulysses-you <ulyssesyou18@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 5dca26d514a150bda58f7c4919624c9638498fec)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1445:       }",
          "1446:     case Sort(orders, false, child) if SortOrder.orderingSatisfies(child.outputOrdering, orders) =>",
          "1447:       applyLocally.lift(child).getOrElse(child)",
          "1449:     case j @ Join(originLeft, originRight, _, cond, _) if cond.forall(_.deterministic) =>",
          "1451:     case g @ Aggregate(_, aggs, originChild) if isOrderIrrelevantAggs(aggs) =>",
          "1453:   }",
          "1456:     if (!plan.containsPattern(SORT)) {",
          "1457:       return plan",
          "1458:     }",
          "1459:     plan match {",
          "1461:       case other if canEliminateSort(other) =>",
          "1463:       case _ => plan",
          "1464:     }",
          "1465:   }",
          "",
          "[Removed Lines]",
          "1448:     case s @ Sort(_, _, child) => s.copy(child = recursiveRemoveSort(child))",
          "1450:       j.copy(left = recursiveRemoveSort(originLeft), right = recursiveRemoveSort(originRight))",
          "1452:       g.copy(child = recursiveRemoveSort(originChild))",
          "1455:   private def recursiveRemoveSort(plan: LogicalPlan): LogicalPlan = {",
          "1460:       case Sort(_, _, child) => recursiveRemoveSort(child)",
          "1462:         other.withNewChildren(other.children.map(recursiveRemoveSort))",
          "",
          "[Added Lines]",
          "1448:     case s @ Sort(_, global, child) => s.copy(child = recursiveRemoveSort(child, global))",
          "1450:       j.copy(left = recursiveRemoveSort(originLeft, true),",
          "1451:         right = recursiveRemoveSort(originRight, true))",
          "1453:       g.copy(child = recursiveRemoveSort(originChild, true))",
          "1460:   private def recursiveRemoveSort(",
          "1461:       plan: LogicalPlan,",
          "1462:       canRemoveGlobalSort: Boolean): LogicalPlan = {",
          "1467:       case Sort(_, global, child) if canRemoveGlobalSort || !global =>",
          "1468:         recursiveRemoveSort(child, canRemoveGlobalSort)",
          "1470:         other.withNewChildren(other.children.map(c => recursiveRemoveSort(c, canRemoveGlobalSort)))",
          "1471:       case other if canEliminateGlobalSort(other) =>",
          "1472:         other.withNewChildren(other.children.map(c => recursiveRemoveSort(c, true)))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1467:   private def canEliminateSort(plan: LogicalPlan): Boolean = plan match {",
          "1468:     case p: Project => p.projectList.forall(_.deterministic)",
          "1469:     case f: Filter => f.condition.deterministic",
          "1470:     case r: RepartitionByExpression => r.partitionExpressions.forall(_.deterministic)",
          "1471:     case r: RebalancePartitions => r.partitionExpressions.forall(_.deterministic)",
          "1472:     case _: Repartition => true",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1480:     case _ => false",
          "1481:   }",
          "1483:   private def canEliminateGlobalSort(plan: LogicalPlan): Boolean = plan match {",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "422:       comparePlans(optimized, correctAnswer)",
          "423:     }",
          "424:   }",
          "425: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "426:   test(\"SPARK-39835: Fix EliminateSorts remove global sort below the local sort\") {",
          "428:     val plan = testRelation.orderBy($\"a\".asc).sortBy($\"c\".asc).analyze",
          "429:     comparePlans(Optimize.execute(plan), plan)",
          "432:     val plan2 = testRelation.orderBy($\"a\".asc).orderBy($\"b\".asc).sortBy($\"c\".asc).analyze",
          "433:     val expected2 = testRelation.orderBy($\"b\".asc).sortBy($\"c\".asc).analyze",
          "434:     comparePlans(Optimize.execute(plan2), expected2)",
          "437:     val plan3 = testRelation.sortBy($\"a\".asc).orderBy($\"b\".asc).sortBy($\"c\".asc).analyze",
          "438:     val expected3 = testRelation.orderBy($\"b\".asc).sortBy($\"c\".asc).analyze",
          "439:     comparePlans(Optimize.execute(plan3), expected3)",
          "440:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5544cce15885b1f12ae5826cd3bd2d151e1d544a",
      "candidate_info": {
        "commit_hash": "5544cce15885b1f12ae5826cd3bd2d151e1d544a",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/5544cce15885b1f12ae5826cd3bd2d151e1d544a",
        "files": [
          "python/pyspark/streaming/context.py",
          "python/pyspark/streaming/dstream.py",
          "python/pyspark/streaming/dstream.pyi"
        ],
        "message": "[SPARK-37015][PYTHON] Inline type hints for python/pyspark/streaming/dstream.py\n\n### What changes were proposed in this pull request?\nInline type hints for python/pyspark/streaming/dstream.py\n\n### Why are the changes needed?\nWe can take advantage of static type checking within the functions by inlining the type hints.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nExisting tests\n\nCloses #34324 from dchvn/SPARK-37015.\n\nLead-authored-by: dch nguyen <dchvn.dgd@gmail.com>\nCo-authored-by: dch nguyen <dgd_contributor@viettel.com.vn>\nSigned-off-by: zero323 <mszymkiewicz@gmail.com>\n(cherry picked from commit dff52d649d1e27baf3b107f75636624e0cfe780f)\nSigned-off-by: zero323 <mszymkiewicz@gmail.com>",
        "before_after_code_files": [
          "python/pyspark/streaming/context.py||python/pyspark/streaming/context.py",
          "python/pyspark/streaming/dstream.py||python/pyspark/streaming/dstream.py",
          "python/pyspark/streaming/dstream.pyi||python/pyspark/streaming/dstream.pyi"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/streaming/context.py||python/pyspark/streaming/context.py": [
          "File: python/pyspark/streaming/context.py -> python/pyspark/streaming/context.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "397:         the transform function parameter will be the same as the order",
          "398:         of corresponding DStreams in the list.",
          "399:         \"\"\"",
          "401:         # change the final serializer to sc.serializer",
          "402:         func = TransformFunction(",
          "403:             self._sc,",
          "404:             lambda t, *rdds: transformFunc(rdds),",
          "406:         )",
          "408:         assert self._jvm is not None",
          "",
          "[Removed Lines]",
          "400:         jdstreams = [d._jdstream for d in dstreams]  # type: ignore[attr-defined]",
          "",
          "[Added Lines]",
          "400:         jdstreams = [d._jdstream for d in dstreams]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "419:             raise ValueError(\"should have at least one DStream to union\")",
          "420:         if len(dstreams) == 1:",
          "421:             return dstreams[0]",
          "423:             raise ValueError(\"All DStreams should have same serializer\")",
          "425:             raise ValueError(\"All DStreams should have same slide duration\")",
          "427:         assert SparkContext._jvm is not None",
          "428:         jdstream_cls = SparkContext._jvm.org.apache.spark.streaming.api.java.JavaDStream",
          "429:         jpair_dstream_cls = SparkContext._jvm.org.apache.spark.streaming.api.java.JavaPairDStream",
          "430:         gw = SparkContext._gateway",
          "432:             cls = jdstream_cls",
          "436:             cls = jpair_dstream_cls",
          "437:         else:",
          "441:             raise TypeError(\"Unsupported Java DStream class %s\" % cls_name)",
          "443:         assert gw is not None",
          "444:         jdstreams = gw.new_array(cls, len(dstreams))",
          "445:         for i in range(0, len(dstreams)):",
          "447:         return DStream(",
          "448:             self._jssc.union(jdstreams),",
          "449:             self,",
          "451:         )",
          "453:     def addStreamingListener(self, streamingListener: StreamingListener) -> None:",
          "",
          "[Removed Lines]",
          "422:         if len(set(s._jrdd_deserializer for s in dstreams)) > 1:  # type: ignore[attr-defined]",
          "424:         if len(set(s._slideDuration for s in dstreams)) > 1:  # type: ignore[attr-defined]",
          "431:         if is_instance_of(gw, dstreams[0]._jdstream, jdstream_cls):  # type: ignore[attr-defined]",
          "433:         elif is_instance_of(",
          "434:             gw, dstreams[0]._jdstream, jpair_dstream_cls  # type: ignore[attr-defined]",
          "435:         ):",
          "438:             cls_name = (",
          "439:                 dstreams[0]._jdstream.getClass().getCanonicalName()  # type: ignore[attr-defined]",
          "440:             )",
          "446:             jdstreams[i] = dstreams[i]._jdstream  # type: ignore[attr-defined]",
          "450:             dstreams[0]._jrdd_deserializer,  # type: ignore[attr-defined]",
          "",
          "[Added Lines]",
          "422:         if len(set(s._jrdd_deserializer for s in dstreams)) > 1:",
          "424:         if len(set(s._slideDuration for s in dstreams)) > 1:",
          "431:         if is_instance_of(gw, dstreams[0]._jdstream, jdstream_cls):",
          "433:         elif is_instance_of(gw, dstreams[0]._jdstream, jpair_dstream_cls):",
          "436:             cls_name = dstreams[0]._jdstream.getClass().getCanonicalName()",
          "442:             jdstreams[i] = dstreams[i]._jdstream",
          "446:             dstreams[0]._jrdd_deserializer,",
          "",
          "---------------"
        ],
        "python/pyspark/streaming/dstream.py||python/pyspark/streaming/dstream.py": [
          "File: python/pyspark/streaming/dstream.py -> python/pyspark/streaming/dstream.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: import time",
          "20: from itertools import chain",
          "21: from datetime import datetime",
          "23: from py4j.protocol import Py4JJavaError",
          "26: from pyspark.storagelevel import StorageLevel",
          "27: from pyspark.streaming.util import rddToFileName, TransformFunction",
          "29: from pyspark.resultiterable import ResultIterable",
          "31: __all__ = [\"DStream\"]",
          "35:     \"\"\"",
          "36:     A Discretized Stream (DStream), the basic abstraction in Spark Streaming,",
          "37:     is a continuous sequence of RDDs (of the same type) representing a",
          "",
          "[Removed Lines]",
          "25: from pyspark import RDD",
          "28: from pyspark.rdd import portable_hash",
          "34: class DStream:",
          "",
          "[Added Lines]",
          "22: from typing import (",
          "23:     Any,",
          "24:     Callable,",
          "25:     Generic,",
          "26:     Hashable,",
          "27:     Iterable,",
          "28:     List,",
          "29:     Optional,",
          "30:     Tuple,",
          "31:     TypeVar,",
          "32:     Union,",
          "33:     TYPE_CHECKING,",
          "34:     cast,",
          "35:     overload,",
          "36: )",
          "42: from pyspark.rdd import portable_hash, RDD",
          "44: from py4j.java_gateway import JavaObject",
          "46: if TYPE_CHECKING:",
          "47:     from pyspark.serializers import Serializer",
          "48:     from pyspark.streaming.context import StreamingContext",
          "52: S = TypeVar(\"S\")",
          "53: T = TypeVar(\"T\")",
          "54: T_co = TypeVar(\"T_co\", covariant=True)",
          "55: U = TypeVar(\"U\")",
          "56: K = TypeVar(\"K\", bound=Hashable)",
          "57: V = TypeVar(\"V\")",
          "60: class DStream(Generic[T_co]):",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "51:      - A function that is used to generate an RDD after each time interval",
          "52:     \"\"\"",
          "55:         self._jdstream = jdstream",
          "56:         self._ssc = ssc",
          "57:         self._sc = ssc._sc",
          "",
          "[Removed Lines]",
          "54:     def __init__(self, jdstream, ssc, jrdd_deserializer):",
          "",
          "[Added Lines]",
          "80:     def __init__(",
          "81:         self,",
          "82:         jdstream: JavaObject,",
          "83:         ssc: \"StreamingContext\",",
          "84:         jrdd_deserializer: \"Serializer\",",
          "85:     ):",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "59:         self.is_cached = False",
          "60:         self.is_checkpointed = False",
          "63:         \"\"\"",
          "64:         Return the StreamingContext associated with this DStream",
          "65:         \"\"\"",
          "66:         return self._ssc",
          "69:         \"\"\"",
          "70:         Return a new DStream in which each RDD has a single element",
          "71:         generated by counting each RDD of this DStream.",
          "72:         \"\"\"",
          "73:         return self.mapPartitions(lambda i: [sum(1 for _ in i)]).reduce(operator.add)",
          "76:         \"\"\"",
          "77:         Return a new DStream containing only the elements that satisfy predicate.",
          "78:         \"\"\"",
          "81:             return filter(f, iterator)",
          "83:         return self.mapPartitions(func, True)",
          "86:         \"\"\"",
          "87:         Return a new DStream by applying a function to all elements of",
          "88:         this DStream, and then flattening the results",
          "89:         \"\"\"",
          "92:             return chain.from_iterable(map(f, iterator))",
          "94:         return self.mapPartitionsWithIndex(func, preservesPartitioning)",
          "97:         \"\"\"",
          "98:         Return a new DStream by applying a function to each element of DStream.",
          "99:         \"\"\"",
          "102:             return map(f, iterator)",
          "104:         return self.mapPartitions(func, preservesPartitioning)",
          "107:         \"\"\"",
          "108:         Return a new DStream in which each RDD is generated by applying",
          "109:         mapPartitions() to each RDDs of this DStream.",
          "110:         \"\"\"",
          "113:             return f(iterator)",
          "115:         return self.mapPartitionsWithIndex(func, preservesPartitioning)",
          "118:         \"\"\"",
          "119:         Return a new DStream in which each RDD is generated by applying",
          "120:         mapPartitionsWithIndex() to each RDDs of this DStream.",
          "121:         \"\"\"",
          "122:         return self.transform(lambda rdd: rdd.mapPartitionsWithIndex(f, preservesPartitioning))",
          "125:         \"\"\"",
          "126:         Return a new DStream in which each RDD has a single element",
          "127:         generated by reducing each RDD of this DStream.",
          "128:         \"\"\"",
          "129:         return self.map(lambda x: (None, x)).reduceByKey(func, 1).map(lambda x: x[1])",
          "132:         \"\"\"",
          "133:         Return a new DStream by applying reduceByKey to each RDD.",
          "134:         \"\"\"",
          "",
          "[Removed Lines]",
          "62:     def context(self):",
          "68:     def count(self):",
          "75:     def filter(self, f):",
          "80:         def func(iterator):",
          "85:     def flatMap(self, f, preservesPartitioning=False):",
          "91:         def func(s, iterator):",
          "96:     def map(self, f, preservesPartitioning=False):",
          "101:         def func(iterator):",
          "106:     def mapPartitions(self, f, preservesPartitioning=False):",
          "112:         def func(s, iterator):",
          "117:     def mapPartitionsWithIndex(self, f, preservesPartitioning=False):",
          "124:     def reduce(self, func):",
          "131:     def reduceByKey(self, func, numPartitions=None):",
          "",
          "[Added Lines]",
          "93:     def context(self) -> \"StreamingContext\":",
          "99:     def count(self) -> \"DStream[int]\":",
          "106:     def filter(self: \"DStream[T]\", f: Callable[[T], bool]) -> \"DStream[T]\":",
          "111:         def func(iterator: Iterable[T]) -> Iterable[T]:",
          "116:     def flatMap(",
          "117:         self: \"DStream[T]\",",
          "118:         f: Callable[[T], Iterable[U]],",
          "119:         preservesPartitioning: bool = False,",
          "120:     ) -> \"DStream[U]\":",
          "126:         def func(s: int, iterator: Iterable[T]) -> Iterable[U]:",
          "131:     def map(",
          "132:         self: \"DStream[T]\", f: Callable[[T], U], preservesPartitioning: bool = False",
          "133:     ) -> \"DStream[U]\":",
          "138:         def func(iterator: Iterable[T]) -> Iterable[U]:",
          "143:     def mapPartitions(",
          "144:         self: \"DStream[T]\",",
          "145:         f: Callable[[Iterable[T]], Iterable[U]],",
          "146:         preservesPartitioning: bool = False,",
          "147:     ) -> \"DStream[U]\":",
          "153:         def func(s: int, iterator: Iterable[T]) -> Iterable[U]:",
          "158:     def mapPartitionsWithIndex(",
          "159:         self: \"DStream[T]\",",
          "160:         f: Callable[[int, Iterable[T]], Iterable[U]],",
          "161:         preservesPartitioning: bool = False,",
          "162:     ) -> \"DStream[U]\":",
          "169:     def reduce(self: \"DStream[T]\", func: Callable[[T, T], T]) -> \"DStream[T]\":",
          "176:     def reduceByKey(",
          "177:         self: \"DStream[Tuple[K, V]]\",",
          "178:         func: Callable[[V, V], V],",
          "179:         numPartitions: Optional[int] = None,",
          "180:     ) -> \"DStream[Tuple[K, V]]\":",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "136:             numPartitions = self._sc.defaultParallelism",
          "137:         return self.combineByKey(lambda x: x, func, func, numPartitions)",
          "140:         \"\"\"",
          "141:         Return a new DStream by applying combineByKey to each RDD.",
          "142:         \"\"\"",
          "143:         if numPartitions is None:",
          "144:             numPartitions = self._sc.defaultParallelism",
          "147:             return rdd.combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions)",
          "149:         return self.transform(func)",
          "152:         \"\"\"",
          "153:         Return a copy of the DStream in which each RDD are partitioned",
          "154:         using the specified partitioner.",
          "155:         \"\"\"",
          "156:         return self.transform(lambda rdd: rdd.partitionBy(numPartitions, partitionFunc))",
          "159:         \"\"\"",
          "160:         Apply a function to each RDD in this DStream.",
          "161:         \"\"\"",
          "162:         if func.__code__.co_argcount == 1:",
          "163:             old_func = func",
          "168:         jfunc = TransformFunction(self._sc, func, self._jrdd_deserializer)",
          "169:         api = self._ssc._jvm.PythonDStream",
          "170:         api.callForeachRDD(self._jdstream, jfunc)",
          "173:         \"\"\"",
          "174:         Print the first num elements of each RDD generated in this DStream.",
          "",
          "[Removed Lines]",
          "139:     def combineByKey(self, createCombiner, mergeValue, mergeCombiners, numPartitions=None):",
          "146:         def func(rdd):",
          "151:     def partitionBy(self, numPartitions, partitionFunc=portable_hash):",
          "158:     def foreachRDD(self, func):",
          "165:             def func(_, rdd):",
          "166:                 return old_func(rdd)",
          "172:     def pprint(self, num=10):",
          "",
          "[Added Lines]",
          "188:     def combineByKey(",
          "189:         self: \"DStream[Tuple[K, V]]\",",
          "190:         createCombiner: Callable[[V], U],",
          "191:         mergeValue: Callable[[U, V], U],",
          "192:         mergeCombiners: Callable[[U, U], U],",
          "193:         numPartitions: Optional[int] = None,",
          "194:     ) -> \"DStream[Tuple[K, U]]\":",
          "201:         def func(rdd: RDD[Tuple[K, V]]) -> RDD[Tuple[K, U]]:",
          "206:     def partitionBy(",
          "207:         self: \"DStream[Tuple[K, V]]\",",
          "208:         numPartitions: int,",
          "209:         partitionFunc: Callable[[K], int] = portable_hash,",
          "210:     ) -> \"DStream[Tuple[K, V]]\":",
          "217:     @overload",
          "218:     def foreachRDD(self: \"DStream[T]\", func: Callable[[RDD[T]], None]) -> None:",
          "219:         ...",
          "221:     @overload",
          "222:     def foreachRDD(self: \"DStream[T]\", func: Callable[[datetime, RDD[T]], None]) -> None:",
          "223:         ...",
          "225:     def foreachRDD(",
          "226:         self: \"DStream[T]\",",
          "227:         func: Union[Callable[[RDD[T]], None], Callable[[datetime, RDD[T]], None]],",
          "228:     ) -> None:",
          "235:             def func(_: datetime, rdd: \"RDD[T]\") -> None:",
          "236:                 return old_func(rdd)  # type: ignore[call-arg, arg-type]",
          "239:         assert self._ssc._jvm is not None",
          "243:     def pprint(self, num: int = 10) -> None:",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "179:             the number of elements from the first will be printed.",
          "180:         \"\"\"",
          "183:             taken = rdd.take(num + 1)",
          "184:             print(\"-------------------------------------------\")",
          "185:             print(\"Time: %s\" % time)",
          "",
          "[Removed Lines]",
          "182:         def takeAndPrint(time, rdd):",
          "",
          "[Added Lines]",
          "253:         def takeAndPrint(time: datetime, rdd: RDD[T]) -> None:",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "193:         self.foreachRDD(takeAndPrint)",
          "196:         \"\"\"",
          "197:         Return a new DStream by applying a map function to the value of",
          "198:         each key-value pairs in this DStream without changing the key.",
          "199:         \"\"\"",
          "202:             return kv[0], f(kv[1])",
          "204:         return self.map(map_values_fn, preservesPartitioning=True)",
          "207:         \"\"\"",
          "208:         Return a new DStream by applying a flatmap function to the value",
          "209:         of each key-value pairs in this DStream without changing the key.",
          "210:         \"\"\"",
          "213:             return ((kv[0], x) for x in f(kv[1]))",
          "215:         return self.flatMap(flat_map_fn, preservesPartitioning=True)",
          "218:         \"\"\"",
          "219:         Return a new DStream in which RDD is generated by applying glom()",
          "220:         to RDD of this DStream.",
          "221:         \"\"\"",
          "224:             yield list(iterator)",
          "226:         return self.mapPartitions(func)",
          "229:         \"\"\"",
          "230:         Persist the RDDs of this DStream with the default storage level",
          "231:         (`MEMORY_ONLY`).",
          "",
          "[Removed Lines]",
          "195:     def mapValues(self, f):",
          "201:         def map_values_fn(kv):",
          "206:     def flatMapValues(self, f):",
          "212:         def flat_map_fn(kv):",
          "217:     def glom(self):",
          "223:         def func(iterator):",
          "228:     def cache(self):",
          "",
          "[Added Lines]",
          "266:     def mapValues(self: \"DStream[Tuple[K, V]]\", f: Callable[[V], U]) -> \"DStream[Tuple[K, U]]\":",
          "272:         def map_values_fn(kv: Tuple[K, V]) -> Tuple[K, U]:",
          "277:     def flatMapValues(",
          "278:         self: \"DStream[Tuple[K, V]]\", f: Callable[[V], Iterable[U]]",
          "279:     ) -> \"DStream[Tuple[K, U]]\":",
          "285:         def flat_map_fn(kv: Tuple[K, V]) -> Iterable[Tuple[K, U]]:",
          "290:     def glom(self: \"DStream[T]\") -> \"DStream[List[T]]\":",
          "296:         def func(iterator: Iterable[T]) -> Iterable[List[T]]:",
          "301:     def cache(self: \"DStream[T]\") -> \"DStream[T]\":",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "234:         self.persist(StorageLevel.MEMORY_ONLY)",
          "235:         return self",
          "238:         \"\"\"",
          "239:         Persist the RDDs of this DStream with the given storage level",
          "240:         \"\"\"",
          "",
          "[Removed Lines]",
          "237:     def persist(self, storageLevel):",
          "",
          "[Added Lines]",
          "310:     def persist(self: \"DStream[T]\", storageLevel: StorageLevel) -> \"DStream[T]\":",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "243:         self._jdstream.persist(javaStorageLevel)",
          "244:         return self",
          "247:         \"\"\"",
          "248:         Enable periodic checkpointing of RDDs of this DStream",
          "",
          "[Removed Lines]",
          "246:     def checkpoint(self, interval):",
          "",
          "[Added Lines]",
          "319:     def checkpoint(self: \"DStream[T]\", interval: int) -> \"DStream[T]\":",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "257:         self._jdstream.checkpoint(self._ssc._jduration(interval))",
          "258:         return self",
          "261:         \"\"\"",
          "262:         Return a new DStream by applying groupByKey on each RDD.",
          "263:         \"\"\"",
          "",
          "[Removed Lines]",
          "260:     def groupByKey(self, numPartitions=None):",
          "",
          "[Added Lines]",
          "333:     def groupByKey(",
          "334:         self: \"DStream[Tuple[K, V]]\", numPartitions: Optional[int] = None",
          "335:     ) -> \"DStream[Tuple[K, Iterable[V]]]\":",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "265:             numPartitions = self._sc.defaultParallelism",
          "266:         return self.transform(lambda rdd: rdd.groupByKey(numPartitions))",
          "269:         \"\"\"",
          "270:         Return a new DStream in which each RDD contains the counts of each",
          "271:         distinct value in each RDD of this DStream.",
          "272:         \"\"\"",
          "273:         return self.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)",
          "276:         \"\"\"",
          "277:         Save each RDD in this DStream as at text file, using string",
          "278:         representation of elements.",
          "279:         \"\"\"",
          "282:             path = rddToFileName(prefix, suffix, t)",
          "283:             try:",
          "284:                 rdd.saveAsTextFile(path)",
          "",
          "[Removed Lines]",
          "268:     def countByValue(self):",
          "275:     def saveAsTextFiles(self, prefix, suffix=None):",
          "281:         def saveAsTextFile(t, rdd):",
          "",
          "[Added Lines]",
          "343:     def countByValue(self: \"DStream[K]\") -> \"DStream[Tuple[K, int]]\":",
          "350:     def saveAsTextFiles(self, prefix: str, suffix: Optional[str] = None) -> None:",
          "356:         def saveAsTextFile(t: Optional[datetime], rdd: RDD[T]) -> None:",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "307:     #                 raise",
          "308:     #     return self.foreachRDD(saveAsPickleFile)",
          "311:         \"\"\"",
          "312:         Return a new DStream in which each RDD is generated by applying a function",
          "313:         on each RDD of this DStream.",
          "",
          "[Removed Lines]",
          "310:     def transform(self, func):",
          "",
          "[Added Lines]",
          "385:     @overload",
          "386:     def transform(self: \"DStream[T]\", func: Callable[[RDD[T]], RDD[U]]) -> \"TransformedDStream[U]\":",
          "387:         ...",
          "389:     @overload",
          "390:     def transform(",
          "391:         self: \"DStream[T]\", func: Callable[[datetime, RDD[T]], RDD[U]]",
          "392:     ) -> \"TransformedDStream[U]\":",
          "393:         ...",
          "395:     def transform(",
          "396:         self: \"DStream[T]\",",
          "397:         func: Union[Callable[[RDD[T]], RDD[U]], Callable[[datetime, RDD[T]], RDD[U]]],",
          "398:     ) -> \"TransformedDStream[U]\":",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "318:         if func.__code__.co_argcount == 1:",
          "319:             oldfunc = func",
          "324:         assert func.__code__.co_argcount == 2, \"func should take one or two arguments\"",
          "325:         return TransformedDStream(self, func)",
          "328:         \"\"\"",
          "329:         Return a new DStream in which each RDD is generated by applying a function",
          "330:         on each RDD of this DStream and 'other' DStream.",
          "",
          "[Removed Lines]",
          "321:             def func(_, rdd):",
          "322:                 return oldfunc(rdd)",
          "327:     def transformWith(self, func, other, keepSerializer=False):",
          "",
          "[Added Lines]",
          "409:             def func(_: datetime, rdd: RDD[T]) -> RDD[U]:",
          "410:                 return oldfunc(rdd)  # type: ignore[arg-type, call-arg]",
          "415:     @overload",
          "416:     def transformWith(",
          "417:         self: \"DStream[T]\",",
          "418:         func: Callable[[RDD[T], RDD[U]], RDD[V]],",
          "419:         other: \"DStream[U]\",",
          "420:         keepSerializer: bool = ...,",
          "421:     ) -> \"DStream[V]\":",
          "422:         ...",
          "424:     @overload",
          "425:     def transformWith(",
          "426:         self: \"DStream[T]\",",
          "427:         func: Callable[[datetime, RDD[T], RDD[U]], RDD[V]],",
          "428:         other: \"DStream[U]\",",
          "429:         keepSerializer: bool = ...,",
          "430:     ) -> \"DStream[V]\":",
          "431:         ...",
          "433:     def transformWith(",
          "434:         self: \"DStream[T]\",",
          "435:         func: Union[",
          "436:             Callable[[RDD[T], RDD[U]], RDD[V]],",
          "437:             Callable[[datetime, RDD[T], RDD[U]], RDD[V]],",
          "438:         ],",
          "439:         other: \"DStream[U]\",",
          "440:         keepSerializer: bool = False,",
          "441:     ) -> \"DStream[V]\":",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "335:         if func.__code__.co_argcount == 2:",
          "336:             oldfunc = func",
          "341:         assert func.__code__.co_argcount == 3, \"func should take two or three arguments\"",
          "343:         dstream = self._sc._jvm.PythonTransformed2DStream(",
          "344:             self._jdstream.dstream(), other._jdstream.dstream(), jfunc",
          "345:         )",
          "346:         jrdd_serializer = self._jrdd_deserializer if keepSerializer else self._sc.serializer",
          "347:         return DStream(dstream.asJavaDStream(), self._ssc, jrdd_serializer)",
          "350:         \"\"\"",
          "351:         Return a new DStream with an increased or decreased level of parallelism.",
          "352:         \"\"\"",
          "353:         return self.transform(lambda rdd: rdd.repartition(numPartitions))",
          "355:     @property",
          "357:         \"\"\"",
          "358:         Return the slideDuration in seconds of this DStream",
          "359:         \"\"\"",
          "360:         return self._jdstream.dstream().slideDuration().milliseconds() / 1000.0",
          "363:         \"\"\"",
          "364:         Return a new DStream by unifying data of another DStream with this DStream.",
          "",
          "[Removed Lines]",
          "338:             def func(_, a, b):",
          "339:                 return oldfunc(a, b)",
          "342:         jfunc = TransformFunction(self._sc, func, self._jrdd_deserializer, other._jrdd_deserializer)",
          "349:     def repartition(self, numPartitions):",
          "356:     def _slideDuration(self):",
          "362:     def union(self, other):",
          "",
          "[Added Lines]",
          "452:             def func(_: datetime, a: RDD[T], b: RDD[U]) -> RDD[V]:",
          "453:                 return oldfunc(a, b)  # type: ignore[call-arg, arg-type]",
          "456:         jfunc = TransformFunction(",
          "457:             self._sc,",
          "458:             func,",
          "459:             self._jrdd_deserializer,",
          "460:             other._jrdd_deserializer,",
          "461:         )",
          "462:         assert self._sc._jvm is not None",
          "469:     def repartition(self: \"DStream[T]\", numPartitions: int) -> \"DStream[T]\":",
          "476:     def _slideDuration(self) -> None:",
          "482:     def union(self: \"DStream[T]\", other: \"DStream[U]\") -> \"DStream[Union[T, U]]\":",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "373:             raise ValueError(\"the two DStream should have same slide duration\")",
          "374:         return self.transformWith(lambda a, b: a.union(b), other, True)",
          "377:         \"\"\"",
          "378:         Return a new DStream by applying 'cogroup' between RDDs of this",
          "379:         DStream and `other` DStream.",
          "",
          "[Removed Lines]",
          "376:     def cogroup(self, other, numPartitions=None):",
          "",
          "[Added Lines]",
          "496:     def cogroup(",
          "497:         self: \"DStream[Tuple[K, V]]\",",
          "498:         other: \"DStream[Tuple[K, U]]\",",
          "499:         numPartitions: Optional[int] = None,",
          "500:     ) -> \"DStream[Tuple[K, Tuple[ResultIterable[V], ResultIterable[U]]]]\":",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "382:         \"\"\"",
          "383:         if numPartitions is None:",
          "384:             numPartitions = self._sc.defaultParallelism",
          "388:         \"\"\"",
          "389:         Return a new DStream by applying 'join' between RDDs of this DStream and",
          "390:         `other` DStream.",
          "",
          "[Removed Lines]",
          "385:         return self.transformWith(lambda a, b: a.cogroup(b, numPartitions), other)",
          "387:     def join(self, other, numPartitions=None):",
          "",
          "[Added Lines]",
          "509:         return self.transformWith(",
          "510:             lambda a, b: a.cogroup(b, numPartitions),",
          "511:             other,",
          "512:         )",
          "514:     def join(",
          "515:         self: \"DStream[Tuple[K, V]]\",",
          "516:         other: \"DStream[Tuple[K, U]]\",",
          "517:         numPartitions: Optional[int] = None,",
          "518:     ) -> \"DStream[Tuple[K, Tuple[V, U]]]\":",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "396:             numPartitions = self._sc.defaultParallelism",
          "397:         return self.transformWith(lambda a, b: a.join(b, numPartitions), other)",
          "400:         \"\"\"",
          "401:         Return a new DStream by applying 'left outer join' between RDDs of this DStream and",
          "402:         `other` DStream.",
          "",
          "[Removed Lines]",
          "399:     def leftOuterJoin(self, other, numPartitions=None):",
          "",
          "[Added Lines]",
          "530:     def leftOuterJoin(",
          "531:         self: \"DStream[Tuple[K, V]]\",",
          "532:         other: \"DStream[Tuple[K, U]]\",",
          "533:         numPartitions: Optional[int] = None,",
          "534:     ) -> \"DStream[Tuple[K, Tuple[V, Optional[U]]]]\":",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "408:             numPartitions = self._sc.defaultParallelism",
          "409:         return self.transformWith(lambda a, b: a.leftOuterJoin(b, numPartitions), other)",
          "412:         \"\"\"",
          "413:         Return a new DStream by applying 'right outer join' between RDDs of this DStream and",
          "414:         `other` DStream.",
          "",
          "[Removed Lines]",
          "411:     def rightOuterJoin(self, other, numPartitions=None):",
          "",
          "[Added Lines]",
          "546:     def rightOuterJoin(",
          "547:         self: \"DStream[Tuple[K, V]]\",",
          "548:         other: \"DStream[Tuple[K, U]]\",",
          "549:         numPartitions: Optional[int] = None,",
          "550:     ) -> \"DStream[Tuple[K, Tuple[Optional[V], U]]]\":",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "420:             numPartitions = self._sc.defaultParallelism",
          "421:         return self.transformWith(lambda a, b: a.rightOuterJoin(b, numPartitions), other)",
          "424:         \"\"\"",
          "425:         Return a new DStream by applying 'full outer join' between RDDs of this DStream and",
          "426:         `other` DStream.",
          "",
          "[Removed Lines]",
          "423:     def fullOuterJoin(self, other, numPartitions=None):",
          "",
          "[Added Lines]",
          "562:     def fullOuterJoin(",
          "563:         self: \"DStream[Tuple[K, V]]\",",
          "564:         other: \"DStream[Tuple[K, U]]\",",
          "565:         numPartitions: Optional[int] = None,",
          "566:     ) -> \"DStream[Tuple[K, Tuple[Optional[V], Optional[U]]]]\":",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "432:             numPartitions = self._sc.defaultParallelism",
          "433:         return self.transformWith(lambda a, b: a.fullOuterJoin(b, numPartitions), other)",
          "436:         \"\"\"Convert datetime or unix_timestamp into Time\"\"\"",
          "437:         if isinstance(timestamp, datetime):",
          "438:             timestamp = time.mktime(timestamp.timetuple())",
          "439:         return self._sc._jvm.Time(int(timestamp * 1000))",
          "442:         \"\"\"",
          "443:         Return all the RDDs between 'begin' to 'end' (both included)",
          "",
          "[Removed Lines]",
          "435:     def _jtime(self, timestamp):",
          "441:     def slice(self, begin, end):",
          "",
          "[Added Lines]",
          "578:     def _jtime(self, timestamp: Union[datetime, int, float]) -> JavaObject:",
          "582:         assert self._sc._jvm is not None",
          "585:     def slice(self, begin: Union[datetime, int], end: Union[datetime, int]) -> List[RDD[T]]:",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "447:         jrdds = self._jdstream.slice(self._jtime(begin), self._jtime(end))",
          "448:         return [RDD(jrdd, self._sc, self._jrdd_deserializer) for jrdd in jrdds]",
          "451:         duration = self._jdstream.dstream().slideDuration().milliseconds()",
          "452:         if int(window * 1000) % duration != 0:",
          "453:             raise ValueError(",
          "",
          "[Removed Lines]",
          "450:     def _validate_window_param(self, window, slide):",
          "",
          "[Added Lines]",
          "594:     def _validate_window_param(self, window: int, slide: Optional[int]) -> None:",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "460:                 \"dstream's slide (batch) duration (%d ms)\" % duration",
          "461:             )",
          "464:         \"\"\"",
          "465:         Return a new DStream in which each RDD contains all the elements in seen in a",
          "466:         sliding window of time over this DStream.",
          "",
          "[Removed Lines]",
          "463:     def window(self, windowDuration, slideDuration=None):",
          "",
          "[Added Lines]",
          "607:     def window(self, windowDuration: int, slideDuration: Optional[int] = None) -> \"DStream[T]\":",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "482:         s = self._ssc._jduration(slideDuration)",
          "483:         return DStream(self._jdstream.window(d, s), self._ssc, self._jrdd_deserializer)",
          "486:         \"\"\"",
          "487:         Return a new DStream in which each RDD has a single element generated by reducing all",
          "488:         elements in a sliding window over this DStream.",
          "",
          "[Removed Lines]",
          "485:     def reduceByWindow(self, reduceFunc, invReduceFunc, windowDuration, slideDuration):",
          "",
          "[Added Lines]",
          "629:     def reduceByWindow(",
          "630:         self: \"DStream[T]\",",
          "631:         reduceFunc: Callable[[T, T], T],",
          "632:         invReduceFunc: Optional[Callable[[T, T], T]],",
          "633:         windowDuration: int,",
          "634:         slideDuration: int,",
          "635:     ) -> \"DStream[T]\":",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "517:         )",
          "518:         return reduced.map(lambda kv: kv[1])",
          "521:         \"\"\"",
          "522:         Return a new DStream in which each RDD has a single element generated",
          "523:         by counting the number of elements in a window over this DStream.",
          "",
          "[Removed Lines]",
          "520:     def countByWindow(self, windowDuration, slideDuration):",
          "",
          "[Added Lines]",
          "670:     def countByWindow(",
          "671:         self: \"DStream[T]\", windowDuration: int, slideDuration: int",
          "672:     ) -> \"DStream[int]\":",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "530:             operator.add, operator.sub, windowDuration, slideDuration",
          "531:         )",
          "534:         \"\"\"",
          "535:         Return a new DStream in which each RDD contains the count of distinct elements in",
          "536:         RDDs in a sliding window over this DStream.",
          "",
          "[Removed Lines]",
          "533:     def countByValueAndWindow(self, windowDuration, slideDuration, numPartitions=None):",
          "",
          "[Added Lines]",
          "685:     def countByValueAndWindow(",
          "686:         self: \"DStream[T]\",",
          "687:         windowDuration: int,",
          "688:         slideDuration: int,",
          "689:         numPartitions: Optional[int] = None,",
          "690:     ) -> \"DStream[Tuple[T, int]]\":",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "553:         )",
          "554:         return counted.filter(lambda kv: kv[1] > 0)",
          "557:         \"\"\"",
          "558:         Return a new DStream by applying `groupByKey` over a sliding window.",
          "559:         Similar to `DStream.groupByKey()`, but applies it over a sliding window.",
          "",
          "[Removed Lines]",
          "556:     def groupByKeyAndWindow(self, windowDuration, slideDuration, numPartitions=None):",
          "",
          "[Added Lines]",
          "713:     def groupByKeyAndWindow(",
          "714:         self: \"DStream[Tuple[K, V]]\",",
          "715:         windowDuration: int,",
          "716:         slideDuration: int,",
          "717:         numPartitions: Optional[int] = None,",
          "718:     ) -> \"DStream[Tuple[K, Iterable[V]]]\":",
          "",
          "---------------",
          "--- Hunk 26 ---",
          "[Context before]",
          "572:         \"\"\"",
          "573:         ls = self.mapValues(lambda x: [x])",
          "574:         grouped = ls.reduceByKeyAndWindow(",
          "576:             lambda a, b: a[len(b) :],",
          "577:             windowDuration,",
          "578:             slideDuration,",
          "",
          "[Removed Lines]",
          "575:             lambda a, b: a.extend(b) or a,",
          "",
          "[Added Lines]",
          "737:             lambda a, b: a.extend(b) or a,  # type: ignore[func-returns-value]",
          "",
          "---------------",
          "--- Hunk 27 ---",
          "[Context before]",
          "581:         return grouped.mapValues(ResultIterable)",
          "583:     def reduceByKeyAndWindow(",
          "586:         \"\"\"",
          "587:         Return a new DStream by applying incremental `reduceByKey` over a sliding window.",
          "",
          "[Removed Lines]",
          "584:         self, func, invFunc, windowDuration, slideDuration=None, numPartitions=None, filterFunc=None",
          "585:     ):",
          "",
          "[Added Lines]",
          "746:         self: \"DStream[Tuple[K, V]]\",",
          "747:         func: Callable[[V, V], V],",
          "748:         invFunc: Optional[Callable[[V, V], V]],",
          "749:         windowDuration: int,",
          "750:         slideDuration: Optional[int] = None,",
          "751:         numPartitions: Optional[int] = None,",
          "752:         filterFunc: Optional[Callable[[Tuple[K, V]], bool]] = None,",
          "753:     ) -> \"DStream[Tuple[K, V]]\":",
          "",
          "---------------",
          "--- Hunk 28 ---",
          "[Context before]",
          "622:         if invFunc:",
          "625:                 b = b.reduceByKey(func, numPartitions)",
          "626:                 r = a.union(b).reduceByKey(func, numPartitions) if a else b",
          "627:                 if filterFunc:",
          "628:                     r = r.filter(filterFunc)",
          "629:                 return r",
          "632:                 b = b.reduceByKey(func, numPartitions)",
          "633:                 joined = a.leftOuterJoin(b, numPartitions)",
          "634:                 return joined.mapValues(",
          "636:                 )",
          "638:             jreduceFunc = TransformFunction(self._sc, reduceFunc, reduced._jrdd_deserializer)",
          "639:             jinvReduceFunc = TransformFunction(self._sc, invReduceFunc, reduced._jrdd_deserializer)",
          "640:             if slideDuration is None:",
          "641:                 slideDuration = self._slideDuration",
          "642:             dstream = self._sc._jvm.PythonReducedWindowedDStream(",
          "643:                 reduced._jdstream.dstream(),",
          "644:                 jreduceFunc,",
          "645:                 jinvReduceFunc,",
          "646:                 self._ssc._jduration(windowDuration),",
          "648:             )",
          "649:             return DStream(dstream.asJavaDStream(), self._ssc, self._sc.serializer)",
          "650:         else:",
          "654:         \"\"\"",
          "655:         Return a new \"state\" DStream where the state for each key is updated by applying",
          "656:         the given function on the previous state of the key and the new values of the key.",
          "",
          "[Removed Lines]",
          "624:             def reduceFunc(t, a, b):",
          "631:             def invReduceFunc(t, a, b):",
          "635:                     lambda kv: invFunc(kv[0], kv[1]) if kv[1] is not None else kv[0]",
          "647:                 self._ssc._jduration(slideDuration),",
          "651:             return reduced.window(windowDuration, slideDuration).reduceByKey(func, numPartitions)",
          "653:     def updateStateByKey(self, updateFunc, numPartitions=None, initialRDD=None):",
          "",
          "[Added Lines]",
          "792:             def reduceFunc(t: datetime, a: Any, b: Any) -> Any:",
          "799:             def invReduceFunc(t: datetime, a: Any, b: Any) -> Any:",
          "803:                     lambda kv: invFunc(kv[0], kv[1])  # type: ignore[misc]",
          "804:                     if kv[1] is not None",
          "805:                     else kv[0]",
          "812:             assert self._sc._jvm is not None",
          "818:                 self._ssc._jduration(slideDuration),  # type: ignore[arg-type]",
          "822:             return reduced.window(windowDuration, slideDuration).reduceByKey(",
          "823:                 func, numPartitions  # type: ignore[arg-type]",
          "824:             )",
          "826:     def updateStateByKey(",
          "827:         self: \"DStream[Tuple[K, V]]\",",
          "828:         updateFunc: Callable[[Iterable[V], Optional[S]], S],",
          "829:         numPartitions: Optional[int] = None,",
          "830:         initialRDD: Optional[Union[RDD[Tuple[K, S]], Iterable[Tuple[K, S]]]] = None,",
          "831:     ) -> \"DStream[Tuple[K, S]]\":",
          "",
          "---------------",
          "--- Hunk 29 ---",
          "[Context before]",
          "667:         if initialRDD and not isinstance(initialRDD, RDD):",
          "668:             initialRDD = self._sc.parallelize(initialRDD)",
          "671:             if a is None:",
          "672:                 g = b.groupByKey(numPartitions).mapValues(lambda vs: (list(vs), None))",
          "673:             else:",
          "675:                 g = g.mapValues(lambda ab: (list(ab[1]), list(ab[0])[0] if len(ab[0]) else None))",
          "676:             state = g.mapValues(lambda vs_s: updateFunc(vs_s[0], vs_s[1]))",
          "677:             return state.filter(lambda k_v: k_v[1] is not None)",
          "679:         jreduceFunc = TransformFunction(",
          "681:         )",
          "682:         if initialRDD:",
          "684:             dstream = self._sc._jvm.PythonStateDStream(",
          "686:             )",
          "687:         else:",
          "688:             dstream = self._sc._jvm.PythonStateDStream(self._jdstream.dstream(), jreduceFunc)",
          "690:         return DStream(dstream.asJavaDStream(), self._ssc, self._sc.serializer)",
          "694:     \"\"\"",
          "695:     TransformedDStream is a DStream generated by an Python function",
          "696:     transforming each RDD of a DStream to another RDDs.",
          "",
          "[Removed Lines]",
          "670:         def reduceFunc(t, a, b):",
          "674:                 g = a.cogroup(b.partitionBy(numPartitions), numPartitions)",
          "680:             self._sc, reduceFunc, self._sc.serializer, self._jrdd_deserializer",
          "683:             initialRDD = initialRDD._reserialize(self._jrdd_deserializer)",
          "685:                 self._jdstream.dstream(), jreduceFunc, initialRDD._jrdd",
          "693: class TransformedDStream(DStream):",
          "",
          "[Added Lines]",
          "848:         def reduceFunc(t: datetime, a: Any, b: Any) -> Any:",
          "852:                 g = a.cogroup(b.partitionBy(cast(int, numPartitions)), numPartitions)",
          "858:             self._sc,",
          "859:             reduceFunc,",
          "860:             self._sc.serializer,",
          "861:             self._jrdd_deserializer,",
          "864:             initialRDD = cast(RDD[Tuple[K, S]], initialRDD)._reserialize(self._jrdd_deserializer)",
          "865:             assert self._sc._jvm is not None",
          "867:                 self._jdstream.dstream(),",
          "868:                 jreduceFunc,",
          "869:                 initialRDD._jrdd,",
          "872:             assert self._sc._jvm is not None",
          "878: class TransformedDStream(DStream[U]):",
          "",
          "---------------",
          "--- Hunk 30 ---",
          "[Context before]",
          "699:     one transformation.",
          "700:     \"\"\"",
          "703:         self._ssc = prev._ssc",
          "704:         self._sc = self._ssc._sc",
          "705:         self._jrdd_deserializer = self._sc.serializer",
          "",
          "[Removed Lines]",
          "702:     def __init__(self, prev, func):",
          "",
          "[Added Lines]",
          "887:     @overload",
          "888:     def __init__(self: DStream[U], prev: DStream[T], func: Callable[[RDD[T]], RDD[U]]):",
          "889:         ...",
          "891:     @overload",
          "892:     def __init__(",
          "893:         self: DStream[U],",
          "894:         prev: DStream[T],",
          "895:         func: Callable[[datetime, RDD[T]], RDD[U]],",
          "896:     ):",
          "897:         ...",
          "899:     def __init__(",
          "900:         self,",
          "901:         prev: DStream[T],",
          "902:         func: Union[Callable[[RDD[T]], RDD[U]], Callable[[datetime, RDD[T]], RDD[U]]],",
          "903:     ):",
          "",
          "---------------",
          "--- Hunk 31 ---",
          "[Context before]",
          "710:         # Using type() to avoid folding the functions and compacting the DStreams which is not",
          "711:         # not strictly an object of TransformedDStream.",
          "712:         if type(prev) is TransformedDStream and not prev.is_cached and not prev.is_checkpointed:",
          "716:         else:",
          "717:             self.prev = prev",
          "718:             self.func = func",
          "720:     @property",
          "722:         if self._jdstream_val is not None:",
          "723:             return self._jdstream_val",
          "725:         jfunc = TransformFunction(self._sc, self.func, self.prev._jrdd_deserializer)",
          "726:         dstream = self._sc._jvm.PythonTransformedDStream(self.prev._jdstream.dstream(), jfunc)",
          "727:         self._jdstream_val = dstream.asJavaDStream()",
          "728:         return self._jdstream_val",
          "",
          "[Removed Lines]",
          "713:             prev_func = prev.func",
          "714:             self.func = lambda t, rdd: func(t, prev_func(t, rdd))",
          "715:             self.prev = prev.prev",
          "721:     def _jdstream(self):",
          "",
          "[Added Lines]",
          "914:             prev_func: Callable = prev.func",
          "915:             func = cast(Callable[[datetime, RDD[T]], RDD[U]], func)",
          "916:             self.func: Union[",
          "917:                 Callable[[RDD[T]], RDD[U]], Callable[[datetime, RDD[T]], RDD[U]]",
          "918:             ] = lambda t, rdd: func(t, prev_func(t, rdd))",
          "919:             self.prev: DStream[T] = prev.prev",
          "925:     def _jdstream(self) -> JavaObject:",
          "930:         assert self._sc._jvm is not None",
          "",
          "---------------"
        ],
        "python/pyspark/streaming/dstream.pyi||python/pyspark/streaming/dstream.pyi": [
          "File: python/pyspark/streaming/dstream.pyi -> python/pyspark/streaming/dstream.pyi",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "016dfeb760dbe1109e3c81c39bcd1bf3316a3e20",
      "candidate_info": {
        "commit_hash": "016dfeb760dbe1109e3c81c39bcd1bf3316a3e20",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/016dfeb760dbe1109e3c81c39bcd1bf3316a3e20",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala"
        ],
        "message": "[SPARK-37527][SQL][FOLLOWUP] Cannot compile COVAR_POP, COVAR_SAMP and CORR in `H2Dialect` if them with `DISTINCT`\n\nhttps://github.com/apache/spark/pull/35145 compile COVAR_POP, COVAR_SAMP and CORR in H2Dialect.\nBecause H2 does't support COVAR_POP, COVAR_SAMP and CORR works with DISTINCT.\nSo https://github.com/apache/spark/pull/35145 introduces a bug that compile COVAR_POP, COVAR_SAMP and CORR if these aggregate functions with DISTINCT.\n\nFix bug that compile COVAR_POP, COVAR_SAMP and CORR if these aggregate functions with DISTINCT.\n\n'Yes'.\nBug will be fix.\n\nNew test cases.\n\nCloses #37090 from beliefer/SPARK-37527_followup2.\n\nAuthored-by: Jiaan Geng <beliefer@163.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 14f2bae208c093dea58e3f947fb660e8345fb256)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala||sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala||sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala -> sql/core/src/main/scala/org/apache/spark/sql/jdbc/H2Dialect.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "55:           assert(f.children().length == 1)",
          "56:           val distinct = if (f.isDistinct) \"DISTINCT \" else \"\"",
          "57:           Some(s\"STDDEV_SAMP($distinct${f.children().head})\")",
          "59:           assert(f.children().length == 2)",
          "63:           assert(f.children().length == 2)",
          "67:           assert(f.children().length == 2)",
          "70:         case _ => None",
          "71:       }",
          "72:     )",
          "",
          "[Removed Lines]",
          "58:         case f: GeneralAggregateFunc if f.name() == \"COVAR_POP\" =>",
          "60:           val distinct = if (f.isDistinct) \"DISTINCT \" else \"\"",
          "61:           Some(s\"COVAR_POP($distinct${f.children().head}, ${f.children().last})\")",
          "62:         case f: GeneralAggregateFunc if f.name() == \"COVAR_SAMP\" =>",
          "64:           val distinct = if (f.isDistinct) \"DISTINCT \" else \"\"",
          "65:           Some(s\"COVAR_SAMP($distinct${f.children().head}, ${f.children().last})\")",
          "66:         case f: GeneralAggregateFunc if f.name() == \"CORR\" =>",
          "68:           val distinct = if (f.isDistinct) \"DISTINCT \" else \"\"",
          "69:           Some(s\"CORR($distinct${f.children().head}, ${f.children().last})\")",
          "",
          "[Added Lines]",
          "58:         case f: GeneralAggregateFunc if f.name() == \"COVAR_POP\" && !f.isDistinct =>",
          "60:           Some(s\"COVAR_POP(${f.children().head}, ${f.children().last})\")",
          "61:         case f: GeneralAggregateFunc if f.name() == \"COVAR_SAMP\" && !f.isDistinct =>",
          "63:           Some(s\"COVAR_SAMP(${f.children().head}, ${f.children().last})\")",
          "64:         case f: GeneralAggregateFunc if f.name() == \"CORR\" && !f.isDistinct =>",
          "66:           Some(s\"CORR(${f.children().head}, ${f.children().last})\")",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala -> sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1028:   }",
          "1030:   test(\"scan with aggregate push-down: COVAR_POP COVAR_SAMP with filter and group by\") {",
          "1036:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], PushedGroupByExpressions: [DEPT]\")",
          "1038:   }",
          "1040:   test(\"scan with aggregate push-down: CORR with filter and group by\") {",
          "1046:       \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0], PushedGroupByExpressions: [DEPT]\")",
          "1048:   }",
          "1050:   test(\"scan with aggregate push-down: aggregate over alias push down\") {",
          "",
          "[Removed Lines]",
          "1031:     val df = sql(\"select COVAR_POP(bonus, bonus), COVAR_SAMP(bonus, bonus)\" +",
          "1032:       \" FROM h2.test.employee where dept > 0 group by DePt\")",
          "1033:     checkFiltersRemoved(df)",
          "1034:     checkAggregateRemoved(df)",
          "1035:     checkPushedInfo(df, \"PushedAggregates: [COVAR_POP(BONUS, BONUS), COVAR_SAMP(BONUS, BONUS)], \" +",
          "1037:     checkAnswer(df, Seq(Row(10000d, 20000d), Row(2500d, 5000d), Row(0d, null)))",
          "1041:     val df = sql(\"select CORR(bonus, bonus) FROM h2.test.employee where dept > 0\" +",
          "1042:       \" group by DePt\")",
          "1043:     checkFiltersRemoved(df)",
          "1044:     checkAggregateRemoved(df)",
          "1045:     checkPushedInfo(df, \"PushedAggregates: [CORR(BONUS, BONUS)], \" +",
          "1047:     checkAnswer(df, Seq(Row(1d), Row(1d), Row(null)))",
          "",
          "[Added Lines]",
          "1031:     val df1 = sql(\"SELECT COVAR_POP(bonus, bonus), COVAR_SAMP(bonus, bonus)\" +",
          "1032:       \" FROM h2.test.employee WHERE dept > 0 GROUP BY DePt\")",
          "1033:     checkFiltersRemoved(df1)",
          "1034:     checkAggregateRemoved(df1)",
          "1035:     checkPushedInfo(df1, \"PushedAggregates: [COVAR_POP(BONUS, BONUS), COVAR_SAMP(BONUS, BONUS)], \" +",
          "1037:     checkAnswer(df1, Seq(Row(10000d, 20000d), Row(2500d, 5000d), Row(0d, null)))",
          "1039:     val df2 = sql(\"SELECT COVAR_POP(DISTINCT bonus, bonus), COVAR_SAMP(DISTINCT bonus, bonus)\" +",
          "1040:       \" FROM h2.test.employee WHERE dept > 0 GROUP BY DePt\")",
          "1041:     checkFiltersRemoved(df2)",
          "1042:     checkAggregateRemoved(df2, false)",
          "1043:     checkPushedInfo(df2, \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0]\")",
          "1044:     checkAnswer(df2, Seq(Row(10000d, 20000d), Row(2500d, 5000d), Row(0d, null)))",
          "1048:     val df1 = sql(\"SELECT CORR(bonus, bonus) FROM h2.test.employee WHERE dept > 0\" +",
          "1049:       \" GROUP BY DePt\")",
          "1050:     checkFiltersRemoved(df1)",
          "1051:     checkAggregateRemoved(df1)",
          "1052:     checkPushedInfo(df1, \"PushedAggregates: [CORR(BONUS, BONUS)], \" +",
          "1054:     checkAnswer(df1, Seq(Row(1d), Row(1d), Row(null)))",
          "1056:     val df2 = sql(\"SELECT CORR(DISTINCT bonus, bonus) FROM h2.test.employee WHERE dept > 0\" +",
          "1057:       \" GROUP BY DePt\")",
          "1058:     checkFiltersRemoved(df2)",
          "1059:     checkAggregateRemoved(df2, false)",
          "1060:     checkPushedInfo(df2, \"PushedFilters: [DEPT IS NOT NULL, DEPT > 0]\")",
          "1061:     checkAnswer(df2, Seq(Row(1d), Row(1d), Row(null)))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e743e68ce62e18ced6c49a22f5d101c72b7bfbe2",
      "candidate_info": {
        "commit_hash": "e743e68ce62e18ced6c49a22f5d101c72b7bfbe2",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/e743e68ce62e18ced6c49a22f5d101c72b7bfbe2",
        "files": [
          "core/src/main/scala/org/apache/spark/util/SparkFatalException.scala"
        ],
        "message": "[SPARK-39178][CORE] SparkFatalException should show root cause when print error stack\n\n### What changes were proposed in this pull request?\nOur user meet an case when running broadcast, throw `SparkFatalException`, but in error stack, it don't show the error case.\n\n### Why are the changes needed?\nMake exception more clear\n\n### Does this PR introduce _any_ user-facing change?\nUser can got root cause when application throw `SparkFatalException`.\n\n### How was this patch tested?\nFor ut\n```\n  test(\"xxxx\") {\n    throw new SparkFatalException(\n      new OutOfMemoryError(\"Not enough memory to build and broadcast the table to all \" +\n      \"worker nodes. As a workaround, you can either disable broadcast by setting \" +\n      s\"driver memory by setting ${SparkLauncher.DRIVER_MEMORY} to a higher value.\")\n      .initCause(null))\n  }\n```\n\nBefore this pr:\n```\n[info]   org.apache.spark.util.SparkFatalException:\n[info]   at org.apache.spark.SparkContextSuite.$anonfun$new$1(SparkContextSuite.scala:59)\n[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)\n[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)\n[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:190)\n[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:203)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:188)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:200)\n[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:200)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:182)\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:64)\n[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)\n[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)\n[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:64)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:233)\n[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)\n[info]   at scala.collection.immutable.List.foreach(List.scala:431)\n[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\n[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)\n[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:233)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:232)\n[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1563)\n[info]   at org.scalatest.Suite.run(Suite.scala:1112)\n```\n\nAfter this pr:\n```\n[info]   org.apache.spark.util.SparkFatalException: java.lang.OutOfMemoryError: Not enough memory to build and broadcast the table to all worker nodes. As a workaround, you can either disable broadcast by setting driver memory by setting spark.driver.memory to a higher value.\n[info]   at org.apache.spark.SparkContextSuite.$anonfun$new$1(SparkContextSuite.scala:59)\n[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)\n[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)\n[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)\n[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:190)\n[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:203)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:188)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:200)\n[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:200)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:182)\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:64)\n[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)\n[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)\n[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:64)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:233)\n[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)\n[info]   at scala.collection.immutable.List.foreach(List.scala:431)\n[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\n[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)\n[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:233)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:232)\n[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1563)\n[info]   at org.scalatest.Suite.run(Suite.scala:1112)\n[info]   at org.scalatest.Suite.run$(Suite.scala:1094)\n[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1563)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:237)\n[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:237)\n[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:236)\n[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:64)\n[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)\n[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)\n[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)\n[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:64)\n[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)\n[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)\n[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)\n[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n[info]   at java.lang.Thread.run(Thread.java:748)\n[info]   Cause: java.lang.OutOfMemoryError: Not enough memory to build and broadcast the table to all worker nodes. As a workaround, you can either disable broadcast by setting driver memory by setting spark.driver.memory to a higher value.\n[info]   at org.apache.spark.SparkContextSuite.$anonfun$new$1(SparkContextSuite.scala:58)\n```\n\nCloses #36539 from AngersZhuuuu/SPARK-39178.\n\nAuthored-by: Angerszhuuuu <angers.zhu@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>\n(cherry picked from commit d7317b03e975f8dc1a8c276dd0a931e00c478717)\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/util/SparkFatalException.scala||core/src/main/scala/org/apache/spark/util/SparkFatalException.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/util/SparkFatalException.scala||core/src/main/scala/org/apache/spark/util/SparkFatalException.scala": [
          "File: core/src/main/scala/org/apache/spark/util/SparkFatalException.scala -> core/src/main/scala/org/apache/spark/util/SparkFatalException.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "27: private[spark] final class SparkFatalException(val throwable: Throwable) extends Exception",
          "",
          "[Added Lines]",
          "27: private[spark] final class SparkFatalException(val throwable: Throwable)",
          "28:   extends Exception(throwable)",
          "",
          "---------------"
        ]
      }
    }
  ]
}