{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "3d93bd13decf270d41d2b279be809fe0388ac2ff",
      "candidate_info": {
        "commit_hash": "3d93bd13decf270d41d2b279be809fe0388ac2ff",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/3d93bd13decf270d41d2b279be809fe0388ac2ff",
        "files": [
          "docs/sql-ref-ansi-compliance.md",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/TryEval.scala"
        ],
        "message": "[SPARK-38818][DOC] Fix the docs of try_multiply/try_subtract/ANSI cast\n\n### What changes were proposed in this pull request?\n\n- Fix the valid combinations of ANSI CAST: Numeric types can be cast as Timestamp instead of Interval.\n- Fix the usage of try_multiply/try_subtract, from `expr1 FUNC expr2` to `FUNC(expr1<  expr2)`\n\n### Why are the changes needed?\n\nFix wrong documention.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nPreview:\n<img width=\"874\" alt=\"image\" src=\"https://user-images.githubusercontent.com/1097932/162204348-5bfa66de-83b6-4732-b854-31b0999c5cc2.png\">\n\nCloses #36099 from gengliangwang/fixDoc.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>\n(cherry picked from commit 26428fe812bd5600a61c4d4efc4dcb0f32646222)\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/TryEval.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/TryEval.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/TryEval.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/TryEval.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/TryEval.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/TryEval.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "123: }",
          "125: @ExpressionDescription(",
          "127:     \"The acceptable input types are the same with the `-` operator.\",",
          "128:   examples = \"\"\"",
          "129:     Examples:",
          "",
          "[Removed Lines]",
          "126:   usage = \"expr1 _FUNC_ expr2 - Returns `expr1`-`expr2` and the result is null on overflow. \" +",
          "",
          "[Added Lines]",
          "126:   usage = \"_FUNC_(expr1, expr2) - Returns `expr1`-`expr2` and the result is null on overflow. \" +",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "156: }",
          "158: @ExpressionDescription(",
          "160:     \"The acceptable input types are the same with the `*` operator.\",",
          "161:   examples = \"\"\"",
          "162:     Examples:",
          "",
          "[Removed Lines]",
          "159:   usage = \"expr1 _FUNC_ expr2 - Returns `expr1`*`expr2` and the result is null on overflow. \" +",
          "",
          "[Added Lines]",
          "159:   usage = \"_FUNC_(expr1, expr2) - Returns `expr1`*`expr2` and the result is null on overflow. \" +",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e088c820e1ee5736e130f5d7d1030990b0059141",
      "candidate_info": {
        "commit_hash": "e088c820e1ee5736e130f5d7d1030990b0059141",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/e088c820e1ee5736e130f5d7d1030990b0059141",
        "files": [
          "core/src/main/resources/error/error-classes.json",
          "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/core/src/test/resources/sql-tests/results/ansi/array.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/cast.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/date.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/datetime-parsing-invalid.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/decimalArithmeticOperations.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/map.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/string-functions.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/timestamp.sql.out",
          "sql/core/src/test/resources/sql-tests/results/interval.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/boolean.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/float4.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/float8.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/int4.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/int8.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/select_having.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/text.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part2.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part3.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part4.sql.out",
          "sql/core/src/test/resources/sql-tests/results/timestampNTZ/timestamp-ansi.sql.out",
          "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-select_having.sql.out",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala"
        ],
        "message": "[SPARK-39212][SQL][3.3] Use double quotes for values of SQL configs/DS options in error messages\n\n### What changes were proposed in this pull request?\nWrap values of SQL configs and datasource options in error messages by double quotes. Added the `toDSOption()` method to `QueryErrorsBase` to quote DS options.\n\nThis is a backport of https://github.com/apache/spark/pull/36579.\n\n### Why are the changes needed?\n1. To highlight SQL config/DS option values and make them more visible for users.\n2. To be able to easily parse values from error text.\n3. To be consistent to other outputs of identifiers, sql statement and etc. where Spark uses quotes or ticks.\n\n### Does this PR introduce _any_ user-facing change?\nYes, it changes user-facing error messages.\n\n### How was this patch tested?\nBy running the modified test suites:\n```\n$ build/sbt \"testOnly *QueryCompilationErrorsSuite\"\n$ build/sbt \"testOnly *QueryExecutionAnsiErrorsSuite\"\n$ build/sbt \"testOnly *QueryExecutionErrorsSuite\"\n```\n\nAuthored-by: Max Gekk <max.gekkgmail.com>\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n(cherry picked from commit 96f4b7dbc1facd1a38be296263606aa312861c95)\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n\nCloses #36600 from MaxGekk/move-ise-from-query-errors-3.3-2.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala||core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala||core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala": [
          "File: core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala -> core/src/test/scala/org/apache/spark/SparkThrowableSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "127:     assert(getMessage(\"DIVIDE_BY_ZERO\", Array(\"foo\", \"bar\", \"baz\")) ==",
          "130:   }",
          "132:   test(\"Error message is formatted\") {",
          "",
          "[Removed Lines]",
          "128:       \"Division by zero. To return NULL instead, use `try_divide`. If necessary set foo to false \" +",
          "129:         \"(except for ANSI interval type) to bypass this error.bar\")",
          "",
          "[Added Lines]",
          "128:       \"Division by zero. To return NULL instead, use `try_divide`. If necessary set foo \" +",
          "129:       \"to \\\"false\\\" (except for ANSI interval type) to bypass this error.bar\")",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "63:   def toSQLConf(conf: String): String = {",
          "64:     quoteByDefault(conf)",
          "65:   }",
          "66: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "67:   def toDSOption(option: String): String = {",
          "68:     quoteByDefault(option)",
          "69:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "214:   }",
          "216:   def mapKeyNotExistError(key: Any, dataType: DataType, context: String): NoSuchElementException = {",
          "219:   }",
          "221:   def inputTypeUnsupportedError(dataType: DataType): Throwable = {",
          "",
          "[Removed Lines]",
          "217:     new SparkNoSuchElementException(errorClass = \"MAP_KEY_DOES_NOT_EXIST\",",
          "218:       messageParameters = Array(toSQLValue(key, dataType), SQLConf.ANSI_ENABLED.key, context))",
          "",
          "[Added Lines]",
          "217:     new SparkNoSuchElementException(",
          "218:       errorClass = \"MAP_KEY_DOES_NOT_EXIST\",",
          "219:       messageParameters = Array(",
          "220:         toSQLValue(key, dataType),",
          "221:         toSQLConf(SQLConf.ANSI_ENABLED.key),",
          "222:         context))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "590:            |Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar",
          "591:            |that is different from Spark 3.0+'s Proleptic Gregorian calendar.",
          "592:            |See more details in SPARK-31404. You can set the SQL config ${toSQLConf(config)} or",
          "594:            |w.r.t. the calendar difference during reading. To read the datetime values",
          "597:            |\"\"\".stripMargin),",
          "598:       cause = null",
          "599:     )",
          "600:   }",
          "602:   def sparkUpgradeInWritingDatesError(format: String, config: String): SparkUpgradeException = {",
          "603:     new SparkUpgradeException(",
          "",
          "[Removed Lines]",
          "593:            |the datasource option '$option' to 'LEGACY' to rebase the datetime values",
          "595:            |as it is, set the SQL config ${toSQLConf(config)} or the datasource option '$option'",
          "596:            |to 'CORRECTED'.",
          "",
          "[Added Lines]",
          "598:            |the datasource option ${toDSOption(option)} to \"LEGACY\" to rebase the datetime values",
          "600:            |as it is, set the SQL config ${toSQLConf(config)} or the datasource option ${toDSOption(option)}",
          "601:            |to \"CORRECTED\".",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "609:           |into $format files can be dangerous, as the files may be read by Spark 2.x",
          "610:           |or legacy versions of Hive later, which uses a legacy hybrid calendar that",
          "611:           |is different from Spark 3.0+'s Proleptic Gregorian calendar. See more",
          "613:           |datetime values w.r.t. the calendar difference during writing, to get maximum",
          "615:           |values as it is, if you are 100% sure that the written files will only be read by",
          "616:           |Spark 3.0+ or other systems that use Proleptic Gregorian calendar.",
          "617:           |\"\"\".stripMargin),",
          "",
          "[Removed Lines]",
          "612:           |details in SPARK-31404. You can set ${toSQLConf(config)} to 'LEGACY' to rebase the",
          "614:           |interoperability. Or set ${toSQLConf(config)} to 'CORRECTED' to write the datetime",
          "",
          "[Added Lines]",
          "618:           |details in SPARK-31404. You can set ${toSQLConf(config)} to \"LEGACY\" to rebase the",
          "620:           |interoperability. Or set ${toSQLConf(config)} to \"CORRECTED\" to write the datetime",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "182:       val format = \"Parquet\"",
          "183:       val config = \"\\\"\" + SQLConf.PARQUET_REBASE_MODE_IN_READ.key + \"\\\"\"",
          "185:       assert(e.getErrorClass === \"INCONSISTENT_BEHAVIOR_CROSS_VERSION\")",
          "186:       assert(e.getMessage ===",
          "187:         \"You may get a different result due to the upgrading to Spark >= 3.0: \" +",
          "",
          "[Removed Lines]",
          "184:       val option = \"datetimeRebaseMode\"",
          "",
          "[Added Lines]",
          "184:       val option = \"\\\"\" + \"datetimeRebaseMode\" + \"\\\"\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "191:           |Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar",
          "192:           |that is different from Spark 3.0+'s Proleptic Gregorian calendar.",
          "193:           |See more details in SPARK-31404. You can set the SQL config $config or",
          "195:           |w.r.t. the calendar difference during reading. To read the datetime values",
          "198:           |\"\"\".stripMargin)",
          "199:     }",
          "",
          "[Removed Lines]",
          "194:           |the datasource option '$option' to 'LEGACY' to rebase the datetime values",
          "196:           |as it is, set the SQL config $config or the datasource option '$option'",
          "197:           |to 'CORRECTED'.",
          "",
          "[Added Lines]",
          "194:           |the datasource option $option to \"LEGACY\" to rebase the datetime values",
          "196:           |as it is, set the SQL config $config or the datasource option $option",
          "197:           |to \"CORRECTED\".",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "216:             |into $format files can be dangerous, as the files may be read by Spark 2.x",
          "217:             |or legacy versions of Hive later, which uses a legacy hybrid calendar that",
          "218:             |is different from Spark 3.0+'s Proleptic Gregorian calendar. See more",
          "220:             |datetime values w.r.t. the calendar difference during writing, to get maximum",
          "222:             |values as it is, if you are 100% sure that the written files will only be read by",
          "223:             |Spark 3.0+ or other systems that use Proleptic Gregorian calendar.",
          "224:             |\"\"\".stripMargin)",
          "",
          "[Removed Lines]",
          "219:             |details in SPARK-31404. You can set $config to 'LEGACY' to rebase the",
          "221:             |interoperability. Or set $config to 'CORRECTED' to write the datetime",
          "",
          "[Added Lines]",
          "219:             |details in SPARK-31404. You can set $config to \"LEGACY\" to rebase the",
          "221:             |interoperability. Or set $config to \"CORRECTED\" to write the datetime",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "399c397e7035665c928b1d439a860f9e7b1ce3b3",
      "candidate_info": {
        "commit_hash": "399c397e7035665c928b1d439a860f9e7b1ce3b3",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/399c397e7035665c928b1d439a860f9e7b1ce3b3",
        "files": [
          "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/DecommissionSuite.scala",
          "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala"
        ],
        "message": "[SPARK-40304][K8S][TESTS] Add `decomTestTag` to K8s Integration Test\n\n### What changes were proposed in this pull request?\n\nThis PR aims to add a new test tag, `decomTestTag`, to K8s Integration Test.\n\n### Why are the changes needed?\n\nDecommission-related tests took over 6 minutes (`363s`). It would be helpful we can run them selectively.\n```\n[info] - Test basic decommissioning (44 seconds, 51 milliseconds)\n[info] - Test basic decommissioning with shuffle cleanup (44 seconds, 450 milliseconds)\n[info] - Test decommissioning with dynamic allocation & shuffle cleanups (2 minutes, 43 seconds)\n[info] - Test decommissioning timeouts (44 seconds, 389 milliseconds)\n[info] - SPARK-37576: Rolling decommissioning (1 minute, 8 seconds)\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, this is a test-only change.\n\n### How was this patch tested?\n\nPass the CIs and test manually.\n```\n$ build/sbt -Psparkr -Pkubernetes -Pkubernetes-integration-tests \\\n-Dspark.kubernetes.test.deployMode=docker-desktop \"kubernetes-integration-tests/test\" \\\n-Dtest.exclude.tags=minikube,local,decom\n...\n[info] KubernetesSuite:\n[info] - Run SparkPi with no resources (12 seconds, 441 milliseconds)\n[info] - Run SparkPi with no resources & statefulset allocation (11 seconds, 949 milliseconds)\n[info] - Run SparkPi with a very long application name. (11 seconds, 999 milliseconds)\n[info] - Use SparkLauncher.NO_RESOURCE (11 seconds, 846 milliseconds)\n[info] - Run SparkPi with a master URL without a scheme. (11 seconds, 176 milliseconds)\n[info] - Run SparkPi with an argument. (11 seconds, 868 milliseconds)\n[info] - Run SparkPi with custom labels, annotations, and environment variables. (11 seconds, 858 milliseconds)\n[info] - All pods have the same service account by default (11 seconds, 5 milliseconds)\n[info] - Run extraJVMOptions check on driver (5 seconds, 757 milliseconds)\n[info] - Verify logging configuration is picked from the provided SPARK_CONF_DIR/log4j2.properties (12 seconds, 467 milliseconds)\n[info] - Run SparkPi with env and mount secrets. (21 seconds, 119 milliseconds)\n[info] - Run PySpark on simple pi.py example (13 seconds, 129 milliseconds)\n[info] - Run PySpark to test a pyfiles example (14 seconds, 937 milliseconds)\n[info] - Run PySpark with memory customization (12 seconds, 195 milliseconds)\n[info] - Run in client mode. (11 seconds, 343 milliseconds)\n[info] - Start pod creation from template (11 seconds, 975 milliseconds)\n[info] - SPARK-38398: Schedule pod creation from template (11 seconds, 901 milliseconds)\n[info] - Run SparkR on simple dataframe.R example (14 seconds, 305 milliseconds)\n...\n```\n\nCloses #37755 from dongjoon-hyun/SPARK-40304.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit fd0498f81df72c196f19a5b26053660f6f3f4d70)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/DecommissionSuite.scala||resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/DecommissionSuite.scala",
          "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala||resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/DecommissionSuite.scala||resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/DecommissionSuite.scala": [
          "File: resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/DecommissionSuite.scala -> resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/DecommissionSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: private[spark] trait DecommissionSuite { k8sSuite: KubernetesSuite =>",
          "36:   import DecommissionSuite._",
          "39:   def runDecommissionTest(f: () => Unit): Unit = {",
          "40:     val logConfFilePath = s\"${sparkHomeDir.toFile}/conf/log4j2.properties\"",
          "",
          "[Removed Lines]",
          "37:   import KubernetesSuite.k8sTestTag",
          "",
          "[Added Lines]",
          "37:   import KubernetesSuite.{decomTestTag, k8sTestTag}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "61:     }",
          "62:   }",
          "65:     runDecommissionTest(() => {",
          "66:       sparkAppConf",
          "67:         .set(config.DECOMMISSION_ENABLED.key, \"true\")",
          "",
          "[Removed Lines]",
          "64:   test(\"Test basic decommissioning\", k8sTestTag) {",
          "",
          "[Added Lines]",
          "64:   test(\"Test basic decommissioning\", k8sTestTag, decomTestTag) {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "91:     })",
          "92:   }",
          "95:     runDecommissionTest(() => {",
          "96:       sparkAppConf",
          "97:         .set(config.DECOMMISSION_ENABLED.key, \"true\")",
          "",
          "[Removed Lines]",
          "94:   test(\"Test basic decommissioning with shuffle cleanup\", k8sTestTag) {",
          "",
          "[Added Lines]",
          "94:   test(\"Test basic decommissioning with shuffle cleanup\", k8sTestTag, decomTestTag) {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "122:     })",
          "123:   }",
          "126:     runDecommissionTest(() => {",
          "127:       sparkAppConf",
          "128:         .set(config.DECOMMISSION_ENABLED.key, \"true\")",
          "",
          "[Removed Lines]",
          "125:   test(\"Test decommissioning with dynamic allocation & shuffle cleanups\", k8sTestTag) {",
          "",
          "[Added Lines]",
          "125:   test(\"Test decommissioning with dynamic allocation & shuffle cleanups\",",
          "126:       k8sTestTag, decomTestTag) {",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "183:     })",
          "184:   }",
          "187:     runDecommissionTest(() => {",
          "188:       sparkAppConf",
          "189:         .set(config.DECOMMISSION_ENABLED.key, \"true\")",
          "",
          "[Removed Lines]",
          "186:   test(\"Test decommissioning timeouts\", k8sTestTag) {",
          "",
          "[Added Lines]",
          "187:   test(\"Test decommissioning timeouts\", k8sTestTag, decomTestTag) {",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "216:     })",
          "217:   }",
          "220:     runDecommissionTest(() => {",
          "221:       sparkAppConf",
          "222:         .set(\"spark.kubernetes.container.image\", pyImage)",
          "",
          "[Removed Lines]",
          "219:   test(\"SPARK-37576: Rolling decommissioning\", k8sTestTag) {",
          "",
          "[Added Lines]",
          "220:   test(\"SPARK-37576: Rolling decommissioning\", k8sTestTag, decomTestTag) {",
          "",
          "---------------"
        ],
        "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala||resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala": [
          "File: resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala -> resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/KubernetesSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "612:   val k8sTestTag = Tag(\"k8s\")",
          "613:   val localTestTag = Tag(\"local\")",
          "614:   val schedulingTestTag = Tag(\"schedule\")",
          "615:   val rTestTag = Tag(\"r\")",
          "616:   val MinikubeTag = Tag(\"minikube\")",
          "617:   val SPARK_PI_MAIN_CLASS: String = \"org.apache.spark.examples.SparkPi\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "615:   val decomTestTag = Tag(\"decom\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8608baad7ab31eef0903b9229789e8112c9c1234",
      "candidate_info": {
        "commit_hash": "8608baad7ab31eef0903b9229789e8112c9c1234",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/8608baad7ab31eef0903b9229789e8112c9c1234",
        "files": [
          "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableCatalog.java",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/V1Table.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowCreateTableExec.scala"
        ],
        "message": "[SPARK-37878][SQL][FOLLOWUP] V1Table should always carry the \"location\" property\n\n### What changes were proposed in this pull request?\n\nThis is a followup of https://github.com/apache/spark/pull/35204 . https://github.com/apache/spark/pull/35204 introduced a potential regression: it removes the \"location\" table property from `V1Table` if the table is not external. The intention was to avoid putting the LOCATION clause for managed tables in `ShowCreateTableExec`. However, if we use the v2 DESCRIBE TABLE command by default in the future, this will bring a behavior change and v2 DESCRIBE TABLE command won't print the table location for managed tables.\n\nThis PR fixes this regression by using a different idea to fix the SHOW CREATE TABLE issue:\n1. introduce a new reserved table property `is_managed_location`, to indicate that the location is managed by the catalog, not user given.\n2. `ShowCreateTableExec` only generates the LOCATION clause if the \"location\" property is present and is not managed.\n\n### Why are the changes needed?\n\navoid a potential regression\n\n### Does this PR introduce _any_ user-facing change?\n\nno\n\n### How was this patch tested?\n\nexisting tests. We can add a test when we use v2 DESCRIBE TABLE command by default.\n\nCloses #36498 from cloud-fan/regression.\n\nAuthored-by: Wenchen Fan <wenchen@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit fa2bda5c4eabb23d5f5b3e14ccd055a2453f579f)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableCatalog.java||sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableCatalog.java",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/V1Table.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/V1Table.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowCreateTableExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowCreateTableExec.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableCatalog.java||sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableCatalog.java": [
          "File: sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableCatalog.java -> sql/catalyst/src/main/java/org/apache/spark/sql/connector/catalog/TableCatalog.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "48:   String PROP_LOCATION = \"location\";",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "54:   String PROP_IS_MANAGED_LOCATION = \"is_managed_location\";",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "41: import org.apache.spark.sql.catalyst.trees.CurrentOrigin",
          "42: import org.apache.spark.sql.catalyst.util.{CharVarcharUtils, DateTimeUtils, IntervalUtils}",
          "43: import org.apache.spark.sql.catalyst.util.DateTimeUtils.{convertSpecialDate, convertSpecialTimestamp, convertSpecialTimestampNTZ, getZoneId, stringToDate, stringToTimestamp, stringToTimestampWithoutTimeZone}",
          "45: import org.apache.spark.sql.connector.catalog.TableChange.ColumnPosition",
          "46: import org.apache.spark.sql.connector.expressions.{ApplyTransform, BucketTransform, DaysTransform, Expression => V2Expression, FieldReference, HoursTransform, IdentityTransform, LiteralValue, MonthsTransform, Transform, YearsTransform}",
          "47: import org.apache.spark.sql.errors.QueryParsingErrors",
          "",
          "[Removed Lines]",
          "44: import org.apache.spark.sql.connector.catalog.{SupportsNamespaces, TableCatalog}",
          "",
          "[Added Lines]",
          "44: import org.apache.spark.sql.connector.catalog.{CatalogV2Util, SupportsNamespaces, TableCatalog}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3215:         throw QueryParsingErrors.cannotCleanReservedTablePropertyError(",
          "3216:           PROP_EXTERNAL, ctx, \"please use CREATE EXTERNAL TABLE\")",
          "3217:       case (PROP_EXTERNAL, _) => false",
          "3219:     }",
          "3220:   }",
          "",
          "[Removed Lines]",
          "3218:       case _ => true",
          "",
          "[Added Lines]",
          "3219:       case (PROP_COMMENT, _) => true",
          "3220:       case (k, _) =>",
          "3221:         val isReserved = CatalogV2Util.TABLE_RESERVED_PROPERTIES.contains(k)",
          "3222:         if (!legacyOn && isReserved) {",
          "3223:           throw QueryParsingErrors.cannotCleanReservedTablePropertyError(",
          "3224:             k, ctx, \"please remove it from the TBLPROPERTIES list.\")",
          "3225:         }",
          "3226:         !isReserved",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/CatalogV2Util.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "48:       TableCatalog.PROP_LOCATION,",
          "49:       TableCatalog.PROP_PROVIDER,",
          "50:       TableCatalog.PROP_OWNER,",
          "",
          "[Removed Lines]",
          "51:       TableCatalog.PROP_EXTERNAL)",
          "",
          "[Added Lines]",
          "51:       TableCatalog.PROP_EXTERNAL,",
          "52:       TableCatalog.PROP_IS_MANAGED_LOCATION)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/V1Table.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/V1Table.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/V1Table.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/connector/catalog/V1Table.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "74: private[sql] object V1Table {",
          "75:   def addV2TableProperties(v1Table: CatalogTable): Map[String, String] = {",
          "76:     val external = v1Table.tableType == CatalogTableType.EXTERNAL",
          "78:     v1Table.properties ++",
          "79:       v1Table.storage.properties.map { case (key, value) =>",
          "80:         TableCatalog.OPTION_PREFIX + key -> value } ++",
          "81:       v1Table.provider.map(TableCatalog.PROP_PROVIDER -> _) ++",
          "82:       v1Table.comment.map(TableCatalog.PROP_COMMENT -> _) ++",
          "86:       (if (external) Some(TableCatalog.PROP_EXTERNAL -> \"true\") else None) ++",
          "87:       Some(TableCatalog.PROP_OWNER -> v1Table.owner)",
          "88:   }",
          "",
          "[Removed Lines]",
          "83:       (if (external) {",
          "84:         v1Table.storage.locationUri.map(TableCatalog.PROP_LOCATION -> _.toString)",
          "85:       } else None) ++",
          "",
          "[Added Lines]",
          "77:     val managed = v1Table.tableType == CatalogTableType.MANAGED",
          "84:       v1Table.storage.locationUri.map(TableCatalog.PROP_LOCATION -> _.toString) ++",
          "85:       (if (managed) Some(TableCatalog.PROP_IS_MANAGED_LOCATION -> \"true\") else None) ++",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowCreateTableExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowCreateTableExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowCreateTableExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ShowCreateTableExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "114:   }",
          "116:   private def showTableLocation(table: Table, builder: StringBuilder): Unit = {",
          "120:   }",
          "122:   private def showTableProperties(",
          "",
          "[Removed Lines]",
          "117:     Option(table.properties.get(TableCatalog.PROP_LOCATION))",
          "118:       .map(\"LOCATION '\" + escapeSingleQuotedString(_) + \"'\\n\")",
          "119:       .foreach(builder.append)",
          "",
          "[Added Lines]",
          "117:     val isManagedOption = Option(table.properties.get(TableCatalog.PROP_IS_MANAGED_LOCATION))",
          "119:     if (isManagedOption.forall(_.equalsIgnoreCase(\"false\"))) {",
          "120:       Option(table.properties.get(TableCatalog.PROP_LOCATION))",
          "121:         .map(\"LOCATION '\" + escapeSingleQuotedString(_) + \"'\\n\")",
          "122:         .foreach(builder.append)",
          "123:     }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "3d4d1edd534e0d19d9c7d978c345ad4973cb9456",
      "candidate_info": {
        "commit_hash": "3d4d1edd534e0d19d9c7d978c345ad4973cb9456",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/3d4d1edd534e0d19d9c7d978c345ad4973cb9456",
        "files": [
          "mllib/src/test/scala/org/apache/spark/ml/recommendation/ALSSuite.scala"
        ],
        "message": "[SPARK-38776][MLLIB][TESTS] Disable ANSI_ENABLED explicitly in `ALSSuite`\n\nThis PR aims to disable `ANSI_ENABLED` explicitly in the following tests of `ALSSuite`.\n```\ntest(\"ALS validate input dataset\") {\ntest(\"input type validation\") {\n```\n\nAfter SPARK-38490, this test became flaky in ANSI mode GitHub Action.\n\n![Screen Shot 2022-04-03 at 12 07 29 AM](https://user-images.githubusercontent.com/9700541/161416006-7b76596f-c19a-4212-91d2-8602df569608.png)\n\n- https://github.com/apache/spark/runs/5800714463?check_suite_focus=true\n- https://github.com/apache/spark/runs/5803714260?check_suite_focus=true\n- https://github.com/apache/spark/runs/5803745768?check_suite_focus=true\n\n```\n[info] ALSSuite:\n...\n[info] - ALS validate input dataset *** FAILED *** (2 seconds, 449 milliseconds)\n[info]   Invalid Long: out of range \"Job aborted due to stage failure: Task 0 in stage 100.0 failed 1 times, most recent failure: Lost task 0.0 in stage 100.0 (TID 348) (localhost executor driver):\norg.apache.spark.SparkArithmeticException:\nCasting 1231000000000 to int causes overflow.\nTo return NULL instead, use 'try_cast'.\nIf necessary set spark.sql.ansi.enabled to false to bypass this error.\n```\n\nNo. This is a test-only bug and fix.\n\nPass the CIs.\n\nCloses #36051 from dongjoon-hyun/SPARK-38776.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit d18fd7bcbdfe028e2e985ec6a8ec2f78bd5599c4)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "mllib/src/test/scala/org/apache/spark/ml/recommendation/ALSSuite.scala||mllib/src/test/scala/org/apache/spark/ml/recommendation/ALSSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "mllib/src/test/scala/org/apache/spark/ml/recommendation/ALSSuite.scala||mllib/src/test/scala/org/apache/spark/ml/recommendation/ALSSuite.scala": [
          "File: mllib/src/test/scala/org/apache/spark/ml/recommendation/ALSSuite.scala -> mllib/src/test/scala/org/apache/spark/ml/recommendation/ALSSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "39: import org.apache.spark.sql.{DataFrame, Encoder, Row, SparkSession}",
          "40: import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder",
          "41: import org.apache.spark.sql.functions.{col, lit}",
          "42: import org.apache.spark.sql.streaming.StreamingQueryException",
          "43: import org.apache.spark.sql.types._",
          "44: import org.apache.spark.storage.StorageLevel",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42: import org.apache.spark.sql.internal.SQLConf",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "213:     }",
          "215:     withClue(\"Valid Long Ids\") {",
          "217:     }",
          "219:     withClue(\"Valid Decimal Ids\") {",
          "",
          "[Removed Lines]",
          "216:       df.select(checkedCast(lit(1231L))).collect()",
          "",
          "[Added Lines]",
          "217:       withSQLConf(SQLConf.ANSI_ENABLED.key -> \"false\") {",
          "218:         df.select(checkedCast(lit(1231L))).collect()",
          "219:       }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "677:       (1, 1L, 1d, 0, 0L, 0d, 5.0)",
          "678:     ).toDF(\"user\", \"user_big\", \"user_small\", \"item\", \"item_big\", \"item_small\", \"rating\")",
          "679:     val msg = \"either out of Integer range or contained a fractional part\"",
          "703:         }",
          "705:       }",
          "714:     }",
          "715:   }",
          "",
          "[Removed Lines]",
          "680:     withClue(\"fit should fail when ids exceed integer range. \") {",
          "681:       assert(intercept[SparkException] {",
          "682:         als.fit(df.select(df(\"user_big\").as(\"user\"), df(\"item\"), df(\"rating\")))",
          "683:       }.getCause.getMessage.contains(msg))",
          "684:       assert(intercept[SparkException] {",
          "685:         als.fit(df.select(df(\"user_small\").as(\"user\"), df(\"item\"), df(\"rating\")))",
          "686:       }.getCause.getMessage.contains(msg))",
          "687:       assert(intercept[SparkException] {",
          "688:         als.fit(df.select(df(\"item_big\").as(\"item\"), df(\"user\"), df(\"rating\")))",
          "689:       }.getCause.getMessage.contains(msg))",
          "690:       assert(intercept[SparkException] {",
          "691:         als.fit(df.select(df(\"item_small\").as(\"item\"), df(\"user\"), df(\"rating\")))",
          "692:       }.getCause.getMessage.contains(msg))",
          "693:     }",
          "694:     withClue(\"transform should fail when ids exceed integer range. \") {",
          "695:       val model = als.fit(df)",
          "696:       def testTransformIdExceedsIntRange[A : Encoder](dataFrame: DataFrame): Unit = {",
          "697:         val e1 = intercept[SparkException] {",
          "698:           model.transform(dataFrame).collect()",
          "699:         }",
          "700:         TestUtils.assertExceptionMsg(e1, msg)",
          "701:         val e2 = intercept[StreamingQueryException] {",
          "702:           testTransformer[A](dataFrame, model, \"prediction\") { _ => }",
          "704:         TestUtils.assertExceptionMsg(e2, msg)",
          "706:       testTransformIdExceedsIntRange[(Long, Int)](df.select(df(\"user_big\").as(\"user\"),",
          "707:         df(\"item\")))",
          "708:       testTransformIdExceedsIntRange[(Double, Int)](df.select(df(\"user_small\").as(\"user\"),",
          "709:         df(\"item\")))",
          "710:       testTransformIdExceedsIntRange[(Long, Int)](df.select(df(\"item_big\").as(\"item\"),",
          "711:         df(\"user\")))",
          "712:       testTransformIdExceedsIntRange[(Double, Int)](df.select(df(\"item_small\").as(\"item\"),",
          "713:         df(\"user\")))",
          "",
          "[Added Lines]",
          "683:     withSQLConf(SQLConf.ANSI_ENABLED.key -> \"false\") {",
          "684:       withClue(\"fit should fail when ids exceed integer range. \") {",
          "685:         assert(intercept[SparkException] {",
          "686:           als.fit(df.select(df(\"user_big\").as(\"user\"), df(\"item\"), df(\"rating\")))",
          "687:         }.getCause.getMessage.contains(msg))",
          "688:         assert(intercept[SparkException] {",
          "689:           als.fit(df.select(df(\"user_small\").as(\"user\"), df(\"item\"), df(\"rating\")))",
          "690:         }.getCause.getMessage.contains(msg))",
          "691:         assert(intercept[SparkException] {",
          "692:           als.fit(df.select(df(\"item_big\").as(\"item\"), df(\"user\"), df(\"rating\")))",
          "693:         }.getCause.getMessage.contains(msg))",
          "694:         assert(intercept[SparkException] {",
          "695:           als.fit(df.select(df(\"item_small\").as(\"item\"), df(\"user\"), df(\"rating\")))",
          "696:         }.getCause.getMessage.contains(msg))",
          "697:       }",
          "698:       withClue(\"transform should fail when ids exceed integer range. \") {",
          "699:         val model = als.fit(df)",
          "700:         def testTransformIdExceedsIntRange[A : Encoder](dataFrame: DataFrame): Unit = {",
          "701:           val e1 = intercept[SparkException] {",
          "702:             model.transform(dataFrame).collect()",
          "703:           }",
          "704:           TestUtils.assertExceptionMsg(e1, msg)",
          "705:           val e2 = intercept[StreamingQueryException] {",
          "706:             testTransformer[A](dataFrame, model, \"prediction\") { _ => }",
          "707:           }",
          "708:           TestUtils.assertExceptionMsg(e2, msg)",
          "710:         testTransformIdExceedsIntRange[(Long, Int)](df.select(df(\"user_big\").as(\"user\"),",
          "711:           df(\"item\")))",
          "712:         testTransformIdExceedsIntRange[(Double, Int)](df.select(df(\"user_small\").as(\"user\"),",
          "713:           df(\"item\")))",
          "714:         testTransformIdExceedsIntRange[(Long, Int)](df.select(df(\"item_big\").as(\"item\"),",
          "715:           df(\"user\")))",
          "716:         testTransformIdExceedsIntRange[(Double, Int)](df.select(df(\"item_small\").as(\"item\"),",
          "717:           df(\"user\")))",
          "",
          "---------------"
        ]
      }
    }
  ]
}