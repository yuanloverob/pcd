{
  "cve_id": "CVE-2023-39441",
  "cve_desc": "Apache Airflow SMTP Provider before 1.3.0, Apache Airflow IMAP Provider before 3.3.0, and\u00a0Apache Airflow before 2.7.0 are affected by the\u00a0Validation of OpenSSL Certificate vulnerability.\n\nThe default SSL context with SSL library did not check a server's X.509\u00a0certificate.\u00a0 Instead, the code accepted any certificate, which could\u00a0result in the disclosure of mail server credentials or mail contents\u00a0when the client connects to an attacker in a MITM position.\n\nUsers are strongly advised to upgrade to Apache Airflow version 2.7.0 or newer, Apache Airflow IMAP Provider version 3.3.0 or newer, and Apache Airflow SMTP Provider version 1.3.0 or newer to mitigate the risk associated with this vulnerability",
  "repo": "apache/airflow",
  "patch_hash": "dbacacbd4d476da757de148a4e747924c34fd7fe",
  "patch_info": {
    "commit_hash": "dbacacbd4d476da757de148a4e747924c34fd7fe",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/dbacacbd4d476da757de148a4e747924c34fd7fe",
    "files": [
      "airflow/providers/smtp/CHANGELOG.rst",
      "airflow/providers/smtp/hooks/smtp.py",
      "airflow/providers/smtp/provider.yaml",
      "docs/apache-airflow-providers-smtp/configurations-ref.rst",
      "docs/apache-airflow-providers-smtp/index.rst",
      "docs/apache-airflow/configurations-ref.rst",
      "tests/providers/smtp/hooks/test_smtp.py"
    ],
    "message": "Allows to choose SSL context for SMTP provider (#33075)\n\n* Allows to choose SSL context for SMTP provider\n\nThis change add two options to choose from when SSL SMTP connection\nis created:\n\n* default - for balance between compatibility and security\n* none - in case compatibility with existing infrastructure is\n\u00a0 preferred\n\nThe fallback is:\n\n* The Airflow \"email\", \"ssl_context\"\n* \"default\"\n\n* Update airflow/providers/smtp/CHANGELOG.rst\n\nCo-authored-by: Ephraim Anierobi <splendidzigy24@gmail.com>\n(cherry picked from commit e20325db38fdfdd9db423a345b13d18aab6fe578)",
    "before_after_code_files": [
      "airflow/providers/smtp/hooks/smtp.py||airflow/providers/smtp/hooks/smtp.py",
      "tests/providers/smtp/hooks/test_smtp.py||tests/providers/smtp/hooks/test_smtp.py"
    ]
  },
  "patch_diff": {
    "airflow/providers/smtp/hooks/smtp.py||airflow/providers/smtp/hooks/smtp.py": [
      "File: airflow/providers/smtp/hooks/smtp.py -> airflow/providers/smtp/hooks/smtp.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "26: import os",
      "27: import re",
      "28: import smtplib",
      "29: from email.mime.application import MIMEApplication",
      "30: from email.mime.multipart import MIMEMultipart",
      "31: from email.mime.text import MIMEText",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "29: import ssl",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "109:             smtp_kwargs[\"port\"] = self.port",
      "110:         smtp_kwargs[\"timeout\"] = self.timeout",
      "112:         return SMTP(**smtp_kwargs)",
      "114:     @classmethod",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "112:         if self.use_ssl:",
      "113:             from airflow.configuration import conf",
      "115:             ssl_context_string = conf.get(\"smtp_provider\", \"SSL_CONTEXT\", fallback=None)",
      "116:             if ssl_context_string is None:",
      "117:                 ssl_context_string = conf.get(\"email\", \"SSL_CONTEXT\", fallback=None)",
      "118:             if ssl_context_string is None:",
      "119:                 ssl_context_string = \"default\"",
      "120:             if ssl_context_string == \"default\":",
      "121:                 ssl_context = ssl.create_default_context()",
      "122:             elif ssl_context_string == \"none\":",
      "123:                 ssl_context = None",
      "124:             else:",
      "125:                 raise RuntimeError(",
      "126:                     f\"The email.ssl_context configuration variable must \"",
      "127:                     f\"be set to 'default' or 'none' and is '{ssl_context_string}'.\"",
      "128:                 )",
      "129:             smtp_kwargs[\"context\"] = ssl_context",
      "",
      "---------------"
    ],
    "tests/providers/smtp/hooks/test_smtp.py||tests/providers/smtp/hooks/test_smtp.py": [
      "File: tests/providers/smtp/hooks/test_smtp.py -> tests/providers/smtp/hooks/test_smtp.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from airflow.providers.smtp.hooks.smtp import SmtpHook",
      "31: from airflow.utils import db",
      "32: from airflow.utils.session import create_session",
      "34: smtplib_string = \"airflow.providers.smtp.hooks.smtp.smtplib\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "75:         )",
      "77:     @patch(smtplib_string)",
      "79:         mock_conn = _create_fake_smtp(mock_smtplib)",
      "81:         with SmtpHook():",
      "82:             pass",
      "85:         mock_conn.login.assert_called_once_with(\"smtp_user\", \"smtp_password\")",
      "86:         assert mock_conn.close.call_count == 1",
      "",
      "[Removed Lines]",
      "78:     def test_connect_and_disconnect(self, mock_smtplib):",
      "84:         mock_smtplib.SMTP_SSL.assert_called_once_with(host=\"smtp_server_address\", port=465, timeout=30)",
      "",
      "[Added Lines]",
      "79:     @patch(\"ssl.create_default_context\")",
      "80:     def test_connect_and_disconnect(self, create_default_context, mock_smtplib):",
      "85:         assert create_default_context.called",
      "86:         mock_smtplib.SMTP_SSL.assert_called_once_with(",
      "87:             host=\"smtp_server_address\", port=465, timeout=30, context=create_default_context.return_value",
      "88:         )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "202:     @patch(\"smtplib.SMTP_SSL\")",
      "203:     @patch(\"smtplib.SMTP\")",
      "205:         mock_smtp_ssl.return_value = Mock()",
      "206:         with SmtpHook() as smtp_hook:",
      "207:             smtp_hook.send_email_smtp(to=\"to\", subject=\"subject\", html_content=\"content\", from_email=\"from\")",
      "208:         assert not mock_smtp.called",
      "211:     @patch(\"smtplib.SMTP_SSL\")",
      "212:     @patch(\"smtplib.SMTP\")",
      "",
      "[Removed Lines]",
      "204:     def test_send_mime_ssl(self, mock_smtp, mock_smtp_ssl):",
      "209:         mock_smtp_ssl.assert_called_once_with(host=\"smtp_server_address\", port=465, timeout=30)",
      "",
      "[Added Lines]",
      "208:     @patch(\"ssl.create_default_context\")",
      "209:     def test_send_mime_ssl(self, create_default_context, mock_smtp, mock_smtp_ssl):",
      "214:         assert create_default_context.called",
      "215:         mock_smtp_ssl.assert_called_once_with(",
      "216:             host=\"smtp_server_address\", port=465, timeout=30, context=create_default_context.return_value",
      "217:         )",
      "219:     @patch(\"smtplib.SMTP_SSL\")",
      "220:     @patch(\"smtplib.SMTP\")",
      "221:     @patch(\"ssl.create_default_context\")",
      "222:     def test_send_mime_ssl_none_email_context(self, create_default_context, mock_smtp, mock_smtp_ssl):",
      "223:         mock_smtp_ssl.return_value = Mock()",
      "224:         with conf_vars({(\"smtp\", \"smtp_ssl\"): \"True\", (\"email\", \"ssl_context\"): \"none\"}):",
      "225:             with SmtpHook() as smtp_hook:",
      "226:                 smtp_hook.send_email_smtp(",
      "227:                     to=\"to\", subject=\"subject\", html_content=\"content\", from_email=\"from\"",
      "228:                 )",
      "229:         assert not mock_smtp.called",
      "230:         assert not create_default_context.called",
      "231:         mock_smtp_ssl.assert_called_once_with(host=\"smtp_server_address\", port=465, timeout=30, context=None)",
      "233:     @patch(\"smtplib.SMTP_SSL\")",
      "234:     @patch(\"smtplib.SMTP\")",
      "235:     @patch(\"ssl.create_default_context\")",
      "236:     def test_send_mime_ssl_none_smtp_provider_context(self, create_default_context, mock_smtp, mock_smtp_ssl):",
      "237:         mock_smtp_ssl.return_value = Mock()",
      "238:         with conf_vars({(\"smtp\", \"smtp_ssl\"): \"True\", (\"smtp_provider\", \"ssl_context\"): \"none\"}):",
      "239:             with SmtpHook() as smtp_hook:",
      "240:                 smtp_hook.send_email_smtp(",
      "241:                     to=\"to\", subject=\"subject\", html_content=\"content\", from_email=\"from\"",
      "242:                 )",
      "243:         assert not mock_smtp.called",
      "244:         assert not create_default_context.called",
      "245:         mock_smtp_ssl.assert_called_once_with(host=\"smtp_server_address\", port=465, timeout=30, context=None)",
      "247:     @patch(\"smtplib.SMTP_SSL\")",
      "248:     @patch(\"smtplib.SMTP\")",
      "249:     @patch(\"ssl.create_default_context\")",
      "250:     def test_send_mime_ssl_none_smtp_provider_default_email_context(",
      "251:         self, create_default_context, mock_smtp, mock_smtp_ssl",
      "252:     ):",
      "253:         mock_smtp_ssl.return_value = Mock()",
      "254:         with conf_vars(",
      "255:             {",
      "256:                 (\"smtp\", \"smtp_ssl\"): \"True\",",
      "257:                 (\"email\", \"ssl_context\"): \"default\",",
      "258:                 (\"smtp_provider\", \"ssl_context\"): \"none\",",
      "259:             }",
      "260:         ):",
      "261:             with SmtpHook() as smtp_hook:",
      "262:                 smtp_hook.send_email_smtp(",
      "263:                     to=\"to\", subject=\"subject\", html_content=\"content\", from_email=\"from\"",
      "264:                 )",
      "265:         assert not mock_smtp.called",
      "266:         assert not create_default_context.called",
      "267:         mock_smtp_ssl.assert_called_once_with(host=\"smtp_server_address\", port=465, timeout=30, context=None)",
      "269:     @patch(\"smtplib.SMTP_SSL\")",
      "270:     @patch(\"smtplib.SMTP\")",
      "271:     @patch(\"ssl.create_default_context\")",
      "272:     def test_send_mime_ssl_default_smtp_provider_none_email_context(",
      "273:         self, create_default_context, mock_smtp, mock_smtp_ssl",
      "274:     ):",
      "275:         mock_smtp_ssl.return_value = Mock()",
      "276:         with conf_vars(",
      "277:             {",
      "278:                 (\"smtp\", \"smtp_ssl\"): \"True\",",
      "279:                 (\"email\", \"ssl_context\"): \"none\",",
      "280:                 (\"smtp_provider\", \"ssl_context\"): \"default\",",
      "281:             }",
      "282:         ):",
      "283:             with SmtpHook() as smtp_hook:",
      "284:                 smtp_hook.send_email_smtp(",
      "285:                     to=\"to\", subject=\"subject\", html_content=\"content\", from_email=\"from\"",
      "286:                 )",
      "287:         assert not mock_smtp.called",
      "288:         assert create_default_context.called",
      "289:         mock_smtp_ssl.assert_called_once_with(",
      "290:             host=\"smtp_server_address\", port=465, timeout=30, context=create_default_context.return_value",
      "291:         )",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "270:     @patch(\"airflow.models.connection.Connection\")",
      "271:     @patch(\"smtplib.SMTP_SSL\")",
      "273:         mock_smtp_ssl().sendmail.side_effect = smtplib.SMTPServerDisconnected()",
      "274:         custom_retry_limit = 10",
      "275:         custom_timeout = 60",
      "",
      "[Removed Lines]",
      "272:     def test_send_mime_custom_timeout_retrylimit(self, mock_smtp_ssl, connection_mock):",
      "",
      "[Added Lines]",
      "354:     @patch(\"ssl.create_default_context\")",
      "355:     def test_send_mime_custom_timeout_retrylimit(",
      "356:         self, create_default_context, mock_smtp_ssl, connection_mock",
      "357:     ):",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "287:             with pytest.raises(smtplib.SMTPServerDisconnected):",
      "288:                 smtp_hook.send_email_smtp(to=\"to\", subject=\"subject\", html_content=\"content\")",
      "289:         mock_smtp_ssl.assert_any_call(",
      "291:         )",
      "292:         assert mock_smtp_ssl().sendmail.call_count == 10",
      "",
      "[Removed Lines]",
      "290:             host=fake_conn.host, port=fake_conn.port, timeout=fake_conn.extra_dejson[\"timeout\"]",
      "",
      "[Added Lines]",
      "375:             host=fake_conn.host,",
      "376:             port=fake_conn.port,",
      "377:             timeout=fake_conn.extra_dejson[\"timeout\"],",
      "378:             context=create_default_context.return_value,",
      "380:         assert create_default_context.called",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "e9c82d9319c0f27d40f49aca612eb1713e06f4ea",
      "candidate_info": {
        "commit_hash": "e9c82d9319c0f27d40f49aca612eb1713e06f4ea",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e9c82d9319c0f27d40f49aca612eb1713e06f4ea",
        "files": [
          "airflow/jobs/backfill_job_runner.py",
          "tests/providers/daskexecutor/test_dask_executor.py"
        ],
        "message": "wOrkaround failing dedlock when running backfill (#32991)\n\nThe dask_executor backfill tests started to fail recently more often due\nto backfill exception, and the likely cause for it is that it is now\nbetter parallelise execution and triggering of the deadlocks because of\ncontention betwee dag_run state update and task state update had\nbecome much easier.\n\nWhile this PR does not fix the underlying issue, it catches the\noperational error where the deadlock occured during the backfill.\nand rolls back the operation.\n\nThis **should** be safe. backfil has a built-in mechanism to loop and\nretry failed tasks and the test passed multiple times, completing the\nbackfill after this fix was applied. It was not easy to reproduce it\nlocally but it failed every 20-30 times. When extra logging was added,\nit was always connected to OperationalException raised (and caught)\nright after _per_task_process. The same exception was observed few times\nwhen rollback was added, and despite it backfill job retried and\ncompleted the process successfully every time. We also leave the logs\nwith exceptions and add reassuring messages that should make it clear\nthat in case backfill completes, the exceptions can be ignored as\nthe updates will be retried by the backfill job.\n\nFixes: #32778\n(cherry picked from commit f616ee8dff8e6ba9b37cbce0d22235dc47d4fc93)",
        "before_after_code_files": [
          "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py",
          "tests/providers/daskexecutor/test_dask_executor.py||tests/providers/daskexecutor/test_dask_executor.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py": [
          "File: airflow/jobs/backfill_job_runner.py -> airflow/jobs/backfill_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "588:             try:",
          "589:                 for task in self.dag.topological_sort(include_subdag_tasks=True):",
          "590:                     for key, ti in list(ti_status.to_run.items()):",
          "615:                             )",
          "621:                                 states=self.STATES_COUNT_AS_RUNNING,",
          "622:                                 session=session,",
          "623:                             )",
          "628:                                 )",
          "642:                                 )",
          "646:             except (NoAvailablePoolSlot, DagConcurrencyLimitReached, TaskConcurrencyLimitReached) as e:",
          "647:                 self.log.debug(e)",
          "",
          "[Removed Lines]",
          "591:                         if task.task_id != ti.task_id:",
          "592:                             continue",
          "594:                         pool = session.scalar(",
          "595:                             select(models.Pool).where(models.Pool.pool == task.pool).limit(1)",
          "596:                         )",
          "597:                         if not pool:",
          "598:                             raise PoolNotFound(f\"Unknown pool: {task.pool}\")",
          "600:                         open_slots = pool.open_slots(session=session)",
          "601:                         if open_slots <= 0:",
          "602:                             raise NoAvailablePoolSlot(",
          "603:                                 f\"Not scheduling since there are {open_slots} open slots in pool {task.pool}\"",
          "604:                             )",
          "606:                         num_running_task_instances_in_dag = DAG.get_num_task_instances(",
          "607:                             self.dag_id,",
          "608:                             states=self.STATES_COUNT_AS_RUNNING,",
          "609:                             session=session,",
          "610:                         )",
          "612:                         if num_running_task_instances_in_dag >= self.dag.max_active_tasks:",
          "613:                             raise DagConcurrencyLimitReached(",
          "614:                                 \"Not scheduling since DAG max_active_tasks limit is reached.\"",
          "617:                         if task.max_active_tis_per_dag is not None:",
          "618:                             num_running_task_instances_in_task = DAG.get_num_task_instances(",
          "619:                                 dag_id=self.dag_id,",
          "620:                                 task_ids=[task.task_id],",
          "625:                             if num_running_task_instances_in_task >= task.max_active_tis_per_dag:",
          "626:                                 raise TaskConcurrencyLimitReached(",
          "627:                                     \"Not scheduling since Task concurrency limit is reached.\"",
          "630:                         if task.max_active_tis_per_dagrun is not None:",
          "631:                             num_running_task_instances_in_task_dagrun = DAG.get_num_task_instances(",
          "632:                                 dag_id=self.dag_id,",
          "633:                                 run_id=ti.run_id,",
          "634:                                 task_ids=[task.task_id],",
          "635:                                 states=self.STATES_COUNT_AS_RUNNING,",
          "636:                                 session=session,",
          "637:                             )",
          "639:                             if num_running_task_instances_in_task_dagrun >= task.max_active_tis_per_dagrun:",
          "640:                                 raise TaskConcurrencyLimitReached(",
          "641:                                     \"Not scheduling since Task concurrency per DAG run limit is reached.\"",
          "644:                         _per_task_process(key, ti, session)",
          "645:                         session.commit()",
          "",
          "[Added Lines]",
          "591:                         # Attempt to workaround deadlock on backfill by attempting to commit the transaction",
          "592:                         # state update few times before giving up",
          "593:                         max_attempts = 5",
          "594:                         for i in range(max_attempts):",
          "595:                             if task.task_id != ti.task_id:",
          "596:                                 continue",
          "598:                             pool = session.scalar(",
          "599:                                 select(models.Pool).where(models.Pool.pool == task.pool).limit(1)",
          "601:                             if not pool:",
          "602:                                 raise PoolNotFound(f\"Unknown pool: {task.pool}\")",
          "604:                             open_slots = pool.open_slots(session=session)",
          "605:                             if open_slots <= 0:",
          "606:                                 raise NoAvailablePoolSlot(",
          "607:                                     f\"Not scheduling since there are {open_slots} \"",
          "608:                                     f\"open slots in pool {task.pool}\"",
          "609:                                 )",
          "611:                             num_running_task_instances_in_dag = DAG.get_num_task_instances(",
          "612:                                 self.dag_id,",
          "617:                             if num_running_task_instances_in_dag >= self.dag.max_active_tasks:",
          "618:                                 raise DagConcurrencyLimitReached(",
          "619:                                     \"Not scheduling since DAG max_active_tasks limit is reached.\"",
          "622:                             if task.max_active_tis_per_dag is not None:",
          "623:                                 num_running_task_instances_in_task = DAG.get_num_task_instances(",
          "624:                                     dag_id=self.dag_id,",
          "625:                                     task_ids=[task.task_id],",
          "626:                                     states=self.STATES_COUNT_AS_RUNNING,",
          "627:                                     session=session,",
          "628:                                 )",
          "630:                                 if num_running_task_instances_in_task >= task.max_active_tis_per_dag:",
          "631:                                     raise TaskConcurrencyLimitReached(",
          "632:                                         \"Not scheduling since Task concurrency limit is reached.\"",
          "633:                                     )",
          "635:                             if task.max_active_tis_per_dagrun is not None:",
          "636:                                 num_running_task_instances_in_task_dagrun = DAG.get_num_task_instances(",
          "637:                                     dag_id=self.dag_id,",
          "638:                                     run_id=ti.run_id,",
          "639:                                     task_ids=[task.task_id],",
          "640:                                     states=self.STATES_COUNT_AS_RUNNING,",
          "641:                                     session=session,",
          "644:                                 if (",
          "645:                                     num_running_task_instances_in_task_dagrun",
          "646:                                     >= task.max_active_tis_per_dagrun",
          "647:                                 ):",
          "648:                                     raise TaskConcurrencyLimitReached(",
          "649:                                         \"Not scheduling since Task concurrency per DAG run limit is reached.\"",
          "650:                                     )",
          "652:                             _per_task_process(key, ti, session)",
          "653:                             try:",
          "654:                                 session.commit()",
          "655:                                 # break the retry loop",
          "656:                                 break",
          "657:                             except OperationalError:",
          "658:                                 self.log.error(",
          "659:                                     \"Failed to commit task state due to operational error. \"",
          "660:                                     \"The job will retry this operation so if your backfill succeeds, \"",
          "661:                                     \"you can safely ignore this message.\",",
          "662:                                     exc_info=True,",
          "663:                                 )",
          "664:                                 session.rollback()",
          "665:                                 if i == max_attempts - 1:",
          "666:                                     raise",
          "667:                                 # retry the loop",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "939:             # TODO: we will need to terminate running task instances and set the",
          "940:             # state to failed.",
          "941:             self._set_unfinished_dag_runs_to_failed(ti_status.active_runs)",
          "942:         finally:",
          "943:             session.commit()",
          "944:             executor.end()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "964:         except OperationalError:",
          "965:             self.log.error(",
          "966:                 \"Backfill job dead-locked. The job will retry the job so it is likely \"",
          "967:                 \"to heal itself. If your backfill succeeds you can ignore this exception.\",",
          "968:                 exc_info=True,",
          "969:             )",
          "970:             raise",
          "",
          "---------------"
        ],
        "tests/providers/daskexecutor/test_dask_executor.py||tests/providers/daskexecutor/test_dask_executor.py": [
          "File: tests/providers/daskexecutor/test_dask_executor.py -> tests/providers/daskexecutor/test_dask_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "104:     # This test is quarantined because it became rather flaky on our CI in July 2023 and reason for this",
          "105:     # is unknown. An issue for that was created: https://github.com/apache/airflow/issues/32778 and the",
          "106:     # marker should be removed while (possibly) the reason for flaky behaviour is found and fixed.",
          "108:     @pytest.mark.execution_timeout(180)",
          "109:     def test_backfill_integration(self):",
          "110:         \"\"\"",
          "",
          "[Removed Lines]",
          "107:     @pytest.mark.quarantined",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a050a6a72d0c00f12869b02fa897980bb4a5d293",
      "candidate_info": {
        "commit_hash": "a050a6a72d0c00f12869b02fa897980bb4a5d293",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a050a6a72d0c00f12869b02fa897980bb4a5d293",
        "files": [
          "RELEASE_NOTES.rst",
          "airflow/migrations/versions/0128_2_7_0_add_include_deferred_column_to_pool.py",
          "docs/apache-airflow/img/airflow_erd.sha256",
          "docs/apache-airflow/img/airflow_erd.svg"
        ],
        "message": "Ensure include_deferred is not nullable (#33280)\n\n* Ensure include_deferred is not nullable\n\nThis uses a multi-step process to introduce the NOT NULL constraint to\nthe column, needed due to MSSQL restrictions.\n\n* MSSQL does not even support boolean literals\n\n* Postgres does not support 0 as boolean\n\nThis is fine. SQL is fun.\n\n* Wrap raw SQL in text()\n\n(cherry picked from commit 4f6d5973589577ffcb7db24c82dd6708b2c00023)",
        "before_after_code_files": [
          "airflow/migrations/versions/0128_2_7_0_add_include_deferred_column_to_pool.py||airflow/migrations/versions/0128_2_7_0_add_include_deferred_column_to_pool.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/migrations/versions/0128_2_7_0_add_include_deferred_column_to_pool.py||airflow/migrations/versions/0128_2_7_0_add_include_deferred_column_to_pool.py": [
          "File: airflow/migrations/versions/0128_2_7_0_add_include_deferred_column_to_pool.py -> airflow/migrations/versions/0128_2_7_0_add_include_deferred_column_to_pool.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "39: def upgrade():",
          "40:     \"\"\"Apply add include_deferred column to pool\"\"\"",
          "41:     with op.batch_alter_table(\"slot_pool\") as batch_op:",
          "45: def downgrade():",
          "",
          "[Removed Lines]",
          "42:         batch_op.add_column(sa.Column(\"include_deferred\", sa.Boolean, default=False))",
          "",
          "[Added Lines]",
          "42:         batch_op.add_column(sa.Column(\"include_deferred\", sa.Boolean))",
          "43:     # Different databases support different literal for FALSE. This is fine.",
          "44:     op.execute(sa.text(f\"UPDATE slot_pool SET include_deferred = {sa.false().compile(op.get_bind())}\"))",
          "45:     with op.batch_alter_table(\"slot_pool\") as batch_op:",
          "46:         batch_op.alter_column(\"include_deferred\", existing_type=sa.Boolean, nullable=False)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "14ee53527043978a42a1747e53edf81f40907e17",
      "candidate_info": {
        "commit_hash": "14ee53527043978a42a1747e53edf81f40907e17",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/14ee53527043978a42a1747e53edf81f40907e17",
        "files": [
          "airflow/www/static/js/dag/details/gantt/GanttTooltip.tsx",
          "airflow/www/static/js/dag/details/gantt/Row.tsx",
          "airflow/www/static/js/dag/details/gantt/index.tsx"
        ],
        "message": "Gantt chart: Use earliest/oldest ti dates if different than dag run start/end (#33215)\n\n(cherry picked from commit 1dcf05f1c69e52667064259308ba5950b594b268)",
        "before_after_code_files": [
          "airflow/www/static/js/dag/details/gantt/GanttTooltip.tsx||airflow/www/static/js/dag/details/gantt/GanttTooltip.tsx",
          "airflow/www/static/js/dag/details/gantt/Row.tsx||airflow/www/static/js/dag/details/gantt/Row.tsx",
          "airflow/www/static/js/dag/details/gantt/index.tsx||airflow/www/static/js/dag/details/gantt/index.tsx"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/static/js/dag/details/gantt/GanttTooltip.tsx||airflow/www/static/js/dag/details/gantt/GanttTooltip.tsx": [
          "File: airflow/www/static/js/dag/details/gantt/GanttTooltip.tsx -> airflow/www/static/js/dag/details/gantt/GanttTooltip.tsx",
          "--- Hunk 1 ---",
          "[Context before]",
          "42:       <Text>",
          "43:         Task{isGroup ? \" Group\" : \"\"}: {task.label}",
          "44:       </Text>",
          "45:       <br />",
          "46:       {instance?.queuedDttm && (",
          "47:         <Text>",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "45:       {!!instance?.tryNumber && instance.tryNumber > 1 && (",
          "46:         <Text>Try Number: {instance.tryNumber}</Text>",
          "47:       )}",
          "",
          "---------------"
        ],
        "airflow/www/static/js/dag/details/gantt/Row.tsx||airflow/www/static/js/dag/details/gantt/Row.tsx": [
          "File: airflow/www/static/js/dag/details/gantt/Row.tsx -> airflow/www/static/js/dag/details/gantt/Row.tsx",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import { SimpleStatus } from \"src/dag/StatusBox\";",
          "25: import { useContainerRef } from \"src/context/containerRef\";",
          "26: import { hoverDelay } from \"src/utils\";",
          "28: import GanttTooltip from \"./GanttTooltip\";",
          "30: interface Props {",
          "31:   ganttWidth?: number;",
          "32:   openGroupIds: string[];",
          "34:   task: Task;",
          "35: }",
          "38:   const {",
          "39:     selected: { runId, taskId },",
          "40:     onSelect,",
          "41:   } = useSelection();",
          "42:   const containerRef = useContainerRef();",
          "46:   const instance = task.instances.find((ti) => ti.runId === runId);",
          "47:   const isSelected = taskId === instance?.taskId;",
          "",
          "[Removed Lines]",
          "27: import type { DagRun, Task } from \"src/types\";",
          "33:   dagRun: DagRun;",
          "37: const Row = ({ ganttWidth = 500, openGroupIds, task, dagRun }: Props) => {",
          "44:   const runDuration = getDuration(dagRun?.startDate, dagRun?.endDate);",
          "",
          "[Added Lines]",
          "27: import type { Task } from \"src/types\";",
          "34:   ganttStartDate?: string | null;",
          "35:   ganttEndDate?: string | null;",
          "38: const Row = ({",
          "39:   ganttWidth = 500,",
          "40:   openGroupIds,",
          "41:   task,",
          "42:   ganttStartDate,",
          "43:   ganttEndDate,",
          "44: }: Props) => {",
          "51:   const runDuration = getDuration(ganttStartDate, ganttEndDate);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "54:     ? getDuration(instance?.queuedDttm, instance?.startDate)",
          "55:     : 0;",
          "56:   const taskStartOffset = getDuration(",
          "58:     instance?.queuedDttm || instance?.startDate",
          "59:   );",
          "",
          "[Removed Lines]",
          "57:     dagRun.startDate,",
          "",
          "[Added Lines]",
          "64:     ganttStartDate,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "127:           <Row",
          "128:             ganttWidth={ganttWidth}",
          "129:             openGroupIds={openGroupIds}",
          "131:             task={c}",
          "132:             key={`gantt-${c.id}`}",
          "133:           />",
          "",
          "[Removed Lines]",
          "130:             dagRun={dagRun}",
          "",
          "[Added Lines]",
          "137:             ganttStartDate={ganttStartDate}",
          "138:             ganttEndDate={ganttEndDate}",
          "",
          "---------------"
        ],
        "airflow/www/static/js/dag/details/gantt/index.tsx||airflow/www/static/js/dag/details/gantt/index.tsx": [
          "File: airflow/www/static/js/dag/details/gantt/index.tsx -> airflow/www/static/js/dag/details/gantt/index.tsx",
          "--- Hunk 1 ---",
          "[Context before]",
          "107:   const dagRun = dagRuns.find((dr) => dr.runId === runId);",
          "111:   const numBars = Math.round(width / 100);",
          "113:   const intervals = runDuration / numBars;",
          "115:   return (",
          "",
          "[Removed Lines]",
          "109:   const startDate = dagRun?.startDate;",
          "112:   const runDuration = getDuration(dagRun?.startDate, dagRun?.endDate);",
          "",
          "[Added Lines]",
          "109:   let startDate = dagRun?.queuedAt || dagRun?.startDate;",
          "110:   let endDate = dagRun?.endDate;",
          "113:   groups.children?.forEach((task) => {",
          "114:     const taskInstance = task.instances.find((ti) => ti.runId === runId);",
          "115:     if (",
          "116:       taskInstance?.queuedDttm &&",
          "117:       (!startDate ||",
          "118:         Date.parse(taskInstance.queuedDttm) < Date.parse(startDate))",
          "119:     ) {",
          "120:       startDate = taskInstance.queuedDttm;",
          "121:     } else if (",
          "122:       taskInstance?.startDate &&",
          "123:       (!startDate || Date.parse(taskInstance.startDate) < Date.parse(startDate))",
          "124:     ) {",
          "125:       startDate = taskInstance.startDate;",
          "126:     }",
          "128:     if (",
          "129:       taskInstance?.endDate &&",
          "130:       (!endDate || Date.parse(taskInstance.endDate) > Date.parse(endDate))",
          "131:     ) {",
          "132:       endDate = taskInstance.endDate;",
          "133:     }",
          "134:   });",
          "137:   const runDuration = getDuration(startDate, endDate);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "166:               <Row",
          "167:                 ganttWidth={width}",
          "168:                 openGroupIds={openGroupIds}",
          "170:                 task={c}",
          "171:                 key={`gantt-${c.id}`}",
          "172:               />",
          "173:             ))}",
          "",
          "[Removed Lines]",
          "169:                 dagRun={dagRun}",
          "",
          "[Added Lines]",
          "195:                 ganttStartDate={startDate}",
          "196:                 ganttEndDate={endDate}",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2f894ca218762960a215be89d2502d5431ccc7a1",
      "candidate_info": {
        "commit_hash": "2f894ca218762960a215be89d2502d5431ccc7a1",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/2f894ca218762960a215be89d2502d5431ccc7a1",
        "files": [
          "airflow/providers/openlineage/plugins/openlineage.py",
          "tests/providers/openlineage/plugins/test_openlineage.py"
        ],
        "message": "openlineage: disable running listener if not configured (#33120)\n\nSigned-off-by: Maciej Obuchowski <obuchowski.maciej@gmail.com>\n(cherry picked from commit 11ff650e1b122aadebcea462adfae5492a76ed94)",
        "before_after_code_files": [
          "airflow/providers/openlineage/plugins/openlineage.py||airflow/providers/openlineage/plugins/openlineage.py",
          "tests/providers/openlineage/plugins/test_openlineage.py||tests/providers/openlineage/plugins/test_openlineage.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/openlineage/plugins/openlineage.py||airflow/providers/openlineage/plugins/openlineage.py": [
          "File: airflow/providers/openlineage/plugins/openlineage.py -> airflow/providers/openlineage/plugins/openlineage.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27:     return (",
          "28:         conf.getboolean(\"openlineage\", \"disabled\")",
          "29:         or os.getenv(\"OPENLINEAGE_DISABLED\", \"false\").lower() == \"true\"",
          "30:     )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "30:         or (",
          "31:             conf.get(\"openlineage\", \"transport\") == \"\"",
          "32:             and conf.get(\"openlineage\", \"config_path\") == \"\"",
          "33:             and os.getenv(\"OPENLINEAGE_URL\", \"\") == \"\"",
          "34:             and os.getenv(\"OPENLINEAGE_CONFIG\", \"\") == \"\"",
          "35:         )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39:     \"\"\"",
          "41:     name = \"OpenLineageProviderPlugin\"",
          "43:     if not _is_disabled():",
          "44:         from airflow.providers.openlineage.plugins.listener import OpenLineageListener",
          "46:         listeners = [OpenLineageListener()]",
          "",
          "[Removed Lines]",
          "42:     macros = [lineage_run_id, lineage_parent_id]",
          "",
          "[Added Lines]",
          "51:         macros = [lineage_run_id, lineage_parent_id]",
          "",
          "---------------"
        ],
        "tests/providers/openlineage/plugins/test_openlineage.py||tests/providers/openlineage/plugins/test_openlineage.py": [
          "File: tests/providers/openlineage/plugins/test_openlineage.py -> tests/providers/openlineage/plugins/test_openlineage.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "39:     @pytest.mark.parametrize(",
          "40:         \"mocks, expected\",",
          "41:         [",
          "44:             (",
          "45:                 [",
          "46:                     conf_vars({(\"openlineage\", \"disabled\"): \"False\"}),",
          "",
          "[Removed Lines]",
          "42:             ([patch.dict(os.environ, {\"OPENLINEAGE_DISABLED\": \"true\"}, 0)], 0),",
          "43:             ([conf_vars({(\"openlineage\", \"disabled\"): \"False\"})], 1),",
          "",
          "[Added Lines]",
          "42:             ([patch.dict(os.environ, {\"OPENLINEAGE_DISABLED\": \"true\"})], 0),",
          "43:             (",
          "44:                 [",
          "45:                     conf_vars(",
          "46:                         {(\"openlineage\", \"transport\"): '{\"type\": \"http\", \"url\": \"http://localhost:5000\"}'}",
          "47:                     ),",
          "48:                     patch.dict(os.environ, {\"OPENLINEAGE_DISABLED\": \"true\"}),",
          "49:                 ],",
          "50:                 0,",
          "51:             ),",
          "52:             ([patch.dict(os.environ, {\"OPENLINEAGE_DISABLED\": \"false\"})], 0),",
          "53:             (",
          "54:                 [",
          "55:                     conf_vars(",
          "56:                         {",
          "57:                             (\"openlineage\", \"disabled\"): \"False\",",
          "58:                             (\"openlineage\", \"transport\"): '{\"type\": \"http\", \"url\": \"http://localhost:5000\"}',",
          "59:                         }",
          "60:                     )",
          "61:                 ],",
          "62:                 1,",
          "63:             ),",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "48:                 ],",
          "49:                 0,",
          "50:             ),",
          "52:         ],",
          "53:     )",
          "54:     def test_plugin_disablements(self, mocks, expected):",
          "",
          "[Removed Lines]",
          "51:             ([], 1),",
          "",
          "[Added Lines]",
          "71:             ([], 0),",
          "72:             ([patch.dict(os.environ, {\"OPENLINEAGE_URL\": \"http://localhost:8080\"})], 1),",
          "73:             (",
          "74:                 [",
          "75:                     conf_vars(",
          "76:                         {(\"openlineage\", \"transport\"): '{\"type\": \"http\", \"url\": \"http://localhost:5000\"}'}",
          "77:                     )",
          "78:                 ],",
          "79:                 1,",
          "80:             ),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b0998038936d290e1ca5e237930c884fec1ce9e4",
      "candidate_info": {
        "commit_hash": "b0998038936d290e1ca5e237930c884fec1ce9e4",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b0998038936d290e1ca5e237930c884fec1ce9e4",
        "files": [
          "airflow/models/abstractoperator.py",
          "airflow/models/taskmixin.py",
          "airflow/models/xcom_arg.py",
          "airflow/utils/edgemodifier.py",
          "airflow/utils/task_group.py"
        ],
        "message": "Remove add_task from TaskGroup (#33262)\n\nThis was added during work for AIP-52, taking inspiration from the method on the setupteardowncontext object.  But it causes problems because it's assumed in the task_id label logic that if the group is set to prefix task ids then the task id has been prefixed.  This results in bad graph labeling in UI.  Rather than fix it now, at release time, better to revert.\n\n(cherry picked from commit 2d2a1d699b8069f59604c6d6ea3e29853faa7945)",
        "before_after_code_files": [
          "airflow/models/abstractoperator.py||airflow/models/abstractoperator.py",
          "airflow/models/taskmixin.py||airflow/models/taskmixin.py",
          "airflow/models/xcom_arg.py||airflow/models/xcom_arg.py",
          "airflow/utils/edgemodifier.py||airflow/utils/edgemodifier.py",
          "airflow/utils/task_group.py||airflow/utils/task_group.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/33247"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/abstractoperator.py||airflow/models/abstractoperator.py": [
          "File: airflow/models/abstractoperator.py -> airflow/models/abstractoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "360:                 yield parent",
          "361:             parent = parent.task_group",
          "371:     def get_closest_mapped_task_group(self) -> MappedTaskGroup | None:",
          "372:         \"\"\"Get the mapped task group \"closest\" to this task in the DAG.",
          "",
          "[Removed Lines]",
          "363:     def add_to_taskgroup(self, task_group: TaskGroup) -> None:",
          "364:         \"\"\"Add the task to the given task group.",
          "366:         :meta private:",
          "367:         \"\"\"",
          "368:         if self.node_id not in task_group.children:",
          "369:             task_group.add(self)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/models/taskmixin.py||airflow/models/taskmixin.py": [
          "File: airflow/models/taskmixin.py -> airflow/models/taskmixin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "113:         self.__rshift__(other)",
          "114:         return self",
          "121:     @classmethod",
          "122:     def _iter_references(cls, obj: Any) -> Iterable[tuple[DependencyMixin, str]]:",
          "123:         from airflow.models.baseoperator import AbstractOperator",
          "",
          "[Removed Lines]",
          "116:     @abstractmethod",
          "117:     def add_to_taskgroup(self, task_group: TaskGroup) -> None:",
          "118:         \"\"\"Add the task to the given task group.\"\"\"",
          "119:         raise NotImplementedError()",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/models/xcom_arg.py||airflow/models/xcom_arg.py": [
          "File: airflow/models/xcom_arg.py -> airflow/models/xcom_arg.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "43: if TYPE_CHECKING:",
          "44:     from airflow.models.dag import DAG",
          "45:     from airflow.models.operator import Operator",
          "48: # Callable objects contained by MapXComArg. We only accept callables from",
          "49: # the user, but deserialize them into strings in a serialized XComArg for",
          "",
          "[Removed Lines]",
          "46:     from airflow.utils.task_group import TaskGroup",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "209:         \"\"\"",
          "210:         raise NotImplementedError()",
          "221:     def __enter__(self):",
          "222:         if not self.operator.is_setup and not self.operator.is_teardown:",
          "223:             raise AirflowException(\"Only setup/teardown tasks can be used as context managers.\")",
          "",
          "[Removed Lines]",
          "212:     def add_to_taskgroup(self, task_group: TaskGroup) -> None:",
          "213:         \"\"\"Add the task to the given task group.",
          "215:         :meta private:",
          "216:         \"\"\"",
          "217:         for op, _ in self.iter_references():",
          "218:             if op.node_id not in task_group.children:",
          "219:                 task_group.add(op)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/utils/edgemodifier.py||airflow/utils/edgemodifier.py": [
          "File: airflow/utils/edgemodifier.py -> airflow/utils/edgemodifier.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "169:         \"\"\"",
          "170:         dag.set_edge_info(upstream_id, downstream_id, {\"label\": self.label})",
          "182: # Factory functions",
          "183: def Label(label: str):",
          "",
          "[Removed Lines]",
          "172:     def add_to_taskgroup(self, task_group: TaskGroup) -> None:",
          "173:         \"\"\"No-op, since we're not a task.",
          "175:         We only add tasks to TaskGroups and not EdgeModifiers, but we need",
          "176:         this to satisfy the interface.",
          "178:         :meta private:",
          "179:         \"\"\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/utils/task_group.py||airflow/utils/task_group.py": [
          "File: airflow/utils/task_group.py -> airflow/utils/task_group.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "551:                         f\"Encountered a DAGNode that is not a TaskGroup or an AbstractOperator: {type(child)}\"",
          "552:                     )",
          "575: class MappedTaskGroup(TaskGroup):",
          "576:     \"\"\"A mapped task group.",
          "",
          "[Removed Lines]",
          "554:     def add_task(self, task: AbstractOperator) -> None:",
          "555:         \"\"\"Add a task to the task group.",
          "557:         :param task: the task to add",
          "558:         \"\"\"",
          "559:         if not TaskGroupContext.active:",
          "560:             raise AirflowException(",
          "561:                 \"Using this method on a task group that's not a context manager is not supported.\"",
          "562:             )",
          "563:         task.add_to_taskgroup(self)",
          "565:     def add_to_taskgroup(self, task_group: TaskGroup) -> None:",
          "566:         \"\"\"No-op, since we're not a task.",
          "568:         We only add tasks to TaskGroups and not TaskGroup, but we need",
          "569:         this to satisfy the interface.",
          "571:         :meta private:",
          "572:         \"\"\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    }
  ]
}