{
  "cve_id": "CVE-2021-45456",
  "cve_desc": "Apache kylin checks the legitimacy of the project before executing some commands with the project name passed in by the user. There is a mismatch between what is being checked and what is being used as the shell command argument in DiagnosisService. This may cause an illegal project name to pass the check and perform the following steps, resulting in a command injection vulnerability. This issue affects Apache Kylin 4.0.0.",
  "repo": "apache/kylin",
  "patch_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
  "patch_info": {
    "commit_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
    "repo": "apache/kylin",
    "commit_url": "https://github.com/apache/kylin/commit/f4daf14dde99b934c92ce2c832509f24342bc845",
    "files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "server/src/main/webapp/WEB-INF/web.xml"
    ],
    "message": "test fix",
    "before_after_code_files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java"
    ]
  },
  "patch_diff": {
    "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "3403:     public String getKerberosPrincipal() {",
      "3404:         return getOptional(\"kylin.kerberos.principal\");",
      "3405:     }",
      "3406: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "3407:     public String getEncryptCipherIvSpec() {",
      "3408:         return getOptional(\"kylin.security.encrypt.cipher.ivSpec\", \"AAAAAAAAAAAAAAAA\");",
      "3409:     }",
      "",
      "---------------"
    ],
    "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java -> core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "25: import java.security.NoSuchAlgorithmException;",
      "27: import org.apache.commons.codec.binary.Base64;",
      "29: import javax.crypto.Cipher;",
      "30: import javax.crypto.NoSuchPaddingException;",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "28: import org.apache.kylin.common.KylinConfig;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "42:             InvalidKeyException, NoSuchPaddingException, NoSuchAlgorithmException, UnsupportedEncodingException {",
      "43:         Cipher cipher = Cipher.getInstance(\"AES/CFB/PKCS5Padding\");",
      "44:         final SecretKeySpec secretKey = new SecretKeySpec(key, \"AES\");",
      "46:         cipher.init(cipherMode, secretKey, ivSpec);",
      "47:         return cipher;",
      "48:     }",
      "",
      "[Removed Lines]",
      "45:         IvParameterSpec ivSpec = new IvParameterSpec(\"AAAAAAAAAAAAAAAA\".getBytes(\"UTF-8\"));",
      "",
      "[Added Lines]",
      "46:         IvParameterSpec ivSpec = new IvParameterSpec(KylinConfig.getInstanceFromEnv().getEncryptCipherIvSpec().getBytes(\"UTF-8\"));",
      "",
      "---------------"
    ],
    "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java": [
      "File: core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java -> core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: package org.apache.kylin.common.util;",
      "21: import org.junit.Assert;",
      "22: import org.junit.Test;",
      "26:     @Test",
      "27:     public void testAESEncrypt(){",
      "",
      "[Removed Lines]",
      "24: public class EncryptUtilTest {",
      "",
      "[Added Lines]",
      "21: import org.junit.After;",
      "23: import org.junit.Before;",
      "26: public class EncryptUtilTest extends LocalFileMetadataTestCase {",
      "27:     @Before",
      "28:     public void setUp() throws Exception {",
      "29:         this.createTestMetadata();",
      "30:     }",
      "32:     @After",
      "33:     public void after() throws Exception {",
      "34:         this.cleanupTestMetadata();",
      "35:     }",
      "",
      "---------------"
    ],
    "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java": [
      "File: server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java -> server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "87:     public String dumpProjectDiagnosisInfo(String project, File exportPath) throws IOException {",
      "88:         Message msg = MsgPicker.getMsg();",
      "89:         ProjectInstance projectInstance =",
      "90:                 ProjectManager.getInstance(KylinConfig.getInstanceFromEnv())",
      "92:         if (null == projectInstance) {",
      "93:             throw new BadRequestException(",
      "95:         }",
      "96:         aclEvaluate.checkProjectOperationPermission(projectInstance);",
      "98:         runDiagnosisCLI(args);",
      "99:         return getDiagnosisPackageName(exportPath);",
      "100:     }",
      "",
      "[Removed Lines]",
      "91:                         .getProject(ValidateUtil.convertStringToBeAlphanumericUnderscore(project));",
      "94:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), project));",
      "97:         String[] args = { project, exportPath.getAbsolutePath() };",
      "",
      "[Added Lines]",
      "89:         String projectName = ValidateUtil.convertStringToBeAlphanumericUnderscore(project);",
      "92:                         .getProject(projectName);",
      "95:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), projectName));",
      "98:         String[] args = { projectName, exportPath.getAbsolutePath() };",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "e33e17e63e0fa273196c5a422c97112ef123383a",
      "candidate_info": {
        "commit_hash": "e33e17e63e0fa273196c5a422c97112ef123383a",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/e33e17e63e0fa273196c5a422c97112ef123383a",
        "files": [
          "docker/conf/kylin/kylin.properties",
          "docker/dockerfile/standalone/conf/kylin/kylin.properties"
        ],
        "message": "minor, change hive.client from `cli` to `spark_catalog`",
        "before_after_code_files": [
          "docker/conf/kylin/kylin.properties||docker/conf/kylin/kylin.properties",
          "docker/dockerfile/standalone/conf/kylin/kylin.properties||docker/dockerfile/standalone/conf/kylin/kylin.properties"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "docker/conf/kylin/kylin.properties||docker/conf/kylin/kylin.properties": [
          "File: docker/conf/kylin/kylin.properties -> docker/conf/kylin/kylin.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "101: ## When user deploy kylin on AWS EMR and Glue is used as external metadata, use gluecatalog instead",
          "102: #kylin.source.hive.metadata-type=hcatalog",
          "103: #",
          "106: #",
          "107: ## Absolute path to beeline shell, can be set to spark beeline instead of the default hive beeline on PATH",
          "108: #kylin.source.hive.beeline-shell=beeline",
          "",
          "[Removed Lines]",
          "104: ## Hive client, valid value [cli, beeline]",
          "105: #kylin.source.hive.client=cli",
          "",
          "[Added Lines]",
          "104: ## Hive client, valid value [spark_catalog]",
          "105: #kylin.source.hive.client=spark_catalog",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "146: #kylin.cube.cuboid-scheduler=org.apache.kylin.cube.cuboid.DefaultCuboidScheduler",
          "147: #kylin.cube.segment-advisor=org.apache.kylin.cube.CubeSegmentAdvisor",
          "148: #",
          "150: #kylin.cube.algorithm=layer",
          "151: #",
          "152: ## A smaller threshold prefers layer, a larger threshold prefers in-mem",
          "",
          "[Removed Lines]",
          "149: ## 'auto', 'inmem', 'layer' or 'random' for testing",
          "",
          "[Added Lines]",
          "149: ## 'auto', 'inmem', 'layer' or 'random' for testing",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "308: # Display timezone on UI,format like[GMT+N or GMT-N]",
          "309: kylin.web.timezone=GMT+8",
          "312: kylin.source.hive.database-for-flat-table=kylin4",
          "314: kylin.engine.spark-conf.spark.eventLog.enabled=true",
          "",
          "[Removed Lines]",
          "311: kylin.source.hive.client=cli",
          "",
          "[Added Lines]",
          "311: kylin.source.hive.client=spark_catalog",
          "",
          "---------------"
        ],
        "docker/dockerfile/standalone/conf/kylin/kylin.properties||docker/dockerfile/standalone/conf/kylin/kylin.properties": [
          "File: docker/dockerfile/standalone/conf/kylin/kylin.properties -> docker/dockerfile/standalone/conf/kylin/kylin.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "104: ## When user deploy kylin on AWS EMR and Glue is used as external metadata, use gluecatalog instead",
          "105: #kylin.source.hive.metadata-type=hcatalog",
          "106: #",
          "109: #",
          "110: ## Absolute path to beeline shell, can be set to spark beeline instead of the default hive beeline on PATH",
          "111: #kylin.source.hive.beeline-shell=beeline",
          "",
          "[Removed Lines]",
          "107: ## Hive client, valid value [cli, beeline]",
          "108: #kylin.source.hive.client=cli",
          "",
          "[Added Lines]",
          "107: ## Hive client, valid value [spark_catalog]",
          "108: #kylin.source.hive.client=spark_catalog",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "149: #kylin.cube.cuboid-scheduler=org.apache.kylin.cube.cuboid.DefaultCuboidScheduler",
          "150: #kylin.cube.segment-advisor=org.apache.kylin.cube.CubeSegmentAdvisor",
          "151: #",
          "153: #kylin.cube.algorithm=layer",
          "154: #",
          "155: ## A smaller threshold prefers layer, a larger threshold prefers in-mem",
          "",
          "[Removed Lines]",
          "152: ## 'auto', 'inmem', 'layer' or 'random' for testing",
          "",
          "[Added Lines]",
          "152: ## 'auto', 'inmem', 'layer' or 'random' for testing",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "312: # Display timezone on UI,format like[GMT+N or GMT-N]",
          "313: kylin.web.timezone=GMT+8",
          "316: kylin.source.hive.database-for-flat-table=kylin4",
          "318: kylin.engine.spark-conf.spark.eventLog.enabled=true",
          "",
          "[Removed Lines]",
          "315: kylin.source.hive.client=cli",
          "",
          "[Added Lines]",
          "315: kylin.source.hive.client=spark_catalog",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c2f261b09c17db9b7e9ee21117c5c7d783811fab",
      "candidate_info": {
        "commit_hash": "c2f261b09c17db9b7e9ee21117c5c7d783811fab",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/c2f261b09c17db9b7e9ee21117c5c7d783811fab",
        "files": [
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/spark/deploy/SparkApplicationClient.scala",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala"
        ],
        "message": "KYLIN-5023 Some fix for spark standalone",
        "before_after_code_files": [
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/spark/deploy/SparkApplicationClient.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/spark/deploy/SparkApplicationClient.scala",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ],
          "candidate": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ]
        }
      },
      "candidate_diff": {
        "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "1591:         return getPropertiesByPrefix(\"kylin.engine.flink-conf.\");",
          "1592:     }",
          "1596:     }",
          "1598:     public Map<String, String> getFlinkConfigOverrideWithSpecificName(String configName) {",
          "",
          "[Removed Lines]",
          "1594:     public Map<String, String> getSparkConfigOverrideWithSpecificName(String configName) {",
          "1595:         return getPropertiesByPrefix(\"kylin.engine.spark-conf-\" + configName + \".\");",
          "",
          "[Added Lines]",
          "1594:     public String getSparkEngineConfigOverrideWithSpecificName(String configName) {",
          "1595:         Map<String, String> config = getPropertiesByPrefix(\"kylin.engine.spark-conf.\" + configName);",
          "1596:         if (config.size() != 0) {",
          "1597:             return String.valueOf(config.values().iterator().next());",
          "1598:         }",
          "1599:         return null;",
          "1600:     }",
          "1602:     public String getSparderConfigOverrideWithSpecificName(String configName) {",
          "1603:         Map<String, String> config = getPropertiesByPrefix(\"kylin.query.spark-conf.\" + configName);",
          "1604:         if (config.size() != 0) {",
          "1605:             return String.valueOf(config.values().iterator().next());",
          "1606:         }",
          "1607:         return null;",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java||kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java -> kylin-spark-project/kylin-spark-engine/src/main/java/org/apache/kylin/engine/spark/job/NSparkExecutable.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "161:             return runLocalMode(filePath, config);",
          "162:         } else {",
          "163:             logger.info(\"Task id: {}\", getId());",
          "165:             return runSparkSubmit(config, hadoopConf, jars, kylinJobJar,",
          "166:                     \"-className \" + getSparkSubmitClassName() + \" \" + filePath, getParent().getId());",
          "167:         }",
          "",
          "[Removed Lines]",
          "164:             killOrphanApplicationIfExists(config, getId());",
          "",
          "[Added Lines]",
          "164:             if (\"yarn\".equals(config.getSparkEngineConfigOverrideWithSpecificName(\"spark.master\"))) {",
          "165:                 logger.info(\"Try to kill orphan application on yarn.\");",
          "166:                 killOrphanApplicationIfExists(config, getId());",
          "167:             }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/spark/deploy/SparkApplicationClient.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/spark/deploy/SparkApplicationClient.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/spark/deploy/SparkApplicationClient.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/spark/deploy/SparkApplicationClient.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "43:       case STANDALONE_CLUSTER =>",
          "44:         var appState = StandaloneAppClient.getAppState(stepId)",
          "45:         while (true) {",
          "46:           logInfo(s\"$stepId state is $appState .\")",
          "49:           }",
          "51:         }",
          "52:         appState",
          "53:       case m => throw new UnsupportedOperationException(\"waitAndCheckAppState \" + m)",
          "",
          "[Removed Lines]",
          "47:           if (!finalStates.contains(appState)) {",
          "48:             Thread.sleep(10000)",
          "50:           appState = StandaloneAppClient.getAppState(stepId)",
          "",
          "[Added Lines]",
          "46:           appState = StandaloneAppClient.getAppState(stepId)",
          "48:           if (finalStates.contains(appState)) {",
          "49:             return appState",
          "51:           Thread.sleep(10000)",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala": [
          "File: kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala -> kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "136:                 case \"true\" =>",
          "137:                   \"local\"",
          "138:                 case _ =>",
          "140:               }",
          "141:               val sparkSession = SparkSession.builder",
          "142:                 .master(master)",
          "143:                 .appName(kylinConf.getSparderAppName)",
          "144:                 .withExtensions { ext =>",
          "145:                   ext.injectPlannerStrategy(_ => KylinSourceStrategy)",
          "",
          "[Removed Lines]",
          "139:                   \"yarn-client\"",
          "",
          "[Added Lines]",
          "139:                   kylinConf.getSparderConfigOverrideWithSpecificName(\"spark.master\")",
          "141:               logInfo(\"SparderContext deploy with spark master: \" + master)",
          "144:                 .config(\"spark.submit.deployMode\", \"client\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d56d72f736db5ea52bcc9a672dd20dc6ee4aaaae",
      "candidate_info": {
        "commit_hash": "d56d72f736db5ea52bcc9a672dd20dc6ee4aaaae",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/d56d72f736db5ea52bcc9a672dd20dc6ee4aaaae",
        "files": [
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/JobMetricsUtils.scala"
        ],
        "message": "KYLIN-5121 fix key not found: numOutputRows on s3",
        "before_after_code_files": [
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/JobMetricsUtils.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/JobMetricsUtils.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/JobMetricsUtils.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/JobMetricsUtils.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/JobMetricsUtils.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/utils/JobMetricsUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "56:       case plan: UnaryExecNode =>",
          "57:         if (aggs.contains(plan.getClass) && !afterAgg) {",
          "58:           afterAgg = true",
          "60:         }",
          "61:       case plan: BinaryExecNode =>",
          "62:         if (joins.contains(plan.getClass) && !afterJoin) {",
          "64:           afterJoin = true",
          "65:         }",
          "66:       case plan: LeafExecNode =>",
          "",
          "[Removed Lines]",
          "59:           rowMetrics.setMetrics(Metrics.CUBOID_ROWS_CNT, plan.metrics.apply(\"numOutputRows\").value)",
          "63:           rowMetrics.setMetrics(Metrics.SOURCE_ROWS_CNT, plan.metrics.apply(\"numOutputRows\").value)",
          "",
          "[Added Lines]",
          "59:           if (plan.metrics.contains(\"numOutputRows\")) {",
          "60:             rowMetrics.setMetrics(Metrics.CUBOID_ROWS_CNT, plan.metrics.apply(\"numOutputRows\").value)",
          "61:           }",
          "65:           if (plan.metrics.contains(\"numOutputRows\")) {",
          "66:             rowMetrics.setMetrics(Metrics.SOURCE_ROWS_CNT, plan.metrics.apply(\"numOutputRows\").value)",
          "67:           }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "72:             rowMetrics.getMetrics(Metrics.SOURCE_ROWS_CNT)",
          "73:           }",
          "77:         }",
          "78:       case _ =>",
          "79:     }",
          "",
          "[Removed Lines]",
          "75:           val rowsCnt = preCnt + plan.metrics.apply(\"numOutputRows\").value",
          "76:           rowMetrics.setMetrics(Metrics.SOURCE_ROWS_CNT, rowsCnt)",
          "",
          "[Added Lines]",
          "79:           if (plan.metrics.contains(\"numOutputRows\")) {",
          "80:             val rowsCnt = preCnt + plan.metrics.apply(\"numOutputRows\").value",
          "81:             rowMetrics.setMetrics(Metrics.SOURCE_ROWS_CNT, rowsCnt)",
          "82:           }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d7d20645d797aa43ee41456d61a0a96bfd42d378",
      "candidate_info": {
        "commit_hash": "d7d20645d797aa43ee41456d61a0a96bfd42d378",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/d7d20645d797aa43ee41456d61a0a96bfd42d378",
        "files": [
          "server-base/src/main/java/org/apache/kylin/rest/job/StorageCleanupJob.java",
          "server-base/src/test/java/org/apache/kylin/rest/job/StorageCleanupJobTest.java"
        ],
        "message": "Fix storage clean up tool",
        "before_after_code_files": [
          "server-base/src/main/java/org/apache/kylin/rest/job/StorageCleanupJob.java||server-base/src/main/java/org/apache/kylin/rest/job/StorageCleanupJob.java",
          "server-base/src/test/java/org/apache/kylin/rest/job/StorageCleanupJobTest.java||server-base/src/test/java/org/apache/kylin/rest/job/StorageCleanupJobTest.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "server-base/src/main/java/org/apache/kylin/rest/job/StorageCleanupJob.java||server-base/src/main/java/org/apache/kylin/rest/job/StorageCleanupJob.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/job/StorageCleanupJob.java -> server-base/src/main/java/org/apache/kylin/rest/job/StorageCleanupJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import org.apache.hadoop.fs.FileStatus;",
          "25: import org.apache.hadoop.fs.FileSystem;",
          "26: import org.apache.hadoop.fs.Path;",
          "27: import org.apache.kylin.common.KylinConfig;",
          "28: import org.apache.kylin.common.util.AbstractApplication;",
          "29: import org.apache.kylin.common.util.HadoopUtil;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: import org.apache.hadoop.fs.PathFilter;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "37: import org.slf4j.LoggerFactory;",
          "39: import java.io.IOException;",
          "40: import java.util.List;",
          "41: import java.util.stream.Collectors;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "41: import java.util.Arrays;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "55:     protected boolean delete = false;",
          "57:     public StorageCleanupJob() throws IOException {",
          "58:         this(KylinConfig.getInstanceFromEnv(), HadoopUtil.getWorkingFileSystem(HadoopUtil.getCurrentConfiguration()));",
          "59:     }",
          "62:     public StorageCleanupJob(KylinConfig config, FileSystem fs) {",
          "63:         this.config = config;",
          "64:         this.fs = fs;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "59:     protected static final List<String> protectedDir = Arrays.asList(\"cube_statistics\", \"resources-jdbc\");",
          "60:     protected static PathFilter pathFilter = status -> !protectedDir.contains(status.getName());",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "83:     public void cleanup() throws Exception {",
          "84:         ProjectManager projectManager = ProjectManager.getInstance(config);",
          "85:         CubeManager cubeManager = CubeManager.getInstance(config);",
          "102:         List<CubeInstance> cubes = cubeManager.listAllCubes();",
          "103:         Path metadataPath = new Path(config.getHdfsWorkingDirectory());",
          "104:         if (fs.exists(metadataPath)) {",
          "106:             if (projectStatus != null) {",
          "107:                 for (FileStatus status : projectStatus) {",
          "108:                     String projectName = status.getPath().getName();",
          "",
          "[Removed Lines]",
          "88:         List<String> projects = projectManager.listAllProjects().stream().map(ProjectInstance::getName).collect(Collectors.toList());",
          "89:         for (String project : projects) {",
          "90:             String tmpPath = config.getJobTmpDir(project);",
          "91:             if (delete) {",
          "92:                 logger.info(\"Deleting HDFS path \" + tmpPath);",
          "93:                 if (fs.exists(new Path(tmpPath))) {",
          "94:                     fs.delete(new Path(tmpPath), true);",
          "95:                 }",
          "96:             } else {",
          "97:                 logger.info(\"Dry run, pending delete HDFS path \" + tmpPath);",
          "98:             }",
          "99:         }",
          "105:             FileStatus[] projectStatus = fs.listStatus(metadataPath);",
          "",
          "[Added Lines]",
          "90:         List<String> projects = projectManager.listAllProjects().stream().map(ProjectInstance::getName)",
          "91:                 .collect(Collectors.toList());",
          "97:             FileStatus[] projectStatus = fs.listStatus(metadataPath, pathFilter);",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "114:                             logger.info(\"Dry run, pending delete HDFS path \" + status.getPath());",
          "115:                         }",
          "116:                     } else {",
          "118:                     }",
          "119:                 }",
          "120:             }",
          "",
          "[Removed Lines]",
          "117:                         cleanupDeletedCubes(projectName, cubes.stream().map(CubeInstance::getName).collect(Collectors.toList()));",
          "",
          "[Added Lines]",
          "109:                         cleanupDeletedCubes(projectName,",
          "110:                                 cubes.stream().map(CubeInstance::getName).collect(Collectors.toList()));",
          "",
          "---------------"
        ],
        "server-base/src/test/java/org/apache/kylin/rest/job/StorageCleanupJobTest.java||server-base/src/test/java/org/apache/kylin/rest/job/StorageCleanupJobTest.java": [
          "File: server-base/src/test/java/org/apache/kylin/rest/job/StorageCleanupJobTest.java -> server-base/src/test/java/org/apache/kylin/rest/job/StorageCleanupJobTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "67:         job.execute(new String[] { \"--delete\", \"true\" });",
          "69:         ArgumentCaptor<Path> pathCaptor = ArgumentCaptor.forClass(Path.class);",
          "71:         ArrayList<Path> expected = Lists.newArrayList(",
          "76:                 new Path(basePath + \"/default/parquet/dropped_cube\"),",
          "",
          "[Removed Lines]",
          "70:         verify(mockFs, times(5)).delete(pathCaptor.capture(), eq(true));",
          "73:                 new Path(basePath + \"/default/job_tmp\"),",
          "",
          "[Added Lines]",
          "70:         verify(mockFs, times(4)).delete(pathCaptor.capture(), eq(true));",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "100:         FileStatus project1 = mock(FileStatus.class);",
          "101:         FileStatus project2 = mock(FileStatus.class);",
          "104:         Path jobTmpPath = new Path(basePath + \"/default/job_tmp\");",
          "105:         when(mockFs.exists(jobTmpPath)).thenReturn(true);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "103:         FileStatus[] protectedStatuses = new FileStatus[2];",
          "104:         FileStatus cubeStatistics = mock(FileStatus.class);",
          "105:         FileStatus resourcesJdbc = mock(FileStatus.class);",
          "107:         FileStatus[] allStatuses = new FileStatus[4];",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "130:         projectStatuses[0] = project1;",
          "131:         projectStatuses[1] = project2;",
          "133:         Path defaultProjectParquetPath = new Path(basePath + \"/default/parquet\");",
          "134:         Path deletedProjectParquetPath = new Path(basePath + \"/deleted_project/parquet\");",
          "135:         when(mockFs.exists(defaultProjectParquetPath)).thenReturn(true);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "139:         Path cubeStatisticsPath = new Path(basePath + \"/default/parquet\");",
          "140:         Path resourcesJdbcPath = new Path(basePath + \"/deleted_project/parquet\");",
          "141:         when(cubeStatistics.getPath()).thenReturn(cubeStatisticsPath);",
          "142:         when(resourcesJdbc.getPath()).thenReturn(resourcesJdbcPath);",
          "143:         protectedStatuses[0] = cubeStatistics;",
          "144:         protectedStatuses[1] = resourcesJdbc;",
          "145:         when(mockFs.delete(cubeStatisticsPath, true)).thenReturn(true);",
          "146:         when(mockFs.delete(resourcesJdbcPath, true)).thenReturn(true);",
          "148:         allStatuses[0] = project1;",
          "149:         allStatuses[1] = project2;",
          "150:         allStatuses[2] = cubeStatistics;",
          "151:         allStatuses[3] = resourcesJdbc;",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "138:         when(mockFs.exists(basePath)).thenReturn(true);",
          "139:         when(mockFs.listStatus(new Path(basePath + \"/default/parquet/ci_left_join_cube\"))).thenReturn(segmentStatuses);",
          "140:         when(mockFs.listStatus(defaultProjectParquetPath)).thenReturn(cubeStatuses);",
          "142:     }",
          "143: }",
          "",
          "[Removed Lines]",
          "141:         when(mockFs.listStatus(basePath)).thenReturn(projectStatuses);",
          "",
          "[Added Lines]",
          "161:         when(mockFs.listStatus(basePath)).thenReturn(allStatuses);",
          "162:         when(mockFs.listStatus(basePath, StorageCleanupJob.pathFilter)).thenReturn(projectStatuses);",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2e9597a099dd24ce54ad8d8b564dc3cfdf2618c9",
      "candidate_info": {
        "commit_hash": "2e9597a099dd24ce54ad8d8b564dc3cfdf2618c9",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/2e9597a099dd24ce54ad8d8b564dc3cfdf2618c9",
        "files": [
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala"
        ],
        "message": "KYLIN-4818 Reduce toString method call",
        "before_after_code_files": [
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "134:             sourceChooser.setNeedStatistics();",
          "135:             sourceChooser.decideFlatTableSource(null);",
          "136:             Map<Long, HLLCounter> hllMap = new HashMap<>();",
          "139:             }",
          "140:             logger.info(\"Cuboid statistics return {} records and cost {} ms.\", hllMap.size(), (System.currentTimeMillis() - startMills));",
          "",
          "[Removed Lines]",
          "137:             for (Tuple2<String, AggInfo> cuboidData : sourceChooser.aggInfo()) {",
          "138:                 hllMap.put(Long.parseLong(cuboidData._1), cuboidData._2.cuboid().counter());",
          "",
          "[Added Lines]",
          "137:             for (Tuple2<Object, AggInfo> cuboidData : sourceChooser.aggInfo()) {",
          "138:                 hllMap.put((Long) cuboidData._1, cuboidData._2.cuboid().counter());",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CuboidStatisticsJob.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "40:     val rkc = seg.allColumns.count(c => c.rowKey)",
          "",
          "[Removed Lines]",
          "38:   def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(String, AggInfo)] = {",
          "",
          "[Added Lines]",
          "38:   def statistics(inputDs: Dataset[Row], seg: SegmentInfo): Array[(Long, AggInfo)] = {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "48: }",
          "50: class CuboidStatisticsJob(ids: List[Long], rkc: Int) extends Serializable {",
          "52:   private var allCuboidsBitSet: Array[Array[Integer]] = Array()",
          "53:   private val hf: HashFunction = Hashing.murmur3_128",
          "54:   private val rowHashCodesLong = new Array[Long](rkc)",
          "",
          "[Removed Lines]",
          "51:   private val info = mutable.Map[String, AggInfo]()",
          "",
          "[Added Lines]",
          "51:   private val info = mutable.Map[Long, AggInfo]()",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "71:   def init(): Unit = {",
          "72:     println(\"CuboidStatisticsJob-Init1-\" + System.currentTimeMillis())",
          "73:     allCuboidsBitSet = getCuboidBitSet(ids, rkc)",
          "75:     println(\"CuboidStatisticsJob-Init2-\" + System.currentTimeMillis())",
          "76:   }",
          "",
          "[Removed Lines]",
          "74:     ids.foreach(i => info.put(i.toString, AggInfo(i.toString)))",
          "",
          "[Added Lines]",
          "74:     ids.foreach(i => info.put(i, AggInfo(i)))",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "109:         value += rowHashCodesLong(allCuboidsBitSet(idx)(position))",
          "110:         position += 1",
          "111:       }",
          "113:       idx += 1",
          "114:     }",
          "115:     endMills = System.currentTimeMillis()",
          "",
          "[Removed Lines]",
          "112:       info(ids(idx).toString).cuboid.counter.addHashDirectly(value)",
          "",
          "[Added Lines]",
          "112:       info(ids(idx)).cuboid.counter.addHashDirectly(value)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "146:   }",
          "147: }",
          "150:                    cuboid: CuboidInfo = CuboidInfo(new HLLCounter()),",
          "151:                    sample: SampleInfo = SampleInfo(),",
          "152:                    dimension: DimensionInfo = DimensionInfo()) {",
          "",
          "[Removed Lines]",
          "149: case class AggInfo(key: String,",
          "",
          "[Added Lines]",
          "149: case class AggInfo(key: Long,",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/ParentSourceChooser.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "39:   config: KylinConfig,",
          "40:   needEncoding: Boolean) extends Logging {",
          "45:   var reuseSources: java.util.Map[java.lang.Long, NBuildSourceInfo] = Maps.newHashMap()",
          "",
          "[Removed Lines]",
          "42:   var aggInfo : Array[(String, AggInfo)]  = _",
          "",
          "[Added Lines]",
          "42:   var aggInfo : Array[(Long, AggInfo)]  = _",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "57:   def setNeedStatistics(): Unit =",
          "58:     needStatistics = true",
          "62:   def decideSources(): Unit = {",
          "63:     toBuildTree.getRootIndexEntities.asScala.foreach { entity =>",
          "",
          "[Removed Lines]",
          "60:   def getAggInfo : Array[(String, AggInfo)] = aggInfo",
          "",
          "[Added Lines]",
          "60:   def getAggInfo : Array[(Long, AggInfo)] = aggInfo",
          "",
          "---------------"
        ]
      }
    }
  ]
}