{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "57078110d0a60eff9e4ecda4252c14549bb05ed0",
      "candidate_info": {
        "commit_hash": "57078110d0a60eff9e4ecda4252c14549bb05ed0",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/57078110d0a60eff9e4ecda4252c14549bb05ed0",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/BaseScriptTransformationExec.scala"
        ],
        "message": "[SPARK-38936][SQL] Script transform feed thread should have name\n\n### What changes were proposed in this pull request?\nre-add thread name(`Thread-ScriptTransformation-Feed`).\n\n### Why are the changes needed?\nLost feed thread name after [SPARK-32105](https://issues.apache.org/jira/browse/SPARK-32105) refactoring.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nexist UT\n\nCloses #36245 from cxzl25/SPARK-38936.\n\nAuthored-by: sychen <sychen@ctrip.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 4dc12eb54544a12ff7ddf078ca8bcec9471212c3)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/BaseScriptTransformationExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/BaseScriptTransformationExec.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/BaseScriptTransformationExec.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/BaseScriptTransformationExec.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/BaseScriptTransformationExec.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/BaseScriptTransformationExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "273:   def taskContext: TaskContext",
          "274:   def conf: Configuration",
          "276:   setDaemon(true)",
          "278:   @volatile protected var _exception: Throwable = null",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "276:   setName(s\"Thread-${this.getClass.getSimpleName}-Feed\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "329:         _exception = t",
          "330:         proc.destroy()",
          "332:     } finally {",
          "333:       try {",
          "334:         Utils.tryLogNonFatalError(outputStream.close())",
          "",
          "[Removed Lines]",
          "331:         logError(\"Thread-ScriptTransformation-Feed exit cause by: \", t)",
          "",
          "[Added Lines]",
          "332:         logError(s\"Thread-${this.getClass.getSimpleName}-Feed exit cause by: \", t)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5c5a68c03cf06a0e3b3b2f24cbd4841c489b89dc",
      "candidate_info": {
        "commit_hash": "5c5a68c03cf06a0e3b3b2f24cbd4841c489b89dc",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/5c5a68c03cf06a0e3b3b2f24cbd4841c489b89dc",
        "files": [
          "docs/sql-ref-ansi-compliance.md",
          "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseLexer.g4",
          "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/percentiles.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/aggregate/PercentileSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala",
          "sql/core/src/test/resources/sql-tests/inputs/group-by.sql",
          "sql/core/src/test/resources/sql-tests/inputs/postgreSQL/aggregates_part4.sql",
          "sql/core/src/test/resources/sql-tests/inputs/udf/postgreSQL/udf-aggregates_part4.sql",
          "sql/core/src/test/resources/sql-tests/inputs/window.sql",
          "sql/core/src/test/resources/sql-tests/results/group-by.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/aggregates_part4.sql.out",
          "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-aggregates_part4.sql.out",
          "sql/core/src/test/resources/sql-tests/results/window.sql.out"
        ],
        "message": "[SPARK-38219][SPARK-37691][3.3] Support ANSI Aggregation Function: percentile_cont and percentile_disc\n\n### What changes were proposed in this pull request?\nThis PR backport https://github.com/apache/spark/pull/35531 and https://github.com/apache/spark/pull/35041 to branch-3.3\n\n### Why are the changes needed?\n`percentile_cont` and `percentile_disc` in Spark3.3 release.\n\n### Does this PR introduce _any_ user-facing change?\n'No'.\nNew feature.\n\n### How was this patch tested?\nNew tests.\n\nCloses #36277 from beliefer/SPARK-38219_SPARK-37691_backport_3.3.\n\nAuthored-by: Jiaan Geng <beliefer@163.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseLexer.g4||sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseLexer.g4",
          "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4||sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Percentile.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/percentiles.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/aggregate/PercentileSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/aggregate/PercentileSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala",
          "sql/core/src/test/resources/sql-tests/inputs/group-by.sql||sql/core/src/test/resources/sql-tests/inputs/group-by.sql",
          "sql/core/src/test/resources/sql-tests/inputs/postgreSQL/aggregates_part4.sql||sql/core/src/test/resources/sql-tests/inputs/postgreSQL/aggregates_part4.sql",
          "sql/core/src/test/resources/sql-tests/inputs/udf/postgreSQL/udf-aggregates_part4.sql||sql/core/src/test/resources/sql-tests/inputs/udf/postgreSQL/udf-aggregates_part4.sql",
          "sql/core/src/test/resources/sql-tests/inputs/window.sql||sql/core/src/test/resources/sql-tests/inputs/window.sql"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseLexer.g4||sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseLexer.g4": [
          "File: sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseLexer.g4 -> sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseLexer.g4",
          "--- Hunk 1 ---",
          "[Context before]",
          "258: PARTITIONED: 'PARTITIONED';",
          "259: PARTITIONS: 'PARTITIONS';",
          "260: PERCENTILE_CONT: 'PERCENTILE_CONT';",
          "261: PERCENTLIT: 'PERCENT';",
          "262: PIVOT: 'PIVOT';",
          "263: PLACING: 'PLACING';",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "261: PERCENTILE_DISC: 'PERCENTILE_DISC';",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4||sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4": [
          "File: sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4 -> sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBaseParser.g4",
          "--- Hunk 1 ---",
          "[Context before]",
          "844:        FROM srcStr=valueExpression RIGHT_PAREN                                                 #trim",
          "845:     | OVERLAY LEFT_PAREN input=valueExpression PLACING replace=valueExpression",
          "846:       FROM position=valueExpression (FOR length=valueExpression)? RIGHT_PAREN                  #overlay",
          "849:     ;",
          "851: constant",
          "",
          "[Removed Lines]",
          "847:     | PERCENTILE_CONT LEFT_PAREN percentage=valueExpression RIGHT_PAREN",
          "848:       WITHIN GROUP LEFT_PAREN ORDER BY sortItem RIGHT_PAREN                                    #percentile",
          "",
          "[Added Lines]",
          "847:     | name=(PERCENTILE_CONT | PERCENTILE_DISC) LEFT_PAREN percentage=valueExpression RIGHT_PAREN",
          "848:       WITHIN GROUP LEFT_PAREN ORDER BY sortItem RIGHT_PAREN ( OVER windowSpec)?                #percentile",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1449:     | PARTITIONED",
          "1450:     | PARTITIONS",
          "1451:     | PERCENTILE_CONT",
          "1452:     | PERCENTLIT",
          "1453:     | PIVOT",
          "1454:     | PLACING",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1452:     | PERCENTILE_DISC",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import org.apache.spark.sql.AnalysisException",
          "22: import org.apache.spark.sql.catalyst.expressions._",
          "23: import org.apache.spark.sql.catalyst.expressions.SubExprUtils._",
          "25: import org.apache.spark.sql.catalyst.optimizer.{BooleanSimplification, DecorrelateInnerQuery, InlineCTE}",
          "26: import org.apache.spark.sql.catalyst.plans._",
          "27: import org.apache.spark.sql.catalyst.plans.logical._",
          "",
          "[Removed Lines]",
          "24: import org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression",
          "",
          "[Added Lines]",
          "24: import org.apache.spark.sql.catalyst.expressions.aggregate.{AggregateExpression, PercentileCont, PercentileDisc}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "235:             w.windowFunction match {",
          "236:               case _: AggregateExpression | _: FrameLessOffsetWindowFunction |",
          "237:                   _: AggregateWindowFunction => // OK",
          "238:               case f: PythonUDF if PythonUDF.isWindowPandasUDF(f) => // OK",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "236:               case agg @ AggregateExpression(_: PercentileCont | _: PercentileDisc, _, _, _, _)",
          "237:                 if w.windowSpec.orderSpec.nonEmpty || w.windowSpec.frameSpecification !=",
          "238:                   SpecifiedWindowFrame(RowFrame, UnboundedPreceding, UnboundedFollowing) =>",
          "239:                 failAnalysis(",
          "240:                   s\"Cannot specify order by or frame for '${agg.aggregateFunction.prettyName}'.\")",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Percentile.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/percentiles.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Percentile.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/percentiles.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import org.apache.spark.sql.catalyst.analysis.TypeCheckResult",
          "25: import org.apache.spark.sql.catalyst.analysis.TypeCheckResult.{TypeCheckFailure, TypeCheckSuccess}",
          "26: import org.apache.spark.sql.catalyst.expressions._",
          "28: import org.apache.spark.sql.catalyst.util._",
          "29: import org.apache.spark.sql.errors.QueryExecutionErrors",
          "30: import org.apache.spark.sql.types._",
          "31: import org.apache.spark.util.collection.OpenHashMap",
          "100:   @transient",
          "101:   private lazy val returnPercentileArray = percentageExpression.dataType.isInstanceOf[ArrayType]",
          "103:   @transient",
          "105:     case null => null",
          "106:     case num: Double => Array(num)",
          "107:     case arrayData: ArrayData => arrayData.toDoubleArray()",
          "108:   }",
          "115:   override def nullable: Boolean = true",
          "",
          "[Removed Lines]",
          "27: import org.apache.spark.sql.catalyst.trees.TernaryLike",
          "47: @ExpressionDescription(",
          "48:   usage =",
          "49:     \"\"\"",
          "50:       _FUNC_(col, percentage [, frequency]) - Returns the exact percentile value of numeric",
          "51:        or ansi interval column `col` at the given percentage. The value of percentage must be",
          "52:        between 0.0 and 1.0. The value of frequency should be positive integral",
          "54:       _FUNC_(col, array(percentage1 [, percentage2]...) [, frequency]) - Returns the exact",
          "55:       percentile value array of numeric column `col` at the given percentage(s). Each value",
          "56:       of the percentage array must be between 0.0 and 1.0. The value of frequency should be",
          "57:       positive integral",
          "59:       \"\"\",",
          "60:   examples = \"\"\"",
          "61:     Examples:",
          "62:       > SELECT _FUNC_(col, 0.3) FROM VALUES (0), (10) AS tab(col);",
          "63:        3.0",
          "64:       > SELECT _FUNC_(col, array(0.25, 0.75)) FROM VALUES (0), (10) AS tab(col);",
          "65:        [2.5,7.5]",
          "66:       > SELECT _FUNC_(col, 0.5) FROM VALUES (INTERVAL '0' MONTH), (INTERVAL '10' MONTH) AS tab(col);",
          "67:        5.0",
          "68:       > SELECT _FUNC_(col, array(0.2, 0.5)) FROM VALUES (INTERVAL '0' SECOND), (INTERVAL '10' SECOND) AS tab(col);",
          "69:        [2000000.0,5000000.0]",
          "70:   \"\"\",",
          "71:   group = \"agg_funcs\",",
          "72:   since = \"2.1.0\")",
          "74: case class Percentile(",
          "75:     child: Expression,",
          "76:     percentageExpression: Expression,",
          "77:     frequencyExpression : Expression,",
          "78:     mutableAggBufferOffset: Int = 0,",
          "79:     inputAggBufferOffset: Int = 0)",
          "80:   extends TypedImperativeAggregate[OpenHashMap[AnyRef, Long]] with ImplicitCastInputTypes",
          "81:   with TernaryLike[Expression] {",
          "83:   def this(child: Expression, percentageExpression: Expression) = {",
          "84:     this(child, percentageExpression, Literal(1L), 0, 0)",
          "85:   }",
          "87:   def this(child: Expression, percentageExpression: Expression, frequency: Expression) = {",
          "88:     this(child, percentageExpression, frequency, 0, 0)",
          "89:   }",
          "91:   override def prettyName: String = \"percentile\"",
          "93:   override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): Percentile =",
          "94:     copy(mutableAggBufferOffset = newMutableAggBufferOffset)",
          "96:   override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): Percentile =",
          "97:     copy(inputAggBufferOffset = newInputAggBufferOffset)",
          "104:   private lazy val percentages = percentageExpression.eval() match {",
          "110:   override def first: Expression = child",
          "111:   override def second: Expression = percentageExpression",
          "112:   override def third: Expression = frequencyExpression",
          "",
          "[Added Lines]",
          "27: import org.apache.spark.sql.catalyst.trees.{BinaryLike, TernaryLike}",
          "33: abstract class PercentileBase extends TypedImperativeAggregate[OpenHashMap[AnyRef, Long]]",
          "34:   with ImplicitCastInputTypes {",
          "36:   val child: Expression",
          "37:   val percentageExpression: Expression",
          "38:   val frequencyExpression : Expression",
          "41:   val reverse: Boolean",
          "44:   protected def discrete: Boolean",
          "51:   protected lazy val percentages = percentageExpression.eval() match {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "151:     }",
          "152:   }",
          "155:     case d: Decimal => d.toDouble",
          "156:     case n: Number => n.doubleValue",
          "157:   }",
          "",
          "[Removed Lines]",
          "154:   private def toDoubleValue(d: Any): Double = d match {",
          "",
          "[Added Lines]",
          "97:   protected def toDoubleValue(d: Any): Double = d match {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "204:       case intervalType: DayTimeIntervalType => intervalType.ordering",
          "205:       case otherType => QueryExecutionErrors.unsupportedTypeError(otherType)",
          "206:     }",
          "208:     val accumulatedCounts = sortedCounts.scanLeft((sortedCounts.head._1, 0L)) {",
          "209:       case ((key1, count1), (key2, count2)) => (key2, count1 + count2)",
          "210:     }.tail",
          "",
          "[Removed Lines]",
          "207:     val sortedCounts = buffer.toSeq.sortBy(_._1)(ordering.asInstanceOf[Ordering[AnyRef]])",
          "",
          "[Added Lines]",
          "150:     val sortedCounts = if (reverse) {",
          "151:       buffer.toSeq.sortBy(_._1)(ordering.asInstanceOf[Ordering[AnyRef]].reverse)",
          "152:     } else {",
          "153:       buffer.toSeq.sortBy(_._1)(ordering.asInstanceOf[Ordering[AnyRef]])",
          "154:     }",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "236:     val lower = position.floor.toLong",
          "237:     val higher = position.ceil.toLong",
          "245:     if (higher == lower) {",
          "247:       return toDoubleValue(lowerKey)",
          "248:     }",
          "251:     if (higherKey == lowerKey) {",
          "253:       return toDoubleValue(lowerKey)",
          "254:     }",
          "258:   }",
          "264:       countsArray: Array[Long], start: Int, end: Int, value: Long): Int = {",
          "265:     util.Arrays.binarySearch(countsArray, 0, end, value) match {",
          "266:       case ix if ix < 0 => -(ix + 1)",
          "",
          "[Removed Lines]",
          "234:   private def getPercentile(aggreCounts: Seq[(AnyRef, Long)], position: Double): Double = {",
          "240:     val countsArray = aggreCounts.map(_._2).toArray[Long]",
          "241:     val lowerIndex = binarySearchCount(countsArray, 0, aggreCounts.size, lower + 1)",
          "242:     val higherIndex = binarySearchCount(countsArray, 0, aggreCounts.size, higher + 1)",
          "244:     val lowerKey = aggreCounts(lowerIndex)._1",
          "250:     val higherKey = aggreCounts(higherIndex)._1",
          "257:     (higher - position) * toDoubleValue(lowerKey) + (position - lower) * toDoubleValue(higherKey)",
          "263:   private def binarySearchCount(",
          "",
          "[Added Lines]",
          "180:   private def getPercentile(",
          "181:       accumulatedCounts: Seq[(AnyRef, Long)], position: Double): Double = {",
          "187:     val countsArray = accumulatedCounts.map(_._2).toArray[Long]",
          "188:     val lowerIndex = binarySearchCount(countsArray, 0, accumulatedCounts.size, lower + 1)",
          "189:     val higherIndex = binarySearchCount(countsArray, 0, accumulatedCounts.size, higher + 1)",
          "191:     val lowerKey = accumulatedCounts(lowerIndex)._1",
          "197:     val higherKey = accumulatedCounts(higherIndex)._1",
          "203:     if (discrete) {",
          "204:       toDoubleValue(lowerKey)",
          "205:     } else {",
          "207:       (higher - position) * toDoubleValue(lowerKey) + (position - lower) * toDoubleValue(higherKey)",
          "208:     }",
          "214:   protected def binarySearchCount(",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "268:     }",
          "269:   }",
          "271:   override def serialize(obj: OpenHashMap[AnyRef, Long]): Array[Byte] = {",
          "272:     val buffer = new Array[Byte](4 << 10)  // 4K",
          "273:     val bos = new ByteArrayOutputStream()",
          "274:     val out = new DataOutputStream(bos)",
          "275:     try {",
          "278:       obj.foreach { case (key, count) =>",
          "279:         val row = InternalRow.apply(key, count)",
          "",
          "[Removed Lines]",
          "276:       val projection = UnsafeProjection.create(Array[DataType](child.dataType, LongType))",
          "",
          "[Added Lines]",
          "222:   private lazy val projection = UnsafeProjection.create(Array[DataType](child.dataType, LongType))",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "316:       bis.close()",
          "317:     }",
          "318:   }",
          "320:   override protected def withNewChildrenInternal(",
          "321:       newFirst: Expression, newSecond: Expression, newThird: Expression): Percentile = copy(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "271: }",
          "287: @ExpressionDescription(",
          "288:   usage =",
          "289:     \"\"\"",
          "290:       _FUNC_(col, percentage [, frequency]) - Returns the exact percentile value of numeric",
          "291:        or ansi interval column `col` at the given percentage. The value of percentage must be",
          "292:        between 0.0 and 1.0. The value of frequency should be positive integral",
          "293:       _FUNC_(col, array(percentage1 [, percentage2]...) [, frequency]) - Returns the exact",
          "294:       percentile value array of numeric column `col` at the given percentage(s). Each value",
          "295:       of the percentage array must be between 0.0 and 1.0. The value of frequency should be",
          "296:       positive integral",
          "297:       \"\"\",",
          "298:   examples = \"\"\"",
          "299:     Examples:",
          "300:       > SELECT _FUNC_(col, 0.3) FROM VALUES (0), (10) AS tab(col);",
          "301:        3.0",
          "302:       > SELECT _FUNC_(col, array(0.25, 0.75)) FROM VALUES (0), (10) AS tab(col);",
          "303:        [2.5,7.5]",
          "304:       > SELECT _FUNC_(col, 0.5) FROM VALUES (INTERVAL '0' MONTH), (INTERVAL '10' MONTH) AS tab(col);",
          "305:        5.0",
          "306:       > SELECT _FUNC_(col, array(0.2, 0.5)) FROM VALUES (INTERVAL '0' SECOND), (INTERVAL '10' SECOND) AS tab(col);",
          "307:        [2000000.0,5000000.0]",
          "308:   \"\"\",",
          "309:   group = \"agg_funcs\",",
          "310:   since = \"2.1.0\")",
          "312: case class Percentile(",
          "313:     child: Expression,",
          "314:     percentageExpression: Expression,",
          "315:     frequencyExpression : Expression,",
          "316:     mutableAggBufferOffset: Int = 0,",
          "317:     inputAggBufferOffset: Int = 0,",
          "318:     reverse: Boolean = false) extends PercentileBase with TernaryLike[Expression] {",
          "320:   def this(child: Expression, percentageExpression: Expression) = {",
          "321:     this(child, percentageExpression, Literal(1L), 0, 0)",
          "322:   }",
          "324:   def this(child: Expression, percentageExpression: Expression, frequency: Expression) = {",
          "325:     this(child, percentageExpression, frequency, 0, 0)",
          "326:   }",
          "328:   def this(child: Expression, percentageExpression: Expression, reverse: Boolean) = {",
          "329:     this(child, percentageExpression, Literal(1L), reverse = reverse)",
          "330:   }",
          "332:   override def first: Expression = child",
          "333:   override def second: Expression = percentageExpression",
          "334:   override def third: Expression = frequencyExpression",
          "336:   override def prettyName: String = \"percentile\"",
          "338:   override def discrete: Boolean = false",
          "340:   override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): Percentile =",
          "341:     copy(mutableAggBufferOffset = newMutableAggBufferOffset)",
          "343:   override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): Percentile =",
          "344:     copy(inputAggBufferOffset = newInputAggBufferOffset)",
          "346:   override protected def stringArgs: Iterator[Any] = if (discrete) {",
          "347:     super.stringArgs ++ Some(discrete)",
          "348:   } else {",
          "349:     super.stringArgs",
          "350:   }",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "324:     frequencyExpression = newThird",
          "325:   )",
          "326: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "365: case class PercentileCont(left: Expression, right: Expression, reverse: Boolean = false)",
          "366:   extends AggregateFunction",
          "367:     with RuntimeReplaceableAggregate",
          "368:     with ImplicitCastInputTypes",
          "369:     with BinaryLike[Expression] {",
          "370:   private lazy val percentile = new Percentile(left, right, reverse)",
          "371:   override def replacement: Expression = percentile",
          "372:   override def nodeName: String = \"percentile_cont\"",
          "373:   override def inputTypes: Seq[AbstractDataType] = percentile.inputTypes",
          "374:   override def sql(isDistinct: Boolean): String = {",
          "375:     val distinct = if (isDistinct) \"DISTINCT \" else \"\"",
          "376:     val direction = if (reverse) \" DESC\" else \"\"",
          "377:     s\"$prettyName($distinct${right.sql}) WITHIN GROUP (ORDER BY v$direction)\"",
          "378:   }",
          "379:   override protected def withNewChildrenInternal(",
          "380:       newLeft: Expression, newRight: Expression): PercentileCont =",
          "381:     this.copy(left = newLeft, right = newRight)",
          "382: }",
          "392: case class PercentileDisc(",
          "393:     child: Expression,",
          "394:     percentageExpression: Expression,",
          "395:     reverse: Boolean = false,",
          "396:     mutableAggBufferOffset: Int = 0,",
          "397:     inputAggBufferOffset: Int = 0) extends PercentileBase with BinaryLike[Expression] {",
          "399:   val frequencyExpression: Expression = Literal(1L)",
          "401:   override def left: Expression = child",
          "402:   override def right: Expression = percentageExpression",
          "404:   override def prettyName: String = \"percentile_disc\"",
          "406:   override def discrete: Boolean = true",
          "408:   override def withNewMutableAggBufferOffset(newMutableAggBufferOffset: Int): PercentileDisc =",
          "409:     copy(mutableAggBufferOffset = newMutableAggBufferOffset)",
          "411:   override def withNewInputAggBufferOffset(newInputAggBufferOffset: Int): PercentileDisc =",
          "412:     copy(inputAggBufferOffset = newInputAggBufferOffset)",
          "414:   override def sql(isDistinct: Boolean): String = {",
          "415:     val distinct = if (isDistinct) \"DISTINCT \" else \"\"",
          "416:     val direction = if (reverse) \" DESC\" else \"\"",
          "417:     s\"$prettyName($distinct${right.sql}) WITHIN GROUP (ORDER BY v$direction)\"",
          "418:   }",
          "420:   override protected def withNewChildrenInternal(",
          "421:       newLeft: Expression, newRight: Expression): PercentileDisc = copy(",
          "422:     child = newLeft,",
          "423:     percentageExpression = newRight",
          "424:   )",
          "425: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: import org.apache.spark.sql.catalyst.analysis._",
          "35: import org.apache.spark.sql.catalyst.catalog.{BucketSpec, CatalogStorageFormat}",
          "36: import org.apache.spark.sql.catalyst.expressions._",
          "38: import org.apache.spark.sql.catalyst.parser.SqlBaseParser._",
          "39: import org.apache.spark.sql.catalyst.plans._",
          "40: import org.apache.spark.sql.catalyst.plans.logical._",
          "",
          "[Removed Lines]",
          "37: import org.apache.spark.sql.catalyst.expressions.aggregate.{First, Last, Percentile}",
          "",
          "[Added Lines]",
          "37: import org.apache.spark.sql.catalyst.expressions.aggregate.{First, Last, PercentileCont, PercentileDisc}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1836:   override def visitPercentile(ctx: PercentileContext): Expression = withOrigin(ctx) {",
          "1837:     val percentage = expression(ctx.percentage)",
          "1838:     val sortOrder = visitSortItem(ctx.sortItem)",
          "1842:     }",
          "1844:   }",
          "",
          "[Removed Lines]",
          "1839:     val percentile = sortOrder.direction match {",
          "1840:       case Ascending => new Percentile(sortOrder.child, percentage)",
          "1841:       case Descending => new Percentile(sortOrder.child, Subtract(Literal(1), percentage))",
          "1843:     percentile.toAggregateExpression()",
          "",
          "[Added Lines]",
          "1839:     val percentile = ctx.name.getType match {",
          "1840:       case SqlBaseParser.PERCENTILE_CONT =>",
          "1841:         sortOrder.direction match {",
          "1842:           case Ascending => PercentileCont(sortOrder.child, percentage)",
          "1843:           case Descending => PercentileCont(sortOrder.child, percentage, true)",
          "1844:         }",
          "1845:       case SqlBaseParser.PERCENTILE_DISC =>",
          "1846:         sortOrder.direction match {",
          "1847:           case Ascending => PercentileDisc(sortOrder.child, percentage)",
          "1848:           case Descending => PercentileDisc(sortOrder.child, percentage, true)",
          "1849:         }",
          "1850:     }",
          "1851:     val aggregateExpression = percentile.toAggregateExpression()",
          "1852:     ctx.windowSpec match {",
          "1853:       case spec: WindowRefContext =>",
          "1854:         UnresolvedWindowExpression(aggregateExpression, visitWindowRef(spec))",
          "1855:       case spec: WindowDefContext =>",
          "1856:         WindowExpression(aggregateExpression, visitWindowDef(spec))",
          "1857:       case _ => aggregateExpression",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/aggregate/PercentileSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/aggregate/PercentileSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/aggregate/PercentileSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/aggregate/PercentileSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "83:   }",
          "85:   private def runTest(agg: Percentile,",
          "88:     assert(agg.nullable)",
          "89:     val group1 = (0 until rows.length / 2)",
          "90:     val group1Buffer = agg.createAggregationBuffer()",
          "",
          "[Removed Lines]",
          "86:         rows : Seq[Seq[Any]],",
          "87:         expectedPercentiles : Seq[Double]): Unit = {",
          "",
          "[Added Lines]",
          "86:        rows : Seq[Seq[Any]],",
          "87:        expectedPercentiles : Seq[Double]): Unit = {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "218:       val percentile2 = new Percentile(child, percentage)",
          "219:       assertEqual(percentile2.checkInputDataTypes(),",
          "220:         TypeCheckFailure(s\"Percentage(s) must be between 0.0 and 1.0, \" +",
          "222:     }",
          "224:     val nonFoldablePercentage = Seq(NonFoldableLiteral(0.5),",
          "",
          "[Removed Lines]",
          "221:         s\"but got ${percentage.simpleString(100)}\"))",
          "",
          "[Added Lines]",
          "221:           s\"but got ${percentage.simpleString(100)}\"))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "280:       Seq(CreateArray(Seq(null).map(Literal(_))), CreateArray(Seq(0.1D, null).map(Literal(_))))",
          "282:     nullPercentageExprs.foreach { percentageExpression =>",
          "291:     }",
          "292:   }",
          "",
          "[Removed Lines]",
          "283:         val wrongPercentage = new Percentile(",
          "284:           AttributeReference(\"a\", DoubleType)(),",
          "285:           percentageExpression = percentageExpression)",
          "286:         assert(",
          "287:           wrongPercentage.checkInputDataTypes() match {",
          "288:             case TypeCheckFailure(msg) if msg.contains(\"argument 2 requires array<double>\") => true",
          "289:             case _ => false",
          "290:           })",
          "",
          "[Added Lines]",
          "282:       val wrongPercentage = new Percentile(",
          "283:         AttributeReference(\"a\", DoubleType)(),",
          "284:         percentageExpression = percentageExpression)",
          "285:       assert(wrongPercentage.checkInputDataTypes() match {",
          "286:         case TypeCheckFailure(msg) if msg.contains(\"argument 2 requires array<double>\") => true",
          "287:         case _ => false",
          "288:       })",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}",
          "21: import org.apache.spark.sql.catalyst.analysis.{AnalysisTest, RelationTimeTravel, UnresolvedAlias, UnresolvedAttribute, UnresolvedFunction, UnresolvedGenerator, UnresolvedInlineTable, UnresolvedRelation, UnresolvedStar, UnresolvedSubqueryColumnAliases, UnresolvedTableValuedFunction}",
          "22: import org.apache.spark.sql.catalyst.expressions._",
          "24: import org.apache.spark.sql.catalyst.plans._",
          "25: import org.apache.spark.sql.catalyst.plans.logical._",
          "26: import org.apache.spark.sql.internal.SQLConf",
          "",
          "[Removed Lines]",
          "23: import org.apache.spark.sql.catalyst.expressions.aggregate.Percentile",
          "",
          "[Added Lines]",
          "23: import org.apache.spark.sql.catalyst.expressions.aggregate.{PercentileCont, PercentileDisc}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1303:       \"timestamp expression cannot contain subqueries\")",
          "1304:   }",
          "1308:       comparePlans(",
          "1309:         parsePlan(inputSQL),",
          "1310:         Project(Seq(UnresolvedAlias(expectedExpression)), OneRowRelation())",
          "1311:       )",
          "1312:     }",
          "1315:       \"SELECT PERCENTILE_CONT(0.1) WITHIN GROUP (ORDER BY col)\",",
          "1317:         .toAggregateExpression()",
          "1318:     )",
          "1321:       \"SELECT PERCENTILE_CONT(0.1) WITHIN GROUP (ORDER BY col DESC)\",",
          "1324:     )",
          "1325:   }",
          "1326: }",
          "",
          "[Removed Lines]",
          "1306:   test(\"PERCENTILE_CONT function\") {",
          "1307:     def assertPercentileContPlans(inputSQL: String, expectedExpression: Expression): Unit = {",
          "1314:     assertPercentileContPlans(",
          "1316:       new Percentile(UnresolvedAttribute(\"col\"), Literal(Decimal(0.1), DecimalType(1, 1)))",
          "1320:     assertPercentileContPlans(",
          "1322:       new Percentile(UnresolvedAttribute(\"col\"),",
          "1323:         Subtract(Literal(1), Literal(Decimal(0.1), DecimalType(1, 1)))).toAggregateExpression()",
          "",
          "[Added Lines]",
          "1306:   test(\"PERCENTILE_CONT & PERCENTILE_DISC\") {",
          "1307:     def assertPercentilePlans(inputSQL: String, expectedExpression: Expression): Unit = {",
          "1314:     assertPercentilePlans(",
          "1316:       PercentileCont(UnresolvedAttribute(\"col\"), Literal(Decimal(0.1), DecimalType(1, 1)))",
          "1320:     assertPercentilePlans(",
          "1322:       PercentileCont(UnresolvedAttribute(\"col\"),",
          "1323:         Literal(Decimal(0.1), DecimalType(1, 1)), true).toAggregateExpression()",
          "1324:     )",
          "1326:     assertPercentilePlans(",
          "1327:       \"SELECT PERCENTILE_DISC(0.1) WITHIN GROUP (ORDER BY col)\",",
          "1328:       PercentileDisc(UnresolvedAttribute(\"col\"), Literal(Decimal(0.1), DecimalType(1, 1)))",
          "1329:         .toAggregateExpression()",
          "1330:     )",
          "1332:     assertPercentilePlans(",
          "1333:       \"SELECT PERCENTILE_DISC(0.1) WITHIN GROUP (ORDER BY col DESC)\",",
          "1334:       PercentileDisc(UnresolvedAttribute(\"col\"),",
          "1335:         Literal(Decimal(0.1), DecimalType(1, 1)), true).toAggregateExpression()",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/group-by.sql||sql/core/src/test/resources/sql-tests/inputs/group-by.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/group-by.sql -> sql/core/src/test/resources/sql-tests/inputs/group-by.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "273: -- SPARK-37676: Support ANSI Aggregation Function: percentile_cont",
          "274: SELECT",
          "277: FROM aggr;",
          "278: SELECT",
          "279:   k,",
          "",
          "[Removed Lines]",
          "275:  percentile_cont(0.25) WITHIN GROUP (ORDER BY v),",
          "276:  percentile_cont(0.25) WITHIN GROUP (ORDER BY v DESC)",
          "",
          "[Added Lines]",
          "275:   percentile_cont(0.25) WITHIN GROUP (ORDER BY v),",
          "276:   percentile_cont(0.25) WITHIN GROUP (ORDER BY v DESC)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "282: FROM aggr",
          "283: GROUP BY k",
          "284: ORDER BY k;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "286: -- SPARK-37691: Support ANSI Aggregation Function: percentile_disc",
          "287: SELECT",
          "288:   percentile_disc(0.25) WITHIN GROUP (ORDER BY v),",
          "289:   percentile_disc(0.25) WITHIN GROUP (ORDER BY v DESC)",
          "290: FROM aggr;",
          "291: SELECT",
          "292:   k,",
          "293:   percentile_disc(0.25) WITHIN GROUP (ORDER BY v),",
          "294:   percentile_disc(0.25) WITHIN GROUP (ORDER BY v DESC)",
          "295: FROM aggr",
          "296: GROUP BY k",
          "297: ORDER BY k;",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/postgreSQL/aggregates_part4.sql||sql/core/src/test/resources/sql-tests/inputs/postgreSQL/aggregates_part4.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/postgreSQL/aggregates_part4.sql -> sql/core/src/test/resources/sql-tests/inputs/postgreSQL/aggregates_part4.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "33: --      (values (0::float8),(0.1),(0.25),(0.4),(0.5),(0.6),(0.75),(0.9),(1)) v(p)",
          "34: -- group by p order by p;",
          "40: -- [SPARK-28661] Hypothetical-Set Aggregate Functions",
          "41: -- select rank(3) within group (order by x)",
          "42: -- from (values (1),(1),(2),(2),(3),(3),(4)) v(x);",
          "",
          "[Removed Lines]",
          "36: -- select percentile_cont(0.5) within group (order by b) from aggtest;",
          "37: -- select percentile_cont(0.5) within group (order by b), sum(b) from aggtest;",
          "38: -- select percentile_cont(0.5) within group (order by thousand) from tenk1;",
          "39: -- select percentile_disc(0.5) within group (order by thousand) from tenk1;",
          "",
          "[Added Lines]",
          "36: select percentile_cont(0.5) within group (order by b) from aggtest;",
          "37: select percentile_cont(0.5) within group (order by b), sum(b) from aggtest;",
          "38: select percentile_cont(0.5) within group (order by thousand) from tenk1;",
          "39: select percentile_disc(0.5) within group (order by thousand) from tenk1;",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/udf/postgreSQL/udf-aggregates_part4.sql||sql/core/src/test/resources/sql-tests/inputs/udf/postgreSQL/udf-aggregates_part4.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/udf/postgreSQL/udf-aggregates_part4.sql -> sql/core/src/test/resources/sql-tests/inputs/udf/postgreSQL/udf-aggregates_part4.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: --      (values (0::float8),(0.1),(0.25),(0.4),(0.5),(0.6),(0.75),(0.9),(1)) v(p)",
          "31: -- group by p order by p;",
          "37: -- [SPARK-28661] Hypothetical-Set Aggregate Functions",
          "38: -- select rank(3) within group (order by x)",
          "39: -- from (values (1),(1),(2),(2),(3),(3),(4)) v(x);",
          "",
          "[Removed Lines]",
          "33: -- select percentile_cont(0.5) within group (order by b) from aggtest;",
          "34: -- select percentile_cont(0.5) within group (order by b), sum(b) from aggtest;",
          "35: -- select percentile_cont(0.5) within group (order by thousand) from tenk1;",
          "36: -- select percentile_disc(0.5) within group (order by thousand) from tenk1;",
          "",
          "[Added Lines]",
          "33: select percentile_cont(0.5) within group (order by b) from aggtest;",
          "34: select percentile_cont(0.5) within group (order by b), sum(b) from aggtest;",
          "35: select percentile_cont(0.5) within group (order by thousand) from tenk1;",
          "36: select percentile_disc(0.5) within group (order by thousand) from tenk1;",
          "",
          "---------------"
        ],
        "sql/core/src/test/resources/sql-tests/inputs/window.sql||sql/core/src/test/resources/sql-tests/inputs/window.sql": [
          "File: sql/core/src/test/resources/sql-tests/inputs/window.sql -> sql/core/src/test/resources/sql-tests/inputs/window.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "440:  SUM(salary) OVER w sum_salary",
          "441: FROM",
          "442:  basic_pays;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "444: SELECT",
          "445:     employee_name,",
          "446:     department,",
          "447:     salary,",
          "448:     percentile_cont(0.25) WITHIN GROUP (ORDER BY salary) OVER (PARTITION BY department),",
          "449:     percentile_disc(0.25) WITHIN GROUP (ORDER BY salary) OVER (PARTITION BY department),",
          "450:     percentile_cont(0.25) WITHIN GROUP (ORDER BY salary DESC) OVER (PARTITION BY department),",
          "451:     percentile_disc(0.25) WITHIN GROUP (ORDER BY salary DESC) OVER (PARTITION BY department)",
          "452: FROM basic_pays",
          "453: ORDER BY salary;",
          "455: SELECT",
          "456:     employee_name,",
          "457:     department,",
          "458:     salary,",
          "459:     percentile_cont(0.25) WITHIN GROUP (ORDER BY salary) OVER (PARTITION BY department ORDER BY salary),",
          "460:     percentile_cont(0.25) WITHIN GROUP (ORDER BY salary DESC) OVER (PARTITION BY department ORDER BY salary)",
          "461: FROM basic_pays",
          "462: ORDER BY salary;",
          "464: SELECT",
          "465:     employee_name,",
          "466:     department,",
          "467:     salary,",
          "468:     percentile_disc(0.25) WITHIN GROUP (ORDER BY salary) OVER (PARTITION BY department ORDER BY salary),",
          "469:     percentile_disc(0.25) WITHIN GROUP (ORDER BY salary DESC) OVER (PARTITION BY department ORDER BY salary)",
          "470: FROM basic_pays",
          "471: ORDER BY salary;",
          "473: SELECT",
          "474:     employee_name,",
          "475:     department,",
          "476:     salary,",
          "477:     percentile_cont(0.25) WITHIN GROUP (ORDER BY salary) OVER (PARTITION BY department ROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING),",
          "478:     percentile_cont(0.25) WITHIN GROUP (ORDER BY salary DESC) OVER (PARTITION BY department ROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING)",
          "479: FROM basic_pays",
          "480: ORDER BY salary;",
          "482: SELECT",
          "483:     employee_name,",
          "484:     department,",
          "485:     salary,",
          "486:     percentile_disc(0.25) WITHIN GROUP (ORDER BY salary) OVER (PARTITION BY department ROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING),",
          "487:     percentile_disc(0.25) WITHIN GROUP (ORDER BY salary DESC) OVER (PARTITION BY department ROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING)",
          "488: FROM basic_pays",
          "489: ORDER BY salary;",
          "491: SELECT",
          "492:     employee_name,",
          "493:     department,",
          "494:     salary,",
          "495:     percentile_cont(0.25) WITHIN GROUP (ORDER BY salary) OVER w,",
          "496:     percentile_disc(0.25) WITHIN GROUP (ORDER BY salary) OVER w,",
          "497:     percentile_cont(0.25) WITHIN GROUP (ORDER BY salary DESC) OVER w,",
          "498:     percentile_disc(0.25) WITHIN GROUP (ORDER BY salary DESC) OVER w",
          "499: FROM basic_pays",
          "500: WINDOW w AS (PARTITION BY department)",
          "501: ORDER BY salary;",
          "503: SELECT",
          "504:     employee_name,",
          "505:     department,",
          "506:     salary,",
          "507:     percentile_cont(0.5) WITHIN GROUP (ORDER BY salary) OVER w,",
          "508:     percentile_disc(0.5) WITHIN GROUP (ORDER BY salary) OVER w,",
          "509:     percentile_cont(0.5) WITHIN GROUP (ORDER BY salary DESC) OVER w,",
          "510:     percentile_disc(0.5) WITHIN GROUP (ORDER BY salary DESC) OVER w",
          "511: FROM basic_pays",
          "512: WHERE salary > 8900",
          "513: WINDOW w AS (PARTITION BY department)",
          "514: ORDER BY salary;",
          "516: SELECT",
          "517:     employee_name,",
          "518:     department,",
          "519:     salary,",
          "520:     percentile_cont(0.25) WITHIN GROUP (ORDER BY salary) OVER w,",
          "521:     percentile_cont(0.25) WITHIN GROUP (ORDER BY salary DESC) OVER w",
          "522: FROM basic_pays",
          "523: WINDOW w AS (PARTITION BY department ORDER BY salary)",
          "524: ORDER BY salary;",
          "526: SELECT",
          "527:     employee_name,",
          "528:     department,",
          "529:     salary,",
          "530:     percentile_disc(0.25) WITHIN GROUP (ORDER BY salary) OVER w,",
          "531:     percentile_disc(0.25) WITHIN GROUP (ORDER BY salary DESC) OVER w",
          "532: FROM basic_pays",
          "533: WINDOW w AS (PARTITION BY department ORDER BY salary)",
          "534: ORDER BY salary;",
          "536: SELECT",
          "537:     employee_name,",
          "538:     department,",
          "539:     salary,",
          "540:     percentile_cont(0.25) WITHIN GROUP (ORDER BY salary) OVER w,",
          "541:     percentile_cont(0.25) WITHIN GROUP (ORDER BY salary DESC) OVER w",
          "542: FROM basic_pays",
          "543: WINDOW w AS (PARTITION BY department ROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING)",
          "544: ORDER BY salary;",
          "546: SELECT",
          "547:     employee_name,",
          "548:     department,",
          "549:     salary,",
          "550:     percentile_disc(0.25) WITHIN GROUP (ORDER BY salary) OVER w,",
          "551:     percentile_disc(0.25) WITHIN GROUP (ORDER BY salary DESC) OVER w",
          "552: FROM basic_pays",
          "553: WINDOW w AS (PARTITION BY department ROWS BETWEEN UNBOUNDED PRECEDING AND 1 FOLLOWING)",
          "554: ORDER BY salary;",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ac404a683a3908f5c80465aded07d6814ff80e90",
      "candidate_info": {
        "commit_hash": "ac404a683a3908f5c80465aded07d6814ff80e90",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/ac404a683a3908f5c80465aded07d6814ff80e90",
        "files": [
          "python/docs/source/reference/pyspark.pandas/general_functions.rst",
          "python/pyspark/pandas/__init__.py",
          "python/pyspark/pandas/missing/general_functions.py",
          "python/pyspark/pandas/tests/test_namespace.py",
          "python/pyspark/pandas/usage_logging/__init__.py"
        ],
        "message": "[SPARK-38755][PYTHON][3.3] Add file to address missing pandas general functions\n\n### What changes were proposed in this pull request?\n\nBackport for https://github.com/apache/spark/pull/36034\n\nThis PR proposes to add `python/pyspark/pandas/missing/general_functions.py` to track the missing [pandas general functions](https://pandas.pydata.org/docs/reference/general_functions.html) API.\n\n### Why are the changes needed?\n\nWe have scripts in `missing` directory to track & address the missing pandas APIs, but one for general functions is missing.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nThe existing tests should cover\n\nCloses #36034 from itholic/SPARK-38755.\n\nAuthored-by: itholic <haejoon.leedatabricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223apache.org>\n\nCloses #36955 from itholic/SPARK-38755-backport.\n\nAuthored-by: itholic <haejoon.lee@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/pandas/__init__.py||python/pyspark/pandas/__init__.py",
          "python/pyspark/pandas/missing/general_functions.py||python/pyspark/pandas/missing/general_functions.py",
          "python/pyspark/pandas/tests/test_namespace.py||python/pyspark/pandas/tests/test_namespace.py",
          "python/pyspark/pandas/usage_logging/__init__.py||python/pyspark/pandas/usage_logging/__init__.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/pandas/__init__.py||python/pyspark/pandas/__init__.py": [
          "File: python/pyspark/pandas/__init__.py -> python/pyspark/pandas/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import os",
          "24: import sys",
          "26: import warnings",
          "28: from pyspark.sql.pandas.utils import require_minimum_pandas_version, require_minimum_pyarrow_version",
          "30: try:",
          "",
          "[Removed Lines]",
          "25: from distutils.version import LooseVersion",
          "",
          "[Added Lines]",
          "26: from distutils.version import LooseVersion",
          "27: from typing import Any",
          "29: from pyspark.pandas.missing.general_functions import _MissingPandasLikeGeneralFunctions",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "151: from pyspark.pandas.config import get_option, options, option_context, reset_option, set_option",
          "152: from pyspark.pandas.namespace import *  # noqa: F403",
          "153: from pyspark.pandas.sql_formatter import sql",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "158: def __getattr__(key: str) -> Any:",
          "159:     if key.startswith(\"__\"):",
          "160:         raise AttributeError(key)",
          "161:     if hasattr(_MissingPandasLikeGeneralFunctions, key):",
          "162:         return getattr(_MissingPandasLikeGeneralFunctions, key)",
          "163:     else:",
          "164:         raise AttributeError(\"module 'pyspark.pandas' has no attribute '%s'\" % (key))",
          "",
          "---------------"
        ],
        "python/pyspark/pandas/missing/general_functions.py||python/pyspark/pandas/missing/general_functions.py": [
          "File: python/pyspark/pandas/missing/general_functions.py -> python/pyspark/pandas/missing/general_functions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one or more",
          "3: # contributor license agreements.  See the NOTICE file distributed with",
          "4: # this work for additional information regarding copyright ownership.",
          "5: # The ASF licenses this file to You under the Apache License, Version 2.0",
          "6: # (the \"License\"); you may not use this file except in compliance with",
          "7: # the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #    http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing, software",
          "12: # distributed under the License is distributed on an \"AS IS\" BASIS,",
          "13: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
          "14: # See the License for the specific language governing permissions and",
          "15: # limitations under the License.",
          "16: #",
          "17: from pyspark.pandas.missing import unsupported_function",
          "20: def _unsupported_function(method_name, deprecated=False, reason=\"\"):",
          "21:     return unsupported_function(",
          "22:         class_name=\"pd\", method_name=method_name, deprecated=deprecated, reason=reason",
          "23:     )",
          "26: class _MissingPandasLikeGeneralFunctions:",
          "28:     pivot = _unsupported_function(\"pivot\")",
          "29:     pivot_table = _unsupported_function(\"pivot_table\")",
          "30:     crosstab = _unsupported_function(\"crosstab\")",
          "31:     cut = _unsupported_function(\"cut\")",
          "32:     qcut = _unsupported_function(\"qcut\")",
          "33:     merge_ordered = _unsupported_function(\"merge_ordered\")",
          "34:     factorize = _unsupported_function(\"factorize\")",
          "35:     unique = _unsupported_function(\"unique\")",
          "36:     wide_to_long = _unsupported_function(\"wide_to_long\")",
          "37:     bdate_range = _unsupported_function(\"bdate_range\")",
          "38:     period_range = _unsupported_function(\"period_range\")",
          "39:     infer_freq = _unsupported_function(\"infer_freq\")",
          "40:     interval_range = _unsupported_function(\"interval_range\")",
          "41:     eval = _unsupported_function(\"eval\")",
          "",
          "---------------"
        ],
        "python/pyspark/pandas/tests/test_namespace.py||python/pyspark/pandas/tests/test_namespace.py": [
          "File: python/pyspark/pandas/tests/test_namespace.py -> python/pyspark/pandas/tests/test_namespace.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: #",
          "18: import itertools",
          "20: import pandas as pd",
          "21: import numpy as np",
          "23: from pyspark import pandas as ps",
          "24: from pyspark.pandas.namespace import _get_index_map, read_delta",
          "25: from pyspark.pandas.utils import spark_column_equals",
          "26: from pyspark.testing.pandasutils import PandasOnSparkTestCase",
          "27: from pyspark.testing.sqlutils import SQLTestUtils",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: import inspect",
          "25: from pyspark.pandas.exceptions import PandasNotImplementedError",
          "28: from pyspark.pandas.missing.general_functions import _MissingPandasLikeGeneralFunctions",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "554:             lambda: ps.to_numeric(psser, errors=\"ignore\"),",
          "555:         )",
          "558: if __name__ == \"__main__\":",
          "559:     import unittest",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "560:     def test_missing(self):",
          "561:         missing_functions = inspect.getmembers(",
          "562:             _MissingPandasLikeGeneralFunctions, inspect.isfunction",
          "563:         )",
          "564:         unsupported_functions = [",
          "565:             name for (name, type_) in missing_functions if type_.__name__ == \"unsupported_function\"",
          "566:         ]",
          "567:         for name in unsupported_functions:",
          "568:             with self.assertRaisesRegex(",
          "569:                 PandasNotImplementedError,",
          "570:                 \"The method.*pd.*{}.*not implemented yet.\".format(name),",
          "571:             ):",
          "572:                 getattr(ps, name)()",
          "",
          "---------------"
        ],
        "python/pyspark/pandas/usage_logging/__init__.py||python/pyspark/pandas/usage_logging/__init__.py": [
          "File: python/pyspark/pandas/usage_logging/__init__.py -> python/pyspark/pandas/usage_logging/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: from pyspark.pandas.indexes.multi import MultiIndex",
          "32: from pyspark.pandas.indexes.numeric import Float64Index, Int64Index",
          "33: from pyspark.pandas.missing.frame import _MissingPandasLikeDataFrame",
          "34: from pyspark.pandas.missing.groupby import (",
          "35:     MissingPandasLikeDataFrameGroupBy,",
          "36:     MissingPandasLikeSeriesGroupBy,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "34: from pyspark.pandas.missing.general_functions import _MissingPandasLikeGeneralFunctions",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "109:     modules.append(sql_formatter)",
          "111:     missings = [",
          "112:         (pd.DataFrame, _MissingPandasLikeDataFrame),",
          "113:         (pd.Series, MissingPandasLikeSeries),",
          "114:         (pd.Index, MissingPandasLikeIndex),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "113:         (pd, _MissingPandasLikeGeneralFunctions),",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "122:         (pd.core.window.RollingGroupby, MissingPandasLikeRollingGroupby),",
          "123:     ]",
          "",
          "[Removed Lines]",
          "125:     _attach(logger_module, modules, classes, missings)",
          "",
          "[Added Lines]",
          "127:     _attach(logger_module, modules, classes, missings)  # type: ignore[arg-type]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "459c4b0c94a39efe9ea8b5ef1da3f6e379417c40",
      "candidate_info": {
        "commit_hash": "459c4b0c94a39efe9ea8b5ef1da3f6e379417c40",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/459c4b0c94a39efe9ea8b5ef1da3f6e379417c40",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/DeduplicateRelations.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala"
        ],
        "message": "[SPARK-39144][SQL] Nested subquery expressions deduplicate relations should be done bottom up\n\n### What changes were proposed in this pull request?\n\nWhen we have nested subquery expressions, there is a chance that deduplicate relations could replace an attributes with a wrong one. This is because the attributes replacement is done by top down than bottom up. This could happen if the subplan gets deduplicate relations first (thus two same relation with different attributes id), then a more complex plan built on top of the subplan (e.g. a UNION of queries with nested subquery expressions) can trigger this wrong attribute replacement error.\n\nFor concrete example please see the added unit test.\n\n### Why are the changes needed?\n\nThis is bug that we can fix. Without this PR, we could hit that outer attribute reference does not exist in the outer relation at certain scenario.\n\n### Does this PR introduce _any_ user-facing change?\n\nNO\n\n### How was this patch tested?\n\nUT\n\nCloses #36503 from amaliujia/testnestedsubqueryexpression.\n\nAuthored-by: Rui Wang <rui.wang@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit d9fd36eb76fcfec95763cc4dc594eb7856b0fad2)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/DeduplicateRelations.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/DeduplicateRelations.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "728:       expressions.exists(_.exists(_.semanticEquals(expr)))",
          "729:     }",
          "732:     checkAnalysis(expr.plan)",
          "734:     expr match {",
          "735:       case ScalarSubquery(query, outerAttrs, _, _) =>",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "731:     def checkOuterReference(p: LogicalPlan, expr: SubqueryExpression): Unit = p match {",
          "732:       case f: Filter =>",
          "733:         if (hasOuterReferences(expr.plan)) {",
          "734:           expr.plan.expressions.foreach(_.foreachUp {",
          "735:             case o: OuterReference =>",
          "736:               p.children.foreach(e =>",
          "737:                 if (!e.output.exists(_.exprId == o.exprId)) {",
          "738:                   failAnalysis(\"outer attribute not found\")",
          "739:                 })",
          "740:             case _ =>",
          "741:           })",
          "742:         }",
          "743:       case _ =>",
          "744:     }",
          "750:     checkOuterReference(plan, expr)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/DeduplicateRelations.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/DeduplicateRelations.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/DeduplicateRelations.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/DeduplicateRelations.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "125:           }",
          "126:         }",
          "128:         if (planChanged) {",
          "131:             val attrMap = AttributeMap(",
          "132:               plan",
          "133:                 .children",
          "",
          "[Removed Lines]",
          "129:           if (plan.childrenResolved) {",
          "130:             val planWithNewChildren = plan.withNewChildren(newChildren.toSeq)",
          "",
          "[Added Lines]",
          "128:         val planWithNewSubquery = plan.transformExpressions {",
          "129:           case subquery: SubqueryExpression =>",
          "130:             val (renewed, collected, changed) = renewDuplicatedRelations(",
          "131:               existingRelations ++ relations, subquery.plan)",
          "132:             relations ++= collected",
          "133:             if (changed) planChanged = true",
          "134:             subquery.withNewPlan(renewed)",
          "135:         }",
          "138:           if (planWithNewSubquery.childrenResolved) {",
          "139:             val planWithNewChildren = planWithNewSubquery.withNewChildren(newChildren.toSeq)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "140:               planWithNewChildren.rewriteAttrs(attrMap)",
          "141:             }",
          "142:           } else {",
          "144:           }",
          "145:         } else {",
          "146:           plan",
          "",
          "[Removed Lines]",
          "143:             plan.withNewChildren(newChildren.toSeq)",
          "",
          "[Added Lines]",
          "152:             planWithNewSubquery.withNewChildren(newChildren.toSeq)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "148:       } else {",
          "149:         plan",
          "150:       }",
          "161:   }",
          "",
          "[Removed Lines]",
          "152:       val planWithNewSubquery = newPlan.transformExpressions {",
          "153:         case subquery: SubqueryExpression =>",
          "154:           val (renewed, collected, changed) = renewDuplicatedRelations(",
          "155:             existingRelations ++ relations, subquery.plan)",
          "156:           relations ++= collected",
          "157:           if (changed) planChanged = true",
          "158:           subquery.withNewPlan(renewed)",
          "159:       }",
          "160:       (planWithNewSubquery, relations, planChanged)",
          "",
          "[Added Lines]",
          "160:       (newPlan, relations, planChanged)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/AnalysisSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1176:         false)",
          "1177:     }",
          "1178:   }",
          "1179: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1180:   test(\"SPARK-39144: nested subquery expressions deduplicate relations should be done bottom up\") {",
          "1181:     val innerRelation = SubqueryAlias(\"src1\", testRelation)",
          "1182:     val outerRelation = SubqueryAlias(\"src2\", testRelation)",
          "1183:     val ref1 = testRelation.output.head",
          "1185:     val subPlan = getAnalyzer.execute(",
          "1186:       Project(",
          "1187:         Seq(UnresolvedStar(None)),",
          "1188:         Filter.apply(",
          "1189:           Exists(",
          "1190:             Filter.apply(",
          "1191:               EqualTo(",
          "1192:                 OuterReference(ref1),",
          "1193:                 ref1),",
          "1194:               innerRelation",
          "1195:             )",
          "1196:           ),",
          "1197:           outerRelation",
          "1198:         )))",
          "1200:     val finalPlan = {",
          "1201:       Union.apply(",
          "1202:         Project(",
          "1203:           Seq(UnresolvedStar(None)),",
          "1204:           subPlan",
          "1205:         ),",
          "1206:         Filter.apply(",
          "1207:           Exists(",
          "1208:             subPlan",
          "1209:           ),",
          "1210:           subPlan",
          "1211:         )",
          "1212:       )",
          "1213:     }",
          "1215:     assertAnalysisSuccess(finalPlan)",
          "1216:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e8e330fbbca5452e9af0a78e5f2cfae0cc6be134",
      "candidate_info": {
        "commit_hash": "e8e330fbbca5452e9af0a78e5f2cfae0cc6be134",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/e8e330fbbca5452e9af0a78e5f2cfae0cc6be134",
        "files": [
          "python/pyspark/sql/tests/test_streaming.py",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamSuite.scala"
        ],
        "message": "[SPARK-39218][SS][PYTHON] Make foreachBatch streaming query stop gracefully\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to make the `foreachBatch` streaming query stop gracefully by handling the interrupted exceptions at `StreamExecution.isInterruptionException`.\n\nBecause there is no straightforward way to access to the original JVM exception, here we rely on string pattern match for now (see also \"Why are the changes needed?\" below). There is only one place from Py4J https://github.com/py4j/py4j/blob/master/py4j-python/src/py4j/protocol.py#L326-L328 so the approach would work at least.\n\n### Why are the changes needed?\n\nIn `foreachBatch`,  the Python user-defined function in the microbatch runs till the end even when `StreamingQuery.stop` is invoked. However, when any Py4J access is attempted within the user-defined function:\n\n- With the pinned thread mode disabled, the interrupt exception is not blocked, and the Python function is executed till the end in a different thread.\n- With the pinned thread mode enabled, the interrupt exception is raised in the same thread, and the Python thread raises a Py4J exception in the same thread.\n\nThe latter case is a problem because the interrupt exception is first thrown from JVM side (`java.lang. InterruptedException`) -> Python callback server (`py4j.protocol.Py4JJavaError`) -> JVM (`py4j.Py4JException`), and `py4j.Py4JException` is not listed in `StreamExecution.isInterruptionException` which doesn't gracefully stop the query.\n\nTherefore, we should handle this exception at `StreamExecution.isInterruptionException`.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, it will make the query gracefully stop.\n\n### How was this patch tested?\n\nManually tested with:\n\n```python\nimport time\n\ndef func(batch_df, batch_id):\n    time.sleep(10)\n    print(batch_df.count())\n\nq = spark.readStream.format(\"rate\").load().writeStream.foreachBatch(func).start()\ntime.sleep(5)\nq.stop()\n```\n\nCloses #36589 from HyukjinKwon/SPARK-39218.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 499de87b77944157828a6d905d9b9df37b7c9a67)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/sql/tests/test_streaming.py||python/pyspark/sql/tests/test_streaming.py",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/sql/tests/test_streaming.py||python/pyspark/sql/tests/test_streaming.py": [
          "File: python/pyspark/sql/tests/test_streaming.py -> python/pyspark/sql/tests/test_streaming.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "592:             if q:",
          "593:                 q.stop()",
          "595:     def test_streaming_read_from_table(self):",
          "596:         with self.table(\"input_table\", \"this_query\"):",
          "597:             self.spark.sql(\"CREATE TABLE input_table (value string) USING parquet\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "595:     def test_streaming_foreachBatch_graceful_stop(self):",
          "596:         # SPARK-39218: Make foreachBatch streaming query stop gracefully",
          "597:         def func(batch_df, _):",
          "598:             batch_df.sparkSession._jvm.java.lang.Thread.sleep(10000)",
          "600:         q = self.spark.readStream.format(\"rate\").load().writeStream.foreachBatch(func).start()",
          "601:         time.sleep(3)  # 'rowsPerSecond' defaults to 1. Waits 3 secs out for the input.",
          "602:         q.stop()",
          "603:         self.assertIsNone(q.exception(), \"No exception has to be propagated.\")",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "618: object StreamExecution {",
          "619:   val QUERY_ID_KEY = \"sql.streaming.queryId\"",
          "620:   val IS_CONTINUOUS_PROCESSING = \"__is_continuous_processing\"",
          "622:   @scala.annotation.tailrec",
          "623:   def isInterruptionException(e: Throwable, sc: SparkContext): Boolean = e match {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "621:   val IO_EXCEPTION_NAMES = Seq(",
          "622:     classOf[InterruptedException].getName,",
          "623:     classOf[InterruptedIOException].getName,",
          "624:     classOf[ClosedByInterruptException].getName)",
          "625:   val PROXY_ERROR = (",
          "626:     \"py4j.protocol.Py4JJavaError: An error occurred while calling\" +",
          "627:     s\".+(\\\\r\\\\n|\\\\r|\\\\n): (${IO_EXCEPTION_NAMES.mkString(\"|\")})\").r",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "647:       } else {",
          "648:         false",
          "649:       }",
          "650:     case _ =>",
          "651:       false",
          "652:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "660:     case e: py4j.Py4JException => PROXY_ERROR.findFirstIn(e.getMessage).isDefined",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1175:     new ClosedByInterruptException,",
          "1176:     new UncheckedIOException(\"test\", new ClosedByInterruptException),",
          "1177:     new ExecutionException(\"test\", new InterruptedException),",
          "1180:       ThrowingExceptionInCreateSource.createSourceLatch = new CountDownLatch(1)",
          "1181:       ThrowingExceptionInCreateSource.exception = e",
          "1182:       val query = spark",
          "",
          "[Removed Lines]",
          "1178:     new UncheckedExecutionException(\"test\", new InterruptedException))) {",
          "1179:     test(s\"view ${e.getClass.getSimpleName} as a normal query stop\") {",
          "",
          "[Added Lines]",
          "1178:     new UncheckedExecutionException(\"test\", new InterruptedException)) ++",
          "1179:     Seq(",
          "1180:       classOf[InterruptedException].getName,",
          "1181:       classOf[InterruptedIOException].getName,",
          "1182:       classOf[ClosedByInterruptException].getName).map { s =>",
          "1183:     new py4j.Py4JException(",
          "1184:       s\"\"\"",
          "1185:         |py4j.protocol.Py4JJavaError: An error occurred while calling o44.count.",
          "1186:         |: $s",
          "1187:         |\"\"\".stripMargin)",
          "1188:     }) {",
          "1189:     test(s\"view ${e.getClass.getSimpleName} [${e.getMessage}] as a normal query stop\") {",
          "",
          "---------------"
        ]
      }
    }
  ]
}