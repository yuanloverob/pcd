{
  "cve_id": "CVE-2023-50944",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows an authenticated user to access the source code of a DAG to which they don't have access.\u00a0This vulnerability is considered low since it requires an authenticated user to exploit it. Users are recommended to upgrade to version 2.8.1, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "8d76538d6e105947272b000581c6fabec20146b1",
  "patch_info": {
    "commit_hash": "8d76538d6e105947272b000581c6fabec20146b1",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/8d76538d6e105947272b000581c6fabec20146b1",
    "files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ],
    "message": "Check DAG read permission before accessing DAG code (#36257)\n\n(cherry picked from commit 30ea37e0d247ce54c2d25b115e807fdb0074d795)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py||airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_source_endpoint.py -> airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "17: from __future__ import annotations",
      "19: from http import HTTPStatus",
      "21: from flask import Response, current_app, request",
      "22: from itsdangerous import BadSignature, URLSafeSerializer",
      "24: from airflow.api_connexion import security",
      "26: from airflow.api_connexion.schemas.dag_source_schema import dag_source_schema",
      "27: from airflow.auth.managers.models.resource_details import DagAccessEntity",
      "28: from airflow.models.dagcode import DagCode",
      "31: @security.requires_access_dag(\"GET\", DagAccessEntity.CODE)",
      "33:     \"\"\"Get source code using file token.\"\"\"",
      "34:     secret_key = current_app.config[\"SECRET_KEY\"]",
      "35:     auth_s = URLSafeSerializer(secret_key)",
      "36:     try:",
      "37:         path = auth_s.loads(file_token)",
      "39:     except (BadSignature, FileNotFoundError):",
      "40:         raise NotFound(\"Dag source not found\")",
      "",
      "[Removed Lines]",
      "25: from airflow.api_connexion.exceptions import NotFound",
      "32: def get_dag_source(*, file_token: str) -> Response:",
      "38:         dag_source = DagCode.code(path)",
      "",
      "[Added Lines]",
      "20: from typing import TYPE_CHECKING",
      "26: from airflow.api_connexion.exceptions import NotFound, PermissionDenied",
      "28: from airflow.api_connexion.security import get_readable_dags",
      "30: from airflow.models.dag import DagModel",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "34: if TYPE_CHECKING:",
      "35:     from sqlalchemy.orm import Session",
      "39: @provide_session",
      "40: def get_dag_source(*, file_token: str, session: Session = NEW_SESSION) -> Response:",
      "46:         dag_ids = session.query(DagModel.dag_id).filter(DagModel.fileloc == path).all()",
      "47:         readable_dags = get_readable_dags()",
      "48:         # Check if user has read access to all the DAGs defined in the file",
      "49:         if any(dag_id[0] not in readable_dags for dag_id in dag_ids):",
      "50:             raise PermissionDenied()",
      "51:         dag_source = DagCode.code(path, session=session)",
      "",
      "---------------"
    ],
    "airflow/models/dagcode.py||airflow/models/dagcode.py": [
      "File: airflow/models/dagcode.py -> airflow/models/dagcode.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "177:         return cls.code(fileloc)",
      "179:     @classmethod",
      "181:         \"\"\"Return source code for this DagCode object.",
      "183:         :return: source code as string",
      "184:         \"\"\"",
      "187:     @staticmethod",
      "188:     def _get_code_from_file(fileloc):",
      "",
      "[Removed Lines]",
      "180:     def code(cls, fileloc) -> str:",
      "185:         return cls._get_code_from_db(fileloc)",
      "",
      "[Added Lines]",
      "180:     @provide_session",
      "181:     def code(cls, fileloc, session: Session = NEW_SESSION) -> str:",
      "186:         return cls._get_code_from_db(fileloc, session)",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_source_endpoint.py -> tests/api_connexion/endpoints/test_dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35: ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))",
      "36: EXAMPLE_DAG_FILE = os.path.join(\"airflow\", \"example_dags\", \"example_bash_operator.py\")",
      "39: @pytest.fixture(scope=\"module\")",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "37: EXAMPLE_DAG_ID = \"example_bash_operator\"",
      "38: TEST_DAG_ID = \"latest_only\"",
      "39: NOT_READABLE_DAG_ID = \"latest_only_with_trigger\"",
      "40: TEST_MULTIPLE_DAGS_ID = \"dataset_produces_1\"",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "45:         role_name=\"Test\",",
      "46:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_CODE)],  # type: ignore",
      "47:     )",
      "48:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "50:     yield app",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "52:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "53:         TEST_DAG_ID,",
      "54:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "55:     )",
      "56:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "57:         EXAMPLE_DAG_ID,",
      "58:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "59:     )",
      "60:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "61:         TEST_MULTIPLE_DAGS_ID,",
      "62:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "63:     )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "80:     def test_should_respond_200_text(self, url_safe_serializer):",
      "81:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "82:         dagbag.sync_to_db()",
      "87:         response = self.client.get(",
      "88:             url, headers={\"Accept\": \"text/plain\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "89:         )",
      "",
      "[Removed Lines]",
      "83:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "84:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "86:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "99:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "100:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "102:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "95:     def test_should_respond_200_json(self, url_safe_serializer):",
      "96:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "97:         dagbag.sync_to_db()",
      "102:         response = self.client.get(",
      "103:             url, headers={\"Accept\": \"application/json\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "104:         )",
      "",
      "[Removed Lines]",
      "98:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "99:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "101:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "114:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "115:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "117:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "110:     def test_should_respond_406(self, url_safe_serializer):",
      "111:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "112:         dagbag.sync_to_db()",
      "116:         response = self.client.get(",
      "117:             url, headers={\"Accept\": \"image/webp\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "118:         )",
      "",
      "[Removed Lines]",
      "113:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "115:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "129:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "131:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "151:             environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"},",
      "152:         )",
      "153:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "171:     def test_should_respond_403_not_readable(self, url_safe_serializer):",
      "172:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "173:         dagbag.sync_to_db()",
      "174:         dag: DAG = dagbag.dags[NOT_READABLE_DAG_ID]",
      "176:         response = self.client.get(",
      "177:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "178:             headers={\"Accept\": \"text/plain\"},",
      "179:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "180:         )",
      "181:         read_dag = self.client.get(",
      "182:             f\"/api/v1/dags/{NOT_READABLE_DAG_ID}\",",
      "183:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "184:         )",
      "185:         assert response.status_code == 403",
      "186:         assert read_dag.status_code == 403",
      "188:     def test_should_respond_403_some_dags_not_readable_in_the_file(self, url_safe_serializer):",
      "189:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "190:         dagbag.sync_to_db()",
      "191:         dag: DAG = dagbag.dags[TEST_MULTIPLE_DAGS_ID]",
      "193:         response = self.client.get(",
      "194:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "195:             headers={\"Accept\": \"text/plain\"},",
      "196:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "197:         )",
      "199:         read_dag = self.client.get(",
      "200:             f\"/api/v1/dags/{TEST_MULTIPLE_DAGS_ID}\",",
      "201:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "202:         )",
      "203:         assert response.status_code == 403",
      "204:         assert read_dag.status_code == 200",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "1e56cfade7aaf28eb8e55f1173a0fd85d225a47a",
      "candidate_info": {
        "commit_hash": "1e56cfade7aaf28eb8e55f1173a0fd85d225a47a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/1e56cfade7aaf28eb8e55f1173a0fd85d225a47a",
        "files": [
          "tests/conftest.py"
        ],
        "message": "Disable `PytestDeprecationWarning` when create warnings_recorder (#36759)\n\n(cherry picked from commit 70cefebbd5e20989b48386742089ebf747d991c1)",
        "before_after_code_files": [
          "tests/conftest.py||tests/conftest.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/conftest.py||tests/conftest.py": [
          "File: tests/conftest.py -> tests/conftest.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: import pytest",
          "31: import time_machine",
          "34: # We should set these before loading _any_ of the rest of airflow so that the",
          "35: # unit test mode config is set as early as possible.",
          "",
          "[Removed Lines]",
          "32: from _pytest.recwarn import WarningsRecorder",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1149: captured_warnings: dict[tuple[str, int, type[Warning], str], warnings.WarningMessage] = {}",
          "1150: captured_warnings_count: dict[tuple[str, int, type[Warning], str], int] = {}",
          "1152: default_formatwarning = warnings_recorder._module.formatwarning  # type: ignore[attr-defined]",
          "1153: default_showwarning = warnings_recorder._module.showwarning  # type: ignore[attr-defined]",
          "",
          "[Removed Lines]",
          "1151: warnings_recorder = WarningsRecorder()",
          "",
          "[Added Lines]",
          "1150: # By set ``_ispytest=True`` in WarningsRecorder we suppress annoying warnings:",
          "1151: # PytestDeprecationWarning: A private pytest class or function was used.",
          "1152: warnings_recorder = pytest.WarningsRecorder(_ispytest=True)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "998dd5c84c2f22a12e255557fa80774d8b8d1086",
      "candidate_info": {
        "commit_hash": "998dd5c84c2f22a12e255557fa80774d8b8d1086",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/998dd5c84c2f22a12e255557fa80774d8b8d1086",
        "files": [
          "airflow/www/yarn.lock"
        ],
        "message": "Bump follow-redirects from 1.15.3 to 1.15.4 in /airflow/www (#36700)\n\nBumps [follow-redirects](https://github.com/follow-redirects/follow-redirects) from 1.15.3 to 1.15.4.\n- [Release notes](https://github.com/follow-redirects/follow-redirects/releases)\n- [Commits](https://github.com/follow-redirects/follow-redirects/compare/v1.15.3...v1.15.4)\n\n---\nupdated-dependencies:\n- dependency-name: follow-redirects\n  dependency-type: indirect\n...\n\nSigned-off-by: dependabot[bot] <support@github.com>\nCo-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>\n(cherry picked from commit 0d7bad20d4398b55bafcd6fa41db89fc2e509d69)",
        "before_after_code_files": [
          "airflow/www/yarn.lock||airflow/www/yarn.lock"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/yarn.lock||airflow/www/yarn.lock": [
          "File: airflow/www/yarn.lock -> airflow/www/yarn.lock",
          "--- Hunk 1 ---",
          "[Context before]",
          "6441:     tslib \"^2.0.3\"",
          "6443: follow-redirects@^1.15.0:",
          "6448: for-each@^0.3.3:",
          "6449:   version \"0.3.3\"",
          "",
          "[Removed Lines]",
          "6444:   version \"1.15.3\"",
          "6445:   resolved \"https://registry.yarnpkg.com/follow-redirects/-/follow-redirects-1.15.3.tgz#fe2f3ef2690afce7e82ed0b44db08165b207123a\"",
          "6446:   integrity sha512-1VzOtuEM8pC9SFU1E+8KfTjZyMztRsgEfwQl44z8A25uy13jSzTj6dyK2Df52iV0vgHCfBwLhDWevLn95w5v6Q==",
          "",
          "[Added Lines]",
          "6444:   version \"1.15.4\"",
          "6445:   resolved \"https://registry.yarnpkg.com/follow-redirects/-/follow-redirects-1.15.4.tgz#cdc7d308bf6493126b17ea2191ea0ccf3e535adf\"",
          "6446:   integrity sha512-Cr4D/5wlrb0z9dgERpUL3LrmPKVDsETIJhaCMeDfuFYcqa5bldGV6wBsAN6X/vxlXQtFBMrXdXxdL8CbDTGniw==",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "525e22907471d08dc23bbcdc82c5c5c9d7687b4e",
      "candidate_info": {
        "commit_hash": "525e22907471d08dc23bbcdc82c5c5c9d7687b4e",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/525e22907471d08dc23bbcdc82c5c5c9d7687b4e",
        "files": [
          "airflow/jobs/backfill_job_runner.py"
        ],
        "message": "Revert \"Refactor _manage_executor_state by refreshing TIs in batch (#36418)\" (#36500)\n\nThis reverts commit 9d45db9e2cca2ad04db72f7e0712c478e5a8e1f1.\n\nt#\n\n(cherry picked from commit 72f43fcc838afc1c95b85dcb27af6519483ef64b)",
        "before_after_code_files": [
          "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py": [
          "File: airflow/jobs/backfill_job_runner.py -> airflow/jobs/backfill_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import attr",
          "24: import pendulum",
          "26: from sqlalchemy.exc import OperationalError",
          "27: from sqlalchemy.orm.session import make_transient",
          "28: from tabulate import tabulate",
          "",
          "[Removed Lines]",
          "25: from sqlalchemy import select, tuple_, update",
          "",
          "[Added Lines]",
          "25: from sqlalchemy import select, update",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "264:         :return: An iterable of expanded TaskInstance per MappedTask",
          "265:         \"\"\"",
          "266:         executor = self.job.executor",
          "291:             state, info = value",
          "294:                 self.log.warning(\"%s state %s not in running=%s\", key, state, running.values())",
          "295:                 continue",
          "299:             self.log.debug(\"Executor state: %s task %s\", state, ti)",
          "",
          "[Removed Lines]",
          "267:         # list of tuples (dag_id, task_id, execution_date, map_index) of running tasks in executor",
          "268:         buffered_events = list(executor.get_event_buffer().items())",
          "269:         running_tis_ids = [",
          "270:             (key.dag_id, key.task_id, key.run_id, key.map_index)",
          "271:             for key, _ in buffered_events",
          "272:             if key in running",
          "273:         ]",
          "274:         # list of TaskInstance of running tasks in executor (refreshed from db in batch)",
          "275:         refreshed_running_tis = session.scalars(",
          "276:             select(TaskInstance).where(",
          "277:                 tuple_(",
          "278:                     TaskInstance.dag_id,",
          "279:                     TaskInstance.task_id,",
          "280:                     TaskInstance.run_id,",
          "281:                     TaskInstance.map_index,",
          "282:                 ).in_(running_tis_ids)",
          "283:             )",
          "284:         ).all()",
          "285:         # dict of refreshed TaskInstance by key to easily find them",
          "286:         refreshed_running_tis_dict = {",
          "287:             (ti.dag_id, ti.task_id, ti.run_id, ti.map_index): ti for ti in refreshed_running_tis",
          "288:         }",
          "290:         for key, value in buffered_events:",
          "292:             ti_key = (key.dag_id, key.task_id, key.run_id, key.map_index)",
          "293:             if ti_key not in refreshed_running_tis_dict:",
          "297:             ti = refreshed_running_tis_dict[ti_key]",
          "",
          "[Added Lines]",
          "268:         # TODO: query all instead of refresh from db",
          "269:         for key, value in list(executor.get_event_buffer().items()):",
          "271:             if key not in running:",
          "275:             ti = running[key]",
          "276:             ti.refresh_from_db()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "119ba542541f754dc2247bf9de8ea5fa7aa3e8f9",
      "candidate_info": {
        "commit_hash": "119ba542541f754dc2247bf9de8ea5fa7aa3e8f9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/119ba542541f754dc2247bf9de8ea5fa7aa3e8f9",
        "files": [
          "airflow/models/dag.py",
          "tests/models/test_dag.py"
        ],
        "message": "Fix Callback exception when a removed task is the last one in the task instance list (#36693)\n\n* Fix Callback exception when a removed task is the last one in the task instance list\n\n* Add test_dag_handle_callback_with_removed_task\n\n* Remove extra break line\n\n* Merge TIs filters\n\n* Fix static check\n\n* Revert changes\n\n(cherry picked from commit 8c1c09bab34be8234d9e152ed7e4c9b925c08459)",
        "before_after_code_files": [
          "airflow/models/dag.py||airflow/models/dag.py",
          "tests/models/test_dag.py||tests/models/test_dag.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1462:             # context for the callback.",
          "1463:             if dag.partial:",
          "1464:                 tis = [ti for ti in tis if not ti.state == State.NONE]",
          "1465:             ti = tis[-1]  # get first TaskInstance of DagRun",
          "1466:             ti.task = dag.get_task(ti.task_id)",
          "1467:             context = ti.get_template_context(session=session)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1465:             # filter out removed tasks",
          "1466:             tis = [ti for ti in tis if ti.state != TaskInstanceState.REMOVED]",
          "",
          "---------------"
        ],
        "tests/models/test_dag.py||tests/models/test_dag.py": [
          "File: tests/models/test_dag.py -> tests/models/test_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1512:         dag.clear()",
          "1513:         self._clean_up(dag_id)",
          "1515:     def test_next_dagrun_after_fake_scheduled_previous(self):",
          "1516:         \"\"\"",
          "1517:         Test scheduling a dag where there is a prior DagRun",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1515:     def test_dag_handle_callback_with_removed_task(self, dag_maker, session):",
          "1516:         \"\"\"",
          "1517:         Tests avoid crashes when a removed task is the last one in the list of task instance",
          "1518:         \"\"\"",
          "1519:         dag_id = \"test_dag_callback_with_removed_task\"",
          "1520:         mock_callback = mock.MagicMock()",
          "1521:         with DAG(",
          "1522:             dag_id=dag_id,",
          "1523:             on_success_callback=mock_callback,",
          "1524:             on_failure_callback=mock_callback,",
          "1525:         ) as dag:",
          "1526:             EmptyOperator(task_id=\"faketastic\")",
          "1527:             task_removed = EmptyOperator(task_id=\"removed_task\")",
          "1529:         with create_session() as session:",
          "1530:             dag_run = dag.create_dagrun(State.RUNNING, TEST_DATE, run_type=DagRunType.MANUAL, session=session)",
          "1531:             dag._remove_task(task_removed.task_id)",
          "1532:             tis = dag_run.get_task_instances(session=session)",
          "1533:             tis[-1].state = TaskInstanceState.REMOVED",
          "1534:             assert dag_run.get_task_instance(task_removed.task_id).state == TaskInstanceState.REMOVED",
          "1536:             # should not raise any exception",
          "1537:             dag.handle_callback(dag_run, success=True)",
          "1538:             dag.handle_callback(dag_run, success=False)",
          "1540:         dag.clear()",
          "1541:         self._clean_up(dag_id)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e49c573d9ce03066c61cce0624576700de1de07a",
      "candidate_info": {
        "commit_hash": "e49c573d9ce03066c61cce0624576700de1de07a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e49c573d9ce03066c61cce0624576700de1de07a",
        "files": [
          "dev/README_RELEASE_PROVIDER_PACKAGES.md",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2"
        ],
        "message": "Update release process for providers to include mixed RC versions (#36385)\n\nThis PR updates released process for providers to enable releasing\nproviders in more regular batches. Sometimes when we exclude a\nprovider from previous voting, we want to release RCN (2,3 etc.)\ncandidate.\n\nHowever, especially when time between previous RC and the new one\nis long (for example because fixing took a long time) we might\nwant to release the RCN release for that cancelled providers and\nRC1 for all the providers that have been changed in the meantime.\n\nThis cchange makes it possible (and easy):\n\n1) release RC1 for all providers (the RCN provider should be skipped,\n   because tag for this provider already exists.\n\n2) release the RCN providers with `--version-suffix-for-pypi rcN`.\n\nThe release process and tools were updated to account for that - where\nrc candidate number is retrieved from packages prepared in `dist`.\n\nFixed a few small missing things in the process.\n\n(cherry picked from commit 4deed641322343cb18eabd9ad199f5ac83f29ff0)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2||dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1495:     )",
          "1498: def get_prs_for_package(provider_id: str) -> list[int]:",
          "1499:     pr_matcher = re.compile(r\".*\\(#([0-9]*)\\)``$\")",
          "1500:     prs = []",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1498: VERSION_MATCH = re.compile(r\"([0-9]+)\\.([0-9]+)\\.([0-9]+)(.*)\")",
          "1501: def get_suffix_from_package_in_dist(dist_files: list[str], package: str) -> str | None:",
          "1502:     \"\"\"Get suffix from package prepared in dist folder.\"\"\"",
          "1503:     for file in dist_files:",
          "1504:         if file.startswith(f'apache_airflow_providers_{package.replace(\".\", \"_\")}') and file.endswith(",
          "1505:             \".tar.gz\"",
          "1506:         ):",
          "1507:             file = file[: -len(\".tar.gz\")]",
          "1508:             version = file.split(\"-\")[-1]",
          "1509:             match = VERSION_MATCH.match(version)",
          "1510:             if match:",
          "1511:                 return match.group(4)",
          "1512:     return None",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1566:         pypi_package_name: str",
          "1567:         version: str",
          "1568:         pr_list: list[PullRequest.PullRequest | Issue.Issue]",
          "1570:     if not provider_packages:",
          "1571:         provider_packages = list(DEPENDENCIES.keys())",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1586:         suffix: str",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1622:                 ).read_text()",
          "1623:             )",
          "1624:             if pull_request_list:",
          "1625:                 providers[provider_id] = ProviderPRInfo(",
          "1626:                     version=provider_yaml_dict[\"versions\"][0],",
          "1627:                     provider_package_id=provider_id,",
          "1628:                     pypi_package_name=provider_yaml_dict[\"package-name\"],",
          "1629:                     pr_list=pull_request_list,",
          "1630:                 )",
          "1631:         template = jinja2.Template(",
          "1632:             (Path(__file__).parents[1] / \"provider_issue_TEMPLATE.md.jinja2\").read_text()",
          "1633:         )",
          "1635:         get_console().print()",
          "1636:         get_console().print(",
          "1637:             \"[green]Below you can find the issue content that you can use \"",
          "",
          "[Removed Lines]",
          "1634:         issue_content = template.render(providers=providers, date=datetime.now(), suffix=suffix)",
          "",
          "[Added Lines]",
          "1643:                 package_suffix = get_suffix_from_package_in_dist(files_in_dist, provider_id)",
          "1649:                     suffix=package_suffix if package_suffix else suffix,",
          "1654:         issue_content = template.render(providers=providers, date=datetime.now())",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2||dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2": [
          "File: dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2 -> dev/breeze/src/airflow_breeze/provider_issue_TEMPLATE.md.jinja2",
          "--- Hunk 1 ---",
          "[Context before]",
          "10: Those are providers that require testing as there were some substantial changes introduced:",
          "12: {% for provider_id, provider_info in providers.items()  %}",
          "14: {%- for pr in provider_info.pr_list %}",
          "15:    - [ ] [{{ pr.title }} (#{{ pr.number }})]({{ pr.html_url }}): @{{ pr.user.login }}",
          "16: {%- endfor %}",
          "",
          "[Removed Lines]",
          "13: ## Provider [{{ provider_id }}: {{ provider_info.version }}{{ suffix }}](https://pypi.org/project/{{ provider_info.pypi_package_name }}/{{ provider_info.version }}{{ suffix }})",
          "",
          "[Added Lines]",
          "13: ## Provider [{{ provider_id }}: {{ provider_info.version }}{{ provider_info.suffix }}](https://pypi.org/project/{{ provider_info.pypi_package_name }}/{{ provider_info.version }}{{ provider_info.suffix }})",
          "",
          "---------------"
        ]
      }
    }
  ]
}