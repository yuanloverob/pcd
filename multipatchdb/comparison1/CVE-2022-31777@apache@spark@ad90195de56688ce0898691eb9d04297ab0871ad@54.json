{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "27c75eae92333add3ba6854b6c46410ec8e6743f",
      "candidate_info": {
        "commit_hash": "27c75eae92333add3ba6854b6c46410ec8e6743f",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/27c75eae92333add3ba6854b6c46410ec8e6743f",
        "files": [
          "docs/sql-migration-guide.md",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala"
        ],
        "message": "[SPARK-37575][SQL][FOLLOWUP] Update the migration guide for added legacy flag for the breaking change of write null value in csv to unquoted empty string\n\n### What changes were proposed in this pull request?\nThis is a follow-up of updating the migration guide for https://github.com/apache/spark/pull/36110 which adds a legacy flag to restore the pre-change behavior.\nIt also fixes a typo in the previous flag description.\n\n### Why are the changes needed?\nThe flag needs to be documented.\n\n### Does this PR introduce _any_ user-facing change?\nIt changes the migration doc for users.\n\n### How was this patch tested?\nNo tests\n\nCloses #36268 from anchovYu/flags-null-to-csv-migration-guide.\n\nAuthored-by: Xinyi Yu <xinyi.yu@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit a67acbaa29d1ab9071910cac09323c2544d65303)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "3728:     buildConf(\"spark.sql.legacy.nullValueWrittenAsQuotedEmptyStringCsv\")",
          "3729:       .internal()",
          "3730:       .doc(\"When set to false, nulls are written as unquoted empty strings in CSV data source. \" +",
          "3732:         \"empty strings, `\\\"\\\"`.\")",
          "3733:       .version(\"3.3.0\")",
          "3734:       .booleanConf",
          "",
          "[Removed Lines]",
          "3731:         \"If set to false, it restores the legacy behavior that nulls were written as quoted \" +",
          "",
          "[Added Lines]",
          "3731:         \"If set to true, it restores the legacy behavior that nulls were written as quoted \" +",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "acf8f66650af53718b08f3778c2a2a3a5d10a88f",
      "candidate_info": {
        "commit_hash": "acf8f66650af53718b08f3778c2a2a3a5d10a88f",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/acf8f66650af53718b08f3778c2a2a3a5d10a88f",
        "files": [
          "core/src/main/scala/org/apache/spark/storage/BlockManager.scala",
          "core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala"
        ],
        "message": "[SPARK-39647][CORE] Register the executor with ESS before registering the BlockManager\n\n### What changes were proposed in this pull request?\nCurrently the executors register with the ESS after the `BlockManager` registration with the `BlockManagerMaster`.  This order creates a problem with the push-based shuffle. A registered BlockManager node is picked up by the driver as a merger but the shuffle service on that node is not yet ready to merge the data which causes block pushes to fail until the local executor registers with it. This fix is to reverse the order, that is, register with the ESS before registering the `BlockManager`\n\n### Why are the changes needed?\nThey are needed to fix the issue which causes block pushes to fail.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nAdded a UT.\n\nCloses #37052 from otterc/SPARK-39647.\n\nAuthored-by: Chandni Singh <singh.chandni@gmail.com>\nSigned-off-by: Mridul Muralidharan <mridul<at>gmail.com>\n(cherry picked from commit 79ba2890f51c5f676b9cd6e3a6682c7969462999)\nSigned-off-by: Mridul Muralidharan <mridulatgmail.com>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/storage/BlockManager.scala||core/src/main/scala/org/apache/spark/storage/BlockManager.scala",
          "core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala||core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/storage/BlockManager.scala||core/src/main/scala/org/apache/spark/storage/BlockManager.scala": [
          "File: core/src/main/scala/org/apache/spark/storage/BlockManager.scala -> core/src/main/scala/org/apache/spark/storage/BlockManager.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "516:       ret",
          "517:     }",
          "519:     val id =",
          "520:       BlockManagerId(executorId, blockTransferService.hostName, blockTransferService.port, None)",
          "522:     val idFromMaster = master.registerBlockManager(",
          "523:       id,",
          "524:       diskBlockManager.localDirsString,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "526:     if (externalShuffleServiceEnabled) {",
          "527:       logInfo(s\"external shuffle service port = $externalShuffleServicePort\")",
          "528:       shuffleServerId = BlockManagerId(executorId, blockTransferService.hostName,",
          "529:         externalShuffleServicePort)",
          "530:       if (!isDriver) {",
          "531:         registerWithExternalShuffleServer()",
          "532:       }",
          "533:     }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "529:     blockManagerId = if (idFromMaster != null) idFromMaster else id",
          "541:     }",
          "543:     hostLocalDirManager = {",
          "",
          "[Removed Lines]",
          "531:     shuffleServerId = if (externalShuffleServiceEnabled) {",
          "532:       logInfo(s\"external shuffle service port = $externalShuffleServicePort\")",
          "533:       BlockManagerId(executorId, blockTransferService.hostName, externalShuffleServicePort)",
          "534:     } else {",
          "535:       blockManagerId",
          "536:     }",
          "539:     if (externalShuffleServiceEnabled && !blockManagerId.isDriver) {",
          "540:       registerWithExternalShuffleServer()",
          "",
          "[Added Lines]",
          "549:     if (!externalShuffleServiceEnabled) {",
          "550:       shuffleServerId = blockManagerId",
          "",
          "---------------"
        ],
        "core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala||core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala": [
          "File: core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala -> core/src/test/scala/org/apache/spark/storage/BlockManagerSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2175:     assert(kryoException.getMessage === \"java.io.IOException: Input/output error\")",
          "2176:   }",
          "2178:   private def createKryoSerializerWithDiskCorruptedInputStream(): KryoSerializer = {",
          "2179:     class TestDiskCorruptedInputStream extends InputStream {",
          "2180:       override def read(): Int = throw new IOException(\"Input/output error\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2178:   test(\"SPARK-39647: Failure to register with ESS should prevent registering the BM\") {",
          "2179:     val handler = new NoOpRpcHandler {",
          "2180:       override def receive(",
          "2181:           client: TransportClient,",
          "2182:           message: ByteBuffer,",
          "2183:           callback: RpcResponseCallback): Unit = {",
          "2184:         val msgObj = BlockTransferMessage.Decoder.fromByteBuffer(message)",
          "2185:         msgObj match {",
          "2186:           case _: RegisterExecutor => () // No reply to generate client-side timeout",
          "2187:         }",
          "2188:       }",
          "2189:     }",
          "2190:     val transConf = SparkTransportConf.fromSparkConf(conf, \"shuffle\")",
          "2191:     Utils.tryWithResource(new TransportContext(transConf, handler, true)) { transCtx =>",
          "2192:       def newShuffleServer(port: Int): (TransportServer, Int) = {",
          "2193:         (transCtx.createServer(port, Seq.empty[TransportServerBootstrap].asJava), port)",
          "2194:       }",
          "2196:       val candidatePort = RandomUtils.nextInt(1024, 65536)",
          "2197:       val (server, shufflePort) = Utils.startServiceOnPort(candidatePort,",
          "2198:         newShuffleServer, conf, \"ShuffleServer\")",
          "2200:       conf.set(SHUFFLE_SERVICE_ENABLED.key, \"true\")",
          "2201:       conf.set(SHUFFLE_SERVICE_PORT.key, shufflePort.toString)",
          "2202:       conf.set(SHUFFLE_REGISTRATION_TIMEOUT.key, \"40\")",
          "2203:       conf.set(SHUFFLE_REGISTRATION_MAX_ATTEMPTS.key, \"1\")",
          "2204:       val e = intercept[SparkException] {",
          "2205:         makeBlockManager(8000, \"timeoutExec\")",
          "2206:       }.getMessage",
          "2207:       assert(e.contains(\"TimeoutException\"))",
          "2208:       verify(master, times(0))",
          "2209:         .registerBlockManager(mc.any(), mc.any(), mc.any(), mc.any(), mc.any())",
          "2210:       server.close()",
          "2211:     }",
          "2212:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e1c5f90c700d844aa56c211e53eb75d0aa99b9ad",
      "candidate_info": {
        "commit_hash": "e1c5f90c700d844aa56c211e53eb75d0aa99b9ad",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/e1c5f90c700d844aa56c211e53eb75d0aa99b9ad",
        "files": [
          "python/pyspark/ml/classification.py"
        ],
        "message": "[SPARK-40132][ML] Restore rawPredictionCol to MultilayerPerceptronClassifier.setParams\n\n### What changes were proposed in this pull request?\n\nRestore rawPredictionCol to MultilayerPerceptronClassifier.setParams\n\n### Why are the changes needed?\n\nThis param was inadvertently removed in the refactoring in https://github.com/apache/spark/commit/40cdb6d51c2befcfeac8fb5cf5faf178d1a5ee7b#r81473316\nWithout it, using this param in the constructor fails.\n\n### Does this PR introduce _any_ user-facing change?\n\nNot aside from the bug fix.\n\n### How was this patch tested?\n\nExisting tests.\n\nCloses #37561 from srowen/SPARK-40132.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n(cherry picked from commit 6768d9cc38a320f7e1c6781afcd170577c5c7d0f)\nSigned-off-by: Sean Owen <srowen@gmail.com>",
        "before_after_code_files": [
          "python/pyspark/ml/classification.py||python/pyspark/ml/classification.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/ml/classification.py||python/pyspark/ml/classification.py": [
          "File: python/pyspark/ml/classification.py -> python/pyspark/ml/classification.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "3230:         solver: str = \"l-bfgs\",",
          "3231:         initialWeights: Optional[Vector] = None,",
          "3232:         probabilityCol: str = \"probability\",",
          "3233:     ) -> \"MultilayerPerceptronClassifier\":",
          "3234:         \"\"\"",
          "3235:         setParams(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3233:         rawPredictionCol: str = \"rawPrediction\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "9cc2ae7804156899850031bd694b1925473fb4cd",
      "candidate_info": {
        "commit_hash": "9cc2ae7804156899850031bd694b1925473fb4cd",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/9cc2ae7804156899850031bd694b1925473fb4cd",
        "files": [
          "core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala"
        ],
        "message": "[SPARK-38992][CORE] Avoid using bash -c in ShellBasedGroupsMappingProvider\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to avoid using `bash -c` in `ShellBasedGroupsMappingProvider`. This could allow users a command injection.\n\n### Why are the changes needed?\n\nFor a security purpose.\n\n### Does this PR introduce _any_ user-facing change?\n\nVirtually no.\n\n### How was this patch tested?\n\nManually tested.\n\nCloses #36315 from HyukjinKwon/SPARK-38992.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit c83618e4e5fc092829a1f2a726f12fb832e802cc)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala||core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala||core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala": [
          "File: core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala -> core/src/main/scala/org/apache/spark/security/ShellBasedGroupsMappingProvider.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: private[spark] class ShellBasedGroupsMappingProvider extends GroupMappingServiceProvider",
          "31:   with Logging {",
          "33:   override def getGroups(username: String): Set[String] = {",
          "34:     val userGroups = getUnixGroups(username)",
          "35:     logDebug(\"User: \" + username + \" Groups: \" + userGroups.mkString(\",\"))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33:   private lazy val idPath = Utils.executeAndGetOutput(\"which\" :: \"id\" :: Nil).stripLineEnd",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "40:   private def getUnixGroups(username: String): Set[String] = {",
          "44:   }",
          "45: }",
          "",
          "[Removed Lines]",
          "41:     val cmdSeq = Seq(\"bash\", \"-c\", \"id -Gn \" + username)",
          "43:     Utils.executeAndGetOutput(cmdSeq).stripLineEnd.split(\" \").toSet",
          "",
          "[Added Lines]",
          "44:     Utils.executeAndGetOutput(idPath ::  \"-Gn\" :: username :: Nil).stripLineEnd.split(\" \").toSet",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d7af1d20f06412f80798c53d8588356ee1490afe",
      "candidate_info": {
        "commit_hash": "d7af1d20f06412f80798c53d8588356ee1490afe",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/d7af1d20f06412f80798c53d8588356ee1490afe",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/util/SQLOpenHashSet.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala"
        ],
        "message": "[SPARK-39976][SQL] ArrayIntersect should handle null in left expression correctly\n\n### What changes were proposed in this pull request?\n`ArrayInterscet` miss judge if null contains in right expression's hash set.\n\n```\n>>> a = [1, 2, 3]\n>>> b = [3, None, 5]\n>>> df = spark.sparkContext.parallelize(data).toDF([\"a\",\"b\"])\n>>> df.show()\n+---------+------------+\n|        a|           b|\n+---------+------------+\n|[1, 2, 3]|[3, null, 5]|\n+---------+------------+\n\n>>> df.selectExpr(\"array_intersect(a,b)\").show()\n+---------------------+\n|array_intersect(a, b)|\n+---------------------+\n|                  [3]|\n+---------------------+\n\n>>> df.selectExpr(\"array_intersect(b,a)\").show()\n+---------------------+\n|array_intersect(b, a)|\n+---------------------+\n|            [3, null]|\n+---------------------+\n```\n\nIn origin code gen's code path, when handle `ArrayIntersect`'s array1, it use the below code\n```\n        def withArray1NullAssignment(body: String) =\n          if (left.dataType.asInstanceOf[ArrayType].containsNull) {\n            if (right.dataType.asInstanceOf[ArrayType].containsNull) {\n              s\"\"\"\n                 |if ($array1.isNullAt($i)) {\n                 |  if ($foundNullElement) {\n                 |    $nullElementIndex = $size;\n                 |    $foundNullElement = false;\n                 |    $size++;\n                 |    $builder.$$plus$$eq($nullValueHolder);\n                 |  }\n                 |} else {\n                 |  $body\n                 |}\n               \"\"\".stripMargin\n            } else {\n              s\"\"\"\n                 |if (!$array1.isNullAt($i)) {\n                 |  $body\n                 |}\n               \"\"\".stripMargin\n            }\n          } else {\n            body\n          }\n```\nWe have a flag `foundNullElement` to indicate if array2 really contains a null value. But when implement https://issues.apache.org/jira/browse/SPARK-36829, misunderstand the meaning of `ArrayType.containsNull`,\nso when implement  `SQLOpenHashSet.withNullCheckCode()`\n```\n  def withNullCheckCode(\n      arrayContainsNull: Boolean,\n      setContainsNull: Boolean,\n      array: String,\n      index: String,\n      hashSet: String,\n      handleNotNull: (String, String) => String,\n      handleNull: String): String = {\n    if (arrayContainsNull) {\n      if (setContainsNull) {\n        s\"\"\"\n           |if ($array.isNullAt($index)) {\n           |  if (!$hashSet.containsNull()) {\n           |    $hashSet.addNull();\n           |    $handleNull\n           |  }\n           |} else {\n           |  ${handleNotNull(array, index)}\n           |}\n         \"\"\".stripMargin\n      } else {\n        s\"\"\"\n           |if (!$array.isNullAt($index)) {\n           | ${handleNotNull(array, index)}\n           |}\n         \"\"\".stripMargin\n      }\n    } else {\n      handleNotNull(array, index)\n    }\n  }\n```\nThe code path of `  if (arrayContainsNull && setContainsNull) `  is misinterpreted that array's openHashSet really have a null value.\n\nIn this pr we add a new parameter `additionalCondition ` to complements the previous implementation of `foundNullElement`. Also refactor the method's parameter name.\n\n### Why are the changes needed?\nFix data correct issue\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nAdded UT\n\nCloses #37436 from AngersZhuuuu/SPARK-39776-FOLLOW_UP.\n\nLead-authored-by: Angerszhuuuu <angers.zhu@gmail.com>\nCo-authored-by: AngersZhuuuu <angers.zhu@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit dff5c2f2e9ce233e270e0e5cde0a40f682ba9534)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/util/SQLOpenHashSet.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/util/SQLOpenHashSet.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "4155:           right.dataType.asInstanceOf[ArrayType].containsNull,",
          "4156:           array1, i, hashSetResult, withArray1NaNCheckCodeGenerator,",
          "4157:           s\"\"\"",
          "4161:            \"\"\".stripMargin)",
          "",
          "[Removed Lines]",
          "4158:              |$nullElementIndex = $size;",
          "4159:              |$size++;",
          "4160:              |$builder.$$plus$$eq($nullValueHolder);",
          "",
          "[Added Lines]",
          "4158:              |if ($hashSet.containsNull()) {",
          "4159:              |  $nullElementIndex = $size;",
          "4160:              |  $size++;",
          "4161:              |  $builder.$$plus$$eq($nullValueHolder);",
          "4162:              |}",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/util/SQLOpenHashSet.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/util/SQLOpenHashSet.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/util/SQLOpenHashSet.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/util/SQLOpenHashSet.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "79:   }",
          "81:   def withNullCheckCode(",
          "84:       array: String,",
          "85:       index: String,",
          "86:       hashSet: String,",
          "87:       handleNotNull: (String, String) => String,",
          "88:       handleNull: String): String = {",
          "91:         s\"\"\"",
          "92:            |if ($array.isNullAt($index)) {",
          "93:            |  if (!$hashSet.containsNull()) {",
          "",
          "[Removed Lines]",
          "82:       arrayContainsNull: Boolean,",
          "83:       setContainsNull: Boolean,",
          "89:     if (arrayContainsNull) {",
          "90:       if (setContainsNull) {",
          "",
          "[Added Lines]",
          "82:       array1ElementNullable: Boolean,",
          "83:       array2ElementNullable: Boolean,",
          "89:     if (array1ElementNullable) {",
          "90:       if (array2ElementNullable) {",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CollectionExpressionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2163:     checkEvaluation(ArrayExcept(empty, oneNull), Seq.empty)",
          "2164:     checkEvaluation(ArrayExcept(oneNull, empty), Seq(null))",
          "2165:     checkEvaluation(ArrayExcept(twoNulls, empty), Seq(null))",
          "2166:   }",
          "2168:   test(\"Array Intersect\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2167:     checkEvaluation(ArrayExcept(",
          "2168:       Literal.create(Seq(1d, 2d, null), ArrayType(DoubleType)),",
          "2169:       Literal.create(Seq(1d), ArrayType(DoubleType))),",
          "2170:       Seq(2d, null))",
          "2171:     checkEvaluation(ArrayExcept(",
          "2172:       Literal.create(Seq(1d, 2d, null), ArrayType(DoubleType)),",
          "2173:       Literal.create(Seq(1d), ArrayType(DoubleType, false))),",
          "2174:       Seq(2d, null))",
          "2175:     checkEvaluation(ArrayExcept(",
          "2176:       Literal.create(Seq(1d, 2d), ArrayType(DoubleType)),",
          "2177:       Literal.create(Seq(1d, null), ArrayType(DoubleType))),",
          "2178:       Seq(2d))",
          "2179:     checkEvaluation(ArrayExcept(",
          "2180:       Literal.create(Seq(1d, 2d), ArrayType(DoubleType, false)),",
          "2181:       Literal.create(Seq(1d, null), ArrayType(DoubleType))),",
          "2182:       Seq(2d))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2288:     checkEvaluation(ArrayIntersect(oneNull, twoNulls), Seq(null))",
          "2289:     checkEvaluation(ArrayIntersect(empty, oneNull), Seq.empty)",
          "2290:     checkEvaluation(ArrayIntersect(oneNull, empty), Seq.empty)",
          "2291:   }",
          "2293:   test(\"SPARK-31980: Start and end equal in month range\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2309:     checkEvaluation(ArrayIntersect(",
          "2310:       Literal.create(Seq(1d, 2d, null), ArrayType(DoubleType)),",
          "2311:       Literal.create(Seq(1d), ArrayType(DoubleType))),",
          "2312:       Seq(1d))",
          "2313:     checkEvaluation(ArrayIntersect(",
          "2314:       Literal.create(Seq(1d, 2d, null), ArrayType(DoubleType)),",
          "2315:       Literal.create(Seq(1d), ArrayType(DoubleType, false))),",
          "2316:       Seq(1d))",
          "2317:     checkEvaluation(ArrayIntersect(",
          "2318:       Literal.create(Seq(1d, 2d), ArrayType(DoubleType)),",
          "2319:       Literal.create(Seq(1d, null), ArrayType(DoubleType))),",
          "2320:       Seq(1d))",
          "2321:     checkEvaluation(ArrayIntersect(",
          "2322:       Literal.create(Seq(1d, 2d), ArrayType(DoubleType, false)),",
          "2323:       Literal.create(Seq(1d, null), ArrayType(DoubleType))),",
          "2324:       Seq(1d))",
          "",
          "---------------"
        ]
      }
    }
  ]
}