{
  "cve_id": "CVE-2024-1892",
  "cve_desc": "A Regular Expression Denial of Service (ReDoS) vulnerability exists in the XMLFeedSpider class of the scrapy/scrapy project, specifically in the parsing of XML content. By crafting malicious XML content that exploits inefficient regular expression complexity used in the parsing process, an attacker can cause a denial-of-service (DoS) condition. This vulnerability allows for the system to hang and consume significant resources, potentially rendering services that utilize Scrapy for XML processing unresponsive.",
  "repo": "scrapy/scrapy",
  "patch_hash": "479619b340f197a8f24c5db45bc068fb8755f2c5",
  "patch_info": {
    "commit_hash": "479619b340f197a8f24c5db45bc068fb8755f2c5",
    "repo": "scrapy/scrapy",
    "commit_url": "https://github.com/scrapy/scrapy/commit/479619b340f197a8f24c5db45bc068fb8755f2c5",
    "files": [
      "docs/faq.rst",
      "docs/news.rst",
      "docs/topics/debug.rst",
      "scrapy/spiders/feed.py",
      "scrapy/utils/iterators.py",
      "scrapy/utils/response.py",
      "tests/test_spider.py",
      "tests/test_utils_iterators.py",
      "tests/test_utils_response.py"
    ],
    "message": "Merge branch '2.11-redos' into 2.11",
    "before_after_code_files": [
      "scrapy/spiders/feed.py||scrapy/spiders/feed.py",
      "scrapy/utils/iterators.py||scrapy/utils/iterators.py",
      "scrapy/utils/response.py||scrapy/utils/response.py",
      "tests/test_spider.py||tests/test_spider.py",
      "tests/test_utils_iterators.py||tests/test_utils_iterators.py",
      "tests/test_utils_response.py||tests/test_utils_response.py"
    ]
  },
  "patch_diff": {
    "scrapy/spiders/feed.py||scrapy/spiders/feed.py": [
      "File: scrapy/spiders/feed.py -> scrapy/spiders/feed.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "7: from scrapy.exceptions import NotConfigured, NotSupported",
      "8: from scrapy.selector import Selector",
      "9: from scrapy.spiders import Spider",
      "11: from scrapy.utils.spider import iterate_spider_output",
      "",
      "[Removed Lines]",
      "10: from scrapy.utils.iterators import csviter, xmliter",
      "",
      "[Added Lines]",
      "10: from scrapy.utils.iterators import csviter, xmliter_lxml",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "84:         return self.parse_nodes(response, nodes)",
      "86:     def _iternodes(self, response):",
      "88:             self._register_namespaces(node)",
      "89:             yield node",
      "",
      "[Removed Lines]",
      "87:         for node in xmliter(response, self.itertag):",
      "",
      "[Added Lines]",
      "87:         for node in xmliter_lxml(response, self.itertag):",
      "",
      "---------------"
    ],
    "scrapy/utils/iterators.py||scrapy/utils/iterators.py": [
      "File: scrapy/utils/iterators.py -> scrapy/utils/iterators.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "16:     cast,",
      "17:     overload,",
      "18: )",
      "20: from scrapy.http import Response, TextResponse",
      "21: from scrapy.selector import Selector",
      "22: from scrapy.utils.python import re_rsearch, to_unicode",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "19: from warnings import warn",
      "21: from lxml import etree",
      "23: from scrapy.exceptions import ScrapyDeprecationWarning",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "38:     - a unicode string",
      "39:     - a string encoded as utf-8",
      "40:     \"\"\"",
      "41:     nodename_patt = re.escape(nodename)",
      "43:     DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]+>\\s*\", re.S)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "45:     warn(",
      "46:         (",
      "47:             \"xmliter is deprecated and its use strongly discouraged because \"",
      "48:             \"it is vulnerable to ReDoS attacks. Use xmliter_lxml instead. See \"",
      "49:             \"https://github.com/scrapy/scrapy/security/advisories/GHSA-cc65-xxvf-f7r9\"",
      "50:         ),",
      "51:         ScrapyDeprecationWarning,",
      "52:         stacklevel=2,",
      "53:     )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "81:     namespace: Optional[str] = None,",
      "82:     prefix: str = \"x\",",
      "83: ) -> Generator[Selector, Any, None]:",
      "86:     reader = _StreamReader(obj)",
      "87:     tag = f\"{{{namespace}}}{nodename}\" if namespace else nodename",
      "88:     iterable = etree.iterparse(",
      "90:     )",
      "91:     selxpath = \"//\" + (f\"{prefix}:{nodename}\" if namespace else nodename)",
      "93:         nodetext = etree.tostring(node, encoding=\"unicode\")",
      "94:         node.clear()",
      "95:         xs = Selector(text=nodetext, type=\"xml\")",
      "",
      "[Removed Lines]",
      "84:     from lxml import etree",
      "89:         cast(\"SupportsReadClose[bytes]\", reader), tag=tag, encoding=reader.encoding",
      "92:     for _, node in iterable:",
      "",
      "[Added Lines]",
      "101:         cast(\"SupportsReadClose[bytes]\", reader),",
      "102:         encoding=reader.encoding,",
      "103:         events=(\"end\", \"start-ns\"),",
      "104:         huge_tree=True,",
      "107:     needs_namespace_resolution = not namespace and \":\" in nodename",
      "108:     if needs_namespace_resolution:",
      "109:         prefix, nodename = nodename.split(\":\", maxsplit=1)",
      "110:     for event, data in iterable:",
      "111:         if event == \"start-ns\":",
      "112:             assert isinstance(data, tuple)",
      "113:             if needs_namespace_resolution:",
      "114:                 _prefix, _namespace = data",
      "115:                 if _prefix != prefix:",
      "116:                     continue",
      "117:                 namespace = _namespace",
      "118:                 needs_namespace_resolution = False",
      "119:                 selxpath = f\"//{prefix}:{nodename}\"",
      "120:                 tag = f\"{{{namespace}}}{nodename}\"",
      "121:             continue",
      "122:         assert isinstance(data, etree._Element)",
      "123:         node = data",
      "124:         if node.tag != tag:",
      "125:             continue",
      "",
      "---------------"
    ],
    "scrapy/utils/response.py||scrapy/utils/response.py": [
      "File: scrapy/utils/response.py -> scrapy/utils/response.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "74:     return b\"\".join(values)",
      "77: def open_in_browser(",
      "78:     response: Union[",
      "79:         \"scrapy.http.response.html.HtmlResponse\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "77: def _remove_html_comments(body):",
      "78:     start = body.find(b\"<!--\")",
      "79:     while start != -1:",
      "80:         end = body.find(b\"-->\", start + 1)",
      "81:         if end == -1:",
      "82:             return body[:start]",
      "83:         else:",
      "84:             body = body[:start] + body[end + 3 :]",
      "85:             start = body.find(b\"<!--\")",
      "86:     return body",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "81:     ],",
      "82:     _openfunc: Callable[[str], Any] = webbrowser.open,",
      "83: ) -> Any:",
      "86:     \"\"\"",
      "87:     from scrapy.http import HtmlResponse, TextResponse",
      "",
      "[Removed Lines]",
      "84:     \"\"\"Open the given response in a local web browser, populating the <base>",
      "85:     tag for external links to work",
      "",
      "[Added Lines]",
      "96:     \"\"\"Open *response* in a local web browser, adjusting the `base tag`_ for",
      "97:     external links to work, e.g. so that images and styles are displayed.",
      "99:     .. _base tag: https://www.w3schools.com/tags/tag_base.asp",
      "101:     For example:",
      "103:     .. code-block:: python",
      "105:         from scrapy.utils.response import open_in_browser",
      "108:         def parse_details(self, response):",
      "109:             if \"item name\" not in response.body:",
      "110:                 open_in_browser(response)",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "90:     body = response.body",
      "91:     if isinstance(response, HtmlResponse):",
      "92:         if b\"<base\" not in body:",
      "96:         ext = \".html\"",
      "97:     elif isinstance(response, TextResponse):",
      "98:         ext = \".txt\"",
      "",
      "[Removed Lines]",
      "93:             repl = rf'\\1<base href=\"{response.url}\">'",
      "94:             body = re.sub(b\"<!--.*?-->\", b\"\", body, flags=re.DOTALL)",
      "95:             body = re.sub(rb\"(<head(?:>|\\s.*?>))\", to_bytes(repl), body)",
      "",
      "[Added Lines]",
      "118:             _remove_html_comments(body)",
      "119:             repl = rf'\\0<base href=\"{response.url}\">'",
      "120:             body = re.sub(rb\"<head(?:[^<>]*?>)\", to_bytes(repl), body, count=1)",
      "",
      "---------------"
    ],
    "tests/test_spider.py||tests/test_spider.py": [
      "File: tests/test_spider.py -> tests/test_spider.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "151:         body = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>",
      "152:         <urlset xmlns:x=\"http://www.google.com/schemas/sitemap/0.84\"",
      "153:                 xmlns:y=\"http://www.example.com/schemas/extras/1.0\">",
      "155:             <other value=\"bar\" y:custom=\"fuu\"/>",
      "156:         </url>",
      "158:         </urlset>\"\"\"",
      "159:         response = XmlResponse(url=\"http://example.com/sitemap.xml\", body=body)",
      "",
      "[Removed Lines]",
      "154:         <url><x:loc>http://www.example.com/Special-Offers.html</loc><y:updated>2009-08-16</updated>",
      "157:         <url><loc>http://www.example.com/</loc><y:updated>2009-08-16</updated><other value=\"foo\"/></url>",
      "",
      "[Added Lines]",
      "154:         <url><x:loc>http://www.example.com/Special-Offers.html</x:loc><y:updated>2009-08-16</y:updated>",
      "157:         <url><loc>http://www.example.com/</loc><y:updated>2009-08-16</y:updated><other value=\"foo\"/></url>",
      "",
      "---------------"
    ],
    "tests/test_utils_iterators.py||tests/test_utils_iterators.py": [
      "File: tests/test_utils_iterators.py -> tests/test_utils_iterators.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "2: from twisted.trial import unittest",
      "4: from scrapy.http import Response, TextResponse, XmlResponse",
      "5: from scrapy.utils.iterators import _body_or_str, csviter, xmliter, xmliter_lxml",
      "6: from tests import get_testdata",
      "12:     def test_xmliter(self):",
      "13:         body = b\"\"\"",
      "14:             <?xml version=\"1.0\" encoding=\"UTF-8\"?>",
      "",
      "[Removed Lines]",
      "1: from pytest import mark",
      "9: class XmliterTestCase(unittest.TestCase):",
      "10:     xmliter = staticmethod(xmliter)",
      "",
      "[Added Lines]",
      "1: import pytest",
      "4: from scrapy.exceptions import ScrapyDeprecationWarning",
      "10: class XmliterBaseTestCase:",
      "11:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "40:             attrs, [(\"001\", [\"Name 1\"], [\"Type 1\"]), (\"002\", [\"Name 2\"], [\"Type 2\"])]",
      "41:         )",
      "43:     def test_xmliter_unusual_node(self):",
      "44:         body = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>",
      "45:             <root>",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "43:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "53:         ]",
      "54:         self.assertEqual(nodenames, [[\"matchme...\"]])",
      "56:     def test_xmliter_unicode(self):",
      "57:         # example taken from https://github.com/scrapy/scrapy/issues/1665",
      "58:         body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "57:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "112:                 [(\"26\", [\"-\"], [\"80\"]), (\"21\", [\"Ab\"], [\"76\"]), (\"27\", [\"A\"], [\"27\"])],",
      "113:             )",
      "115:     def test_xmliter_text(self):",
      "116:         body = (",
      "117:             '<?xml version=\"1.0\" encoding=\"UTF-8\"?>'",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "117:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "123:             [[\"one\"], [\"two\"]],",
      "124:         )",
      "126:     def test_xmliter_namespaces(self):",
      "127:         body = b\"\"\"",
      "128:             <?xml version=\"1.0\" encoding=\"UTF-8\"?>",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "129:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "162:         self.assertEqual(node.xpath(\"id/text()\").getall(), [])",
      "163:         self.assertEqual(node.xpath(\"price/text()\").getall(), [])",
      "165:     def test_xmliter_namespaced_nodename(self):",
      "166:         body = b\"\"\"",
      "167:             <?xml version=\"1.0\" encoding=\"UTF-8\"?>",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "169:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
      "",
      "---------------",
      "--- Hunk 7 ---",
      "[Context before]",
      "190:             [\"http://www.mydummycompany.com/images/item1.jpg\"],",
      "191:         )",
      "193:     def test_xmliter_namespaced_nodename_missing(self):",
      "194:         body = b\"\"\"",
      "195:             <?xml version=\"1.0\" encoding=\"UTF-8\"?>",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "198:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
      "",
      "---------------",
      "--- Hunk 8 ---",
      "[Context before]",
      "214:         with self.assertRaises(StopIteration):",
      "215:             next(my_iter)",
      "217:     def test_xmliter_exception(self):",
      "218:         body = (",
      "219:             '<?xml version=\"1.0\" encoding=\"UTF-8\"?>'",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "223:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
      "",
      "---------------",
      "--- Hunk 9 ---",
      "[Context before]",
      "227:         self.assertRaises(StopIteration, next, iter)",
      "229:     def test_xmliter_objtype_exception(self):",
      "230:         i = self.xmliter(42, \"product\")",
      "231:         self.assertRaises(TypeError, next, i)",
      "233:     def test_xmliter_encoding(self):",
      "234:         body = (",
      "235:             b'<?xml version=\"1.0\" encoding=\"ISO-8859-9\"?>\\n'",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "236:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
      "241:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
      "",
      "---------------",
      "--- Hunk 10 ---",
      "[Context before]",
      "244:         )",
      "254:     def test_xmliter_iterate_namespace(self):",
      "255:         body = b\"\"\"",
      "",
      "[Removed Lines]",
      "247: class LxmlXmliterTestCase(XmliterTestCase):",
      "248:     xmliter = staticmethod(xmliter_lxml)",
      "250:     @mark.xfail(reason=\"known bug of the current implementation\")",
      "251:     def test_xmliter_namespaced_nodename(self):",
      "252:         super().test_xmliter_namespaced_nodename()",
      "",
      "[Added Lines]",
      "256: class XmliterTestCase(XmliterBaseTestCase, unittest.TestCase):",
      "257:     xmliter = staticmethod(xmliter)",
      "259:     def test_deprecation(self):",
      "260:         body = b\"\"\"",
      "261:             <?xml version=\"1.0\" encoding=\"UTF-8\"?>",
      "262:             <products>",
      "263:               <product></product>",
      "264:             </products>",
      "265:         \"\"\"",
      "266:         with pytest.warns(",
      "267:             ScrapyDeprecationWarning,",
      "268:             match=\"xmliter\",",
      "269:         ):",
      "270:             next(self.xmliter(body, \"product\"))",
      "273: class LxmlXmliterTestCase(XmliterBaseTestCase, unittest.TestCase):",
      "274:     xmliter = staticmethod(xmliter_lxml)",
      "",
      "---------------"
    ],
    "tests/test_utils_response.py||tests/test_utils_response.py": [
      "File: tests/test_utils_response.py -> tests/test_utils_response.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: import unittest",
      "2: import warnings",
      "3: from pathlib import Path",
      "4: from urllib.parse import urlparse",
      "6: from scrapy.exceptions import ScrapyDeprecationWarning",
      "7: from scrapy.http import HtmlResponse, Response, TextResponse",
      "8: from scrapy.utils.python import to_bytes",
      "9: from scrapy.utils.response import (",
      "10:     get_base_url,",
      "11:     get_meta_refresh,",
      "12:     open_in_browser,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "4: from time import process_time",
      "7: import pytest",
      "13:     _remove_html_comments,",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "198:         assert open_in_browser(",
      "199:             r5, _openfunc=check_base_url",
      "200:         ), \"Inject unique base url with conditional comment\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "206:     def test_open_in_browser_redos_comment(self):",
      "207:         MAX_CPU_TIME = 0.001",
      "209:         # Exploit input from",
      "210:         # https://makenowjust-labs.github.io/recheck/playground/",
      "211:         # for /<!--.*?-->/ (old pattern to remove comments).",
      "212:         body = b\"-><!--\\x00\" * 25_000 + b\"->\\n<!---->\"",
      "214:         response = HtmlResponse(\"https://example.com\", body=body)",
      "216:         start_time = process_time()",
      "218:         open_in_browser(response, lambda url: True)",
      "220:         end_time = process_time()",
      "221:         self.assertLess(end_time - start_time, MAX_CPU_TIME)",
      "223:     def test_open_in_browser_redos_head(self):",
      "224:         MAX_CPU_TIME = 0.001",
      "226:         # Exploit input from",
      "227:         # https://makenowjust-labs.github.io/recheck/playground/",
      "228:         # for /(<head(?:>|\\s.*?>))/ (old pattern to find the head element).",
      "229:         body = b\"<head\\t\" * 8_000",
      "231:         response = HtmlResponse(\"https://example.com\", body=body)",
      "233:         start_time = process_time()",
      "235:         open_in_browser(response, lambda url: True)",
      "237:         end_time = process_time()",
      "238:         self.assertLess(end_time - start_time, MAX_CPU_TIME)",
      "241: @pytest.mark.parametrize(",
      "242:     \"input_body,output_body\",",
      "243:     (",
      "244:         (",
      "245:             b\"a<!--\",",
      "246:             b\"a\",",
      "247:         ),",
      "248:         (",
      "249:             b\"a<!---->b\",",
      "250:             b\"ab\",",
      "251:         ),",
      "252:         (",
      "253:             b\"a<!--b-->c\",",
      "254:             b\"ac\",",
      "255:         ),",
      "256:         (",
      "257:             b\"a<!--b-->c<!--\",",
      "258:             b\"ac\",",
      "259:         ),",
      "260:         (",
      "261:             b\"a<!--b-->c<!--d\",",
      "262:             b\"ac\",",
      "263:         ),",
      "264:         (",
      "265:             b\"a<!--b-->c<!---->d\",",
      "266:             b\"acd\",",
      "267:         ),",
      "268:         (",
      "269:             b\"a<!--b--><!--c-->d\",",
      "270:             b\"ad\",",
      "271:         ),",
      "272:     ),",
      "273: )",
      "274: def test_remove_html_comments(input_body, output_body):",
      "275:     assert (",
      "276:         _remove_html_comments(input_body) == output_body",
      "277:     ), f\"{_remove_html_comments(input_body)=} == {output_body=}\"",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "03d9866518ab43844ba0309394529240f4cf115e",
      "candidate_info": {
        "commit_hash": "03d9866518ab43844ba0309394529240f4cf115e",
        "repo": "scrapy/scrapy",
        "commit_url": "https://github.com/scrapy/scrapy/commit/03d9866518ab43844ba0309394529240f4cf115e",
        "files": [
          "scrapy/downloadermiddlewares/httpcompression.py",
          "scrapy/spiders/sitemap.py",
          "scrapy/utils/_compression.py",
          "scrapy/utils/gz.py",
          "tests/test_downloadermiddleware_httpcompression.py",
          "tests/test_spider.py"
        ],
        "message": "Also use DOWNLOAD_WARNSIZE for decompressions",
        "before_after_code_files": [
          "scrapy/downloadermiddlewares/httpcompression.py||scrapy/downloadermiddlewares/httpcompression.py",
          "scrapy/spiders/sitemap.py||scrapy/spiders/sitemap.py",
          "scrapy/utils/_compression.py||scrapy/utils/_compression.py",
          "scrapy/utils/gz.py||scrapy/utils/gz.py",
          "tests/test_downloadermiddleware_httpcompression.py||tests/test_downloadermiddleware_httpcompression.py",
          "tests/test_spider.py||tests/test_spider.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/scrapy/scrapy/pull/6222"
        ],
        "olp_code_files": {
          "patch": [
            "tests/test_spider.py||tests/test_spider.py"
          ],
          "candidate": [
            "tests/test_spider.py||tests/test_spider.py"
          ]
        }
      },
      "candidate_diff": {
        "scrapy/downloadermiddlewares/httpcompression.py||scrapy/downloadermiddlewares/httpcompression.py": [
          "File: scrapy/downloadermiddlewares/httpcompression.py -> scrapy/downloadermiddlewares/httpcompression.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: import warnings",
          "3: from scrapy import signals",
          "4: from scrapy.exceptions import IgnoreRequest, NotConfigured",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2: from logging import getLogger",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "13: from scrapy.utils.deprecate import ScrapyDeprecationWarning",
          "14: from scrapy.utils.gz import gunzip",
          "16: ACCEPTED_ENCODINGS = [b\"gzip\", b\"deflate\"]",
          "18: try:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "17: logger = getLogger(__name__)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "39:             return",
          "40:         self.stats = crawler.stats",
          "41:         self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")",
          "42:         crawler.signals.connect(self.open_spider, signals.spider_opened)",
          "44:     @classmethod",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "45:         self._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "57:             spider = cls()",
          "58:             spider.stats = crawler.stats",
          "59:             spider._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")",
          "60:             crawler.signals.connect(spider.open_spider, signals.spider_opened)",
          "61:             return spider",
          "63:     def open_spider(self, spider):",
          "64:         if hasattr(spider, \"download_maxsize\"):",
          "65:             self._max_size = spider.download_maxsize",
          "67:     def process_request(self, request, spider):",
          "68:         request.headers.setdefault(\"Accept-Encoding\", b\", \".join(ACCEPTED_ENCODINGS))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "64:             spider._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")",
          "71:         if hasattr(spider, \"download_warnsize\"):",
          "72:             self._warn_size = spider.download_warnsize",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "75:             if content_encoding:",
          "76:                 encoding = content_encoding.pop()",
          "77:                 max_size = request.meta.get(\"download_maxsize\", self._max_size)",
          "78:                 try:",
          "79:                     decoded_body = self._decode(",
          "80:                         response.body, encoding.lower(), max_size",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "85:                 warn_size = request.meta.get(\"download_warnsize\", self._warn_size)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "82:                 except _DecompressionMaxSizeExceeded:",
          "83:                     raise IgnoreRequest(",
          "84:                         f\"Ignored response {response} because its body \"",
          "87:                     )",
          "88:                 if self.stats:",
          "89:                     self.stats.inc_value(",
          "",
          "[Removed Lines]",
          "85:                         f\"({len(response.body)}B) exceeded DOWNLOAD_MAXSIZE \"",
          "86:                         f\"({self._max_size}B) during decompression.\"",
          "",
          "[Added Lines]",
          "93:                         f\"({len(response.body)} B) exceeded DOWNLOAD_MAXSIZE \"",
          "94:                         f\"({self._max_size} B) during decompression.\"",
          "95:                     )",
          "96:                 if len(response.body) < warn_size and len(decoded_body) >= warn_size:",
          "97:                     logger.warning(",
          "98:                         f\"{response} body size after decompression \"",
          "99:                         f\"({len(decoded_body)} B) is larger than the \"",
          "100:                         f\"download warning size ({warn_size} B).\"",
          "",
          "---------------"
        ],
        "scrapy/spiders/sitemap.py||scrapy/spiders/sitemap.py": [
          "File: scrapy/spiders/sitemap.py -> scrapy/spiders/sitemap.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: import logging",
          "2: import re",
          "4: from scrapy.http import Request, XmlResponse",
          "5: from scrapy.spiders import Spider",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3: from typing import TYPE_CHECKING, Any",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "7: from scrapy.utils.gz import gunzip, gzip_magic_number",
          "8: from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots",
          "10: logger = logging.getLogger(__name__)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "11: if TYPE_CHECKING:",
          "12:     # typing.Self requires Python 3.11",
          "13:     from typing_extensions import Self",
          "15:     from scrapy.crawler import Crawler",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "16:     sitemap_follow = [\"\"]",
          "17:     sitemap_alternate_links = False",
          "19:     def __init__(self, *a, **kw):",
          "20:         super().__init__(*a, **kw)",
          "21:         self._cbs = []",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26:     @classmethod",
          "27:     def from_crawler(cls, crawler: \"Crawler\", *args: Any, **kwargs: Any) -> \"Self\":",
          "28:         spider = super().from_crawler(crawler, *args, **kwargs)",
          "29:         spider._max_size = getattr(",
          "30:             spider, \"download_maxsize\", spider.settings.getint(\"DOWNLOAD_MAXSIZE\")",
          "31:         )",
          "32:         spider._warn_size = getattr(",
          "33:             spider, \"download_warnsize\", spider.settings.getint(\"DOWNLOAD_WARNSIZE\")",
          "34:         )",
          "35:         return spider",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "72:         if isinstance(response, XmlResponse):",
          "73:             return response.body",
          "74:         if gzip_magic_number(response):",
          "81:             try:",
          "83:             except _DecompressionMaxSizeExceeded:",
          "84:                 return None",
          "85:         # actual gzipped sitemap files are decompressed above ;",
          "86:         # if we are here (response body is not gzipped)",
          "87:         # and have a response for .xml.gz,",
          "",
          "[Removed Lines]",
          "75:             max_size = response.meta.get(",
          "76:                 \"download_maxsize\",",
          "77:                 getattr(",
          "78:                     self, \"download_maxsize\", self.settings.getint(\"DOWNLOAD_MAXSIZE\")",
          "79:                 ),",
          "80:             )",
          "82:                 return gunzip(response.body, max_size=max_size)",
          "",
          "[Added Lines]",
          "93:             uncompressed_size = len(response.body)",
          "94:             max_size = response.meta.get(\"download_maxsize\", self._max_size)",
          "95:             warn_size = response.meta.get(\"download_warnsize\", self._warn_size)",
          "97:                 body = gunzip(response.body, max_size=max_size)",
          "100:             if uncompressed_size < warn_size and len(body) >= warn_size:",
          "101:                 logger.warning(",
          "102:                     f\"{response} body size after decompression ({len(body)} B) \"",
          "103:                     f\"is larger than the download warning size ({warn_size} B).\"",
          "104:                 )",
          "105:             return body",
          "",
          "---------------"
        ],
        "scrapy/utils/_compression.py||scrapy/utils/_compression.py": [
          "File: scrapy/utils/_compression.py -> scrapy/utils/_compression.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:         if max_size and decompressed_size > max_size:",
          "45:             raise _DecompressionMaxSizeExceeded(",
          "46:                 f\"The number of bytes decompressed so far \"",
          "49:             )",
          "50:         output_list.append(output_chunk)",
          "51:     return b\"\".join(output_list)",
          "",
          "[Removed Lines]",
          "47:                 f\"({decompressed_size}B) exceed the specified maximum \"",
          "48:                 f\"({max_size}B).\"",
          "",
          "[Added Lines]",
          "47:                 f\"({decompressed_size} B) exceed the specified maximum \"",
          "48:                 f\"({max_size} B).\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "65:         if max_size and decompressed_size > max_size:",
          "66:             raise _DecompressionMaxSizeExceeded(",
          "67:                 f\"The number of bytes decompressed so far \"",
          "70:             )",
          "71:         output_list.append(output_chunk)",
          "72:     return b\"\".join(output_list)",
          "",
          "[Removed Lines]",
          "68:                 f\"({decompressed_size}B) exceed the specified maximum \"",
          "69:                 f\"({max_size}B).\"",
          "",
          "[Added Lines]",
          "68:                 f\"({decompressed_size} B) exceed the specified maximum \"",
          "69:                 f\"({max_size} B).\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "85:         if max_size and decompressed_size > max_size:",
          "86:             raise _DecompressionMaxSizeExceeded(",
          "87:                 f\"The number of bytes decompressed so far \"",
          "90:             )",
          "91:         output_list.append(output_chunk)",
          "92:     return b\"\".join(output_list)",
          "",
          "[Removed Lines]",
          "88:                 f\"({decompressed_size}B) exceed the specified maximum \"",
          "89:                 f\"({max_size}B).\"",
          "",
          "[Added Lines]",
          "88:                 f\"({decompressed_size} B) exceed the specified maximum \"",
          "89:                 f\"({max_size} B).\"",
          "",
          "---------------"
        ],
        "scrapy/utils/gz.py||scrapy/utils/gz.py": [
          "File: scrapy/utils/gz.py -> scrapy/utils/gz.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31:         if max_size and decompressed_size > max_size:",
          "32:             raise _DecompressionMaxSizeExceeded(",
          "33:                 f\"The number of bytes decompressed so far \"",
          "36:             )",
          "37:         output_list.append(chunk)",
          "38:     return b\"\".join(output_list)",
          "",
          "[Removed Lines]",
          "34:                 f\"({decompressed_size}B) exceed the specified maximum \"",
          "35:                 f\"({max_size}B).\"",
          "",
          "[Added Lines]",
          "34:                 f\"({decompressed_size} B) exceed the specified maximum \"",
          "35:                 f\"({max_size} B).\"",
          "",
          "---------------"
        ],
        "tests/test_downloadermiddleware_httpcompression.py||tests/test_downloadermiddleware_httpcompression.py": [
          "File: tests/test_downloadermiddleware_httpcompression.py -> tests/test_downloadermiddleware_httpcompression.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: from gzip import GzipFile",
          "2: from io import BytesIO",
          "3: from pathlib import Path",
          "4: from unittest import SkipTest, TestCase",
          "5: from warnings import catch_warnings",
          "7: from w3lib.encoding import resolve_encoding",
          "9: from scrapy.downloadermiddlewares.httpcompression import (",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3: from logging import WARNING",
          "8: from testfixtures import LogCapture",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "468:     def test_compression_bomb_request_meta_zstd(self):",
          "469:         self._test_compression_bomb_request_meta(\"zstd\")",
          "472: class HttpCompressionSubclassTest(TestCase):",
          "473:     def test_init_missing_stats(self):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "473:     def _test_download_warnsize_setting(self, compression_id):",
          "474:         settings = {\"DOWNLOAD_WARNSIZE\": 10_000_000}",
          "475:         crawler = get_crawler(Spider, settings_dict=settings)",
          "476:         spider = crawler._create_spider(\"scrapytest.org\")",
          "477:         mw = HttpCompressionMiddleware.from_crawler(crawler)",
          "478:         mw.open_spider(spider)",
          "479:         response = self._getresponse(f\"bomb-{compression_id}\")",
          "481:         with LogCapture(",
          "482:             \"scrapy.downloadermiddlewares.httpcompression\",",
          "483:             propagate=False,",
          "484:             level=WARNING,",
          "485:         ) as log:",
          "486:             mw.process_response(response.request, response, spider)",
          "487:         log.check(",
          "488:             (",
          "489:                 \"scrapy.downloadermiddlewares.httpcompression\",",
          "490:                 \"WARNING\",",
          "491:                 (",
          "492:                     \"<200 http://scrapytest.org/> body size after \"",
          "493:                     \"decompression (11511612 B) is larger than the download \"",
          "494:                     \"warning size (10000000 B).\"",
          "495:                 ),",
          "496:             ),",
          "497:         )",
          "499:     def test_download_warnsize_setting_br(self):",
          "500:         try:",
          "501:             import brotli  # noqa: F401",
          "502:         except ImportError:",
          "503:             raise SkipTest(\"no brotli\")",
          "504:         self._test_download_warnsize_setting(\"br\")",
          "506:     def test_download_warnsize_setting_deflate(self):",
          "507:         self._test_download_warnsize_setting(\"deflate\")",
          "509:     def test_download_warnsize_setting_gzip(self):",
          "510:         self._test_download_warnsize_setting(\"gzip\")",
          "512:     def test_download_warnsize_setting_zstd(self):",
          "513:         self._test_download_warnsize_setting(\"zstd\")",
          "515:     def _test_download_warnsize_spider_attr(self, compression_id):",
          "516:         class DownloadWarnSizeSpider(Spider):",
          "517:             download_warnsize = 10_000_000",
          "519:         crawler = get_crawler(DownloadWarnSizeSpider)",
          "520:         spider = crawler._create_spider(\"scrapytest.org\")",
          "521:         mw = HttpCompressionMiddleware.from_crawler(crawler)",
          "522:         mw.open_spider(spider)",
          "523:         response = self._getresponse(f\"bomb-{compression_id}\")",
          "525:         with LogCapture(",
          "526:             \"scrapy.downloadermiddlewares.httpcompression\",",
          "527:             propagate=False,",
          "528:             level=WARNING,",
          "529:         ) as log:",
          "530:             mw.process_response(response.request, response, spider)",
          "531:         log.check(",
          "532:             (",
          "533:                 \"scrapy.downloadermiddlewares.httpcompression\",",
          "534:                 \"WARNING\",",
          "535:                 (",
          "536:                     \"<200 http://scrapytest.org/> body size after \"",
          "537:                     \"decompression (11511612 B) is larger than the download \"",
          "538:                     \"warning size (10000000 B).\"",
          "539:                 ),",
          "540:             ),",
          "541:         )",
          "543:     def test_download_warnsize_spider_attr_br(self):",
          "544:         try:",
          "545:             import brotli  # noqa: F401",
          "546:         except ImportError:",
          "547:             raise SkipTest(\"no brotli\")",
          "548:         self._test_download_warnsize_spider_attr(\"br\")",
          "550:     def test_download_warnsize_spider_attr_deflate(self):",
          "551:         self._test_download_warnsize_spider_attr(\"deflate\")",
          "553:     def test_download_warnsize_spider_attr_gzip(self):",
          "554:         self._test_download_warnsize_spider_attr(\"gzip\")",
          "556:     def test_download_warnsize_spider_attr_zstd(self):",
          "557:         self._test_download_warnsize_spider_attr(\"zstd\")",
          "559:     def _test_download_warnsize_request_meta(self, compression_id):",
          "560:         crawler = get_crawler(Spider)",
          "561:         spider = crawler._create_spider(\"scrapytest.org\")",
          "562:         mw = HttpCompressionMiddleware.from_crawler(crawler)",
          "563:         mw.open_spider(spider)",
          "564:         response = self._getresponse(f\"bomb-{compression_id}\")",
          "565:         response.meta[\"download_warnsize\"] = 10_000_000",
          "567:         with LogCapture(",
          "568:             \"scrapy.downloadermiddlewares.httpcompression\",",
          "569:             propagate=False,",
          "570:             level=WARNING,",
          "571:         ) as log:",
          "572:             mw.process_response(response.request, response, spider)",
          "573:         log.check(",
          "574:             (",
          "575:                 \"scrapy.downloadermiddlewares.httpcompression\",",
          "576:                 \"WARNING\",",
          "577:                 (",
          "578:                     \"<200 http://scrapytest.org/> body size after \"",
          "579:                     \"decompression (11511612 B) is larger than the download \"",
          "580:                     \"warning size (10000000 B).\"",
          "581:                 ),",
          "582:             ),",
          "583:         )",
          "585:     def test_download_warnsize_request_meta_br(self):",
          "586:         try:",
          "587:             import brotli  # noqa: F401",
          "588:         except ImportError:",
          "589:             raise SkipTest(\"no brotli\")",
          "590:         self._test_download_warnsize_request_meta(\"br\")",
          "592:     def test_download_warnsize_request_meta_deflate(self):",
          "593:         self._test_download_warnsize_request_meta(\"deflate\")",
          "595:     def test_download_warnsize_request_meta_gzip(self):",
          "596:         self._test_download_warnsize_request_meta(\"gzip\")",
          "598:     def test_download_warnsize_request_meta_zstd(self):",
          "599:         self._test_download_warnsize_request_meta(\"zstd\")",
          "",
          "---------------"
        ],
        "tests/test_spider.py||tests/test_spider.py": [
          "File: tests/test_spider.py -> tests/test_spider.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2: import inspect",
          "3: import warnings",
          "4: from io import BytesIO",
          "5: from pathlib import Path",
          "6: from typing import Any",
          "7: from unittest import mock",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "5: from logging import WARNING",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "732:         response = Response(url=\"https://example.com\", body=body, request=request)",
          "733:         self.assertIsNone(spider._get_sitemap_body(response))",
          "736: class DeprecationTest(unittest.TestCase):",
          "737:     def test_crawl_spider(self):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "736:     def test_download_warnsize_setting(self):",
          "737:         settings = {\"DOWNLOAD_WARNSIZE\": 10_000_000}",
          "738:         crawler = get_crawler(settings_dict=settings)",
          "739:         spider = self.spider_class.from_crawler(crawler, \"example.com\")",
          "740:         body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")",
          "741:         body = body_path.read_bytes()",
          "742:         request = Request(url=\"https://example.com\")",
          "743:         response = Response(url=\"https://example.com\", body=body, request=request)",
          "744:         with LogCapture(",
          "745:             \"scrapy.spiders.sitemap\", propagate=False, level=WARNING",
          "746:         ) as log:",
          "747:             spider._get_sitemap_body(response)",
          "748:         log.check(",
          "749:             (",
          "750:                 \"scrapy.spiders.sitemap\",",
          "751:                 \"WARNING\",",
          "752:                 (",
          "753:                     \"<200 https://example.com> body size after decompression \"",
          "754:                     \"(11511612 B) is larger than the download warning size \"",
          "755:                     \"(10000000 B).\"",
          "756:                 ),",
          "757:             ),",
          "758:         )",
          "760:     def test_download_warnsize_spider_attr(self):",
          "761:         class DownloadWarnSizeSpider(self.spider_class):",
          "762:             download_warnsize = 10_000_000",
          "764:         crawler = get_crawler()",
          "765:         spider = DownloadWarnSizeSpider.from_crawler(crawler, \"example.com\")",
          "766:         body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")",
          "767:         body = body_path.read_bytes()",
          "768:         request = Request(",
          "769:             url=\"https://example.com\", meta={\"download_warnsize\": 10_000_000}",
          "770:         )",
          "771:         response = Response(url=\"https://example.com\", body=body, request=request)",
          "772:         with LogCapture(",
          "773:             \"scrapy.spiders.sitemap\", propagate=False, level=WARNING",
          "774:         ) as log:",
          "775:             spider._get_sitemap_body(response)",
          "776:         log.check(",
          "777:             (",
          "778:                 \"scrapy.spiders.sitemap\",",
          "779:                 \"WARNING\",",
          "780:                 (",
          "781:                     \"<200 https://example.com> body size after decompression \"",
          "782:                     \"(11511612 B) is larger than the download warning size \"",
          "783:                     \"(10000000 B).\"",
          "784:                 ),",
          "785:             ),",
          "786:         )",
          "788:     def test_download_warnsize_request_meta(self):",
          "789:         crawler = get_crawler()",
          "790:         spider = self.spider_class.from_crawler(crawler, \"example.com\")",
          "791:         body_path = Path(tests_datadir, \"compressed\", \"bomb-gzip.bin\")",
          "792:         body = body_path.read_bytes()",
          "793:         request = Request(",
          "794:             url=\"https://example.com\", meta={\"download_warnsize\": 10_000_000}",
          "795:         )",
          "796:         response = Response(url=\"https://example.com\", body=body, request=request)",
          "797:         with LogCapture(",
          "798:             \"scrapy.spiders.sitemap\", propagate=False, level=WARNING",
          "799:         ) as log:",
          "800:             spider._get_sitemap_body(response)",
          "801:         log.check(",
          "802:             (",
          "803:                 \"scrapy.spiders.sitemap\",",
          "804:                 \"WARNING\",",
          "805:                 (",
          "806:                     \"<200 https://example.com> body size after decompression \"",
          "807:                     \"(11511612 B) is larger than the download warning size \"",
          "808:                     \"(10000000 B).\"",
          "809:                 ),",
          "810:             ),",
          "811:         )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "09a7efef7c75558c9ea198a00fc11ab26fb16ce5",
      "candidate_info": {
        "commit_hash": "09a7efef7c75558c9ea198a00fc11ab26fb16ce5",
        "repo": "scrapy/scrapy",
        "commit_url": "https://github.com/scrapy/scrapy/commit/09a7efef7c75558c9ea198a00fc11ab26fb16ce5",
        "files": [
          "tests/test_feedexport.py"
        ],
        "message": "Remove a defer.returnValue call.",
        "before_after_code_files": [
          "tests/test_feedexport.py||tests/test_feedexport.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/scrapy/scrapy/pull/6222"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/test_feedexport.py||tests/test_feedexport.py": [
          "File: tests/test_feedexport.py -> tests/test_feedexport.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2300:                     content[feed[\"format\"]].append(file.read_bytes())",
          "2301:         finally:",
          "2302:             self.tearDown()",
          "2305:     @defer.inlineCallbacks",
          "2306:     def assertExportedJsonLines(self, items, rows, settings=None):",
          "",
          "[Removed Lines]",
          "2303:         defer.returnValue(content)",
          "",
          "[Added Lines]",
          "2303:         return content",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1533b69032e2fb5e495e88a3fed57c0d98502612",
      "candidate_info": {
        "commit_hash": "1533b69032e2fb5e495e88a3fed57c0d98502612",
        "repo": "scrapy/scrapy",
        "commit_url": "https://github.com/scrapy/scrapy/commit/1533b69032e2fb5e495e88a3fed57c0d98502612",
        "files": [
          "scrapy/utils/response.py",
          "tests/test_utils_response.py"
        ],
        "message": "Test and address ReDoS attack vectors for open_in_browser",
        "before_after_code_files": [
          "scrapy/utils/response.py||scrapy/utils/response.py",
          "tests/test_utils_response.py||tests/test_utils_response.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/scrapy/scrapy/pull/6222"
        ],
        "olp_code_files": {
          "patch": [
            "scrapy/utils/response.py||scrapy/utils/response.py",
            "tests/test_utils_response.py||tests/test_utils_response.py"
          ],
          "candidate": [
            "scrapy/utils/response.py||scrapy/utils/response.py",
            "tests/test_utils_response.py||tests/test_utils_response.py"
          ]
        }
      },
      "candidate_diff": {
        "scrapy/utils/response.py||scrapy/utils/response.py": [
          "File: scrapy/utils/response.py -> scrapy/utils/response.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "103:     body = response.body",
          "104:     if isinstance(response, HtmlResponse):",
          "105:         if b\"<base\" not in body:",
          "109:         ext = \".html\"",
          "110:     elif isinstance(response, TextResponse):",
          "111:         ext = \".txt\"",
          "",
          "[Removed Lines]",
          "106:             repl = rf'\\1<base href=\"{response.url}\">'",
          "107:             body = re.sub(b\"<!--.{,1024}?-->\", b\"\", body, flags=re.DOTALL)",
          "108:             body = re.sub(rb\"(<head(?:>|\\s.{,1024}?>))\", to_bytes(repl), body)",
          "",
          "[Added Lines]",
          "106:             repl = rf'\\0<base href=\"{response.url}\">'",
          "107:             body = re.sub(b\"(?s)<!--.*?(?:-->|$)\", b\"\", body)",
          "108:             body = re.sub(rb\"<head(?:[^<>]*?>)\", to_bytes(repl), body, count=1)",
          "",
          "---------------"
        ],
        "tests/test_utils_response.py||tests/test_utils_response.py": [
          "File: tests/test_utils_response.py -> tests/test_utils_response.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: import unittest",
          "2: import warnings",
          "3: from pathlib import Path",
          "4: from urllib.parse import urlparse",
          "6: from scrapy.exceptions import ScrapyDeprecationWarning",
          "7: from scrapy.http import HtmlResponse, Response, TextResponse",
          "8: from scrapy.utils.python import to_bytes",
          "9: from scrapy.utils.response import (",
          "10:     get_base_url,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4: from time import process_time",
          "9: from scrapy.settings.default_settings import DOWNLOAD_MAXSIZE",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "198:         assert open_in_browser(",
          "199:             r5, _openfunc=check_base_url",
          "200:         ), \"Inject unique base url with conditional comment\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "204:     def test_open_in_browser_redos_comment(self):",
          "205:         MAX_CPU_TIME = 30",
          "207:         # Exploit input from",
          "208:         # https://makenowjust-labs.github.io/recheck/playground/",
          "209:         # for /<!--.*?-->/ (old pattern to remove comments).",
          "210:         body = b\"-><!--\\x00\" * (int(DOWNLOAD_MAXSIZE / 7) - 10) + b\"->\\n<!---->\"",
          "212:         response = HtmlResponse(\"https://example.com\", body=body)",
          "214:         start_time = process_time()",
          "216:         open_in_browser(response, lambda url: True)",
          "218:         end_time = process_time()",
          "219:         self.assertLess(end_time - start_time, MAX_CPU_TIME)",
          "221:     def test_open_in_browser_redos_head(self):",
          "222:         MAX_CPU_TIME = 15",
          "224:         # Exploit input from",
          "225:         # https://makenowjust-labs.github.io/recheck/playground/",
          "226:         # for /(<head(?:>|\\s.*?>))/ (old pattern to find the head element).",
          "227:         body = b\"<head\\t\" * int(DOWNLOAD_MAXSIZE / 6)",
          "229:         response = HtmlResponse(\"https://example.com\", body=body)",
          "231:         start_time = process_time()",
          "233:         open_in_browser(response, lambda url: True)",
          "235:         end_time = process_time()",
          "236:         self.assertLess(end_time - start_time, MAX_CPU_TIME)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c947f51077e1ab246f30764cc4cc7a1cc5835d40",
      "candidate_info": {
        "commit_hash": "c947f51077e1ab246f30764cc4cc7a1cc5835d40",
        "repo": "scrapy/scrapy",
        "commit_url": "https://github.com/scrapy/scrapy/commit/c947f51077e1ab246f30764cc4cc7a1cc5835d40",
        "files": [
          "docs/news.rst",
          "scrapy/utils/iterators.py",
          "scrapy/utils/response.py"
        ],
        "message": "Set an arbitrary upper limit on ReDoS-vulnerable regexps",
        "before_after_code_files": [
          "scrapy/utils/iterators.py||scrapy/utils/iterators.py",
          "scrapy/utils/response.py||scrapy/utils/response.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/scrapy/scrapy/pull/6222"
        ],
        "olp_code_files": {
          "patch": [
            "scrapy/utils/iterators.py||scrapy/utils/iterators.py",
            "scrapy/utils/response.py||scrapy/utils/response.py"
          ],
          "candidate": [
            "scrapy/utils/iterators.py||scrapy/utils/iterators.py",
            "scrapy/utils/response.py||scrapy/utils/response.py"
          ]
        }
      },
      "candidate_diff": {
        "scrapy/utils/iterators.py||scrapy/utils/iterators.py": [
          "File: scrapy/utils/iterators.py -> scrapy/utils/iterators.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "40:     \"\"\"",
          "41:     nodename_patt = re.escape(nodename)",
          "44:     HEADER_END_RE = re.compile(rf\"<\\s*/{nodename_patt}\\s*>\", re.S)",
          "47:     text = _body_or_str(obj)",
          "49:     document_header_match = re.search(DOCUMENT_HEADER_RE, text)",
          "",
          "[Removed Lines]",
          "43:     DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]+>\\s*\", re.S)",
          "45:     END_TAG_RE = re.compile(r\"<\\s*/([^\\s>]+)\\s*>\", re.S)",
          "46:     NAMESPACE_RE = re.compile(r\"((xmlns[:A-Za-z]*)=[^>\\s]+)\", re.S)",
          "",
          "[Added Lines]",
          "43:     DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]{1,1024}>\\s*\", re.S)",
          "45:     END_TAG_RE = re.compile(r\"<\\s*/([^\\s>]{1,1024})\\s*>\", re.S)",
          "46:     NAMESPACE_RE = re.compile(r\"((xmlns[:A-Za-z]{,1024})=[^>\\s]+)\", re.S)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "57:         for tagname in reversed(re.findall(END_TAG_RE, header_end)):",
          "58:             assert header_end_idx",
          "59:             tag = re.search(",
          "61:             )",
          "62:             if tag:",
          "63:                 for x in re.findall(NAMESPACE_RE, tag.group()):",
          "64:                     namespaces[x[1]] = x[0]",
          "67:     for match in r.finditer(text):",
          "68:         nodetext = (",
          "69:             document_header",
          "",
          "[Removed Lines]",
          "60:                 rf\"<\\s*{tagname}.*?xmlns[:=][^>]*>\", text[: header_end_idx[1]], re.S",
          "66:     r = re.compile(rf\"<{nodename_patt}[\\s>].*?</{nodename_patt}>\", re.DOTALL)",
          "",
          "[Added Lines]",
          "60:                 rf\"<\\s*{tagname}.{{,1024}}?xmlns[:=][^>]{{,1024}}>\",",
          "61:                 text[: header_end_idx[1]],",
          "62:                 re.S,",
          "68:     r = re.compile(rf\"<{nodename_patt}[\\s>].{{,1024}}?</{nodename_patt}>\", re.DOTALL)",
          "",
          "---------------"
        ],
        "scrapy/utils/response.py||scrapy/utils/response.py": [
          "File: scrapy/utils/response.py -> scrapy/utils/response.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "91:     if isinstance(response, HtmlResponse):",
          "92:         if b\"<base\" not in body:",
          "93:             repl = rf'\\1<base href=\"{response.url}\">'",
          "96:         ext = \".html\"",
          "97:     elif isinstance(response, TextResponse):",
          "98:         ext = \".txt\"",
          "",
          "[Removed Lines]",
          "94:             body = re.sub(b\"<!--.*?-->\", b\"\", body, flags=re.DOTALL)",
          "95:             body = re.sub(rb\"(<head(?:>|\\s.*?>))\", to_bytes(repl), body)",
          "",
          "[Added Lines]",
          "94:             body = re.sub(b\"<!--.{,1024}?-->\", b\"\", body, flags=re.DOTALL)",
          "95:             body = re.sub(rb\"(<head(?:>|\\s.{,1024}?>))\", to_bytes(repl), body)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "080fecd8900b6b1f94e8e143e90338279ba8d6e5",
      "candidate_info": {
        "commit_hash": "080fecd8900b6b1f94e8e143e90338279ba8d6e5",
        "repo": "scrapy/scrapy",
        "commit_url": "https://github.com/scrapy/scrapy/commit/080fecd8900b6b1f94e8e143e90338279ba8d6e5",
        "files": [
          "docs/news.rst",
          "scrapy/downloadermiddlewares/redirect.py",
          "tests/test_downloadermiddleware_redirect.py"
        ],
        "message": "Drop the Authorization header on cross-domain redirect",
        "before_after_code_files": [
          "scrapy/downloadermiddlewares/redirect.py||scrapy/downloadermiddlewares/redirect.py",
          "tests/test_downloadermiddleware_redirect.py||tests/test_downloadermiddleware_redirect.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/scrapy/scrapy/pull/6222"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scrapy/downloadermiddlewares/redirect.py||scrapy/downloadermiddlewares/redirect.py": [
          "File: scrapy/downloadermiddlewares/redirect.py -> scrapy/downloadermiddlewares/redirect.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18:         cookies=None,",
          "19:     )",
          "21:         source_request_netloc = urlparse_cached(source_request).netloc",
          "22:         redirect_request_netloc = urlparse_cached(redirect_request).netloc",
          "23:         if source_request_netloc != redirect_request_netloc:",
          "25:     return redirect_request",
          "",
          "[Removed Lines]",
          "20:     if \"Cookie\" in redirect_request.headers:",
          "24:             del redirect_request.headers[\"Cookie\"]",
          "",
          "[Added Lines]",
          "20:     has_cookie_header = \"Cookie\" in redirect_request.headers",
          "21:     has_authorization_header = \"Authorization\" in redirect_request.headers",
          "22:     if has_cookie_header or has_authorization_header:",
          "26:             if has_cookie_header:",
          "27:                 del redirect_request.headers[\"Cookie\"]",
          "28:             # https://fetch.spec.whatwg.org/#ref-for-cors-non-wildcard-request-header-name",
          "29:             if has_authorization_header:",
          "30:                 del redirect_request.headers[\"Authorization\"]",
          "",
          "---------------"
        ],
        "tests/test_downloadermiddleware_redirect.py||tests/test_downloadermiddleware_redirect.py": [
          "File: tests/test_downloadermiddleware_redirect.py -> tests/test_downloadermiddleware_redirect.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "247:         perc_encoded_utf8_url = \"http://scrapytest.org/a%C3%A7%C3%A3o\"",
          "248:         self.assertEqual(perc_encoded_utf8_url, req_result.url)",
          "251: class MetaRefreshMiddlewareTest(unittest.TestCase):",
          "252:     def setUp(self):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "250:     def test_cross_domain_header_dropping(self):",
          "251:         safe_headers = {\"A\": \"B\"}",
          "252:         original_request = Request(",
          "253:             \"https://example.com\",",
          "254:             headers={\"Cookie\": \"a=b\", \"Authorization\": \"a\", **safe_headers},",
          "255:         )",
          "257:         internal_response = Response(",
          "258:             \"https://example.com\",",
          "259:             headers={\"Location\": \"https://example.com/a\"},",
          "260:             status=301,",
          "261:         )",
          "262:         internal_redirect_request = self.mw.process_response(",
          "263:             original_request, internal_response, self.spider",
          "264:         )",
          "265:         self.assertIsInstance(internal_redirect_request, Request)",
          "266:         self.assertEqual(original_request.headers, internal_redirect_request.headers)",
          "268:         external_response = Response(",
          "269:             \"https://example.com\",",
          "270:             headers={\"Location\": \"https://example.org/a\"},",
          "271:             status=301,",
          "272:         )",
          "273:         external_redirect_request = self.mw.process_response(",
          "274:             original_request, external_response, self.spider",
          "275:         )",
          "276:         self.assertIsInstance(external_redirect_request, Request)",
          "277:         self.assertEqual(",
          "278:             safe_headers, external_redirect_request.headers.to_unicode_dict()",
          "279:         )",
          "",
          "---------------"
        ]
      }
    }
  ]
}