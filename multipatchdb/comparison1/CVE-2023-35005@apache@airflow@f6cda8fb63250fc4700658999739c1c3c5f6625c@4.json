{
  "cve_id": "CVE-2023-35005",
  "cve_desc": "In Apache Airflow, some potentially sensitive values were being shown to the user in certain situations.\n\nThis vulnerability is mitigated by the fact configuration is not shown in the UI by default (only if `[webserver] expose_config` is set to `non-sensitive-only`), and not all uncensored values are actually sentitive.\n\n\nThis issue affects Apache Airflow: from 2.5.0 before 2.6.2. Users are recommended to update to version 2.6.2 or later.\n\n\n",
  "repo": "apache/airflow",
  "patch_hash": "f6cda8fb63250fc4700658999739c1c3c5f6625c",
  "patch_info": {
    "commit_hash": "f6cda8fb63250fc4700658999739c1c3c5f6625c",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/f6cda8fb63250fc4700658999739c1c3c5f6625c",
    "files": [
      "airflow/configuration.py"
    ],
    "message": "Mark `[secrets] backend_kwargs` as a sensitive config (#31788)\n\n(cherry picked from commit 8062756fa9e01eeeee1f2c6df74f376c0a526bd5)",
    "before_after_code_files": [
      "airflow/configuration.py||airflow/configuration.py"
    ]
  },
  "patch_diff": {
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "156:     (\"atlas\", \"password\"),",
      "157:     (\"smtp\", \"smtp_password\"),",
      "158:     (\"webserver\", \"secret_key\"),",
      "159:     # The following options are deprecated",
      "160:     (\"core\", \"sql_alchemy_conn\"),",
      "161: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "159:     (\"secrets\", \"backend_kwargs\"),",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "af11a193694a100c75042fa0644b2818c2ccd6db",
      "candidate_info": {
        "commit_hash": "af11a193694a100c75042fa0644b2818c2ccd6db",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/af11a193694a100c75042fa0644b2818c2ccd6db",
        "files": [
          "airflow/providers/amazon/aws/log/s3_task_handler.py",
          "airflow/providers/microsoft/azure/log/wasb_task_handler.py",
          "airflow/utils/log/file_task_handler.py"
        ],
        "message": "Add docstring and signature for _read_remote_logs (#31623)\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit ce7766e0a52e15b2b1ef7e7f9c613ea686fbfca6)",
        "before_after_code_files": [
          "airflow/providers/amazon/aws/log/s3_task_handler.py||airflow/providers/amazon/aws/log/s3_task_handler.py",
          "airflow/providers/microsoft/azure/log/wasb_task_handler.py||airflow/providers/microsoft/azure/log/wasb_task_handler.py",
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/amazon/aws/log/s3_task_handler.py||airflow/providers/amazon/aws/log/s3_task_handler.py": [
          "File: airflow/providers/amazon/aws/log/s3_task_handler.py -> airflow/providers/amazon/aws/log/s3_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "110:         # Mark closed so we don't double write if close is called twice",
          "111:         self.closed = True",
          "114:         # Explicitly getting log relative path is necessary as the given",
          "115:         # task instance might be different than task instance passed in",
          "116:         # in set_context method.",
          "",
          "[Removed Lines]",
          "113:     def _read_remote_logs(self, ti, try_number, metadata=None):",
          "",
          "[Added Lines]",
          "113:     def _read_remote_logs(self, ti, try_number, metadata=None) -> tuple[list[str], list[str]]:",
          "",
          "---------------"
        ],
        "airflow/providers/microsoft/azure/log/wasb_task_handler.py||airflow/providers/microsoft/azure/log/wasb_task_handler.py": [
          "File: airflow/providers/microsoft/azure/log/wasb_task_handler.py -> airflow/providers/microsoft/azure/log/wasb_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "128:         # Mark closed so we don't double write if close is called twice",
          "129:         self.closed = True",
          "132:         messages = []",
          "133:         logs = []",
          "134:         worker_log_relative_path = self._render_filename(ti, try_number)",
          "",
          "[Removed Lines]",
          "131:     def _read_remote_logs(self, ti, try_number, metadata=None):",
          "",
          "[Added Lines]",
          "131:     def _read_remote_logs(self, ti, try_number, metadata=None) -> tuple[list[str], list[str]]:",
          "",
          "---------------"
        ],
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "522:             logger.exception(\"Could not read served logs\")",
          "523:         return messages, logs",
          "527:         raise NotImplementedError",
          "",
          "[Removed Lines]",
          "525:     def _read_remote_logs(self, ti, try_number, metadata=None):",
          "526:         \"\"\"Implement in subclasses to read from the remote service\"\"\"",
          "",
          "[Added Lines]",
          "525:     def _read_remote_logs(self, ti, try_number, metadata=None) -> tuple[list[str], list[str]]:",
          "526:         \"\"\"",
          "527:         Implement in subclasses to read from the remote service.",
          "529:         This method should return two lists, messages and logs.",
          "532:           such as, \"reading from x file\".",
          "534:         \"\"\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "91cbb84bc924543b6b3754556410509ff902508b",
      "candidate_info": {
        "commit_hash": "91cbb84bc924543b6b3754556410509ff902508b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/91cbb84bc924543b6b3754556410509ff902508b",
        "files": [
          "RELEASE_NOTES.rst",
          "airflow/utils/db.py"
        ],
        "message": "Update release note and revision head",
        "before_after_code_files": [
          "airflow/utils/db.py||airflow/utils/db.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/db.py||airflow/utils/db.py": [
          "File: airflow/utils/db.py -> airflow/utils/db.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "81:     \"2.5.3\": \"290244fb8b83\",",
          "82:     \"2.6.0\": \"98ae134e6fff\",",
          "83:     \"2.6.1\": \"98ae134e6fff\",",
          "85: }",
          "",
          "[Removed Lines]",
          "84:     \"2.6.2\": \"98ae134e6fff\",",
          "",
          "[Added Lines]",
          "84:     \"2.6.2\": \"c804e5c76e3e\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "aeb4ef367952510e4693da3c991400ca90425a27",
      "candidate_info": {
        "commit_hash": "aeb4ef367952510e4693da3c991400ca90425a27",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/aeb4ef367952510e4693da3c991400ca90425a27",
        "files": [
          "airflow/utils/task_group.py",
          "tests/decorators/test_task_group.py",
          "tests/utils/test_task_group.py"
        ],
        "message": "Fix overriding `default_args` in nested task groups (#31608)\n\n* add unit tests for default_args overriding in task group\n\nSigned-off-by: Hussein Awala <hussein@awala.fr>\n\n* fix overriding default args in nested task groups\n\nSigned-off-by: Hussein Awala <hussein@awala.fr>\n\n* Update airflow/utils/task_group.py\n\nCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>\n\n---------\n\nSigned-off-by: Hussein Awala <hussein@awala.fr>\nCo-authored-by: Ash Berlin-Taylor <ash_github@firemirror.com>\n(cherry picked from commit 9e8627faa71e9d2047816b291061c28585809508)",
        "before_after_code_files": [
          "airflow/utils/task_group.py||airflow/utils/task_group.py",
          "tests/decorators/test_task_group.py||tests/decorators/test_task_group.py",
          "tests/utils/test_task_group.py||tests/utils/test_task_group.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/task_group.py||airflow/utils/task_group.py": [
          "File: airflow/utils/task_group.py -> airflow/utils/task_group.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "141:         if parent_group:",
          "142:             parent_group.add(self)",
          "144:         self.used_group_ids.add(self.group_id)",
          "145:         if self.group_id:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "143:             self._update_default_args(parent_group)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "176:             else:",
          "177:                 self._group_id = f\"{base}__{suffixes[-1] + 1}\"",
          "179:     @classmethod",
          "180:     def create_root(cls, dag: DAG) -> TaskGroup:",
          "181:         \"\"\"Create a root TaskGroup with no group_id or parent.\"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "180:     def _update_default_args(self, parent_group: TaskGroup):",
          "181:         if parent_group.default_args:",
          "182:             self.default_args = {**self.default_args, **parent_group.default_args}",
          "",
          "---------------"
        ],
        "tests/decorators/test_task_group.py||tests/decorators/test_task_group.py": [
          "File: tests/decorators/test_task_group.py -> tests/decorators/test_task_group.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: import pendulum",
          "21: import pytest",
          "23: from airflow.decorators import dag, task_group",
          "24: from airflow.models.expandinput import DictOfListsExpandInput, ListOfDictsExpandInput, MappedArgument",
          "25: from airflow.utils.task_group import MappedTaskGroup",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: from datetime import timedelta",
          "27: from airflow.operators.empty import EmptyOperator",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "186:     assert tg._expand_input == ListOfDictsExpandInput([{\"b\": \"x\"}, {\"b\": None}])",
          "188:     assert saved == {\"a\": 1, \"b\": MappedArgument(input=tg._expand_input, key=\"b\")}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "194: def test_override_dag_default_args():",
          "195:     @dag(",
          "196:         dag_id=\"test_dag\",",
          "197:         start_date=pendulum.parse(\"20200101\"),",
          "198:         default_args={",
          "199:             \"retries\": 1,",
          "200:             \"owner\": \"x\",",
          "201:         },",
          "202:     )",
          "203:     def pipeline():",
          "204:         @task_group(",
          "205:             group_id=\"task_group\",",
          "206:             default_args={",
          "207:                 \"owner\": \"y\",",
          "208:                 \"execution_timeout\": timedelta(seconds=10),",
          "209:             },",
          "210:         )",
          "211:         def tg():",
          "212:             EmptyOperator(task_id=\"task\")",
          "214:         tg()",
          "216:     test_dag = pipeline()",
          "217:     test_task = test_dag.task_group_dict[\"task_group\"].children[\"task_group.task\"]",
          "218:     assert test_task.retries == 1",
          "219:     assert test_task.owner == \"y\"",
          "220:     assert test_task.execution_timeout == timedelta(seconds=10)",
          "223: def test_override_dag_default_args_nested_tg():",
          "224:     @dag(",
          "225:         dag_id=\"test_dag\",",
          "226:         start_date=pendulum.parse(\"20200101\"),",
          "227:         default_args={",
          "228:             \"retries\": 1,",
          "229:             \"owner\": \"x\",",
          "230:         },",
          "231:     )",
          "232:     def pipeline():",
          "233:         @task_group(",
          "234:             group_id=\"task_group\",",
          "235:             default_args={",
          "236:                 \"owner\": \"y\",",
          "237:                 \"execution_timeout\": timedelta(seconds=10),",
          "238:             },",
          "239:         )",
          "240:         def tg():",
          "241:             @task_group(group_id=\"nested_task_group\")",
          "242:             def nested_tg():",
          "243:                 @task_group(group_id=\"another_task_group\")",
          "244:                 def another_tg():",
          "245:                     EmptyOperator(task_id=\"task\")",
          "247:                 another_tg()",
          "249:             nested_tg()",
          "251:         tg()",
          "253:     test_dag = pipeline()",
          "254:     test_task = (",
          "255:         test_dag.task_group_dict[\"task_group\"]",
          "256:         .children[\"task_group.nested_task_group\"]",
          "257:         .children[\"task_group.nested_task_group.another_task_group\"]",
          "258:         .children[\"task_group.nested_task_group.another_task_group.task\"]",
          "259:     )",
          "260:     assert test_task.retries == 1",
          "261:     assert test_task.owner == \"y\"",
          "262:     assert test_task.execution_timeout == timedelta(seconds=10)",
          "",
          "---------------"
        ],
        "tests/utils/test_task_group.py||tests/utils/test_task_group.py": [
          "File: tests/utils/test_task_group.py -> tests/utils/test_task_group.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: import pendulum",
          "21: import pytest",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: from datetime import timedelta",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1301:         \"section_2.task3\",",
          "1302:         \"section_2.bash_task\",",
          "1303:     ]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1308: def test_override_dag_default_args():",
          "1309:     with DAG(",
          "1310:         dag_id=\"test_dag\",",
          "1311:         start_date=pendulum.parse(\"20200101\"),",
          "1312:         default_args={",
          "1313:             \"retries\": 1,",
          "1314:             \"owner\": \"x\",",
          "1315:         },",
          "1316:     ):",
          "1317:         with TaskGroup(",
          "1318:             group_id=\"task_group\",",
          "1319:             default_args={",
          "1320:                 \"owner\": \"y\",",
          "1321:                 \"execution_timeout\": timedelta(seconds=10),",
          "1322:             },",
          "1323:         ):",
          "1324:             task = EmptyOperator(task_id=\"task\")",
          "1326:     assert task.retries == 1",
          "1327:     assert task.owner == \"y\"",
          "1328:     assert task.execution_timeout == timedelta(seconds=10)",
          "1331: def test_override_dag_default_args_in_nested_tg():",
          "1332:     with DAG(",
          "1333:         dag_id=\"test_dag\",",
          "1334:         start_date=pendulum.parse(\"20200101\"),",
          "1335:         default_args={",
          "1336:             \"retries\": 1,",
          "1337:             \"owner\": \"x\",",
          "1338:         },",
          "1339:     ):",
          "1340:         with TaskGroup(",
          "1341:             group_id=\"task_group\",",
          "1342:             default_args={",
          "1343:                 \"owner\": \"y\",",
          "1344:                 \"execution_timeout\": timedelta(seconds=10),",
          "1345:             },",
          "1346:         ):",
          "1347:             with TaskGroup(group_id=\"nested_task_group\"):",
          "1348:                 task = EmptyOperator(task_id=\"task\")",
          "1350:     assert task.retries == 1",
          "1351:     assert task.owner == \"y\"",
          "1352:     assert task.execution_timeout == timedelta(seconds=10)",
          "1355: def test_override_dag_default_args_in_multi_level_nested_tg():",
          "1356:     with DAG(",
          "1357:         dag_id=\"test_dag\",",
          "1358:         start_date=pendulum.parse(\"20200101\"),",
          "1359:         default_args={",
          "1360:             \"retries\": 1,",
          "1361:             \"owner\": \"x\",",
          "1362:         },",
          "1363:     ):",
          "1364:         with TaskGroup(",
          "1365:             group_id=\"task_group\",",
          "1366:             default_args={",
          "1367:                 \"owner\": \"y\",",
          "1368:                 \"execution_timeout\": timedelta(seconds=10),",
          "1369:             },",
          "1370:         ):",
          "1371:             with TaskGroup(group_id=\"first_nested_task_group\"):",
          "1372:                 with TaskGroup(group_id=\"second_nested_task_group\"):",
          "1373:                     with TaskGroup(group_id=\"third_nested_task_group\"):",
          "1374:                         task = EmptyOperator(task_id=\"task\")",
          "1376:     assert task.retries == 1",
          "1377:     assert task.owner == \"y\"",
          "1378:     assert task.execution_timeout == timedelta(seconds=10)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "3bbf6fd73aef62c2030b8babaacb4791c49ba04a",
      "candidate_info": {
        "commit_hash": "3bbf6fd73aef62c2030b8babaacb4791c49ba04a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/3bbf6fd73aef62c2030b8babaacb4791c49ba04a",
        "files": [
          "airflow/configuration.py",
          "airflow/providers/odbc/hooks/odbc.py",
          "docs/apache-airflow-providers-odbc/connections/odbc.rst",
          "docs/apache-airflow/howto/set-config.rst",
          "tests/core/test_configuration.py",
          "tests/providers/odbc/hooks/test_odbc.py"
        ],
        "message": "Control permissibility of driver config in extra from airflow.cfg (#31754)\n\nRefines https://github.com/apache/airflow/pull/31713, which disabled (by default) setting driver through extra.  Here we make it so that the flag to enable is located in airflow config instead of hook param.\n\n(cherry picked from commit 438ba41e142593f2b0916893eccbd08fbe4d277b)",
        "before_after_code_files": [
          "airflow/configuration.py||airflow/configuration.py",
          "airflow/providers/odbc/hooks/odbc.py||airflow/providers/odbc/hooks/odbc.py",
          "tests/core/test_configuration.py||tests/core/test_configuration.py",
          "tests/providers/odbc/hooks/test_odbc.py||tests/providers/odbc/hooks/test_odbc.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [
            "airflow/configuration.py||airflow/configuration.py"
          ],
          "candidate": [
            "airflow/configuration.py||airflow/configuration.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/configuration.py||airflow/configuration.py": [
          "File: airflow/configuration.py -> airflow/configuration.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "488:         )",
          "490:     def _env_var_name(self, section: str, key: str) -> str:",
          "493:     def _get_env_var_option(self, section: str, key: str):",
          "494:         # must have format AIRFLOW__{SECTION}__{KEY} (note double underscore)",
          "",
          "[Removed Lines]",
          "491:         return f\"{ENV_VAR_PREFIX}{section.upper()}__{key.upper()}\"",
          "",
          "[Added Lines]",
          "491:         return f\"{ENV_VAR_PREFIX}{section.replace('.', '_').upper()}__{key.upper()}\"",
          "",
          "---------------"
        ],
        "airflow/providers/odbc/hooks/odbc.py||airflow/providers/odbc/hooks/odbc.py": [
          "File: airflow/providers/odbc/hooks/odbc.py -> airflow/providers/odbc/hooks/odbc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30:     \"\"\"",
          "31:     Interact with odbc data sources using pyodbc.",
          "33:     See :doc:`/connections/odbc` for full documentation.",
          "34:     \"\"\"",
          "36:     DEFAULT_SQLALCHEMY_SCHEME = \"mssql+pyodbc\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33:     To configure driver, in addition to supplying as constructor arg, the following are also supported:",
          "36:           section ``providers.odbc`` section of airflow config.",
          "41:     :param args: passed to DbApiHook",
          "42:     :param database: database to use -- overrides connection ``schema``",
          "43:     :param driver: name of driver or path to driver. see above for more info",
          "44:     :param dsn: name of DSN to use.  overrides DSN supplied in connection ``extra``",
          "45:     :param connect_kwargs: keyword arguments passed to ``pyodbc.connect``",
          "46:     :param sqlalchemy_scheme: Scheme sqlalchemy connection.  Default is ``mssql+pyodbc`` Only used for",
          "47:         ``get_sqlalchemy_engine`` and ``get_sqlalchemy_connection`` methods.",
          "48:     :param kwargs: passed to DbApiHook",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "40:     hook_name = \"ODBC\"",
          "41:     supports_autocommit = True",
          "43:     def __init__(",
          "44:         self,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "58:     default_driver: str | None = None",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "102:     @property",
          "103:     def driver(self) -> str | None:",
          "104:         \"\"\"Driver from init param if given; else try to find one in connection extra.\"\"\"",
          "105:         if not self._driver:",
          "106:             driver = self.connection_extra_lower.get(\"driver\")",
          "107:             if driver:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "122:         extra_driver = self.connection_extra_lower.get(\"driver\")",
          "123:         from airflow.configuration import conf",
          "125:         if extra_driver and conf.getboolean(\"providers.odbc\", \"allow_driver_in_extra\", fallback=False):",
          "126:             self._driver = extra_driver",
          "127:         else:",
          "128:             self.log.warning(",
          "129:                 \"You have supplied 'driver' via connection extra but it will not be used. In order to \"",
          "130:                 \"use 'driver' from extra you must set airflow config setting `allow_driver_in_extra = True` \"",
          "131:                 \"in section `providers.odbc`. Alternatively you may specify driver via 'driver' parameter of \"",
          "132:                 \"the hook constructor or via 'hook_params' dictionary with key 'driver' if using SQL \"",
          "133:                 \"operators.\"",
          "134:             )",
          "",
          "---------------"
        ],
        "tests/core/test_configuration.py||tests/core/test_configuration.py": [
          "File: tests/core/test_configuration.py -> tests/core/test_configuration.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "226:         assert \"key4\" not in cfg_dict[\"test\"]",
          "227:         assert \"printf key4_result\" == cfg_dict[\"test\"][\"key4_cmd\"]",
          "229:     @mock.patch(\"airflow.providers.hashicorp._internal_client.vault_client.hvac\")",
          "230:     @conf_vars(",
          "231:         {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "229:     def test_can_read_dot_section(self):",
          "230:         test_config = \"\"\"[test.abc]",
          "231: key1 = true",
          "232: \"\"\"",
          "233:         test_conf = AirflowConfigParser()",
          "234:         test_conf.read_string(test_config)",
          "235:         section = \"test.abc\"",
          "236:         key = \"key1\"",
          "237:         assert test_conf.getboolean(section, key) is True",
          "239:         with mock.patch.dict(",
          "240:             \"os.environ\",",
          "241:             {",
          "242:                 \"AIRFLOW__TEST_ABC__KEY1\": \"false\",  # note that the '.' is converted to '_'",
          "243:             },",
          "244:         ):",
          "245:             assert test_conf.getboolean(section, key) is False",
          "",
          "---------------"
        ],
        "tests/providers/odbc/hooks/test_odbc.py||tests/providers/odbc/hooks/test_odbc.py": [
          "File: tests/providers/odbc/hooks/test_odbc.py -> tests/providers/odbc/hooks/test_odbc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "177:         assert hook.driver == \"Blah driver\"",
          "178:         hook = self.get_hook(hook_params=dict(driver=\"{Blah driver}\"))",
          "179:         assert hook.driver == \"Blah driver\"",
          "181:         assert hook.driver == \"Blah driver\"",
          "182:         hook = self.get_hook(conn_params=dict(extra='{\"driver\": \"{Blah driver}\"}'))",
          "183:         assert hook.driver == \"Blah driver\"",
          "185:     def test_database(self):",
          "186:         hook = self.get_hook(hook_params=dict(database=\"abc\"))",
          "187:         assert hook.database == \"abc\"",
          "",
          "[Removed Lines]",
          "180:         hook = self.get_hook(conn_params=dict(extra='{\"driver\": \"Blah driver\"}'))",
          "",
          "[Added Lines]",
          "181:     def test_driver_extra_raises_warning_by_default(self, caplog):",
          "182:         with caplog.at_level(logging.WARNING, logger=\"airflow.providers.odbc.hooks.test_odbc\"):",
          "183:             driver = self.get_hook(conn_params=dict(extra='{\"driver\": \"Blah driver\"}')).driver",
          "184:             assert \"You have supplied 'driver' via connection extra but it will not be used\" in caplog.text",
          "185:             assert driver is None",
          "187:     @mock.patch.dict(\"os.environ\", {\"AIRFLOW__PROVIDERS_ODBC__ALLOW_DRIVER_IN_EXTRA\": \"TRUE\"})",
          "188:     def test_driver_extra_works_when_allow_driver_extra(self):",
          "189:         hook = self.get_hook(",
          "190:             conn_params=dict(extra='{\"driver\": \"Blah driver\"}'), hook_params=dict(allow_driver_extra=True)",
          "191:         )",
          "196:     def test_driver_none_by_default(self):",
          "197:         hook = self.get_hook()",
          "198:         assert hook.driver is None",
          "200:     def test_driver_extra_raises_warning_and_returns_default_driver_by_default(self, caplog):",
          "201:         with patch.object(OdbcHook, \"default_driver\", \"Blah driver\"):",
          "202:             with caplog.at_level(logging.WARNING, logger=\"airflow.providers.odbc.hooks.test_odbc\"):",
          "203:                 driver = self.get_hook(conn_params=dict(extra='{\"driver\": \"Blah driver2\"}')).driver",
          "204:                 assert \"have supplied 'driver' via connection extra but it will not be used\" in caplog.text",
          "205:                 assert driver == \"Blah driver\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a1598ed80d392b4f986af03a302bfbd44f736ede",
      "candidate_info": {
        "commit_hash": "a1598ed80d392b4f986af03a302bfbd44f736ede",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a1598ed80d392b4f986af03a302bfbd44f736ede",
        "files": [
          "airflow/models/skipmixin.py",
          "airflow/operators/python.py",
          "airflow/operators/subdag.py",
          "tests/operators/test_python.py",
          "tests/operators/test_subdag_operator.py"
        ],
        "message": "Add the missing `map_index` to the xcom key when skipping downstream tasks (#31541)\n\n* Add the missing map_index to the xcom key when skiping downstream tasks\n\n* fix subdag operator tests\n\n* Update tests/operators/test_subdag_operator.py\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n\n---------\n\nCo-authored-by: Tzu-ping Chung <uranusjr@gmail.com>\n(cherry picked from commit e2da3151d49dae636cb6901f3d3e124a49cbf514)",
        "before_after_code_files": [
          "airflow/models/skipmixin.py||airflow/models/skipmixin.py",
          "airflow/operators/python.py||airflow/operators/python.py",
          "airflow/operators/subdag.py||airflow/operators/subdag.py",
          "tests/operators/test_python.py||tests/operators/test_python.py",
          "tests/operators/test_subdag_operator.py||tests/operators/test_subdag_operator.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/skipmixin.py||airflow/models/skipmixin.py": [
          "File: airflow/models/skipmixin.py -> airflow/models/skipmixin.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "86:         execution_date: DateTime,",
          "87:         tasks: Iterable[DAGNode],",
          "88:         session: Session = NEW_SESSION,",
          "89:     ):",
          "90:         \"\"\"",
          "91:         Sets tasks instances to skipped from the same dag run.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "89:         map_index: int = -1,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "98:         :param execution_date: execution_date",
          "99:         :param tasks: tasks to skip (not task_ids)",
          "100:         :param session: db session to use",
          "101:         \"\"\"",
          "102:         task_list = _ensure_tasks(tasks)",
          "103:         if not task_list:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "102:         :param map_index: map_index of the current task instance",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "142:                 task_id=task_id,",
          "143:                 dag_id=dag_run.dag_id,",
          "144:                 run_id=dag_run.run_id,",
          "145:                 session=session,",
          "146:             )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "147:                 map_index=map_index,",
          "",
          "---------------"
        ],
        "airflow/operators/python.py||airflow/operators/python.py": [
          "File: airflow/operators/python.py -> airflow/operators/python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "266:             if self.ignore_downstream_trigger_rules is True:",
          "267:                 self.log.info(\"Skipping all downstream tasks...\")",
          "269:             else:",
          "270:                 self.log.info(\"Skipping downstream tasks while respecting trigger rules...\")",
          "271:                 # Explicitly setting the state of the direct, downstream task(s) to \"skipped\" and letting the",
          "272:                 # Scheduler handle the remaining downstream task(s) appropriately.",
          "275:         self.log.info(\"Done.\")",
          "",
          "[Removed Lines]",
          "268:                 self.skip(dag_run, execution_date, downstream_tasks)",
          "273:                 self.skip(dag_run, execution_date, context[\"task\"].get_direct_relatives(upstream=False))",
          "",
          "[Added Lines]",
          "268:                 self.skip(dag_run, execution_date, downstream_tasks, map_index=context[\"ti\"].map_index)",
          "273:                 self.skip(",
          "274:                     dag_run,",
          "275:                     execution_date,",
          "276:                     context[\"task\"].get_direct_relatives(upstream=False),",
          "277:                     map_index=context[\"ti\"].map_index,",
          "278:                 )",
          "",
          "---------------"
        ],
        "airflow/operators/subdag.py||airflow/operators/subdag.py": [
          "File: airflow/operators/subdag.py -> airflow/operators/subdag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "226:         self.log.debug(\"Downstream task_ids %s\", downstream_tasks)",
          "228:         if downstream_tasks:",
          "231:         self.log.info(\"Done.\")",
          "",
          "[Removed Lines]",
          "229:             self.skip(context[\"dag_run\"], context[\"execution_date\"], downstream_tasks)",
          "",
          "[Added Lines]",
          "229:             self.skip(",
          "230:                 context[\"dag_run\"],",
          "231:                 context[\"execution_date\"],",
          "232:                 downstream_tasks,",
          "233:                 map_index=context[\"ti\"].map_index,",
          "234:             )",
          "",
          "---------------"
        ],
        "tests/operators/test_python.py||tests/operators/test_python.py": [
          "File: tests/operators/test_python.py -> tests/operators/test_python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: import pytest",
          "32: from slugify import slugify",
          "35: from airflow.models import DAG, DagRun, TaskInstance as TI",
          "36: from airflow.models.baseoperator import BaseOperator",
          "37: from airflow.models.taskinstance import clear_task_instances, set_current_context",
          "",
          "[Removed Lines]",
          "34: from airflow.exceptions import AirflowException, RemovedInAirflow3Warning",
          "",
          "[Added Lines]",
          "34: from airflow.decorators import task_group",
          "35: from airflow.exceptions import AirflowException, DeserializingResultError, RemovedInAirflow3Warning",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "604:         assert tis[0].xcom_pull(task_ids=short_op_push_xcom.task_id, key=\"return_value\") == \"signature\"",
          "605:         assert tis[0].xcom_pull(task_ids=short_op_no_push_xcom.task_id, key=\"return_value\") is None",
          "608: virtualenv_string_args: list[str] = []",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "608:     def test_xcom_push_skipped_tasks(self):",
          "609:         with self.dag:",
          "610:             short_op_push_xcom = ShortCircuitOperator(",
          "611:                 task_id=\"push_xcom_from_shortcircuit\", python_callable=lambda: False",
          "612:             )",
          "613:             empty_task = EmptyOperator(task_id=\"empty_task\")",
          "614:             short_op_push_xcom >> empty_task",
          "615:         dr = self.create_dag_run()",
          "616:         short_op_push_xcom.run(start_date=self.default_date, end_date=self.default_date)",
          "617:         tis = dr.get_task_instances()",
          "618:         assert tis[0].xcom_pull(task_ids=short_op_push_xcom.task_id, key=\"skipmixin_key\") == {",
          "619:             \"skipped\": [\"empty_task\"]",
          "620:         }",
          "622:     def test_mapped_xcom_push_skipped_tasks(self, session):",
          "623:         with self.dag:",
          "625:             @task_group",
          "626:             def group(x):",
          "627:                 short_op_push_xcom = ShortCircuitOperator(",
          "628:                     task_id=\"push_xcom_from_shortcircuit\",",
          "629:                     python_callable=lambda arg: arg % 2 == 0,",
          "630:                     op_kwargs={\"arg\": x},",
          "631:                 )",
          "632:                 empty_task = EmptyOperator(task_id=\"empty_task\")",
          "633:                 short_op_push_xcom >> empty_task",
          "635:             group.expand(x=[0, 1])",
          "636:         dr = self.create_dag_run()",
          "637:         decision = dr.task_instance_scheduling_decisions(session=session)",
          "638:         for ti in decision.schedulable_tis:",
          "639:             ti.run()",
          "640:         # dr.run(start_date=self.default_date, end_date=self.default_date)",
          "641:         tis = dr.get_task_instances()",
          "643:         assert (",
          "644:             tis[0].xcom_pull(task_ids=\"group.push_xcom_from_shortcircuit\", key=\"return_value\", map_indexes=0)",
          "645:             is True",
          "646:         )",
          "647:         assert (",
          "648:             tis[0].xcom_pull(task_ids=\"group.push_xcom_from_shortcircuit\", key=\"skipmixin_key\", map_indexes=0)",
          "649:             is None",
          "650:         )",
          "651:         assert tis[0].xcom_pull(",
          "652:             task_ids=\"group.push_xcom_from_shortcircuit\", key=\"skipmixin_key\", map_indexes=1",
          "653:         ) == {\"skipped\": [\"group.empty_task\"]}",
          "",
          "---------------"
        ],
        "tests/operators/test_subdag_operator.py||tests/operators/test_subdag_operator.py": [
          "File: tests/operators/test_subdag_operator.py -> tests/operators/test_subdag_operator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "333:             for task, state in zip(dummy_subdag_tasks, states)",
          "334:         ]",
          "337:         subdag_task.post_execute(context)",
          "339:         if skip_parent:",
          "341:         else:",
          "342:             mock_skip.assert_not_called()",
          "",
          "[Removed Lines]",
          "336:         context = {\"execution_date\": DEFAULT_DATE, \"dag_run\": dag_run, \"task\": subdag_task}",
          "340:             mock_skip.assert_called_once_with(context[\"dag_run\"], context[\"execution_date\"], [dummy_dag_task])",
          "",
          "[Added Lines]",
          "336:         context = {",
          "337:             \"execution_date\": DEFAULT_DATE,",
          "338:             \"dag_run\": dag_run,",
          "339:             \"task\": subdag_task,",
          "340:             \"ti\": mock.MagicMock(map_index=-1),",
          "341:         }",
          "345:             mock_skip.assert_called_once_with(",
          "346:                 context[\"dag_run\"], context[\"execution_date\"], [dummy_dag_task], map_index=-1",
          "347:             )",
          "",
          "---------------"
        ]
      }
    }
  ]
}