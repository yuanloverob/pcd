{
  "cve_id": "CVE-2023-35005",
  "cve_desc": "In Apache Airflow, some potentially sensitive values were being shown to the user in certain situations.\n\nThis vulnerability is mitigated by the fact configuration is not shown in the UI by default (only if `[webserver] expose_config` is set to `non-sensitive-only`), and not all uncensored values are actually sentitive.\n\n\nThis issue affects Apache Airflow: from 2.5.0 before 2.6.2. Users are recommended to update to version 2.6.2 or later.\n\n\n",
  "repo": "apache/airflow",
  "patch_hash": "f6cda8fb63250fc4700658999739c1c3c5f6625c",
  "patch_info": {
    "commit_hash": "f6cda8fb63250fc4700658999739c1c3c5f6625c",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/f6cda8fb63250fc4700658999739c1c3c5f6625c",
    "files": [
      "airflow/configuration.py"
    ],
    "message": "Mark `[secrets] backend_kwargs` as a sensitive config (#31788)\n\n(cherry picked from commit 8062756fa9e01eeeee1f2c6df74f376c0a526bd5)",
    "before_after_code_files": [
      "airflow/configuration.py||airflow/configuration.py"
    ]
  },
  "patch_diff": {
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "156:     (\"atlas\", \"password\"),",
      "157:     (\"smtp\", \"smtp_password\"),",
      "158:     (\"webserver\", \"secret_key\"),",
      "159:     # The following options are deprecated",
      "160:     (\"core\", \"sql_alchemy_conn\"),",
      "161: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "159:     (\"secrets\", \"backend_kwargs\"),",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "e19ff6f620a2d0d682a2b243593119ca5eb501fb",
      "candidate_info": {
        "commit_hash": "e19ff6f620a2d0d682a2b243593119ca5eb501fb",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e19ff6f620a2d0d682a2b243593119ca5eb501fb",
        "files": [
          "airflow/migrations/versions/0125_2_6_2_add_onupdate_cascade_to_taskmap.py",
          "airflow/models/taskmap.py",
          "docs/apache-airflow/img/airflow_erd.sha256",
          "docs/apache-airflow/img/airflow_erd.svg",
          "docs/apache-airflow/migrations-ref.rst",
          "tests/models/test_taskinstance.py"
        ],
        "message": "Cascade update of taskinstance to TaskMap table (#31445)\n\n(cherry picked from commit f6bb4746efbc6a94fa17b6c77b31d9fb17305ffc)",
        "before_after_code_files": [
          "airflow/migrations/versions/0125_2_6_2_add_onupdate_cascade_to_taskmap.py||airflow/migrations/versions/0125_2_6_2_add_onupdate_cascade_to_taskmap.py",
          "airflow/models/taskmap.py||airflow/models/taskmap.py",
          "tests/models/test_taskinstance.py||tests/models/test_taskinstance.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/migrations/versions/0125_2_6_2_add_onupdate_cascade_to_taskmap.py||airflow/migrations/versions/0125_2_6_2_add_onupdate_cascade_to_taskmap.py": [
          "File: airflow/migrations/versions/0125_2_6_2_add_onupdate_cascade_to_taskmap.py -> airflow/migrations/versions/0125_2_6_2_add_onupdate_cascade_to_taskmap.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "19: \"\"\"Add ``onupdate`` cascade to ``task_map`` table",
          "21: Revision ID: c804e5c76e3e",
          "22: Revises: 98ae134e6fff",
          "23: Create Date: 2023-05-19 23:30:57.368617",
          "25: \"\"\"",
          "26: from __future__ import annotations",
          "28: from alembic import op",
          "30: # revision identifiers, used by Alembic.",
          "31: revision = \"c804e5c76e3e\"",
          "32: down_revision = \"98ae134e6fff\"",
          "33: branch_labels = None",
          "34: depends_on = None",
          "35: airflow_version = \"2.6.2\"",
          "38: def upgrade():",
          "39:     \"\"\"Apply Add onupdate cascade to taskmap\"\"\"",
          "40:     with op.batch_alter_table(\"task_map\") as batch_op:",
          "41:         batch_op.drop_constraint(\"task_map_task_instance_fkey\", type_=\"foreignkey\")",
          "42:         batch_op.create_foreign_key(",
          "43:             \"task_map_task_instance_fkey\",",
          "44:             \"task_instance\",",
          "45:             [\"dag_id\", \"task_id\", \"run_id\", \"map_index\"],",
          "46:             [\"dag_id\", \"task_id\", \"run_id\", \"map_index\"],",
          "47:             ondelete=\"CASCADE\",",
          "48:             onupdate=\"CASCADE\",",
          "49:         )",
          "52: def downgrade():",
          "53:     \"\"\"Unapply Add onupdate cascade to taskmap\"\"\"",
          "54:     with op.batch_alter_table(\"task_map\") as batch_op:",
          "55:         batch_op.drop_constraint(\"task_map_task_instance_fkey\", type_=\"foreignkey\")",
          "56:         batch_op.create_foreign_key(",
          "57:             \"task_map_task_instance_fkey\",",
          "58:             \"task_instance\",",
          "59:             [\"dag_id\", \"task_id\", \"run_id\", \"map_index\"],",
          "60:             [\"dag_id\", \"task_id\", \"run_id\", \"map_index\"],",
          "61:             ondelete=\"CASCADE\",",
          "62:         )",
          "",
          "---------------"
        ],
        "airflow/models/taskmap.py||airflow/models/taskmap.py": [
          "File: airflow/models/taskmap.py -> airflow/models/taskmap.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "72:             ],",
          "73:             name=\"task_map_task_instance_fkey\",",
          "74:             ondelete=\"CASCADE\",",
          "75:         ),",
          "76:     )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "75:             onupdate=\"CASCADE\",",
          "",
          "---------------"
        ],
        "tests/models/test_taskinstance.py||tests/models/test_taskinstance.py": [
          "File: tests/models/test_taskinstance.py -> tests/models/test_taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "69: from airflow.operators.python import PythonOperator",
          "70: from airflow.sensors.base import BaseSensorOperator",
          "71: from airflow.sensors.python import PythonSensor",
          "73: from airflow.settings import TIMEZONE",
          "74: from airflow.stats import Stats",
          "75: from airflow.ti_deps.dep_context import DepContext",
          "",
          "[Removed Lines]",
          "72: from airflow.serialization.serialized_objects import SerializedBaseOperator",
          "",
          "[Added Lines]",
          "72: from airflow.serialization.serialized_objects import SerializedBaseOperator, SerializedDAG",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "3484:         assert task_map.length == expected_length",
          "3485:         assert task_map.keys == expected_keys",
          "3488: class TestMappedTaskInstanceReceiveValue:",
          "3489:     @pytest.mark.parametrize(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3487:     def test_no_error_on_changing_from_non_mapped_to_mapped(self, dag_maker, session):",
          "3488:         \"\"\"If a task changes from non-mapped to mapped, don't fail on integrity error.\"\"\"",
          "3489:         with dag_maker(dag_id=\"test_no_error_on_changing_from_non_mapped_to_mapped\") as dag:",
          "3491:             @dag.task()",
          "3492:             def add_one(x):",
          "3493:                 return [x + 1]",
          "3495:             @dag.task()",
          "3496:             def add_two(x):",
          "3497:                 return x + 2",
          "3499:             task1 = add_one(2)",
          "3500:             add_two.expand(x=task1)",
          "3502:         dr = dag_maker.create_dagrun()",
          "3503:         ti = dr.get_task_instance(task_id=\"add_one\")",
          "3504:         ti.run()",
          "3505:         assert ti.state == TaskInstanceState.SUCCESS",
          "3506:         dag._remove_task(\"add_one\")",
          "3507:         with dag:",
          "3508:             task1 = add_one.expand(x=[1, 2, 3]).operator",
          "3509:         serialized_dag = SerializedDAG.from_dict(SerializedDAG.to_dict(dag))",
          "3511:         dr.dag = serialized_dag",
          "3512:         dr.verify_integrity(session=session)",
          "3513:         ti = dr.get_task_instance(task_id=\"add_one\")",
          "3514:         assert ti.state == TaskInstanceState.REMOVED",
          "3515:         dag.clear()",
          "3516:         ti.refresh_from_task(task1)",
          "3517:         # This should not raise an integrity error",
          "3518:         dr.task_instance_scheduling_decisions()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f529d150aaa484b7ebebeb1c1e15a2b2cc89c0c5",
      "candidate_info": {
        "commit_hash": "f529d150aaa484b7ebebeb1c1e15a2b2cc89c0c5",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f529d150aaa484b7ebebeb1c1e15a2b2cc89c0c5",
        "files": [
          "airflow/example_dags/example_params_ui_tutorial.py",
          "airflow/www/templates/airflow/trigger.html"
        ],
        "message": "Fix dropdown default and adjust tutorial to use 42 as default for proof (#31400)\n\n(cherry picked from commit 58aab1118a95ef63ba00784760fd13730dd46501)",
        "before_after_code_files": [
          "airflow/example_dags/example_params_ui_tutorial.py||airflow/example_dags/example_params_ui_tutorial.py",
          "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/example_dags/example_params_ui_tutorial.py||airflow/example_dags/example_params_ui_tutorial.py": [
          "File: airflow/example_dags/example_params_ui_tutorial.py -> airflow/example_dags/example_params_ui_tutorial.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "65:         ),",
          "66:         # If you want to have a selection list box then you can use the enum feature of JSON schema",
          "67:         \"pick_one\": Param(",
          "69:             type=\"string\",",
          "70:             title=\"Select one Value\",",
          "71:             description=\"You can use JSON schema enum's to generate drop down selection boxes.\",",
          "73:         ),",
          "74:         # Boolean as proper parameter with description",
          "75:         \"bool\": Param(",
          "",
          "[Removed Lines]",
          "68:             \"value 1\",",
          "72:             enum=[f\"value {i}\" for i in range(1, 42)],",
          "",
          "[Added Lines]",
          "68:             \"value 42\",",
          "72:             enum=[f\"value {i}\" for i in range(16, 64)],",
          "",
          "---------------"
        ],
        "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html": [
          "File: airflow/www/templates/airflow/trigger.html -> airflow/www/templates/airflow/trigger.html",
          "--- Hunk 1 ---",
          "[Context before]",
          "71:       </div>",
          "72:     {% elif \"enum\" in form_details.schema and form_details.schema.enum %}",
          "73:       <select class=\"my_select2 form-control\" name=\"element_{{ form_key }}\" id=\"element_{{ form_key }}\" data-placeholder=\"Select Value\"",
          "75:         {%- if not \"null\" in form_details.schema.type %} required=\"\"{% endif %}>",
          "76:         {% for option in form_details.schema.enum -%}",
          "78:         {% endfor -%}",
          "79:       </select>",
          "80:     {% elif form_details.schema and \"array\" in form_details.schema.type %}",
          "",
          "[Removed Lines]",
          "74:         value=\"{% if form_details.value %}{{ form_details.value }}{% endif %}\" onchange=\"updateJSONconf();\"",
          "77:         <option>{{ option }}</option>",
          "",
          "[Added Lines]",
          "74:         onchange=\"updateJSONconf();\"",
          "77:         <option{% if form_details.value == option %} selected=\"true\"{% endif %}>{{ option }}</option>",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e9721441977d0eed53199abe836f412a290c2f91",
      "candidate_info": {
        "commit_hash": "e9721441977d0eed53199abe836f412a290c2f91",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/e9721441977d0eed53199abe836f412a290c2f91",
        "files": [
          "airflow/jobs/scheduler_job_runner.py",
          "tests/jobs/test_scheduler_job.py"
        ],
        "message": "Save scheduler execution time during search for queued dag_runs (#30699)\n\n* Function returns list of dagruns and not query\n\n* Changed pytests\n\n* Changed all to _start_queued_dagruns\n\n* Added comment and fixed tests\n\n* Fixed typo\n\n(cherry picked from commit 0fd42ff015be02d1a6a6c2e1a080f8267194b3a5)",
        "before_after_code_files": [
          "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py",
          "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py": [
          "File: airflow/jobs/scheduler_job_runner.py -> airflow/jobs/scheduler_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1299:     def _start_queued_dagruns(self, session: Session) -> None:",
          "1300:         \"\"\"Find DagRuns in queued state and decide moving them to running state.\"\"\"",
          "1303:         active_runs_of_dags = Counter(",
          "1304:             DagRun.active_runs_of_dags((dr.dag_id for dr in dag_runs), only_running=True, session=session),",
          "",
          "[Removed Lines]",
          "1301:         dag_runs = self._get_next_dagruns_to_examine(DagRunState.QUEUED, session)",
          "",
          "[Added Lines]",
          "1301:         # added all() to save runtime, otherwise query is executed more than once",
          "1302:         dag_runs: Collection[DagRun] = self._get_next_dagruns_to_examine(DagRunState.QUEUED, session).all()",
          "",
          "---------------"
        ],
        "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py": [
          "File: tests/jobs/test_scheduler_job.py -> tests/jobs/test_scheduler_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "5144:             with assert_queries_count(expected_query_count, margin=15):",
          "5145:                 with mock.patch.object(DagRun, \"next_dagruns_to_examine\") as mock_dagruns:",
          "5148:                     self.job_runner._run_scheduler_loop()",
          "",
          "[Removed Lines]",
          "5146:                     mock_dagruns.return_value = dagruns",
          "",
          "[Added Lines]",
          "5146:                     query = MagicMock()",
          "5147:                     query.all.return_value = dagruns",
          "5148:                     mock_dagruns.return_value = query",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8ba85baa3a7008ece44d7c108344a5f0ecd61e97",
      "candidate_info": {
        "commit_hash": "8ba85baa3a7008ece44d7c108344a5f0ecd61e97",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8ba85baa3a7008ece44d7c108344a5f0ecd61e97",
        "files": [
          "tests/www/views/test_views_acl.py"
        ],
        "message": "Fix failing AIP-52 test",
        "before_after_code_files": [
          "tests/www/views/test_views_acl.py||tests/www/views/test_views_acl.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/www/views/test_views_acl.py||tests/www/views/test_views_acl.py": [
          "File: tests/www/views/test_views_acl.py -> tests/www/views/test_views_acl.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: from airflow.models import DagModel",
          "27: from airflow.security import permissions",
          "29: from airflow.utils import timezone",
          "30: from airflow.utils.session import create_session",
          "31: from airflow.utils.state import State",
          "",
          "[Removed Lines]",
          "28: from airflow.settings import _ENABLE_AIP_52",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "258:         {\"name\": \"tutorial_taskflow_api\", \"type\": \"dag\"},",
          "259:         {\"name\": \"tutorial_taskflow_api_virtualenv\", \"type\": \"dag\"},",
          "260:     ]",
          "264:     assert resp.json == expected",
          "",
          "[Removed Lines]",
          "261:     if _ENABLE_AIP_52:",
          "262:         expected.insert(1, {\"name\": \"example_setup_teardown_taskflow\", \"type\": \"dag\"})",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7295524620aeb15ce15b2f33091e3100e0d13d39",
      "candidate_info": {
        "commit_hash": "7295524620aeb15ce15b2f33091e3100e0d13d39",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7295524620aeb15ce15b2f33091e3100e0d13d39",
        "files": [
          "airflow/www/views.py"
        ],
        "message": "Fix sorting of tags (#31553)\n\n* Fix sorting of tags\n\n* Change sorting in views.py\n\n* Add ordering into query\n\n(cherry picked from commit 24e52f92bd9305bf534c411f9455460060515ea7)",
        "before_after_code_files": [
          "airflow/www/views.py||airflow/www/views.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "806:                 dag.can_trigger = dag.can_edit and can_create_dag_run",
          "807:                 dag.can_delete = get_airflow_app().appbuilder.sm.can_delete_dag(dag.dag_id, g.user)",
          "810:             tags = [",
          "811:                 {\"name\": name, \"selected\": bool(arg_tags_filter and name in arg_tags_filter)}",
          "812:                 for name, in dagtags",
          "",
          "[Removed Lines]",
          "809:             dagtags = session.query(func.distinct(DagTag.name)).all()",
          "",
          "[Added Lines]",
          "809:             dagtags = session.query(func.distinct(DagTag.name)).order_by(DagTag.name).all()",
          "",
          "---------------"
        ]
      }
    }
  ]
}