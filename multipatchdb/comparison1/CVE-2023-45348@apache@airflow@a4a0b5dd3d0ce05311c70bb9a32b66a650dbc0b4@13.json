{
  "cve_id": "CVE-2023-45348",
  "cve_desc": "Apache Airflow, versions 2.7.0 and 2.7.1, is affected by a vulnerability that allows an authenticated user to retrieve sensitive configuration information when the \"expose_config\" option is set to \"non-sensitive-only\". The `expose_config` option is False by default.\nIt is recommended to upgrade to a version that is not affected.",
  "repo": "apache/airflow",
  "patch_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
  "patch_info": {
    "commit_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
    "files": [
      "airflow/api_connexion/endpoints/config_endpoint.py",
      "airflow/configuration.py",
      "tests/api_connexion/endpoints/test_config_endpoint.py"
    ],
    "message": "Check if the lower of provided values are sensitives in config endpoint (#34712)\n\n* Check if the lower of provided values are sensitives in config endpoint\n\n* update unit test\n\n* ensure that all values in sensitive dict are in lower characters\n\n(cherry picked from commit f044589b685855a8fce8f5376bea2564c5a001f7)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py",
      "airflow/configuration.py||airflow/configuration.py",
      "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py": [
      "File: airflow/api_connexion/endpoints/config_endpoint.py -> airflow/api_connexion/endpoints/config_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "123:                 \"Config not found.\", detail=f\"The option [{section}/{option}] is not found in config.\"",
      "124:             )",
      "127:             value = \"< hidden >\"",
      "128:         else:",
      "129:             value = conf.get(section, option)",
      "",
      "[Removed Lines]",
      "126:         if (section, option) in conf.sensitive_config_values:",
      "",
      "[Added Lines]",
      "126:         if (section.lower(), option.lower()) in conf.sensitive_config_values:",
      "",
      "---------------"
    ],
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "311:             for s, s_c in self.configuration_description.items()",
      "312:             for k, item in s_c.get(\"options\").items()  # type: ignore[union-attr]",
      "313:         }",
      "315:         depr_option = {self.deprecated_options[x][:-1] for x in sensitive if x in self.deprecated_options}",
      "316:         depr_section = {",
      "317:             (self.deprecated_sections[s][0], k) for s, k in sensitive if s in self.deprecated_sections",
      "",
      "[Removed Lines]",
      "314:         sensitive = {(section, key) for (section, key), v in flattened.items() if v.get(\"sensitive\") is True}",
      "",
      "[Added Lines]",
      "314:         sensitive = {",
      "315:             (section.lower(), key.lower())",
      "316:             for (section, key), v in flattened.items()",
      "317:             if v.get(\"sensitive\") is True",
      "318:         }",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_config_endpoint.py -> tests/api_connexion/endpoints/test_config_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "247:         return_value=MOCK_CONF_WITH_SENSITIVE_VALUE,",
      "248:     )",
      "249:     @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "251:         response = self.client.get(",
      "253:             headers={\"Accept\": \"text/plain\"},",
      "254:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "255:         )",
      "256:         assert response.status_code == 200",
      "257:         expected = textwrap.dedent(",
      "261:         \"\"\"",
      "262:         )",
      "263:         assert expected == response.data.decode()",
      "",
      "[Removed Lines]",
      "250:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict):",
      "252:             \"/api/v1/config/section/core/option/sql_alchemy_conn\",",
      "258:             \"\"\"\\",
      "259:         [core]",
      "260:         sql_alchemy_conn = < hidden >",
      "",
      "[Added Lines]",
      "250:     @pytest.mark.parametrize(",
      "251:         \"section, option\",",
      "252:         [",
      "253:             (\"core\", \"sql_alchemy_conn\"),",
      "254:             (\"core\", \"SQL_ALCHEMY_CONN\"),",
      "255:             (\"corE\", \"sql_alchemy_conn\"),",
      "256:             (\"CORE\", \"sql_alchemy_conn\"),",
      "257:         ],",
      "258:     )",
      "259:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict, section, option):",
      "261:             f\"/api/v1/config/section/{section}/option/{option}\",",
      "267:             f\"\"\"\\",
      "268:         [{section}]",
      "269:         {option} = < hidden >",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "df605262940d8c74b6da0d0980e5fe34ffc6042e",
      "candidate_info": {
        "commit_hash": "df605262940d8c74b6da0d0980e5fe34ffc6042e",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/df605262940d8c74b6da0d0980e5fe34ffc6042e",
        "files": [
          "airflow/dag_processing/manager.py",
          "airflow/dag_processing/processor.py",
          "airflow/example_dags/plugins/workday.py"
        ],
        "message": "Refactor unneeded 'continue' jumps in dag processing (#33839)\n\n(cherry picked from commit 562357528d85a1d1a3054b2f7b88511e1c5337f4)",
        "before_after_code_files": [
          "airflow/dag_processing/manager.py||airflow/dag_processing/manager.py",
          "airflow/dag_processing/processor.py||airflow/dag_processing/processor.py",
          "airflow/example_dags/plugins/workday.py||airflow/example_dags/plugins/workday.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/dag_processing/manager.py||airflow/dag_processing/manager.py": [
          "File: airflow/dag_processing/manager.py -> airflow/dag_processing/manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "607:                 continue",
          "609:             for sentinel in ready:",
          "621:             if self.standalone_dag_processor:",
          "622:                 self._fetch_callbacks(max_callbacks_per_loop)",
          "",
          "[Removed Lines]",
          "610:                 if sentinel is self._direct_scheduler_conn:",
          "611:                     continue",
          "613:                 processor = self.waitables.get(sentinel)",
          "614:                 if not processor:",
          "615:                     continue",
          "617:                 self._collect_results_from_processor(processor)",
          "618:                 self.waitables.pop(sentinel)",
          "619:                 self._processors.pop(processor.file_path)",
          "",
          "[Added Lines]",
          "610:                 if sentinel is not self._direct_scheduler_conn:",
          "611:                     processor = self.waitables.get(sentinel)",
          "612:                     if processor:",
          "613:                         self._collect_results_from_processor(processor)",
          "614:                         self.waitables.pop(sentinel)",
          "615:                         self._processors.pop(processor.file_path)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1055:         )",
          "1057:         for sentinel in ready:",
          "1065:         self.log.debug(\"%s/%s DAG parsing processes running\", len(self._processors), self._parallelism)",
          "",
          "[Removed Lines]",
          "1058:             if sentinel is self._direct_scheduler_conn:",
          "1059:                 continue",
          "1060:             processor = cast(DagFileProcessorProcess, self.waitables[sentinel])",
          "1061:             self.waitables.pop(processor.waitable_handle)",
          "1062:             self._processors.pop(processor.file_path)",
          "1063:             self._collect_results_from_processor(processor)",
          "",
          "[Added Lines]",
          "1054:             if sentinel is not self._direct_scheduler_conn:",
          "1055:                 processor = cast(DagFileProcessorProcess, self.waitables[sentinel])",
          "1056:                 self.waitables.pop(processor.waitable_handle)",
          "1057:                 self._processors.pop(processor.file_path)",
          "1058:                 self._collect_results_from_processor(processor)",
          "",
          "---------------"
        ],
        "airflow/dag_processing/processor.py||airflow/dag_processing/processor.py": [
          "File: airflow/dag_processing/processor.py -> airflow/dag_processing/processor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "566:                     cls.logger().warning(",
          "567:                         \"Task %s doesn't exist in DAG anymore, skipping SLA miss notification.\", sla.task_id",
          "568:                     )",
          "572:             emails: set[str] = set()",
          "573:             for task in tasks_missed_sla:",
          "",
          "[Removed Lines]",
          "569:                     continue",
          "570:                 tasks_missed_sla.append(task)",
          "",
          "[Added Lines]",
          "569:                 else:",
          "570:                     tasks_missed_sla.append(task)",
          "",
          "---------------"
        ],
        "airflow/example_dags/plugins/workday.py||airflow/example_dags/plugins/workday.py": [
          "File: airflow/example_dags/plugins/workday.py -> airflow/example_dags/plugins/workday.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "41:     def get_next_workday(self, d: DateTime, incr=1) -> DateTime:",
          "42:         next_start = d",
          "43:         while True:",
          "53:         return next_start",
          "55:     # [START howto_timetable_infer_manual_data_interval]",
          "",
          "[Removed Lines]",
          "44:             if next_start.weekday() in (5, 6):  # If next start is in the weekend go to next day",
          "45:                 next_start = next_start + incr * timedelta(days=1)",
          "46:                 continue",
          "47:             if holiday_calendar is not None:",
          "48:                 holidays = holiday_calendar.holidays(start=next_start, end=next_start).to_pydatetime()",
          "49:                 if next_start in holidays:  # If next start is a holiday go to next day",
          "50:                     next_start = next_start + incr * timedelta(days=1)",
          "51:                     continue",
          "52:             break",
          "",
          "[Added Lines]",
          "44:             if next_start.weekday() not in (5, 6):  # not on weekend",
          "45:                 if holiday_calendar is None:",
          "46:                     holidays = set()",
          "47:                 else:",
          "48:                     holidays = holiday_calendar.holidays(start=next_start, end=next_start).to_pydatetime()",
          "49:                 if next_start not in holidays:",
          "50:                     break",
          "51:             next_start = next_start.add(days=incr)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "21c59d7eede162bec02574d19955d0105b6e59af",
      "candidate_info": {
        "commit_hash": "21c59d7eede162bec02574d19955d0105b6e59af",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/21c59d7eede162bec02574d19955d0105b6e59af",
        "files": [
          "README.md",
          "airflow/__init__.py",
          "airflow/api_connexion/openapi/v1.yaml",
          "docs/apache-airflow/installation/supported-versions.rst",
          "docs/docker-stack/README.md",
          "docs/docker-stack/docker-examples/extending/add-airflow-configuration/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-apt-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-build-essential-extend/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-providers/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-pypi-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-requirement-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/custom-providers/Dockerfile",
          "docs/docker-stack/docker-examples/extending/embedding-dags/Dockerfile",
          "docs/docker-stack/docker-examples/extending/writable-directory/Dockerfile",
          "docs/docker-stack/entrypoint.rst",
          "generated/PYPI_README.md",
          "scripts/ci/pre_commit/pre_commit_supported_versions.py"
        ],
        "message": "Update version to 2.7.2",
        "before_after_code_files": [
          "airflow/__init__.py||airflow/__init__.py",
          "scripts/ci/pre_commit/pre_commit_supported_versions.py||scripts/ci/pre_commit/pre_commit_supported_versions.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/__init__.py||airflow/__init__.py": [
          "File: airflow/__init__.py -> airflow/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: \"\"\"",
          "27: from __future__ import annotations",
          "31: # flake8: noqa: F401",
          "",
          "[Removed Lines]",
          "29: __version__ = \"2.7.1\"",
          "",
          "[Added Lines]",
          "29: __version__ = \"2.7.2\"",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_supported_versions.py||scripts/ci/pre_commit/pre_commit_supported_versions.py": [
          "File: scripts/ci/pre_commit/pre_commit_supported_versions.py -> scripts/ci/pre_commit/pre_commit_supported_versions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: HEADERS = (\"Version\", \"Current Patch/Minor\", \"State\", \"First Release\", \"Limited Support\", \"EOL/Terminated\")",
          "29: SUPPORTED_VERSIONS = (",
          "31:     (\"1.10\", \"1.10.15\", \"EOL\", \"Aug 27, 2018\", \"Dec 17, 2020\", \"June 17, 2021\"),",
          "32:     (\"1.9\", \"1.9.0\", \"EOL\", \"Jan 03, 2018\", \"Aug 27, 2018\", \"Aug 27, 2018\"),",
          "33:     (\"1.8\", \"1.8.2\", \"EOL\", \"Mar 19, 2017\", \"Jan 03, 2018\", \"Jan 03, 2018\"),",
          "",
          "[Removed Lines]",
          "30:     (\"2\", \"2.7.1\", \"Supported\", \"Dec 17, 2020\", \"TBD\", \"TBD\"),",
          "",
          "[Added Lines]",
          "30:     (\"2\", \"2.7.2\", \"Supported\", \"Dec 17, 2020\", \"TBD\", \"TBD\"),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "53564e9f1d92cf5e08e60386b92d62167f83821f",
      "candidate_info": {
        "commit_hash": "53564e9f1d92cf5e08e60386b92d62167f83821f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/53564e9f1d92cf5e08e60386b92d62167f83821f",
        "files": [
          "airflow/decorators/base.py"
        ],
        "message": "replace = by is for type comparaison (#33983)\n\n(cherry picked from commit 8b6ab5a392f816c19ed48d24621a5390059d1109)",
        "before_after_code_files": [
          "airflow/decorators/base.py||airflow/decorators/base.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/decorators/base.py||airflow/decorators/base.py": [
          "File: airflow/decorators/base.py -> airflow/decorators/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "330:         except TypeError:  # Can't evaluate return type.",
          "331:             return False",
          "332:         ttype = getattr(return_type, \"__origin__\", return_type)",
          "335:     def __attrs_post_init__(self):",
          "336:         if \"self\" in self.function_signature.parameters:",
          "",
          "[Removed Lines]",
          "333:         return ttype == dict or ttype == Dict",
          "",
          "[Added Lines]",
          "333:         return ttype is dict or ttype is Dict",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bf07d012df951e409b783a9eaa5a157e2a01ffcf",
      "candidate_info": {
        "commit_hash": "bf07d012df951e409b783a9eaa5a157e2a01ffcf",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/bf07d012df951e409b783a9eaa5a157e2a01ffcf",
        "files": [
          "airflow/utils/dates.py",
          "dev/assign_cherry_picked_prs_with_milestone.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/utils/github.py",
          "dev/breeze/src/airflow_breeze/utils/parallel.py",
          "dev/example_dags/update_example_dags_paths.py",
          "kubernetes_tests/test_kubernetes_pod_operator.py",
          "setup.py"
        ],
        "message": "Replace lambdas with comprehensions (#33745)\n\n(cherry picked from commit 9a0f54dbf9ddb4b556c66e3d24bf98a0b8c607e8)",
        "before_after_code_files": [
          "airflow/utils/dates.py||airflow/utils/dates.py",
          "dev/assign_cherry_picked_prs_with_milestone.py||dev/assign_cherry_picked_prs_with_milestone.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/utils/github.py||dev/breeze/src/airflow_breeze/utils/github.py",
          "dev/breeze/src/airflow_breeze/utils/parallel.py||dev/breeze/src/airflow_breeze/utils/parallel.py",
          "dev/example_dags/update_example_dags_paths.py||dev/example_dags/update_example_dags_paths.py",
          "kubernetes_tests/test_kubernetes_pod_operator.py||kubernetes_tests/test_kubernetes_pod_operator.py",
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/dates.py||airflow/utils/dates.py": [
          "File: airflow/utils/dates.py -> airflow/utils/dates.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "243: def scale_time_units(time_seconds_arr: Collection[float], unit: TimeUnit) -> Collection[float]:",
          "244:     \"\"\"Convert an array of time durations in seconds to the specified time unit.\"\"\"",
          "245:     if unit == \"minutes\":",
          "247:     elif unit == \"hours\":",
          "249:     elif unit == \"days\":",
          "254: def days_ago(n, hour=0, minute=0, second=0, microsecond=0):",
          "",
          "[Removed Lines]",
          "246:         return [x / 60 for x in time_seconds_arr]",
          "248:         return [x / (60 * 60) for x in time_seconds_arr]",
          "250:         return [x / (24 * 60 * 60) for x in time_seconds_arr]",
          "251:     return time_seconds_arr",
          "",
          "[Added Lines]",
          "246:         factor = 60",
          "248:         factor = 60 * 60",
          "250:         factor = 24 * 60 * 60",
          "251:     else:",
          "252:         factor = 1",
          "253:     return [x / factor for x in time_seconds_arr]",
          "",
          "---------------"
        ],
        "dev/assign_cherry_picked_prs_with_milestone.py||dev/assign_cherry_picked_prs_with_milestone.py": [
          "File: dev/assign_cherry_picked_prs_with_milestone.py -> dev/assign_cherry_picked_prs_with_milestone.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "261:     output_folder: str,",
          "262: ):",
          "263:     changes = get_changes(verbose, previous_release, current_release)",
          "265:     prs = [change.pr for change in changes]",
          "267:     g = Github(github_token)",
          "",
          "[Removed Lines]",
          "264:     changes = list(filter(lambda change: change.pr is not None, changes))",
          "",
          "[Added Lines]",
          "264:     changes = [change for change in changes if change.pr is not None]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1148:                 )",
          "1149:                 continue",
          "1150:             prs = get_prs_for_package(package_id)",
          "1152:             all_prs.update(provider_prs[package_id])",
          "1153:         g = Github(github_token)",
          "1154:         repo = g.get_repo(\"apache/airflow\")",
          "",
          "[Removed Lines]",
          "1151:             provider_prs[package_id] = list(filter(lambda pr: pr not in excluded_prs, prs))",
          "",
          "[Added Lines]",
          "1151:             provider_prs[package_id] = [pr for pr in prs if pr not in excluded_prs]",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/github.py||dev/breeze/src/airflow_breeze/utils/github.py": [
          "File: dev/breeze/src/airflow_breeze/utils/github.py -> dev/breeze/src/airflow_breeze/utils/github.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "100:         match = ACTIVE_TAG_MATCH.match(tag)",
          "101:         if match and match.group(1) == \"2\":",
          "102:             all_active_tags.append(tag)",
          "104:     if confirm:",
          "105:         get_console().print(f\"All Airflow 2 versions: {all_active_tags}\")",
          "106:         answer = user_confirm(",
          "",
          "[Removed Lines]",
          "103:     airflow_versions = sorted(all_active_tags, key=lambda x: Version(x))",
          "",
          "[Added Lines]",
          "103:     airflow_versions = sorted(all_active_tags, key=Version)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/parallel.py||dev/breeze/src/airflow_breeze/utils/parallel.py": [
          "File: dev/breeze/src/airflow_breeze/utils/parallel.py -> dev/breeze/src/airflow_breeze/utils/parallel.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "358: def get_completed_result_list(results: list[ApplyResult]) -> list[ApplyResult]:",
          "359:     \"\"\"Return completed results from the list.\"\"\"",
          "363: class SummarizeAfter(Enum):",
          "",
          "[Removed Lines]",
          "360:     return list(filter(lambda result: result.ready(), results))",
          "",
          "[Added Lines]",
          "360:     return [result for result in results if result.ready()]",
          "",
          "---------------"
        ],
        "dev/example_dags/update_example_dags_paths.py||dev/example_dags/update_example_dags_paths.py": [
          "File: dev/example_dags/update_example_dags_paths.py -> dev/example_dags/update_example_dags_paths.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "103: if __name__ == \"__main__\":",
          "104:     curdir: Path = Path(os.curdir).resolve()",
          "106:     with Progress(console=console) as progress:",
          "107:         task = progress.add_task(f\"Updating {len(dirs)}\", total=len(dirs))",
          "108:         for directory in dirs:",
          "109:             if directory.name.startswith(\"apache-airflow-providers-\"):",
          "110:                 provider = directory.name[len(\"apache-airflow-providers-\") :]",
          "111:                 console.print(f\"[bright_blue] Processing {directory}\")",
          "120:             progress.advance(task)",
          "",
          "[Removed Lines]",
          "105:     dirs: list[Path] = list(filter(os.path.isdir, curdir.iterdir()))",
          "112:                 version_dirs = list(filter(os.path.isdir, directory.iterdir()))",
          "113:                 for version_dir in version_dirs:",
          "114:                     version = version_dir.name",
          "115:                     console.print(version)",
          "116:                     for file in version_dir.rglob(\"*.html\"):",
          "117:                         candidate_file = file",
          "118:                         if candidate_file.exists():",
          "119:                             find_matches(candidate_file, provider, version)",
          "",
          "[Added Lines]",
          "105:     dirs: list[Path] = [p for p in curdir.iterdir() if p.is_dir()]",
          "112:                 for version_dir in directory.iterdir():",
          "113:                     if version_dir.is_dir():",
          "114:                         console.print(version_dir.name)",
          "115:                         for candidate_file in version_dir.rglob(\"*.html\"):",
          "116:                             if candidate_file.exists():",
          "117:                                 find_matches(candidate_file, provider, version_dir.name)",
          "",
          "---------------"
        ],
        "kubernetes_tests/test_kubernetes_pod_operator.py||kubernetes_tests/test_kubernetes_pod_operator.py": [
          "File: kubernetes_tests/test_kubernetes_pod_operator.py -> kubernetes_tests/test_kubernetes_pod_operator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "79: @pytest.fixture",
          "80: def test_label(request):",
          "82:     return label[-63:]",
          "",
          "[Removed Lines]",
          "81:     label = \"\".join(filter(str.isalnum, f\"{request.node.cls.__name__}.{request.node.name}\")).lower()",
          "",
          "[Added Lines]",
          "81:     label = \"\".join(c for c in f\"{request.node.cls.__name__}.{request.node.name}\" if c.isalnum()).lower()",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "266:             k.execute(context)",
          "267:         actual_pod = k.find_pod(\"default\", context, exclude_checked=False)",
          "268:         actual_pod = self.api_client.sanitize_for_serialization(actual_pod)",
          "270:         assert status[\"state\"][\"terminated\"][\"reason\"] == \"Error\"",
          "271:         assert actual_pod[\"metadata\"][\"labels\"][\"already_checked\"] == \"True\"",
          "",
          "[Removed Lines]",
          "269:         status = next(iter(filter(lambda x: x[\"name\"] == \"base\", actual_pod[\"status\"][\"containerStatuses\"])))",
          "",
          "[Added Lines]",
          "269:         status = next(x for x in actual_pod[\"status\"][\"containerStatuses\"] if x[\"name\"] == \"base\")",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "720: # to separately add providers dependencies - they have been already added as 'providers' extras above",
          "721: _all_dependencies = get_unique_dependency_list(EXTRAS_DEPENDENCIES.values())",
          "727: # All user extras here",
          "728: # all is purely development extra and it should contain only direct dependencies of Airflow",
          "",
          "[Removed Lines]",
          "723: _all_dependencies_without_airflow_providers = list(",
          "724:     filter(lambda k: \"apache-airflow-\" not in k, _all_dependencies)",
          "725: )",
          "",
          "[Added Lines]",
          "723: _all_dependencies_without_airflow_providers = [k for k in _all_dependencies if \"apache-airflow-\" not in k]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bda8b550ea0b895e0f0ac65246f665d67bff2c62",
      "candidate_info": {
        "commit_hash": "bda8b550ea0b895e0f0ac65246f665d67bff2c62",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/bda8b550ea0b895e0f0ac65246f665d67bff2c62",
        "files": [
          "airflow/cli/commands/task_command.py",
          "airflow/models/mappedoperator.py",
          "airflow/models/taskinstance.py",
          "airflow/utils/task_instance_session.py",
          "tests/decorators/test_python.py",
          "tests/models/test_mappedoperator.py",
          "tests/models/test_renderedtifields.py",
          "tests/models/test_xcom_arg_map.py"
        ],
        "message": "Reuse _run_task_session in mapped render_template_fields (#33309)\n\nThe `render_template_fields` method of mapped operator needs to use\ndatabase session object to render mapped fields, but it cannot\nget the session passed by @provide_session decorator, because it is\nused in derived classes and we cannot change the signature without\nimpacting those classes.\n\nSo far it was done by creating new session in mapped_operator, but\nit has the drawback of creating an extra session while one is\nalready created (remnder_template_fields is always run in the\ncontext of task run and it always has a session created already\nin _run_raw_task). It also causes problems in our tests where\ntwo opened database session accessed database at the same time\nand it cases sqlite exception on concurrent access and mysql\nerror on running operations out of sync - likely when the same\nobject was modified in both sessions.\n\nThis PR changes the approach - rather than creating a new session\nin the mapped_operator, we are retrieving the session from one\nstored by the _run_raw_task. It is done by context manager and\nadequate protection has been added to make sure that:\n\na) the call is made within the context manager\nb) context manageer is never initialized twice in the same\n   call stack\n\nAfter this change, resources used by running task will be smaller,\nand mapped tasks will not always open 2 DB sesions.\n\nFixes: #33178\n(cherry picked from commit ef85c673d81cbeb60f29a978c5dc61787d61253e)",
        "before_after_code_files": [
          "airflow/cli/commands/task_command.py||airflow/cli/commands/task_command.py",
          "airflow/models/mappedoperator.py||airflow/models/mappedoperator.py",
          "airflow/models/taskinstance.py||airflow/models/taskinstance.py",
          "airflow/utils/task_instance_session.py||airflow/utils/task_instance_session.py",
          "tests/decorators/test_python.py||tests/decorators/test_python.py",
          "tests/models/test_mappedoperator.py||tests/models/test_mappedoperator.py",
          "tests/models/test_renderedtifields.py||tests/models/test_renderedtifields.py",
          "tests/models/test_xcom_arg_map.py||tests/models/test_xcom_arg_map.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/commands/task_command.py||airflow/cli/commands/task_command.py": [
          "File: airflow/cli/commands/task_command.py -> airflow/cli/commands/task_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "68: from airflow.utils.providers_configuration_loader import providers_configuration_loaded",
          "69: from airflow.utils.session import NEW_SESSION, create_session, provide_session",
          "70: from airflow.utils.state import DagRunState",
          "72: log = logging.getLogger(__name__)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "71: from airflow.utils.task_instance_session import set_current_task_instance_session",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "645:     ti, _ = _get_ti(",
          "646:         task, args.map_index, exec_date_or_run_id=args.execution_date_or_run_id, create_if_necessary=\"memory\"",
          "647:     )",
          "649:     for attr in task.template_fields:",
          "650:         print(",
          "651:             textwrap.dedent(",
          "",
          "[Removed Lines]",
          "648:     ti.render_templates()",
          "",
          "[Added Lines]",
          "649:     with create_session() as session, set_current_task_instance_session(session=session):",
          "650:         ti.render_templates()",
          "",
          "---------------"
        ],
        "airflow/models/mappedoperator.py||airflow/models/mappedoperator.py": [
          "File: airflow/models/mappedoperator.py -> airflow/models/mappedoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: import pendulum",
          "30: from sqlalchemy.orm.session import Session",
          "33: from airflow.compat.functools import cache",
          "34: from airflow.exceptions import AirflowException, UnmappableOperator",
          "35: from airflow.models.abstractoperator import (",
          "",
          "[Removed Lines]",
          "32: from airflow import settings",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "64: from airflow.utils.context import Context, context_update_for_unmapped",
          "65: from airflow.utils.helpers import is_container, prevent_duplicates",
          "66: from airflow.utils.operator_resources import Resources",
          "67: from airflow.utils.trigger_rule import TriggerRule",
          "68: from airflow.utils.types import NOTSET",
          "69: from airflow.utils.xcom import XCOM_RETURN_KEY",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "66: from airflow.utils.task_instance_session import get_current_task_instance_session",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "714:         if not jinja_env:",
          "715:             jinja_env = self.get_template_env()",
          "724:         mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)",
          "725:         unmapped_task = self.unmap(mapped_kwargs)",
          "",
          "[Removed Lines]",
          "717:         # Ideally we'd like to pass in session as an argument to this function,",
          "718:         # but we can't easily change this function signature since operators",
          "719:         # could override this. We can't use @provide_session since it closes and",
          "720:         # expunges everything, which we don't want to do when we are so \"deep\"",
          "721:         # in the weeds here. We don't close this session for the same reason.",
          "722:         session = settings.Session()",
          "",
          "[Added Lines]",
          "717:         # We retrieve the session here, stored by _run_raw_task in set_current_task_session",
          "718:         # context manager - we cannot pass the session via @provide_session because the signature",
          "719:         # of render_template_fields is defined by BaseOperator and there are already many subclasses",
          "720:         # overriding it, so changing the signature is not an option. However render_template_fields is",
          "721:         # always executed within \"_run_raw_task\" so we make sure that _run_raw_task uses the",
          "722:         # set_current_task_session context manager to store the session in the current task.",
          "723:         session = get_current_task_instance_session()",
          "",
          "---------------"
        ],
        "airflow/models/taskinstance.py||airflow/models/taskinstance.py": [
          "File: airflow/models/taskinstance.py -> airflow/models/taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "124: )",
          "125: from airflow.utils.state import DagRunState, JobState, State, TaskInstanceState",
          "126: from airflow.utils.task_group import MappedTaskGroup",
          "127: from airflow.utils.timeout import timeout",
          "128: from airflow.utils.xcom import XCOM_RETURN_KEY",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "127: from airflow.utils.task_instance_session import set_current_task_instance_session",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1507:                 count=0,",
          "1508:                 tags={**self.stats_tags, \"state\": str(state)},",
          "1509:             )",
          "1563:                 session.commit()",
          "1564:                 return None",
          "1566:                 self.handle_failure(e, test_mode, context, session=session)",
          "1567:                 session.commit()",
          "1568:                 raise",
          "1603:     def _register_dataset_changes(self, *, session: Session) -> None:",
          "1604:         for obj in self.task.outlets or []:",
          "",
          "[Removed Lines]",
          "1511:         self.task = self.task.prepare_for_execution()",
          "1512:         context = self.get_template_context(ignore_param_exceptions=False)",
          "1514:         try:",
          "1515:             if not mark_success:",
          "1516:                 self._execute_task_with_callbacks(context, test_mode, session=session)",
          "1517:             if not test_mode:",
          "1518:                 self.refresh_from_db(lock_for_update=True, session=session)",
          "1519:             self.state = TaskInstanceState.SUCCESS",
          "1520:         except TaskDeferred as defer:",
          "1521:             # The task has signalled it wants to defer execution based on",
          "1522:             # a trigger.",
          "1523:             self._defer_task(defer=defer, session=session)",
          "1524:             self.log.info(",
          "1525:                 \"Pausing task as DEFERRED. dag_id=%s, task_id=%s, execution_date=%s, start_date=%s\",",
          "1526:                 self.dag_id,",
          "1527:                 self.task_id,",
          "1528:                 self._date_or_empty(\"execution_date\"),",
          "1529:                 self._date_or_empty(\"start_date\"),",
          "1530:             )",
          "1531:             if not test_mode:",
          "1532:                 session.add(Log(self.state, self))",
          "1533:                 session.merge(self)",
          "1534:                 session.commit()",
          "1535:             return TaskReturnCode.DEFERRED",
          "1536:         except AirflowSkipException as e:",
          "1537:             # Recording SKIP",
          "1538:             # log only if exception has any arguments to prevent log flooding",
          "1539:             if e.args:",
          "1540:                 self.log.info(e)",
          "1541:             if not test_mode:",
          "1542:                 self.refresh_from_db(lock_for_update=True, session=session)",
          "1543:             self.state = TaskInstanceState.SKIPPED",
          "1544:         except AirflowRescheduleException as reschedule_exception:",
          "1545:             self._handle_reschedule(actual_start_date, reschedule_exception, test_mode, session=session)",
          "1546:             session.commit()",
          "1547:             return None",
          "1548:         except (AirflowFailException, AirflowSensorTimeout) as e:",
          "1549:             # If AirflowFailException is raised, task should not retry.",
          "1550:             # If a sensor in reschedule mode reaches timeout, task should not retry.",
          "1551:             self.handle_failure(e, test_mode, context, force_fail=True, session=session)",
          "1552:             session.commit()",
          "1553:             raise",
          "1554:         except AirflowException as e:",
          "1555:             if not test_mode:",
          "1556:                 self.refresh_from_db(lock_for_update=True, session=session)",
          "1557:             # for case when task is marked as success/failed externally",
          "1558:             # or dagrun timed out and task is marked as skipped",
          "1559:             # current behavior doesn't hit the callbacks",
          "1560:             if self.state in State.finished:",
          "1561:                 self.clear_next_method_args()",
          "1562:                 session.merge(self)",
          "1565:             else:",
          "1569:         except (Exception, KeyboardInterrupt) as e:",
          "1570:             self.handle_failure(e, test_mode, context, session=session)",
          "1571:             session.commit()",
          "1572:             raise",
          "1573:         finally:",
          "1574:             Stats.incr(f\"ti.finish.{self.dag_id}.{self.task_id}.{self.state}\", tags=self.stats_tags)",
          "1575:             # Same metric with tagging",
          "1576:             Stats.incr(\"ti.finish\", tags={**self.stats_tags, \"state\": str(self.state)})",
          "1578:         # Recording SKIPPED or SUCCESS",
          "1579:         self.clear_next_method_args()",
          "1580:         self.end_date = timezone.utcnow()",
          "1581:         self._log_state()",
          "1582:         self.set_duration()",
          "1584:         # run on_success_callback before db committing",
          "1585:         # otherwise, the LocalTaskJob sees the state is changed to `success`,",
          "1586:         # but the task_runner is still running, LocalTaskJob then treats the state is set externally!",
          "1587:         self._run_finished_callback(self.task.on_success_callback, context, \"on_success\")",
          "1589:         if not test_mode:",
          "1590:             session.add(Log(self.state, self))",
          "1591:             session.merge(self).task = self.task",
          "1592:             if self.state == TaskInstanceState.SUCCESS:",
          "1593:                 self._register_dataset_changes(session=session)",
          "1595:             session.commit()",
          "1596:             if self.state == TaskInstanceState.SUCCESS:",
          "1597:                 get_listener_manager().hook.on_task_instance_success(",
          "1598:                     previous_state=TaskInstanceState.RUNNING, task_instance=self, session=session",
          "1599:                 )",
          "1601:         return None",
          "",
          "[Added Lines]",
          "1511:         with set_current_task_instance_session(session=session):",
          "1512:             self.task = self.task.prepare_for_execution()",
          "1513:             context = self.get_template_context(ignore_param_exceptions=False)",
          "1515:             try:",
          "1516:                 if not mark_success:",
          "1517:                     self._execute_task_with_callbacks(context, test_mode, session=session)",
          "1518:                 if not test_mode:",
          "1519:                     self.refresh_from_db(lock_for_update=True, session=session)",
          "1520:                 self.state = TaskInstanceState.SUCCESS",
          "1521:             except TaskDeferred as defer:",
          "1522:                 # The task has signalled it wants to defer execution based on",
          "1523:                 # a trigger.",
          "1524:                 self._defer_task(defer=defer, session=session)",
          "1525:                 self.log.info(",
          "1526:                     \"Pausing task as DEFERRED. dag_id=%s, task_id=%s, execution_date=%s, start_date=%s\",",
          "1527:                     self.dag_id,",
          "1528:                     self.task_id,",
          "1529:                     self._date_or_empty(\"execution_date\"),",
          "1530:                     self._date_or_empty(\"start_date\"),",
          "1531:                 )",
          "1532:                 if not test_mode:",
          "1533:                     session.add(Log(self.state, self))",
          "1534:                     session.merge(self)",
          "1535:                     session.commit()",
          "1536:                 return TaskReturnCode.DEFERRED",
          "1537:             except AirflowSkipException as e:",
          "1538:                 # Recording SKIP",
          "1539:                 # log only if exception has any arguments to prevent log flooding",
          "1540:                 if e.args:",
          "1541:                     self.log.info(e)",
          "1542:                 if not test_mode:",
          "1543:                     self.refresh_from_db(lock_for_update=True, session=session)",
          "1544:                 self.state = TaskInstanceState.SKIPPED",
          "1545:             except AirflowRescheduleException as reschedule_exception:",
          "1546:                 self._handle_reschedule(actual_start_date, reschedule_exception, test_mode, session=session)",
          "1549:             except (AirflowFailException, AirflowSensorTimeout) as e:",
          "1550:                 # If AirflowFailException is raised, task should not retry.",
          "1551:                 # If a sensor in reschedule mode reaches timeout, task should not retry.",
          "1552:                 self.handle_failure(e, test_mode, context, force_fail=True, session=session)",
          "1553:                 session.commit()",
          "1554:                 raise",
          "1555:             except AirflowException as e:",
          "1556:                 if not test_mode:",
          "1557:                     self.refresh_from_db(lock_for_update=True, session=session)",
          "1558:                 # for case when task is marked as success/failed externally",
          "1559:                 # or dagrun timed out and task is marked as skipped",
          "1560:                 # current behavior doesn't hit the callbacks",
          "1561:                 if self.state in State.finished:",
          "1562:                     self.clear_next_method_args()",
          "1563:                     session.merge(self)",
          "1564:                     session.commit()",
          "1565:                     return None",
          "1566:                 else:",
          "1567:                     self.handle_failure(e, test_mode, context, session=session)",
          "1568:                     session.commit()",
          "1569:                     raise",
          "1570:             except (Exception, KeyboardInterrupt) as e:",
          "1574:             finally:",
          "1575:                 Stats.incr(f\"ti.finish.{self.dag_id}.{self.task_id}.{self.state}\", tags=self.stats_tags)",
          "1576:                 # Same metric with tagging",
          "1577:                 Stats.incr(\"ti.finish\", tags={**self.stats_tags, \"state\": str(self.state)})",
          "1579:             # Recording SKIPPED or SUCCESS",
          "1580:             self.clear_next_method_args()",
          "1581:             self.end_date = timezone.utcnow()",
          "1582:             self._log_state()",
          "1583:             self.set_duration()",
          "1585:             # run on_success_callback before db committing",
          "1586:             # otherwise, the LocalTaskJob sees the state is changed to `success`,",
          "1587:             # but the task_runner is still running, LocalTaskJob then treats the state is set externally!",
          "1588:             self._run_finished_callback(self.task.on_success_callback, context, \"on_success\")",
          "1590:             if not test_mode:",
          "1591:                 session.add(Log(self.state, self))",
          "1592:                 session.merge(self).task = self.task",
          "1593:                 if self.state == TaskInstanceState.SUCCESS:",
          "1594:                     self._register_dataset_changes(session=session)",
          "1596:                 session.commit()",
          "1597:                 if self.state == TaskInstanceState.SUCCESS:",
          "1598:                     get_listener_manager().hook.on_task_instance_success(",
          "1599:                         previous_state=TaskInstanceState.RUNNING, task_instance=self, session=session",
          "1600:                     )",
          "1602:             return None",
          "",
          "---------------"
        ],
        "airflow/utils/task_instance_session.py||airflow/utils/task_instance_session.py": [
          "File: airflow/utils/task_instance_session.py -> airflow/utils/task_instance_session.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "18: from __future__ import annotations",
          "20: import contextlib",
          "21: import logging",
          "22: import traceback",
          "23: from typing import TYPE_CHECKING",
          "25: from airflow.utils.session import create_session",
          "27: if TYPE_CHECKING:",
          "28:     from sqlalchemy.orm import Session",
          "30: __current_task_instance_session: Session | None = None",
          "32: log = logging.getLogger(__name__)",
          "35: def get_current_task_instance_session() -> Session:",
          "36:     global __current_task_instance_session",
          "37:     if not __current_task_instance_session:",
          "38:         log.warning(\"No task session set for this task. Continuing but this likely causes a resource leak.\")",
          "39:         log.warning(\"Please report this and stacktrace below to https://github.com/apache/airflow/issues\")",
          "40:         for filename, line_number, name, line in traceback.extract_stack():",
          "41:             log.warning('File: \"%s\", %s , in %s', filename, line_number, name)",
          "42:             if line:",
          "43:                 log.warning(\"  %s\", line.strip())",
          "44:         __current_task_instance_session = create_session()",
          "45:     return __current_task_instance_session",
          "48: @contextlib.contextmanager",
          "49: def set_current_task_instance_session(session: Session):",
          "50:     global __current_task_instance_session",
          "51:     if __current_task_instance_session:",
          "52:         raise RuntimeError(",
          "53:             \"Session already set for this task. \"",
          "54:             \"You can only have one 'set_current_task_session' context manager active at a time.\"",
          "55:         )",
          "56:     __current_task_instance_session = session",
          "57:     try:",
          "58:         yield",
          "59:     finally:",
          "60:         __current_task_instance_session = None",
          "",
          "---------------"
        ],
        "tests/decorators/test_python.py||tests/decorators/test_python.py": [
          "File: tests/decorators/test_python.py -> tests/decorators/test_python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "36: from airflow.utils import timezone",
          "37: from airflow.utils.state import State",
          "38: from airflow.utils.task_group import TaskGroup",
          "39: from airflow.utils.trigger_rule import TriggerRule",
          "40: from airflow.utils.types import DagRunType",
          "41: from airflow.utils.xcom import XCOM_RETURN_KEY",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "39: from airflow.utils.task_instance_session import set_current_task_instance_session",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "744:     def fn(arg1, arg2):",
          "745:         ...",
          "764:         )",
          "779: def test_task_decorator_has_wrapped_attr():",
          "",
          "[Removed Lines]",
          "747:     with dag_maker(session=session):",
          "748:         task1 = BaseOperator(task_id=\"op1\")",
          "749:         mapped = fn.partial(arg2=\"{{ ti.task_id }}\").expand(arg1=task1.output)",
          "751:     dr = dag_maker.create_dagrun()",
          "752:     ti: TaskInstance = dr.get_task_instance(task1.task_id, session=session)",
          "754:     ti.xcom_push(key=XCOM_RETURN_KEY, value=[\"{{ ds }}\"], session=session)",
          "756:     session.add(",
          "757:         TaskMap(",
          "758:             dag_id=dr.dag_id,",
          "759:             task_id=task1.task_id,",
          "760:             run_id=dr.run_id,",
          "761:             map_index=-1,",
          "762:             length=1,",
          "763:             keys=None,",
          "765:     )",
          "766:     session.flush()",
          "768:     mapped_ti: TaskInstance = dr.get_task_instance(mapped.operator.task_id, session=session)",
          "769:     mapped_ti.map_index = 0",
          "771:     assert isinstance(mapped_ti.task, MappedOperator)",
          "772:     mapped.operator.render_template_fields(context=mapped_ti.get_template_context(session=session))",
          "773:     assert isinstance(mapped_ti.task, BaseOperator)",
          "775:     assert mapped_ti.task.op_kwargs[\"arg1\"] == \"{{ ds }}\"",
          "776:     assert mapped_ti.task.op_kwargs[\"arg2\"] == \"fn\"",
          "",
          "[Added Lines]",
          "748:     with set_current_task_instance_session(session=session):",
          "749:         with dag_maker(session=session):",
          "750:             task1 = BaseOperator(task_id=\"op1\")",
          "751:             mapped = fn.partial(arg2=\"{{ ti.task_id }}\").expand(arg1=task1.output)",
          "753:         dr = dag_maker.create_dagrun()",
          "754:         ti: TaskInstance = dr.get_task_instance(task1.task_id, session=session)",
          "756:         ti.xcom_push(key=XCOM_RETURN_KEY, value=[\"{{ ds }}\"], session=session)",
          "758:         session.add(",
          "759:             TaskMap(",
          "760:                 dag_id=dr.dag_id,",
          "761:                 task_id=task1.task_id,",
          "762:                 run_id=dr.run_id,",
          "763:                 map_index=-1,",
          "764:                 length=1,",
          "765:                 keys=None,",
          "766:             )",
          "768:         session.flush()",
          "770:         mapped_ti: TaskInstance = dr.get_task_instance(mapped.operator.task_id, session=session)",
          "771:         mapped_ti.map_index = 0",
          "773:         assert isinstance(mapped_ti.task, MappedOperator)",
          "774:         mapped.operator.render_template_fields(context=mapped_ti.get_template_context(session=session))",
          "775:         assert isinstance(mapped_ti.task, BaseOperator)",
          "777:         assert mapped_ti.task.op_kwargs[\"arg1\"] == \"{{ ds }}\"",
          "778:         assert mapped_ti.task.op_kwargs[\"arg2\"] == \"fn\"",
          "",
          "---------------"
        ],
        "tests/models/test_mappedoperator.py||tests/models/test_mappedoperator.py": [
          "File: tests/models/test_mappedoperator.py -> tests/models/test_mappedoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "39: from airflow.utils.context import Context",
          "40: from airflow.utils.state import TaskInstanceState",
          "41: from airflow.utils.task_group import TaskGroup",
          "42: from airflow.utils.trigger_rule import TriggerRule",
          "43: from airflow.utils.xcom import XCOM_RETURN_KEY",
          "44: from tests.models import DEFAULT_DATE",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42: from airflow.utils.task_instance_session import set_current_task_instance_session",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "402: def test_mapped_render_template_fields_validating_operator(dag_maker, session):",
          "419:         def execute(self, context):",
          "420:             pass",
          "508: def test_mapped_render_nested_template_fields(dag_maker, session):",
          "",
          "[Removed Lines]",
          "403:     class MyOperator(BaseOperator):",
          "404:         template_fields = (\"partial_template\", \"map_template\", \"file_template\")",
          "405:         template_ext = (\".ext\",)",
          "407:         def __init__(",
          "408:             self, partial_template, partial_static, map_template, map_static, file_template, **kwargs",
          "409:         ):",
          "410:             for value in [partial_template, partial_static, map_template, map_static, file_template]:",
          "411:                 assert isinstance(value, str), \"value should have been resolved before unmapping\"",
          "412:             super().__init__(**kwargs)",
          "413:             self.partial_template = partial_template",
          "414:             self.partial_static = partial_static",
          "415:             self.map_template = map_template",
          "416:             self.map_static = map_static",
          "417:             self.file_template = file_template",
          "422:     with dag_maker(session=session):",
          "423:         task1 = BaseOperator(task_id=\"op1\")",
          "424:         output1 = task1.output",
          "425:         mapped = MyOperator.partial(",
          "426:             task_id=\"a\", partial_template=\"{{ ti.task_id }}\", partial_static=\"{{ ti.task_id }}\"",
          "427:         ).expand(map_template=output1, map_static=output1, file_template=[\"/path/to/file.ext\"])",
          "429:     dr = dag_maker.create_dagrun()",
          "430:     ti: TaskInstance = dr.get_task_instance(task1.task_id, session=session)",
          "432:     ti.xcom_push(key=XCOM_RETURN_KEY, value=[\"{{ ds }}\"], session=session)",
          "434:     session.add(",
          "435:         TaskMap(",
          "436:             dag_id=dr.dag_id,",
          "437:             task_id=task1.task_id,",
          "438:             run_id=dr.run_id,",
          "439:             map_index=-1,",
          "440:             length=1,",
          "441:             keys=None,",
          "442:         )",
          "443:     )",
          "444:     session.flush()",
          "446:     mapped_ti: TaskInstance = dr.get_task_instance(mapped.task_id, session=session)",
          "447:     mapped_ti.map_index = 0",
          "449:     assert isinstance(mapped_ti.task, MappedOperator)",
          "450:     with patch(\"builtins.open\", mock.mock_open(read_data=b\"loaded data\")), patch(",
          "451:         \"os.path.isfile\", return_value=True",
          "452:     ), patch(\"os.path.getmtime\", return_value=0):",
          "453:         mapped.render_template_fields(context=mapped_ti.get_template_context(session=session))",
          "454:     assert isinstance(mapped_ti.task, MyOperator)",
          "456:     assert mapped_ti.task.partial_template == \"a\", \"Should be templated!\"",
          "457:     assert mapped_ti.task.partial_static == \"{{ ti.task_id }}\", \"Should not be templated!\"",
          "458:     assert mapped_ti.task.map_template == \"{{ ds }}\", \"Should not be templated!\"",
          "459:     assert mapped_ti.task.map_static == \"{{ ds }}\", \"Should not be templated!\"",
          "460:     assert mapped_ti.task.file_template == \"loaded data\", \"Should be templated!\"",
          "463: def test_mapped_expand_kwargs_render_template_fields_validating_operator(dag_maker, session):",
          "464:     class MyOperator(BaseOperator):",
          "465:         template_fields = (\"partial_template\", \"map_template\", \"file_template\")",
          "466:         template_ext = (\".ext\",)",
          "468:         def __init__(",
          "469:             self, partial_template, partial_static, map_template, map_static, file_template, **kwargs",
          "470:         ):",
          "471:             for value in [partial_template, partial_static, map_template, map_static, file_template]:",
          "472:                 assert isinstance(value, str), \"value should have been resolved before unmapping\"",
          "473:             super().__init__(**kwargs)",
          "474:             self.partial_template = partial_template",
          "475:             self.partial_static = partial_static",
          "476:             self.map_template = map_template",
          "477:             self.map_static = map_static",
          "478:             self.file_template = file_template",
          "480:         def execute(self, context):",
          "481:             pass",
          "483:     with dag_maker(session=session):",
          "484:         mapped = MyOperator.partial(",
          "485:             task_id=\"a\", partial_template=\"{{ ti.task_id }}\", partial_static=\"{{ ti.task_id }}\"",
          "486:         ).expand_kwargs(",
          "487:             [{\"map_template\": \"{{ ds }}\", \"map_static\": \"{{ ds }}\", \"file_template\": \"/path/to/file.ext\"}]",
          "488:         )",
          "490:     dr = dag_maker.create_dagrun()",
          "492:     mapped_ti: TaskInstance = dr.get_task_instance(mapped.task_id, session=session, map_index=0)",
          "494:     assert isinstance(mapped_ti.task, MappedOperator)",
          "495:     with patch(\"builtins.open\", mock.mock_open(read_data=b\"loaded data\")), patch(",
          "496:         \"os.path.isfile\", return_value=True",
          "497:     ), patch(\"os.path.getmtime\", return_value=0):",
          "498:         mapped.render_template_fields(context=mapped_ti.get_template_context(session=session))",
          "499:     assert isinstance(mapped_ti.task, MyOperator)",
          "501:     assert mapped_ti.task.partial_template == \"a\", \"Should be templated!\"",
          "502:     assert mapped_ti.task.partial_static == \"{{ ti.task_id }}\", \"Should not be templated!\"",
          "503:     assert mapped_ti.task.map_template == \"2016-01-01\", \"Should be templated!\"",
          "504:     assert mapped_ti.task.map_static == \"{{ ds }}\", \"Should not be templated!\"",
          "505:     assert mapped_ti.task.file_template == \"loaded data\", \"Should be templated!\"",
          "",
          "[Added Lines]",
          "404:     with set_current_task_instance_session(session=session):",
          "406:         class MyOperator(BaseOperator):",
          "407:             template_fields = (\"partial_template\", \"map_template\", \"file_template\")",
          "408:             template_ext = (\".ext\",)",
          "410:             def __init__(",
          "411:                 self, partial_template, partial_static, map_template, map_static, file_template, **kwargs",
          "412:             ):",
          "413:                 for value in [partial_template, partial_static, map_template, map_static, file_template]:",
          "414:                     assert isinstance(value, str), \"value should have been resolved before unmapping\"",
          "415:                     super().__init__(**kwargs)",
          "416:                     self.partial_template = partial_template",
          "417:                 self.partial_static = partial_static",
          "418:                 self.map_template = map_template",
          "419:                 self.map_static = map_static",
          "420:                 self.file_template = file_template",
          "425:         with dag_maker(session=session):",
          "426:             task1 = BaseOperator(task_id=\"op1\")",
          "427:             output1 = task1.output",
          "428:             mapped = MyOperator.partial(",
          "429:                 task_id=\"a\", partial_template=\"{{ ti.task_id }}\", partial_static=\"{{ ti.task_id }}\"",
          "430:             ).expand(map_template=output1, map_static=output1, file_template=[\"/path/to/file.ext\"])",
          "432:         dr = dag_maker.create_dagrun()",
          "433:         ti: TaskInstance = dr.get_task_instance(task1.task_id, session=session)",
          "435:         ti.xcom_push(key=XCOM_RETURN_KEY, value=[\"{{ ds }}\"], session=session)",
          "437:         session.add(",
          "438:             TaskMap(",
          "439:                 dag_id=dr.dag_id,",
          "440:                 task_id=task1.task_id,",
          "441:                 run_id=dr.run_id,",
          "442:                 map_index=-1,",
          "443:                 length=1,",
          "444:                 keys=None,",
          "445:             )",
          "446:         )",
          "447:         session.flush()",
          "449:         mapped_ti: TaskInstance = dr.get_task_instance(mapped.task_id, session=session)",
          "450:         mapped_ti.map_index = 0",
          "452:         assert isinstance(mapped_ti.task, MappedOperator)",
          "453:         with patch(\"builtins.open\", mock.mock_open(read_data=b\"loaded data\")), patch(",
          "454:             \"os.path.isfile\", return_value=True",
          "455:         ), patch(\"os.path.getmtime\", return_value=0):",
          "456:             mapped.render_template_fields(context=mapped_ti.get_template_context(session=session))",
          "457:         assert isinstance(mapped_ti.task, MyOperator)",
          "459:         assert mapped_ti.task.partial_template == \"a\", \"Should be templated!\"",
          "460:         assert mapped_ti.task.partial_static == \"{{ ti.task_id }}\", \"Should not be templated!\"",
          "461:         assert mapped_ti.task.map_template == \"{{ ds }}\", \"Should not be templated!\"",
          "462:         assert mapped_ti.task.map_static == \"{{ ds }}\", \"Should not be templated!\"",
          "463:         assert mapped_ti.task.file_template == \"loaded data\", \"Should be templated!\"",
          "466: def test_mapped_expand_kwargs_render_template_fields_validating_operator(dag_maker, session):",
          "468:     with set_current_task_instance_session(session=session):",
          "470:         class MyOperator(BaseOperator):",
          "471:             template_fields = (\"partial_template\", \"map_template\", \"file_template\")",
          "472:             template_ext = (\".ext\",)",
          "474:             def __init__(",
          "475:                 self, partial_template, partial_static, map_template, map_static, file_template, **kwargs",
          "476:             ):",
          "477:                 for value in [partial_template, partial_static, map_template, map_static, file_template]:",
          "478:                     assert isinstance(value, str), \"value should have been resolved before unmapping\"",
          "479:                 super().__init__(**kwargs)",
          "480:                 self.partial_template = partial_template",
          "481:                 self.partial_static = partial_static",
          "482:                 self.map_template = map_template",
          "483:                 self.map_static = map_static",
          "484:                 self.file_template = file_template",
          "486:             def execute(self, context):",
          "487:                 pass",
          "489:         with dag_maker(session=session):",
          "490:             mapped = MyOperator.partial(",
          "491:                 task_id=\"a\", partial_template=\"{{ ti.task_id }}\", partial_static=\"{{ ti.task_id }}\"",
          "492:             ).expand_kwargs(",
          "493:                 [{\"map_template\": \"{{ ds }}\", \"map_static\": \"{{ ds }}\", \"file_template\": \"/path/to/file.ext\"}]",
          "494:             )",
          "496:         dr = dag_maker.create_dagrun()",
          "498:         mapped_ti: TaskInstance = dr.get_task_instance(mapped.task_id, session=session, map_index=0)",
          "500:         assert isinstance(mapped_ti.task, MappedOperator)",
          "501:         with patch(\"builtins.open\", mock.mock_open(read_data=b\"loaded data\")), patch(",
          "502:             \"os.path.isfile\", return_value=True",
          "503:         ), patch(\"os.path.getmtime\", return_value=0):",
          "504:             mapped.render_template_fields(context=mapped_ti.get_template_context(session=session))",
          "505:         assert isinstance(mapped_ti.task, MyOperator)",
          "507:         assert mapped_ti.task.partial_template == \"a\", \"Should be templated!\"",
          "508:         assert mapped_ti.task.partial_static == \"{{ ti.task_id }}\", \"Should not be templated!\"",
          "509:         assert mapped_ti.task.map_template == \"2016-01-01\", \"Should be templated!\"",
          "510:         assert mapped_ti.task.map_static == \"{{ ds }}\", \"Should not be templated!\"",
          "511:         assert mapped_ti.task.file_template == \"loaded data\", \"Should be templated!\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "604:     ],",
          "605: )",
          "606: def test_expand_kwargs_render_template_fields_validating_operator(dag_maker, session, map_index, expected):",
          "624:         )",
          "638: def test_xcomarg_property_of_mapped_operator(dag_maker):",
          "",
          "[Removed Lines]",
          "607:     with dag_maker(session=session):",
          "608:         task1 = BaseOperator(task_id=\"op1\")",
          "609:         mapped = MockOperator.partial(task_id=\"a\", arg2=\"{{ ti.task_id }}\").expand_kwargs(task1.output)",
          "611:     dr = dag_maker.create_dagrun()",
          "612:     ti: TaskInstance = dr.get_task_instance(task1.task_id, session=session)",
          "614:     ti.xcom_push(key=XCOM_RETURN_KEY, value=[{\"arg1\": \"{{ ds }}\"}, {\"arg1\": 2}], session=session)",
          "616:     session.add(",
          "617:         TaskMap(",
          "618:             dag_id=dr.dag_id,",
          "619:             task_id=task1.task_id,",
          "620:             run_id=dr.run_id,",
          "621:             map_index=-1,",
          "622:             length=2,",
          "623:             keys=None,",
          "625:     )",
          "626:     session.flush()",
          "628:     ti: TaskInstance = dr.get_task_instance(mapped.task_id, session=session)",
          "629:     ti.refresh_from_task(mapped)",
          "630:     ti.map_index = map_index",
          "631:     assert isinstance(ti.task, MappedOperator)",
          "632:     mapped.render_template_fields(context=ti.get_template_context(session=session))",
          "633:     assert isinstance(ti.task, MockOperator)",
          "634:     assert ti.task.arg1 == expected",
          "635:     assert ti.task.arg2 == \"a\"",
          "",
          "[Added Lines]",
          "613:     with set_current_task_instance_session(session=session):",
          "614:         with dag_maker(session=session):",
          "615:             task1 = BaseOperator(task_id=\"op1\")",
          "616:             mapped = MockOperator.partial(task_id=\"a\", arg2=\"{{ ti.task_id }}\").expand_kwargs(task1.output)",
          "618:         dr = dag_maker.create_dagrun()",
          "619:         ti: TaskInstance = dr.get_task_instance(task1.task_id, session=session)",
          "621:         ti.xcom_push(key=XCOM_RETURN_KEY, value=[{\"arg1\": \"{{ ds }}\"}, {\"arg1\": 2}], session=session)",
          "623:         session.add(",
          "624:             TaskMap(",
          "625:                 dag_id=dr.dag_id,",
          "626:                 task_id=task1.task_id,",
          "627:                 run_id=dr.run_id,",
          "628:                 map_index=-1,",
          "629:                 length=2,",
          "630:                 keys=None,",
          "631:             )",
          "633:         session.flush()",
          "635:         ti: TaskInstance = dr.get_task_instance(mapped.task_id, session=session)",
          "636:         ti.refresh_from_task(mapped)",
          "637:         ti.map_index = map_index",
          "638:         assert isinstance(ti.task, MappedOperator)",
          "639:         mapped.render_template_fields(context=ti.get_template_context(session=session))",
          "640:         assert isinstance(ti.task, MockOperator)",
          "641:         assert ti.task.arg1 == expected",
          "642:         assert ti.task.arg2 == \"a\"",
          "",
          "---------------"
        ],
        "tests/models/test_renderedtifields.py||tests/models/test_renderedtifields.py": [
          "File: tests/models/test_renderedtifields.py -> tests/models/test_renderedtifields.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: from airflow.models import Variable",
          "30: from airflow.models.renderedtifields import RenderedTaskInstanceFields as RTIF",
          "31: from airflow.operators.bash import BashOperator",
          "32: from airflow.utils.timezone import datetime",
          "33: from tests.test_utils.asserts import assert_queries_count",
          "34: from tests.test_utils.db import clear_db_dags, clear_db_runs, clear_rendered_ti_fields",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32: from airflow.utils.task_instance_session import set_current_task_instance_session",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "153:         ],",
          "154:     )",
          "155:     def test_delete_old_records(",
          "157:     ):",
          "158:         \"\"\"",
          "159:         Test that old records are deleted from rendered_task_instance_fields table",
          "160:         for a given task_id and dag_id.",
          "161:         \"\"\"",
          "195:     @pytest.mark.parametrize(",
          "196:         \"num_runs, num_to_keep, remaining_rtifs, expected_query_count\",",
          "",
          "[Removed Lines]",
          "156:         self, rtif_num, num_to_keep, remaining_rtifs, expected_query_count, dag_maker",
          "162:         session = settings.Session()",
          "163:         with dag_maker(\"test_delete_old_records\") as dag:",
          "164:             task = BashOperator(task_id=\"test\", bash_command=\"echo {{ ds }}\")",
          "165:         rtif_list = []",
          "166:         for num in range(rtif_num):",
          "167:             dr = dag_maker.create_dagrun(run_id=str(num), execution_date=dag.start_date + timedelta(days=num))",
          "168:             ti = dr.task_instances[0]",
          "169:             ti.task = task",
          "170:             rtif_list.append(RTIF(ti))",
          "172:         session.add_all(rtif_list)",
          "173:         session.flush()",
          "175:         result = session.query(RTIF).filter(RTIF.dag_id == dag.dag_id, RTIF.task_id == task.task_id).all()",
          "177:         for rtif in rtif_list:",
          "178:             assert rtif in result",
          "180:         assert rtif_num == len(result)",
          "182:         # Verify old records are deleted and only 'num_to_keep' records are kept",
          "183:         # For other DBs,an extra query is fired in RenderedTaskInstanceFields.delete_old_records",
          "184:         expected_query_count_based_on_db = (",
          "185:             expected_query_count + 1",
          "186:             if session.bind.dialect.name == \"mssql\" and expected_query_count != 0",
          "187:             else expected_query_count",
          "188:         )",
          "190:         with assert_queries_count(expected_query_count_based_on_db):",
          "191:             RTIF.delete_old_records(task_id=task.task_id, dag_id=task.dag_id, num_to_keep=num_to_keep)",
          "192:         result = session.query(RTIF).filter(RTIF.dag_id == dag.dag_id, RTIF.task_id == task.task_id).all()",
          "193:         assert remaining_rtifs == len(result)",
          "",
          "[Added Lines]",
          "157:         self, rtif_num, num_to_keep, remaining_rtifs, expected_query_count, dag_maker, session",
          "163:         with set_current_task_instance_session(session=session):",
          "164:             with dag_maker(\"test_delete_old_records\") as dag:",
          "165:                 task = BashOperator(task_id=\"test\", bash_command=\"echo {{ ds }}\")",
          "166:             rtif_list = []",
          "167:             for num in range(rtif_num):",
          "168:                 dr = dag_maker.create_dagrun(",
          "169:                     run_id=str(num), execution_date=dag.start_date + timedelta(days=num)",
          "170:                 )",
          "171:                 ti = dr.task_instances[0]",
          "172:                 ti.task = task",
          "173:                 rtif_list.append(RTIF(ti))",
          "175:             session.add_all(rtif_list)",
          "176:             session.flush()",
          "178:             result = session.query(RTIF).filter(RTIF.dag_id == dag.dag_id, RTIF.task_id == task.task_id).all()",
          "180:             for rtif in rtif_list:",
          "181:                 assert rtif in result",
          "183:             assert rtif_num == len(result)",
          "185:             # Verify old records are deleted and only 'num_to_keep' records are kept",
          "186:             # For other DBs,an extra query is fired in RenderedTaskInstanceFields.delete_old_records",
          "187:             expected_query_count_based_on_db = (",
          "188:                 expected_query_count + 1",
          "189:                 if session.bind.dialect.name == \"mssql\" and expected_query_count != 0",
          "190:                 else expected_query_count",
          "191:             )",
          "193:             with assert_queries_count(expected_query_count_based_on_db):",
          "194:                 RTIF.delete_old_records(task_id=task.task_id, dag_id=task.dag_id, num_to_keep=num_to_keep)",
          "195:             result = session.query(RTIF).filter(RTIF.dag_id == dag.dag_id, RTIF.task_id == task.task_id).all()",
          "196:             assert remaining_rtifs == len(result)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "207:         Test that old records are deleted from rendered_task_instance_fields table",
          "208:         for a given task_id and dag_id with mapped tasks.",
          "209:         \"\"\"",
          "215:             )",
          "245:     def test_write(self, dag_maker):",
          "246:         \"\"\"",
          "",
          "[Removed Lines]",
          "210:         with dag_maker(\"test_delete_old_records\", session=session) as dag:",
          "211:             mapped = BashOperator.partial(task_id=\"mapped\").expand(bash_command=[\"a\", \"b\"])",
          "212:         for num in range(num_runs):",
          "213:             dr = dag_maker.create_dagrun(",
          "214:                 run_id=f\"run_{num}\", execution_date=dag.start_date + timedelta(days=num)",
          "217:             mapped.expand_mapped_task(dr.run_id, session=dag_maker.session)",
          "218:             session.refresh(dr)",
          "219:             for ti in dr.task_instances:",
          "220:                 ti.task = dag.get_task(ti.task_id)",
          "221:                 session.add(RTIF(ti))",
          "222:         session.flush()",
          "224:         result = session.query(RTIF).filter(RTIF.dag_id == dag.dag_id).all()",
          "225:         assert len(result) == num_runs * 2",
          "227:         # Verify old records are deleted and only 'num_to_keep' records are kept",
          "228:         # For other DBs,an extra query is fired in RenderedTaskInstanceFields.delete_old_records",
          "229:         expected_query_count_based_on_db = (",
          "230:             expected_query_count + 1",
          "231:             if session.bind.dialect.name == \"mssql\" and expected_query_count != 0",
          "232:             else expected_query_count",
          "233:         )",
          "235:         with assert_queries_count(expected_query_count_based_on_db):",
          "236:             RTIF.delete_old_records(",
          "237:                 task_id=mapped.task_id, dag_id=dr.dag_id, num_to_keep=num_to_keep, session=session",
          "238:             )",
          "239:         result = session.query(RTIF).filter_by(dag_id=dag.dag_id, task_id=mapped.task_id).all()",
          "240:         rtif_num_runs = Counter(rtif.run_id for rtif in result)",
          "241:         assert len(rtif_num_runs) == remaining_rtifs",
          "242:         # Check that we have _all_ the data for each row",
          "243:         assert len(result) == remaining_rtifs * 2",
          "",
          "[Added Lines]",
          "213:         with set_current_task_instance_session(session=session):",
          "214:             with dag_maker(\"test_delete_old_records\", session=session) as dag:",
          "215:                 mapped = BashOperator.partial(task_id=\"mapped\").expand(bash_command=[\"a\", \"b\"])",
          "216:             for num in range(num_runs):",
          "217:                 dr = dag_maker.create_dagrun(",
          "218:                     run_id=f\"run_{num}\", execution_date=dag.start_date + timedelta(days=num)",
          "219:                 )",
          "221:                 mapped.expand_mapped_task(dr.run_id, session=dag_maker.session)",
          "222:                 session.refresh(dr)",
          "223:                 for ti in dr.task_instances:",
          "224:                     ti.task = dag.get_task(ti.task_id)",
          "225:                     session.add(RTIF(ti))",
          "226:             session.flush()",
          "228:             result = session.query(RTIF).filter(RTIF.dag_id == dag.dag_id).all()",
          "229:             assert len(result) == num_runs * 2",
          "231:             # Verify old records are deleted and only 'num_to_keep' records are kept",
          "232:             # For other DBs,an extra query is fired in RenderedTaskInstanceFields.delete_old_records",
          "233:             expected_query_count_based_on_db = (",
          "234:                 expected_query_count + 1",
          "235:                 if session.bind.dialect.name == \"mssql\" and expected_query_count != 0",
          "236:                 else expected_query_count",
          "239:             with assert_queries_count(expected_query_count_based_on_db):",
          "240:                 RTIF.delete_old_records(",
          "241:                     task_id=mapped.task_id, dag_id=dr.dag_id, num_to_keep=num_to_keep, session=session",
          "242:                 )",
          "243:             result = session.query(RTIF).filter_by(dag_id=dag.dag_id, task_id=mapped.task_id).all()",
          "244:             rtif_num_runs = Counter(rtif.run_id for rtif in result)",
          "245:             assert len(rtif_num_runs) == remaining_rtifs",
          "246:             # Check that we have _all_ the data for each row",
          "247:             assert len(result) == remaining_rtifs * 2",
          "",
          "---------------"
        ],
        "tests/models/test_xcom_arg_map.py||tests/models/test_xcom_arg_map.py": [
          "File: tests/models/test_xcom_arg_map.py -> tests/models/test_xcom_arg_map.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "41:     # The function passed to \"map\" is *NOT* a task.",
          "42:     assert set(dag.task_dict) == {\"push\", \"pull\"}",
          "46:     # Run \"push\".",
          "47:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "",
          "[Removed Lines]",
          "44:     dr = dag_maker.create_dagrun()",
          "",
          "[Added Lines]",
          "44:     dr = dag_maker.create_dagrun(session=session)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "80:         pull.expand(value=push().map(c_to_none))",
          "84:     # Run \"push\".",
          "85:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "",
          "[Removed Lines]",
          "82:     dr = dag_maker.create_dagrun()",
          "",
          "[Added Lines]",
          "82:     dr = dag_maker.create_dagrun(session=session)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "114:         pull.expand_kwargs(push().map(c_to_none))",
          "118:     # Run \"push\".",
          "119:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "",
          "[Removed Lines]",
          "116:     dr = dag_maker.create_dagrun()",
          "",
          "[Added Lines]",
          "116:     dr = dag_maker.create_dagrun(session=session)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "159:         pull.expand_kwargs(push().map(does_not_work_with_c))",
          "163:     # The \"push\" task should not fail.",
          "164:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "",
          "[Removed Lines]",
          "161:     dr = dag_maker.create_dagrun()",
          "",
          "[Added Lines]",
          "161:     dr = dag_maker.create_dagrun(session=session)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "212:         collect(value=forward.expand_kwargs(push().map(skip_c)))",
          "216:     # Run \"push\".",
          "217:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "",
          "[Removed Lines]",
          "214:     dr = dag_maker.create_dagrun()",
          "",
          "[Added Lines]",
          "214:     dr = dag_maker.create_dagrun(session=session)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "246:         converted = push().map(lambda v: v * 2).map(lambda v: {\"value\": v})",
          "247:         pull.expand_kwargs(converted)",
          "251:     # Run \"push\".",
          "252:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "",
          "[Removed Lines]",
          "249:     dr = dag_maker.create_dagrun()",
          "",
          "[Added Lines]",
          "249:     dr = dag_maker.create_dagrun(session=session)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "293:         pull.expand(value=combined.map(convert_zipped))",
          "297:     # Run \"push_letters\" and \"push_numbers\".",
          "298:     decision = dr.task_instance_scheduling_decisions(session=session)",
          "",
          "[Removed Lines]",
          "295:     dr = dag_maker.create_dagrun()",
          "",
          "[Added Lines]",
          "295:     dr = dag_maker.create_dagrun(session=session)",
          "",
          "---------------"
        ]
      }
    }
  ]
}