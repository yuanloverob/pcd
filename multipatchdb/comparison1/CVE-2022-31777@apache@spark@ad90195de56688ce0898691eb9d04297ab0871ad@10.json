{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "e7caea5483d5ed10b02aff6e53713a34c88e36f8",
      "candidate_info": {
        "commit_hash": "e7caea5483d5ed10b02aff6e53713a34c88e36f8",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/e7caea5483d5ed10b02aff6e53713a34c88e36f8",
        "files": [
          "core/src/main/resources/error/error-classes.json",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/text.sql.out",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala"
        ],
        "message": "[SPARK-39047][SQL][3.3] Replace the error class ILLEGAL_SUBSTRING by INVALID_PARAMETER_VALUE\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to remove the `ILLEGAL_SUBSTRING` error class, and use `INVALID_PARAMETER_VALUE` in the case when the `strfmt` parameter of the `format_string()` function contains `%0$`. The last value is handled differently by JDKs:  _\"... Java 8 and Java 11 uses it as \"%1$\", and Java 17 throws IllegalFormatArgumentIndexException(Illegal format argument index = 0)\"_.\n\nThis is a backport of https://github.com/apache/spark/pull/36380.\n\n### Why are the changes needed?\nTo improve code maintenance and user experience with Spark SQL by reducing the number of user-facing error classes.\n\n### Does this PR introduce _any_ user-facing change?\nYes, it changes user-facing error message.\n\nBefore:\n```sql\nspark-sql> select format_string('%0$s', 'Hello');\nError in query: [ILLEGAL_SUBSTRING] The argument_index of string format cannot contain position 0$.; line 1 pos 7\n```\n\nAfter:\n```sql\nspark-sql> select format_string('%0$s', 'Hello');\nError in query: [INVALID_PARAMETER_VALUE] The value of parameter(s) 'strfmt' in `format_string` is invalid: expects %1$, %2$ and so on, but got %0$.; line 1 pos 7\n```\n\n### How was this patch tested?\nBy running the affected test suites:\n```\n$ build/sbt \"test:testOnly *SparkThrowableSuite\"\n$ build/sbt \"sql/testOnly org.apache.spark.sql.SQLQueryTestSuite -- -z text.sql\"\n$ build/sbt \"test:testOnly *QueryCompilationErrorsSuite\"\n```\n\nAuthored-by: Max Gekk <max.gekkgmail.com>\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n(cherry picked from commit 9dcc24c36f6fcdf43bf66fe50415be575f7b2918)\nSigned-off-by: Max Gekk <max.gekkgmail.com>\n\nCloses #36390 from MaxGekk/error-class-ILLEGAL_SUBSTRING-3.3.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1899:   private def checkArgumentIndexNotZero(expression: Expression): Unit = expression match {",
          "1900:     case StringLiteral(pattern) if pattern.contains(\"%0$\") =>",
          "1903:     case _ => // do nothing",
          "1904:   }",
          "1905: }",
          "",
          "[Removed Lines]",
          "1901:       throw QueryCompilationErrors.illegalSubstringError(",
          "1902:         \"The argument_index of string format\", \"position 0$\")",
          "",
          "[Added Lines]",
          "1901:       throw QueryCompilationErrors.zeroArgumentIndexError()",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryCompilationErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "66:       messageParameters = Array(sizeLimit.toString))",
          "67:   }",
          "70:     new AnalysisException(",
          "73:   }",
          "75:   def unorderablePivotColError(pivotCol: Expression): Throwable = {",
          "",
          "[Removed Lines]",
          "69:   def illegalSubstringError(subject: String, illegalContent: String): Throwable = {",
          "71:       errorClass = \"ILLEGAL_SUBSTRING\",",
          "72:       messageParameters = Array(subject, illegalContent))",
          "",
          "[Added Lines]",
          "69:   def zeroArgumentIndexError(): Throwable = {",
          "71:       errorClass = \"INVALID_PARAMETER_VALUE\",",
          "72:       messageParameters = Array(",
          "73:         \"strfmt\", toSQLId(\"format_string\"), \"expects %1$, %2$ and so on, but got %0$.\"))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "94:     }",
          "95:   }",
          "98:     withSQLConf(SQLConf.ALLOW_ZERO_INDEX_IN_FORMAT_STRING.key -> \"false\") {",
          "99:       val e = intercept[AnalysisException] {",
          "100:         sql(\"select format_string('%0$s', 'Hello')\")",
          "101:       }",
          "105:     }",
          "106:   }",
          "",
          "[Removed Lines]",
          "97:   test(\"ILLEGAL_SUBSTRING: the argument_index of string format is invalid\") {",
          "102:       assert(e.errorClass === Some(\"ILLEGAL_SUBSTRING\"))",
          "103:       assert(e.message ===",
          "104:         \"The argument_index of string format cannot contain position 0$.\")",
          "",
          "[Added Lines]",
          "97:   test(\"INVALID_PARAMETER_VALUE: the argument_index of string format is invalid\") {",
          "102:       assert(e.errorClass === Some(\"INVALID_PARAMETER_VALUE\"))",
          "103:       assert(e.message === \"The value of parameter(s) 'strfmt' in `format_string` is invalid: \" +",
          "104:         \"expects %1$, %2$ and so on, but got %0$.\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e2f82fdaeba8d56cf6fa2f75d6d52b5b4f9da4ea",
      "candidate_info": {
        "commit_hash": "e2f82fdaeba8d56cf6fa2f75d6d52b5b4f9da4ea",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/e2f82fdaeba8d56cf6fa2f75d6d52b5b4f9da4ea",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcQuerySuite.scala"
        ],
        "message": "[SPARK-39019][TESTS] Use `withTempPath` to clean up temporary data directory after `SPARK-37463: read/write Timestamp ntz to Orc with different time zone`\n\n### What changes were proposed in this pull request?\n`SPARK-37463: read/write Timestamp ntz to Orc with different time zone` use the absolute path to save the test data, and does not clean up the test data after the test.\n\nThis pr change to use `withTempPath` to ensure the data directory is cleaned up after testing.\n\n### Why are the changes needed?\nClean up  the temporary data directory after test.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\n\n- Pass GA\n- Manual test\uff1a\n\nRun\n```\nmvn clean install -pl sql/core -am -DskipTests\nmvn clean test -pl sql/core -Dtest=none -DwildcardSuites=org.apache.spark.sql.execution.datasources.orc.OrcV1QuerySuite\ngit status\n```\n\n**Before**\n\n```\nsql/core/ts_ntz_orc/\n\nls sql/core/ts_ntz_orc\n_SUCCESS\t\t\t\t\t\t\tpart-00000-9523e257-5024-4980-8bb3-12070222b0bd-c000.snappy.orc\n```\n\n**After**\n\nNo residual `sql/core/ts_ntz_orc/`\n\nCloses #36352 from LuciferYang/SPARK-39019.\n\nAuthored-by: yangjie01 <yangjie01@baidu.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 97449d23a3b2232e14e63c6645919c5d93e4491c)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/orc/OrcQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "814:                       | timestamp_ntz '2021-03-14 02:15:00.0' as ts_ntz3",
          "815:                       |\"\"\".stripMargin",
          "827:           }",
          "828:         }",
          "829:       }",
          "",
          "[Removed Lines]",
          "817:       val df = sql(sqlText)",
          "819:       df.write.mode(\"overwrite\").orc(\"ts_ntz_orc\")",
          "821:       val query = \"select * from `orc`.`ts_ntz_orc`\"",
          "823:       DateTimeTestUtils.outstandingZoneIds.foreach { zoneId =>",
          "824:         DateTimeTestUtils.withDefaultTimeZone(zoneId) {",
          "825:           withAllNativeOrcReaders {",
          "826:             checkAnswer(sql(query), df)",
          "",
          "[Added Lines]",
          "817:       withTempPath { dir =>",
          "818:         val path = dir.getCanonicalPath",
          "819:         val df = sql(sqlText)",
          "821:         df.write.mode(\"overwrite\").orc(path)",
          "823:         val query = s\"select * from `orc`.`$path`\"",
          "825:         DateTimeTestUtils.outstandingZoneIds.foreach { zoneId =>",
          "826:           DateTimeTestUtils.withDefaultTimeZone(zoneId) {",
          "827:             withAllNativeOrcReaders {",
          "828:               checkAnswer(sql(query), df)",
          "829:             }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "fe95b03eff2bab17d4aab7e9814f04abb2991b11",
      "candidate_info": {
        "commit_hash": "fe95b03eff2bab17d4aab7e9814f04abb2991b11",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/fe95b03eff2bab17d4aab7e9814f04abb2991b11",
        "files": [
          "core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala"
        ],
        "message": "[SPARK-38446][CORE] Fix deadlock between ExecutorClassLoader and FileDownloadCallback caused by Log4j\n\n### What changes were proposed in this pull request?\n\nWhile `log4j.ignoreTCL/log4j2.ignoreTCL` is false, which is the default, it uses the context ClassLoader for the current Thread, see `org.apache.logging.log4j.util.LoaderUtil.loadClass`. While ExecutorClassLoader try to loadClass through remotely though the FileDownload, if error occurs, we will long on debug level, and `log4j...LoaderUtil` will be blocked by ExecutorClassLoader acquired classloading lock.\n\nFortunately, it only happens when ThresholdFilter's level is `debug`.\n\nor we can set `log4j.ignoreTCL/log4j2.ignoreTCL` to true, but I don't know what else it will cause.\n\nSo in this PR, I simply remove the debug log which cause this deadlock\n\n### Why are the changes needed?\n\nfix deadlock\n\n### Does this PR introduce _any_ user-facing change?\n\nno\n\n### How was this patch tested?\n\nhttps://github.com/apache/incubator-kyuubi/pull/2046#discussion_r821414439, with a ut in kyuubi project, resolved(https://github.com/apache/incubator-kyuubi/actions/runs/1950222737)\n\n### Additional Resources\n\n[ut.jstack.txt](https://github.com/apache/spark/files/8206457/ut.jstack.txt)\n\nCloses #35765 from yaooqinn/SPARK-38446.\n\nAuthored-by: Kent Yao <yao@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit aef674564ff12e78bd2f30846e3dcb69988249ae)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala||core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala||core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala": [
          "File: core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala -> core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "451:     }",
          "453:     override def onFailure(streamId: String, cause: Throwable): Unit = {",
          "455:       source.setError(cause)",
          "456:       sink.close()",
          "457:     }",
          "",
          "[Removed Lines]",
          "454:       logDebug(s\"Error downloading stream $streamId.\", cause)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c04aa36b7713a7ebaf368fc2ad4065478e264d85",
      "candidate_info": {
        "commit_hash": "c04aa36b7713a7ebaf368fc2ad4065478e264d85",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/c04aa36b7713a7ebaf368fc2ad4065478e264d85",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparison.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparisonSuite.scala"
        ],
        "message": "[SPARK-39896][SQL] UnwrapCastInBinaryComparison should work when the literal of In/InSet downcast failed\n\n### Why are the changes needed?\n\nThis PR aims to fix the case\n\n```scala\nsql(\"create table t1(a decimal(3, 0)) using parquet\")\nsql(\"insert into t1 values(100), (10), (1)\")\nsql(\"select * from t1 where a in(100000, 1.00)\").show\n```\n\n```\njava.lang.RuntimeException: After applying rule org.apache.spark.sql.catalyst.optimizer.UnwrapCastInBinaryComparison in batch Operator Optimization before Inferring Filters, the structural integrity of the plan is broken.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.structuralIntegrityIsBrokenAfterApplyingRuleError(QueryExecutionErrors.scala:1325)\n```\n\n1. the rule `UnwrapCastInBinaryComparison` transforms the expression `In` to Equals\n\n```\nCAST(a as decimal(12,2)) IN (100000.00,1.00)\n\nOR(\n   CAST(a as decimal(12,2)) = 100000.00,\n   CAST(a as decimal(12,2)) = 1.00\n)\n```\n\n2. using `UnwrapCastInBinaryComparison.unwrapCast()` to optimize each `EqualTo`\n\n```\n// Expression1\nCAST(a as decimal(12,2)) = 100000.00 => CAST(a as decimal(12,2)) = 100000.00\n\n// Expression2\nCAST(a as decimal(12,2)) = 1.00      => a = 1\n```\n\n3. return the new unwrapped cast expression `In`\n\n```\na IN (100000.00, 1.00)\n```\n\nBefore this PR:\n\nthe method `UnwrapCastInBinaryComparison.unwrapCast()` returns the original expression when downcasting to a decimal type fails (the `Expression1`),returns the original expression if the downcast to the decimal type succeeds (the `Expression2`), the two expressions have different data type which would break the structural integrity\n\n```\na IN (100000.00, 1.00)\n       |           |\n    decimal(12, 2) |\n               decimal(3, 0)\n```\n\nAfter this PR:\n\nthe PR transform the downcasting failed expression to `falseIfNotNull(fromExp)`\n```\n\n((isnull(a) AND null) OR a IN (1.00)\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, only bug fix.\n\n### How was this patch tested?\n\nUnit test.\n\nCloses #37439 from cfmcgrady/SPARK-39896.\n\nAuthored-by: Fu Chen <cfmcgrady@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 6e62b93f3d1ef7e2d6be0a3bb729ab9b2d55a36d)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparison.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparison.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparisonSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparisonSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparison.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparison.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparison.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparison.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.catalyst.optimizer",
          "21: import scala.collection.mutable.ArrayBuffer",
          "23: import org.apache.spark.sql.catalyst.expressions._",
          "",
          "[Removed Lines]",
          "20: import scala.collection.immutable.HashSet",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "145:     case in @ In(Cast(fromExp, toType: NumericType, _, _), list @ Seq(firstLit, _*))",
          "146:       if canImplicitlyCast(fromExp, toType, firstLit.dataType) && in.inSetConvertible =>",
          "183:       }",
          "189:       if hset.nonEmpty && canImplicitlyCast(fromExp, toType, toType) =>",
          "223:     case _ => exp",
          "224:   }",
          "",
          "[Removed Lines]",
          "153:       val (nullList, canCastList, cannotCastList) =",
          "154:         (ArrayBuffer[Literal](), ArrayBuffer[Literal](), ArrayBuffer[Expression]())",
          "155:       list.foreach {",
          "156:         case lit @ Literal(null, _) => nullList += lit",
          "157:         case lit @ NonNullLiteral(_, _) =>",
          "158:           unwrapCast(EqualTo(in.value, lit)) match {",
          "159:             case EqualTo(_, unwrapLit: Literal) => canCastList += unwrapLit",
          "160:             case e @ And(IsNull(_), Literal(null, BooleanType)) => cannotCastList += e",
          "161:             case _ => throw new IllegalStateException(\"Illegal unwrap cast result found.\")",
          "162:           }",
          "163:         case _ => throw new IllegalStateException(\"Illegal value found in in.list.\")",
          "164:       }",
          "167:       if (canCastList.isEmpty && cannotCastList.isEmpty) {",
          "168:         exp",
          "169:       } else {",
          "172:         val newList = nullList.map(lit => Cast(lit, fromExp.dataType)) ++ canCastList",
          "173:         val unwrapIn = In(fromExp, newList.toSeq)",
          "174:         cannotCastList.headOption match {",
          "175:           case None => unwrapIn",
          "178:           case Some(falseIfNotNull @ And(IsNull(_), Literal(null, BooleanType)))",
          "179:               if cannotCastList.map(_.canonicalized).distinct.length == 1 =>",
          "180:             Or(falseIfNotNull, unwrapIn)",
          "181:           case _ => exp",
          "182:         }",
          "188:     case inSet @ InSet(Cast(fromExp, toType: NumericType, _, _), hset)",
          "195:       var (nullSet, canCastSet, cannotCastSet) =",
          "196:         (HashSet[Any](), HashSet[Any](), HashSet[Expression]())",
          "197:       hset.map(value => Literal.create(value, toType))",
          "198:         .foreach {",
          "199:           case lit @ Literal(null, _) => nullSet += lit.value",
          "200:           case lit @ NonNullLiteral(_, _) =>",
          "201:             unwrapCast(EqualTo(inSet.child, lit)) match {",
          "202:               case EqualTo(_, unwrapLit: Literal) => canCastSet += unwrapLit.value",
          "203:               case e @ And(IsNull(_), Literal(null, BooleanType)) => cannotCastSet += e",
          "204:               case _ => throw new IllegalStateException(\"Illegal unwrap cast result found.\")",
          "205:             }",
          "206:           case _ => throw new IllegalStateException(\"Illegal value found in hset.\")",
          "207:         }",
          "209:       if (canCastSet.isEmpty && cannotCastSet.isEmpty) {",
          "210:         exp",
          "211:       } else {",
          "212:         val unwrapInSet = InSet(fromExp, nullSet ++ canCastSet)",
          "213:         cannotCastSet.headOption match {",
          "214:           case None => unwrapInSet",
          "217:           case Some(falseIfNotNull @ And(IsNull(_), Literal(null, BooleanType)))",
          "218:             if cannotCastSet.map(_.canonicalized).size == 1 => Or(falseIfNotNull, unwrapInSet)",
          "219:           case _ => exp",
          "220:         }",
          "221:       }",
          "",
          "[Added Lines]",
          "147:       val buildIn = {",
          "148:         (nullList: ArrayBuffer[Literal], canCastList: ArrayBuffer[Literal]) =>",
          "151:           val newList = nullList.map(lit => Cast(lit, fromExp.dataType)) ++ canCastList",
          "152:           In(fromExp, newList.toSeq)",
          "154:       simplifyIn(fromExp, toType, list, buildIn).getOrElse(exp)",
          "159:     case InSet(Cast(fromExp, toType: NumericType, _, _), hset)",
          "161:       val buildInSet =",
          "162:         (nullList: ArrayBuffer[Literal], canCastList: ArrayBuffer[Literal]) =>",
          "163:           InSet(fromExp, (nullList ++ canCastList).map(_.value).toSet)",
          "164:       simplifyIn(",
          "165:         fromExp,",
          "166:         toType,",
          "167:         hset.map(v => Literal.create(v, toType)).toSeq,",
          "168:         buildInSet).getOrElse(exp)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "346:     }",
          "347:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "296:   private def simplifyIn[IN <: Expression](",
          "297:       fromExp: Expression,",
          "298:       toType: NumericType,",
          "299:       list: Seq[Expression],",
          "300:       buildExpr: (ArrayBuffer[Literal], ArrayBuffer[Literal]) => IN): Option[Expression] = {",
          "318:     val (nullList, canCastList) = (ArrayBuffer[Literal](), ArrayBuffer[Literal]())",
          "319:     val fromType = fromExp.dataType",
          "320:     val ordering = toType.ordering.asInstanceOf[Ordering[Any]]",
          "322:     list.foreach {",
          "323:       case lit @ Literal(null, _) => nullList += lit",
          "324:       case NonNullLiteral(value, _) =>",
          "325:         val newValue = Cast(Literal(value), fromType, ansiEnabled = false).eval()",
          "326:         val valueRoundTrip = Cast(Literal(newValue, fromType), toType).eval()",
          "327:         if (newValue != null && ordering.compare(value, valueRoundTrip) == 0) {",
          "328:           canCastList += Literal(newValue, fromType)",
          "329:         }",
          "330:     }",
          "332:     if (nullList.isEmpty && canCastList.isEmpty) {",
          "334:       Option(falseIfNotNull(fromExp))",
          "335:     } else {",
          "336:       val unwrapExpr = buildExpr(nullList, canCastList)",
          "337:       Option(unwrapExpr)",
          "338:     }",
          "339:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparisonSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparisonSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparisonSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparisonSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "247:     val intLit = Literal.create(null, IntegerType)",
          "248:     val shortLit = Literal.create(null, ShortType)",
          "262:     checkInAndInSet(",
          "263:       In(Cast(f, LongType), Seq(1.toLong, 2.toLong, 3.toLong)),",
          "264:       f.in(1.toShort, 2.toShort, 3.toShort))",
          "",
          "[Removed Lines]",
          "250:     def checkInAndInSet(in: In, expected: Expression): Unit = {",
          "251:       assertEquivalent(in, expected)",
          "252:       val toInSet = (in: In) => InSet(in.value, HashSet() ++ in.list.map(_.eval()))",
          "253:       val expectedInSet = expected match {",
          "254:         case expectedIn: In =>",
          "255:           toInSet(expectedIn)",
          "256:         case Or(falseIfNotNull: And, expectedIn: In) =>",
          "257:           Or(falseIfNotNull, toInSet(expectedIn))",
          "258:       }",
          "259:       assertEquivalent(toInSet(in), expectedInSet)",
          "260:     }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "267:     checkInAndInSet(",
          "268:       In(Cast(f, LongType), Seq(1.toLong, Int.MaxValue.toLong, Long.MaxValue)),",
          "272:     checkInAndInSet(",
          "273:       In(Cast(f, LongType), Seq(Int.MaxValue.toLong, Long.MaxValue)),",
          "277:     checkInAndInSet(",
          "",
          "[Removed Lines]",
          "269:       Or(falseIfNotNull(f), f.in(1.toShort)))",
          "274:       Or(falseIfNotNull(f), f.in()))",
          "",
          "[Added Lines]",
          "257:       f.in(1.toShort))",
          "262:       falseIfNotNull(f))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "281:     checkInAndInSet(",
          "283:     checkInAndInSet(",
          "285:     checkInAndInSet(",
          "286:       In(Cast(f, IntegerType), Seq(intLit, 1)), f.in(shortLit, 1.toShort))",
          "287:     checkInAndInSet(",
          "288:       In(Cast(f, LongType), Seq(longLit, 1.toLong, Long.MaxValue)),",
          "290:     )",
          "291:   }",
          "293:   test(\"SPARK-36130: unwrap In should skip when in.list contains an expression that \" +",
          "294:     \"is not literal\") {",
          "295:     val add = Cast(f2, DoubleType) + 1.0d",
          "",
          "[Removed Lines]",
          "282:       In(Cast(f, IntegerType), Seq(intLit)), In(Cast(f, IntegerType), Seq(intLit)))",
          "284:       In(Cast(f, IntegerType), Seq(intLit, intLit)), In(Cast(f, IntegerType), Seq(intLit, intLit)))",
          "289:       Or(falseIfNotNull(f), f.in(shortLit, 1.toShort))",
          "",
          "[Added Lines]",
          "270:       In(Cast(f, IntegerType), Seq(intLit)), f.in(shortLit))",
          "272:       In(Cast(f, IntegerType), Seq(intLit, intLit)), f.in(shortLit, shortLit))",
          "277:       f.in(shortLit, 1.toShort)",
          "278:     )",
          "279:     checkInAndInSet(",
          "280:       In(Cast(f, LongType), Seq(longLit, Long.MaxValue)),",
          "281:       f.in(shortLit)",
          "285:   test(\"SPARK-39896: unwrap cast when the literal of In/InSet downcast failed\") {",
          "286:     val decimalValue = decimal2(123456.1234)",
          "287:     val decimalValue2 = decimal2(100.20)",
          "288:     checkInAndInSet(",
          "289:       In(castDecimal2(f3), Seq(decimalValue, decimalValue2)),",
          "290:       f3.in(decimal(decimalValue2)))",
          "291:   }",
          "293:   test(\"SPARK-39896: unwrap cast when the literal of In/Inset has round up or down\") {",
          "295:     val doubleValue = 1.0",
          "296:     val doubleValue1 = 100.6",
          "297:     checkInAndInSet(",
          "298:       In(castDouble(f), Seq(doubleValue1, doubleValue)),",
          "299:       f.in(doubleValue.toShort))",
          "302:     val doubleValue2 = 3.14",
          "303:     checkInAndInSet(",
          "304:       In(castDouble(f2), Seq(doubleValue2, doubleValue)),",
          "305:       f2.in(doubleValue.toFloat))",
          "308:     val decimalValue1 = decimal2(400.5678)",
          "309:     val decimalValue2 = decimal2(1.0)",
          "310:     checkInAndInSet(",
          "311:       In(castDecimal2(f3), Seq(decimalValue1, decimalValue2)),",
          "312:       f3.in(decimal(decimalValue2)))",
          "313:   }",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "374:       })",
          "375:     }",
          "376:   }",
          "377: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "400:   private def checkInAndInSet(in: In, expected: Expression): Unit = {",
          "401:     assertEquivalent(in, expected)",
          "402:     val toInSet = (in: In) => InSet(in.value, HashSet() ++ in.list.map(_.eval()))",
          "403:     val expectedInSet = expected match {",
          "404:       case expectedIn: In =>",
          "405:         toInSet(expectedIn)",
          "406:       case falseIfNotNull: And =>",
          "407:         falseIfNotNull",
          "408:     }",
          "409:     assertEquivalent(toInSet(in), expectedInSet)",
          "410:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7c69614f067c9eb68d997e8881d9b5845cde00fd",
      "candidate_info": {
        "commit_hash": "7c69614f067c9eb68d997e8881d9b5845cde00fd",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/7c69614f067c9eb68d997e8881d9b5845cde00fd",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala"
        ],
        "message": "[SPARK-39833][SQL] Disable Parquet column index in DSv1 to fix a correctness issue in the case of overlapping partition and data columns\n\n### What changes were proposed in this pull request?\n\nThis PR fixes a correctness issue in Parquet DSv1 FileFormat when projection does not contain columns referenced in pushed filters. This typically happens when partition columns and data columns overlap.\n\nThis could result in empty result when in fact there were records matching predicate as can be seen in the provided fields.\n\nThe problem is especially visible with `count()` and `show()` reporting different results, for example, show() would return 1+ records where the count() would return 0.\n\nIn Parquet, when the predicate is provided and column index is enabled, we would try to filter row ranges to figure out what the count should be. Unfortunately, there is an issue that if the projection is empty or is not in the set of filter columns, any checks on columns would fail and 0 rows are returned (`RowRanges.EMPTY`) even though there is data matching the filter.\n\nNote that this is rather a mitigation, a quick fix. The actual fix needs to go into Parquet-MR: https://issues.apache.org/jira/browse/PARQUET-2170.\n\nThe fix is not required in DSv2 where the overlapping columns are removed in `FileScanBuilder::readDataSchema()`.\n\n### Why are the changes needed?\n\nFixes a correctness issue when projection columns are not referenced by columns in pushed down filters or the schema is empty in Parquet DSv1.\n\nDownsides: Parquet column filter would be disabled if it had not been explicitly enabled which could affect performance.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nI added a unit test that reproduces this behaviour. The test fails without the fix and passes with the fix.\n\nCloses #37419 from sadikovi/SPARK-39833.\n\nAuthored-by: Ivan Sadikov <ivan.sadikov@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit cde71aaf173aadd14dd6393b09e9851b5caad903)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "230:       SQLConf.PARQUET_INT96_AS_TIMESTAMP.key,",
          "231:       sparkSession.sessionState.conf.isParquetINT96AsTimestamp)",
          "233:     val broadcastedHadoopConf =",
          "234:       sparkSession.sparkContext.broadcast(new SerializableConfiguration(hadoopConf))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "236:     hadoopConf.setBooleanIfUnset(ParquetInputFormat.COLUMN_INDEX_FILTERING_ENABLED, false)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1065:       }",
          "1066:     }",
          "1067:   }",
          "1068: }",
          "1070: class ParquetV2QuerySuite extends ParquetQuerySuite {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1069:   test(\"SPARK-39833: pushed filters with count()\") {",
          "1070:     withTempPath { path =>",
          "1071:       val p = s\"${path.getCanonicalPath}${File.separator}col=0${File.separator}\"",
          "1072:       Seq(0).toDF(\"COL\").coalesce(1).write.save(p)",
          "1073:       val df = spark.read.parquet(path.getCanonicalPath)",
          "1074:       checkAnswer(df.filter(\"col = 0\"), Seq(Row(0)))",
          "1075:       assert(df.filter(\"col = 0\").count() == 1, \"col\")",
          "1076:       assert(df.filter(\"COL = 0\").count() == 1, \"COL\")",
          "1077:     }",
          "1078:   }",
          "1080:   test(\"SPARK-39833: pushed filters with project without filter columns\") {",
          "1081:     withTempPath { path =>",
          "1082:       val p = s\"${path.getCanonicalPath}${File.separator}col=0${File.separator}\"",
          "1083:       Seq((0, 1)).toDF(\"COL\", \"a\").coalesce(1).write.save(p)",
          "1084:       val df = spark.read.parquet(path.getCanonicalPath)",
          "1085:       checkAnswer(df.filter(\"col = 0\"), Seq(Row(0, 1)))",
          "1086:       assert(df.filter(\"col = 0\").select(\"a\").collect().toSeq == Row(1) :: Nil)",
          "1087:       assert(df.filter(\"col = 0 and a = 1\").select(\"a\").collect().toSeq == Row(1) :: Nil)",
          "1088:     }",
          "1089:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}