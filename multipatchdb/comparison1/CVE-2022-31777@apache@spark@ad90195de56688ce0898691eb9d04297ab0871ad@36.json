{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "793ba608181b3eba8f1f57fcdd12dcd3fe035362",
      "candidate_info": {
        "commit_hash": "793ba608181b3eba8f1f57fcdd12dcd3fe035362",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/793ba608181b3eba8f1f57fcdd12dcd3fe035362",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ProjectionOverSchema.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/objects.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SchemaPruning.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala"
        ],
        "message": "[SPARK-38918][SQL] Nested column pruning should filter out attributes that do not belong to the current relation\n\n### What changes were proposed in this pull request?\nThis PR updates `ProjectionOverSchema`  to use the outputs of the data source relation to filter the attributes in the nested schema pruning. This is needed because the attributes in the schema do not necessarily belong to the current data source relation. For example, if a filter contains a correlated subquery, then the subquery's children can contain attributes from both the inner query and the outer query. Since the `RewriteSubquery` batch happens after early scan pushdown rules, nested schema pruning can wrongly use the inner query's attributes to prune the outer query data schema, thus causing wrong results and unexpected exceptions.\n\n### Why are the changes needed?\n\nTo fix a bug in `SchemaPruning`.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nUnit test\n\nCloses #36216 from allisonwang-db/spark-38918-nested-column-pruning.\n\nAuthored-by: allisonwang-db <allison.wang@databricks.com>\nSigned-off-by: Liang-Chi Hsieh <viirya@gmail.com>\n(cherry picked from commit 150434b5d7909dcf8248ffa5ec3d937ea3da09fd)\nSigned-off-by: Liang-Chi Hsieh <viirya@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ProjectionOverSchema.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ProjectionOverSchema.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/objects.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/objects.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SchemaPruning.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SchemaPruning.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ProjectionOverSchema.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ProjectionOverSchema.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ProjectionOverSchema.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/ProjectionOverSchema.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "29:   private val fieldNames = schema.fieldNames.toSet",
          "31:   def unapply(expr: Expression): Option[Expression] = getProjection(expr)",
          "33:   private def getProjection(expr: Expression): Option[Expression] =",
          "34:     expr match {",
          "36:         Some(a.copy(dataType = schema(a.name).dataType)(a.exprId, a.qualifier))",
          "37:       case GetArrayItem(child, arrayItemOrdinal, failOnError) =>",
          "38:         getProjection(child).map {",
          "",
          "[Removed Lines]",
          "28: case class ProjectionOverSchema(schema: StructType) {",
          "35:       case a: AttributeReference if fieldNames.contains(a.name) =>",
          "",
          "[Added Lines]",
          "32: case class ProjectionOverSchema(schema: StructType, output: AttributeSet) {",
          "39:       case a: AttributeReference if fieldNames.contains(a.name) && output.contains(a) =>",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "60:   override protected val excludedOnceBatches: Set[String] =",
          "61:     Set(",
          "62:       \"PartitionPruning\",",
          "63:       \"Extract Python UDFs\")",
          "65:   protected def fixedPoint =",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "63:       \"RewriteSubquery\",",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/objects.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/objects.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/objects.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/objects.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "229:         }",
          "233:         val newProjects = p.projectList.map(_.transformDown {",
          "234:           case projectionOverSchema(expr) => expr",
          "235:         }).map { case expr: NamedExpression => expr }",
          "",
          "[Removed Lines]",
          "232:         val projectionOverSchema = ProjectionOverSchema(prunedSchema)",
          "",
          "[Added Lines]",
          "232:         val projectionOverSchema = ProjectionOverSchema(prunedSchema, AttributeSet(s.output))",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SchemaPruning.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SchemaPruning.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SchemaPruning.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/SchemaPruning.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "91:       if (countLeaves(hadoopFsRelation.dataSchema) > countLeaves(prunedDataSchema) ||",
          "92:         countLeaves(metadataSchema) > countLeaves(prunedMetadataSchema)) {",
          "93:         val prunedRelation = leafNodeBuilder(prunedDataSchema, prunedMetadataSchema)",
          "96:         Some(buildNewProjection(projects, normalizedProjects, normalizedFilters,",
          "97:           prunedRelation, projectionOverSchema))",
          "98:       } else {",
          "",
          "[Removed Lines]",
          "94:         val projectionOverSchema =",
          "95:           ProjectionOverSchema(prunedDataSchema.merge(prunedMetadataSchema))",
          "",
          "[Added Lines]",
          "94:         val projectionOverSchema = ProjectionOverSchema(",
          "95:           prunedDataSchema.merge(prunedMetadataSchema), AttributeSet(relation.output))",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/V2ScanRelationPushDown.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import scala.collection.mutable",
          "24: import org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression",
          "25: import org.apache.spark.sql.catalyst.optimizer.CollapseProject",
          "26: import org.apache.spark.sql.catalyst.planning.ScanOperation",
          "",
          "[Removed Lines]",
          "22: import org.apache.spark.sql.catalyst.expressions.{Alias, AliasHelper, And, Attribute, AttributeReference, Cast, Expression, IntegerLiteral, NamedExpression, PredicateHelper, ProjectionOverSchema, SortOrder, SubqueryExpression}",
          "23: import org.apache.spark.sql.catalyst.expressions.aggregate",
          "",
          "[Added Lines]",
          "22: import org.apache.spark.sql.catalyst.expressions.{aggregate, Alias, AliasHelper, And, Attribute, AttributeReference, AttributeSet, Cast, Expression, IntegerLiteral, NamedExpression, PredicateHelper, ProjectionOverSchema, SortOrder, SubqueryExpression}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "321:       val scanRelation = DataSourceV2ScanRelation(sHolder.relation, wrappedScan, output)",
          "324:       val projectionFunc = (expr: Expression) => expr transformDown {",
          "325:         case projectionOverSchema(newExpr) => newExpr",
          "326:       }",
          "",
          "[Removed Lines]",
          "323:       val projectionOverSchema = ProjectionOverSchema(output.toStructType)",
          "",
          "[Added Lines]",
          "322:       val projectionOverSchema =",
          "323:         ProjectionOverSchema(output.toStructType, AttributeSet(output))",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "61:   override protected def sparkConf: SparkConf =",
          "62:     super.sparkConf.set(SQLConf.ANSI_STRICT_INDEX_OPERATOR.key, \"false\")",
          "64:   val janeDoe = FullName(\"Jane\", \"X.\", \"Doe\")",
          "65:   val johnDoe = FullName(\"John\", \"Y.\", \"Doe\")",
          "66:   val susanSmith = FullName(\"Susan\", \"Z.\", \"Smith\")",
          "69:   val employerWithNullCompany = Employer(1, null)",
          "70:   val employerWithNullCompany2 = Employer(2, null)",
          "",
          "[Removed Lines]",
          "68:   val employer = Employer(0, Company(\"abc\", \"123 Business Street\"))",
          "",
          "[Added Lines]",
          "64:   case class Employee(id: Int, name: FullName, employer: Company)",
          "70:   val company = Company(\"abc\", \"123 Business Street\")",
          "72:   val employer = Employer(0, company)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "81:     Department(1, \"Marketing\", 1, employerWithNullCompany) ::",
          "82:     Department(2, \"Operation\", 4, employerWithNullCompany2) :: Nil",
          "84:   case class Name(first: String, last: String)",
          "85:   case class BriefContact(id: Int, name: Name, address: String)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "88:   val employees = Employee(0, janeDoe, company) :: Employee(1, johnDoe, company) :: Nil",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "621:     }",
          "622:   }",
          "624:   protected def testSchemaPruning(testName: String)(testThunk: => Unit): Unit = {",
          "625:     test(s\"Spark vectorized reader - without partition data column - $testName\") {",
          "626:       withSQLConf(vectorizedReaderEnabledKey -> \"true\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "630:   testSchemaPruning(\"SPARK-38918: nested schema pruning with correlated subqueries\") {",
          "631:     withContacts {",
          "632:       withEmployees {",
          "633:         val query = sql(",
          "634:           \"\"\"",
          "635:             |select count(*)",
          "636:             |from contacts c",
          "637:             |where not exists (select null from employees e where e.name.first = c.name.first",
          "638:             |  and e.employer.name = c.employer.company.name)",
          "639:             |\"\"\".stripMargin)",
          "640:         checkScan(query,",
          "641:           \"struct<name:struct<first:string,middle:string,last:string>,\" +",
          "642:             \"employer:struct<id:int,company:struct<name:string,address:string>>>\",",
          "643:           \"struct<name:struct<first:string,middle:string,last:string>,\" +",
          "644:             \"employer:struct<name:string,address:string>>\")",
          "645:         checkAnswer(query, Row(3))",
          "646:       }",
          "647:     }",
          "648:   }",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "701:     }",
          "702:   }",
          "704:   case class MixedCaseColumn(a: String, B: Int)",
          "705:   case class MixedCase(id: Int, CoL1: String, coL2: MixedCaseColumn)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "730:   private def withEmployees(testThunk: => Unit): Unit = {",
          "731:     withTempPath { dir =>",
          "732:       val path = dir.getCanonicalPath",
          "734:       makeDataSourceFile(employees, new File(path + \"/employees\"))",
          "738:       val schema = \"`id` INT,`name` STRUCT<`first`: STRING, `middle`: STRING, `last`: STRING>, \" +",
          "739:         \"`employer` STRUCT<`name`: STRING, `address`: STRING>\"",
          "740:       spark.read.format(dataSourceName).schema(schema).load(path + \"/employees\")",
          "741:         .createOrReplaceTempView(\"employees\")",
          "743:       testThunk",
          "744:     }",
          "745:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2069fd03fd30faaabd1d73ca0416a76ab5908937",
      "candidate_info": {
        "commit_hash": "2069fd03fd30faaabd1d73ca0416a76ab5908937",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/2069fd03fd30faaabd1d73ca0416a76ab5908937",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala"
        ],
        "message": "[SPARK-39677][SQL][DOCS] Fix args formatting of the regexp and like functions\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to fix args formatting of some regexp functions by adding explicit new lines. That fixes the following items in arg lists.\n\nBefore:\n\n<img width=\"745\" alt=\"Screenshot 2022-07-05 at 09 48 28\" src=\"https://user-images.githubusercontent.com/1580697/177274234-04209d43-a542-4c71-b5ca-6f3239208015.png\">\n\nAfter:\n\n<img width=\"704\" alt=\"Screenshot 2022-07-05 at 11 06 13\" src=\"https://user-images.githubusercontent.com/1580697/177280718-cb05184c-8559-4461-b94d-dfaaafda7dd2.png\">\n\n### Why are the changes needed?\nTo improve readability of Spark SQL docs.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nBy building docs and checking manually:\n```\n$ SKIP_SCALADOC=1 SKIP_PYTHONDOC=1 SKIP_RDOC=1 bundle exec jekyll build\n```\n\nCloses #37082 from MaxGekk/fix-regexp-docs.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Max Gekk <max.gekk@gmail.com>\n(cherry picked from commit 4e42f8b12e8dc57a15998f22d508a19cf3c856aa)\nSigned-off-by: Max Gekk <max.gekk@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/regexpExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "84:     Arguments:",
          "91:           % matches zero or more characters in the input (similar to .* in posix regular",
          "94:           Since Spark 2.0, string literals are unescaped in our SQL parser. For example, in order",
          "97:           When SQL config 'spark.sql.parser.escapedStringLiterals' is enabled, it falls back",
          "98:           to Spark 1.6 behavior regarding string literal parsing. For example, if the config is",
          "99:           enabled, the pattern to match \"\\abc\" should be \"\\abc\".",
          "",
          "[Removed Lines]",
          "87:           exception to the following special symbols:",
          "89:           _ matches any one character in the input (similar to . in posix regular expressions)",
          "92:           expressions)",
          "95:           to match \"\\abc\", the pattern should be \"\\\\abc\".",
          "",
          "[Added Lines]",
          "87:           exception to the following special symbols:<br><br>",
          "88:           _ matches any one character in the input (similar to . in posix regular expressions)\\",
          "90:           expressions)<br><br>",
          "92:           to match \"\\abc\", the pattern should be \"\\\\abc\".<br><br>",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "200:     Arguments:",
          "207:           % matches zero or more characters in the input (similar to .* in posix regular",
          "210:           Since Spark 2.0, string literals are unescaped in our SQL parser. For example, in order",
          "213:           When SQL config 'spark.sql.parser.escapedStringLiterals' is enabled, it falls back",
          "214:           to Spark 1.6 behavior regarding string literal parsing. For example, if the config is",
          "215:           enabled, the pattern to match \"\\abc\" should be \"\\abc\".",
          "",
          "[Removed Lines]",
          "203:           case-insensitively, with exception to the following special symbols:",
          "205:           _ matches any one character in the input (similar to . in posix regular expressions)",
          "208:           expressions)",
          "211:           to match \"\\abc\", the pattern should be \"\\\\abc\".",
          "",
          "[Added Lines]",
          "199:           case-insensitively, with exception to the following special symbols:<br><br>",
          "200:           _ matches any one character in the input (similar to . in posix regular expressions)<br><br>",
          "202:           expressions)<br><br>",
          "204:           to match \"\\abc\", the pattern should be \"\\\\abc\".<br><br>",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "574:     Arguments:",
          "579:           Since Spark 2.0, string literals (including regex patterns) are unescaped in our SQL",
          "580:           parser. For example, to match \"\\abc\", a regular expression for `regexp` can be",
          "583:           There is a SQL config 'spark.sql.parser.escapedStringLiterals' that can be used to",
          "584:           fallback to the Spark 1.6 behavior regarding string literal parsing. For example,",
          "585:           if the config is enabled, the `regexp` that can match \"\\abc\" is \"^\\abc$\".",
          "",
          "[Removed Lines]",
          "577:           Java regular expression.",
          "581:           \"^\\\\abc$\".",
          "",
          "[Added Lines]",
          "569:           Java regular expression.<br><br>",
          "572:           \"^\\\\abc$\".<br><br>",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "783:     Arguments:",
          "788:           Since Spark 2.0, string literals (including regex patterns) are unescaped in our SQL",
          "789:           parser. For example, to match \"\\abc\", a regular expression for `regexp` can be",
          "792:           There is a SQL config 'spark.sql.parser.escapedStringLiterals' that can be used to",
          "793:           fallback to the Spark 1.6 behavior regarding string literal parsing. For example,",
          "794:           if the config is enabled, the `regexp` that can match \"\\abc\" is \"^\\abc$\".",
          "",
          "[Removed Lines]",
          "786:           Java regular expression.",
          "790:           \"^\\\\abc$\".",
          "",
          "[Added Lines]",
          "776:           Java regular expression.<br><br>",
          "779:           \"^\\\\abc$\".<br><br>",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "888:     Arguments:",
          "893:           Since Spark 2.0, string literals (including regex patterns) are unescaped in our SQL",
          "894:           parser. For example, to match \"\\abc\", a regular expression for `regexp` can be",
          "897:           There is a SQL config 'spark.sql.parser.escapedStringLiterals' that can be used to",
          "898:           fallback to the Spark 1.6 behavior regarding string literal parsing. For example,",
          "899:           if the config is enabled, the `regexp` that can match \"\\abc\" is \"^\\abc$\".",
          "",
          "[Removed Lines]",
          "891:           Java regular expression.",
          "895:           \"^\\\\abc$\".",
          "",
          "[Added Lines]",
          "879:           Java regular expression.<br><br>",
          "882:           \"^\\\\abc$\".<br><br>",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "30bb19e23d28f454e35c96d20db70db5650bd160",
      "candidate_info": {
        "commit_hash": "30bb19e23d28f454e35c96d20db70db5650bd160",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/30bb19e23d28f454e35c96d20db70db5650bd160",
        "files": [
          "dev/deps/spark-deps-hadoop-2-hive-2.3",
          "pom.xml"
        ],
        "message": "[SPARK-39183][BUILD] Upgrade Apache Xerces Java to 2.12.2\n\n### What changes were proposed in this pull request?\nUpgrade Apache Xerces Java to 2.12.2\n\n[Release notes](https://xerces.apache.org/xerces2-j/releases.html)\n\n### Why are the changes needed?\n[Infinite Loop in Apache Xerces Java](https://github.com/advisories/GHSA-h65f-jvqw-m9fj)\n\nThere's a vulnerability within the Apache Xerces Java (XercesJ) XML parser when handling specially crafted XML document payloads. This causes, the XercesJ XML parser to wait in an infinite loop, which may sometimes consume system resources for prolonged duration. This vulnerability is present within XercesJ version 2.12.1 and the previous versions.\n\nReferences\nhttps://nvd.nist.gov/vuln/detail/CVE-2022-23437\nhttps://lists.apache.org/thread/6pjwm10bb69kq955fzr1n0nflnjd27dl\nhttp://www.openwall.com/lists/oss-security/2022/01/24/3\nhttps://www.oracle.com/security-alerts/cpuapr2022.html\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nPass GA.\n\nCloses #36544 from bjornjorgensen/Upgrade-xerces-to-2.12.2.\n\nAuthored-by: bjornjorgensen <bjornjorgensen@gmail.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n(cherry picked from commit 181436bd990d3bdf178a33fa6489ad416f3e7f94)\nSigned-off-by: Sean Owen <srowen@gmail.com>",
        "before_after_code_files": [
          "dev/deps/spark-deps-hadoop-2-hive-2.3||dev/deps/spark-deps-hadoop-2-hive-2.3"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/deps/spark-deps-hadoop-2-hive-2.3||dev/deps/spark-deps-hadoop-2-hive-2.3": [
          "File: dev/deps/spark-deps-hadoop-2-hive-2.3 -> dev/deps/spark-deps-hadoop-2-hive-2.3",
          "--- Hunk 1 ---",
          "[Context before]",
          "260: univocity-parsers/2.9.1//univocity-parsers-2.9.1.jar",
          "261: velocity/1.5//velocity-1.5.jar",
          "262: xbean-asm9-shaded/4.20//xbean-asm9-shaded-4.20.jar",
          "264: xml-apis/1.4.01//xml-apis-1.4.01.jar",
          "265: xmlenc/0.52//xmlenc-0.52.jar",
          "266: xz/1.8//xz-1.8.jar",
          "",
          "[Removed Lines]",
          "263: xercesImpl/2.12.0//xercesImpl-2.12.0.jar",
          "",
          "[Added Lines]",
          "263: xercesImpl/2.12.2//xercesImpl-2.12.2.jar",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "baa93ec66e3068dcbcb52b83fb94101ddaf0a7e4",
      "candidate_info": {
        "commit_hash": "baa93ec66e3068dcbcb52b83fb94101ddaf0a7e4",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/baa93ec66e3068dcbcb52b83fb94101ddaf0a7e4",
        "files": [
          "sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java",
          "sql/catalyst/src/main/java/org/apache/spark/sql/types/DataTypes.java",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/Encoders.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CatalystTypeConverters.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/JavaTypeInference.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/RowEncoder.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/types/TimestampNTZType.scala",
          "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetVectorUpdaterFactory.java",
          "sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala",
          "sql/core/src/test/java/test/org/apache/spark/sql/JavaDatasetSuite.java",
          "sql/core/src/test/scala/org/apache/spark/sql/CsvFunctionsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/JsonFunctionsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/UDFSuite.scala"
        ],
        "message": "[SPARK-38813][SQL][3.3] Remove TimestampNTZ type support in Spark 3.3\n\n### What changes were proposed in this pull request?\n\nRemove TimestampNTZ type support in the production code of Spark 3.3.\nTo archive the goal, this PR adds the check \"Utils.isTesting\" in the following code branches:\n- keyword \"timestamp_ntz\" and \"timestamp_ltz\" in parser\n- New expressions from https://issues.apache.org/jira/browse/SPARK-35662\n- Using java.time.localDateTime as the external type for TimestampNTZType\n- `SQLConf.timestampType` which determines the default timestamp type of Spark SQL.\n\nThis is to minimize the code difference between the master branch. So that future users won't think TimestampNTZ is already available in Spark 3.3.\nThe downside is that users can still find TimestampNTZType under package `org.apache.spark.sql.types`. There should be nothing left other than this.\n\n### Why are the changes needed?\n\nThe TimestampNTZ project is not finished yet:\n* Lack Hive metastore support\n* Lack JDBC support\n* Need to spend time scanning the codebase to find out any missing support. The current code usages of `TimestampType` are larger than `TimestampNTZType`\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, the TimestampNTZ type is not released yet.\n\n### How was this patch tested?\n\nUT\n\nCloses #36094 from gengliangwang/disableNTZ.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java||sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java",
          "sql/catalyst/src/main/java/org/apache/spark/sql/types/DataTypes.java||sql/catalyst/src/main/java/org/apache/spark/sql/types/DataTypes.java",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/Encoders.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/Encoders.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CatalystTypeConverters.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CatalystTypeConverters.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/JavaTypeInference.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/JavaTypeInference.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/RowEncoder.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/RowEncoder.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/types/TimestampNTZType.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/types/TimestampNTZType.scala",
          "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetVectorUpdaterFactory.java||sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetVectorUpdaterFactory.java",
          "sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala||sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala",
          "sql/core/src/test/java/test/org/apache/spark/sql/JavaDatasetSuite.java||sql/core/src/test/java/test/org/apache/spark/sql/JavaDatasetSuite.java",
          "sql/core/src/test/scala/org/apache/spark/sql/CsvFunctionsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/CsvFunctionsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/JsonFunctionsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/JsonFunctionsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/UDFSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/UDFSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java||sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java": [
          "File: sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java -> sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/UnsafeRow.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "91:           DoubleType,",
          "92:           DateType,",
          "93:           TimestampType,",
          "95:         )));",
          "96:   }",
          "",
          "[Removed Lines]",
          "94:           TimestampNTZType",
          "",
          "[Added Lines]",
          "94:           TimestampNTZType$.MODULE$",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/java/org/apache/spark/sql/types/DataTypes.java||sql/catalyst/src/main/java/org/apache/spark/sql/types/DataTypes.java": [
          "File: sql/catalyst/src/main/java/org/apache/spark/sql/types/DataTypes.java -> sql/catalyst/src/main/java/org/apache/spark/sql/types/DataTypes.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "55:   public static final DataType TimestampType = TimestampType$.MODULE$;",
          "",
          "[Removed Lines]",
          "60:   public static final DataType TimestampNTZType = TimestampNTZType$.MODULE$;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/Encoders.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/Encoders.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/Encoders.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/Encoders.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "115:   def LOCALDATE: Encoder[java.time.LocalDate] = ExpressionEncoder()",
          "",
          "[Removed Lines]",
          "123:   def LOCALDATETIME: Encoder[java.time.LocalDateTime] = ExpressionEncoder()",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CatalystTypeConverters.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CatalystTypeConverters.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CatalystTypeConverters.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CatalystTypeConverters.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "35: import org.apache.spark.sql.types.DayTimeIntervalType._",
          "36: import org.apache.spark.sql.types.YearMonthIntervalType._",
          "37: import org.apache.spark.unsafe.types.UTF8String",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "38: import org.apache.spark.util.Utils",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "503:     case ld: LocalDate => LocalDateConverter.toCatalyst(ld)",
          "504:     case t: Timestamp => TimestampConverter.toCatalyst(t)",
          "505:     case i: Instant => InstantConverter.toCatalyst(i)",
          "507:     case d: BigDecimal => new DecimalConverter(DecimalType(d.precision, d.scale)).toCatalyst(d)",
          "508:     case d: JavaBigDecimal => new DecimalConverter(DecimalType(d.precision, d.scale)).toCatalyst(d)",
          "509:     case seq: Seq[Any] => new GenericArrayData(seq.map(convertToCatalyst).toArray)",
          "",
          "[Removed Lines]",
          "506:     case l: LocalDateTime => TimestampNTZConverter.toCatalyst(l)",
          "",
          "[Added Lines]",
          "508:     case l: LocalDateTime if Utils.isTesting => TimestampNTZConverter.toCatalyst(l)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/JavaTypeInference.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/JavaTypeInference.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/JavaTypeInference.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/JavaTypeInference.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "35: import org.apache.spark.sql.catalyst.util.ArrayBasedMapData",
          "36: import org.apache.spark.sql.errors.QueryExecutionErrors",
          "37: import org.apache.spark.sql.types._",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "38: import org.apache.spark.util.Utils",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "119:       case c: Class[_] if c == classOf[java.sql.Date] => (DateType, true)",
          "120:       case c: Class[_] if c == classOf[java.time.Instant] => (TimestampType, true)",
          "121:       case c: Class[_] if c == classOf[java.sql.Timestamp] => (TimestampType, true)",
          "123:       case c: Class[_] if c == classOf[java.time.Duration] => (DayTimeIntervalType(), true)",
          "124:       case c: Class[_] if c == classOf[java.time.Period] => (YearMonthIntervalType(), true)",
          "",
          "[Removed Lines]",
          "122:       case c: Class[_] if c == classOf[java.time.LocalDateTime] => (TimestampNTZType, true)",
          "",
          "[Added Lines]",
          "124:       case c: Class[_] if c == classOf[java.time.LocalDateTime] && Utils.isTesting =>",
          "125:         (TimestampNTZType, true)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "251:       case c if c == classOf[java.sql.Timestamp] =>",
          "252:         createDeserializerForSqlTimestamp(path)",
          "255:         createDeserializerForLocalDateTime(path)",
          "257:       case c if c == classOf[java.time.Duration] =>",
          "",
          "[Removed Lines]",
          "254:       case c if c == classOf[java.time.LocalDateTime] =>",
          "",
          "[Added Lines]",
          "258:       case c if c == classOf[java.time.LocalDateTime] && Utils.isTesting =>",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "414:         case c if c == classOf[java.sql.Timestamp] => createSerializerForSqlTimestamp(inputObject)",
          "417:           createSerializerForLocalDateTime(inputObject)",
          "419:         case c if c == classOf[java.time.LocalDate] => createSerializerForJavaLocalDate(inputObject)",
          "",
          "[Removed Lines]",
          "416:         case c if c == classOf[java.time.LocalDateTime] =>",
          "",
          "[Added Lines]",
          "421:         case c if c == classOf[java.time.LocalDateTime] && Utils.isTesting =>",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: import org.apache.spark.sql.errors.QueryExecutionErrors",
          "32: import org.apache.spark.sql.types._",
          "33: import org.apache.spark.unsafe.types.{CalendarInterval, UTF8String}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "34: import org.apache.spark.util.Utils",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "752:         Schema(TimestampType, nullable = true)",
          "753:       case t if isSubtype(t, localTypeOf[java.sql.Timestamp]) =>",
          "754:         Schema(TimestampType, nullable = true)",
          "756:         Schema(TimestampNTZType, nullable = true)",
          "757:       case t if isSubtype(t, localTypeOf[java.time.LocalDate]) => Schema(DateType, nullable = true)",
          "758:       case t if isSubtype(t, localTypeOf[java.sql.Date]) => Schema(DateType, nullable = true)",
          "",
          "[Removed Lines]",
          "755:       case t if isSubtype(t, localTypeOf[java.time.LocalDateTime]) =>",
          "",
          "[Added Lines]",
          "757:       case t if isSubtype(t, localTypeOf[java.time.LocalDateTime]) && Utils.isTesting =>",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "858:     StringType -> classOf[UTF8String],",
          "859:     DateType -> classOf[DateType.InternalType],",
          "860:     TimestampType -> classOf[TimestampType.InternalType],",
          "862:     BinaryType -> classOf[BinaryType.InternalType],",
          "863:     CalendarIntervalType -> classOf[CalendarInterval]",
          "864:   )",
          "",
          "[Removed Lines]",
          "861:     TimestampNTZType -> classOf[TimestampNTZType.InternalType],",
          "",
          "[Added Lines]",
          "864:     TimestampNTZType ->",
          "865:       (if (Utils.isTesting) classOf[TimestampNTZType.InternalType] else classOf[java.lang.Object]),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "873:     DoubleType -> classOf[java.lang.Double],",
          "874:     DateType -> classOf[java.lang.Integer],",
          "875:     TimestampType -> classOf[java.lang.Long],",
          "877:   )",
          "879:   def dataTypeJavaClass(dt: DataType): Class[_] = {",
          "",
          "[Removed Lines]",
          "876:     TimestampNTZType -> classOf[java.lang.Long]",
          "",
          "[Added Lines]",
          "881:     TimestampNTZType ->",
          "882:       (if (Utils.isTesting) classOf[java.lang.Long] else classOf[java.lang.Object])",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "32: import org.apache.spark.sql.internal.SQLConf",
          "33: import org.apache.spark.sql.types._",
          "34: import org.apache.spark.sql.util.SchemaUtils",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "35: import org.apache.spark.util.Utils",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "157:       case _: ShowTableExtended =>",
          "158:         throw QueryCompilationErrors.commandUnsupportedInV2TableError(\"SHOW TABLE EXTENDED\")",
          "160:       case operator: LogicalPlan =>",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "161:       case operator: LogicalPlan",
          "162:         if !Utils.isTesting && operator.output.exists(_.dataType.isInstanceOf[TimestampNTZType]) =>",
          "163:         operator.failAnalysis(\"TimestampNTZ type is not supported in Spark 3.3.\")",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: import org.apache.spark.sql.catalyst.trees.TreeNodeTag",
          "35: import org.apache.spark.sql.errors.QueryCompilationErrors",
          "36: import org.apache.spark.sql.types._",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "37: import org.apache.spark.util.Utils",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "361:   val expressions: Map[String, (ExpressionInfo, FunctionBuilder)] = Map(",
          "363:     expression[Abs](\"abs\"),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "362:   val expressionsForTimestampNTZSupport: Map[String, (ExpressionInfo, FunctionBuilder)] =",
          "364:     if (Utils.isTesting) {",
          "365:       Map(",
          "366:         expression[LocalTimestamp](\"localtimestamp\"),",
          "367:         expression[ConvertTimezone](\"convert_timezone\"),",
          "369:         expressionBuilder(",
          "370:           \"to_timestamp_ntz\", ParseToTimestampNTZExpressionBuilder, setAlias = true),",
          "371:         expressionBuilder(",
          "372:           \"to_timestamp_ltz\", ParseToTimestampLTZExpressionBuilder, setAlias = true),",
          "374:         expressionBuilder(\"make_timestamp_ntz\", MakeTimestampNTZExpressionBuilder, setAlias = true),",
          "375:         expressionBuilder(\"make_timestamp_ltz\", MakeTimestampLTZExpressionBuilder, setAlias = true)",
          "376:       )",
          "377:     } else {",
          "378:       Map.empty",
          "379:     }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "580:     expression[CurrentDate](\"current_date\"),",
          "581:     expression[CurrentTimestamp](\"current_timestamp\"),",
          "582:     expression[CurrentTimeZone](\"current_timezone\"),",
          "584:     expression[DateDiff](\"datediff\"),",
          "585:     expression[DateAdd](\"date_add\"),",
          "586:     expression[DateFormatClass](\"date_format\"),",
          "",
          "[Removed Lines]",
          "583:     expression[LocalTimestamp](\"localtimestamp\"),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "604:     expression[ToBinary](\"to_binary\"),",
          "605:     expression[ToUnixTimestamp](\"to_unix_timestamp\"),",
          "606:     expression[ToUTCTimestamp](\"to_utc_timestamp\"),",
          "610:     expression[TruncDate](\"trunc\"),",
          "611:     expression[TruncTimestamp](\"date_trunc\"),",
          "612:     expression[UnixTimestamp](\"unix_timestamp\"),",
          "",
          "[Removed Lines]",
          "608:     expressionBuilder(\"to_timestamp_ntz\", ParseToTimestampNTZExpressionBuilder, setAlias = true),",
          "609:     expressionBuilder(\"to_timestamp_ltz\", ParseToTimestampLTZExpressionBuilder, setAlias = true),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "618:     expression[SessionWindow](\"session_window\"),",
          "619:     expression[MakeDate](\"make_date\"),",
          "620:     expression[MakeTimestamp](\"make_timestamp\"),",
          "624:     expression[MakeInterval](\"make_interval\"),",
          "625:     expression[MakeDTInterval](\"make_dt_interval\"),",
          "626:     expression[MakeYMInterval](\"make_ym_interval\"),",
          "",
          "[Removed Lines]",
          "622:     expressionBuilder(\"make_timestamp_ntz\", MakeTimestampNTZExpressionBuilder, setAlias = true),",
          "623:     expressionBuilder(\"make_timestamp_ltz\", MakeTimestampLTZExpressionBuilder, setAlias = true),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "635:     expression[UnixSeconds](\"unix_seconds\"),",
          "636:     expression[UnixMillis](\"unix_millis\"),",
          "637:     expression[UnixMicros](\"unix_micros\"),",
          "641:     expression[CreateArray](\"array\"),",
          "",
          "[Removed Lines]",
          "638:     expression[ConvertTimezone](\"convert_timezone\"),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "782:     expression[CsvToStructs](\"from_csv\"),",
          "783:     expression[SchemaOfCsv](\"schema_of_csv\"),",
          "784:     expression[StructsToCsv](\"to_csv\")",
          "787:   val builtin: SimpleFunctionRegistry = {",
          "788:     val fr = new SimpleFunctionRegistry",
          "",
          "[Removed Lines]",
          "785:   )",
          "",
          "[Added Lines]",
          "798:   ) ++ expressionsForTimestampNTZSupport",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "316:       def timestamp: AttributeReference = AttributeReference(s, TimestampType, nullable = true)()",
          "323:       def dayTimeInterval(startField: Byte, endField: Byte): AttributeReference = {",
          "324:         AttributeReference(s, DayTimeIntervalType(startField, endField), nullable = true)()",
          "",
          "[Removed Lines]",
          "319:       def timestampNTZ: AttributeReference =",
          "320:         AttributeReference(s, TimestampNTZType, nullable = true)()",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/RowEncoder.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/RowEncoder.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/RowEncoder.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/RowEncoder.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "32: import org.apache.spark.sql.errors.QueryExecutionErrors",
          "33: import org.apache.spark.sql.internal.SQLConf",
          "34: import org.apache.spark.sql.types._",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "35: import org.apache.spark.util.Utils",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "112:         createSerializerForSqlTimestamp(inputObject)",
          "113:       }",
          "117:     case DateType =>",
          "118:       if (lenient) {",
          "",
          "[Removed Lines]",
          "115:     case TimestampNTZType => createSerializerForLocalDateTime(inputObject)",
          "",
          "[Added Lines]",
          "117:     case TimestampNTZType if Utils.isTesting => createSerializerForLocalDateTime(inputObject)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "243:       } else {",
          "244:         ObjectType(classOf[java.sql.Timestamp])",
          "245:       }",
          "247:       ObjectType(classOf[java.time.LocalDateTime])",
          "248:     case DateType =>",
          "249:       if (SQLConf.get.datetimeJava8ApiEnabled) {",
          "",
          "[Removed Lines]",
          "246:     case TimestampNTZType =>",
          "",
          "[Added Lines]",
          "249:     case TimestampNTZType if Utils.isTesting =>",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "301:         createDeserializerForSqlTimestamp(input)",
          "302:       }",
          "305:       createDeserializerForLocalDateTime(input)",
          "307:     case DateType =>",
          "",
          "[Removed Lines]",
          "304:     case TimestampNTZType =>",
          "",
          "[Added Lines]",
          "308:     case TimestampNTZType if Utils.isTesting =>",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/datetimeExpressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "219:        2020-04-25 15:49:11.914",
          "220:   \"\"\",",
          "221:   group = \"datetime_funcs\",",
          "223: case class LocalTimestamp(timeZoneId: Option[String] = None) extends LeafExpression",
          "224:   with TimeZoneAwareExpression with CodegenFallback {",
          "",
          "[Removed Lines]",
          "222:   since = \"3.3.0\")",
          "",
          "[Added Lines]",
          "222:   since = \"3.4.0\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1111:        2016-12-31 00:00:00",
          "1112:   \"\"\",",
          "1113:   group = \"datetime_funcs\",",
          "1116: object ParseToTimestampNTZExpressionBuilder extends ExpressionBuilder {",
          "1117:   override def build(funcName: String, expressions: Seq[Expression]): Expression = {",
          "",
          "[Removed Lines]",
          "1114:   since = \"3.3.0\")",
          "",
          "[Added Lines]",
          "1114:   since = \"3.4.0\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1148:        2016-12-31 00:00:00",
          "1149:   \"\"\",",
          "1150:   group = \"datetime_funcs\",",
          "1153: object ParseToTimestampLTZExpressionBuilder extends ExpressionBuilder {",
          "1154:   override def build(funcName: String, expressions: Seq[Expression]): Expression = {",
          "",
          "[Removed Lines]",
          "1151:   since = \"3.3.0\")",
          "",
          "[Added Lines]",
          "1151:   since = \"3.4.0\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "2440:        NULL",
          "2441:   \"\"\",",
          "2442:   group = \"datetime_funcs\",",
          "2445: object MakeTimestampNTZExpressionBuilder extends ExpressionBuilder {",
          "2446:   override def build(funcName: String, expressions: Seq[Expression]): Expression = {",
          "",
          "[Removed Lines]",
          "2443:   since = \"3.3.0\")",
          "",
          "[Added Lines]",
          "2443:   since = \"3.4.0\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "2487:        NULL",
          "2488:   \"\"\",",
          "2489:   group = \"datetime_funcs\",",
          "2492: object MakeTimestampLTZExpressionBuilder extends ExpressionBuilder {",
          "2493:   override def build(funcName: String, expressions: Seq[Expression]): Expression = {",
          "",
          "[Removed Lines]",
          "2490:   since = \"3.3.0\")",
          "",
          "[Added Lines]",
          "2490:   since = \"3.4.0\")",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "3015:        2021-12-06 00:00:00",
          "3016:   \"\"\",",
          "3017:   group = \"datetime_funcs\",",
          "3020: case class ConvertTimezone(",
          "3021:     sourceTz: Expression,",
          "",
          "[Removed Lines]",
          "3018:   since = \"3.3.0\")",
          "",
          "[Added Lines]",
          "3018:   since = \"3.4.0\")",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/literals.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "80:     case d: Decimal => Literal(d, DecimalType(Math.max(d.precision, d.scale), d.scale))",
          "81:     case i: Instant => Literal(instantToMicros(i), TimestampType)",
          "82:     case t: Timestamp => Literal(DateTimeUtils.fromJavaTimestamp(t), TimestampType)",
          "84:     case ld: LocalDate => Literal(ld.toEpochDay.toInt, DateType)",
          "85:     case d: Date => Literal(DateTimeUtils.fromJavaDate(d), DateType)",
          "86:     case d: Duration => Literal(durationToMicros(d), DayTimeIntervalType())",
          "",
          "[Removed Lines]",
          "83:     case l: LocalDateTime => Literal(DateTimeUtils.localDateTimeToMicros(l), TimestampNTZType)",
          "",
          "[Added Lines]",
          "84:     case l: LocalDateTime if Utils.isTesting =>",
          "85:       Literal(DateTimeUtils.localDateTimeToMicros(l), TimestampNTZType)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "120:     case _ if clz == classOf[Date] => DateType",
          "121:     case _ if clz == classOf[Instant] => TimestampType",
          "122:     case _ if clz == classOf[Timestamp] => TimestampType",
          "124:     case _ if clz == classOf[Duration] => DayTimeIntervalType()",
          "125:     case _ if clz == classOf[Period] => YearMonthIntervalType()",
          "126:     case _ if clz == classOf[JavaBigDecimal] => DecimalType.SYSTEM_DEFAULT",
          "",
          "[Removed Lines]",
          "123:     case _ if clz == classOf[LocalDateTime] => TimestampNTZType",
          "",
          "[Added Lines]",
          "126:     case _ if clz == classOf[LocalDateTime] && Utils.isTesting => TimestampNTZType",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "186:     case dt: DecimalType => Literal(Decimal(0, dt.precision, dt.scale))",
          "187:     case DateType => create(0, DateType)",
          "188:     case TimestampType => create(0L, TimestampType)",
          "190:     case it: DayTimeIntervalType => create(0L, it)",
          "191:     case it: YearMonthIntervalType => create(0, it)",
          "192:     case StringType => Literal(\"\")",
          "",
          "[Removed Lines]",
          "189:     case TimestampNTZType => create(0L, TimestampNTZType)",
          "",
          "[Added Lines]",
          "193:     case TimestampNTZType if Utils.isTesting => create(0L, TimestampNTZType)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "208:       case ByteType => v.isInstanceOf[Byte]",
          "209:       case ShortType => v.isInstanceOf[Short]",
          "210:       case IntegerType | DateType | _: YearMonthIntervalType => v.isInstanceOf[Int]",
          "212:         v.isInstanceOf[Long]",
          "213:       case FloatType => v.isInstanceOf[Float]",
          "214:       case DoubleType => v.isInstanceOf[Double]",
          "",
          "[Removed Lines]",
          "211:       case LongType | TimestampType | TimestampNTZType | _: DayTimeIntervalType =>",
          "",
          "[Added Lines]",
          "216:       case TimestampNTZType if Utils.isTesting => v.isInstanceOf[Long]",
          "217:       case LongType | TimestampType | _: DayTimeIntervalType =>",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "48: import org.apache.spark.sql.internal.SQLConf",
          "49: import org.apache.spark.sql.types._",
          "50: import org.apache.spark.unsafe.types.{CalendarInterval, UTF8String}",
          "51: import org.apache.spark.util.random.RandomSampler",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "51: import org.apache.spark.util.Utils.isTesting",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2203:           val zoneId = getZoneId(conf.sessionLocalTimeZone)",
          "2204:           val specialDate = convertSpecialDate(value, zoneId).map(Literal(_, DateType))",
          "2205:           specialDate.getOrElse(toLiteral(stringToDate, DateType))",
          "2207:           convertSpecialTimestampNTZ(value, getZoneId(conf.sessionLocalTimeZone))",
          "2208:             .map(Literal(_, TimestampNTZType))",
          "2209:             .getOrElse(toLiteral(stringToTimestampWithoutTimeZone, TimestampNTZType))",
          "2211:           constructTimestampLTZLiteral(value)",
          "2212:         case \"TIMESTAMP\" =>",
          "2213:           SQLConf.get.timestampType match {",
          "",
          "[Removed Lines]",
          "2206:         case \"TIMESTAMP_NTZ\" =>",
          "2210:         case \"TIMESTAMP_LTZ\" =>",
          "",
          "[Added Lines]",
          "2207:         case \"TIMESTAMP_NTZ\" if isTesting =>",
          "2211:         case \"TIMESTAMP_LTZ\" if isTesting =>",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "2658:       case (\"double\", Nil) => DoubleType",
          "2659:       case (\"date\", Nil) => DateType",
          "2660:       case (\"timestamp\", Nil) => SQLConf.get.timestampType",
          "2663:       case (\"string\", Nil) => StringType",
          "2664:       case (\"character\" | \"char\", length :: Nil) => CharType(length.getText.toInt)",
          "2665:       case (\"varchar\", length :: Nil) => VarcharType(length.getText.toInt)",
          "",
          "[Removed Lines]",
          "2661:       case (\"timestamp_ntz\", Nil) => TimestampNTZType",
          "2662:       case (\"timestamp_ltz\", Nil) => TimestampType",
          "",
          "[Added Lines]",
          "2663:       case (\"timestamp_ntz\", Nil) if isTesting => TimestampNTZType",
          "2664:       case (\"timestamp_ltz\", Nil) if isTesting => TimestampType",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "3215:         s\"and type literal. Setting the configuration as ${TimestampTypes.TIMESTAMP_NTZ} will \" +",
          "3216:         \"use TIMESTAMP WITHOUT TIME ZONE as the default type while putting it as \" +",
          "3217:         s\"${TimestampTypes.TIMESTAMP_LTZ} will use TIMESTAMP WITH LOCAL TIME ZONE. \" +",
          "3219:         \"LOCAL TIME ZONE type.\")",
          "3221:       .stringConf",
          "3222:       .transform(_.toUpperCase(Locale.ROOT))",
          "3223:       .checkValues(TimestampTypes.values.map(_.toString))",
          "",
          "[Removed Lines]",
          "3218:         \"Before the 3.3.0 release, Spark only supports the TIMESTAMP WITH \" +",
          "3220:       .version(\"3.3.0\")",
          "",
          "[Added Lines]",
          "3218:         \"Before the 3.4.0 release, Spark only supports the TIMESTAMP WITH \" +",
          "3220:       .version(\"3.4.0\")",
          "3221:       .internal()",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "4359:   def strictIndexOperator: Boolean = ansiEnabled && getConf(ANSI_STRICT_INDEX_OPERATOR)",
          "4361:   def timestampType: AtomicType = getConf(TIMESTAMP_TYPE) match {",
          "4364:       TimestampType",
          "4368:   }",
          "4370:   def nestedSchemaPruningEnabled: Boolean = getConf(NESTED_SCHEMA_PRUNING_ENABLED)",
          "",
          "[Removed Lines]",
          "4362:     case \"TIMESTAMP_LTZ\" =>",
          "4366:     case \"TIMESTAMP_NTZ\" =>",
          "4367:       TimestampNTZType",
          "",
          "[Added Lines]",
          "4365:     case \"TIMESTAMP_NTZ\" if Utils.isTesting =>",
          "4366:       TimestampNTZType",
          "4368:     case _ =>",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/types/TimestampNTZType.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/types/TimestampNTZType.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/types/TimestampNTZType.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/types/TimestampNTZType.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import scala.math.Ordering",
          "21: import scala.reflect.runtime.universe.typeTag",
          "",
          "[Removed Lines]",
          "23: import org.apache.spark.annotation.Unstable",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "34: @Unstable",
          "35: class TimestampNTZType private() extends DatetimeType {",
          "",
          "[Added Lines]",
          "31: private[spark] class TimestampNTZType private() extends DatetimeType {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "64: @Unstable",
          "65: case object TimestampNTZType extends TimestampNTZType",
          "",
          "[Added Lines]",
          "58: private[spark] case object TimestampNTZType extends TimestampNTZType",
          "",
          "---------------"
        ],
        "sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetVectorUpdaterFactory.java||sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetVectorUpdaterFactory.java": [
          "File: sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetVectorUpdaterFactory.java -> sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/ParquetVectorUpdaterFactory.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "140:         }",
          "141:         break;",
          "142:       case INT96:",
          "144:           convertErrorForTimestampNTZ(typeName.name());",
          "145:         } else if (sparkType == DataTypes.TimestampType) {",
          "146:           final boolean failIfRebase = \"EXCEPTION\".equals(int96RebaseMode);",
          "",
          "[Removed Lines]",
          "143:         if (sparkType == DataTypes.TimestampNTZType) {",
          "",
          "[Added Lines]",
          "143:         if (sparkType == TimestampNTZType$.MODULE$) {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "199:     if (((TimestampLogicalTypeAnnotation) logicalTypeAnnotation).isAdjustedToUTC() &&",
          "201:       convertErrorForTimestampNTZ(\"int64 time(\" + logicalTypeAnnotation + \")\");",
          "202:     }",
          "203:   }",
          "205:   void convertErrorForTimestampNTZ(String parquetType) {",
          "206:     throw new RuntimeException(\"Unable to create Parquet converter for data type \" +",
          "208:   }",
          "210:   boolean isUnsignedIntTypeMatched(int bitWidth) {",
          "",
          "[Removed Lines]",
          "200:       sparkType == DataTypes.TimestampNTZType) {",
          "207:       DataTypes.TimestampNTZType.json() + \" whose Parquet type is \" + parquetType);",
          "",
          "[Added Lines]",
          "200:       sparkType == TimestampNTZType$.MODULE$) {",
          "207:       TimestampNTZType$.MODULE$.json() + \" whose Parquet type is \" + parquetType);",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala||sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala -> sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "83:   implicit def newLocalDateEncoder: Encoder[java.time.LocalDate] = Encoders.LOCALDATE",
          "89:   implicit def newTimeStampEncoder: Encoder[java.sql.Timestamp] = Encoders.TIMESTAMP",
          "",
          "[Removed Lines]",
          "86:   implicit def newLocalDateTimeEncoder: Encoder[java.time.LocalDateTime] = Encoders.LOCALDATETIME",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/core/src/test/java/test/org/apache/spark/sql/JavaDatasetSuite.java||sql/core/src/test/java/test/org/apache/spark/sql/JavaDatasetSuite.java": [
          "File: sql/core/src/test/java/test/org/apache/spark/sql/JavaDatasetSuite.java -> sql/core/src/test/java/test/org/apache/spark/sql/JavaDatasetSuite.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "607:     Assert.assertEquals(data, ds.collectAsList());",
          "608:   }",
          "618:   @Test",
          "619:   public void testDurationEncoder() {",
          "620:     Encoder<Duration> encoder = Encoders.DURATION();",
          "",
          "[Removed Lines]",
          "610:   @Test",
          "611:   public void testLocalDateTimeEncoder() {",
          "612:     Encoder<LocalDateTime> encoder = Encoders.LOCALDATETIME();",
          "613:     List<LocalDateTime> data = Arrays.asList(LocalDateTime.of(1, 1, 1, 1, 1));",
          "614:     Dataset<LocalDateTime> ds = spark.createDataset(data, encoder);",
          "615:     Assert.assertEquals(data, ds.collectAsList());",
          "616:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/CsvFunctionsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/CsvFunctionsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/CsvFunctionsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/CsvFunctionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql",
          "20: import java.text.SimpleDateFormat",
          "22: import java.util.Locale",
          "24: import scala.collection.JavaConverters._",
          "",
          "[Removed Lines]",
          "21: import java.time.{Duration, LocalDateTime, Period}",
          "",
          "[Added Lines]",
          "21: import java.time.{Duration, Period}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "354:     }",
          "355:   }",
          "372:   test(\"SPARK-37326: Handle incorrectly formatted timestamp_ntz values in from_csv\") {",
          "373:     val fromCsvDF = Seq(\"2021-08-12T15:16:23.000+11:00\").toDF(\"csv\")",
          "374:       .select(",
          "",
          "[Removed Lines]",
          "357:   test(\"SPARK-36490: Make from_csv/to_csv to handle timestamp_ntz type properly\") {",
          "358:     val localDT = LocalDateTime.parse(\"2021-08-12T15:16:23\")",
          "359:     val df = Seq(localDT).toDF",
          "360:     val toCsvDF = df.select(to_csv(struct($\"value\")) as \"csv\")",
          "361:     checkAnswer(toCsvDF, Row(\"2021-08-12T15:16:23.000\"))",
          "362:     val fromCsvDF = toCsvDF",
          "363:       .select(",
          "364:         from_csv(",
          "365:           $\"csv\",",
          "366:           StructType(StructField(\"a\", TimestampNTZType) :: Nil),",
          "367:           Map.empty[String, String]) as \"value\")",
          "368:       .selectExpr(\"value.a\")",
          "369:     checkAnswer(fromCsvDF, Row(localDT))",
          "370:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql",
          "22: import scala.util.Random",
          "",
          "[Removed Lines]",
          "20: import java.time.{Duration, LocalDateTime, Period}",
          "",
          "[Added Lines]",
          "20: import java.time.{Duration, Period}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1434:     checkAnswer(df2, Row(Period.ofYears(1), 1))",
          "1435:   }",
          "1448:   test(\"SPARK-36926: decimal average mistakenly overflow\") {",
          "1449:     val df = (1 to 10).map(_ => \"9999999999.99\").toDF(\"d\")",
          "1450:     val res = df.select($\"d\".cast(\"decimal(12, 2)\").as(\"d\")).agg(avg($\"d\").cast(\"string\"))",
          "",
          "[Removed Lines]",
          "1437:   test(\"SPARK-36054: Support group by TimestampNTZ column\") {",
          "1438:     val ts1 = \"2021-01-01T00:00:00\"",
          "1439:     val ts2 = \"2021-01-01T00:00:01\"",
          "1440:     val localDateTime = Seq(ts1, ts1, ts2).map(LocalDateTime.parse)",
          "1441:     val df = localDateTime.toDF(\"ts\").groupBy(\"ts\").count().orderBy(\"ts\")",
          "1442:     val expectedSchema =",
          "1443:       new StructType().add(StructField(\"ts\", TimestampNTZType)).add(\"count\", LongType, false)",
          "1444:     assert (df.schema == expectedSchema)",
          "1445:     checkAnswer(df, Seq(Row(LocalDateTime.parse(ts1), 2), Row(LocalDateTime.parse(ts2), 1)))",
          "1446:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2106:     checkAnswer(withUDF, Row(Row(1), null, null) :: Row(Row(1), null, null) :: Nil)",
          "2107:   }",
          "2114:   test(\"SPARK-34605: implicit encoder for java.time.Duration\") {",
          "2115:     val duration = java.time.Duration.ofMinutes(10)",
          "2116:     assert(spark.range(1).map { _ => duration }.head === duration)",
          "",
          "[Removed Lines]",
          "2109:   test(\"SPARK-35664: implicit encoder for java.time.LocalDateTime\") {",
          "2110:     val localDateTime = java.time.LocalDateTime.parse(\"2021-06-08T12:31:58.999999\")",
          "2111:     assert(Seq(localDateTime).toDS().head() === localDateTime)",
          "2112:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/JsonFunctionsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/JsonFunctionsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/JsonFunctionsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/JsonFunctionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql",
          "20: import java.text.SimpleDateFormat",
          "22: import java.util.Locale",
          "24: import collection.JavaConverters._",
          "",
          "[Removed Lines]",
          "21: import java.time.{Duration, LocalDateTime, Period}",
          "",
          "[Added Lines]",
          "21: import java.time.{Duration, Period}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "918:       }",
          "919:     }",
          "920:   }",
          "933: }",
          "",
          "[Removed Lines]",
          "922:   test(\"SPARK-36491: Make from_json/to_json to handle timestamp_ntz type properly\") {",
          "923:     val localDT = LocalDateTime.parse(\"2021-08-12T15:16:23\")",
          "924:     val df = Seq(localDT).toDF",
          "925:     val toJsonDF = df.select(to_json(map(lit(\"key\"), $\"value\")) as \"json\")",
          "926:     checkAnswer(toJsonDF, Row(\"\"\"{\"key\":\"2021-08-12T15:16:23.000\"}\"\"\"))",
          "927:     val fromJsonDF = toJsonDF",
          "928:       .select(",
          "929:         from_json($\"json\", StructType(StructField(\"key\", TimestampNTZType) :: Nil)) as \"value\")",
          "930:       .selectExpr(\"value['key']\")",
          "931:     checkAnswer(fromJsonDF, Row(localDT))",
          "932:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/UDFSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/UDFSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/UDFSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/UDFSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "848:     }",
          "849:   }",
          "879:   test(\"SPARK-34663, SPARK-35730: using java.time.Duration in UDF\") {",
          "881:     val input = Seq(java.time.Duration.ofHours(23)).toDF(\"d\")",
          "",
          "[Removed Lines]",
          "851:   test(\"SPARK-35674: using java.time.LocalDateTime in UDF\") {",
          "853:     val input = Seq(java.time.LocalDateTime.parse(\"2021-01-01T00:00:00\")).toDF(\"dateTime\")",
          "854:     val plusYear = udf((l: java.time.LocalDateTime) => l.plusYears(1))",
          "855:     val result = input.select(plusYear($\"dateTime\").as(\"newDateTime\"))",
          "856:     checkAnswer(result, Row(java.time.LocalDateTime.parse(\"2022-01-01T00:00:00\")) :: Nil)",
          "857:     assert(result.schema === new StructType().add(\"newDateTime\", TimestampNTZType))",
          "859:     val nullFunc = udf((_: java.time.LocalDateTime) => null.asInstanceOf[java.time.LocalDateTime])",
          "860:     val nullResult = input.select(nullFunc($\"dateTime\").as(\"nullDateTime\"))",
          "861:     checkAnswer(nullResult, Row(null) :: Nil)",
          "862:     assert(nullResult.schema === new StructType().add(\"nullDateTime\", TimestampNTZType))",
          "864:     val nullInput = Seq(null.asInstanceOf[java.time.LocalDateTime]).toDF(\"nullDateTime\")",
          "865:     val constDuration = udf((_: java.time.LocalDateTime) =>",
          "866:       java.time.LocalDateTime.parse(\"2021-01-01T00:00:00\"))",
          "867:     val constResult = nullInput.select(constDuration($\"nullDateTime\").as(\"firstDayOf2021\"))",
          "868:     checkAnswer(constResult, Row(java.time.LocalDateTime.parse(\"2021-01-01T00:00:00\")) :: Nil)",
          "869:     assert(constResult.schema === new StructType().add(\"firstDayOf2021\", TimestampNTZType))",
          "872:     val overflowFunc = udf((l: java.time.LocalDateTime) => l.plusDays(Long.MaxValue))",
          "873:     val e = intercept[SparkException] {",
          "874:       input.select(overflowFunc($\"dateTime\")).collect()",
          "875:     }.getCause.getCause",
          "876:     assert(e.isInstanceOf[java.lang.ArithmeticException])",
          "877:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "84addc5d1d8359a5b716ec869489fc961af23cf2",
      "candidate_info": {
        "commit_hash": "84addc5d1d8359a5b716ec869489fc961af23cf2",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/84addc5d1d8359a5b716ec869489fc961af23cf2",
        "files": [
          "python/pyspark/sql/pandas/conversion.py"
        ],
        "message": "[SPARK-39051][PYTHON] Minor refactoring of `python/pyspark/sql/pandas/conversion.py`\n\nMinor refactoring of `python/pyspark/sql/pandas/conversion.py`, which includes:\n- doc change\n- renaming\n\nTo improve code readability and maintainability.\n\nNo.\n\nExisting tests.\n\nCloses #36384 from xinrong-databricks/conversion.py.\n\nAuthored-by: Xinrong Meng <xinrong.meng@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit c19fadabde3ef3f9c7e4fa9bf74632a4f8e1f3e2)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/sql/pandas/conversion.py||python/pyspark/sql/pandas/conversion.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/sql/pandas/conversion.py||python/pyspark/sql/pandas/conversion.py": [
          "File: python/pyspark/sql/pandas/conversion.py -> python/pyspark/sql/pandas/conversion.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "52: class PandasConversionMixin:",
          "53:     \"\"\"",
          "55:     can use this class.",
          "56:     \"\"\"",
          "",
          "[Removed Lines]",
          "54:     Min-in for the conversion from Spark to pandas. Currently, only :class:`DataFrame`",
          "",
          "[Added Lines]",
          "54:     Mix-in for the conversion from Spark to pandas. Currently, only :class:`DataFrame`",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "66:         Notes",
          "67:         -----",
          "69:         expected to be small, as all the data is loaded into the driver's memory.",
          "73:         Examples",
          "74:         --------",
          "",
          "[Removed Lines]",
          "68:         This method should only be used if the resulting Pandas's :class:`DataFrame` is",
          "71:         Usage with spark.sql.execution.arrow.pyspark.enabled=True is experimental.",
          "",
          "[Added Lines]",
          "68:         This method should only be used if the resulting Pandas ``pandas.DataFrame`` is",
          "71:         Usage with ``spark.sql.execution.arrow.pyspark.enabled=True`` is experimental.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "137:                     # Rename columns to avoid duplicated column names.",
          "138:                     tmp_column_names = [\"col_{}\".format(i) for i in range(len(self.columns))]",
          "141:                     batches = self.toDF(*tmp_column_names)._collect_as_arrow(",
          "142:                         split_batches=self_destruct",
          "143:                     )",
          "",
          "[Removed Lines]",
          "139:                     c = self.sparkSession._jconf",
          "140:                     self_destruct = c.arrowPySparkSelfDestructEnabled()",
          "",
          "[Added Lines]",
          "139:                     self_destruct = jconf.arrowPySparkSelfDestructEnabled()",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "176:                     else:",
          "177:                         corrected_panda_types = {}",
          "178:                         for index, field in enumerate(self.schema):",
          "180:                                 field.dataType",
          "181:                             )",
          "182:                             corrected_panda_types[tmp_column_names[index]] = (",
          "184:                             )",
          "186:                         pdf = pd.DataFrame(columns=tmp_column_names).astype(",
          "",
          "[Removed Lines]",
          "179:                             panda_type = PandasConversionMixin._to_corrected_pandas_type(",
          "183:                                 np.object0 if panda_type is None else panda_type",
          "",
          "[Added Lines]",
          "178:                             pandas_type = PandasConversionMixin._to_corrected_pandas_type(",
          "182:                                 np.object0 if pandas_type is None else pandas_type",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "206:         pdf = pd.DataFrame.from_records(self.collect(), columns=self.columns)",
          "207:         column_counter = Counter(self.columns)",
          "212:             if column_counter[field.name] > 1:",
          "214:             else:",
          "215:                 pandas_col = pdf[field.name]",
          "217:             pandas_type = PandasConversionMixin._to_corrected_pandas_type(field.dataType)",
          "218:             # SPARK-21766: if an integer field is nullable and has null values, it can be",
          "222:             if pandas_type is not None and not (",
          "223:                 isinstance(field.dataType, IntegralType)",
          "224:                 and field.nullable",
          "225:                 and pandas_col.isnull().any()",
          "226:             ):",
          "229:             if isinstance(field.dataType, IntegralType) and pandas_col.isnull().any():",
          "231:             if isinstance(field.dataType, BooleanType) and pandas_col.isnull().any():",
          "234:         df = pd.DataFrame()",
          "236:             column_name = self.schema[index].name",
          "239:             if column_counter[column_name] > 1:",
          "240:                 series = pdf.iloc[:, index]",
          "241:             else:",
          "",
          "[Removed Lines]",
          "209:         dtype: List[Optional[Type]] = [None] * len(self.schema)",
          "210:         for fieldIdx, field in enumerate(self.schema):",
          "211:             # For duplicate column name, we use `iloc` to access it.",
          "213:                 pandas_col = pdf.iloc[:, fieldIdx]",
          "219:             # inferred by pandas as float column. Once we convert the column with NaN back",
          "220:             # to integer type e.g., np.int16, we will hit exception. So we use the inferred",
          "221:             # float type, not the corrected type from the schema in this case.",
          "227:                 dtype[fieldIdx] = pandas_type",
          "228:             # Ensure we fall back to nullable numpy types, even when whole column is null:",
          "230:                 dtype[fieldIdx] = np.float64",
          "232:                 dtype[fieldIdx] = np.object  # type: ignore[attr-defined]",
          "235:         for index, t in enumerate(dtype):",
          "238:             # For duplicate column name, we use `iloc` to access it.",
          "",
          "[Added Lines]",
          "208:         corrected_dtypes: List[Optional[Type]] = [None] * len(self.schema)",
          "209:         for index, field in enumerate(self.schema):",
          "210:             # We use `iloc` to access columns with duplicate column names.",
          "212:                 pandas_col = pdf.iloc[:, index]",
          "218:             # inferred by pandas as a float column. If we convert the column with NaN back",
          "219:             # to integer type e.g., np.int16, we will hit an exception. So we use the",
          "220:             # pandas-inferred float type, rather than the corrected type from the schema",
          "221:             # in this case.",
          "227:                 corrected_dtypes[index] = pandas_type",
          "228:             # Ensure we fall back to nullable numpy types.",
          "230:                 corrected_dtypes[index] = np.float64",
          "232:                 corrected_dtypes[index] = np.object  # type: ignore[attr-defined]",
          "235:         for index, t in enumerate(corrected_dtypes):",
          "238:             # We use `iloc` to access columns with duplicate column names.",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "255:             else:",
          "256:                 df[column_name] = series",
          "260:         if timezone is None:",
          "262:         else:",
          "263:             from pyspark.sql.pandas.types import _check_series_convert_timestamps_local_tz",
          "265:             for field in self.schema:",
          "266:                 # TODO: handle nested timestamps, such as ArrayType(TimestampType())?",
          "267:                 if isinstance(field.dataType, TimestampType):",
          "270:                     )",
          "273:     @staticmethod",
          "274:     def _to_corrected_pandas_type(dt: DataType) -> Optional[Type]:",
          "275:         \"\"\"",
          "277:         may be wrong. This method gets the corrected data type for Pandas if that type may be",
          "278:         inferred incorrectly.",
          "279:         \"\"\"",
          "",
          "[Removed Lines]",
          "258:         pdf = df",
          "261:             return pdf",
          "268:                     pdf[field.name] = _check_series_convert_timestamps_local_tz(",
          "269:                         pdf[field.name], timezone",
          "271:             return pdf",
          "276:         When converting Spark SQL records to Pandas :class:`DataFrame`, the inferred data type",
          "",
          "[Added Lines]",
          "259:             return df",
          "266:                     df[field.name] = _check_series_convert_timestamps_local_tz(",
          "267:                         df[field.name], timezone",
          "269:             return df",
          "274:         When converting Spark SQL records to Pandas `pandas.DataFrame`, the inferred data type",
          "",
          "---------------"
        ]
      }
    }
  ]
}