{
  "cve_id": "CVE-2022-40604",
  "cve_desc": "In Apache Airflow 2.3.0 through 2.3.4, part of a url was unnecessarily formatted, allowing for possible information extraction.",
  "repo": "apache/airflow",
  "patch_hash": "6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
  "patch_info": {
    "commit_hash": "6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
    "files": [
      "airflow/utils/log/file_task_handler.py"
    ],
    "message": "Properly build URL to retrieve logs independently from system (#26337)\n\nThe previous way of building the path depended on the OS path\nbut it was really used to build the URL so we should use\nurllib instead of os.path.join.\n\n(cherry picked from commit 18386026c28939fa6d91d198c5489c295a05dcd2)",
    "before_after_code_files": [
      "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
    ]
  },
  "patch_diff": {
    "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
      "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "21: import warnings",
      "22: from pathlib import Path",
      "23: from typing import TYPE_CHECKING, Optional",
      "25: from airflow.configuration import AirflowConfigException, conf",
      "26: from airflow.exceptions import RemovedInAirflow3Warning",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "24: from urllib.parse import urljoin",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "194:         else:",
      "195:             import httpx",
      "199:             )",
      "200:             log += f\"*** Log file does not exist: {location}\\n\"",
      "201:             log += f\"*** Fetching from: {url}\\n\"",
      "",
      "[Removed Lines]",
      "197:             url = os.path.join(\"http://{ti.hostname}:{worker_log_server_port}/log\", log_relative_path).format(",
      "198:                 ti=ti, worker_log_server_port=conf.get('logging', 'WORKER_LOG_SERVER_PORT')",
      "",
      "[Added Lines]",
      "198:             url = urljoin(",
      "199:                 f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log\", log_relative_path",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "62322ef5189bad4c6a4e58880a967b4a714df4d1",
      "candidate_info": {
        "commit_hash": "62322ef5189bad4c6a4e58880a967b4a714df4d1",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/62322ef5189bad4c6a4e58880a967b4a714df4d1",
        "files": [
          "setup.cfg"
        ],
        "message": "Add min attrs version (#26408)\n\nI'm not sure when we started using attrs directly, but we do and we need\n>=22.1.0 as we use the min_length validator.\n\n(cherry picked from commit fdecf12051308a4e064f5e4bf5464ffc9b183dad)",
        "before_after_code_files": [
          "setup.cfg||setup.cfg"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.cfg||setup.cfg": [
          "File: setup.cfg -> setup.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "85:     # together with SQLAlchemy. Our experience with Alembic is that it very stable in minor version",
          "86:     alembic>=1.5.1, <2.0",
          "87:     argcomplete>=1.10",
          "88:     blinker",
          "89:     cached_property>=1.5.0;python_version<=\"3.7\"",
          "90:     cattrs>=22.1.0",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "88:     attrs>=22.1.0",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "95eda4bc0ef6a4e003f1b5a684d090a4b4255738",
      "candidate_info": {
        "commit_hash": "95eda4bc0ef6a4e003f1b5a684d090a4b4255738",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/95eda4bc0ef6a4e003f1b5a684d090a4b4255738",
        "files": [
          "README.md",
          "docs/apache-airflow/installation/supported-versions.rst",
          "docs/docker-stack/README.md",
          "docs/docker-stack/docker-examples/extending/add-apt-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-build-essential-extend/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-providers/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-pypi-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/custom-providers/Dockerfile",
          "docs/docker-stack/docker-examples/extending/embedding-dags/Dockerfile",
          "docs/docker-stack/docker-examples/extending/writable-directory/Dockerfile",
          "docs/docker-stack/entrypoint.rst",
          "scripts/ci/pre_commit/pre_commit_supported_versions.py",
          "setup.py"
        ],
        "message": "Update Airflow version",
        "before_after_code_files": [
          "scripts/ci/pre_commit/pre_commit_supported_versions.py||scripts/ci/pre_commit/pre_commit_supported_versions.py",
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/pre_commit/pre_commit_supported_versions.py||scripts/ci/pre_commit/pre_commit_supported_versions.py": [
          "File: scripts/ci/pre_commit/pre_commit_supported_versions.py -> scripts/ci/pre_commit/pre_commit_supported_versions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: HEADERS = (\"Version\", \"Current Patch/Minor\", \"State\", \"First Release\", \"Limited Support\", \"EOL/Terminated\")",
          "27: SUPPORTED_VERSIONS = (",
          "29:     (\"1.10\", \"1.10.15\", \"EOL\", \"Aug 27, 2018\", \"Dec 17, 2020\", \"June 17, 2021\"),",
          "30:     (\"1.9\", \"1.9.0\", \"EOL\", \"Jan 03, 2018\", \"Aug 27, 2018\", \"Aug 27, 2018\"),",
          "31:     (\"1.8\", \"1.8.2\", \"EOL\", \"Mar 19, 2017\", \"Jan 03, 2018\", \"Jan 03, 2018\"),",
          "",
          "[Removed Lines]",
          "28:     (\"2\", \"2.3.4\", \"Supported\", \"Dec 17, 2020\", \"TBD\", \"TBD\"),",
          "",
          "[Added Lines]",
          "28:     (\"2\", \"2.4.0\", \"Supported\", \"Dec 17, 2020\", \"TBD\", \"TBD\"),",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "48: logger = logging.getLogger(__name__)",
          "52: AIRFLOW_SOURCES_ROOT = Path(__file__).parent.resolve()",
          "53: PROVIDERS_ROOT = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"providers\"",
          "",
          "[Removed Lines]",
          "50: version = '2.4.0b1'",
          "",
          "[Added Lines]",
          "50: version = '2.4.0'",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "92eaa09d1ff316e4715fb8ca0fffee467d8c8f65",
      "candidate_info": {
        "commit_hash": "92eaa09d1ff316e4715fb8ca0fffee467d8c8f65",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/92eaa09d1ff316e4715fb8ca0fffee467d8c8f65",
        "files": [
          "airflow/utils/log/file_task_handler.py"
        ],
        "message": "Fix proper joining of the path for logs retrieved from celery workers (#26493)\n\nThe change #26377 \"fixed\" the way how logs were retrieved from\nCelery, but it - unfortunately broke the retrieval eventually.\n\nThis PR should fix it.\n\nFixes: #26492\n(cherry picked from commit 52560b87c991c9739791ca8419219b0d86debacd)",
        "before_after_code_files": [
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ],
          "candidate": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "198:             import httpx",
          "200:             url = urljoin(",
          "202:             )",
          "203:             log += f\"*** Log file does not exist: {location}\\n\"",
          "204:             log += f\"*** Fetching from: {url}\\n\"",
          "",
          "[Removed Lines]",
          "201:                 f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log\", log_relative_path",
          "",
          "[Added Lines]",
          "201:                 f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log/\",",
          "202:                 log_relative_path,",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "329f706d9e59d690bd6ff6a3c8c035172668a217",
      "candidate_info": {
        "commit_hash": "329f706d9e59d690bd6ff6a3c8c035172668a217",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/329f706d9e59d690bd6ff6a3c8c035172668a217",
        "files": [
          "airflow/utils/log/file_task_handler.py",
          "tests/utils/test_log_handlers.py"
        ],
        "message": "Add unit test for log retrieval url (#26603)\n\n* Add unit test for log retrieval url\n\nAdded unit test to #26493\n\n(cherry picked from commit 061caff2862d9df078336dc94efa5a6915935b7e)",
        "before_after_code_files": [
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py",
          "tests/utils/test_log_handlers.py||tests/utils/test_log_handlers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ],
          "candidate": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "197:         else:",
          "198:             import httpx",
          "204:             log += f\"*** Log file does not exist: {location}\\n\"",
          "205:             log += f\"*** Fetching from: {url}\\n\"",
          "206:             try:",
          "",
          "[Removed Lines]",
          "200:             url = urljoin(",
          "201:                 f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log/\",",
          "202:                 log_relative_path,",
          "203:             )",
          "",
          "[Added Lines]",
          "200:             url = self._get_log_retrieval_url(ti, log_relative_path)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "245:         return log, {'end_of_log': True}",
          "247:     def read(self, task_instance, try_number=None, metadata=None):",
          "248:         \"\"\"",
          "249:         Read logs of given task instance from local machine.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "244:     @staticmethod",
          "245:     def _get_log_retrieval_url(ti: TaskInstance, log_relative_path: str) -> str:",
          "246:         url = urljoin(",
          "247:             f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log/\",",
          "248:             log_relative_path,",
          "249:         )",
          "250:         return url",
          "",
          "---------------"
        ],
        "tests/utils/test_log_handlers.py||tests/utils/test_log_handlers.py": [
          "File: tests/utils/test_log_handlers.py -> tests/utils/test_log_handlers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "251:         fth = FileTaskHandler(\"\")",
          "252:         rendered_filename = fth._render_filename(filename_rendering_ti, 42)",
          "253:         assert expected_filename == rendered_filename",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "256: class TestLogUrl:",
          "257:     def test_log_retrieval_valid(self, create_task_instance):",
          "258:         log_url_ti = create_task_instance(",
          "259:             dag_id=\"dag_for_testing_filename_rendering\",",
          "260:             task_id=\"task_for_testing_filename_rendering\",",
          "261:             run_type=DagRunType.SCHEDULED,",
          "262:             execution_date=DEFAULT_DATE,",
          "263:         )",
          "264:         log_url_ti.hostname = 'hostname'",
          "265:         url = FileTaskHandler._get_log_retrieval_url(log_url_ti, 'DYNAMIC_PATH')",
          "266:         assert url == \"http://hostname:8793/log/DYNAMIC_PATH\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1a8a897120762692ca98ac5ce4da881678c073aa",
      "candidate_info": {
        "commit_hash": "1a8a897120762692ca98ac5ce4da881678c073aa",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/1a8a897120762692ca98ac5ce4da881678c073aa",
        "files": [
          "airflow/exceptions.py",
          "airflow/executors/executor_loader.py",
          "airflow/models/param.py",
          "airflow/models/taskinstance.py",
          "airflow/serialization/serialized_objects.py",
          "airflow/utils/cli.py",
          "airflow/utils/helpers.py",
          "airflow/utils/log/file_task_handler.py"
        ],
        "message": "Improve speed to run `airflow` by 6x (#21438)\n\nBy delaying expensive/slow imports to where they are needed, this gets\n`airflow` printing it's usage information in under 0.8s, down from almost\n3s which makes it feel much much snappier.\n\nBy not loading BaseExecutor we can get down to <0.5s",
        "before_after_code_files": [
          "airflow/exceptions.py||airflow/exceptions.py",
          "airflow/executors/executor_loader.py||airflow/executors/executor_loader.py",
          "airflow/models/param.py||airflow/models/param.py",
          "airflow/models/taskinstance.py||airflow/models/taskinstance.py",
          "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py",
          "airflow/utils/cli.py||airflow/utils/cli.py",
          "airflow/utils/helpers.py||airflow/utils/helpers.py",
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ],
          "candidate": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/exceptions.py||airflow/exceptions.py": [
          "File: airflow/exceptions.py -> airflow/exceptions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import warnings",
          "24: from typing import Any, Dict, List, NamedTuple, Optional, Sized",
          "31: class AirflowException(Exception):",
          "32:     \"\"\"",
          "",
          "[Removed Lines]",
          "26: from airflow.api_connexion.exceptions import NotFound as ApiConnexionNotFound",
          "27: from airflow.utils.code_utils import prepare_code_snippet",
          "28: from airflow.utils.platform import is_tty",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "44:     status_code = 400",
          "48:     \"\"\"Raise when the requested object/resource is not available in the system.\"\"\"",
          "50:     status_code = 404",
          "",
          "[Removed Lines]",
          "47: class AirflowNotFoundException(AirflowException, ApiConnexionNotFound):",
          "",
          "[Added Lines]",
          "43: class AirflowNotFoundException(AirflowException):",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "249:         self.parse_errors = parse_errors",
          "251:     def __str__(self):",
          "252:         result = f\"{self.msg}\\nFilename: {self.file_path}\\n\\n\"",
          "254:         for error_no, parse_error in enumerate(self.parse_errors, 1):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "248:         from airflow.utils.code_utils import prepare_code_snippet",
          "249:         from airflow.utils.platform import is_tty",
          "",
          "---------------"
        ],
        "airflow/executors/executor_loader.py||airflow/executors/executor_loader.py": [
          "File: airflow/executors/executor_loader.py -> airflow/executors/executor_loader.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: import logging",
          "19: from contextlib import suppress",
          "20: from enum import Enum, unique",
          "23: from airflow.exceptions import AirflowConfigException",
          "25: from airflow.executors.executor_constants import (",
          "26:     CELERY_EXECUTOR,",
          "27:     CELERY_KUBERNETES_EXECUTOR,",
          "",
          "[Removed Lines]",
          "21: from typing import Optional, Tuple, Type",
          "24: from airflow.executors.base_executor import BaseExecutor",
          "",
          "[Added Lines]",
          "21: from typing import TYPE_CHECKING, Optional, Tuple, Type",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "36: log = logging.getLogger(__name__)",
          "39: @unique",
          "40: class ConnectorSource(Enum):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "37: if TYPE_CHECKING:",
          "38:     from airflow.executors.base_executor import BaseExecutor",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "48: class ExecutorLoader:",
          "49:     \"\"\"Keeps constants for all the currently available executors.\"\"\"",
          "52:     executors = {",
          "53:         LOCAL_EXECUTOR: 'airflow.executors.local_executor.LocalExecutor',",
          "54:         SEQUENTIAL_EXECUTOR: 'airflow.executors.sequential_executor.SequentialExecutor',",
          "",
          "[Removed Lines]",
          "51:     _default_executor: Optional[BaseExecutor] = None",
          "",
          "[Added Lines]",
          "53:     _default_executor: Optional[\"BaseExecutor\"] = None",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "60:     }",
          "62:     @classmethod",
          "64:         \"\"\"Creates a new instance of the configured executor if none exists and returns it\"\"\"",
          "65:         if cls._default_executor is not None:",
          "66:             return cls._default_executor",
          "",
          "[Removed Lines]",
          "63:     def get_default_executor(cls) -> BaseExecutor:",
          "",
          "[Added Lines]",
          "65:     def get_default_executor(cls) -> \"BaseExecutor\":",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "74:         return cls._default_executor",
          "76:     @classmethod",
          "78:         \"\"\"",
          "79:         Loads the executor.",
          "",
          "[Removed Lines]",
          "77:     def load_executor(cls, executor_name: str) -> BaseExecutor:",
          "",
          "[Added Lines]",
          "79:     def load_executor(cls, executor_name: str) -> \"BaseExecutor\":",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "101:         return executor_cls()",
          "103:     @classmethod",
          "105:         \"\"\"",
          "106:         Imports the executor class.",
          "",
          "[Removed Lines]",
          "104:     def import_executor_cls(cls, executor_name: str) -> Tuple[Type[BaseExecutor], ConnectorSource]:",
          "",
          "[Added Lines]",
          "106:     def import_executor_cls(cls, executor_name: str) -> Tuple[Type[\"BaseExecutor\"], ConnectorSource]:",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "127:         return import_string(executor_name), ConnectorSource.CUSTOM_PATH",
          "129:     @classmethod",
          "131:         \"\"\":return: an instance of CeleryKubernetesExecutor\"\"\"",
          "132:         celery_executor = import_string(cls.executors[CELERY_EXECUTOR])()",
          "133:         kubernetes_executor = import_string(cls.executors[KUBERNETES_EXECUTOR])()",
          "",
          "[Removed Lines]",
          "130:     def __load_celery_kubernetes_executor(cls) -> BaseExecutor:",
          "",
          "[Added Lines]",
          "132:     def __load_celery_kubernetes_executor(cls) -> \"BaseExecutor\":",
          "",
          "---------------"
        ],
        "airflow/models/param.py||airflow/models/param.py": [
          "File: airflow/models/param.py -> airflow/models/param.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: import warnings",
          "20: from typing import Any, Dict, ItemsView, MutableMapping, Optional, ValuesView",
          "26: from airflow.exceptions import AirflowException, ParamValidationError",
          "27: from airflow.utils.context import Context",
          "28: from airflow.utils.types import NOTSET, ArgNotSet",
          "",
          "[Removed Lines]",
          "22: import jsonschema",
          "23: from jsonschema import FormatChecker",
          "24: from jsonschema.exceptions import ValidationError",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "61:         :param suppress_exception: To raise an exception or not when the validations fails.",
          "62:             If true and validations fails, the return value would be None.",
          "63:         \"\"\"",
          "64:         try:",
          "65:             json.dumps(value)",
          "66:         except Exception:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "60:         import jsonschema",
          "61:         from jsonschema import FormatChecker",
          "62:         from jsonschema.exceptions import ValidationError",
          "",
          "---------------"
        ],
        "airflow/models/taskinstance.py||airflow/models/taskinstance.py": [
          "File: airflow/models/taskinstance.py -> airflow/models/taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "115: from airflow.utils.state import DagRunState, State, TaskInstanceState",
          "116: from airflow.utils.timeout import timeout",
          "126: TR = TaskReschedule",
          "128: _CURRENT_CONTEXT: List[Context] = []",
          "",
          "[Removed Lines]",
          "118: try:",
          "119:     from kubernetes.client.api_client import ApiClient",
          "121:     from airflow.kubernetes.kube_config import KubeConfig",
          "122:     from airflow.kubernetes.pod_generator import PodGenerator",
          "123: except ImportError:",
          "124:     ApiClient = None",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2033:     def render_k8s_pod_yaml(self) -> Optional[dict]:",
          "2034:         \"\"\"Render k8s pod yaml\"\"\"",
          "2035:         from airflow.kubernetes.kubernetes_helper_functions import create_pod_id  # Circular import",
          "2037:         kube_config = KubeConfig()",
          "2038:         pod = PodGenerator.construct_pod(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2027:         from kubernetes.client.api_client import ApiClient",
          "2029:         from airflow.kubernetes.kube_config import KubeConfig",
          "2031:         from airflow.kubernetes.pod_generator import PodGenerator",
          "",
          "---------------"
        ],
        "airflow/serialization/serialized_objects.py||airflow/serialization/serialized_objects.py": [
          "File: airflow/serialization/serialized_objects.py -> airflow/serialization/serialized_objects.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "48: from airflow.utils.module_loading import as_importable_string, import_string",
          "49: from airflow.utils.task_group import MappedTaskGroup, TaskGroup",
          "61: if TYPE_CHECKING:",
          "62:     from airflow.ti_deps.deps.base_ti_dep import BaseTIDep",
          "64: log = logging.getLogger(__name__)",
          "66: _OPERATOR_EXTRA_LINKS: Set[str] = {",
          "",
          "[Removed Lines]",
          "51: try:",
          "52:     # isort: off",
          "53:     from kubernetes.client import models as k8s",
          "54:     from airflow.kubernetes.pod_generator import PodGenerator",
          "56:     # isort: on",
          "57:     HAS_KUBERNETES = True",
          "58: except ImportError:",
          "59:     HAS_KUBERNETES = False",
          "",
          "[Added Lines]",
          "54:     HAS_KUBERNETES: bool",
          "55:     try:",
          "56:         from kubernetes.client import models as k8s",
          "58:         from airflow.kubernetes.pod_generator import PodGenerator",
          "59:     except ImportError:",
          "60:         pass",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "313:             return cls._encode({str(k): cls._serialize(v) for k, v in var.items()}, type_=DAT.DICT)",
          "314:         elif isinstance(var, list):",
          "315:             return [cls._serialize(v) for v in var]",
          "317:             json_pod = PodGenerator.serialize_pod(var)",
          "318:             return cls._encode(json_pod, type_=DAT.POD)",
          "319:         elif isinstance(var, DAG):",
          "",
          "[Removed Lines]",
          "316:         elif HAS_KUBERNETES and isinstance(var, k8s.V1Pod):",
          "",
          "[Added Lines]",
          "314:         elif _has_kubernetes() and isinstance(var, k8s.V1Pod):",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "374:         elif type_ == DAT.DATETIME:",
          "375:             return pendulum.from_timestamp(var)",
          "376:         elif type_ == DAT.POD:",
          "378:                 raise RuntimeError(\"Cannot deserialize POD objects without kubernetes libraries installed!\")",
          "379:             pod = PodGenerator.deserialize_model_dict(var)",
          "380:             return pod",
          "",
          "[Removed Lines]",
          "377:             if not HAS_KUBERNETES:",
          "",
          "[Added Lines]",
          "375:             if not _has_kubernetes():",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1120:     def node_id(self):",
          "1121:         \"\"\"Node ID for graph rendering\"\"\"",
          "1122:         return f\"{self.dependency_type}:{self.source}:{self.target}:{self.dependency_id}\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1123: def _has_kubernetes() -> bool:",
          "1124:     global HAS_KUBERNETES",
          "1125:     if \"HAS_KUBERNETES\" in globals():",
          "1126:         return HAS_KUBERNETES",
          "1128:     # Loading kube modules is expensive, so delay it until the last moment",
          "1130:     try:",
          "1131:         from kubernetes.client import models as k8s",
          "1133:         from airflow.kubernetes.pod_generator import PodGenerator",
          "1135:         globals()['k8s'] = k8s",
          "1136:         globals()['PodGenerator'] = PodGenerator",
          "1138:         # isort: on",
          "1139:         HAS_KUBERNETES = True",
          "1140:     except ImportError:",
          "1141:         HAS_KUBERNETES = False",
          "1142:     return HAS_KUBERNETES",
          "",
          "---------------"
        ],
        "airflow/utils/cli.py||airflow/utils/cli.py": [
          "File: airflow/utils/cli.py -> airflow/utils/cli.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: from airflow import settings",
          "35: from airflow.exceptions import AirflowException",
          "36: from airflow.utils import cli_action_loggers",
          "38: from airflow.utils.log.non_caching_file_handler import NonCachingFileHandler",
          "39: from airflow.utils.platform import getuser, is_terminal_support_colors",
          "40: from airflow.utils.session import provide_session",
          "",
          "[Removed Lines]",
          "37: from airflow.utils.db import check_and_run_migrations, synchronize_log_template",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "93:             try:",
          "94:                 # Check and run migrations if necessary",
          "95:                 if check_db:",
          "96:                     check_and_run_migrations()",
          "97:                     synchronize_log_template()",
          "98:                 return f(*args, **kwargs)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "95:                     from airflow.utils.db import check_and_run_migrations, synchronize_log_template",
          "",
          "---------------"
        ],
        "airflow/utils/helpers.py||airflow/utils/helpers.py": [
          "File: airflow/utils/helpers.py -> airflow/utils/helpers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "37: )",
          "38: from urllib import parse",
          "44: from airflow.configuration import conf",
          "45: from airflow.exceptions import AirflowException",
          "46: from airflow.utils.module_loading import import_string",
          "48: if TYPE_CHECKING:",
          "49:     from airflow.models import TaskInstance",
          "51: KEY_REGEX = re.compile(r'^[\\w.-]+$')",
          "",
          "[Removed Lines]",
          "40: import flask",
          "41: import jinja2",
          "42: import jinja2.nativetypes",
          "",
          "[Added Lines]",
          "45:     import jinja2",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "171:     return [e for i in iterable for e in i]",
          "175:     \"\"\"Parses Jinja template string.\"\"\"",
          "176:     if \"{{\" in template_string:  # jinja mode",
          "177:         return None, jinja2.Template(template_string)",
          "178:     else:",
          "",
          "[Removed Lines]",
          "174: def parse_template_string(template_string: str) -> Tuple[Optional[str], Optional[jinja2.Template]]:",
          "",
          "[Added Lines]",
          "172: def parse_template_string(template_string: str) -> Tuple[Optional[str], Optional[\"jinja2.Template\"]]:",
          "174:     import jinja2",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "255:     For example:",
          "256:     'http://0.0.0.0:8000/base/graph?dag_id=my-task&root=&execution_date=2020-10-27T10%3A59%3A25.615587",
          "257:     \"\"\"",
          "258:     view = conf.get('webserver', 'dag_default_view').lower()",
          "259:     url = flask.url_for(f\"Airflow.{view}\")",
          "260:     return f\"{url}?{parse.urlencode(query)}\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "258:     import flask",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "285:     except Exception:",
          "286:         env.handle_exception()  # Rewrite traceback to point to the template.",
          "287:     if native:",
          "288:         return jinja2.nativetypes.native_concat(nodes)",
          "289:     return \"\".join(nodes)",
          "293:     \"\"\"Shorthand to ``render_template(native=False)`` with better typing support.\"\"\"",
          "294:     return render_template(template, context, native=False)",
          "298:     \"\"\"Shorthand to ``render_template(native=True)`` with better typing support.\"\"\"",
          "299:     return render_template(template, context, native=True)",
          "",
          "[Removed Lines]",
          "292: def render_template_to_string(template: jinja2.Template, context: MutableMapping[str, Any]) -> str:",
          "297: def render_template_as_native(template: jinja2.Template, context: MutableMapping[str, Any]) -> Any:",
          "",
          "[Added Lines]",
          "290:         import jinja2.nativetypes",
          "296: def render_template_to_string(template: \"jinja2.Template\", context: MutableMapping[str, Any]) -> str:",
          "301: def render_template_as_native(template: \"jinja2.Template\", context: MutableMapping[str, Any]) -> Any:",
          "",
          "---------------"
        ],
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: from pathlib import Path",
          "22: from typing import TYPE_CHECKING, Optional",
          "25: from itsdangerous import TimedJSONWebSignatureSerializer",
          "27: from airflow.configuration import AirflowConfigException, conf",
          "",
          "[Removed Lines]",
          "24: import httpx",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "159:             except Exception as f:",
          "160:                 log += f'*** Unable to fetch logs from worker pod {ti.hostname} ***\\n{str(f)}\\n\\n'",
          "161:         else:",
          "162:             url = os.path.join(\"http://{ti.hostname}:{worker_log_server_port}/log\", log_relative_path).format(",
          "163:                 ti=ti, worker_log_server_port=conf.get('logging', 'WORKER_LOG_SERVER_PORT')",
          "164:             )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "161:             import httpx",
          "",
          "---------------"
        ]
      }
    }
  ]
}