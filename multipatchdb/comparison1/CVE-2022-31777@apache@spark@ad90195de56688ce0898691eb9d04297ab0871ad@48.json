{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "bd6fd7e1320f689c42c8ef6710f250123a78707d",
      "candidate_info": {
        "commit_hash": "bd6fd7e1320f689c42c8ef6710f250123a78707d",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/bd6fd7e1320f689c42c8ef6710f250123a78707d",
        "files": [
          "python/pyspark/sql/tests/test_dataframe.py",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePython.scala"
        ],
        "message": "[SPARK-39084][PYSPARK] Fix df.rdd.isEmpty() by using TaskContext to stop iterator on task completion\n\n### What changes were proposed in this pull request?\n\nThis PR fixes the issue described in https://issues.apache.org/jira/browse/SPARK-39084 where calling `df.rdd.isEmpty()` on a particular dataset could result in a JVM crash and/or executor failure.\n\nThe issue was due to Python iterator not being synchronised with Java iterator so when the task is complete, the Python iterator continues to process data. We have introduced ContextAwareIterator as part of https://issues.apache.org/jira/browse/SPARK-33277 but we did not fix all of the places where this should be used.\n\n### Why are the changes needed?\n\nFixes the JVM crash when checking isEmpty() on a dataset.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nI added a test case that reproduces the issue 100%. I confirmed that the test fails without the fix and passes with the fix.\n\nCloses #36425 from sadikovi/fix-pyspark-iter-2.\n\nAuthored-by: Ivan Sadikov <ivan.sadikov@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 9305cc744d27daa6a746d3eb30e7639c63329072)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/sql/tests/test_dataframe.py||python/pyspark/sql/tests/test_dataframe.py",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePython.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePython.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/sql/tests/test_dataframe.py||python/pyspark/sql/tests/test_dataframe.py": [
          "File: python/pyspark/sql/tests/test_dataframe.py -> python/pyspark/sql/tests/test_dataframe.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import tempfile",
          "23: import time",
          "24: import unittest",
          "25: from typing import cast",
          "27: from pyspark.sql import SparkSession, Row",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: import uuid",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1141:         with self.assertRaisesRegex(TypeError, \"Parameter 'truncate=foo'\"):",
          "1142:             df.show(truncate=\"foo\")",
          "1144:     @unittest.skipIf(",
          "1145:         not have_pandas or not have_pyarrow,",
          "1146:         cast(str, pandas_requirement_message or pyarrow_requirement_message),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1145:     def test_df_is_empty(self):",
          "1146:         # SPARK-39084: Fix df.rdd.isEmpty() resulting in JVM crash.",
          "1148:         # This particular example of DataFrame reproduces an issue in isEmpty call",
          "1149:         # which could result in JVM crash.",
          "1150:         data = []",
          "1151:         for t in range(0, 10000):",
          "1152:             id = str(uuid.uuid4())",
          "1153:             if t == 0:",
          "1154:                 for i in range(0, 99):",
          "1155:                     data.append((id,))",
          "1156:             elif t < 10:",
          "1157:                 for i in range(0, 75):",
          "1158:                     data.append((id,))",
          "1159:             elif t < 100:",
          "1160:                 for i in range(0, 50):",
          "1161:                     data.append((id,))",
          "1162:             elif t < 1000:",
          "1163:                 for i in range(0, 25):",
          "1164:                     data.append((id,))",
          "1165:             else:",
          "1166:                 for i in range(0, 10):",
          "1167:                     data.append((id,))",
          "1169:         tmpPath = tempfile.mkdtemp()",
          "1170:         shutil.rmtree(tmpPath)",
          "1171:         try:",
          "1172:             df = self.spark.createDataFrame(data, [\"col\"])",
          "1173:             df.coalesce(1).write.parquet(tmpPath)",
          "1175:             res = self.spark.read.parquet(tmpPath).groupBy(\"col\").count()",
          "1176:             self.assertFalse(res.rdd.isEmpty())",
          "1177:         finally:",
          "1178:             shutil.rmtree(tmpPath)",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePython.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePython.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePython.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/python/EvaluatePython.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: import net.razorvine.pickle.{IObjectPickler, Opcodes, Pickler}",
          "27: import org.apache.spark.api.python.SerDeUtil",
          "28: import org.apache.spark.rdd.RDD",
          "29: import org.apache.spark.sql.catalyst.InternalRow",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: import org.apache.spark.{ContextAwareIterator, TaskContext}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "301:   def javaToPython(rdd: RDD[Any]): RDD[Array[Byte]] = {",
          "302:     rdd.mapPartitions { iter =>",
          "303:       registerPicklers()  // let it called in executor",
          "305:     }",
          "306:   }",
          "307: }",
          "",
          "[Removed Lines]",
          "304:       new SerDeUtil.AutoBatchedPickler(iter)",
          "",
          "[Added Lines]",
          "305:       new SerDeUtil.AutoBatchedPickler(new ContextAwareIterator(TaskContext.get, iter))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cf72e52c3df77231cce829111c96a13a79c9b529",
      "candidate_info": {
        "commit_hash": "cf72e52c3df77231cce829111c96a13a79c9b529",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/cf72e52c3df77231cce829111c96a13a79c9b529",
        "files": [
          "python/run-tests.py"
        ],
        "message": "[SPARK-39621][PYTHON][TESTS] Make `run-tests.py` robust by avoiding `rmtree` on MacOS\n\n### What changes were proposed in this pull request?\n\nThis PR aims to make `run-tests.py` robust by avoiding `rmtree` on MacOS.\n\n### Why are the changes needed?\n\nThere exists a race condition in Python and it causes flakiness in MacOS\n- https://bugs.python.org/issue29699\n- https://github.com/python/cpython/issues/73885\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nAfter passing CIs, this should be tested on MacOS.\n\nCloses #37010 from dongjoon-hyun/SPARK-39621.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 432945db743965f1beb59e3a001605335ca2df83)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "python/run-tests.py||python/run-tests.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/run-tests.py||python/run-tests.py": [
          "File: python/run-tests.py -> python/run-tests.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import logging",
          "21: from argparse import ArgumentParser",
          "22: import os",
          "23: import re",
          "24: import shutil",
          "25: import subprocess",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "23: import platform",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "113:         retcode = subprocess.Popen(",
          "114:             [os.path.join(SPARK_HOME, \"bin/pyspark\")] + test_name.split(),",
          "115:             stderr=per_test_output, stdout=per_test_output, env=env).wait()",
          "117:     except BaseException:",
          "118:         LOGGER.exception(\"Got exception while running %s with %s\", test_name, pyspark_python)",
          "119:         # Here, we use os._exit() instead of sys.exit() in order to force Python to exit even if",
          "",
          "[Removed Lines]",
          "116:         shutil.rmtree(tmp_dir, ignore_errors=True)",
          "",
          "[Added Lines]",
          "117:         # There exists a race condition in Python and it causes flakiness in MacOS",
          "118:         # https://github.com/python/cpython/issues/73885",
          "119:         if platform.system() == \"Darwin\":",
          "120:             os.system(\"rm -rf \" + tmp_dir)",
          "121:         else:",
          "122:             shutil.rmtree(tmp_dir, ignore_errors=True)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "052d60c28a8fd0e4e33051aa0682d3df4d979ae8",
      "candidate_info": {
        "commit_hash": "052d60c28a8fd0e4e33051aa0682d3df4d979ae8",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/052d60c28a8fd0e4e33051aa0682d3df4d979ae8",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LikeSimplificationSuite.scala"
        ],
        "message": "[SPARK-40228][SQL][3.3] Do not simplify multiLike if child is not a cheap expression\n\nThis PR backport https://github.com/apache/spark/pull/37672 to branch-3.3.\n\nThe original PR's description:\n\n### What changes were proposed in this pull request?\n\nDo not simplify multiLike if child is not a cheap expression.\n\n### Why are the changes needed?\n\n1. Simplifying multiLike in this cases can not benefit the query because it cannot be pushed down.\n2. Reduce the number of evaluations for these expressions.\n\nFor example:\n```sql\nselect * from t1 where substr(name, 1, 5) like any('%a', 'b%', '%c%');\n```\n```\n== Physical Plan ==\n*(1) Filter ((EndsWith(substr(name#0, 1, 5), a) OR StartsWith(substr(name#0, 1, 5), b)) OR Contains(substr(name#0, 1, 5), c))\n   +- *(1) ColumnarToRow\n      +- FileScan parquet default.t1[name#0] Batched: true, DataFilters: [((EndsWith(substr(name#0, 1, 5), a) OR StartsWith(substr(name#0, 1, 5), b)) OR Contains(substr(n..., Format: Parquet, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<name:string>\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nUnit test.\n\nCloses #37813 from wangyum/SPARK-40228-branch-3.3.\n\nAuthored-by: Yuming Wang <yumwang@ebay.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LikeSimplificationSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LikeSimplificationSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1075:     case _ => false",
          "1076:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1081:   def isCheap(e: Expression): Boolean = e match {",
          "1082:     case _: Attribute | _: OuterReference => true",
          "1083:     case _ if e.foldable => true",
          "1085:     case _: PythonUDF => true",
          "1087:     case _: Alias | _: ExtractValue => e.children.forall(isCheap)",
          "1088:     case _ => false",
          "1089:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "773:       } else {",
          "774:         simplifyLike(input, pattern.toString, escapeChar).getOrElse(l)",
          "775:       }",
          "780:   }",
          "781: }",
          "",
          "[Removed Lines]",
          "776:     case l @ LikeAll(child, patterns) => simplifyMultiLike(child, patterns, l)",
          "777:     case l @ NotLikeAll(child, patterns) => simplifyMultiLike(child, patterns, l)",
          "778:     case l @ LikeAny(child, patterns) => simplifyMultiLike(child, patterns, l)",
          "779:     case l @ NotLikeAny(child, patterns) => simplifyMultiLike(child, patterns, l)",
          "",
          "[Added Lines]",
          "776:     case l @ LikeAll(child, patterns) if CollapseProject.isCheap(child) =>",
          "777:       simplifyMultiLike(child, patterns, l)",
          "778:     case l @ NotLikeAll(child, patterns) if CollapseProject.isCheap(child) =>",
          "779:       simplifyMultiLike(child, patterns, l)",
          "780:     case l @ LikeAny(child, patterns) if CollapseProject.isCheap(child) =>",
          "781:       simplifyMultiLike(child, patterns, l)",
          "782:     case l @ NotLikeAny(child, patterns) if CollapseProject.isCheap(child) =>",
          "783:       simplifyMultiLike(child, patterns, l)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LikeSimplificationSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LikeSimplificationSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LikeSimplificationSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/LikeSimplificationSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import org.apache.spark.sql.catalyst.plans.logical._",
          "25: import org.apache.spark.sql.catalyst.rules._",
          "26: import org.apache.spark.sql.types.{BooleanType, StringType}",
          "28: class LikeSimplificationSuite extends PlanTest {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: import org.apache.spark.unsafe.types.UTF8String",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "233:     comparePlans(optimized, correctAnswer)",
          "234:   }",
          "235: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "237:   test(\"SPARK-40228: Simplify multiLike if child is foldable expression\") {",
          "238:     comparePlans(Optimize.execute(testRelation.where(\"a\" likeAny(\"abc%\", \"\", \"ab\")).analyze),",
          "239:       testRelation.where(StartsWith(\"a\", \"abc\") || EqualTo(\"a\", \"\") || EqualTo(\"a\", \"ab\") ||",
          "240:         LikeAny(\"a\", Seq.empty[UTF8String])).analyze)",
          "241:   }",
          "243:   test(\"SPARK-40228: Do not simplify multiLike if child is not a cheap expression\") {",
          "244:     val originalQuery = testRelation.where($\"a\".substring(1, 5) likeAny(\"abc%\", \"\", \"ab\")).analyze",
          "246:     comparePlans(Optimize.execute(originalQuery), originalQuery)",
          "247:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a7480e647fe1ed930c0cd2ad1679b3685a675d02",
      "candidate_info": {
        "commit_hash": "a7480e647fe1ed930c0cd2ad1679b3685a675d02",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/a7480e647fe1ed930c0cd2ad1679b3685a675d02",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala",
          "sql/core/src/test/java/test/org/apache/spark/sql/JavaBeanDeserializationSuite.java"
        ],
        "message": "[SPARK-38823][SQL] Make `NewInstance` non-foldable to fix aggregation buffer corruption issue\n\n### What changes were proposed in this pull request?\n\nMake `NewInstance` non-foldable.\n\n### Why are the changes needed?\n\nWhen handling Java beans as input, Spark creates `NewInstance` with no arguments. On master and 3.3, `NewInstance` with no arguments is considered foldable. As a result, the `ConstantFolding` rule converts `NewInstance` into a `Literal` holding an instance of the user's specified Java bean. The instance becomes a singleton that gets reused for each input record (although its fields get updated by `InitializeJavaBean`).\n\nBecause the instance gets reused, sometimes multiple buffers in `AggregationIterator` are actually referring to the same Java bean instance.\n\nTake, for example, the test I added in this PR, or the `spark-shell` example I added to SPARK-38823 as a comment.\n\nThe input is:\n```\n    new Item(\"a\", 1),\n    new Item(\"b\", 3),\n    new Item(\"c\", 2),\n    new Item(\"a\", 7)\n```\nAs `ObjectAggregationIterator` reads the input, the buffers get set up as follows (note that the first field of Item should be the same as the key):\n```\n- Read Item(\"a\", 1)\n\n- Buffers are now:\n  Key \"a\" --> Item(\"a\", 1)\n\n- Read Item(\"b\", 3)\n\n- Buffers are now:\n  Key \"a\" -> Item(\"b\", 3)\n  Key \"b\" -> Item(\"b\", 3)\n```\n\nThe buffer for key \"a\" now contains `Item(\"b\", 3)`. That's because both buffers contain a reference to the same Item instance, and that Item instance's fields were updated when `Item(\"b\", 3)` was read.\n\nThis PR makes `NewInstance` non-foldable, so it will not get optimized away, thus ensuring a new instance of the Java bean for each input record.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nNew unit test.\n\nCloses #36183 from bersprockets/newinstance_issue.\n\nAuthored-by: Bruce Robbins <bersprockets@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit cc7cb7a803d5de03c526480c8968bbb2c3e82484)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala",
          "sql/core/src/test/java/test/org/apache/spark/sql/JavaBeanDeserializationSuite.java||sql/core/src/test/java/test/org/apache/spark/sql/JavaBeanDeserializationSuite.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "517:   override def nullable: Boolean = needNullCheck",
          "519:   override def children: Seq[Expression] = arguments",
          "521:   final override val nodePatterns: Seq[TreePattern] = Seq(NEW_INSTANCE)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "521:   override def foldable: Boolean = false",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ConstantFoldingSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import org.apache.spark.sql.catalyst.dsl.expressions._",
          "22: import org.apache.spark.sql.catalyst.dsl.plans._",
          "23: import org.apache.spark.sql.catalyst.expressions._",
          "25: import org.apache.spark.sql.catalyst.plans.PlanTest",
          "26: import org.apache.spark.sql.catalyst.plans.logical.{LocalRelation, LogicalPlan}",
          "27: import org.apache.spark.sql.catalyst.rules.RuleExecutor",
          "29: import org.apache.spark.sql.types._",
          "30: import org.apache.spark.unsafe.types.ByteArray",
          "",
          "[Removed Lines]",
          "24: import org.apache.spark.sql.catalyst.expressions.objects.{Invoke, NewInstance, StaticInvoke}",
          "28: import org.apache.spark.sql.catalyst.util.GenericArrayData",
          "",
          "[Added Lines]",
          "24: import org.apache.spark.sql.catalyst.expressions.objects.{Invoke, StaticInvoke}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "318:             Literal.create(\"a\", StringType),",
          "319:             \"substring\",",
          "320:             StringType,",
          "330:     val optimized = Optimize.execute(originalQuery.analyze)",
          "",
          "[Removed Lines]",
          "321:             Seq(Literal(0), Literal(1))).as(\"c2\"),",
          "322:           NewInstance(",
          "323:             cls = classOf[GenericArrayData],",
          "324:             arguments = Literal.fromObject(List(1, 2, 3)) :: Nil,",
          "325:             inputTypes = Nil,",
          "326:             propagateNull = false,",
          "327:             dataType = ArrayType(IntegerType),",
          "328:             outerPointer = None).as(\"c3\"))",
          "",
          "[Added Lines]",
          "320:             Seq(Literal(0), Literal(1))).as(\"c2\"))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "333:       testRelation",
          "334:         .select(",
          "335:           Literal(\"WWSpark\".getBytes()).as(\"c1\"),",
          "338:         .analyze",
          "340:     comparePlans(optimized, correctAnswer)",
          "",
          "[Removed Lines]",
          "336:           Literal.create(\"a\", StringType).as(\"c2\"),",
          "337:           Literal.create(new GenericArrayData(List(1, 2, 3)), ArrayType(IntegerType)).as(\"c3\"))",
          "",
          "[Added Lines]",
          "328:           Literal.create(\"a\", StringType).as(\"c2\"))",
          "",
          "---------------"
        ],
        "sql/core/src/test/java/test/org/apache/spark/sql/JavaBeanDeserializationSuite.java||sql/core/src/test/java/test/org/apache/spark/sql/JavaBeanDeserializationSuite.java": [
          "File: sql/core/src/test/java/test/org/apache/spark/sql/JavaBeanDeserializationSuite.java -> sql/core/src/test/java/test/org/apache/spark/sql/JavaBeanDeserializationSuite.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: import org.apache.commons.lang3.builder.ToStringBuilder;",
          "28: import org.apache.commons.lang3.builder.ToStringStyle;",
          "29: import org.junit.*;",
          "31: import org.apache.spark.sql.*;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "29: import org.apache.spark.api.java.function.MapFunction;",
          "30: import org.apache.spark.api.java.function.ReduceFunction;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "37: import org.apache.spark.sql.types.StructType;",
          "39: import org.apache.spark.sql.test.TestSparkSession;",
          "41: public class JavaBeanDeserializationSuite implements Serializable {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42: import scala.Tuple2;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "562:     }",
          "563:   }",
          "565:   public static final class LocalDateInstantRecord {",
          "566:     private String localDateField;",
          "567:     private String instantField;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "568:   @Test",
          "569:   public void testSPARK38823NoBeanReuse() {",
          "570:     List<Item> items = Arrays.asList(",
          "571:             new Item(\"a\", 1),",
          "572:             new Item(\"b\", 3),",
          "573:             new Item(\"c\", 2),",
          "574:             new Item(\"a\", 7));",
          "576:     Encoder<Item> encoder = Encoders.bean(Item.class);",
          "578:     Dataset<Item> ds = spark.createDataFrame(items, Item.class)",
          "579:             .as(encoder)",
          "580:             .coalesce(1);",
          "582:     MapFunction<Item, String> mf = new MapFunction<Item, String>() {",
          "583:       @Override",
          "584:       public String call(Item item) throws Exception {",
          "585:         return item.getK();",
          "586:       }",
          "587:     };",
          "589:     ReduceFunction<Item> rf = new ReduceFunction<Item>() {",
          "590:       @Override",
          "591:       public Item call(Item item1, Item item2) throws Exception {",
          "592:         Assert.assertNotSame(item1, item2);",
          "593:         return item1.addValue(item2.getV());",
          "594:       }",
          "595:     };",
          "597:     Dataset<Tuple2<String, Item>> finalDs = ds",
          "598:             .groupByKey(mf, Encoders.STRING())",
          "599:             .reduceGroups(rf);",
          "601:     List<Tuple2<String, Item>> expectedRecords = Arrays.asList(",
          "602:             new Tuple2(\"a\", new Item(\"a\", 8)),",
          "603:             new Tuple2(\"b\", new Item(\"b\", 3)),",
          "604:             new Tuple2(\"c\", new Item(\"c\", 2)));",
          "606:     List<Tuple2<String, Item>> result = finalDs.collectAsList();",
          "608:     Assert.assertEquals(expectedRecords, result);",
          "609:   }",
          "611:   public static class Item implements Serializable {",
          "612:     private String k;",
          "613:     private int v;",
          "615:     public String getK() {",
          "616:       return k;",
          "617:     }",
          "619:     public int getV() {",
          "620:       return v;",
          "621:     }",
          "623:     public void setK(String k) {",
          "624:       this.k = k;",
          "625:     }",
          "627:     public void setV(int v) {",
          "628:       this.v = v;",
          "629:     }",
          "631:     public Item() { }",
          "633:     public Item(String k, int v) {",
          "634:       this.k = k;",
          "635:       this.v = v;",
          "636:     }",
          "638:     public Item addValue(int inc) {",
          "639:       return new Item(k, v + inc);",
          "640:     }",
          "642:     public String toString() {",
          "643:       return \"Item(\" + k + \",\" + v + \")\";",
          "644:     }",
          "646:     public boolean equals(Object o) {",
          "647:       if (!(o instanceof Item)) {",
          "648:         return false;",
          "649:       }",
          "650:       Item other = (Item) o;",
          "651:       if (other.getK().equals(k) && other.getV() == v) {",
          "652:         return true;",
          "653:       }",
          "654:       return false;",
          "655:     }",
          "656:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "96d66b030d914ccd7ded74e33287e45d09935e27",
      "candidate_info": {
        "commit_hash": "96d66b030d914ccd7ded74e33287e45d09935e27",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/96d66b030d914ccd7ded74e33287e45d09935e27",
        "files": [
          "python/pyspark/sql/pandas/conversion.py"
        ],
        "message": "[SPARK-38988][PYTHON] Suppress PerformanceWarnings of `DataFrame.info`\n\n### What changes were proposed in this pull request?\nSuppress PerformanceWarnings of DataFrame.info\n\n### Why are the changes needed?\nTo improve usability.\n\n### Does this PR introduce _any_ user-facing change?\nNo. Only PerformanceWarnings of DataFrame.info are suppressed.\n\n### How was this patch tested?\nManual tests.\n\nCloses #36367 from xinrong-databricks/frame.info.\n\nAuthored-by: Xinrong Meng <xinrong.meng@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 594337fad131280f62107326062fb554f0566d43)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/sql/pandas/conversion.py||python/pyspark/sql/pandas/conversion.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/sql/pandas/conversion.py||python/pyspark/sql/pandas/conversion.py": [
          "File: python/pyspark/sql/pandas/conversion.py -> python/pyspark/sql/pandas/conversion.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: # limitations under the License.",
          "16: #",
          "17: import sys",
          "19: from collections import Counter",
          "20: from typing import List, Optional, Type, Union, no_type_check, overload, TYPE_CHECKING",
          "22: from pyspark.rdd import _load_from_socket",
          "23: from pyspark.sql.pandas.serializers import ArrowCollectSerializer",
          "",
          "[Removed Lines]",
          "18: import warnings",
          "",
          "[Added Lines]",
          "20: from warnings import catch_warnings, simplefilter, warn",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "111:                         \"'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to \"",
          "112:                         \"true.\" % str(e)",
          "113:                     )",
          "115:                     use_arrow = False",
          "116:                 else:",
          "117:                     msg = (",
          "",
          "[Removed Lines]",
          "114:                     warnings.warn(msg)",
          "",
          "[Added Lines]",
          "114:                     warn(msg)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "121:                         \"with 'spark.sql.execution.arrow.pyspark.fallback.enabled' has been set to \"",
          "122:                         \"false.\\n  %s\" % str(e)",
          "123:                     )",
          "125:                     raise",
          "127:             # Try to use Arrow optimization when the schema is supported and the required version",
          "",
          "[Removed Lines]",
          "124:                     warnings.warn(msg)",
          "",
          "[Added Lines]",
          "124:                     warn(msg)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "198:                         \"effect on failures in the middle of \"",
          "199:                         \"computation.\\n  %s\" % str(e)",
          "200:                     )",
          "202:                     raise",
          "204:         # Below is toPandas without Arrow optimization.",
          "",
          "[Removed Lines]",
          "201:                     warnings.warn(msg)",
          "",
          "[Added Lines]",
          "201:                     warn(msg)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "247:             if (t is not None and not is_timedelta64_dtype(t)) or should_check_timedelta:",
          "248:                 series = series.astype(t, copy=False)",
          "258:         if timezone is None:",
          "259:             return df",
          "",
          "[Removed Lines]",
          "250:             # `insert` API makes copy of data, we only do it for Series of duplicate column names.",
          "251:             # `pdf.iloc[:, index] = pdf.iloc[:, index]...` doesn't always work because `iloc` could",
          "252:             # return a view or a copy depending by context.",
          "253:             if column_counter[column_name] > 1:",
          "254:                 df.insert(index, column_name, series, allow_duplicates=True)",
          "255:             else:",
          "256:                 df[column_name] = series",
          "",
          "[Added Lines]",
          "250:             with catch_warnings():",
          "251:                 from pandas.errors import PerformanceWarning",
          "253:                 simplefilter(action=\"ignore\", category=PerformanceWarning)",
          "254:                 # `insert` API makes copy of data,",
          "255:                 # we only do it for Series of duplicate column names.",
          "256:                 # `pdf.iloc[:, index] = pdf.iloc[:, index]...` doesn't always work",
          "257:                 # because `iloc` could return a view or a copy depending by context.",
          "258:                 if column_counter[column_name] > 1:",
          "259:                     df.insert(index, column_name, series, allow_duplicates=True)",
          "260:                 else:",
          "261:                     df[column_name] = series",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "417:                         \"'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to \"",
          "418:                         \"true.\" % str(e)",
          "419:                     )",
          "421:                 else:",
          "422:                     msg = (",
          "423:                         \"createDataFrame attempted Arrow optimization because \"",
          "",
          "[Removed Lines]",
          "420:                     warnings.warn(msg)",
          "",
          "[Added Lines]",
          "425:                     warn(msg)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "426:                         \"fallback with 'spark.sql.execution.arrow.pyspark.fallback.enabled' \"",
          "427:                         \"has been set to false.\\n  %s\" % str(e)",
          "428:                     )",
          "430:                     raise",
          "431:         converted_data = self._convert_from_pandas(data, schema, timezone)",
          "432:         return self._create_dataframe(converted_data, schema, samplingRatio, verifySchema)",
          "",
          "[Removed Lines]",
          "429:                     warnings.warn(msg)",
          "",
          "[Added Lines]",
          "434:                     warn(msg)",
          "",
          "---------------"
        ]
      }
    }
  ]
}