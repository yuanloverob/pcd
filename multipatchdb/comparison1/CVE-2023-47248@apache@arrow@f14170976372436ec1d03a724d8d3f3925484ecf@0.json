{
  "cve_id": "CVE-2023-47248",
  "cve_desc": "Deserialization of untrusted data in IPC and Parquet readers in PyArrow versions 0.14.0 to 14.0.0 allows arbitrary code execution. An application is vulnerable if it reads Arrow IPC, Feather or Parquet data from untrusted sources (for example user-supplied input files).\n\nThis vulnerability only affects PyArrow, not other Apache Arrow implementations or bindings.\n\nIt is recommended that users of PyArrow upgrade to 14.0.1. Similarly, it is recommended that downstream libraries upgrade their dependency requirements to PyArrow 14.0.1 or later. PyPI packages are already available, and we hope that conda-forge packages will be available soon.\n\nIf it is not possible to upgrade, we provide a separate package `pyarrow-hotfix` that disables the vulnerability on older PyArrow versions. See  https://pypi.org/project/pyarrow-hotfix/  for instructions.",
  "repo": "apache/arrow",
  "patch_hash": "f14170976372436ec1d03a724d8d3f3925484ecf",
  "patch_info": {
    "commit_hash": "f14170976372436ec1d03a724d8d3f3925484ecf",
    "repo": "apache/arrow",
    "commit_url": "https://github.com/apache/arrow/commit/f14170976372436ec1d03a724d8d3f3925484ecf",
    "files": [
      "docs/source/python/extending_types.rst",
      "python/pyarrow/tests/test_cffi.py",
      "python/pyarrow/tests/test_extension_type.py",
      "python/pyarrow/tests/test_pandas.py",
      "python/pyarrow/types.pxi"
    ],
    "message": "GH-38607: [Python] Disable PyExtensionType autoload (#38608)\n\n### Rationale for this change\n\nPyExtensionType autoload is really a misfeature. It creates PyArrow-specific extension types, though using ExtensionType is almost the same complexity while allowing deserialization from non-PyArrow software.\n\n### What changes are included in this PR?\n\n* Disable PyExtensionType autoloading and deprecate PyExtensionType instantiation.\n* Update the docs to emphasize ExtensionType.\n\n### Are these changes tested?\n\nYes.\n\n### Are there any user-facing changes?\n\nYes.\n\n* Closes: #38607\n\nAuthored-by: Antoine Pitrou <antoine@python.org>\nSigned-off-by: Ra\u00fal Cumplido <raulcumplido@gmail.com>",
    "before_after_code_files": [
      "python/pyarrow/tests/test_cffi.py||python/pyarrow/tests/test_cffi.py",
      "python/pyarrow/tests/test_extension_type.py||python/pyarrow/tests/test_extension_type.py",
      "python/pyarrow/tests/test_pandas.py||python/pyarrow/tests/test_pandas.py",
      "python/pyarrow/types.pxi||python/pyarrow/types.pxi"
    ]
  },
  "patch_diff": {
    "python/pyarrow/tests/test_cffi.py||python/pyarrow/tests/test_cffi.py": [
      "File: python/pyarrow/tests/test_cffi.py -> python/pyarrow/tests/test_cffi.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "16: # specific language governing permissions and limitations",
      "17: # under the License.",
      "19: import ctypes",
      "20: import gc",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "19: import contextlib",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "51:     return ctypes.pythonapi.PyCapsule_IsValid(ctypes.py_object(capsule), name) == 1",
      "56:     def __init__(self, width):",
      "57:         self._width = width",
      "60:     @property",
      "61:     def width(self):",
      "62:         return self._width",
      "68: def make_schema():",
      "",
      "[Removed Lines]",
      "54: class ParamExtType(pa.PyExtensionType):",
      "58:         pa.PyExtensionType.__init__(self, pa.binary(width))",
      "64:     def __reduce__(self):",
      "65:         return ParamExtType, (self.width,)",
      "",
      "[Added Lines]",
      "55: @contextlib.contextmanager",
      "56: def registered_extension_type(ext_type):",
      "57:     pa.register_extension_type(ext_type)",
      "58:     try:",
      "59:         yield",
      "60:     finally:",
      "61:         pa.unregister_extension_type(ext_type.extension_name)",
      "64: class ParamExtType(pa.ExtensionType):",
      "68:         super().__init__(pa.binary(width),",
      "69:                          \"pyarrow.tests.test_cffi.ParamExtType\")",
      "75:     def __arrow_ext_serialize__(self):",
      "76:         return str(self.width).encode()",
      "78:     @classmethod",
      "79:     def __arrow_ext_deserialize__(cls, storage_type, serialized):",
      "80:         width = int(serialized.decode())",
      "81:         return cls(width)",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "75:                      metadata={b'key1': b'value1'})",
      "78: def make_batch():",
      "79:     return pa.record_batch([[[1], [2, 42]]], make_schema())",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "94: def make_extension_storage_schema():",
      "95:     # Should be kept in sync with make_extension_schema",
      "96:     return pa.schema([('ext', ParamExtType(3).storage_type)],",
      "97:                      metadata={b'key1': b'value1'})",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "204:         pa.Array._import_from_c(ptr_array, ptr_schema)",
      "208:     c_schema = ffi.new(\"struct ArrowSchema*\")",
      "209:     ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))",
      "",
      "[Removed Lines]",
      "207: def check_export_import_schema(schema_factory):",
      "",
      "[Added Lines]",
      "229: def check_export_import_schema(schema_factory, expected_schema_factory=None):",
      "230:     if expected_schema_factory is None:",
      "231:         expected_schema_factory = schema_factory",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "215:     assert pa.total_allocated_bytes() > old_allocated",
      "216:     # Delete and recreate C++ object from exported pointer",
      "217:     schema_new = pa.Schema._import_from_c(ptr_schema)",
      "219:     assert pa.total_allocated_bytes() == old_allocated",
      "220:     del schema_new",
      "221:     assert pa.total_allocated_bytes() == old_allocated",
      "",
      "[Removed Lines]",
      "218:     assert schema_new == schema_factory()",
      "",
      "[Added Lines]",
      "243:     assert schema_new == expected_schema_factory()",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "241: @needs_cffi",
      "242: def test_export_import_schema_with_extension():",
      "246: @needs_cffi",
      "",
      "[Removed Lines]",
      "243:     check_export_import_schema(make_extension_schema)",
      "",
      "[Added Lines]",
      "268:     # Extension type is unregistered => the storage type is imported",
      "269:     check_export_import_schema(make_extension_schema,",
      "270:                                make_extension_storage_schema)",
      "272:     # Extension type is registered => the extension type is imported",
      "273:     with registered_extension_type(ParamExtType(1)):",
      "274:         check_export_import_schema(make_extension_schema)",
      "",
      "---------------",
      "--- Hunk 7 ---",
      "[Context before]",
      "320: @needs_cffi",
      "321: def test_export_import_batch_with_extension():",
      "325: def _export_import_batch_reader(ptr_stream, reader_factory):",
      "",
      "[Removed Lines]",
      "322:     check_export_import_batch(make_extension_batch)",
      "",
      "[Added Lines]",
      "353:     with registered_extension_type(ParamExtType(1)):",
      "354:         check_export_import_batch(make_extension_batch)",
      "",
      "---------------"
    ],
    "python/pyarrow/tests/test_extension_type.py||python/pyarrow/tests/test_extension_type.py": [
      "File: python/pyarrow/tests/test_extension_type.py -> python/pyarrow/tests/test_extension_type.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "15: # specific language governing permissions and limitations",
      "16: # under the License.",
      "18: import os",
      "19: import shutil",
      "20: import subprocess",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "18: import contextlib",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "29: import pytest",
      "34:     def __init__(self):",
      "43:     def __init__(self):",
      "52:     def __init__(self):",
      "59: class UuidScalarType(pa.ExtensionScalar):",
      "",
      "[Removed Lines]",
      "32: class TinyIntType(pa.PyExtensionType):",
      "35:         pa.PyExtensionType.__init__(self, pa.int8())",
      "37:     def __reduce__(self):",
      "38:         return TinyIntType, ()",
      "41: class IntegerType(pa.PyExtensionType):",
      "44:         pa.PyExtensionType.__init__(self, pa.int64())",
      "46:     def __reduce__(self):",
      "47:         return IntegerType, ()",
      "50: class IntegerEmbeddedType(pa.PyExtensionType):",
      "53:         pa.PyExtensionType.__init__(self, IntegerType())",
      "55:     def __reduce__(self):",
      "56:         return IntegerEmbeddedType, ()",
      "",
      "[Added Lines]",
      "33: @contextlib.contextmanager",
      "34: def registered_extension_type(ext_type):",
      "35:     pa.register_extension_type(ext_type)",
      "36:     try:",
      "37:         yield",
      "38:     finally:",
      "39:         pa.unregister_extension_type(ext_type.extension_name)",
      "42: @contextlib.contextmanager",
      "43: def enabled_auto_load():",
      "44:     pa.PyExtensionType.set_auto_load(True)",
      "45:     try:",
      "46:         yield",
      "47:     finally:",
      "48:         pa.PyExtensionType.set_auto_load(False)",
      "51: class TinyIntType(pa.ExtensionType):",
      "54:         super().__init__(pa.int8(), 'pyarrow.tests.TinyIntType')",
      "56:     def __arrow_ext_serialize__(self):",
      "57:         return b''",
      "59:     @classmethod",
      "60:     def __arrow_ext_deserialize__(cls, storage_type, serialized):",
      "61:         assert serialized == b''",
      "62:         assert storage_type == pa.int8()",
      "63:         return cls()",
      "66: class IntegerType(pa.ExtensionType):",
      "69:         super().__init__(pa.int64(), 'pyarrow.tests.IntegerType')",
      "71:     def __arrow_ext_serialize__(self):",
      "72:         return b''",
      "74:     @classmethod",
      "75:     def __arrow_ext_deserialize__(cls, storage_type, serialized):",
      "76:         assert serialized == b''",
      "77:         assert storage_type == pa.int64()",
      "78:         return cls()",
      "81: class IntegerEmbeddedType(pa.ExtensionType):",
      "84:         super().__init__(IntegerType(), 'pyarrow.tests.IntegerType')",
      "86:     def __arrow_ext_serialize__(self):",
      "87:         # XXX pa.BaseExtensionType should expose C++ serialization method",
      "88:         return self.storage_type.__arrow_ext_serialize__()",
      "90:     @classmethod",
      "91:     def __arrow_ext_deserialize__(cls, storage_type, serialized):",
      "92:         deserialized_storage_type = storage_type.__arrow_ext_deserialize__(",
      "93:             serialized)",
      "94:         assert deserialized_storage_type == storage_type",
      "95:         return cls()",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "61:         return None if self.value is None else UUID(bytes=self.value.as_py())",
      "66:     def __init__(self):",
      "72:     def __arrow_ext_scalar_class__(self):",
      "73:         return UuidScalarType",
      "78:     def __init__(self):",
      "87:     def __init__(self):",
      "96:     def __init__(self, width):",
      "97:         self._width = width",
      "100:     @property",
      "101:     def width(self):",
      "102:         return self._width",
      "109:     storage_type = pa.struct([('left', pa.int64()),",
      "110:                               ('right', pa.int64())])",
      "112:     def __init__(self):",
      "121:     def __init__(self, storage_type):",
      "129:     \"\"\"",
      "130:     Generic extension type that can store any storage type.",
      "131:     \"\"\"",
      "133:     def __init__(self, storage_type, annotation):",
      "134:         self.annotation = annotation",
      "137:     def __reduce__(self):",
      "141: def ipc_write_batch(batch):",
      "",
      "[Removed Lines]",
      "64: class UuidType(pa.PyExtensionType):",
      "67:         pa.PyExtensionType.__init__(self, pa.binary(16))",
      "69:     def __reduce__(self):",
      "70:         return UuidType, ()",
      "76: class UuidType2(pa.PyExtensionType):",
      "79:         pa.PyExtensionType.__init__(self, pa.binary(16))",
      "81:     def __reduce__(self):",
      "82:         return UuidType2, ()",
      "85: class LabelType(pa.PyExtensionType):",
      "88:         pa.PyExtensionType.__init__(self, pa.string())",
      "90:     def __reduce__(self):",
      "91:         return LabelType, ()",
      "94: class ParamExtType(pa.PyExtensionType):",
      "98:         pa.PyExtensionType.__init__(self, pa.binary(width))",
      "104:     def __reduce__(self):",
      "105:         return ParamExtType, (self.width,)",
      "108: class MyStructType(pa.PyExtensionType):",
      "113:         pa.PyExtensionType.__init__(self, self.storage_type)",
      "115:     def __reduce__(self):",
      "116:         return MyStructType, ()",
      "119: class MyListType(pa.PyExtensionType):",
      "122:         pa.PyExtensionType.__init__(self, storage_type)",
      "124:     def __reduce__(self):",
      "125:         return MyListType, (self.storage_type,)",
      "128: class AnnotatedType(pa.PyExtensionType):",
      "135:         super().__init__(storage_type)",
      "138:         return AnnotatedType, (self.storage_type, self.annotation)",
      "",
      "[Added Lines]",
      "103: class UuidType(pa.ExtensionType):",
      "106:         super().__init__(pa.binary(16), 'pyarrow.tests.UuidType')",
      "111:     def __arrow_ext_serialize__(self):",
      "112:         return b''",
      "114:     @classmethod",
      "115:     def __arrow_ext_deserialize__(cls, storage_type, serialized):",
      "116:         return cls()",
      "119: class UuidType2(pa.ExtensionType):",
      "122:         super().__init__(pa.binary(16), 'pyarrow.tests.UuidType2')",
      "124:     def __arrow_ext_serialize__(self):",
      "125:         return b''",
      "127:     @classmethod",
      "128:     def __arrow_ext_deserialize__(cls, storage_type, serialized):",
      "129:         return cls()",
      "132: class LabelType(pa.ExtensionType):",
      "135:         super().__init__(pa.string(), 'pyarrow.tests.LabelType')",
      "137:     def __arrow_ext_serialize__(self):",
      "138:         return b''",
      "140:     @classmethod",
      "141:     def __arrow_ext_deserialize__(cls, storage_type, serialized):",
      "142:         return cls()",
      "145: class ParamExtType(pa.ExtensionType):",
      "149:         super().__init__(pa.binary(width), 'pyarrow.tests.ParamExtType')",
      "155:     def __arrow_ext_serialize__(self):",
      "156:         return str(self._width).encode()",
      "158:     @classmethod",
      "159:     def __arrow_ext_deserialize__(cls, storage_type, serialized):",
      "160:         width = int(serialized.decode())",
      "161:         assert storage_type == pa.binary(width)",
      "162:         return cls(width)",
      "165: class MyStructType(pa.ExtensionType):",
      "170:         super().__init__(self.storage_type, 'pyarrow.tests.MyStructType')",
      "172:     def __arrow_ext_serialize__(self):",
      "173:         return b''",
      "175:     @classmethod",
      "176:     def __arrow_ext_deserialize__(cls, storage_type, serialized):",
      "177:         assert serialized == b''",
      "178:         assert storage_type == cls.storage_type",
      "179:         return cls()",
      "182: class MyListType(pa.ExtensionType):",
      "185:         assert isinstance(storage_type, pa.ListType)",
      "186:         super().__init__(storage_type, 'pyarrow.tests.MyListType')",
      "188:     def __arrow_ext_serialize__(self):",
      "189:         return b''",
      "191:     @classmethod",
      "192:     def __arrow_ext_deserialize__(cls, storage_type, serialized):",
      "193:         assert serialized == b''",
      "194:         return cls(storage_type)",
      "197: class AnnotatedType(pa.ExtensionType):",
      "204:         super().__init__(storage_type, 'pyarrow.tests.AnnotatedType')",
      "206:     def __arrow_ext_serialize__(self):",
      "207:         return b''",
      "209:     @classmethod",
      "210:     def __arrow_ext_deserialize__(cls, storage_type, serialized):",
      "211:         assert serialized == b''",
      "212:         return cls(storage_type)",
      "215: class LegacyIntType(pa.PyExtensionType):",
      "217:     def __init__(self):",
      "218:         pa.PyExtensionType.__init__(self, pa.int8())",
      "221:         return LegacyIntType, ()",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "154: def test_ext_type_basics():",
      "155:     ty = UuidType()",
      "159: def test_ext_type_str():",
      "160:     ty = IntegerType()",
      "162:     assert str(ty) == expected",
      "163:     assert pa.DataType.__str__(ty) == expected",
      "",
      "[Removed Lines]",
      "156:     assert ty.extension_name == \"arrow.py_extension_type\"",
      "161:     expected = \"extension<arrow.py_extension_type<IntegerType>>\"",
      "",
      "[Added Lines]",
      "239:     assert ty.extension_name == \"pyarrow.tests.UuidType\"",
      "244:     expected = \"extension<pyarrow.tests.IntegerType<IntegerType>>\"",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "223:         del ty",
      "224:         ty = pickle_module.loads(ser)",
      "225:         wr = weakref.ref(ty)",
      "227:         del ty",
      "228:         assert wr() is None",
      "",
      "[Removed Lines]",
      "226:         assert ty.extension_name == \"arrow.py_extension_type\"",
      "",
      "[Added Lines]",
      "309:         assert ty.extension_name == \"pyarrow.tests.UuidType\"",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "571:     assert tiny_int_arr.type == TinyIntType()",
      "573:     # Casting between extension types w/ different storage types not okay.",
      "575:            \"to different extension type \"",
      "577:            \"One can first cast to the storage type, \"",
      "578:            \"then to the extension type.\"",
      "579:            )",
      "",
      "[Removed Lines]",
      "574:     msg = (\"Casting from 'extension<arrow.py_extension_type<TinyIntType>>' \"",
      "576:            \"'extension<arrow.py_extension_type<IntegerType>>' not permitted. \"",
      "",
      "[Added Lines]",
      "657:     msg = (\"Casting from 'extension<.*?<TinyIntType>>' \"",
      "659:            \"'extension<.*?<IntegerType>>' not permitted. \"",
      "",
      "---------------",
      "--- Hunk 7 ---",
      "[Context before]",
      "660:     return pa.RecordBatch.from_arrays([arr], [\"exts\"])",
      "664:     arr = batch.column(0)",
      "668:     return arr",
      "672:     batch = example_batch()",
      "673:     buf = ipc_write_batch(batch)",
      "674:     del batch",
      "676:     batch = ipc_read_batch(buf)",
      "692:         batch = ipc_read_batch(buf)",
      "712: class PeriodArray(pa.ExtensionArray):",
      "",
      "[Removed Lines]",
      "663: def check_example_batch(batch):",
      "665:     assert isinstance(arr, pa.ExtensionArray)",
      "666:     assert arr.type.storage_type == pa.binary(3)",
      "667:     assert arr.storage.to_pylist() == [b\"foo\", b\"bar\"]",
      "671: def test_ipc():",
      "677:     arr = check_example_batch(batch)",
      "678:     assert arr.type == ParamExtType(3)",
      "681: def test_ipc_unknown_type():",
      "682:     batch = example_batch()",
      "683:     buf = ipc_write_batch(batch)",
      "684:     del batch",
      "686:     orig_type = ParamExtType",
      "687:     try:",
      "688:         # Simulate the original Python type being unavailable.",
      "689:         # Deserialization should not fail but return a placeholder type.",
      "690:         del globals()['ParamExtType']",
      "693:         arr = check_example_batch(batch)",
      "694:         assert isinstance(arr.type, pa.UnknownExtensionType)",
      "696:         # Can be serialized again",
      "697:         buf2 = ipc_write_batch(batch)",
      "698:         del batch, arr",
      "700:         batch = ipc_read_batch(buf2)",
      "701:         arr = check_example_batch(batch)",
      "702:         assert isinstance(arr.type, pa.UnknownExtensionType)",
      "703:     finally:",
      "704:         globals()['ParamExtType'] = orig_type",
      "706:     # Deserialize again with the type restored",
      "707:     batch = ipc_read_batch(buf2)",
      "708:     arr = check_example_batch(batch)",
      "709:     assert arr.type == ParamExtType(3)",
      "",
      "[Added Lines]",
      "746: def check_example_batch(batch, *, expect_extension):",
      "748:     if expect_extension:",
      "749:         assert isinstance(arr, pa.ExtensionArray)",
      "750:         assert arr.type.storage_type == pa.binary(3)",
      "751:         assert arr.storage.to_pylist() == [b\"foo\", b\"bar\"]",
      "752:     else:",
      "753:         assert arr.type == pa.binary(3)",
      "754:         assert arr.to_pylist() == [b\"foo\", b\"bar\"]",
      "758: def test_ipc_unregistered():",
      "764:     batch.validate(full=True)",
      "765:     check_example_batch(batch, expect_extension=False)",
      "768: def test_ipc_registered():",
      "769:     with registered_extension_type(ParamExtType(1)):",
      "770:         batch = example_batch()",
      "771:         buf = ipc_write_batch(batch)",
      "772:         del batch",
      "775:         batch.validate(full=True)",
      "776:         arr = check_example_batch(batch, expect_extension=True)",
      "777:         assert arr.type == ParamExtType(3)",
      "",
      "---------------",
      "--- Hunk 8 ---",
      "[Context before]",
      "931:     # When reading in, properly create extension type if it is registered",
      "932:     result = pq.read_table(filename)",
      "933:     assert result.schema.field(\"ext\").type == period_type",
      "934:     assert result.schema.field(\"ext\").metadata == {}",
      "935:     # Get the exact array class defined by the registered type.",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1001:     result.validate(full=True)",
      "",
      "---------------",
      "--- Hunk 9 ---",
      "[Context before]",
      "939:     # When the type is not registered, read in as storage type",
      "940:     pa.unregister_extension_type(period_type.extension_name)",
      "941:     result = pq.read_table(filename)",
      "942:     assert result.schema.field(\"ext\").type == pa.int64()",
      "943:     # The extension metadata is present for roundtripping.",
      "944:     assert result.schema.field(\"ext\").metadata == {",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1011:     result.validate(full=True)",
      "",
      "---------------",
      "--- Hunk 10 ---",
      "[Context before]",
      "967:     filename = tmpdir / 'nested_extension_storage.parquet'",
      "968:     pq.write_table(orig_table, filename)",
      "970:     table = pq.read_table(filename)",
      "979: @pytest.mark.parquet",
      "",
      "[Removed Lines]",
      "971:     assert table.column('structs').type == mystruct_array.type",
      "972:     assert table.column('lists').type == mylist_array.type",
      "973:     assert table == orig_table",
      "975:     with pytest.raises(pa.ArrowInvalid, match='without all of its fields'):",
      "976:         pq.ParquetFile(filename).read(columns=['structs.left'])",
      "",
      "[Added Lines]",
      "1040:     # Unregistered",
      "1042:     table.validate(full=True)",
      "1043:     assert table.column('structs').type == struct_array.type",
      "1044:     assert table.column('structs').combine_chunks() == struct_array",
      "1045:     assert table.column('lists').type == list_array.type",
      "1046:     assert table.column('lists').combine_chunks() == list_array",
      "1048:     # Registered",
      "1049:     with registered_extension_type(mystruct_array.type):",
      "1050:         with registered_extension_type(mylist_array.type):",
      "1051:             table = pq.read_table(filename)",
      "1052:             table.validate(full=True)",
      "1053:             assert table.column('structs').type == mystruct_array.type",
      "1054:             assert table.column('lists').type == mylist_array.type",
      "1055:             assert table == orig_table",
      "1057:             # Cannot select a subfield of an extension type with",
      "1058:             # a struct storage type.",
      "1059:             with pytest.raises(pa.ArrowInvalid,",
      "1060:                                match='without all of its fields'):",
      "1061:                 pq.ParquetFile(filename).read(columns=['structs.left'])",
      "",
      "---------------",
      "--- Hunk 11 ---",
      "[Context before]",
      "995:     pq.write_table(orig_table, filename)",
      "997:     table = pq.read_table(filename)",
      "1001:     # List of extensions",
      "1002:     list_array = pa.ListArray.from_arrays([0, 1, None, 3], ext_array)",
      "",
      "[Removed Lines]",
      "998:     assert table.column(0).type == struct_array.type",
      "999:     assert table == orig_table",
      "",
      "[Added Lines]",
      "1083:     table.validate(full=True)",
      "1084:     assert table.column(0).type == pa.struct({'ints': pa.int64(),",
      "1085:                                               'exts': pa.int64()})",
      "1086:     with registered_extension_type(ext_type):",
      "1087:         table = pq.read_table(filename)",
      "1088:         table.validate(full=True)",
      "1089:         assert table.column(0).type == struct_array.type",
      "1090:         assert table == orig_table",
      "",
      "---------------",
      "--- Hunk 12 ---",
      "[Context before]",
      "1006:     pq.write_table(orig_table, filename)",
      "1008:     table = pq.read_table(filename)",
      "1012:     # Large list of extensions",
      "1013:     list_array = pa.LargeListArray.from_arrays([0, 1, None, 3], ext_array)",
      "",
      "[Removed Lines]",
      "1009:     assert table.column(0).type == list_array.type",
      "1010:     assert table == orig_table",
      "",
      "[Added Lines]",
      "1100:     table.validate(full=True)",
      "1101:     assert table.column(0).type == pa.list_(pa.int64())",
      "1102:     with registered_extension_type(ext_type):",
      "1103:         table = pq.read_table(filename)",
      "1104:         table.validate(full=True)",
      "1105:         assert table.column(0).type == list_array.type",
      "1106:         assert table == orig_table",
      "",
      "---------------",
      "--- Hunk 13 ---",
      "[Context before]",
      "1017:     pq.write_table(orig_table, filename)",
      "1019:     table = pq.read_table(filename)",
      "1024: @pytest.mark.parquet",
      "",
      "[Removed Lines]",
      "1020:     assert table.column(0).type == list_array.type",
      "1021:     assert table == orig_table",
      "",
      "[Added Lines]",
      "1116:     table.validate(full=True)",
      "1117:     assert table.column(0).type == pa.large_list(pa.int64())",
      "1118:     with registered_extension_type(ext_type):",
      "1119:         table = pq.read_table(filename)",
      "1120:         table.validate(full=True)",
      "1121:         assert table.column(0).type == list_array.type",
      "1122:         assert table == orig_table",
      "",
      "---------------",
      "--- Hunk 14 ---",
      "[Context before]",
      "1040:     pq.write_table(orig_table, filename)",
      "1042:     table = pq.read_table(filename)",
      "1047: def test_to_numpy():",
      "",
      "[Removed Lines]",
      "1043:     assert table.column(0).type == mylist_array.type",
      "1044:     assert table == orig_table",
      "",
      "[Added Lines]",
      "1144:     assert table.column(0).type == pa.list_(pa.int64())",
      "1145:     with registered_extension_type(mylist_array.type):",
      "1146:         with registered_extension_type(inner_ext_array.type):",
      "1147:             table = pq.read_table(filename)",
      "1148:             assert table.column(0).type == mylist_array.type",
      "1149:             assert table == orig_table",
      "",
      "---------------",
      "--- Hunk 15 ---",
      "[Context before]",
      "1370: def test_tensor_type_str(tensor_type, text):",
      "1371:     tensor_type_str = tensor_type.__str__()",
      "1372:     assert text in tensor_type_str",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1480: def test_legacy_int_type():",
      "1481:     with pytest.warns(FutureWarning, match=\"PyExtensionType is deprecated\"):",
      "1482:         ext_ty = LegacyIntType()",
      "1483:     arr = pa.array([1, 2, 3], type=ext_ty.storage_type)",
      "1484:     ext_arr = pa.ExtensionArray.from_storage(ext_ty, arr)",
      "1485:     batch = pa.RecordBatch.from_arrays([ext_arr], names=['ext'])",
      "1486:     buf = ipc_write_batch(batch)",
      "1488:     with pytest.warns(",
      "1489:             RuntimeWarning,",
      "1490:             match=\"pickle-based deserialization of pyarrow.PyExtensionType \"",
      "1491:                   \"subclasses is disabled by default\"):",
      "1492:         batch = ipc_read_batch(buf)",
      "1493:         assert isinstance(batch.column(0).type, pa.UnknownExtensionType)",
      "1495:     with enabled_auto_load():",
      "1496:         with pytest.warns(FutureWarning, match=\"PyExtensionType is deprecated\"):",
      "1497:             batch = ipc_read_batch(buf)",
      "1498:             assert isinstance(batch.column(0).type, LegacyIntType)",
      "1499:             assert batch.column(0) == ext_arr",
      "",
      "---------------"
    ],
    "python/pyarrow/tests/test_pandas.py||python/pyarrow/tests/test_pandas.py": [
      "File: python/pyarrow/tests/test_pandas.py -> python/pyarrow/tests/test_pandas.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "4096:     assert result.equals(expected2)",
      "4101:     def __init__(self):",
      "4108: def PandasArray__arrow_array__(self, type=None):",
      "",
      "[Removed Lines]",
      "4099: class DummyExtensionType(pa.PyExtensionType):",
      "4102:         pa.PyExtensionType.__init__(self, pa.int64())",
      "4104:     def __reduce__(self):",
      "4105:         return DummyExtensionType, ()",
      "",
      "[Added Lines]",
      "4099: class DummyExtensionType(pa.ExtensionType):",
      "4102:         super().__init__(pa.int64(),",
      "4103:                          'pyarrow.tests.test_pandas.DummyExtensionType')",
      "4105:     def __arrow_ext_serialize__(self):",
      "4106:         return b''",
      "4108:     @classmethod",
      "4109:     def __arrow_ext_deserialize__(cls, storage_type, serialized):",
      "4110:         assert serialized == b''",
      "4111:         assert storage_type == pa.int64()",
      "4112:         return cls()",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "4198:     assert not isinstance(_get_mgr(result).blocks[0], _int.ExtensionBlock)",
      "4203:     def __init__(self):",
      "4209:     def to_pandas_dtype(self):",
      "4210:         return pd.Int64Dtype()",
      "",
      "[Removed Lines]",
      "4201: class MyCustomIntegerType(pa.PyExtensionType):",
      "4204:         pa.PyExtensionType.__init__(self, pa.int64())",
      "4206:     def __reduce__(self):",
      "4207:         return MyCustomIntegerType, ()",
      "",
      "[Added Lines]",
      "4208: class MyCustomIntegerType(pa.ExtensionType):",
      "4211:         super().__init__(pa.int64(),",
      "4212:                          'pyarrow.tests.test_pandas.MyCustomIntegerType')",
      "4214:     def __arrow_ext_serialize__(self):",
      "4215:         return b''",
      "",
      "---------------"
    ],
    "python/pyarrow/types.pxi||python/pyarrow/types.pxi": [
      "File: python/pyarrow/types.pxi -> python/pyarrow/types.pxi",
      "--- Hunk 1 ---",
      "[Context before]",
      "1437:     Parameters",
      "1438:     ----------",
      "1439:     storage_type : DataType",
      "1440:     extension_name : str",
      "1442:     Examples",
      "1443:     --------",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1440:         The underlying storage type for the extension type.",
      "1442:         A unique name distinguishing this extension type. The name will be",
      "1443:         used when deserializing IPC data.",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "1671:                                     self.dim_names, self.permutation)",
      "1674: cdef class PyExtensionType(ExtensionType):",
      "1675:     \"\"\"",
      "1676:     Concrete base class for Python-defined extension types based on pickle",
      "1677:     for (de)serialization.",
      "1679:     Parameters",
      "1680:     ----------",
      "1681:     storage_type : DataType",
      "1682:         The storage type for which the extension is built.",
      "1728:     \"\"\"",
      "1730:     def __cinit__(self):",
      "",
      "[Removed Lines]",
      "1684:     Examples",
      "1685:     --------",
      "1686:     Define a UuidType extension type subclassing PyExtensionType:",
      "1688:     >>> import pyarrow as pa",
      "1689:     >>> class UuidType(pa.PyExtensionType):",
      "1690:     ...     def __init__(self):",
      "1691:     ...         pa.PyExtensionType.__init__(self, pa.binary(16))",
      "1692:     ...     def __reduce__(self):",
      "1693:     ...         return UuidType, ()",
      "1694:     ...",
      "1696:     Create an instance of UuidType extension type:",
      "1698:     >>> uuid_type = UuidType() # doctest: +SKIP",
      "1699:     >>> uuid_type # doctest: +SKIP",
      "1700:     UuidType(FixedSizeBinaryType(fixed_size_binary[16]))",
      "1702:     Inspect the extension type:",
      "1704:     >>> uuid_type.extension_name # doctest: +SKIP",
      "1705:     'arrow.py_extension_type'",
      "1706:     >>> uuid_type.storage_type # doctest: +SKIP",
      "1707:     FixedSizeBinaryType(fixed_size_binary[16])",
      "1709:     Wrap an array as an extension array:",
      "1711:     >>> import uuid",
      "1712:     >>> storage_array = pa.array([uuid.uuid4().bytes for _ in range(4)],",
      "1713:     ...                          pa.binary(16)) # doctest: +SKIP",
      "1714:     >>> uuid_type.wrap_array(storage_array) # doctest: +SKIP",
      "1715:     <pyarrow.lib.ExtensionArray object at ...>",
      "1716:     [",
      "1717:       ...",
      "1718:     ]",
      "1720:     Or do the same with creating an ExtensionArray:",
      "1722:     >>> pa.ExtensionArray.from_storage(uuid_type,",
      "1723:     ...                                storage_array) # doctest: +SKIP",
      "1724:     <pyarrow.lib.ExtensionArray object at ...>",
      "1725:     [",
      "1726:       ...",
      "1727:     ]",
      "",
      "[Added Lines]",
      "1677: _py_extension_type_auto_load = False",
      "1685:     .. warning::",
      "1686:        This class is deprecated and its deserialization is disabled by default.",
      "1687:        :class:`ExtensionType` is recommended instead.",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "1733:                             \"PyExtensionType\")",
      "1735:     def __init__(self, DataType storage_type):",
      "1736:         ExtensionType.__init__(self, storage_type, \"arrow.py_extension_type\")",
      "1738:     def __reduce__(self):",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1701:         warnings.warn(",
      "1702:             \"pyarrow.PyExtensionType is deprecated \"",
      "1703:             \"and will refuse deserialization by default. \"",
      "1704:             \"Instead, please derive from pyarrow.ExtensionType and implement \"",
      "1705:             \"your own serialization mechanism.\",",
      "1706:             FutureWarning)",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "1745:     @classmethod",
      "1746:     def __arrow_ext_deserialize__(cls, storage_type, serialized):",
      "1747:         try:",
      "1748:             ty = pickle.loads(serialized)",
      "1749:         except Exception:",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1718:         if not _py_extension_type_auto_load:",
      "1719:             warnings.warn(",
      "1720:                 \"pickle-based deserialization of pyarrow.PyExtensionType subclasses \"",
      "1721:                 \"is disabled by default; if you only ingest \"",
      "1722:                 \"trusted data files, you may re-enable this using \"",
      "1723:                 \"`pyarrow.PyExtensionType.set_auto_load(True)`.\\n\"",
      "1724:                 \"In the future, Python-defined extension subclasses should \"",
      "1725:                 \"derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) \"",
      "1726:                 \"and implement their own serialization mechanism.\\n\",",
      "1727:                 RuntimeWarning)",
      "1728:             return UnknownExtensionType(storage_type, serialized)",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "1759:                             .format(ty.storage_type, storage_type))",
      "1760:         return ty",
      "1763: cdef class UnknownExtensionType(PyExtensionType):",
      "1764:     \"\"\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "1744:     # XXX Cython marks extension types as immutable, so cannot expose this",
      "1745:     # as a writable class attribute.",
      "1746:     @classmethod",
      "1747:     def set_auto_load(cls, value):",
      "1748:         \"\"\"",
      "1749:         Enable or disable auto-loading of serialized PyExtensionType instances.",
      "1751:         Parameters",
      "1752:         ----------",
      "1753:         value : bool",
      "1754:             Whether to enable auto-loading.",
      "1755:         \"\"\"",
      "1756:         global _py_extension_type_auto_load",
      "1757:         assert isinstance(value, bool)",
      "1758:         _py_extension_type_auto_load = value",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "97821aa5af650e6478116cae7c0128fe37dad067",
      "candidate_info": {
        "commit_hash": "97821aa5af650e6478116cae7c0128fe37dad067",
        "repo": "apache/arrow",
        "commit_url": "https://github.com/apache/arrow/commit/97821aa5af650e6478116cae7c0128fe37dad067",
        "files": [
          "python/pyarrow/types.pxi"
        ],
        "message": "GH-33742: [Python] Address docstrings in Data Types classes  (#34380)\n\n### Rationale for this change\n\nEnsure docstrings for [Data Types Classes](https://arrow.apache.org/docs/python/api/datatypes.html#type-classes) have an Examples section.\n\n### What changes are included in this PR?\n\nDocstrings examples are added.\n\n### Are these changes tested?\n\nYes, with `python -m pytest --doctest-cython python/pyarrow `\n* Closes: #33742\n\nAuthored-by: Alenka Frim <frim.alenka@gmail.com>\nSigned-off-by: Ra\u00fal Cumplido <raulcumplido@gmail.com>",
        "before_after_code_files": [
          "python/pyarrow/types.pxi||python/pyarrow/types.pxi"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "python/pyarrow/types.pxi||python/pyarrow/types.pxi"
          ],
          "candidate": [
            "python/pyarrow/types.pxi||python/pyarrow/types.pxi"
          ]
        }
      },
      "candidate_diff": {
        "python/pyarrow/types.pxi||python/pyarrow/types.pxi": [
          "File: python/pyarrow/types.pxi -> python/pyarrow/types.pxi",
          "--- Hunk 1 ---",
          "[Context before]",
          "125:     Base class of all Arrow data types.",
          "127:     Each data type is an *instance* of this class.",
          "128:     \"\"\"",
          "130:     def __cinit__(self):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "129:     Examples",
          "130:     --------",
          "131:     Instance of int64 type:",
          "133:     >>> import pyarrow as pa",
          "134:     >>> pa.int64()",
          "135:     DataType(int64)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "154:     @property",
          "155:     def bit_width(self):",
          "156:         cdef _CFixedWidthTypePtr ty",
          "157:         ty = dynamic_cast[_CFixedWidthTypePtr](self.type)",
          "158:         if ty == nullptr:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "164:         \"\"\"",
          "165:         Bit width for fixed width type.",
          "167:         Examples",
          "168:         --------",
          "169:         >>> import pyarrow as pa",
          "170:         >>> pa.int64()",
          "171:         DataType(int64)",
          "172:         >>> pa.int64().bit_width",
          "173:         64",
          "174:         \"\"\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "163:     def num_fields(self):",
          "164:         \"\"\"",
          "165:         The number of child fields.",
          "166:         \"\"\"",
          "167:         return self.type.num_fields()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "186:         Examples",
          "187:         --------",
          "188:         >>> import pyarrow as pa",
          "189:         >>> pa.int64()",
          "190:         DataType(int64)",
          "191:         >>> pa.int64().num_fields",
          "192:         0",
          "193:         >>> pa.list_(pa.string())",
          "194:         ListType(list<item: string>)",
          "195:         >>> pa.list_(pa.string()).num_fields",
          "196:         1",
          "197:         >>> struct = pa.struct({'x': pa.int32(), 'y': pa.string()})",
          "198:         >>> struct.num_fields",
          "199:         2",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "171:         \"\"\"",
          "172:         Number of data buffers required to construct Array type",
          "173:         excluding children.",
          "174:         \"\"\"",
          "175:         return self.type.layout().buffers.size()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "209:         Examples",
          "210:         --------",
          "211:         >>> import pyarrow as pa",
          "212:         >>> pa.int64().num_buffers",
          "213:         2",
          "214:         >>> pa.string().num_buffers",
          "215:         3",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "205:         Returns",
          "206:         -------",
          "207:         is_equal : bool",
          "208:         \"\"\"",
          "209:         cdef:",
          "210:             DataType other_type",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "251:         Examples",
          "252:         --------",
          "253:         >>> import pyarrow as pa",
          "254:         >>> pa.int64().equals(pa.string())",
          "255:         False",
          "256:         >>> pa.int64().equals(pa.int64())",
          "257:         True",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "217:     def to_pandas_dtype(self):",
          "218:         \"\"\"",
          "219:         Return the equivalent NumPy / Pandas dtype.",
          "220:         \"\"\"",
          "221:         cdef Type type_id = self.type.id()",
          "222:         if type_id in _pandas_type_map:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "271:         Examples",
          "272:         --------",
          "273:         >>> import pyarrow as pa",
          "274:         >>> pa.int64().to_pandas_dtype()",
          "275:         <class 'numpy.int64'>",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "260: cdef class DictionaryType(DataType):",
          "261:     \"\"\"",
          "262:     Concrete class for dictionary data types.",
          "263:     \"\"\"",
          "265:     cdef void init(self, const shared_ptr[CDataType]& type) except *:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "320:     Examples",
          "321:     --------",
          "322:     Create an instance of dictionary type:",
          "324:     >>> import pyarrow as pa",
          "325:     >>> pa.dictionary(pa.int64(), pa.utf8())",
          "326:     DictionaryType(dictionary<values=string, indices=int64, ordered=0>)",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "274:         \"\"\"",
          "275:         Whether the dictionary is ordered, i.e. whether the ordering of values",
          "276:         in the dictionary is important.",
          "277:         \"\"\"",
          "278:         return self.dict_type.ordered()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "342:         Examples",
          "343:         --------",
          "344:         >>> import pyarrow as pa",
          "345:         >>> pa.dictionary(pa.int64(), pa.utf8()).ordered",
          "346:         False",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "281:     def index_type(self):",
          "282:         \"\"\"",
          "283:         The data type of dictionary indices (a signed integer type).",
          "284:         \"\"\"",
          "285:         return pyarrow_wrap_data_type(self.dict_type.index_type())",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "355:         Examples",
          "356:         --------",
          "357:         >>> import pyarrow as pa",
          "358:         >>> pa.dictionary(pa.int16(), pa.utf8()).index_type",
          "359:         DataType(int16)",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "290:         The dictionary value type.",
          "292:         The dictionary values are found in an instance of DictionaryArray.",
          "293:         \"\"\"",
          "294:         return pyarrow_wrap_data_type(self.dict_type.value_type())",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "370:         Examples",
          "371:         --------",
          "372:         >>> import pyarrow as pa",
          "373:         >>> pa.dictionary(pa.int16(), pa.utf8()).value_type",
          "374:         DataType(string)",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "297: cdef class ListType(DataType):",
          "298:     \"\"\"",
          "299:     Concrete class for list data types.",
          "300:     \"\"\"",
          "302:     cdef void init(self, const shared_ptr[CDataType]& type) except *:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "383:     Examples",
          "384:     --------",
          "385:     Create an instance of ListType:",
          "387:     >>> import pyarrow as pa",
          "388:     >>> pa.list_(pa.string())",
          "389:     ListType(list<item: string>)",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "309:     @property",
          "310:     def value_field(self):",
          "311:         return pyarrow_wrap_field(self.list_type.value_field())",
          "313:     @property",
          "314:     def value_type(self):",
          "315:         \"\"\"",
          "316:         The data type of list values.",
          "317:         \"\"\"",
          "318:         return pyarrow_wrap_data_type(self.list_type.value_type())",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "401:         \"\"\"",
          "402:         The field for list values.",
          "404:         Examples",
          "405:         --------",
          "406:         >>> import pyarrow as pa",
          "407:         >>> pa.list_(pa.string()).value_field",
          "408:         pyarrow.Field<item: string>",
          "409:         \"\"\"",
          "417:         Examples",
          "418:         --------",
          "419:         >>> import pyarrow as pa",
          "420:         >>> pa.list_(pa.string()).value_type",
          "421:         DataType(string)",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "322:     \"\"\"",
          "323:     Concrete class for large list data types",
          "324:     (like ListType, but with 64-bit offsets).",
          "325:     \"\"\"",
          "327:     cdef void init(self, const shared_ptr[CDataType]& type) except *:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "431:     Examples",
          "432:     --------",
          "433:     Create an instance of LargeListType:",
          "435:     >>> import pyarrow as pa",
          "436:     >>> pa.large_list(pa.string())",
          "437:     LargeListType(large_list<item: string>)",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "339:     def value_type(self):",
          "340:         \"\"\"",
          "341:         The data type of large list values.",
          "342:         \"\"\"",
          "343:         return pyarrow_wrap_data_type(self.list_type.value_type())",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "456:         Examples",
          "457:         --------",
          "458:         >>> import pyarrow as pa",
          "459:         >>> pa.large_list(pa.string()).value_type",
          "460:         DataType(string)",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "346: cdef class MapType(DataType):",
          "347:     \"\"\"",
          "348:     Concrete class for map data types.",
          "349:     \"\"\"",
          "351:     cdef void init(self, const shared_ptr[CDataType]& type) except *:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "469:     Examples",
          "470:     --------",
          "471:     Create an instance of MapType:",
          "473:     >>> import pyarrow as pa",
          "474:     >>> pa.map_(pa.string(), pa.int32())",
          "475:     MapType(map<string, int32>)",
          "476:     >>> pa.map_(pa.string(), pa.int32(), keys_sorted=True)",
          "477:     MapType(map<string, int32, keys_sorted>)",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "359:     def key_field(self):",
          "360:         \"\"\"",
          "361:         The field for keys in the map entries.",
          "362:         \"\"\"",
          "363:         return pyarrow_wrap_field(self.map_type.key_field())",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "492:         Examples",
          "493:         --------",
          "494:         >>> import pyarrow as pa",
          "495:         >>> pa.map_(pa.string(), pa.int32()).key_field",
          "496:         pyarrow.Field<key: string not null>",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "366:     def key_type(self):",
          "367:         \"\"\"",
          "368:         The data type of keys in the map entries.",
          "369:         \"\"\"",
          "370:         return pyarrow_wrap_data_type(self.map_type.key_type())",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "505:         Examples",
          "506:         --------",
          "507:         >>> import pyarrow as pa",
          "508:         >>> pa.map_(pa.string(), pa.int32()).key_type",
          "509:         DataType(string)",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "373:     def item_field(self):",
          "374:         \"\"\"",
          "375:         The field for items in the map entries.",
          "376:         \"\"\"",
          "377:         return pyarrow_wrap_field(self.map_type.item_field())",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "518:         Examples",
          "519:         --------",
          "520:         >>> import pyarrow as pa",
          "521:         >>> pa.map_(pa.string(), pa.int32()).item_field",
          "522:         pyarrow.Field<value: int32>",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "380:     def item_type(self):",
          "381:         \"\"\"",
          "382:         The data type of items in the map entries.",
          "383:         \"\"\"",
          "384:         return pyarrow_wrap_data_type(self.map_type.item_type())",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "531:         Examples",
          "532:         --------",
          "533:         >>> import pyarrow as pa",
          "534:         >>> pa.map_(pa.string(), pa.int32()).item_type",
          "535:         DataType(int32)",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "387: cdef class FixedSizeListType(DataType):",
          "388:     \"\"\"",
          "389:     Concrete class for fixed size list data types.",
          "390:     \"\"\"",
          "392:     cdef void init(self, const shared_ptr[CDataType]& type) except *:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "544:     Examples",
          "545:     --------",
          "546:     Create an instance of FixedSizeListType:",
          "548:     >>> import pyarrow as pa",
          "549:     >>> pa.list_(pa.int32(), 2)",
          "550:     FixedSizeListType(fixed_size_list<item: int32>[2])",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "399:     @property",
          "400:     def value_field(self):",
          "401:         return pyarrow_wrap_field(self.list_type.value_field())",
          "403:     @property",
          "404:     def value_type(self):",
          "405:         \"\"\"",
          "406:         The data type of large list values.",
          "407:         \"\"\"",
          "408:         return pyarrow_wrap_data_type(self.list_type.value_type())",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "562:         \"\"\"",
          "563:         The field for list values.",
          "565:         Examples",
          "566:         --------",
          "567:         >>> import pyarrow as pa",
          "568:         >>> pa.list_(pa.int32(), 2).value_field",
          "569:         pyarrow.Field<item: int32>",
          "570:         \"\"\"",
          "578:         Examples",
          "579:         --------",
          "580:         >>> import pyarrow as pa",
          "581:         >>> pa.list_(pa.int32(), 2).value_type",
          "582:         DataType(int32)",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "411:     def list_size(self):",
          "412:         \"\"\"",
          "413:         The size of the fixed size lists.",
          "414:         \"\"\"",
          "415:         return self.list_type.list_size()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "591:         Examples",
          "592:         --------",
          "593:         >>> import pyarrow as pa",
          "594:         >>> pa.list_(pa.int32(), 2).list_size",
          "595:         2",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "499:             The index of the field with the given name; -1 if the",
          "500:             name isn't found or there are several fields with the given",
          "501:             name.",
          "502:         \"\"\"",
          "503:         return self.struct_type.GetFieldIndex(tobytes(name))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "685:         Examples",
          "686:         --------",
          "687:         >>> import pyarrow as pa",
          "688:         >>> struct_type = pa.struct({'x': pa.int32(), 'y': pa.string()})",
          "690:         Index of the field with a name 'y':",
          "692:         >>> struct_type.get_field_index('y')",
          "693:         1",
          "695:         Index of the field that does not exist:",
          "697:         >>> struct_type.get_field_index('z')",
          "698:         -1",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "549:         Returns",
          "550:         -------",
          "551:         indices : List[int]",
          "552:         \"\"\"",
          "553:         return self.struct_type.GetAllFieldIndices(tobytes(name))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "750:         Examples",
          "751:         --------",
          "752:         >>> import pyarrow as pa",
          "753:         >>> struct_type = pa.struct({'x': pa.int32(), 'y': pa.string()})",
          "754:         >>> struct_type.get_all_field_indices('x')",
          "755:         [0]",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "580: cdef class UnionType(DataType):",
          "581:     \"\"\"",
          "582:     Base class for union data types.",
          "583:     \"\"\"",
          "585:     cdef void init(self, const shared_ptr[CDataType]& type) except *:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "788:     Examples",
          "789:     --------",
          "790:     Create an instance of a dense UnionType using ``pa.union``:",
          "792:     >>> import pyarrow as pa",
          "793:     >>> pa.union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())],",
          "794:     ...          mode=pa.lib.UnionMode_DENSE),",
          "795:     (DenseUnionType(dense_union<a: fixed_size_binary[10]=0, b: string=1>),)",
          "797:     Create an instance of a dense UnionType using ``pa.dense_union``:",
          "799:     >>> pa.dense_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])",
          "800:     DenseUnionType(dense_union<a: fixed_size_binary[10]=0, b: string=1>)",
          "802:     Create an instance of a sparse UnionType using ``pa.union``:",
          "804:     >>> pa.union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())],",
          "805:     ...          mode=pa.lib.UnionMode_SPARSE),",
          "806:     (SparseUnionType(sparse_union<a: fixed_size_binary[10]=0, b: string=1>),)",
          "808:     Create an instance of a sparse UnionType using ``pa.sparse_union``:",
          "810:     >>> pa.sparse_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])",
          "811:     SparseUnionType(sparse_union<a: fixed_size_binary[10]=0, b: string=1>)",
          "",
          "---------------",
          "--- Hunk 26 ---",
          "[Context before]",
          "589:     def mode(self):",
          "590:         \"\"\"",
          "591:         The mode of the union (\"dense\" or \"sparse\").",
          "592:         \"\"\"",
          "593:         cdef CUnionType* type = <CUnionType*> self.sp_type.get()",
          "594:         cdef int mode = type.mode()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "822:         Examples",
          "823:         --------",
          "824:         >>> import pyarrow as pa",
          "825:         >>> union = pa.sparse_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])",
          "826:         >>> union.mode",
          "827:         'sparse'",
          "",
          "---------------",
          "--- Hunk 27 ---",
          "[Context before]",
          "602:     def type_codes(self):",
          "603:         \"\"\"",
          "604:         The type code to indicate each data type in this union.",
          "605:         \"\"\"",
          "606:         cdef CUnionType* type = <CUnionType*> self.sp_type.get()",
          "607:         return type.type_codes()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "842:         Examples",
          "843:         --------",
          "844:         >>> import pyarrow as pa",
          "845:         >>> union = pa.sparse_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])",
          "846:         >>> union.type_codes",
          "847:         [0, 1]",
          "",
          "---------------",
          "--- Hunk 28 ---",
          "[Context before]",
          "630:         Returns",
          "631:         -------",
          "632:         pyarrow.Field",
          "633:         \"\"\"",
          "634:         if isinstance(i, int):",
          "635:             return DataType.field(self, i)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "877:         Examples",
          "878:         --------",
          "879:         >>> import pyarrow as pa",
          "880:         >>> union = pa.sparse_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])",
          "881:         >>> union[0]",
          "882:         pyarrow.Field<a: fixed_size_binary[10]>",
          "",
          "---------------",
          "--- Hunk 29 ---",
          "[Context before]",
          "651: cdef class SparseUnionType(UnionType):",
          "652:     \"\"\"",
          "653:     Concrete class for sparse union types.",
          "654:     \"\"\"",
          "657: cdef class DenseUnionType(UnionType):",
          "658:     \"\"\"",
          "659:     Concrete class for dense union types.",
          "660:     \"\"\"",
          "663: cdef class TimestampType(DataType):",
          "664:     \"\"\"",
          "665:     Concrete class for timestamp data types.",
          "666:     \"\"\"",
          "668:     cdef void init(self, const shared_ptr[CDataType]& type) except *:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "905:     Examples",
          "906:     --------",
          "907:     Create an instance of a sparse UnionType using ``pa.union``:",
          "909:     >>> import pyarrow as pa",
          "910:     >>> pa.union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())],",
          "911:     ...          mode=pa.lib.UnionMode_SPARSE),",
          "912:     (SparseUnionType(sparse_union<a: fixed_size_binary[10]=0, b: string=1>),)",
          "914:     Create an instance of a sparse UnionType using ``pa.sparse_union``:",
          "916:     >>> pa.sparse_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])",
          "917:     SparseUnionType(sparse_union<a: fixed_size_binary[10]=0, b: string=1>)",
          "925:     Examples",
          "926:     --------",
          "927:     Create an instance of a dense UnionType using ``pa.union``:",
          "929:     >>> import pyarrow as pa",
          "930:     >>> pa.union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())],",
          "931:     ...          mode=pa.lib.UnionMode_DENSE),",
          "932:     (DenseUnionType(dense_union<a: fixed_size_binary[10]=0, b: string=1>),)",
          "934:     Create an instance of a dense UnionType using ``pa.dense_union``:",
          "936:     >>> pa.dense_union([pa.field('a', pa.binary(10)), pa.field('b', pa.string())])",
          "937:     DenseUnionType(dense_union<a: fixed_size_binary[10]=0, b: string=1>)",
          "945:     Examples",
          "946:     --------",
          "947:     >>> import pyarrow as pa",
          "949:     Create an instance of timestamp type:",
          "951:     >>> pa.timestamp('us')",
          "952:     TimestampType(timestamp[us])",
          "954:     Create an instance of timestamp type with timezone:",
          "956:     >>> pa.timestamp('s', tz='UTC')",
          "957:     TimestampType(timestamp[s, tz=UTC])",
          "",
          "---------------",
          "--- Hunk 30 ---",
          "[Context before]",
          "673:     def unit(self):",
          "674:         \"\"\"",
          "675:         The timestamp unit ('s', 'ms', 'us' or 'ns').",
          "676:         \"\"\"",
          "677:         return timeunit_to_string(self.ts_type.unit())",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "969:         Examples",
          "970:         --------",
          "971:         >>> import pyarrow as pa",
          "972:         >>> t = pa.timestamp('us')",
          "973:         >>> t.unit",
          "974:         'us'",
          "",
          "---------------",
          "--- Hunk 31 ---",
          "[Context before]",
          "680:     def tz(self):",
          "681:         \"\"\"",
          "682:         The timestamp time zone, if any, or None.",
          "683:         \"\"\"",
          "684:         if self.ts_type.timezone().size() > 0:",
          "685:             return frombytes(self.ts_type.timezone())",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "983:         Examples",
          "984:         --------",
          "985:         >>> import pyarrow as pa",
          "986:         >>> t = pa.timestamp('s', tz='UTC')",
          "987:         >>> t.tz",
          "988:         'UTC'",
          "",
          "---------------",
          "--- Hunk 32 ---",
          "[Context before]",
          "689:     def to_pandas_dtype(self):",
          "690:         \"\"\"",
          "691:         Return the equivalent NumPy / Pandas dtype.",
          "692:         \"\"\"",
          "693:         if self.tz is None:",
          "694:             return _pandas_type_map[_Type_TIMESTAMP]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "999:         Examples",
          "1000:         --------",
          "1001:         >>> import pyarrow as pa",
          "1002:         >>> t = pa.timestamp('s', tz='UTC')",
          "1003:         >>> t.to_pandas_dtype()",
          "1004:         datetime64[ns, UTC]",
          "",
          "---------------",
          "--- Hunk 33 ---",
          "[Context before]",
          "704: cdef class Time32Type(DataType):",
          "705:     \"\"\"",
          "706:     Concrete class for time32 data types.",
          "707:     \"\"\"",
          "709:     cdef void init(self, const shared_ptr[CDataType]& type) except *:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1021:     Examples",
          "1022:     --------",
          "1023:     Create an instance of time32 type:",
          "1025:     >>> import pyarrow as pa",
          "1026:     >>> pa.time32('ms')",
          "1027:     Time32Type(time32[ms])",
          "",
          "---------------",
          "--- Hunk 34 ---",
          "[Context before]",
          "714:     def unit(self):",
          "715:         \"\"\"",
          "716:         The time unit ('s', 'ms', 'us' or 'ns').",
          "717:         \"\"\"",
          "718:         return timeunit_to_string(self.time_type.unit())",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1039:         Examples",
          "1040:         --------",
          "1041:         >>> import pyarrow as pa",
          "1042:         >>> t = pa.time32('ms')",
          "1043:         >>> t.unit",
          "1044:         'ms'",
          "",
          "---------------",
          "--- Hunk 35 ---",
          "[Context before]",
          "721: cdef class Time64Type(DataType):",
          "722:     \"\"\"",
          "723:     Concrete class for time64 data types.",
          "724:     \"\"\"",
          "726:     cdef void init(self, const shared_ptr[CDataType]& type) except *:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1053:     Examples",
          "1054:     --------",
          "1055:     Create an instance of time64 type:",
          "1057:     >>> import pyarrow as pa",
          "1058:     >>> pa.time64('us')",
          "1059:     Time64Type(time64[us])",
          "",
          "---------------",
          "--- Hunk 36 ---",
          "[Context before]",
          "731:     def unit(self):",
          "732:         \"\"\"",
          "733:         The time unit ('s', 'ms', 'us' or 'ns').",
          "734:         \"\"\"",
          "735:         return timeunit_to_string(self.time_type.unit())",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1071:         Examples",
          "1072:         --------",
          "1073:         >>> import pyarrow as pa",
          "1074:         >>> t = pa.time64('us')",
          "1075:         >>> t.unit",
          "1076:         'us'",
          "",
          "---------------",
          "--- Hunk 37 ---",
          "[Context before]",
          "738: cdef class DurationType(DataType):",
          "739:     \"\"\"",
          "740:     Concrete class for duration data types.",
          "741:     \"\"\"",
          "743:     cdef void init(self, const shared_ptr[CDataType]& type) except *:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1085:     Examples",
          "1086:     --------",
          "1087:     Create an instance of duration type:",
          "1089:     >>> import pyarrow as pa",
          "1090:     >>> pa.duration('s')",
          "1091:     DurationType(duration[s])",
          "",
          "---------------",
          "--- Hunk 38 ---",
          "[Context before]",
          "748:     def unit(self):",
          "749:         \"\"\"",
          "750:         The duration unit ('s', 'ms', 'us' or 'ns').",
          "751:         \"\"\"",
          "752:         return timeunit_to_string(self.duration_type.unit())",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1103:         Examples",
          "1104:         --------",
          "1105:         >>> import pyarrow as pa",
          "1106:         >>> t = pa.duration('s')",
          "1107:         >>> t.unit",
          "1108:         's'",
          "",
          "---------------",
          "--- Hunk 39 ---",
          "[Context before]",
          "755: cdef class FixedSizeBinaryType(DataType):",
          "756:     \"\"\"",
          "757:     Concrete class for fixed-size binary data types.",
          "758:     \"\"\"",
          "760:     cdef void init(self, const shared_ptr[CDataType]& type) except *:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1117:     Examples",
          "1118:     --------",
          "1119:     Create an instance of fixed-size binary type:",
          "1121:     >>> import pyarrow as pa",
          "1122:     >>> pa.binary(3)",
          "1123:     FixedSizeBinaryType(fixed_size_binary[3])",
          "",
          "---------------",
          "--- Hunk 40 ---",
          "[Context before]",
          "769:     def byte_width(self):",
          "770:         \"\"\"",
          "771:         The binary size in bytes.",
          "772:         \"\"\"",
          "773:         return self.fixed_size_binary_type.byte_width()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1139:         Examples",
          "1140:         --------",
          "1141:         >>> import pyarrow as pa",
          "1142:         >>> t = pa.binary(3)",
          "1143:         >>> t.byte_width",
          "1144:         3",
          "",
          "---------------",
          "--- Hunk 41 ---",
          "[Context before]",
          "776: cdef class Decimal128Type(FixedSizeBinaryType):",
          "777:     \"\"\"",
          "778:     Concrete class for decimal128 data types.",
          "779:     \"\"\"",
          "781:     cdef void init(self, const shared_ptr[CDataType]& type) except *:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1153:     Examples",
          "1154:     --------",
          "1155:     Create an instance of decimal128 type:",
          "1157:     >>> import pyarrow as pa",
          "1158:     >>> pa.decimal128(5, 2)",
          "1159:     Decimal128Type(decimal128(5, 2))",
          "",
          "---------------",
          "--- Hunk 42 ---",
          "[Context before]",
          "789:     def precision(self):",
          "790:         \"\"\"",
          "791:         The decimal precision, in number of decimal digits (an integer).",
          "792:         \"\"\"",
          "793:         return self.decimal128_type.precision()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1174:         Examples",
          "1175:         --------",
          "1176:         >>> import pyarrow as pa",
          "1177:         >>> t = pa.decimal128(5, 2)",
          "1178:         >>> t.precision",
          "1179:         5",
          "",
          "---------------",
          "--- Hunk 43 ---",
          "[Context before]",
          "796:     def scale(self):",
          "797:         \"\"\"",
          "798:         The decimal scale (an integer).",
          "799:         \"\"\"",
          "800:         return self.decimal128_type.scale()",
          "803: cdef class Decimal256Type(FixedSizeBinaryType):",
          "804:     \"\"\"",
          "806:     \"\"\"",
          "808:     cdef void init(self, const shared_ptr[CDataType]& type) except *:",
          "",
          "[Removed Lines]",
          "805:     Concrete class for Decimal256 data types.",
          "",
          "[Added Lines]",
          "1188:         Examples",
          "1189:         --------",
          "1190:         >>> import pyarrow as pa",
          "1191:         >>> t = pa.decimal128(5, 2)",
          "1192:         >>> t.scale",
          "1193:         2",
          "1200:     Concrete class for decimal256 data types.",
          "1202:     Examples",
          "1203:     --------",
          "1204:     Create an instance of decimal256 type:",
          "1206:     >>> import pyarrow as pa",
          "1207:     >>> pa.decimal256(76, 38)",
          "1208:     Decimal256Type(decimal256(76, 38))",
          "",
          "---------------",
          "--- Hunk 44 ---",
          "[Context before]",
          "816:     def precision(self):",
          "817:         \"\"\"",
          "818:         The decimal precision, in number of decimal digits (an integer).",
          "819:         \"\"\"",
          "820:         return self.decimal256_type.precision()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1223:         Examples",
          "1224:         --------",
          "1225:         >>> import pyarrow as pa",
          "1226:         >>> t = pa.decimal256(76, 38)",
          "1227:         >>> t.precision",
          "1228:         76",
          "",
          "---------------",
          "--- Hunk 45 ---",
          "[Context before]",
          "823:     def scale(self):",
          "824:         \"\"\"",
          "825:         The decimal scale (an integer).",
          "826:         \"\"\"",
          "827:         return self.decimal256_type.scale()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1237:         Examples",
          "1238:         --------",
          "1239:         >>> import pyarrow as pa",
          "1240:         >>> t = pa.decimal256(76, 38)",
          "1241:         >>> t.scale",
          "1242:         38",
          "",
          "---------------",
          "--- Hunk 46 ---",
          "[Context before]",
          "910:     ----------",
          "911:     storage_type : DataType",
          "912:     extension_name : str",
          "913:     \"\"\"",
          "915:     def __cinit__(self):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1331:     Examples",
          "1332:     --------",
          "1333:     Define a UuidType extension type subclassing ExtensionType:",
          "1335:     >>> import pyarrow as pa",
          "1336:     >>> class UuidType(pa.ExtensionType):",
          "1337:     ...    def __init__(self):",
          "1338:     ...       pa.ExtensionType.__init__(self, pa.binary(16), \"my_package.uuid\")",
          "1339:     ...    def __arrow_ext_serialize__(self):",
          "1340:     ...       # since we don't have a parameterized type, we don't need extra",
          "1341:     ...       # metadata to be deserialized",
          "1342:     ...       return b''",
          "1343:     ...    @classmethod",
          "1344:     ...    def __arrow_ext_deserialize__(self, storage_type, serialized):",
          "1345:     ...       # return an instance of this subclass given the serialized",
          "1346:     ...       # metadata.",
          "1347:     ...       return UuidType()",
          "1348:     ...",
          "1350:     Register the extension type:",
          "1352:     >>> pa.register_extension_type(UuidType())",
          "1354:     Create an instance of UuidType extension type:",
          "1356:     >>> uuid_type = UuidType()",
          "1358:     Inspect the extension type:",
          "1360:     >>> uuid_type.extension_name",
          "1361:     'my_package.uuid'",
          "1362:     >>> uuid_type.storage_type",
          "1363:     FixedSizeBinaryType(fixed_size_binary[16])",
          "1365:     Wrap an array as an extension array:",
          "1367:     >>> import uuid",
          "1368:     >>> storage_array = pa.array([uuid.uuid4().bytes for _ in range(4)], pa.binary(16))",
          "1369:     >>> uuid_type.wrap_array(storage_array)",
          "1370:     <pyarrow.lib.ExtensionArray object at ...>",
          "1371:     [",
          "1372:       ...",
          "1373:     ]",
          "1375:     Or do the same with creating an ExtensionArray:",
          "1377:     >>> pa.ExtensionArray.from_storage(uuid_type, storage_array)",
          "1378:     <pyarrow.lib.ExtensionArray object at ...>",
          "1379:     [",
          "1380:       ...",
          "1381:     ]",
          "1383:     Unregister the extension type:",
          "1385:     >>> pa.unregister_extension_type(\"my_package.uuid\")",
          "",
          "---------------",
          "--- Hunk 47 ---",
          "[Context before]",
          "1009:     ----------",
          "1010:     storage_type : DataType",
          "1011:         The storage type for which the extension is built.",
          "1012:     \"\"\"",
          "1014:     def __cinit__(self):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1486:     Examples",
          "1487:     --------",
          "1488:     Define a UuidType extension type subclassing PyExtensionType:",
          "1490:     >>> import pyarrow as pa",
          "1491:     >>> class UuidType(pa.PyExtensionType):",
          "1492:     ...     def __init__(self):",
          "1493:     ...         pa.PyExtensionType.__init__(self, pa.binary(16))",
          "1494:     ...     def __reduce__(self):",
          "1495:     ...         return UuidType, ()",
          "1496:     ...",
          "1498:     Create an instance of UuidType extension type:",
          "1500:     >>> uuid_type = UuidType() # doctest: +SKIP",
          "1501:     >>> uuid_type # doctest: +SKIP",
          "1502:     UuidType(FixedSizeBinaryType(fixed_size_binary[16]))",
          "1504:     Inspect the extension type:",
          "1506:     >>> uuid_type.extension_name # doctest: +SKIP",
          "1507:     'arrow.py_extension_type'",
          "1508:     >>> uuid_type.storage_type # doctest: +SKIP",
          "1509:     FixedSizeBinaryType(fixed_size_binary[16])",
          "1511:     Wrap an array as an extension array:",
          "1513:     >>> import uuid",
          "1514:     >>> storage_array = pa.array([uuid.uuid4().bytes for _ in range(4)],",
          "1515:     ...                          pa.binary(16)) # doctest: +SKIP",
          "1516:     >>> uuid_type.wrap_array(storage_array) # doctest: +SKIP",
          "1517:     <pyarrow.lib.ExtensionArray object at ...>",
          "1518:     [",
          "1519:       ...",
          "1520:     ]",
          "1522:     Or do the same with creating an ExtensionArray:",
          "1524:     >>> pa.ExtensionArray.from_storage(uuid_type,",
          "1525:     ...                                storage_array) # doctest: +SKIP",
          "1526:     <pyarrow.lib.ExtensionArray object at ...>",
          "1527:     [",
          "1528:       ...",
          "1529:     ]",
          "",
          "---------------",
          "--- Hunk 48 ---",
          "[Context before]",
          "1085:     ext_type : BaseExtensionType instance",
          "1086:         The ExtensionType subclass to register.",
          "1088:     \"\"\"",
          "1089:     cdef:",
          "1090:         DataType _type = ensure_type(ext_type, allow_none=False)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1606:     Examples",
          "1607:     --------",
          "1608:     Define a UuidType extension type subclassing ExtensionType:",
          "1610:     >>> import pyarrow as pa",
          "1611:     >>> class UuidType(pa.ExtensionType):",
          "1612:     ...    def __init__(self):",
          "1613:     ...       pa.ExtensionType.__init__(self, pa.binary(16), \"my_package.uuid\")",
          "1614:     ...    def __arrow_ext_serialize__(self):",
          "1615:     ...       # since we don't have a parameterized type, we don't need extra",
          "1616:     ...       # metadata to be deserialized",
          "1617:     ...       return b''",
          "1618:     ...    @classmethod",
          "1619:     ...    def __arrow_ext_deserialize__(self, storage_type, serialized):",
          "1620:     ...       # return an instance of this subclass given the serialized",
          "1621:     ...       # metadata.",
          "1622:     ...       return UuidType()",
          "1623:     ...",
          "1625:     Register the extension type:",
          "1627:     >>> pa.register_extension_type(UuidType())",
          "1629:     Unregister the extension type:",
          "1631:     >>> pa.unregister_extension_type(\"my_package.uuid\")",
          "",
          "---------------",
          "--- Hunk 49 ---",
          "[Context before]",
          "1109:     type_name : str",
          "1110:         The name of the ExtensionType subclass to unregister.",
          "1112:     \"\"\"",
          "1113:     cdef:",
          "1114:         c_string c_type_name = tobytes(type_name)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1656:     Examples",
          "1657:     --------",
          "1658:     Define a UuidType extension type subclassing ExtensionType:",
          "1660:     >>> import pyarrow as pa",
          "1661:     >>> class UuidType(pa.ExtensionType):",
          "1662:     ...    def __init__(self):",
          "1663:     ...       pa.ExtensionType.__init__(self, pa.binary(16), \"my_package.uuid\")",
          "1664:     ...    def __arrow_ext_serialize__(self):",
          "1665:     ...       # since we don't have a parameterized type, we don't need extra",
          "1666:     ...       # metadata to be deserialized",
          "1667:     ...       return b''",
          "1668:     ...    @classmethod",
          "1669:     ...    def __arrow_ext_deserialize__(self, storage_type, serialized):",
          "1670:     ...       # return an instance of this subclass given the serialized",
          "1671:     ...       # metadata.",
          "1672:     ...       return UuidType()",
          "1673:     ...",
          "1675:     Register the extension type:",
          "1677:     >>> pa.register_extension_type(UuidType())",
          "1679:     Unregister the extension type:",
          "1681:     >>> pa.unregister_extension_type(\"my_package.uuid\")",
          "",
          "---------------",
          "--- Hunk 50 ---",
          "[Context before]",
          "1258:     Notes",
          "1259:     -----",
          "1260:     Do not use this class's constructor directly; use pyarrow.field",
          "1261:     \"\"\"",
          "1263:     def __cinit__(self):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1832:     Examples",
          "1833:     --------",
          "1834:     Create an instance of pyarrow.Field:",
          "1836:     >>> import pyarrow as pa",
          "1837:     >>> pa.field('key', pa.int32())",
          "1838:     pyarrow.Field<key: int32>",
          "1839:     >>> pa.field('key', pa.int32(), nullable=False)",
          "1840:     pyarrow.Field<key: int32 not null>",
          "1841:     >>> field = pa.field('key', pa.int32(),",
          "1842:     ...                  metadata={\"key\": \"Something important\"})",
          "1843:     >>> field",
          "1844:     pyarrow.Field<key: int32>",
          "1845:     >>> field.metadata",
          "1846:     {b'key': b'Something important'}",
          "1848:     Use the field to create a struct type:",
          "1850:     >>> pa.struct([field])",
          "1851:     StructType(struct<key: int32>)",
          "",
          "---------------",
          "--- Hunk 51 ---",
          "[Context before]",
          "1285:         Returns",
          "1286:         -------",
          "1287:         is_equal : bool",
          "1288:         \"\"\"",
          "1289:         return self.field.Equals(deref(other.field), check_metadata)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1880:         Examples",
          "1881:         --------",
          "1882:         >>> import pyarrow as pa",
          "1883:         >>> f1 = pa.field('key', pa.int32())",
          "1884:         >>> f2 = pa.field('key', pa.int32(), nullable=False)",
          "1885:         >>> f1.equals(f2)",
          "1886:         False",
          "1887:         >>> f1.equals(f1)",
          "1888:         True",
          "",
          "---------------",
          "--- Hunk 52 ---",
          "[Context before]",
          "1310:     @property",
          "1311:     def nullable(self):",
          "1312:         return self.field.nullable()",
          "1314:     @property",
          "1315:     def name(self):",
          "1316:         return frombytes(self.field.name())",
          "1318:     @property",
          "1319:     def metadata(self):",
          "1320:         wrapped = pyarrow_wrap_metadata(self.field.metadata())",
          "1321:         if wrapped is not None:",
          "1322:             return wrapped.to_dict()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1913:         \"\"\"",
          "1914:         The field nullability.",
          "1916:         Examples",
          "1917:         --------",
          "1918:         >>> import pyarrow as pa",
          "1919:         >>> f1 = pa.field('key', pa.int32())",
          "1920:         >>> f2 = pa.field('key', pa.int32(), nullable=False)",
          "1921:         >>> f1.nullable",
          "1922:         True",
          "1923:         >>> f2.nullable",
          "1924:         False",
          "1925:         \"\"\"",
          "1930:         \"\"\"",
          "1931:         The field name.",
          "1933:         Examples",
          "1934:         --------",
          "1935:         >>> import pyarrow as pa",
          "1936:         >>> field = pa.field('key', pa.int32())",
          "1937:         >>> field.name",
          "1938:         'key'",
          "1939:         \"\"\"",
          "1944:         \"\"\"",
          "1945:         The field metadata.",
          "1947:         Examples",
          "1948:         --------",
          "1949:         >>> import pyarrow as pa",
          "1950:         >>> field = pa.field('key', pa.int32(),",
          "1951:         ...                  metadata={\"key\": \"Something important\"})",
          "1952:         >>> field.metadata",
          "1953:         {b'key': b'Something important'}",
          "1954:         \"\"\"",
          "",
          "---------------",
          "--- Hunk 53 ---",
          "[Context before]",
          "1335:         Returns",
          "1336:         -------",
          "1337:         field : pyarrow.Field",
          "1338:         \"\"\"",
          "1339:         cdef shared_ptr[CField] c_field",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1974:         Examples",
          "1975:         --------",
          "1976:         >>> import pyarrow as pa",
          "1977:         >>> field = pa.field('key', pa.int32())",
          "1979:         Create new field by adding metadata to existing one:",
          "1981:         >>> field_new = field.with_metadata({\"key\": \"Something important\"})",
          "1982:         >>> field_new",
          "1983:         pyarrow.Field<key: int32>",
          "1984:         >>> field_new.metadata",
          "1985:         {b'key': b'Something important'}",
          "",
          "---------------",
          "--- Hunk 54 ---",
          "[Context before]",
          "1351:         Returns",
          "1352:         -------",
          "1353:         field : pyarrow.Field",
          "1354:         \"\"\"",
          "1355:         cdef shared_ptr[CField] new_field",
          "1356:         with nogil:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2003:         Examples",
          "2004:         --------",
          "2005:         >>> import pyarrow as pa",
          "2006:         >>> field = pa.field('key', pa.int32(),",
          "2007:         ...                  metadata={\"key\": \"Something important\"})",
          "2008:         >>> field.metadata",
          "2009:         {b'key': b'Something important'}",
          "2011:         Create new field by removing the metadata from the existing one:",
          "2013:         >>> field_new = field.remove_metadata()",
          "2014:         >>> field_new.metadata",
          "",
          "---------------",
          "--- Hunk 55 ---",
          "[Context before]",
          "1368:         Returns",
          "1369:         -------",
          "1370:         field : pyarrow.Field",
          "1371:         \"\"\"",
          "1372:         cdef:",
          "1373:             shared_ptr[CField] c_field",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2033:         Examples",
          "2034:         --------",
          "2035:         >>> import pyarrow as pa",
          "2036:         >>> field = pa.field('key', pa.int32())",
          "2037:         >>> field",
          "2038:         pyarrow.Field<key: int32>",
          "2040:         Create new field by replacing type of an existing one:",
          "2042:         >>> field_new = field.with_type(pa.int64())",
          "2043:         >>> field_new",
          "2044:         pyarrow.Field<key: int64>",
          "",
          "---------------",
          "--- Hunk 56 ---",
          "[Context before]",
          "1390:         Returns",
          "1391:         -------",
          "1392:         field : pyarrow.Field",
          "1393:         \"\"\"",
          "1394:         cdef:",
          "1395:             shared_ptr[CField] c_field",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2068:         Examples",
          "2069:         --------",
          "2070:         >>> import pyarrow as pa",
          "2071:         >>> field = pa.field('key', pa.int32())",
          "2072:         >>> field",
          "2073:         pyarrow.Field<key: int32>",
          "2075:         Create new field by replacing the name of an existing one:",
          "2077:         >>> field_new = field.with_name('lock')",
          "2078:         >>> field_new",
          "2079:         pyarrow.Field<lock: int32>",
          "",
          "---------------",
          "--- Hunk 57 ---",
          "[Context before]",
          "1409:         Returns",
          "1410:         -------",
          "1411:         field: pyarrow.Field",
          "1412:         \"\"\"",
          "1413:         cdef:",
          "1414:             shared_ptr[CField] field",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2100:         Examples",
          "2101:         --------",
          "2102:         >>> import pyarrow as pa",
          "2103:         >>> field = pa.field('key', pa.int32())",
          "2104:         >>> field",
          "2105:         pyarrow.Field<key: int32>",
          "2106:         >>> field.nullable",
          "2107:         True",
          "2109:         Create new field by replacing the nullability of an existing one:",
          "2111:         >>> field_new = field.with_nullable(False)",
          "2112:         >>> field_new",
          "2113:         pyarrow.Field<key: int32 not null>",
          "2114:         >>> field_new.nullable",
          "2115:         False",
          "",
          "---------------",
          "--- Hunk 58 ---",
          "[Context before]",
          "1428:         Returns",
          "1429:         -------",
          "1430:         fields : List[pyarrow.Field]",
          "1431:         \"\"\"",
          "1432:         cdef vector[shared_ptr[CField]] flattened",
          "1433:         with nogil:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2136:         Examples",
          "2137:         --------",
          "2138:         >>> import pyarrow as pa",
          "2139:         >>> f1 = pa.field('bar', pa.float64(), nullable=False)",
          "2140:         >>> f2 = pa.field('foo', pa.int32()).with_metadata({\"key\": \"Something important\"})",
          "2141:         >>> ff = pa.field('ff', pa.struct([f1, f2]), nullable=False)",
          "2143:         Flatten a struct field:",
          "2145:         >>> ff",
          "2146:         pyarrow.Field<ff: struct<bar: double not null, foo: int32> not null>",
          "2147:         >>> ff.flatten()",
          "2148:         [pyarrow.Field<ff.bar: double not null>, pyarrow.Field<ff.foo: int32>]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b679958fa892207373b445939c611320ba78a4e1",
      "candidate_info": {
        "commit_hash": "b679958fa892207373b445939c611320ba78a4e1",
        "repo": "apache/arrow",
        "commit_url": "https://github.com/apache/arrow/commit/b679958fa892207373b445939c611320ba78a4e1",
        "files": [
          "python/pyarrow/tests/test_feather.py",
          "python/pyarrow/tests/test_pandas.py"
        ],
        "message": "GH-36412: [Python] Fix deprecation warnings with latest pandas version (#36464)\n\n### What changes are included in this PR?\n\nFixes some of the deprecation warnings coming from the pandas development version, by updating our test code to avoid the deprecated patterns.\n\n* Issue: #36412\n* Closes: #36412\n\nAuthored-by: Joris Van den Bossche <jorisvandenbossche@gmail.com>\nSigned-off-by: Joris Van den Bossche <jorisvandenbossche@gmail.com>",
        "before_after_code_files": [
          "python/pyarrow/tests/test_feather.py||python/pyarrow/tests/test_feather.py",
          "python/pyarrow/tests/test_pandas.py||python/pyarrow/tests/test_pandas.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "python/pyarrow/tests/test_pandas.py||python/pyarrow/tests/test_pandas.py"
          ],
          "candidate": [
            "python/pyarrow/tests/test_pandas.py||python/pyarrow/tests/test_pandas.py"
          ]
        }
      },
      "candidate_diff": {
        "python/pyarrow/tests/test_feather.py||python/pyarrow/tests/test_feather.py": [
          "File: python/pyarrow/tests/test_feather.py -> python/pyarrow/tests/test_feather.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "395:     values = [b'foo', None, 'bar', 'qux', np.nan]",
          "396:     df = pd.DataFrame({'strings': values * repeats})",
          "399:     expected = pd.DataFrame({'strings': ex_values * repeats})",
          "400:     _check_pandas_roundtrip(df, expected, version=version)",
          "",
          "[Removed Lines]",
          "398:     ex_values = [b'foo', None, b'bar', b'qux', np.nan]",
          "",
          "[Added Lines]",
          "398:     ex_values = [b'foo', None, b'bar', b'qux', None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "408:     values = ['foo', None, 'bar', 'qux', np.nan]",
          "409:     df = pd.DataFrame({'strings': values * repeats})",
          "411:     _check_pandas_roundtrip(df, expected, version=version)",
          "",
          "[Removed Lines]",
          "410:     expected = pd.DataFrame({'strings': values * repeats})",
          "",
          "[Added Lines]",
          "410:     ex_values = ['foo', None, 'bar', 'qux', None]",
          "411:     expected = pd.DataFrame({'strings': ex_values * repeats})",
          "",
          "---------------"
        ],
        "python/pyarrow/tests/test_pandas.py||python/pyarrow/tests/test_pandas.py": [
          "File: python/pyarrow/tests/test_pandas.py -> python/pyarrow/tests/test_pandas.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "132:         if mask is None:",
          "133:             expected = pd.Series(values)",
          "134:         else:",
          "137:     tm.assert_series_equal(pd.Series(result), expected, check_names=False)",
          "",
          "[Removed Lines]",
          "135:             expected = pd.Series(np.ma.masked_array(values, mask=mask))",
          "",
          "[Added Lines]",
          "135:             expected = pd.Series(values).copy()",
          "136:             expected[mask.copy()] = None",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1564:         df = pd.DataFrame({'strings': values * repeats})",
          "1565:         field = pa.field('strings', pa.string())",
          "1566:         schema = pa.schema([field])",
          "1570:     def test_bytes_to_binary(self):",
          "1571:         values = ['qux', b'foo', None, bytearray(b'barz'), 'qux', np.nan]",
          "",
          "[Removed Lines]",
          "1568:         _check_pandas_roundtrip(df, expected_schema=schema)",
          "",
          "[Added Lines]",
          "1568:         ex_values = ['foo', None, 'bar', 'ma\u00f1ana', None]",
          "1569:         expected = pd.DataFrame({'strings': ex_values * repeats})",
          "1571:         _check_pandas_roundtrip(df, expected=expected, expected_schema=schema)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1574:         table = pa.Table.from_pandas(df)",
          "1575:         assert table[0].type == pa.binary()",
          "1578:         expected = pd.DataFrame({'strings': values2})",
          "1579:         _check_pandas_roundtrip(df, expected)",
          "",
          "[Removed Lines]",
          "1577:         values2 = [b'qux', b'foo', None, b'barz', b'qux', np.nan]",
          "",
          "[Added Lines]",
          "1580:         values2 = [b'qux', b'foo', None, b'barz', b'qux', None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "2069:     def test_infer_lists(self):",
          "2070:         data = OrderedDict([",
          "2072:             ('ints', [[0, 1], [2, 3]]),",
          "2073:             ('strs', [[None, 'b'], ['c', 'd']]),",
          "2074:             ('nested_strs', [[[None, 'b'], ['c', 'd']], None])",
          "",
          "[Removed Lines]",
          "2071:             ('nan_ints', [[None, 1], [2, 3]]),",
          "",
          "[Added Lines]",
          "2074:             ('nan_ints', [[np.nan, 1], [2, 3]]),",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "3562:     _check_to_pandas_memory_unchanged(t, split_blocks=True)",
          "3565: def _check_blocks_created(t, number):",
          "3566:     x = t.to_pandas(split_blocks=True)",
          "3570: def test_to_pandas_self_destruct():",
          "",
          "[Removed Lines]",
          "3567:     assert len(x._data.blocks) == number",
          "",
          "[Added Lines]",
          "3568: def _get_mgr(df):",
          "3569:     if Version(pd.__version__) < Version(\"1.1.0\"):",
          "3570:         return df._data",
          "3571:     else:",
          "3572:         return df._mgr",
          "3577:     assert len(_get_mgr(x).blocks) == number",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "4069:     # Int64Dtype is recognized -> convert to extension block by default",
          "4070:     # for a proper roundtrip",
          "4071:     result = table.to_pandas()",
          "4075:     tm.assert_frame_equal(result, df)",
          "4077:     # test with missing values",
          "4078:     df2 = pd.DataFrame({'a': pd.array([1, 2, None], dtype='Int64')})",
          "4079:     table2 = pa.table(df2)",
          "4080:     result = table2.to_pandas()",
          "4082:     tm.assert_frame_equal(result, df2)",
          "4084:     # monkeypatch pandas Int64Dtype to *not* have the protocol method",
          "",
          "[Removed Lines]",
          "4072:     assert not isinstance(result._data.blocks[0], _int.ExtensionBlock)",
          "4073:     assert result._data.blocks[0].values.dtype == np.dtype(\"int64\")",
          "4074:     assert isinstance(result._data.blocks[1], _int.ExtensionBlock)",
          "4081:     assert isinstance(result._data.blocks[0], _int.ExtensionBlock)",
          "",
          "[Added Lines]",
          "4082:     assert not isinstance(_get_mgr(result).blocks[0], _int.ExtensionBlock)",
          "4083:     assert _get_mgr(result).blocks[0].values.dtype == np.dtype(\"int64\")",
          "4084:     assert isinstance(_get_mgr(result).blocks[1], _int.ExtensionBlock)",
          "4091:     assert isinstance(_get_mgr(result).blocks[0], _int.ExtensionBlock)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "4090:             pd.core.arrays.integer.NumericDtype, \"__from_arrow__\")",
          "4091:     # Int64Dtype has no __from_arrow__ -> use normal conversion",
          "4092:     result = table.to_pandas()",
          "4097: class MyCustomIntegerType(pa.PyExtensionType):",
          "",
          "[Removed Lines]",
          "4093:     assert len(result._data.blocks) == 1",
          "4094:     assert not isinstance(result._data.blocks[0], _int.ExtensionBlock)",
          "",
          "[Added Lines]",
          "4103:     assert len(_get_mgr(result).blocks) == 1",
          "4104:     assert not isinstance(_get_mgr(result).blocks[0], _int.ExtensionBlock)",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "4117:     # extension type points to Int64Dtype, which knows how to create a",
          "4118:     # pandas ExtensionArray",
          "4119:     result = arr.to_pandas()",
          "4121:     expected = pd.Series([1, 2, 3, 4], dtype='Int64')",
          "4122:     tm.assert_series_equal(result, expected)",
          "4124:     result = table.to_pandas()",
          "4126:     expected = pd.DataFrame({'a': pd.array([1, 2, 3, 4], dtype='Int64')})",
          "4127:     tm.assert_frame_equal(result, expected)",
          "",
          "[Removed Lines]",
          "4120:     assert isinstance(result._data.blocks[0], _int.ExtensionBlock)",
          "4125:     assert isinstance(result._data.blocks[0], _int.ExtensionBlock)",
          "",
          "[Added Lines]",
          "4130:     assert isinstance(_get_mgr(result).blocks[0], _int.ExtensionBlock)",
          "4135:     assert isinstance(_get_mgr(result).blocks[0], _int.ExtensionBlock)",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "4136:             pd.core.arrays.integer.NumericDtype, \"__from_arrow__\")",
          "4138:     result = arr.to_pandas()",
          "4140:     expected = pd.Series([1, 2, 3, 4])",
          "4141:     tm.assert_series_equal(result, expected)",
          "",
          "[Removed Lines]",
          "4139:     assert not isinstance(result._data.blocks[0], _int.ExtensionBlock)",
          "",
          "[Added Lines]",
          "4149:     assert not isinstance(_get_mgr(result).blocks[0], _int.ExtensionBlock)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bd61239a32c94e37b9510071c0ffacad455798c0",
      "candidate_info": {
        "commit_hash": "bd61239a32c94e37b9510071c0ffacad455798c0",
        "repo": "apache/arrow",
        "commit_url": "https://github.com/apache/arrow/commit/bd61239a32c94e37b9510071c0ffacad455798c0",
        "files": [
          "docs/source/format/CDataInterface.rst",
          "docs/source/format/CDataInterface/PyCapsuleInterface.rst",
          "docs/source/python/extending_types.rst",
          "docs/source/python/interchange_protocol.rst",
          "python/pyarrow/array.pxi",
          "python/pyarrow/includes/libarrow.pxd",
          "python/pyarrow/ipc.pxi",
          "python/pyarrow/table.pxi",
          "python/pyarrow/tests/test_array.py",
          "python/pyarrow/tests/test_cffi.py",
          "python/pyarrow/tests/test_table.py",
          "python/pyarrow/tests/test_types.py",
          "python/pyarrow/types.pxi"
        ],
        "message": "GH-35531: [Python] C Data Interface PyCapsule Protocol (#37797)\n\n### Rationale for this change\n\n### What changes are included in this PR?\n\n* A new specification for Arrow PyCapsules and related dunder methods\n* Implementing the dunder methods for `DataType`, `Field`, `Schema`, `Array`, `RecordBatch`, `Table`, and `RecordBatchReader`.\n\n### Are these changes tested?\n\nYes, I've added various roundtrip tests for each of the types.\n\n### Are there any user-facing changes?\n\nThis introduces some new APIs and documents them.\n\n* Closes: #34031\n* Closes: #35531\n\nAuthored-by: Joris Van den Bossche <jorisvandenbossche@gmail.com>\nSigned-off-by: Antoine Pitrou <antoine@python.org>",
        "before_after_code_files": [
          "python/pyarrow/array.pxi||python/pyarrow/array.pxi",
          "python/pyarrow/includes/libarrow.pxd||python/pyarrow/includes/libarrow.pxd",
          "python/pyarrow/ipc.pxi||python/pyarrow/ipc.pxi",
          "python/pyarrow/table.pxi||python/pyarrow/table.pxi",
          "python/pyarrow/tests/test_array.py||python/pyarrow/tests/test_array.py",
          "python/pyarrow/tests/test_cffi.py||python/pyarrow/tests/test_cffi.py",
          "python/pyarrow/tests/test_table.py||python/pyarrow/tests/test_table.py",
          "python/pyarrow/tests/test_types.py||python/pyarrow/tests/test_types.py",
          "python/pyarrow/types.pxi||python/pyarrow/types.pxi"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "python/pyarrow/tests/test_cffi.py||python/pyarrow/tests/test_cffi.py",
            "python/pyarrow/types.pxi||python/pyarrow/types.pxi"
          ],
          "candidate": [
            "python/pyarrow/tests/test_cffi.py||python/pyarrow/tests/test_cffi.py",
            "python/pyarrow/types.pxi||python/pyarrow/types.pxi"
          ]
        }
      },
      "candidate_diff": {
        "python/pyarrow/array.pxi||python/pyarrow/array.pxi": [
          "File: python/pyarrow/array.pxi -> python/pyarrow/array.pxi",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "18: import os",
          "19: import warnings",
          "22: cdef _sequence_to_array(object sequence, object mask, object size,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: from cpython.pycapsule cimport PyCapsule_CheckExact, PyCapsule_GetPointer, PyCapsule_New",
          "22: from cython import sizeof",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "124:     Parameters",
          "125:     ----------",
          "127:         If both type and size are specified may be a single use iterable. If",
          "128:         not strongly-typed, Arrow type will be inferred for resulting array.",
          "129:     type : pyarrow.DataType",
          "130:         Explicit type to attempt to coerce to, otherwise will be inferred from",
          "131:         the data.",
          "",
          "[Removed Lines]",
          "126:     obj : sequence, iterable, ndarray or pandas.Series",
          "",
          "[Added Lines]",
          "129:     obj : sequence, iterable, ndarray, pandas.Series, Arrow-compatible array",
          "132:         Any Arrow-compatible array that implements the Arrow PyCapsule Protocol",
          "133:         (has an ``__arrow_c_array__`` method) can be passed as well.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "242:     if hasattr(obj, '__arrow_array__'):",
          "243:         return _handle_arrow_array_protocol(obj, type, mask, size)",
          "244:     elif _is_array_like(obj):",
          "245:         if mask is not None:",
          "246:             if _is_array_like(mask):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "249:     elif hasattr(obj, '__arrow_c_array__'):",
          "250:         if type is not None:",
          "251:             requested_type = type.__arrow_c_schema__()",
          "252:         else:",
          "253:             requested_type = None",
          "254:         schema_capsule, array_capsule = obj.__arrow_c_array__(requested_type)",
          "255:         out_array = Array._import_from_c_capsule(schema_capsule, array_capsule)",
          "256:         if type is not None and out_array.type != type:",
          "257:             # PyCapsule interface type coersion is best effort, so we need to",
          "258:             # check the type of the returned array and cast if necessary",
          "259:             out_array = array.cast(type, safe=safe, memory_pool=memory_pool)",
          "260:         return out_array",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1699:                                                      c_type))",
          "1700:         return pyarrow_wrap_array(c_array)",
          "1703: cdef _array_like_to_pandas(obj, options, types_mapper):",
          "1704:     cdef:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1719:     def __arrow_c_array__(self, requested_schema=None):",
          "1720:         \"\"\"",
          "1721:         Get a pair of PyCapsules containing a C ArrowArray representation of the object.",
          "1723:         Parameters",
          "1724:         ----------",
          "1725:         requested_schema : PyCapsule | None",
          "1726:             A PyCapsule containing a C ArrowSchema representation of a requested",
          "1727:             schema. PyArrow will attempt to cast the array to this data type.",
          "1728:             If None, the array will be returned as-is, with a type matching the",
          "1729:             one returned by :meth:`__arrow_c_schema__()`.",
          "1731:         Returns",
          "1732:         -------",
          "1733:         Tuple[PyCapsule, PyCapsule]",
          "1734:             A pair of PyCapsules containing a C ArrowSchema and ArrowArray,",
          "1735:             respectively.",
          "1736:         \"\"\"",
          "1737:         cdef:",
          "1738:             ArrowArray* c_array",
          "1739:             ArrowSchema* c_schema",
          "1740:             shared_ptr[CArray] inner_array",
          "1742:         if requested_schema is not None:",
          "1743:             target_type = DataType._import_from_c_capsule(requested_schema)",
          "1745:             if target_type != self.type:",
          "1746:                 try:",
          "1747:                     casted_array = _pc().cast(self, target_type, safe=True)",
          "1748:                     inner_array = pyarrow_unwrap_array(casted_array)",
          "1749:                 except ArrowInvalid as e:",
          "1750:                     raise ValueError(",
          "1751:                         f\"Could not cast {self.type} to requested type {target_type}: {e}\"",
          "1752:                     )",
          "1753:             else:",
          "1754:                 inner_array = self.sp_array",
          "1755:         else:",
          "1756:             inner_array = self.sp_array",
          "1758:         schema_capsule = alloc_c_schema(&c_schema)",
          "1759:         array_capsule = alloc_c_array(&c_array)",
          "1761:         with nogil:",
          "1762:             check_status(ExportArray(deref(inner_array), c_array, c_schema))",
          "1764:         return schema_capsule, array_capsule",
          "1766:     @staticmethod",
          "1767:     def _import_from_c_capsule(schema_capsule, array_capsule):",
          "1768:         cdef:",
          "1769:             ArrowSchema* c_schema",
          "1770:             ArrowArray* c_array",
          "1771:             shared_ptr[CArray] array",
          "1773:         c_schema = <ArrowSchema*> PyCapsule_GetPointer(schema_capsule, 'arrow_schema')",
          "1774:         c_array = <ArrowArray*> PyCapsule_GetPointer(array_capsule, 'arrow_array')",
          "1776:         with nogil:",
          "1777:             array = GetResultValue(ImportArray(c_array, c_schema))",
          "1779:         return pyarrow_wrap_array(array)",
          "",
          "---------------"
        ],
        "python/pyarrow/includes/libarrow.pxd||python/pyarrow/includes/libarrow.pxd": [
          "File: python/pyarrow/includes/libarrow.pxd -> python/pyarrow/includes/libarrow.pxd",
          "--- Hunk 1 ---",
          "[Context before]",
          "2744: cdef extern from \"arrow/c/abi.h\":",
          "2745:     cdef struct ArrowSchema:",
          "2748:     cdef struct ArrowArray:",
          "2751:     cdef struct ArrowArrayStream:",
          "2754: cdef extern from \"arrow/c/bridge.h\" namespace \"arrow\" nogil:",
          "2755:     CStatus ExportType(CDataType&, ArrowSchema* out)",
          "",
          "[Removed Lines]",
          "2746:         pass",
          "2749:         pass",
          "2752:         pass",
          "",
          "[Added Lines]",
          "2746:         void (*release)(ArrowSchema*) noexcept nogil",
          "2749:         void (*release)(ArrowArray*) noexcept nogil",
          "2752:         void (*release)(ArrowArrayStream*) noexcept nogil",
          "",
          "---------------"
        ],
        "python/pyarrow/ipc.pxi||python/pyarrow/ipc.pxi": [
          "File: python/pyarrow/ipc.pxi -> python/pyarrow/ipc.pxi",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "18: from collections import namedtuple",
          "19: import warnings",
          "22: cpdef enum MetadataVersion:",
          "23:     V1 = <char> CMetadataVersion_V1",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "18: from cpython.pycapsule cimport PyCapsule_CheckExact, PyCapsule_GetPointer, PyCapsule_New",
          "22: from cython import sizeof",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "815:         self.reader = c_reader",
          "816:         return self",
          "818:     @staticmethod",
          "819:     def from_batches(Schema schema not None, batches):",
          "820:         \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "817:         self.reader = c_reader",
          "818:         return self",
          "820:     def __arrow_c_stream__(self, requested_schema=None):",
          "821:         \"\"\"",
          "822:         Export to a C ArrowArrayStream PyCapsule.",
          "824:         Parameters",
          "825:         ----------",
          "826:         requested_schema: Schema, default None",
          "827:             The schema to which the stream should be casted. Currently, this is",
          "828:             not supported and will raise a NotImplementedError if the schema",
          "829:             doesn't match the current schema.",
          "831:         Returns",
          "832:         -------",
          "833:         PyCapsule",
          "834:             A capsule containing a C ArrowArrayStream struct.",
          "835:         \"\"\"",
          "836:         cdef:",
          "837:             ArrowArrayStream* c_stream",
          "839:         if requested_schema is not None:",
          "840:             out_schema = Schema._import_from_c_capsule(requested_schema)",
          "841:             # TODO: figure out a way to check if one schema is castable to",
          "842:             # another. Once we have that, we can perform validation here and",
          "843:             # if successful creating a wrapping reader that casts each batch.",
          "844:             if self.schema != out_schema:",
          "845:                 raise NotImplementedError(\"Casting to requested_schema\")",
          "847:         stream_capsule = alloc_c_stream(&c_stream)",
          "849:         with nogil:",
          "850:             check_status(ExportRecordBatchReader(self.reader, c_stream))",
          "852:         return stream_capsule",
          "854:     @staticmethod",
          "855:     def _import_from_c_capsule(stream):",
          "856:         \"\"\"",
          "857:         Import RecordBatchReader from a C ArrowArrayStream PyCapsule.",
          "859:         Parameters",
          "860:         ----------",
          "861:         stream: PyCapsule",
          "862:             A capsule containing a C ArrowArrayStream PyCapsule.",
          "864:         Returns",
          "865:         -------",
          "866:         RecordBatchReader",
          "867:         \"\"\"",
          "868:         cdef:",
          "869:             ArrowArrayStream* c_stream",
          "870:             shared_ptr[CRecordBatchReader] c_reader",
          "871:             RecordBatchReader self",
          "873:         c_stream = <ArrowArrayStream*>PyCapsule_GetPointer(",
          "874:             stream, 'arrow_array_stream'",
          "875:         )",
          "877:         with nogil:",
          "878:             c_reader = GetResultValue(ImportRecordBatchReader(c_stream))",
          "880:         self = RecordBatchReader.__new__(RecordBatchReader)",
          "",
          "---------------"
        ],
        "python/pyarrow/table.pxi||python/pyarrow/table.pxi": [
          "File: python/pyarrow/table.pxi -> python/pyarrow/table.pxi",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "21: cdef class ChunkedArray(_PandasConvertible):",
          "22:     \"\"\"",
          "",
          "[Removed Lines]",
          "18: import warnings",
          "",
          "[Added Lines]",
          "18: from cpython.pycapsule cimport PyCapsule_CheckExact, PyCapsule_GetPointer, PyCapsule_New",
          "20: import warnings",
          "21: from cython import sizeof",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2983:                     <ArrowArray*> c_ptr, c_schema))",
          "2984:         return pyarrow_wrap_batch(c_batch)",
          "2987: def _reconstruct_record_batch(columns, schema):",
          "2988:     \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2988:     def __arrow_c_array__(self, requested_schema=None):",
          "2989:         \"\"\"",
          "2990:         Get a pair of PyCapsules containing a C ArrowArray representation of the object.",
          "2992:         Parameters",
          "2993:         ----------",
          "2994:         requested_schema : PyCapsule | None",
          "2995:             A PyCapsule containing a C ArrowSchema representation of a requested",
          "2996:             schema. PyArrow will attempt to cast the batch to this schema.",
          "2997:             If None, the schema will be returned as-is, with a schema matching the",
          "2998:             one returned by :meth:`__arrow_c_schema__()`.",
          "3000:         Returns",
          "3001:         -------",
          "3002:         Tuple[PyCapsule, PyCapsule]",
          "3003:             A pair of PyCapsules containing a C ArrowSchema and ArrowArray,",
          "3004:             respectively.",
          "3005:         \"\"\"",
          "3006:         cdef:",
          "3007:             ArrowArray* c_array",
          "3008:             ArrowSchema* c_schema",
          "3010:         if requested_schema is not None:",
          "3011:             target_schema = Schema._import_from_c_capsule(requested_schema)",
          "3013:             if target_schema != self.schema:",
          "3014:                 try:",
          "3015:                     # We don't expose .cast() on RecordBatch, only on Table.",
          "3016:                     casted_batch = Table.from_batches([self]).cast(",
          "3017:                         target_schema, safe=True).to_batches()[0]",
          "3018:                     inner_batch = pyarrow_unwrap_batch(casted_batch)",
          "3019:                 except ArrowInvalid as e:",
          "3020:                     raise ValueError(",
          "3021:                         f\"Could not cast {self.schema} to requested schema {target_schema}: {e}\"",
          "3022:                     )",
          "3023:             else:",
          "3024:                 inner_batch = self.sp_batch",
          "3025:         else:",
          "3026:             inner_batch = self.sp_batch",
          "3028:         schema_capsule = alloc_c_schema(&c_schema)",
          "3029:         array_capsule = alloc_c_array(&c_array)",
          "3031:         with nogil:",
          "3032:             check_status(ExportRecordBatch(deref(inner_batch), c_array, c_schema))",
          "3034:         return schema_capsule, array_capsule",
          "3036:     def __arrow_c_stream__(self, requested_schema=None):",
          "3037:         \"\"\"",
          "3038:         Export the batch as an Arrow C stream PyCapsule.",
          "3040:         Parameters",
          "3041:         ----------",
          "3042:         requested_schema : pyarrow.lib.Schema, default None",
          "3043:             A schema to attempt to cast the streamed data to. This is currently",
          "3044:             unsupported and will raise an error.",
          "3046:         Returns",
          "3047:         -------",
          "3048:         PyCapsule",
          "3049:         \"\"\"",
          "3050:         return Table.from_batches([self]).__arrow_c_stream__(requested_schema)",
          "3052:     @staticmethod",
          "3053:     def _import_from_c_capsule(schema_capsule, array_capsule):",
          "3054:         \"\"\"",
          "3055:         Import RecordBatch from a pair of PyCapsules containing a C ArrowArray",
          "3056:         and ArrowSchema, respectively.",
          "3058:         Parameters",
          "3059:         ----------",
          "3060:         schema_capsule : PyCapsule",
          "3061:             A PyCapsule containing a C ArrowSchema representation of the schema.",
          "3062:         array_capsule : PyCapsule",
          "3063:             A PyCapsule containing a C ArrowArray representation of the array.",
          "3065:         Returns",
          "3066:         -------",
          "3067:         pyarrow.RecordBatch",
          "3068:         \"\"\"",
          "3069:         cdef:",
          "3070:             ArrowSchema* c_schema",
          "3071:             ArrowArray* c_array",
          "3072:             shared_ptr[CRecordBatch] c_batch",
          "3074:         c_schema = <ArrowSchema*> PyCapsule_GetPointer(schema_capsule, 'arrow_schema')",
          "3075:         c_array = <ArrowArray*> PyCapsule_GetPointer(array_capsule, 'arrow_array')",
          "3077:         with nogil:",
          "3078:             c_batch = GetResultValue(ImportRecordBatch(c_array, c_schema))",
          "3080:         return pyarrow_wrap_batch(c_batch)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "4757:             output_type=Table",
          "4758:         )",
          "4761: def _reconstruct_table(arrays, schema):",
          "4762:     \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4856:     def __arrow_c_stream__(self, requested_schema=None):",
          "4857:         \"\"\"",
          "4858:         Export the table as an Arrow C stream PyCapsule.",
          "4860:         Parameters",
          "4861:         ----------",
          "4862:         requested_schema : pyarrow.lib.Schema, default None",
          "4863:             A schema to attempt to cast the streamed data to. This is currently",
          "4864:             unsupported and will raise an error.",
          "4866:         Returns",
          "4867:         -------",
          "4868:         PyCapsule",
          "4869:         \"\"\"",
          "4870:         return self.to_reader().__arrow_c_stream__(requested_schema)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "4773:     Parameters",
          "4774:     ----------",
          "4777:     names : list, default None",
          "4778:         Column names if list of arrays passed as data. Mutually exclusive with",
          "4779:         'schema' argument.",
          "",
          "[Removed Lines]",
          "4775:     data : pandas.DataFrame, list",
          "4776:         A DataFrame or list of arrays or chunked arrays.",
          "",
          "[Added Lines]",
          "4887:     data : pandas.DataFrame, list, Arrow-compatible table",
          "4888:         A DataFrame, list of arrays or chunked arrays, or a tabular object",
          "4889:         implementing the Arrow PyCapsule Protocol (has an",
          "4890:         ``__arrow_c_array__`` method).",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "4892:     if isinstance(data, (list, tuple)):",
          "4893:         return RecordBatch.from_arrays(data, names=names, schema=schema,",
          "4894:                                        metadata=metadata)",
          "4895:     elif _pandas_api.is_data_frame(data):",
          "4896:         return RecordBatch.from_pandas(data, schema=schema)",
          "4897:     else:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "5009:     elif hasattr(data, \"__arrow_c_array__\"):",
          "5010:         if schema is not None:",
          "5011:             requested_schema = schema.__arrow_c_schema__()",
          "5012:         else:",
          "5013:             requested_schema = None",
          "5014:         schema_capsule, array_capsule = data.__arrow_c_array__(requested_schema)",
          "5015:         batch = RecordBatch._import_from_c_capsule(schema_capsule, array_capsule)",
          "5016:         if schema is not None and batch.schema != schema:",
          "5017:             # __arrow_c_array__ coerces schema with best effort, so we might",
          "5018:             # need to cast it if the producer wasn't able to cast to exact schema.",
          "5019:             batch = Table.from_batches([batch]).cast(schema).to_batches()[0]",
          "5020:         return batch",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "5013:             raise ValueError(",
          "5014:                 \"The 'names' argument is not valid when passing a dictionary\")",
          "5015:         return Table.from_pydict(data, schema=schema, metadata=metadata)",
          "5016:     elif _pandas_api.is_data_frame(data):",
          "5017:         if names is not None or metadata is not None:",
          "5018:             raise ValueError(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "5142:     elif hasattr(data, \"__arrow_c_stream__\"):",
          "5143:         if schema is not None:",
          "5144:             requested = schema.__arrow_c_schema__()",
          "5145:         else:",
          "5146:             requested = None",
          "5147:         capsule = data.__arrow_c_stream__(requested)",
          "5148:         reader = RecordBatchReader._import_from_c_capsule(capsule)",
          "5149:         table = reader.read_all()",
          "5150:         if schema is not None and table.schema != schema:",
          "5151:             # __arrow_c_array__ coerces schema with best effort, so we might",
          "5152:             # need to cast it if the producer wasn't able to cast to exact schema.",
          "5153:             table = table.cast(schema)",
          "5154:         return table",
          "5155:     elif hasattr(data, \"__arrow_c_array__\"):",
          "5156:         batch = record_batch(data, schema)",
          "5157:         return Table.from_batches([batch])",
          "",
          "---------------"
        ],
        "python/pyarrow/tests/test_array.py||python/pyarrow/tests/test_array.py": [
          "File: python/pyarrow/tests/test_array.py -> python/pyarrow/tests/test_array.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "3336:     assert result.equals(expected)",
          "3339: def test_concat_array():",
          "3340:     concatenated = pa.concat_arrays(",
          "3341:         [pa.array([1, 2]), pa.array([3, 4])])",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3339: def test_c_array_protocol():",
          "3340:     class ArrayWrapper:",
          "3341:         def __init__(self, data):",
          "3342:             self.data = data",
          "3344:         def __arrow_c_array__(self, requested_type=None):",
          "3345:             return self.data.__arrow_c_array__(requested_type)",
          "3347:     # Can roundtrip through the C array protocol",
          "3348:     arr = ArrayWrapper(pa.array([1, 2, 3], type=pa.int64()))",
          "3349:     result = pa.array(arr)",
          "3350:     assert result == arr.data",
          "3352:     # Will case to requested type",
          "3353:     result = pa.array(arr, type=pa.int32())",
          "3354:     assert result == pa.array([1, 2, 3], type=pa.int32())",
          "",
          "---------------"
        ],
        "python/pyarrow/tests/test_cffi.py||python/pyarrow/tests/test_cffi.py": [
          "File: python/pyarrow/tests/test_cffi.py -> python/pyarrow/tests/test_cffi.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "19: import gc",
          "21: import pyarrow as pa",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: import ctypes",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "47:     ValueError, match=\"Cannot import released ArrowArrayStream\")",
          "50: class ParamExtType(pa.PyExtensionType):",
          "52:     def __init__(self, width):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "50: def PyCapsule_IsValid(capsule, name):",
          "51:     return ctypes.pythonapi.PyCapsule_IsValid(ctypes.py_object(capsule), name) == 1",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "411:                        match=\"Expected to be able to read 16 bytes \"",
          "412:                              \"for message body, got 8\"):",
          "413:         reader_new.read_all()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "420: @pytest.mark.parametrize('obj', [pa.int32(), pa.field('foo', pa.int32()),",
          "421:                                  pa.schema({'foo': pa.int32()})],",
          "422:                          ids=['type', 'field', 'schema'])",
          "423: def test_roundtrip_schema_capsule(obj):",
          "424:     gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
          "425:     old_allocated = pa.total_allocated_bytes()",
          "427:     capsule = obj.__arrow_c_schema__()",
          "428:     assert PyCapsule_IsValid(capsule, b\"arrow_schema\") == 1",
          "429:     assert pa.total_allocated_bytes() > old_allocated",
          "430:     obj_out = type(obj)._import_from_c_capsule(capsule)",
          "431:     assert obj_out == obj",
          "433:     assert pa.total_allocated_bytes() == old_allocated",
          "435:     capsule = obj.__arrow_c_schema__()",
          "437:     assert pa.total_allocated_bytes() > old_allocated",
          "438:     del capsule",
          "439:     assert pa.total_allocated_bytes() == old_allocated",
          "442: @pytest.mark.parametrize('arr,schema_accessor,bad_type,good_type', [",
          "443:     (pa.array(['a', 'b', 'c']), lambda x: x.type, pa.int32(), pa.string()),",
          "444:     (",
          "445:         pa.record_batch([pa.array(['a', 'b', 'c'])], names=['x']),",
          "446:         lambda x: x.schema,",
          "447:         pa.schema({'x': pa.int32()}),",
          "448:         pa.schema({'x': pa.string()})",
          "449:     ),",
          "450: ], ids=['array', 'record_batch'])",
          "451: def test_roundtrip_array_capsule(arr, schema_accessor, bad_type, good_type):",
          "452:     gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
          "453:     old_allocated = pa.total_allocated_bytes()",
          "455:     import_array = type(arr)._import_from_c_capsule",
          "457:     schema_capsule, capsule = arr.__arrow_c_array__()",
          "458:     assert PyCapsule_IsValid(schema_capsule, b\"arrow_schema\") == 1",
          "459:     assert PyCapsule_IsValid(capsule, b\"arrow_array\") == 1",
          "460:     arr_out = import_array(schema_capsule, capsule)",
          "461:     assert arr_out.equals(arr)",
          "463:     assert pa.total_allocated_bytes() > old_allocated",
          "464:     del arr_out",
          "466:     assert pa.total_allocated_bytes() == old_allocated",
          "468:     capsule = arr.__arrow_c_array__()",
          "470:     assert pa.total_allocated_bytes() > old_allocated",
          "471:     del capsule",
          "472:     assert pa.total_allocated_bytes() == old_allocated",
          "474:     with pytest.raises(ValueError,",
          "475:                        match=r\"Could not cast.* string to requested .* int32\"):",
          "476:         arr.__arrow_c_array__(bad_type.__arrow_c_schema__())",
          "478:     schema_capsule, array_capsule = arr.__arrow_c_array__(",
          "479:         good_type.__arrow_c_schema__())",
          "480:     arr_out = import_array(schema_capsule, array_capsule)",
          "481:     assert schema_accessor(arr_out) == good_type",
          "484: # TODO: implement requested_schema for stream",
          "485: @pytest.mark.parametrize('constructor', [",
          "486:     pa.RecordBatchReader.from_batches,",
          "487:     # Use a lambda because we need to re-order the parameters",
          "488:     lambda schema, batches: pa.Table.from_batches(batches, schema),",
          "489: ], ids=['recordbatchreader', 'table'])",
          "490: def test_roundtrip_reader_capsule(constructor):",
          "491:     batches = make_batches()",
          "492:     schema = batches[0].schema",
          "494:     gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
          "495:     old_allocated = pa.total_allocated_bytes()",
          "497:     obj = constructor(schema, batches)",
          "499:     capsule = obj.__arrow_c_stream__()",
          "500:     assert PyCapsule_IsValid(capsule, b\"arrow_array_stream\") == 1",
          "501:     imported_reader = pa.RecordBatchReader._import_from_c_capsule(capsule)",
          "502:     assert imported_reader.schema == schema",
          "503:     imported_batches = list(imported_reader)",
          "504:     assert len(imported_batches) == len(batches)",
          "505:     for batch, expected in zip(imported_batches, batches):",
          "506:         assert batch.equals(expected)",
          "508:     del obj, imported_reader, batch, expected, imported_batches",
          "510:     assert pa.total_allocated_bytes() == old_allocated",
          "512:     obj = constructor(schema, batches)",
          "514:     # TODO: turn this to ValueError once we implement validation.",
          "515:     bad_schema = pa.schema({'ints': pa.int32()})",
          "516:     with pytest.raises(NotImplementedError):",
          "517:         obj.__arrow_c_stream__(bad_schema.__arrow_c_schema__())",
          "519:     # Can work with matching schema",
          "520:     matching_schema = pa.schema({'ints': pa.list_(pa.int32())})",
          "521:     capsule = obj.__arrow_c_stream__(matching_schema.__arrow_c_schema__())",
          "522:     imported_reader = pa.RecordBatchReader._import_from_c_capsule(capsule)",
          "523:     assert imported_reader.schema == matching_schema",
          "524:     for batch, expected in zip(imported_reader, batches):",
          "525:         assert batch.equals(expected)",
          "528: def test_roundtrip_batch_reader_capsule():",
          "529:     batch = make_batch()",
          "531:     capsule = batch.__arrow_c_stream__()",
          "532:     assert PyCapsule_IsValid(capsule, b\"arrow_array_stream\") == 1",
          "533:     imported_reader = pa.RecordBatchReader._import_from_c_capsule(capsule)",
          "534:     assert imported_reader.schema == batch.schema",
          "535:     assert imported_reader.read_next_batch().equals(batch)",
          "536:     with pytest.raises(StopIteration):",
          "537:         imported_reader.read_next_batch()",
          "",
          "---------------"
        ],
        "python/pyarrow/tests/test_table.py||python/pyarrow/tests/test_table.py": [
          "File: python/pyarrow/tests/test_table.py -> python/pyarrow/tests/test_table.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "553:         pa.RecordBatch()",
          "556: def test_recordbatch_itercolumns():",
          "557:     data = [",
          "558:         pa.array(range(5), type='int16'),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "556: def test_recordbatch_c_array_interface():",
          "557:     class BatchWrapper:",
          "558:         def __init__(self, batch):",
          "559:             self.batch = batch",
          "561:         def __arrow_c_array__(self, requested_type=None):",
          "562:             return self.batch.__arrow_c_array__(requested_type)",
          "564:     data = pa.record_batch([",
          "565:         pa.array([1, 2, 3], type=pa.int64())",
          "566:     ], names=['a'])",
          "567:     wrapper = BatchWrapper(data)",
          "569:     # Can roundtrip through the wrapper.",
          "570:     result = pa.record_batch(wrapper)",
          "571:     assert result == data",
          "573:     # Can also import with a schema that implementer can cast to.",
          "574:     castable_schema = pa.schema([",
          "575:         pa.field('a', pa.int32())",
          "576:     ])",
          "577:     result = pa.record_batch(wrapper, schema=castable_schema)",
          "578:     expected = pa.record_batch([",
          "579:         pa.array([1, 2, 3], type=pa.int32())",
          "580:     ], names=['a'])",
          "581:     assert result == expected",
          "584: def test_table_c_array_interface():",
          "585:     class BatchWrapper:",
          "586:         def __init__(self, batch):",
          "587:             self.batch = batch",
          "589:         def __arrow_c_array__(self, requested_type=None):",
          "590:             return self.batch.__arrow_c_array__(requested_type)",
          "592:     data = pa.record_batch([",
          "593:         pa.array([1, 2, 3], type=pa.int64())",
          "594:     ], names=['a'])",
          "595:     wrapper = BatchWrapper(data)",
          "597:     # Can roundtrip through the wrapper.",
          "598:     result = pa.table(wrapper)",
          "599:     expected = pa.Table.from_batches([data])",
          "600:     assert result == expected",
          "602:     # Can also import with a schema that implementer can cast to.",
          "603:     castable_schema = pa.schema([",
          "604:         pa.field('a', pa.int32())",
          "605:     ])",
          "606:     result = pa.table(wrapper, schema=castable_schema)",
          "607:     expected = pa.table({",
          "608:         'a': pa.array([1, 2, 3], type=pa.int32())",
          "609:     })",
          "610:     assert result == expected",
          "613: def test_table_c_stream_interface():",
          "614:     class StreamWrapper:",
          "615:         def __init__(self, batches):",
          "616:             self.batches = batches",
          "618:         def __arrow_c_stream__(self, requested_type=None):",
          "619:             reader = pa.RecordBatchReader.from_batches(",
          "620:                 self.batches[0].schema, self.batches)",
          "621:             return reader.__arrow_c_stream__(requested_type)",
          "623:     data = [",
          "624:         pa.record_batch([pa.array([1, 2, 3], type=pa.int64())], names=['a']),",
          "625:         pa.record_batch([pa.array([4, 5, 6], type=pa.int64())], names=['a'])",
          "626:     ]",
          "627:     wrapper = StreamWrapper(data)",
          "629:     # Can roundtrip through the wrapper.",
          "630:     result = pa.table(wrapper)",
          "631:     expected = pa.Table.from_batches(data)",
          "632:     assert result == expected",
          "634:     # Passing schema works if already that schema",
          "635:     result = pa.table(wrapper, schema=data[0].schema)",
          "636:     assert result == expected",
          "638:     # If schema doesn't match, raises NotImplementedError",
          "639:     with pytest.raises(NotImplementedError):",
          "640:         pa.table(wrapper, schema=pa.schema([pa.field('a', pa.int32())]))",
          "",
          "---------------"
        ],
        "python/pyarrow/tests/test_types.py||python/pyarrow/tests/test_types.py": [
          "File: python/pyarrow/tests/test_types.py -> python/pyarrow/tests/test_types.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1225:         schema = pa.schema([pa.field(\"field_name\", arrow_type)])",
          "1226:         type_back = schema.field(\"field_name\").type",
          "1227:         assert type(type_back) is type(arrow_type)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1230: def test_schema_import_c_schema_interface():",
          "1231:     class Wrapper:",
          "1232:         def __init__(self, schema):",
          "1233:             self.schema = schema",
          "1235:         def __arrow_c_schema__(self):",
          "1236:             return self.schema.__arrow_c_schema__()",
          "1238:     schema = pa.schema([pa.field(\"field_name\", pa.int32())])",
          "1239:     wrapped_schema = Wrapper(schema)",
          "1241:     assert pa.schema(wrapped_schema) == schema",
          "",
          "---------------"
        ],
        "python/pyarrow/types.pxi||python/pyarrow/types.pxi": [
          "File: python/pyarrow/types.pxi -> python/pyarrow/types.pxi",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "20: import atexit",
          "21: from collections.abc import Mapping",
          "",
          "[Removed Lines]",
          "18: from cpython.pycapsule cimport PyCapsule_CheckExact, PyCapsule_GetPointer",
          "",
          "[Added Lines]",
          "18: from cpython.pycapsule cimport PyCapsule_CheckExact, PyCapsule_GetPointer, PyCapsule_New, PyCapsule_IsValid",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "23: import re",
          "24: import sys",
          "25: import warnings",
          "28: # These are imprecise because the type (in pandas 0.x) depends on the presence",
          "29: # of nulls",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: from cython import sizeof",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "357:                                            _as_c_pointer(in_ptr)))",
          "358:         return pyarrow_wrap_data_type(result)",
          "361: cdef class DictionaryMemo(_Weakrefable):",
          "362:     \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "360:     def __arrow_c_schema__(self):",
          "361:         \"\"\"",
          "362:         Export to a ArrowSchema PyCapsule",
          "364:         Unlike _export_to_c, this will not leak memory if the capsule is not used.",
          "365:         \"\"\"",
          "366:         cdef ArrowSchema* c_schema",
          "367:         capsule = alloc_c_schema(&c_schema)",
          "369:         with nogil:",
          "370:             check_status(ExportType(deref(self.type), c_schema))",
          "372:         return capsule",
          "374:     @staticmethod",
          "375:     def _import_from_c_capsule(schema):",
          "376:         \"\"\"",
          "377:         Import a DataType from a ArrowSchema PyCapsule",
          "379:         Parameters",
          "380:         ----------",
          "381:         schema : PyCapsule",
          "382:             A valid PyCapsule with name 'arrow_schema' containing an",
          "383:             ArrowSchema pointer.",
          "384:         \"\"\"",
          "385:         cdef:",
          "386:             ArrowSchema* c_schema",
          "387:             shared_ptr[CDataType] c_type",
          "389:         if not PyCapsule_IsValid(schema, 'arrow_schema'):",
          "390:             raise TypeError(",
          "391:                 \"Not an ArrowSchema object\"",
          "392:             )",
          "393:         c_schema = <ArrowSchema*> PyCapsule_GetPointer(schema, 'arrow_schema')",
          "395:         with nogil:",
          "396:             c_type = GetResultValue(ImportType(c_schema))",
          "398:         return pyarrow_wrap_data_type(c_type)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "2369:             result = GetResultValue(ImportField(<ArrowSchema*> c_ptr))",
          "2370:         return pyarrow_wrap_field(result)",
          "2373: cdef class Schema(_Weakrefable):",
          "2374:     \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2412:     def __arrow_c_schema__(self):",
          "2413:         \"\"\"",
          "2414:         Export to a ArrowSchema PyCapsule",
          "2416:         Unlike _export_to_c, this will not leak memory if the capsule is not used.",
          "2417:         \"\"\"",
          "2418:         cdef ArrowSchema* c_schema",
          "2419:         capsule = alloc_c_schema(&c_schema)",
          "2421:         with nogil:",
          "2422:             check_status(ExportField(deref(self.field), c_schema))",
          "2424:         return capsule",
          "2426:     @staticmethod",
          "2427:     def _import_from_c_capsule(schema):",
          "2428:         \"\"\"",
          "2429:         Import a Field from a ArrowSchema PyCapsule",
          "2431:         Parameters",
          "2432:         ----------",
          "2433:         schema : PyCapsule",
          "2434:             A valid PyCapsule with name 'arrow_schema' containing an",
          "2435:             ArrowSchema pointer.",
          "2436:         \"\"\"",
          "2437:         cdef:",
          "2438:             ArrowSchema* c_schema",
          "2439:             shared_ptr[CField] c_field",
          "2441:         if not PyCapsule_IsValid(schema, 'arrow_schema'):",
          "2442:             raise ValueError(",
          "2443:                 \"Not an ArrowSchema object\"",
          "2444:             )",
          "2445:         c_schema = <ArrowSchema*> PyCapsule_GetPointer(schema, 'arrow_schema')",
          "2447:         with nogil:",
          "2448:             c_field = GetResultValue(ImportField(c_schema))",
          "2450:         return pyarrow_wrap_field(c_field)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "3153:     def __repr__(self):",
          "3154:         return self.__str__()",
          "3157: def unify_schemas(schemas, *, promote_options=\"default\"):",
          "3158:     \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3236:     def __arrow_c_schema__(self):",
          "3237:         \"\"\"",
          "3238:         Export to a ArrowSchema PyCapsule",
          "3240:         Unlike _export_to_c, this will not leak memory if the capsule is not used.",
          "3241:         \"\"\"",
          "3242:         cdef ArrowSchema* c_schema",
          "3243:         capsule = alloc_c_schema(&c_schema)",
          "3245:         with nogil:",
          "3246:             check_status(ExportSchema(deref(self.schema), c_schema))",
          "3248:         return capsule",
          "3250:     @staticmethod",
          "3251:     def _import_from_c_capsule(schema):",
          "3252:         \"\"\"",
          "3253:         Import a Schema from a ArrowSchema PyCapsule",
          "3255:         Parameters",
          "3256:         ----------",
          "3257:         schema : PyCapsule",
          "3258:             A valid PyCapsule with name 'arrow_schema' containing an",
          "3259:             ArrowSchema pointer.",
          "3260:         \"\"\"",
          "3261:         cdef:",
          "3262:             ArrowSchema* c_schema",
          "3264:         if not PyCapsule_IsValid(schema, 'arrow_schema'):",
          "3265:             raise ValueError(",
          "3266:                 \"Not an ArrowSchema object\"",
          "3267:             )",
          "3268:         c_schema = <ArrowSchema*> PyCapsule_GetPointer(schema, 'arrow_schema')",
          "3270:         with nogil:",
          "3271:             result = GetResultValue(ImportSchema(c_schema))",
          "3273:         return pyarrow_wrap_schema(result)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "4930:     Parameters",
          "4931:     ----------",
          "4932:     fields : iterable of Fields or tuples, or mapping of strings to DataTypes",
          "4933:     metadata : dict, default None",
          "4934:         Keys and values must be coercible to bytes.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "5052:         Can also pass an object that implements the Arrow PyCapsule Protocol",
          "5053:         for schemas (has an ``__arrow_c_schema__`` method).",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "4970:     if isinstance(fields, Mapping):",
          "4971:         fields = fields.items()",
          "4973:     for item in fields:",
          "4974:         if isinstance(item, tuple):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "5093:     elif hasattr(fields, \"__arrow_c_schema__\"):",
          "5094:         return Schema._import_from_c_capsule(fields.__arrow_c_schema__())",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "5104: _register_py_extension_type()",
          "5105: atexit.register(_unregister_py_extension_types)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "5231: #",
          "5232: # PyCapsule export utilities",
          "5233: #",
          "5235: cdef void pycapsule_schema_deleter(object schema_capsule) noexcept:",
          "5236:     cdef ArrowSchema* schema = <ArrowSchema*>PyCapsule_GetPointer(",
          "5237:         schema_capsule, 'arrow_schema'",
          "5238:     )",
          "5239:     if schema.release != NULL:",
          "5240:         schema.release(schema)",
          "5242:     free(schema)",
          "5244: cdef object alloc_c_schema(ArrowSchema** c_schema) noexcept:",
          "5245:     c_schema[0] = <ArrowSchema*> malloc(sizeof(ArrowSchema))",
          "5246:     # Ensure the capsule destructor doesn't call a random release pointer",
          "5247:     c_schema[0].release = NULL",
          "5248:     return PyCapsule_New(c_schema[0], 'arrow_schema', &pycapsule_schema_deleter)",
          "5251: cdef void pycapsule_array_deleter(object array_capsule) noexcept:",
          "5252:     cdef:",
          "5253:         ArrowArray* array",
          "5254:     # Do not invoke the deleter on a used/moved capsule",
          "5255:     array = <ArrowArray*>cpython.PyCapsule_GetPointer(",
          "5256:         array_capsule, 'arrow_array'",
          "5257:     )",
          "5258:     if array.release != NULL:",
          "5259:         array.release(array)",
          "5261:     free(array)",
          "5263: cdef object alloc_c_array(ArrowArray** c_array) noexcept:",
          "5264:     c_array[0] = <ArrowArray*> malloc(sizeof(ArrowArray))",
          "5265:     # Ensure the capsule destructor doesn't call a random release pointer",
          "5266:     c_array[0].release = NULL",
          "5267:     return PyCapsule_New(c_array[0], 'arrow_array', &pycapsule_array_deleter)",
          "5270: cdef void pycapsule_stream_deleter(object stream_capsule) noexcept:",
          "5271:     cdef:",
          "5272:         ArrowArrayStream* stream",
          "5273:     # Do not invoke the deleter on a used/moved capsule",
          "5274:     stream = <ArrowArrayStream*>PyCapsule_GetPointer(",
          "5275:         stream_capsule, 'arrow_array_stream'",
          "5276:     )",
          "5277:     if stream.release != NULL:",
          "5278:         stream.release(stream)",
          "5280:     free(stream)",
          "5282: cdef object alloc_c_stream(ArrowArrayStream** c_stream) noexcept:",
          "5283:     c_stream[0] = <ArrowArrayStream*> malloc(sizeof(ArrowArrayStream))",
          "5284:     # Ensure the capsule destructor doesn't call a random release pointer",
          "5285:     c_stream[0].release = NULL",
          "5286:     return PyCapsule_New(c_stream[0], 'arrow_array_stream', &pycapsule_stream_deleter)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5a1ca69d617288a12769b279502d8815a859f487",
      "candidate_info": {
        "commit_hash": "5a1ca69d617288a12769b279502d8815a859f487",
        "repo": "apache/arrow",
        "commit_url": "https://github.com/apache/arrow/commit/5a1ca69d617288a12769b279502d8815a859f487",
        "files": [
          "python/pyarrow/compat.pxi",
          "python/pyarrow/io.pxi",
          "python/pyarrow/pandas_compat.py",
          "python/pyarrow/types.pxi",
          "python/requirements-wheel-test.txt"
        ],
        "message": "GH-37244: [Python] Remove support for pickle5 (#37644)\n\nResolve issue https://github.com/apache/arrow/issues/37244 by removing pickle5 usage in pyarrow.\n\n### Rationale for this change\n\nSee issue https://github.com/apache/arrow/issues/37244 . \n\n### What changes are included in this PR?\n\npickle5 usage is removed from pyarrow . \n\n### Are these changes tested?\n\nYes, the python test suite was run. \n\n### Are there any user-facing changes?\n\nNo. \n\n* Closes: #37244\n\nAuthored-by: Chris Jordan-Squire <chris.jordansquire@gmail.com>\nSigned-off-by: AlenkaF <frim.alenka@gmail.com>",
        "before_after_code_files": [
          "python/pyarrow/compat.pxi||python/pyarrow/compat.pxi",
          "python/pyarrow/io.pxi||python/pyarrow/io.pxi",
          "python/pyarrow/pandas_compat.py||python/pyarrow/pandas_compat.py",
          "python/pyarrow/types.pxi||python/pyarrow/types.pxi"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "python/pyarrow/types.pxi||python/pyarrow/types.pxi"
          ],
          "candidate": [
            "python/pyarrow/types.pxi||python/pyarrow/types.pxi"
          ]
        }
      },
      "candidate_diff": {
        "python/pyarrow/compat.pxi||python/pyarrow/compat.pxi": [
          "File: python/pyarrow/compat.pxi -> python/pyarrow/compat.pxi",
          "--- Hunk 1 ---",
          "[Context before]",
          "33: ordered_dict = dict",
          "42: try:",
          "43:     import cloudpickle as pickle",
          "44: except ImportError:",
          "48: def tobytes(o):",
          "",
          "[Removed Lines]",
          "36: try:",
          "37:     import pickle5 as builtin_pickle",
          "38: except ImportError:",
          "39:     import pickle as builtin_pickle",
          "45:     pickle = builtin_pickle",
          "",
          "[Added Lines]",
          "39:     import pickle",
          "",
          "---------------"
        ],
        "python/pyarrow/io.pxi||python/pyarrow/io.pxi": [
          "File: python/pyarrow/io.pxi -> python/pyarrow/io.pxi",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: from libc.stdlib cimport malloc, free",
          "23: import codecs",
          "24: import re",
          "25: import sys",
          "26: import threading",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24: import pickle",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1369:     def __reduce_ex__(self, protocol):",
          "1370:         if protocol >= 5:",
          "1372:         elif self.buffer.get().is_mutable():",
          "1373:             # Need to pass a bytearray to recreate a mutable buffer when",
          "1374:             # unpickling.",
          "",
          "[Removed Lines]",
          "1371:             bufobj = builtin_pickle.PickleBuffer(self)",
          "",
          "[Added Lines]",
          "1372:             bufobj = pickle.PickleBuffer(self)",
          "",
          "---------------"
        ],
        "python/pyarrow/pandas_compat.py||python/pyarrow/pandas_compat.py": [
          "File: python/pyarrow/pandas_compat.py -> python/pyarrow/pandas_compat.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: from itertools import zip_longest",
          "27: import json",
          "28: import operator",
          "29: import re",
          "30: import warnings",
          "32: import numpy as np",
          "34: import pyarrow as pa",
          "38: _logical_type_map = {}",
          "",
          "[Removed Lines]",
          "35: from pyarrow.lib import _pandas_api, builtin_pickle, frombytes  # noqa",
          "",
          "[Added Lines]",
          "29: import pickle",
          "36: from pyarrow.lib import _pandas_api, frombytes  # noqa",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "720:                                 klass=_int.DatetimeTZBlock,",
          "721:                                 dtype=dtype)",
          "722:     elif 'object' in item:",
          "724:                                 placement=placement)",
          "725:     elif 'py_array' in item:",
          "726:         # create ExtensionBlock",
          "",
          "[Removed Lines]",
          "723:         block = _int.make_block(builtin_pickle.loads(block_arr),",
          "",
          "[Added Lines]",
          "724:         block = _int.make_block(pickle.loads(block_arr),",
          "",
          "---------------"
        ],
        "python/pyarrow/types.pxi||python/pyarrow/types.pxi": [
          "File: python/pyarrow/types.pxi -> python/pyarrow/types.pxi",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: import atexit",
          "21: from collections.abc import Mapping",
          "22: import re",
          "23: import sys",
          "24: import warnings",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: import pickle",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1699:                                   .format(type(self).__name__))",
          "1701:     def __arrow_ext_serialize__(self):",
          "1704:     @classmethod",
          "1705:     def __arrow_ext_deserialize__(cls, storage_type, serialized):",
          "1706:         try:",
          "1708:         except Exception:",
          "1709:             # For some reason, it's impossible to deserialize the",
          "1710:             # ExtensionType instance.  Perhaps the serialized data is",
          "",
          "[Removed Lines]",
          "1702:         return builtin_pickle.dumps(self)",
          "1707:             ty = builtin_pickle.loads(serialized)",
          "",
          "[Added Lines]",
          "1703:         return pickle.dumps(self)",
          "1708:             ty = pickle.loads(serialized)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f59e37f8a676a13f0bd82a09c211ddb45c6b48ac",
      "candidate_info": {
        "commit_hash": "f59e37f8a676a13f0bd82a09c211ddb45c6b48ac",
        "repo": "apache/arrow",
        "commit_url": "https://github.com/apache/arrow/commit/f59e37f8a676a13f0bd82a09c211ddb45c6b48ac",
        "files": [
          "cpp/src/parquet/arrow/reader.cc",
          "python/pyarrow/tests/test_extension_type.py"
        ],
        "message": "GH-20385: [C++][Parquet] Reject partial load of an extension type (#33634)\n\nWhen the parquet reader is performing a partial read it will try and maintain field structure.  So, for example, given a schema of `points: struct<x: int32, y:int32>` and a load of `points.x` it will return `points: struct<x: int32>`.  However, if there is an extension type `points: Point` where `Point` has a storage type `struct<x: int32, y: int32>` then suddenly the reference `points.x` no longer makes sense.\n* Closes: #20385\n\nLead-authored-by: Weston Pace <weston.pace@gmail.com>\nCo-authored-by: Antoine Pitrou <antoine@python.org>\nSigned-off-by: Antoine Pitrou <antoine@python.org>",
        "before_after_code_files": [
          "cpp/src/parquet/arrow/reader.cc||cpp/src/parquet/arrow/reader.cc",
          "python/pyarrow/tests/test_extension_type.py||python/pyarrow/tests/test_extension_type.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "python/pyarrow/tests/test_extension_type.py||python/pyarrow/tests/test_extension_type.py"
          ],
          "candidate": [
            "python/pyarrow/tests/test_extension_type.py||python/pyarrow/tests/test_extension_type.py"
          ]
        }
      },
      "candidate_diff": {
        "cpp/src/parquet/arrow/reader.cc||cpp/src/parquet/arrow/reader.cc": [
          "File: cpp/src/parquet/arrow/reader.cc -> cpp/src/parquet/arrow/reader.cc",
          "--- Hunk 1 ---",
          "[Context before]",
          "842:     auto storage_field = arrow_field->WithType(",
          "843:         checked_cast<const ExtensionType&>(*arrow_field->type()).storage_type());",
          "844:     RETURN_NOT_OK(GetReader(field, storage_field, ctx, out));",
          "846:     return Status::OK();",
          "847:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "845:     if (*out) {",
          "846:       auto storage_type = (*out)->field()->type();",
          "847:       if (!storage_type->Equals(storage_field->type())) {",
          "848:         return Status::Invalid(",
          "849:             \"Due to column pruning only part of an extension's storage type was loaded.  \"",
          "850:             \"An extension type cannot be created without all of its fields\");",
          "851:       }",
          "853:     }",
          "",
          "---------------"
        ],
        "python/pyarrow/tests/test_extension_type.py||python/pyarrow/tests/test_extension_type.py": [
          "File: python/pyarrow/tests/test_extension_type.py -> python/pyarrow/tests/test_extension_type.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "948:     assert table.column('lists').type == mylist_array.type",
          "949:     assert table == orig_table",
          "952: @pytest.mark.parquet",
          "953: def test_parquet_nested_extension(tmpdir):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "951:     with pytest.raises(pa.ArrowInvalid, match='without all of its fields'):",
          "952:         pq.ParquetFile(filename).read(columns=['structs.left'])",
          "",
          "---------------"
        ]
      }
    }
  ]
}