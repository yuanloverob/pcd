{
  "cve_id": "CVE-2022-40754",
  "cve_desc": "In Apache Airflow 2.3.0 through 2.3.4, there was an open redirect in the webserver's `/confirm` endpoint.",
  "repo": "apache/airflow",
  "patch_hash": "3871f00230b7064e6234aa65b8a00ee2b6d33e4c",
  "patch_info": {
    "commit_hash": "3871f00230b7064e6234aa65b8a00ee2b6d33e4c",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/3871f00230b7064e6234aa65b8a00ee2b6d33e4c",
    "files": [
      "airflow/www/views.py"
    ],
    "message": "Fix UI redirect (#26409)\n\nCo-authored-by: Konstantin Weddige <konstantin.weddige@lutrasecurity.com>\n(cherry picked from commit 56e7555c42f013f789a4b718676ff09b4a9d5135)",
    "before_after_code_files": [
      "airflow/www/views.py||airflow/www/views.py"
    ]
  },
  "patch_diff": {
    "airflow/www/views.py||airflow/www/views.py": [
      "File: airflow/www/views.py -> airflow/www/views.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "2329:         task_id = args.get('task_id')",
      "2330:         dag_run_id = args.get('dag_run_id')",
      "2331:         state = args.get('state')",
      "2334:         if 'map_index' not in args:",
      "2335:             map_indexes: list[int] | None = None",
      "",
      "[Removed Lines]",
      "2332:         origin = args.get('origin')",
      "",
      "[Added Lines]",
      "2332:         origin = get_safe_url(args.get('origin'))",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "7f6e0d7728f370aff3a17d104dd656502ed19581",
      "candidate_info": {
        "commit_hash": "7f6e0d7728f370aff3a17d104dd656502ed19581",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7f6e0d7728f370aff3a17d104dd656502ed19581",
        "files": [
          "dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py"
        ],
        "message": "Fix case when SHELL variable is not set in kubernetes tests (#26235)\n\nWhen SHELL variable is not set, kubernetes tests will fall back\nto using 'bash'\n\n(cherry picked from commit b7db311336bbe909ee7f559af2b1da7635c54823)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py||dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py||dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py -> dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1332:     env = get_k8s_env(python=python, kubernetes_version=kubernetes_version, executor=executor)",
          "1333:     kubectl_cluster_name = get_kubectl_cluster_name(python=python, kubernetes_version=kubernetes_version)",
          "1334:     get_console(output=output).print(f\"\\n[info]Running tests with {kubectl_cluster_name} cluster.\")",
          "1336:     extra_shell_args: List[str] = []",
          "1337:     if shell_binary.endswith(\"zsh\"):",
          "1338:         extra_shell_args.append('--no-rcs')",
          "",
          "[Removed Lines]",
          "1335:     shell_binary = env['SHELL']",
          "",
          "[Added Lines]",
          "1335:     shell_binary = env.get('SHELL', shutil.which('bash'))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d36dcc1f581ea2d55ccbe7bfc340c2dfa4cc1fdc",
      "candidate_info": {
        "commit_hash": "d36dcc1f581ea2d55ccbe7bfc340c2dfa4cc1fdc",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/d36dcc1f581ea2d55ccbe7bfc340c2dfa4cc1fdc",
        "files": [
          "airflow/executors/executor_loader.py",
          "airflow/migrations/versions/0038_1_10_2_add_sm_dag_index.py",
          "airflow/migrations/versions/0108_2_3_0_default_dag_view_grid.py",
          "airflow/providers/amazon/aws/transfers/redshift_to_s3.py",
          "airflow/providers/databricks/hooks/databricks_base.py",
          "airflow/providers/google/cloud/hooks/cloud_memorystore.py",
          "airflow/providers/mysql/hooks/mysql.py",
          "airflow/providers/qubole/sensors/qubole.py",
          "airflow/utils/process_utils.py",
          "docs/apache-airflow/migrations-ref.rst"
        ],
        "message": "D400 first line should end with period batch02 (#25268)\n\n(cherry picked from commit 50668445137e4037bb4a3b652bec22e53d1eddd7)",
        "before_after_code_files": [
          "airflow/executors/executor_loader.py||airflow/executors/executor_loader.py",
          "airflow/migrations/versions/0038_1_10_2_add_sm_dag_index.py||airflow/migrations/versions/0038_1_10_2_add_sm_dag_index.py",
          "airflow/migrations/versions/0108_2_3_0_default_dag_view_grid.py||airflow/migrations/versions/0108_2_3_0_default_dag_view_grid.py",
          "airflow/providers/amazon/aws/transfers/redshift_to_s3.py||airflow/providers/amazon/aws/transfers/redshift_to_s3.py",
          "airflow/providers/databricks/hooks/databricks_base.py||airflow/providers/databricks/hooks/databricks_base.py",
          "airflow/providers/google/cloud/hooks/cloud_memorystore.py||airflow/providers/google/cloud/hooks/cloud_memorystore.py",
          "airflow/providers/mysql/hooks/mysql.py||airflow/providers/mysql/hooks/mysql.py",
          "airflow/providers/qubole/sensors/qubole.py||airflow/providers/qubole/sensors/qubole.py",
          "airflow/utils/process_utils.py||airflow/utils/process_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/executors/executor_loader.py||airflow/executors/executor_loader.py": [
          "File: airflow/executors/executor_loader.py -> airflow/executors/executor_loader.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "66:     @classmethod",
          "67:     def get_default_executor(cls) -> \"BaseExecutor\":",
          "69:         if cls._default_executor is not None:",
          "70:             return cls._default_executor",
          "",
          "[Removed Lines]",
          "68:         \"\"\"Creates a new instance of the configured executor if none exists and returns it\"\"\"",
          "",
          "[Added Lines]",
          "68:         \"\"\"Creates a new instance of the configured executor if none exists and returns it.\"\"\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "135:     @classmethod",
          "136:     def __load_celery_kubernetes_executor(cls) -> \"BaseExecutor\":",
          "138:         celery_executor = import_string(cls.executors[CELERY_EXECUTOR])()",
          "139:         kubernetes_executor = import_string(cls.executors[KUBERNETES_EXECUTOR])()",
          "",
          "[Removed Lines]",
          "137:         \"\"\":return: an instance of CeleryKubernetesExecutor\"\"\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "144:     @classmethod",
          "145:     def __load_local_kubernetes_executor(cls) -> \"BaseExecutor\":",
          "147:         local_executor = import_string(cls.executors[LOCAL_EXECUTOR])()",
          "148:         kubernetes_executor = import_string(cls.executors[KUBERNETES_EXECUTOR])()",
          "",
          "[Removed Lines]",
          "146:         \"\"\":return: an instance of LocalKubernetesExecutor\"\"\"",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0038_1_10_2_add_sm_dag_index.py||airflow/migrations/versions/0038_1_10_2_add_sm_dag_index.py": [
          "File: airflow/migrations/versions/0038_1_10_2_add_sm_dag_index.py -> airflow/migrations/versions/0038_1_10_2_add_sm_dag_index.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "20: Revision ID: 03bc53e68815",
          "21: Revises: 0a2a5b66e19d, bf00311e1990",
          "",
          "[Removed Lines]",
          "18: \"\"\"Merge migrations Heads",
          "",
          "[Added Lines]",
          "18: \"\"\"Merge migrations Heads.",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0108_2_3_0_default_dag_view_grid.py||airflow/migrations/versions/0108_2_3_0_default_dag_view_grid.py": [
          "File: airflow/migrations/versions/0108_2_3_0_default_dag_view_grid.py -> airflow/migrations/versions/0108_2_3_0_default_dag_view_grid.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "21: Revision ID: b1b348e02d07",
          "22: Revises: 75d5ed6c2b43",
          "",
          "[Removed Lines]",
          "19: \"\"\"Update dag.default_view to grid",
          "",
          "[Added Lines]",
          "19: \"\"\"Update dag.default_view to grid.",
          "",
          "---------------"
        ],
        "airflow/providers/amazon/aws/transfers/redshift_to_s3.py||airflow/providers/amazon/aws/transfers/redshift_to_s3.py": [
          "File: airflow/providers/amazon/aws/transfers/redshift_to_s3.py -> airflow/providers/amazon/aws/transfers/redshift_to_s3.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: class RedshiftToS3Operator(BaseOperator):",
          "31:     \"\"\"",
          "34:     .. seealso::",
          "35:         For more information on how to use this operator, take a look at the guide:",
          "",
          "[Removed Lines]",
          "32:     Executes an UNLOAD command to s3 as a CSV with headers",
          "",
          "[Added Lines]",
          "32:     Execute an UNLOAD command to s3 as a CSV with headers.",
          "",
          "---------------"
        ],
        "airflow/providers/databricks/hooks/databricks_base.py||airflow/providers/databricks/hooks/databricks_base.py": [
          "File: airflow/providers/databricks/hooks/databricks_base.py -> airflow/providers/databricks/hooks/databricks_base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "169:     @staticmethod",
          "170:     def _parse_host(host: str) -> str:",
          "171:         \"\"\"",
          "175:         For example -- when users supply ``https://xx.cloud.databricks.com`` as the",
          "176:         host, we must strip out the protocol to get the host.::",
          "",
          "[Removed Lines]",
          "172:         The purpose of this function is to be robust to improper connections",
          "173:         settings provided by users, specifically in the host field.",
          "",
          "[Added Lines]",
          "172:         This function is resistant to incorrect connection settings provided by users, in the host field.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "196:     def _get_retry_object(self) -> Retrying:",
          "197:         \"\"\"",
          "199:         :return: instance of Retrying class",
          "200:         \"\"\"",
          "201:         return Retrying(**self.retry_args)",
          "203:     def _a_get_retry_object(self) -> AsyncRetrying:",
          "204:         \"\"\"",
          "206:         :return: instance of AsyncRetrying class",
          "207:         \"\"\"",
          "208:         return AsyncRetrying(**self.retry_args)",
          "210:     def _get_aad_token(self, resource: str) -> str:",
          "211:         \"\"\"",
          "213:         :param resource: resource to issue token to",
          "214:         :return: AAD token, or raise an exception",
          "215:         \"\"\"",
          "",
          "[Removed Lines]",
          "198:         Instantiates a retry object",
          "205:         Instantiates an async retry object",
          "212:         Function to get AAD token for given resource. Supports managed identity or service principal auth",
          "",
          "[Added Lines]",
          "197:         Instantiate a retry object.",
          "204:         Instantiate an async retry object.",
          "211:         Function to get AAD token for given resource.",
          "213:         Supports managed identity or service principal auth.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "341:     def _get_aad_headers(self) -> dict:",
          "342:         \"\"\"",
          "344:         :return: dictionary with filled AAD headers",
          "345:         \"\"\"",
          "346:         headers = {}",
          "",
          "[Removed Lines]",
          "343:         Fills AAD headers if necessary (SPN is outside of the workspace)",
          "",
          "[Added Lines]",
          "344:         Fill AAD headers if necessary (SPN is outside of the workspace).",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "369:     @staticmethod",
          "370:     def _is_aad_token_valid(aad_token: dict) -> bool:",
          "371:         \"\"\"",
          "373:         :param aad_token: dict with properties of AAD token",
          "374:         :return: true if token is valid, false otherwise",
          "375:         :rtype: bool",
          "",
          "[Removed Lines]",
          "372:         Utility function to check AAD token hasn't expired yet",
          "",
          "[Added Lines]",
          "373:         Utility function to check AAD token hasn't expired yet.",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "382:     @staticmethod",
          "383:     def _check_azure_metadata_service() -> None:",
          "384:         \"\"\"",
          "386:         https://docs.microsoft.com/en-us/azure/virtual-machines/linux/instance-metadata-service",
          "387:         \"\"\"",
          "388:         try:",
          "",
          "[Removed Lines]",
          "385:         Check for Azure Metadata Service",
          "",
          "[Added Lines]",
          "387:         Check for Azure Metadata Service.",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "472:         wrap_http_errors: bool = True,",
          "473:     ):",
          "474:         \"\"\"",
          "477:         :param endpoint_info: Tuple of method and endpoint",
          "478:         :param json: Parameters for this API call.",
          "",
          "[Removed Lines]",
          "475:         Utility function to perform an API call with retries",
          "",
          "[Added Lines]",
          "477:         Utility function to perform an API call with retries.",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "618: class _TokenAuth(AuthBase):",
          "619:     \"\"\"",
          "621:     magic function.",
          "622:     \"\"\"",
          "",
          "[Removed Lines]",
          "620:     Helper class for requests Auth field. AuthBase requires you to implement the __call__",
          "",
          "[Added Lines]",
          "622:     Helper class for requests Auth field.",
          "624:     AuthBase requires you to implement the ``__call__``",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/hooks/cloud_memorystore.py||airflow/providers/google/cloud/hooks/cloud_memorystore.py": [
          "File: airflow/providers/google/cloud/hooks/cloud_memorystore.py -> airflow/providers/google/cloud/hooks/cloud_memorystore.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "90:     @staticmethod",
          "91:     def _append_label(instance: Instance, key: str, val: str) -> Instance:",
          "92:         \"\"\"",
          "95:         Labels must fit the regex ``[a-z]([-a-z0-9]*[a-z0-9])?`` (current",
          "96:          airflow version string follows semantic versioning spec: x.y.z).",
          "",
          "[Removed Lines]",
          "93:         Append labels to provided Instance type",
          "",
          "[Added Lines]",
          "93:         Append labels to provided Instance type.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "275:         metadata: Sequence[Tuple[str, str]] = (),",
          "276:     ):",
          "277:         \"\"\"",
          "278:         Initiates a failover of the primary node to current replica node for a specific STANDARD tier Cloud",
          "279:         Memorystore for Redis instance.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "278:         Failover of the primary node to current replica node.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "392:         metadata: Sequence[Tuple[str, str]] = (),",
          "393:     ):",
          "394:         \"\"\"",
          "398:         :param location: The location of the Cloud Memorystore instance (for example europe-west1)",
          "",
          "[Removed Lines]",
          "395:         Lists all Redis instances owned by a project in either the specified location (region) or all",
          "396:         locations.",
          "",
          "[Added Lines]",
          "397:         List Redis instances owned by a project at the specified location (region) or all locations.",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "528:     @staticmethod",
          "529:     def _append_label(instance: cloud_memcache.Instance, key: str, val: str) -> cloud_memcache.Instance:",
          "530:         \"\"\"",
          "533:         Labels must fit the regex ``[a-z]([-a-z0-9]*[a-z0-9])?`` (current",
          "534:          airflow version string follows semantic versioning spec: x.y.z).",
          "",
          "[Removed Lines]",
          "531:         Append labels to provided Instance type",
          "",
          "[Added Lines]",
          "532:         Append labels to provided Instance type.",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "752:         metadata: Sequence[Tuple[str, str]] = (),",
          "753:     ):",
          "754:         \"\"\"",
          "758:         :param location: The location of the Cloud Memorystore instance (for example europe-west1)",
          "",
          "[Removed Lines]",
          "755:         Lists all Memcached instances owned by a project in either the specified location (region) or all",
          "756:         locations.",
          "",
          "[Added Lines]",
          "756:         List Memcached instances owned by a project at the specified location (region) or all locations.",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "851:         metadata: Sequence[Tuple[str, str]] = (),",
          "852:     ):",
          "853:         \"\"\"",
          "858:         :param update_mask: Required. Mask of fields to update.",
          "859:             If a dict is provided, it must be of the same form as the protobuf message",
          "",
          "[Removed Lines]",
          "854:         Updates the defined Memcached Parameters for an existing Instance. This method only stages the",
          "855:             parameters, it must be followed by apply_parameters to apply the parameters to nodes of",
          "856:             the Memcached Instance.",
          "",
          "[Added Lines]",
          "854:         Update the defined Memcached Parameters for an existing Instance.",
          "856:         This method only stages the parameters, it must be followed by apply_parameters",
          "857:         to apply the parameters to nodes of the Memcached Instance.",
          "",
          "---------------"
        ],
        "airflow/providers/mysql/hooks/mysql.py||airflow/providers/mysql/hooks/mysql.py": [
          "File: airflow/providers/mysql/hooks/mysql.py -> airflow/providers/mysql/hooks/mysql.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "62:     def set_autocommit(self, conn: MySQLConnectionTypes, autocommit: bool) -> None:",
          "63:         \"\"\"",
          "67:         :param conn: connection to set autocommit setting",
          "68:         :param autocommit: autocommit setting",
          "",
          "[Removed Lines]",
          "64:         The MySQLdb (mysqlclient) client uses an `autocommit` method rather",
          "65:         than an `autocommit` property to set the autocommit setting",
          "",
          "[Added Lines]",
          "64:         Set *autocommit*.",
          "67:         property, so we need to override this to support it.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "76:     def get_autocommit(self, conn: MySQLConnectionTypes) -> bool:",
          "77:         \"\"\"",
          "81:         :param conn: connection to get autocommit setting from.",
          "82:         :return: connection autocommit setting",
          "",
          "[Removed Lines]",
          "78:         The MySQLdb (mysqlclient) client uses a `get_autocommit` method",
          "79:         rather than an `autocommit` property to get the autocommit setting",
          "",
          "[Added Lines]",
          "80:         Whether *autocommit* is active.",
          "83:         property, so we need to override this to support it.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "147:     def get_conn(self) -> MySQLConnectionTypes:",
          "148:         \"\"\"",
          "149:         Establishes a connection to a mysql database",
          "150:         by extracting the connection configuration from the Airflow connection.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "153:         Connection to a MySQL database.",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "174:         raise ValueError('Unknown MySQL client name provided!')",
          "176:     def bulk_load(self, table: str, tmp_file: str) -> None:",
          "178:         conn = self.get_conn()",
          "179:         cur = conn.cursor()",
          "180:         cur.execute(",
          "",
          "[Removed Lines]",
          "177:         \"\"\"Loads a tab-delimited file into a database table\"\"\"",
          "",
          "[Added Lines]",
          "183:         \"\"\"Load a tab-delimited file into a database table.\"\"\"",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "187:         conn.close()",
          "189:     def bulk_dump(self, table: str, tmp_file: str) -> None:",
          "191:         conn = self.get_conn()",
          "192:         cur = conn.cursor()",
          "193:         cur.execute(",
          "",
          "[Removed Lines]",
          "190:         \"\"\"Dumps a database table into a tab-delimited file\"\"\"",
          "",
          "[Added Lines]",
          "196:         \"\"\"Dump a database table into a tab-delimited file.\"\"\"",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "202:     @staticmethod",
          "203:     def _serialize_cell(cell: object, conn: Optional[Connection] = None) -> object:",
          "204:         \"\"\"",
          "205:         The package MySQLdb converts an argument to a literal",
          "206:         when passing those separately to execute. Hence, this method does nothing.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "211:         Convert argument to a literal.",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "215:     def get_iam_token(self, conn: Connection) -> Tuple[str, int]:",
          "216:         \"\"\"",
          "217:         Uses AWSHook to retrieve a temporary password to connect to MySQL",
          "218:         Port is required. If none is provided, default 3306 is used",
          "219:         \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "225:         Retrieve a temporary password to connect to MySQL.",
          "",
          "---------------"
        ],
        "airflow/providers/qubole/sensors/qubole.py||airflow/providers/qubole/sensors/qubole.py": [
          "File: airflow/providers/qubole/sensors/qubole.py -> airflow/providers/qubole/sensors/qubole.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: class QuboleSensor(BaseSensorOperator):",
          "34:     template_fields: Sequence[str] = ('data', 'qubole_conn_id')",
          "",
          "[Removed Lines]",
          "32:     \"\"\"Base class for all Qubole Sensors\"\"\"",
          "",
          "[Added Lines]",
          "32:     \"\"\"Base class for all Qubole Sensors.\"\"\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "69: class QuboleFileSensor(QuboleSensor):",
          "70:     \"\"\"",
          "74:     .. seealso::",
          "75:         For more information on how to use this sensor, take a look at the guide:",
          "",
          "[Removed Lines]",
          "71:     Wait for a file or folder to be present in cloud storage",
          "72:     and check for its presence via QDS APIs",
          "",
          "[Added Lines]",
          "71:     Wait for a file or folder to be present in cloud storage.",
          "73:     Check for file or folder presence via QDS APIs.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "93: class QubolePartitionSensor(QuboleSensor):",
          "94:     \"\"\"",
          "98:     .. seealso::",
          "99:         For more information on how to use this sensor, take a look at the guide:",
          "",
          "[Removed Lines]",
          "95:     Wait for a Hive partition to show up in QHS (Qubole Hive Service)",
          "96:     and check for its presence via QDS APIs",
          "",
          "[Added Lines]",
          "96:     Wait for a Hive partition to show up in QHS (Qubole Hive Service).",
          "98:     Check for Hive partition presence via QDS APIs.",
          "",
          "---------------"
        ],
        "airflow/utils/process_utils.py||airflow/utils/process_utils.py": [
          "File: airflow/utils/process_utils.py -> airflow/utils/process_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: #",
          "20: import errno",
          "21: import logging",
          "22: import os",
          "",
          "[Removed Lines]",
          "19: \"\"\"Utilities for running or stopping processes\"\"\"",
          "",
          "[Added Lines]",
          "19: \"\"\"Utilities for running or stopping processes.\"\"\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "56:     timeout: int = DEFAULT_TIME_TO_WAIT_AFTER_SIGTERM,",
          "57: ) -> Dict[int, int]:",
          "58:     \"\"\"",
          "59:     Tries really hard to terminate all processes in the group (including grandchildren). Will send",
          "60:     sig (SIGTERM) to the process group of pid. If any process is alive after timeout",
          "61:     a SIGKILL will be send.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "59:     Send sig (SIGTERM) to the process group of pid.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "159: def execute_in_subprocess(cmd: List[str], cwd: Optional[str] = None) -> None:",
          "160:     \"\"\"",
          "162:     :param cmd: command and arguments to run",
          "163:     :param cwd: Current working directory passed to the Popen constructor",
          "164:     \"\"\"",
          "",
          "[Removed Lines]",
          "161:     Execute a process and stream output to logger",
          "",
          "[Added Lines]",
          "163:     Execute a process and stream output to logger.",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "168: def execute_in_subprocess_with_kwargs(cmd: List[str], **kwargs) -> None:",
          "169:     \"\"\"",
          "172:     :param cmd: command and arguments to run",
          "",
          "[Removed Lines]",
          "170:     Execute a process and stream output to logger",
          "",
          "[Added Lines]",
          "172:     Execute a process and stream output to logger.",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "191: def execute_interactive(cmd: List[str], **kwargs) -> None:",
          "192:     \"\"\"",
          "193:     Runs the new command as a subprocess and ensures that the terminal's state is restored to its original",
          "194:     state after the process is completed e.g. if the subprocess hides the cursor, it will be restored after",
          "195:     the process is completed.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "195:     Run the new command as a subprocess.",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "271: @contextmanager",
          "272: def patch_environ(new_env_variables: Dict[str, str]) -> Generator[None, None, None]:",
          "273:     \"\"\"",
          "276:     :param new_env_variables: Environment variables to set",
          "277:     \"\"\"",
          "278:     current_env_state = {key: os.environ.get(key) for key in new_env_variables.keys()}",
          "",
          "[Removed Lines]",
          "274:     Sets environment variables in context. After leaving the context, it restores its original state.",
          "",
          "[Added Lines]",
          "278:     Set environment variables in context.",
          "280:     After leaving the context, it restores its original state.",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "317: def set_new_process_group() -> None:",
          "318:     \"\"\"",
          "320:     That makes it easy to kill all sub-process of this at the OS-level,",
          "321:     rather than having to iterate the child processes.",
          "322:     If current process spawn by system call ``exec()`` than keep current process group",
          "",
          "[Removed Lines]",
          "319:     Tries to set current process to a new process group",
          "",
          "[Added Lines]",
          "324:     Try to set current process to a new process group.",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ea22657e3f243491c137a12f74fe2c5b8be0e29a",
      "candidate_info": {
        "commit_hash": "ea22657e3f243491c137a12f74fe2c5b8be0e29a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ea22657e3f243491c137a12f74fe2c5b8be0e29a",
        "files": [
          "airflow/datasets/manager.py",
          "tests/datasets/test_manager.py"
        ],
        "message": "Don't blow up when a task produces a dataset that is not consumed. (#26257)\n\nIf you have a dataset outlet on a task, and no DAG was recorded as\nconsuming that dataset it failed with a null value constraint violation\nin the db\n\n(cherry picked from commit fe82e9609522fa302d3eaf7193689718571d4af4)",
        "before_after_code_files": [
          "airflow/datasets/manager.py||airflow/datasets/manager.py",
          "tests/datasets/test_manager.py||tests/datasets/test_manager.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/datasets/manager.py||airflow/datasets/manager.py": [
          "File: airflow/datasets/manager.py -> airflow/datasets/manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "61:                 extra=extra,",
          "62:             )",
          "63:         )",
          "66:     def _queue_dagruns(self, dataset: DatasetModel, session: Session) -> None:",
          "67:         # Possible race condition: if multiple dags or multiple (usually",
          "",
          "[Removed Lines]",
          "64:         self._queue_dagruns(dataset_model, session)",
          "",
          "[Added Lines]",
          "64:         if dataset_model.consuming_dags:",
          "65:             self._queue_dagruns(dataset_model, session)",
          "66:         session.flush()",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "91:             except exc.IntegrityError:",
          "92:                 self.log.debug(\"Skipping record %s\", item, exc_info=True)",
          "96:     def _postgres_queue_dagruns(self, dataset: DatasetModel, session: Session) -> None:",
          "97:         from sqlalchemy.dialects.postgresql import insert",
          "",
          "[Removed Lines]",
          "94:         session.flush()",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "101:             stmt,",
          "102:             [{'target_dag_id': target_dag.dag_id} for target_dag in dataset.consuming_dags],",
          "103:         )",
          "107: def resolve_dataset_manager() -> \"DatasetManager\":",
          "",
          "[Removed Lines]",
          "104:         session.flush()",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/datasets/test_manager.py||tests/datasets/test_manager.py": [
          "File: tests/datasets/test_manager.py -> tests/datasets/test_manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "80:         # Ensure we've created a dataset",
          "81:         assert session.query(DatasetEvent).filter_by(dataset_id=dsm.id).count() == 1",
          "82:         assert session.query(DatasetDagRunQueue).count() == 2",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "84:     def test_register_dataset_change_no_downstreams(self, session, mock_task_instance):",
          "85:         dsem = DatasetManager()",
          "87:         ds = Dataset(uri=\"never_consumed\")",
          "88:         dsm = DatasetModel(uri=\"never_consumed\")",
          "89:         session.add(dsm)",
          "90:         session.flush()",
          "92:         dsem.register_dataset_change(task_instance=mock_task_instance, dataset=ds, session=session)",
          "94:         # Ensure we've created a dataset",
          "95:         assert session.query(DatasetEvent).filter_by(dataset_id=dsm.id).count() == 1",
          "96:         assert session.query(DatasetDagRunQueue).count() == 0",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "553f7c9d399dc88bb04925b06b2f691840f7ab78",
      "candidate_info": {
        "commit_hash": "553f7c9d399dc88bb04925b06b2f691840f7ab78",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/553f7c9d399dc88bb04925b06b2f691840f7ab78",
        "files": [
          ".github/workflows/ci.yml",
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py"
        ],
        "message": "Make \"quick build\" actually test build time (#26251)\n\n* Make \"quick build\" actually test build time\n\nThere was a \"string\" instead of array passed in case of --max-time\ncommand flag which caused an error - regardless of the time it\ntook to run the build, but it was also ignored because the\njob was run within \"continue-on-error\" cache push job.\n\nThis PR fixes it to use proper \"exit\" command and separates it out\nto a job where quick build failure will be noticed (not in PRs but\nin the merge-run)\n\n(cherry picked from commit 63da96b4331bdc70ec49e360e87610960b31371c)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/developer_commands.py||dev/breeze/src/airflow_breeze/commands/developer_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/developer_commands.py -> dev/breeze/src/airflow_breeze/commands/developer_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "184:         force_build=force_build,",
          "185:         db_reset=db_reset,",
          "186:         include_mypy_volume=include_mypy_volume,",
          "188:         answer=answer,",
          "189:         image_tag=image_tag,",
          "190:         platform=platform,",
          "",
          "[Removed Lines]",
          "187:         extra_args=extra_args if not max_time else \"exit\",",
          "",
          "[Added Lines]",
          "187:         extra_args=extra_args if not max_time else [\"exit\"],",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1ea86de21886e977cab2fcda7f87b6ebc91b0e3f",
      "candidate_info": {
        "commit_hash": "1ea86de21886e977cab2fcda7f87b6ebc91b0e3f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/1ea86de21886e977cab2fcda7f87b6ebc91b0e3f",
        "files": [
          ".pre-commit-config.yaml",
          "airflow/models/baseoperator.py",
          "airflow/providers/google/cloud/hooks/gcs.py"
        ],
        "message": "Work around pyupgrade edge cases (#26384)\n\n(cherry picked from commit 9444d9789bc88e1063d81d28e219446b2251c0e1)",
        "before_after_code_files": [
          "airflow/models/baseoperator.py||airflow/models/baseoperator.py",
          "airflow/providers/google/cloud/hooks/gcs.py||airflow/providers/google/cloud/hooks/gcs.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/baseoperator.py||airflow/models/baseoperator.py": [
          "File: airflow/models/baseoperator.py -> airflow/models/baseoperator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1738:         task.set_downstream(to_tasks)",
          "1741: @attr.s(auto_attribs=True)",
          "1742: class BaseOperatorLink(metaclass=ABCMeta):",
          "1743:     \"\"\"Abstract base class that defines how we get an operator link.\"\"\"",
          "1746:     \"\"\"",
          "1747:     This property will be used by Airflow Plugins to find the Operators to which you want",
          "1748:     to assign this Operator Link",
          "",
          "[Removed Lines]",
          "1745:     operators: ClassVar[List[Type[BaseOperator]]] = []",
          "",
          "[Added Lines]",
          "1741: # pyupgrade assumes all type annotations can be lazily evaluated, but this is",
          "1742: # not the case for attrs-decorated classes, since cattrs needs to evaluate the",
          "1743: # annotation expressions at runtime, and Python before 3.9.0 does not lazily",
          "1744: # evaluate those. Putting the expression in a top-level assignment statement",
          "1745: # communicates this runtime requirement to pyupgrade.",
          "1746: BaseOperatorClassList = List[Type[BaseOperator]]",
          "1753:     operators: ClassVar[BaseOperatorClassList] = []",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/hooks/gcs.py||airflow/providers/google/cloud/hooks/gcs.py": [
          "File: airflow/providers/google/cloud/hooks/gcs.py -> airflow/providers/google/cloud/hooks/gcs.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "19: \"\"\"This module contains a Google Cloud Storage hook.\"\"\"",
          "20: import functools",
          "21: import gzip as gz",
          "22: import os",
          "",
          "[Removed Lines]",
          "18: #",
          "",
          "[Added Lines]",
          "19: from __future__ import annotations",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "28: from io import BytesIO",
          "29: from os import path",
          "30: from tempfile import NamedTemporaryFile",
          "45: from urllib.parse import urlparse",
          "47: from google.api_core.exceptions import NotFound",
          "",
          "[Removed Lines]",
          "31: from typing import (",
          "32:     IO,",
          "33:     Callable,",
          "34:     Generator,",
          "35:     List,",
          "36:     Optional,",
          "37:     Sequence,",
          "38:     Set,",
          "39:     Tuple,",
          "40:     TypeVar,",
          "41:     Union,",
          "42:     cast,",
          "43:     overload,",
          "44: )",
          "",
          "[Added Lines]",
          "32: from typing import IO, Callable, Generator, Sequence, TypeVar, cast, overload",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "60: RT = TypeVar('RT')",
          "61: T = TypeVar(\"T\", bound=Callable)",
          "63: # Use default timeout from google-cloud-storage",
          "64: DEFAULT_TIMEOUT = 60",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "51: # GCSHook has a method named 'list' (to junior devs: please don't do this), so",
          "52: # we need to create an alias to prevent Mypy being confused.",
          "53: List = list",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "81:     def _wrapper(func: T):",
          "82:         @functools.wraps(func)",
          "84:             if args:",
          "85:                 raise AirflowException(",
          "86:                     \"You must use keyword arguments in this methods rather than positional\"",
          "",
          "[Removed Lines]",
          "83:         def _inner_wrapper(self: \"GCSHook\", *args, **kwargs) -> RT:",
          "",
          "[Added Lines]",
          "75:         def _inner_wrapper(self: GCSHook, *args, **kwargs) -> RT:",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "139:     connection.",
          "140:     \"\"\"",
          "144:     def __init__(",
          "145:         self,",
          "146:         gcp_conn_id: str = \"google_cloud_default\",",
          "149:     ) -> None:",
          "150:         super().__init__(",
          "151:             gcp_conn_id=gcp_conn_id,",
          "",
          "[Removed Lines]",
          "142:     _conn = None  # type: Optional[storage.Client]",
          "147:         delegate_to: Optional[str] = None,",
          "148:         impersonation_chain: Optional[Union[str, Sequence[str]]] = None,",
          "",
          "[Added Lines]",
          "134:     _conn: storage.Client | None = None",
          "139:         delegate_to: str | None = None,",
          "140:         impersonation_chain: str | Sequence[str] | None = None,",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "166:         self,",
          "167:         source_bucket: str,",
          "168:         source_object: str,",
          "171:     ) -> None:",
          "172:         \"\"\"",
          "173:         Copies an object from a bucket to another, with renaming if requested.",
          "",
          "[Removed Lines]",
          "169:         destination_bucket: Optional[str] = None,",
          "170:         destination_object: Optional[str] = None,",
          "",
          "[Added Lines]",
          "161:         destination_bucket: str | None = None,",
          "162:         destination_object: str | None = None,",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "215:         source_bucket: str,",
          "216:         source_object: str,",
          "217:         destination_bucket: str,",
          "219:     ) -> None:",
          "220:         \"\"\"",
          "221:         Has the same functionality as copy, except that will work on files",
          "",
          "[Removed Lines]",
          "218:         destination_object: Optional[str] = None,",
          "",
          "[Added Lines]",
          "210:         destination_object: str | None = None,",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "270:         bucket_name: str,",
          "271:         object_name: str,",
          "272:         filename: None = None,",
          "276:     ) -> bytes:",
          "277:         ...",
          "",
          "[Removed Lines]",
          "273:         chunk_size: Optional[int] = None,",
          "274:         timeout: Optional[int] = DEFAULT_TIMEOUT,",
          "275:         num_max_attempts: Optional[int] = 1,",
          "",
          "[Added Lines]",
          "265:         chunk_size: int | None = None,",
          "266:         timeout: int | None = DEFAULT_TIMEOUT,",
          "267:         num_max_attempts: int | None = 1,",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "282:         bucket_name: str,",
          "283:         object_name: str,",
          "284:         filename: str,",
          "288:     ) -> str:",
          "289:         ...",
          "",
          "[Removed Lines]",
          "285:         chunk_size: Optional[int] = None,",
          "286:         timeout: Optional[int] = DEFAULT_TIMEOUT,",
          "287:         num_max_attempts: Optional[int] = 1,",
          "",
          "[Added Lines]",
          "277:         chunk_size: int | None = None,",
          "278:         timeout: int | None = DEFAULT_TIMEOUT,",
          "279:         num_max_attempts: int | None = 1,",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "292:         self,",
          "293:         bucket_name: str,",
          "294:         object_name: str,",
          "300:         \"\"\"",
          "301:         Downloads a file from Google Cloud Storage.",
          "",
          "[Removed Lines]",
          "295:         filename: Optional[str] = None,",
          "296:         chunk_size: Optional[int] = None,",
          "297:         timeout: Optional[int] = DEFAULT_TIMEOUT,",
          "298:         num_max_attempts: Optional[int] = 1,",
          "299:     ) -> Union[str, bytes]:",
          "",
          "[Added Lines]",
          "287:         filename: str | None = None,",
          "288:         chunk_size: int | None = None,",
          "289:         timeout: int | None = DEFAULT_TIMEOUT,",
          "290:         num_max_attempts: int | None = 1,",
          "291:     ) -> str | bytes:",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "351:         self,",
          "352:         bucket_name: str,",
          "353:         object_name: str,",
          "357:     ) -> bytes:",
          "358:         \"\"\"",
          "359:         Downloads a file from Google Cloud Storage.",
          "",
          "[Removed Lines]",
          "354:         chunk_size: Optional[int] = None,",
          "355:         timeout: Optional[int] = DEFAULT_TIMEOUT,",
          "356:         num_max_attempts: Optional[int] = 1,",
          "",
          "[Added Lines]",
          "346:         chunk_size: int | None = None,",
          "347:         timeout: int | None = DEFAULT_TIMEOUT,",
          "348:         num_max_attempts: int | None = 1,",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "383:     def provide_file(",
          "384:         self,",
          "385:         bucket_name: str = PROVIDE_BUCKET,",
          "389:     ) -> Generator[IO[bytes], None, None]:",
          "390:         \"\"\"",
          "391:         Downloads the file to a temporary directory and returns a file handle",
          "",
          "[Removed Lines]",
          "386:         object_name: Optional[str] = None,",
          "387:         object_url: Optional[str] = None,",
          "388:         dir: Optional[str] = None,",
          "",
          "[Added Lines]",
          "378:         object_name: str | None = None,",
          "379:         object_url: str | None = None,",
          "380:         dir: str | None = None,",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "412:     def provide_file_and_upload(",
          "413:         self,",
          "414:         bucket_name: str = PROVIDE_BUCKET,",
          "417:     ) -> Generator[IO[bytes], None, None]:",
          "418:         \"\"\"",
          "419:         Creates temporary file, returns a file handle and uploads the files content",
          "",
          "[Removed Lines]",
          "415:         object_name: Optional[str] = None,",
          "416:         object_url: Optional[str] = None,",
          "",
          "[Added Lines]",
          "407:         object_name: str | None = None,",
          "408:         object_url: str | None = None,",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "440:         self,",
          "441:         bucket_name: str,",
          "442:         object_name: str,",
          "446:         gzip: bool = False,",
          "447:         encoding: str = 'utf-8',",
          "450:         num_max_attempts: int = 1,",
          "452:     ) -> None:",
          "453:         \"\"\"",
          "454:         Uploads a local file or file data as string or bytes to Google Cloud Storage.",
          "",
          "[Removed Lines]",
          "443:         filename: Optional[str] = None,",
          "444:         data: Optional[Union[str, bytes]] = None,",
          "445:         mime_type: Optional[str] = None,",
          "448:         chunk_size: Optional[int] = None,",
          "449:         timeout: Optional[int] = DEFAULT_TIMEOUT,",
          "451:         metadata: Optional[dict] = None,",
          "",
          "[Added Lines]",
          "435:         filename: str | None = None,",
          "436:         data: str | bytes | None = None,",
          "437:         mime_type: str | None = None,",
          "440:         chunk_size: int | None = None,",
          "441:         timeout: int | None = DEFAULT_TIMEOUT,",
          "443:         metadata: dict | None = None,",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "683:         except NotFound:",
          "684:             self.log.info(\"Bucket %s not exists\", bucket_name)",
          "687:         \"\"\"",
          "688:         List all objects from the bucket with the give string prefix in name",
          "",
          "[Removed Lines]",
          "686:     def list(self, bucket_name, versions=None, max_results=None, prefix=None, delimiter=None) -> list:",
          "",
          "[Added Lines]",
          "678:     def list(self, bucket_name, versions=None, max_results=None, prefix=None, delimiter=None) -> List:",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "730:         bucket_name: str,",
          "731:         timespan_start: datetime,",
          "732:         timespan_end: datetime,",
          "737:     ) -> List[str]:",
          "738:         \"\"\"",
          "739:         List all objects from the bucket with the give string prefix in name that were",
          "",
          "[Removed Lines]",
          "733:         versions: Optional[bool] = None,",
          "734:         max_results: Optional[int] = None,",
          "735:         prefix: Optional[str] = None,",
          "736:         delimiter: Optional[str] = None,",
          "",
          "[Added Lines]",
          "725:         versions: bool | None = None,",
          "726:         max_results: int | None = None,",
          "727:         prefix: str | None = None,",
          "728:         delimiter: str | None = None,",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "838:     def create_bucket(",
          "839:         self,",
          "840:         bucket_name: str,",
          "842:         storage_class: str = 'MULTI_REGIONAL',",
          "843:         location: str = 'US',",
          "846:     ) -> str:",
          "847:         \"\"\"",
          "848:         Creates a new bucket. Google Cloud Storage uses a flat namespace, so",
          "",
          "[Removed Lines]",
          "841:         resource: Optional[dict] = None,",
          "844:         project_id: Optional[str] = None,",
          "845:         labels: Optional[dict] = None,",
          "",
          "[Added Lines]",
          "833:         resource: dict | None = None,",
          "836:         project_id: str | None = None,",
          "837:         labels: dict | None = None,",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "900:         return bucket.id",
          "902:     def insert_bucket_acl(",
          "904:     ) -> None:",
          "905:         \"\"\"",
          "906:         Creates a new ACL entry on the specified bucket_name.",
          "",
          "[Removed Lines]",
          "903:         self, bucket_name: str, entity: str, role: str, user_project: Optional[str] = None",
          "",
          "[Added Lines]",
          "895:         self, bucket_name: str, entity: str, role: str, user_project: str | None = None",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "933:         object_name: str,",
          "934:         entity: str,",
          "935:         role: str,",
          "938:     ) -> None:",
          "939:         \"\"\"",
          "940:         Creates a new ACL entry on the specified object.",
          "",
          "[Removed Lines]",
          "936:         generation: Optional[int] = None,",
          "937:         user_project: Optional[str] = None,",
          "",
          "[Added Lines]",
          "928:         generation: int | None = None,",
          "929:         user_project: str | None = None,",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "968:         self.log.info('A new ACL entry created for object: %s in bucket: %s', object_name, bucket_name)",
          "971:         \"\"\"",
          "972:         Composes a list of existing object into a new object in the same storage bucket_name",
          "",
          "[Removed Lines]",
          "970:     def compose(self, bucket_name: str, source_objects: List, destination_object: str) -> None:",
          "",
          "[Added Lines]",
          "962:     def compose(self, bucket_name: str, source_objects: List[str], destination_object: str) -> None:",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "1002:         self,",
          "1003:         source_bucket: str,",
          "1004:         destination_bucket: str,",
          "1007:         recursive: bool = True,",
          "1008:         allow_overwrite: bool = False,",
          "1009:         delete_extra_files: bool = False,",
          "",
          "[Removed Lines]",
          "1005:         source_object: Optional[str] = None,",
          "1006:         destination_object: Optional[str] = None,",
          "",
          "[Added Lines]",
          "997:         source_object: str | None = None,",
          "998:         destination_object: str | None = None,",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "1104:         self.log.info(\"Synchronization finished.\")",
          "1106:     def _calculate_sync_destination_path(",
          "1108:     ) -> str:",
          "1109:         return (",
          "1110:             path.join(destination_object, blob.name[source_object_prefix_len:])",
          "",
          "[Removed Lines]",
          "1107:         self, blob: storage.Blob, destination_object: Optional[str], source_object_prefix_len: int",
          "",
          "[Added Lines]",
          "1099:         self, blob: storage.Blob, destination_object: str | None, source_object_prefix_len: int",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "1116:     def _prepare_sync_plan(",
          "1117:         source_bucket: storage.Bucket,",
          "1118:         destination_bucket: storage.Bucket,",
          "1121:         recursive: bool,",
          "1123:         # Calculate the number of characters that remove from the name, because they contain information",
          "1124:         # about the parent's path",
          "1125:         source_object_prefix_len = len(source_object) if source_object else 0",
          "",
          "[Removed Lines]",
          "1119:         source_object: Optional[str],",
          "1120:         destination_object: Optional[str],",
          "1122:     ) -> Tuple[Set[storage.Blob], Set[storage.Blob], Set[storage.Blob]]:",
          "",
          "[Added Lines]",
          "1111:         source_object: str | None,",
          "1112:         destination_object: str | None,",
          "1114:     ) -> tuple[set[storage.Blob], set[storage.Blob], set[storage.Blob]]:",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "1139:         # Determine objects to copy and delete",
          "1140:         to_copy = source_names - destination_names",
          "1141:         to_delete = destination_names - source_names",
          "1144:         # Find names that are in both buckets",
          "1145:         names_to_check = source_names.intersection(destination_names)",
          "1147:         # Compare objects based on crc32",
          "1148:         for current_name in names_to_check:",
          "1149:             source_blob = source_names_index[current_name]",
          "",
          "[Removed Lines]",
          "1142:         to_copy_blobs = {source_names_index[a] for a in to_copy}  # type: Set[storage.Blob]",
          "1143:         to_delete_blobs = {destination_names_index[a] for a in to_delete}  # type: Set[storage.Blob]",
          "1146:         to_rewrite_blobs = set()  # type: Set[storage.Blob]",
          "",
          "[Added Lines]",
          "1134:         to_copy_blobs: set[storage.Blob] = {source_names_index[a] for a in to_copy}",
          "1135:         to_delete_blobs: set[storage.Blob] = {destination_names_index[a] for a in to_delete}",
          "1138:         to_rewrite_blobs: set[storage.Blob] = set()",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "1164:     return len(blob) == 0 or blob.endswith('/')",
          "1168:     \"\"\"",
          "1169:     Given a Google Cloud Storage URL (gs://<bucket>/<blob>), returns a",
          "1170:     tuple containing the corresponding bucket and blob.",
          "",
          "[Removed Lines]",
          "1167: def _parse_gcs_url(gsurl: str) -> Tuple[str, str]:",
          "",
          "[Added Lines]",
          "1159: def _parse_gcs_url(gsurl: str) -> tuple[str, str]:",
          "",
          "---------------"
        ]
      }
    }
  ]
}