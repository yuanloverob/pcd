{
  "cve_id": "CVE-2023-50944",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows an authenticated user to access the source code of a DAG to which they don't have access.\u00a0This vulnerability is considered low since it requires an authenticated user to exploit it. Users are recommended to upgrade to version 2.8.1, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "8d76538d6e105947272b000581c6fabec20146b1",
  "patch_info": {
    "commit_hash": "8d76538d6e105947272b000581c6fabec20146b1",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/8d76538d6e105947272b000581c6fabec20146b1",
    "files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ],
    "message": "Check DAG read permission before accessing DAG code (#36257)\n\n(cherry picked from commit 30ea37e0d247ce54c2d25b115e807fdb0074d795)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py||airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_source_endpoint.py -> airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "17: from __future__ import annotations",
      "19: from http import HTTPStatus",
      "21: from flask import Response, current_app, request",
      "22: from itsdangerous import BadSignature, URLSafeSerializer",
      "24: from airflow.api_connexion import security",
      "26: from airflow.api_connexion.schemas.dag_source_schema import dag_source_schema",
      "27: from airflow.auth.managers.models.resource_details import DagAccessEntity",
      "28: from airflow.models.dagcode import DagCode",
      "31: @security.requires_access_dag(\"GET\", DagAccessEntity.CODE)",
      "33:     \"\"\"Get source code using file token.\"\"\"",
      "34:     secret_key = current_app.config[\"SECRET_KEY\"]",
      "35:     auth_s = URLSafeSerializer(secret_key)",
      "36:     try:",
      "37:         path = auth_s.loads(file_token)",
      "39:     except (BadSignature, FileNotFoundError):",
      "40:         raise NotFound(\"Dag source not found\")",
      "",
      "[Removed Lines]",
      "25: from airflow.api_connexion.exceptions import NotFound",
      "32: def get_dag_source(*, file_token: str) -> Response:",
      "38:         dag_source = DagCode.code(path)",
      "",
      "[Added Lines]",
      "20: from typing import TYPE_CHECKING",
      "26: from airflow.api_connexion.exceptions import NotFound, PermissionDenied",
      "28: from airflow.api_connexion.security import get_readable_dags",
      "30: from airflow.models.dag import DagModel",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "34: if TYPE_CHECKING:",
      "35:     from sqlalchemy.orm import Session",
      "39: @provide_session",
      "40: def get_dag_source(*, file_token: str, session: Session = NEW_SESSION) -> Response:",
      "46:         dag_ids = session.query(DagModel.dag_id).filter(DagModel.fileloc == path).all()",
      "47:         readable_dags = get_readable_dags()",
      "48:         # Check if user has read access to all the DAGs defined in the file",
      "49:         if any(dag_id[0] not in readable_dags for dag_id in dag_ids):",
      "50:             raise PermissionDenied()",
      "51:         dag_source = DagCode.code(path, session=session)",
      "",
      "---------------"
    ],
    "airflow/models/dagcode.py||airflow/models/dagcode.py": [
      "File: airflow/models/dagcode.py -> airflow/models/dagcode.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "177:         return cls.code(fileloc)",
      "179:     @classmethod",
      "181:         \"\"\"Return source code for this DagCode object.",
      "183:         :return: source code as string",
      "184:         \"\"\"",
      "187:     @staticmethod",
      "188:     def _get_code_from_file(fileloc):",
      "",
      "[Removed Lines]",
      "180:     def code(cls, fileloc) -> str:",
      "185:         return cls._get_code_from_db(fileloc)",
      "",
      "[Added Lines]",
      "180:     @provide_session",
      "181:     def code(cls, fileloc, session: Session = NEW_SESSION) -> str:",
      "186:         return cls._get_code_from_db(fileloc, session)",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_source_endpoint.py -> tests/api_connexion/endpoints/test_dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35: ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))",
      "36: EXAMPLE_DAG_FILE = os.path.join(\"airflow\", \"example_dags\", \"example_bash_operator.py\")",
      "39: @pytest.fixture(scope=\"module\")",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "37: EXAMPLE_DAG_ID = \"example_bash_operator\"",
      "38: TEST_DAG_ID = \"latest_only\"",
      "39: NOT_READABLE_DAG_ID = \"latest_only_with_trigger\"",
      "40: TEST_MULTIPLE_DAGS_ID = \"dataset_produces_1\"",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "45:         role_name=\"Test\",",
      "46:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_CODE)],  # type: ignore",
      "47:     )",
      "48:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "50:     yield app",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "52:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "53:         TEST_DAG_ID,",
      "54:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "55:     )",
      "56:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "57:         EXAMPLE_DAG_ID,",
      "58:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "59:     )",
      "60:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "61:         TEST_MULTIPLE_DAGS_ID,",
      "62:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "63:     )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "80:     def test_should_respond_200_text(self, url_safe_serializer):",
      "81:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "82:         dagbag.sync_to_db()",
      "87:         response = self.client.get(",
      "88:             url, headers={\"Accept\": \"text/plain\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "89:         )",
      "",
      "[Removed Lines]",
      "83:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "84:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "86:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "99:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "100:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "102:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "95:     def test_should_respond_200_json(self, url_safe_serializer):",
      "96:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "97:         dagbag.sync_to_db()",
      "102:         response = self.client.get(",
      "103:             url, headers={\"Accept\": \"application/json\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "104:         )",
      "",
      "[Removed Lines]",
      "98:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "99:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "101:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "114:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "115:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "117:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "110:     def test_should_respond_406(self, url_safe_serializer):",
      "111:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "112:         dagbag.sync_to_db()",
      "116:         response = self.client.get(",
      "117:             url, headers={\"Accept\": \"image/webp\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "118:         )",
      "",
      "[Removed Lines]",
      "113:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "115:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "129:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "131:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "151:             environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"},",
      "152:         )",
      "153:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "171:     def test_should_respond_403_not_readable(self, url_safe_serializer):",
      "172:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "173:         dagbag.sync_to_db()",
      "174:         dag: DAG = dagbag.dags[NOT_READABLE_DAG_ID]",
      "176:         response = self.client.get(",
      "177:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "178:             headers={\"Accept\": \"text/plain\"},",
      "179:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "180:         )",
      "181:         read_dag = self.client.get(",
      "182:             f\"/api/v1/dags/{NOT_READABLE_DAG_ID}\",",
      "183:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "184:         )",
      "185:         assert response.status_code == 403",
      "186:         assert read_dag.status_code == 403",
      "188:     def test_should_respond_403_some_dags_not_readable_in_the_file(self, url_safe_serializer):",
      "189:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "190:         dagbag.sync_to_db()",
      "191:         dag: DAG = dagbag.dags[TEST_MULTIPLE_DAGS_ID]",
      "193:         response = self.client.get(",
      "194:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "195:             headers={\"Accept\": \"text/plain\"},",
      "196:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "197:         )",
      "199:         read_dag = self.client.get(",
      "200:             f\"/api/v1/dags/{TEST_MULTIPLE_DAGS_ID}\",",
      "201:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "202:         )",
      "203:         assert response.status_code == 403",
      "204:         assert read_dag.status_code == 200",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "56ecbffc40183834b941cd84b861f997e580a7d3",
      "candidate_info": {
        "commit_hash": "56ecbffc40183834b941cd84b861f997e580a7d3",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/56ecbffc40183834b941cd84b861f997e580a7d3",
        "files": [
          "airflow/serialization/serializers/datetime.py",
          "tests/serialization/serializers/test_serializers.py"
        ],
        "message": "Stop serializing timezone-naive datetime to timezone-aware dateime with UTC tz (#36379)\n\n(cherry picked from commit 69f556dd136598662db9e87478584a3c96362fc9)",
        "before_after_code_files": [
          "airflow/serialization/serializers/datetime.py||airflow/serialization/serializers/datetime.py",
          "tests/serialization/serializers/test_serializers.py||tests/serialization/serializers/test_serializers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/serialization/serializers/datetime.py||airflow/serialization/serializers/datetime.py": [
          "File: airflow/serialization/serializers/datetime.py -> airflow/serialization/serializers/datetime.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24:     serialize as serialize_timezone,",
          "25: )",
          "26: from airflow.utils.module_loading import qualname",
          "29: if TYPE_CHECKING:",
          "30:     import datetime",
          "",
          "[Removed Lines]",
          "27: from airflow.utils.timezone import convert_to_utc, is_naive",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "46:     if isinstance(o, datetime):",
          "47:         qn = qualname(o)",
          "53:         return {TIMESTAMP: o.timestamp(), TIMEZONE: tz}, qn, __version__, True",
          "",
          "[Removed Lines]",
          "48:         if is_naive(o):",
          "49:             o = convert_to_utc(o)",
          "51:         tz = serialize_timezone(o.tzinfo)",
          "",
          "[Added Lines]",
          "48:         tz = serialize_timezone(o.tzinfo) if o.tzinfo else None",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "83:             else:",
          "84:                 tz = timezone(data[TIMEZONE])",
          "85:         else:",
          "88:     if classname == qualname(datetime.datetime) and isinstance(data, dict):",
          "89:         return datetime.datetime.fromtimestamp(float(data[TIMESTAMP]), tz=tz)",
          "",
          "[Removed Lines]",
          "86:             tz = deserialize_timezone(data[TIMEZONE][1], data[TIMEZONE][2], data[TIMEZONE][0])",
          "",
          "[Added Lines]",
          "83:             tz = (",
          "84:                 deserialize_timezone(data[TIMEZONE][1], data[TIMEZONE][2], data[TIMEZONE][0])",
          "85:                 if data[TIMEZONE]",
          "86:                 else None",
          "87:             )",
          "",
          "---------------"
        ],
        "tests/serialization/serializers/test_serializers.py||tests/serialization/serializers/test_serializers.py": [
          "File: tests/serialization/serializers/test_serializers.py -> tests/serialization/serializers/test_serializers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "84:         d = deserialize(s)",
          "85:         assert i.timestamp() == d.timestamp()",
          "87:     def test_deserialize_datetime_v1(self):",
          "88:         s = {",
          "89:             \"__classname__\": \"pendulum.datetime.DateTime\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "87:         i = datetime.datetime.now()",
          "88:         s = serialize(i)",
          "89:         d = deserialize(s)",
          "90:         assert i.timestamp() == d.timestamp()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4b4fd21037640c10ef99e635ef6d8eec98839e74",
      "candidate_info": {
        "commit_hash": "4b4fd21037640c10ef99e635ef6d8eec98839e74",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4b4fd21037640c10ef99e635ef6d8eec98839e74",
        "files": [
          "airflow/utils/log/file_task_handler.py"
        ],
        "message": "Restore function scoped httpx import in file_task_handler for perf (#36753)\n\n(cherry picked from commit c792b259699550d06054984f1643bb62df5ea37d)",
        "before_after_code_files": [
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: from typing import TYPE_CHECKING, Any, Callable, Iterable",
          "30: from urllib.parse import urljoin",
          "33: import pendulum",
          "35: from airflow.configuration import conf",
          "",
          "[Removed Lines]",
          "32: import httpx",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "81: def _fetch_logs_from_service(url, log_relative_path):",
          "82:     from airflow.utils.jwt_signer import JWTSigner",
          "84:     timeout = conf.getint(\"webserver\", \"log_fetch_timeout_sec\", fallback=None)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "81:     # Import occurs in function scope for perf. Ref: https://github.com/apache/airflow/pull/21438",
          "82:     import httpx",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "557:                 messages.append(f\"Found logs served from host {url}\")",
          "558:                 logs.append(response.text)",
          "559:         except Exception as e:",
          "561:                 messages.append(self.inherits_from_empty_operator_log_message)",
          "562:             else:",
          "563:                 messages.append(f\"Could not read served logs: {e}\")",
          "",
          "[Removed Lines]",
          "560:             if isinstance(e, httpx.UnsupportedProtocol) and ti.task.inherits_from_empty_operator is True:",
          "",
          "[Added Lines]",
          "562:             from httpx import UnsupportedProtocol",
          "564:             if isinstance(e, UnsupportedProtocol) and ti.task.inherits_from_empty_operator is True:",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "fcd235777ec8df4dd7d43ce56cfec3762c79dbaf",
      "candidate_info": {
        "commit_hash": "fcd235777ec8df4dd7d43ce56cfec3762c79dbaf",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/fcd235777ec8df4dd7d43ce56cfec3762c79dbaf",
        "files": [
          "airflow/utils/log/file_processor_handler.py",
          "tests/utils/log/test_file_processor_handler.py"
        ],
        "message": "Create latest log dir symlink as relative link (#36019)\n\nSo `logs/**/*` is self-contained and the symlink doesn't break for\nexporting, build artifact, etc\n\n(cherry picked from commit 17e91727b29448e46470a8dd4b5909a0bdf38eb2)",
        "before_after_code_files": [
          "airflow/utils/log/file_processor_handler.py||airflow/utils/log/file_processor_handler.py",
          "tests/utils/log/test_file_processor_handler.py||tests/utils/log/test_file_processor_handler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/log/file_processor_handler.py||airflow/utils/log/file_processor_handler.py": [
          "File: airflow/utils/log/file_processor_handler.py -> airflow/utils/log/file_processor_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "117:         log_directory = self._get_log_directory()",
          "118:         latest_log_directory_path = os.path.join(self.base_log_folder, \"latest\")",
          "119:         if os.path.isdir(log_directory):",
          "120:             try:",
          "121:                 # if symlink exists but is stale, update it",
          "122:                 if os.path.islink(latest_log_directory_path):",
          "124:                         os.unlink(latest_log_directory_path)",
          "126:                 elif os.path.isdir(latest_log_directory_path) or os.path.isfile(latest_log_directory_path):",
          "127:                     logging.warning(",
          "128:                         \"%s already exists as a dir/file. Skip creating symlink.\", latest_log_directory_path",
          "129:                     )",
          "130:                 else:",
          "132:             except OSError:",
          "133:                 logging.warning(\"OSError while attempting to symlink the latest log directory\")",
          "",
          "[Removed Lines]",
          "123:                     if os.readlink(latest_log_directory_path) != log_directory:",
          "125:                         os.symlink(log_directory, latest_log_directory_path)",
          "131:                     os.symlink(log_directory, latest_log_directory_path)",
          "",
          "[Added Lines]",
          "120:             rel_link_target = Path(log_directory).relative_to(Path(latest_log_directory_path).parent)",
          "124:                     if os.path.realpath(latest_log_directory_path) != log_directory:",
          "126:                         os.symlink(rel_link_target, latest_log_directory_path)",
          "132:                     os.symlink(rel_link_target, latest_log_directory_path)",
          "",
          "---------------"
        ],
        "tests/utils/log/test_file_processor_handler.py||tests/utils/log/test_file_processor_handler.py": [
          "File: tests/utils/log/test_file_processor_handler.py -> tests/utils/log/test_file_processor_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "80:         with time_machine.travel(date1, tick=False):",
          "81:             handler.set_context(filename=os.path.join(self.dag_dir, \"log1\"))",
          "82:             assert os.path.islink(link)",
          "84:             assert os.path.exists(os.path.join(link, \"log1\"))",
          "86:         with time_machine.travel(date2, tick=False):",
          "87:             handler.set_context(filename=os.path.join(self.dag_dir, \"log2\"))",
          "88:             assert os.path.islink(link)",
          "90:             assert os.path.exists(os.path.join(link, \"log2\"))",
          "92:     def test_symlink_latest_log_directory_exists(self):",
          "",
          "[Removed Lines]",
          "83:             assert os.path.basename(os.readlink(link)) == date1",
          "89:             assert os.path.basename(os.readlink(link)) == date2",
          "",
          "[Added Lines]",
          "83:             assert os.path.basename(os.path.realpath(link)) == date1",
          "89:             assert os.path.basename(os.path.realpath(link)) == date2",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1dc0b2ec8401533a87c655e97fe37f04a1319a7e",
      "candidate_info": {
        "commit_hash": "1dc0b2ec8401533a87c655e97fe37f04a1319a7e",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/1dc0b2ec8401533a87c655e97fe37f04a1319a7e",
        "files": [
          "airflow/config_templates/config.yml",
          "airflow/www/app.py"
        ],
        "message": "Add flask config: `MAX_CONTENT_LENGTH` (#36401)\n\n(cherry picked from commit 84063e74fb2b0dd3a8308ff4170cb3e7236cf51e)",
        "before_after_code_files": [
          "airflow/www/app.py||airflow/www/app.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/app.py||airflow/www/app.py": [
          "File: airflow/www/app.py -> airflow/www/app.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "76:     flask_app.config[\"PERMANENT_SESSION_LIFETIME\"] = timedelta(minutes=settings.get_session_lifetime_config())",
          "78:     webserver_config = conf.get_mandatory_value(\"webserver\", \"config_file\")",
          "79:     # Enable customizations in webserver_config.py to be applied via Flask.current_app.",
          "80:     with flask_app.app_context():",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "78:     flask_app.config[\"MAX_CONTENT_LENGTH\"] = conf.getfloat(\"webserver\", \"allowed_payload_size\") * 1024 * 1024",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "842fd3b7fae758b53a2331f2bce87dc055bd174b",
      "candidate_info": {
        "commit_hash": "842fd3b7fae758b53a2331f2bce87dc055bd174b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/842fd3b7fae758b53a2331f2bce87dc055bd174b",
        "files": [
          "airflow/jobs/backfill_job_runner.py"
        ],
        "message": "Refactor _manage_executor_state by refreshing TIs in batch (#36418)\n\n* Refactor _manage_executor_state by refreshing TIs in batch\n\n* Use a short key without retry number\n\n(cherry picked from commit 9d45db9e2cca2ad04db72f7e0712c478e5a8e1f1)",
        "before_after_code_files": [
          "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py": [
          "File: airflow/jobs/backfill_job_runner.py -> airflow/jobs/backfill_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import attr",
          "24: import pendulum",
          "26: from sqlalchemy.exc import OperationalError",
          "27: from sqlalchemy.orm.session import make_transient",
          "28: from tabulate import tabulate",
          "",
          "[Removed Lines]",
          "25: from sqlalchemy import select, update",
          "",
          "[Added Lines]",
          "25: from sqlalchemy import select, tuple_, update",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "264:         :return: An iterable of expanded TaskInstance per MappedTask",
          "265:         \"\"\"",
          "266:         executor = self.job.executor",
          "270:             state, info = value",
          "272:                 self.log.warning(\"%s state %s not in running=%s\", key, state, running.values())",
          "273:                 continue",
          "278:             self.log.debug(\"Executor state: %s task %s\", state, ti)",
          "",
          "[Removed Lines]",
          "268:         # TODO: query all instead of refresh from db",
          "269:         for key, value in list(executor.get_event_buffer().items()):",
          "271:             if key not in running:",
          "275:             ti = running[key]",
          "276:             ti.refresh_from_db()",
          "",
          "[Added Lines]",
          "267:         # list of tuples (dag_id, task_id, execution_date, map_index) of running tasks in executor",
          "268:         buffered_events = list(executor.get_event_buffer().items())",
          "269:         running_tis_ids = [",
          "270:             (key.dag_id, key.task_id, key.run_id, key.map_index)",
          "271:             for key, _ in buffered_events",
          "272:             if key in running",
          "273:         ]",
          "274:         # list of TaskInstance of running tasks in executor (refreshed from db in batch)",
          "275:         refreshed_running_tis = session.scalars(",
          "276:             select(TaskInstance).where(",
          "277:                 tuple_(",
          "278:                     TaskInstance.dag_id,",
          "279:                     TaskInstance.task_id,",
          "280:                     TaskInstance.run_id,",
          "281:                     TaskInstance.map_index,",
          "282:                 ).in_(running_tis_ids)",
          "283:             )",
          "284:         ).all()",
          "285:         # dict of refreshed TaskInstance by key to easily find them",
          "286:         refreshed_running_tis_dict = {",
          "287:             (ti.dag_id, ti.task_id, ti.run_id, ti.map_index): ti for ti in refreshed_running_tis",
          "288:         }",
          "290:         for key, value in buffered_events:",
          "292:             ti_key = (key.dag_id, key.task_id, key.run_id, key.map_index)",
          "293:             if ti_key not in refreshed_running_tis_dict:",
          "297:             ti = refreshed_running_tis_dict[ti_key]",
          "",
          "---------------"
        ]
      }
    }
  ]
}