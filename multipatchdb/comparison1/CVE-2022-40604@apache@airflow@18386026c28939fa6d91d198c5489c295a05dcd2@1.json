{
  "cve_id": "CVE-2022-40604",
  "cve_desc": "In Apache Airflow 2.3.0 through 2.3.4, part of a url was unnecessarily formatted, allowing for possible information extraction.",
  "repo": "apache/airflow",
  "patch_hash": "18386026c28939fa6d91d198c5489c295a05dcd2",
  "patch_info": {
    "commit_hash": "18386026c28939fa6d91d198c5489c295a05dcd2",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/18386026c28939fa6d91d198c5489c295a05dcd2",
    "files": [
      "airflow/utils/log/file_task_handler.py"
    ],
    "message": "Properly build URL to retrieve logs independently from system (#26337)\n\nThe previous way of building the path depended on the OS path\nbut it was really used to build the URL so we should use\nurllib instead of os.path.join.",
    "before_after_code_files": [
      "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
    ]
  },
  "patch_diff": {
    "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
      "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "21: import warnings",
      "22: from pathlib import Path",
      "23: from typing import TYPE_CHECKING, Optional",
      "25: from airflow.configuration import AirflowConfigException, conf",
      "26: from airflow.exceptions import RemovedInAirflow3Warning",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "24: from urllib.parse import urljoin",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "194:         else:",
      "195:             import httpx",
      "199:             )",
      "200:             log += f\"*** Log file does not exist: {location}\\n\"",
      "201:             log += f\"*** Fetching from: {url}\\n\"",
      "",
      "[Removed Lines]",
      "197:             url = os.path.join(\"http://{ti.hostname}:{worker_log_server_port}/log\", log_relative_path).format(",
      "198:                 ti=ti, worker_log_server_port=conf.get('logging', 'WORKER_LOG_SERVER_PORT')",
      "",
      "[Added Lines]",
      "198:             url = urljoin(",
      "199:                 f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log\", log_relative_path",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "86d62d3c0398d0d842c9368c5832de92bc6ec4c5",
      "candidate_info": {
        "commit_hash": "86d62d3c0398d0d842c9368c5832de92bc6ec4c5",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/86d62d3c0398d0d842c9368c5832de92bc6ec4c5",
        "files": [
          "airflow/executors/kubernetes_executor.py",
          "airflow/utils/log/file_task_handler.py",
          "tests/executors/test_kubernetes_executor.py"
        ],
        "message": "Fix timestamp parse failure for k8s executor pod tailing (#31175)",
        "before_after_code_files": [
          "airflow/executors/kubernetes_executor.py||airflow/executors/kubernetes_executor.py",
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py",
          "tests/executors/test_kubernetes_executor.py||tests/executors/test_kubernetes_executor.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ],
          "candidate": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/executors/kubernetes_executor.py||airflow/executors/kubernetes_executor.py": [
          "File: airflow/executors/kubernetes_executor.py -> airflow/executors/kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "48: from airflow.kubernetes.pod_generator import PodGenerator",
          "49: from airflow.models.taskinstance import TaskInstance",
          "50: from airflow.utils.event_scheduler import EventScheduler",
          "52: from airflow.utils.session import NEW_SESSION, provide_session",
          "53: from airflow.utils.state import State, TaskInstanceState",
          "",
          "[Removed Lines]",
          "51: from airflow.utils.log.logging_mixin import LoggingMixin",
          "",
          "[Added Lines]",
          "51: from airflow.utils.log.logging_mixin import LoggingMixin, remove_escape_codes",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "795:             client = get_kube_client()",
          "798:             selector = PodGenerator.build_selector_for_k8s_executor_pod(",
          "799:                 dag_id=ti.dag_id,",
          "800:                 task_id=ti.task_id,",
          "",
          "[Removed Lines]",
          "797:             messages.append(f\"Trying to get logs (last 100 lines) from worker pod {ti.hostname}\")",
          "",
          "[Added Lines]",
          "797:             messages.append(f\"Attempting to fetch logs from pod {ti.hostname} through kube API\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "820:                 tail_lines=100,",
          "821:                 _preload_content=False,",
          "822:             )",
          "824:             for line in res:",
          "826:         except Exception as e:",
          "827:             messages.append(f\"Reading from k8s pod logs failed: {str(e)}\")",
          "828:         return messages, [\"\\n\".join(log)]",
          "",
          "[Removed Lines]",
          "825:                 log.append(line.decode())",
          "",
          "[Added Lines]",
          "824:                 log.append(remove_escape_codes(line.decode()))",
          "825:             if log:",
          "826:                 messages.append(\"Found logs through kube API\")",
          "",
          "---------------"
        ],
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "315:             if response:",
          "316:                 executor_messages, executor_logs = response",
          "317:             if executor_messages:",
          "319:         if not (remote_logs and ti.state not in State.unfinished):",
          "320:             # when finished, if we have remote logs, no need to check local",
          "321:             worker_log_full_path = Path(self.local_base, worker_log_rel_path)",
          "",
          "[Removed Lines]",
          "318:                 messages_list.extend(messages_list)",
          "",
          "[Added Lines]",
          "318:                 messages_list.extend(executor_messages)",
          "",
          "---------------"
        ],
        "tests/executors/test_kubernetes_executor.py||tests/executors/test_kubernetes_executor.py": [
          "File: tests/executors/test_kubernetes_executor.py -> tests/executors/test_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1140:         messages, logs = executor.get_task_log(ti=ti, try_number=1)",
          "1142:         mock_kube_client.read_namespaced_pod_log.assert_called_once()",
          "1144:         assert logs[0] == \"a_\\nb_\\nc_\"",
          "1146:         mock_kube_client.reset_mock()",
          "",
          "[Removed Lines]",
          "1143:         assert \"Trying to get logs (last 100 lines) from worker pod \" in messages",
          "",
          "[Added Lines]",
          "1143:         assert messages == [",
          "1144:             \"Attempting to fetch logs from pod  through kube API\",",
          "1145:             \"Found logs through kube API\",",
          "1146:         ]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1149:         messages, logs = executor.get_task_log(ti=ti, try_number=1)",
          "1150:         assert logs == [\"\"]",
          "1151:         assert messages == [",
          "1153:             \"Reading from k8s pod logs failed: error_fetching_pod_log\",",
          "1154:         ]",
          "",
          "[Removed Lines]",
          "1152:             \"Trying to get logs (last 100 lines) from worker pod \",",
          "",
          "[Added Lines]",
          "1155:             \"Attempting to fetch logs from pod  through kube API\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "061caff2862d9df078336dc94efa5a6915935b7e",
      "candidate_info": {
        "commit_hash": "061caff2862d9df078336dc94efa5a6915935b7e",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/061caff2862d9df078336dc94efa5a6915935b7e",
        "files": [
          "airflow/utils/log/file_task_handler.py",
          "tests/utils/test_log_handlers.py"
        ],
        "message": "Add unit test for log retrieval url (#26603)\n\n* Add unit test for log retrieval url\n\nAdded unit test to #26493",
        "before_after_code_files": [
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py",
          "tests/utils/test_log_handlers.py||tests/utils/test_log_handlers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ],
          "candidate": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "211:         else:",
          "212:             import httpx",
          "218:             log += f\"*** Log file does not exist: {location}\\n\"",
          "219:             log += f\"*** Fetching from: {url}\\n\"",
          "220:             try:",
          "",
          "[Removed Lines]",
          "214:             url = urljoin(",
          "215:                 f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log/\",",
          "216:                 log_relative_path,",
          "217:             )",
          "",
          "[Added Lines]",
          "214:             url = self._get_log_retrieval_url(ti, log_relative_path)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "267:         return log, {'end_of_log': end_of_log, 'log_pos': log_pos}",
          "269:     def read(self, task_instance, try_number=None, metadata=None):",
          "270:         \"\"\"",
          "271:         Read logs of given task instance from local machine.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "266:     @staticmethod",
          "267:     def _get_log_retrieval_url(ti: TaskInstance, log_relative_path: str) -> str:",
          "268:         url = urljoin(",
          "269:             f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log/\",",
          "270:             log_relative_path,",
          "271:         )",
          "272:         return url",
          "",
          "---------------"
        ],
        "tests/utils/test_log_handlers.py||tests/utils/test_log_handlers.py": [
          "File: tests/utils/test_log_handlers.py -> tests/utils/test_log_handlers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "251:         fth = FileTaskHandler(\"\")",
          "252:         rendered_filename = fth._render_filename(filename_rendering_ti, 42)",
          "253:         assert expected_filename == rendered_filename",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "256: class TestLogUrl:",
          "257:     def test_log_retrieval_valid(self, create_task_instance):",
          "258:         log_url_ti = create_task_instance(",
          "259:             dag_id=\"dag_for_testing_filename_rendering\",",
          "260:             task_id=\"task_for_testing_filename_rendering\",",
          "261:             run_type=DagRunType.SCHEDULED,",
          "262:             execution_date=DEFAULT_DATE,",
          "263:         )",
          "264:         log_url_ti.hostname = 'hostname'",
          "265:         url = FileTaskHandler._get_log_retrieval_url(log_url_ti, 'DYNAMIC_PATH')",
          "266:         assert url == \"http://hostname:8793/log/DYNAMIC_PATH\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cbfbf8b843f178de1e1aa1066e5ea3377a8de774",
      "candidate_info": {
        "commit_hash": "cbfbf8b843f178de1e1aa1066e5ea3377a8de774",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cbfbf8b843f178de1e1aa1066e5ea3377a8de774",
        "files": [
          "airflow/utils/log/file_task_handler.py",
          "tests/utils/test_log_handlers.py"
        ],
        "message": "Make live logs reading work for \"other\" k8s executors (#28213)",
        "before_after_code_files": [
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py",
          "tests/utils/test_log_handlers.py||tests/utils/test_log_handlers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ],
          "candidate": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "132:     def _read_grouped_logs(self):",
          "133:         return False",
          "135:     def _read(self, ti: TaskInstance, try_number: int, metadata: dict[str, Any] | None = None):",
          "136:         \"\"\"",
          "137:         Template method that contains custom logic of reading",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "135:     @staticmethod",
          "136:     def _should_check_k8s(queue):",
          "137:         \"\"\"",
          "138:         If the task is running through kubernetes executor, return True.",
          "140:         When logs aren't available locally, in this case we read from k8s pod logs.",
          "141:         \"\"\"",
          "142:         executor = conf.get(\"core\", \"executor\")",
          "143:         if executor == \"KubernetesExecutor\":",
          "144:             return True",
          "145:         elif executor == \"LocalKubernetesExecutor\":",
          "146:             if queue == conf.get(\"local_kubernetes_executor\", \"kubernetes_queue\"):",
          "147:                 return True",
          "148:         elif executor == \"CeleryKubernetesExecutor\":",
          "149:             if queue == conf.get(\"celery_kubernetes_executor\", \"kubernetes_queue\"):",
          "150:                 return True",
          "151:         return False",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "173:                 log = f\"*** Failed to load local log file: {location}\\n\"",
          "174:                 log += f\"*** {str(e)}\\n\"",
          "175:                 return log, {\"end_of_log\": True}",
          "177:             try:",
          "178:                 from airflow.kubernetes.kube_client import get_kube_client",
          "",
          "[Removed Lines]",
          "176:         elif conf.get(\"core\", \"executor\") == \"KubernetesExecutor\":",
          "",
          "[Added Lines]",
          "193:         elif self._should_check_k8s(ti.queue):",
          "",
          "---------------"
        ],
        "tests/utils/test_log_handlers.py||tests/utils/test_log_handlers.py": [
          "File: tests/utils/test_log_handlers.py -> tests/utils/test_log_handlers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import logging.config",
          "22: import os",
          "23: import re",
          "25: from airflow.config_templates.airflow_local_settings import DEFAULT_LOGGING_CONFIG",
          "26: from airflow.models import DAG, DagRun, TaskInstance",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24: from unittest.mock import patch",
          "26: import pytest",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "264:         log_url_ti.hostname = \"hostname\"",
          "265:         url = FileTaskHandler._get_log_retrieval_url(log_url_ti, \"DYNAMIC_PATH\")",
          "266:         assert url == \"http://hostname:8793/log/DYNAMIC_PATH\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "272: @pytest.mark.parametrize(",
          "273:     \"config, queue, expected\",",
          "274:     [",
          "275:         (dict(AIRFLOW__CORE__EXECUTOR=\"LocalExecutor\"), None, False),",
          "276:         (dict(AIRFLOW__CORE__EXECUTOR=\"LocalExecutor\"), \"kubernetes\", False),",
          "277:         (dict(AIRFLOW__CORE__EXECUTOR=\"KubernetesExecutor\"), None, True),",
          "278:         (dict(AIRFLOW__CORE__EXECUTOR=\"CeleryKubernetesExecutor\"), \"any\", False),",
          "279:         (dict(AIRFLOW__CORE__EXECUTOR=\"CeleryKubernetesExecutor\"), \"kubernetes\", True),",
          "280:         (",
          "281:             dict(",
          "282:                 AIRFLOW__CORE__EXECUTOR=\"CeleryKubernetesExecutor\",",
          "283:                 AIRFLOW__CELERY_KUBERNETES_EXECUTOR__KUBERNETES_QUEUE=\"hithere\",",
          "284:             ),",
          "285:             \"hithere\",",
          "286:             True,",
          "287:         ),",
          "288:         (dict(AIRFLOW__CORE__EXECUTOR=\"LocalKubernetesExecutor\"), \"any\", False),",
          "289:         (dict(AIRFLOW__CORE__EXECUTOR=\"LocalKubernetesExecutor\"), \"kubernetes\", True),",
          "290:         (",
          "291:             dict(",
          "292:                 AIRFLOW__CORE__EXECUTOR=\"LocalKubernetesExecutor\",",
          "293:                 AIRFLOW__LOCAL_KUBERNETES_EXECUTOR__KUBERNETES_QUEUE=\"hithere\",",
          "294:             ),",
          "295:             \"hithere\",",
          "296:             True,",
          "297:         ),",
          "298:     ],",
          "299: )",
          "300: def test__should_check_k8s(config, queue, expected):",
          "301:     with patch.dict(\"os.environ\", **config):",
          "302:         assert FileTaskHandler._should_check_k8s(queue) == expected",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c22fc000b6c0075429b9d1e51c9ee3d384141ff3",
      "candidate_info": {
        "commit_hash": "c22fc000b6c0075429b9d1e51c9ee3d384141ff3",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c22fc000b6c0075429b9d1e51c9ee3d384141ff3",
        "files": [
          "airflow/kubernetes/kubernetes_helper_functions.py",
          "airflow/kubernetes/pod_generator.py",
          "airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py",
          "airflow/utils/log/file_task_handler.py",
          "tests/kubernetes/test_kubernetes_helper_functions.py",
          "tests/kubernetes/test_pod_generator.py",
          "tests/utils/test_log_handlers.py"
        ],
        "message": "Use labels instead of pod name for pod log read in k8s exec (#28546)\n\nThis means we don't have to use ti.hostname as a proxy for pod name, and allows us to lift the 63 charcter limit, which was a consequence of getting pod name through hostname.",
        "before_after_code_files": [
          "airflow/kubernetes/kubernetes_helper_functions.py||airflow/kubernetes/kubernetes_helper_functions.py",
          "airflow/kubernetes/pod_generator.py||airflow/kubernetes/pod_generator.py",
          "airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py||airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py",
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py",
          "tests/kubernetes/test_kubernetes_helper_functions.py||tests/kubernetes/test_kubernetes_helper_functions.py",
          "tests/kubernetes/test_pod_generator.py||tests/kubernetes/test_pod_generator.py",
          "tests/utils/test_log_handlers.py||tests/utils/test_log_handlers.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ],
          "candidate": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/kubernetes/kubernetes_helper_functions.py||airflow/kubernetes/kubernetes_helper_functions.py": [
          "File: airflow/kubernetes/kubernetes_helper_functions.py -> airflow/kubernetes/kubernetes_helper_functions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "48:     dag_id: str | None = None,",
          "49:     task_id: str | None = None,",
          "52:     unique: bool = True,",
          "53: ) -> str:",
          "54:     \"\"\"",
          "55:     Generates unique pod ID given a dag_id and / or task_id.",
          "62:     :param dag_id: DAG ID",
          "63:     :param task_id: Task ID",
          "",
          "[Removed Lines]",
          "51:     max_length: int = 63,  # must be 63 for now, see below",
          "57:     Because of the way that the task log handler reads from running k8s executor pods,",
          "58:     we must keep pod name <= 63 characters.  The handler gets pod name from ti.hostname.",
          "59:     TI hostname is derived from the container hostname, which is truncated to 63 characters.",
          "60:     We could lift this limit by using label selectors instead of pod name to find the pod.",
          "",
          "[Added Lines]",
          "51:     max_length: int = 80,",
          "57:     The default of 80 for max length is somewhat arbitrary, mainly a balance between",
          "58:     content and not overwhelming terminal windows of reasonable width. The true",
          "59:     upper limit is 253, and this is enforced in construct_pod.",
          "",
          "---------------"
        ],
        "airflow/kubernetes/pod_generator.py||airflow/kubernetes/pod_generator.py": [
          "File: airflow/kubernetes/pod_generator.py -> airflow/kubernetes/pod_generator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "339:             base_containers[1:], client_containers[1:]",
          "340:         )",
          "343:     def construct_pod(",
          "344:         dag_id: str,",
          "345:         task_id: str,",
          "346:         pod_id: str,",
          "",
          "[Removed Lines]",
          "342:     @staticmethod",
          "",
          "[Added Lines]",
          "342:     @classmethod",
          "344:         cls,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "370:                 \"pod_id supplied is longer than 253 characters; truncating and adding unique suffix.\"",
          "371:             )",
          "372:             pod_id = add_pod_suffix(pod_name=pod_id, max_len=253)",
          "382:         try:",
          "383:             image = pod_override_object.spec.containers[0].image  # type: ignore",
          "384:             if not image:",
          "",
          "[Removed Lines]",
          "373:         if len(pod_id) > 63:",
          "374:             # because in task handler we get pod name from ti hostname (which truncates",
          "375:             # pod_id to 63 characters) we won't be able to find the pod unless it is <= 63 characters.",
          "376:             # our code creates pod names shorter than this so this warning should not normally be triggered.",
          "377:             warnings.warn(",
          "378:                 \"Supplied pod_id is longer than 63 characters. Due to implementation details, the webserver \"",
          "379:                 \"may not be able to stream logs while task is running. Please choose a shorter pod name.\"",
          "380:             )",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "391:             \"task_id\": task_id,",
          "392:             \"try_number\": str(try_number),",
          "393:         }",
          "402:         if map_index >= 0:",
          "403:             annotations[\"map_index\"] = str(map_index)",
          "405:         if date:",
          "406:             annotations[\"execution_date\"] = date.isoformat()",
          "408:         if run_id:",
          "409:             annotations[\"run_id\"] = run_id",
          "412:         dynamic_pod = k8s.V1Pod(",
          "413:             metadata=k8s.V1ObjectMeta(",
          "414:                 namespace=namespace,",
          "415:                 annotations=annotations,",
          "416:                 name=pod_id,",
          "418:             ),",
          "419:             spec=k8s.V1PodSpec(",
          "420:                 containers=[",
          "",
          "[Removed Lines]",
          "394:         labels = {",
          "395:             \"airflow-worker\": make_safe_label_value(scheduler_job_id),",
          "396:             \"dag_id\": make_safe_label_value(dag_id),",
          "397:             \"task_id\": make_safe_label_value(task_id),",
          "398:             \"try_number\": str(try_number),",
          "399:             \"airflow_version\": airflow_version.replace(\"+\", \"-\"),",
          "400:             \"kubernetes_executor\": \"True\",",
          "401:         }",
          "404:             labels[\"map_index\"] = str(map_index)",
          "407:             labels[\"execution_date\"] = datetime_to_label_safe_datestring(date)",
          "410:             labels[\"run_id\"] = make_safe_label_value(run_id)",
          "417:                 labels=labels,",
          "",
          "[Added Lines]",
          "398:                 labels=cls.build_labels_for_k8s_executor_pod(",
          "399:                     dag_id=dag_id,",
          "400:                     task_id=task_id,",
          "401:                     try_number=try_number,",
          "402:                     airflow_worker=scheduler_job_id,",
          "403:                     map_index=map_index,",
          "404:                     execution_date=date,",
          "405:                     run_id=run_id,",
          "406:                 ),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "448:         return pod",
          "450:     @staticmethod",
          "451:     def serialize_pod(pod: k8s.V1Pod) -> dict:",
          "452:         \"\"\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "439:     @classmethod",
          "440:     def build_selector_for_k8s_executor_pod(",
          "441:         cls,",
          "443:         dag_id,",
          "444:         task_id,",
          "445:         try_number,",
          "446:         map_index=None,",
          "447:         execution_date=None,",
          "448:         run_id=None,",
          "449:         airflow_worker=None,",
          "450:     ):",
          "451:         \"\"\"",
          "452:         Generate selector for kubernetes executor pod",
          "454:         :meta private:",
          "455:         \"\"\"",
          "456:         labels = cls.build_labels_for_k8s_executor_pod(",
          "457:             dag_id=dag_id,",
          "458:             task_id=task_id,",
          "459:             try_number=try_number,",
          "460:             map_index=map_index,",
          "461:             execution_date=execution_date,",
          "462:             run_id=run_id,",
          "463:             airflow_worker=airflow_worker,",
          "464:         )",
          "465:         label_strings = [f\"{label_id}={label}\" for label_id, label in sorted(labels.items())]",
          "466:         selector = \",\".join(label_strings)",
          "467:         if not airflow_worker:  # this filters out KPO pods even when we don't know the scheduler job id",
          "468:             selector += \",airflow-worker\"",
          "469:         return selector",
          "471:     @classmethod",
          "472:     def build_labels_for_k8s_executor_pod(",
          "473:         cls,",
          "475:         dag_id,",
          "476:         task_id,",
          "477:         try_number,",
          "478:         airflow_worker=None,",
          "479:         map_index=None,",
          "480:         execution_date=None,",
          "481:         run_id=None,",
          "482:     ):",
          "483:         \"\"\"",
          "484:         Generate labels for kubernetes executor pod",
          "486:         :meta private:",
          "487:         \"\"\"",
          "488:         labels = {",
          "489:             \"dag_id\": make_safe_label_value(dag_id),",
          "490:             \"task_id\": make_safe_label_value(task_id),",
          "491:             \"try_number\": str(try_number),",
          "492:             \"kubernetes_executor\": \"True\",",
          "493:             \"airflow_version\": airflow_version.replace(\"+\", \"-\"),",
          "494:         }",
          "495:         if airflow_worker is not None:",
          "496:             labels[\"airflow-worker\"] = make_safe_label_value(str(airflow_worker))",
          "497:         if map_index is not None and map_index >= 0:",
          "498:             labels[\"map_index\"] = str(map_index)",
          "499:         if execution_date:",
          "500:             labels[\"execution_date\"] = datetime_to_label_safe_datestring(execution_date)",
          "501:         if run_id:",
          "502:             labels[\"run_id\"] = make_safe_label_value(run_id)",
          "503:         return labels",
          "",
          "---------------"
        ],
        "airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py||airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py": [
          "File: airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py -> airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "92:     dag_id: str | None = None,",
          "93:     task_id: str | None = None,",
          "96:     unique: bool = True,",
          "97: ) -> str:",
          "98:     \"\"\"",
          "",
          "[Removed Lines]",
          "95:     max_length: int = 63,",
          "",
          "[Added Lines]",
          "95:     max_length: int = 80,",
          "",
          "---------------"
        ],
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import logging",
          "22: import os",
          "23: import warnings",
          "24: from pathlib import Path",
          "25: from typing import TYPE_CHECKING, Any",
          "26: from urllib.parse import urljoin",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24: from contextlib import suppress",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "191:                 log += f\"*** {str(e)}\\n\"",
          "192:                 return log, {\"end_of_log\": True}",
          "193:         elif self._should_check_k8s(ti.queue):",
          "199:             try:",
          "200:                 from airflow.kubernetes.kube_client import get_kube_client",
          "204:                 log += f\"*** Trying to get logs (last 100 lines) from worker pod {ti.hostname} ***\\n\\n\"",
          "207:                     namespace=namespace,",
          "208:                     container=\"base\",",
          "209:                     follow=False,",
          "",
          "[Removed Lines]",
          "194:             pod_override = ti.executor_config.get(\"pod_override\")",
          "195:             if pod_override and pod_override.metadata and pod_override.metadata.namespace:",
          "196:                 namespace = pod_override.metadata.namespace",
          "197:             else:",
          "198:                 namespace = conf.get(\"kubernetes_executor\", \"namespace\")",
          "202:                 kube_client = get_kube_client()",
          "205:                 res = kube_client.read_namespaced_pod_log(",
          "206:                     name=ti.hostname,",
          "",
          "[Added Lines]",
          "197:                 from airflow.kubernetes.pod_generator import PodGenerator",
          "199:                 client = get_kube_client()",
          "202:                 selector = PodGenerator.build_selector_for_k8s_executor_pod(",
          "203:                     dag_id=ti.dag_id,",
          "204:                     task_id=ti.task_id,",
          "205:                     try_number=ti.try_number,",
          "206:                     map_index=ti.map_index,",
          "207:                     run_id=ti.run_id,",
          "208:                     airflow_worker=ti.queued_by_job_id,",
          "209:                 )",
          "210:                 namespace = self._get_pod_namespace(ti)",
          "211:                 pod_list = client.list_namespaced_pod(",
          "212:                     namespace=namespace,",
          "213:                     label_selector=selector,",
          "214:                 ).items",
          "215:                 if not pod_list:",
          "216:                     raise RuntimeError(\"Cannot find pod for ti %s\", ti)",
          "217:                 elif len(pod_list) > 1:",
          "218:                     raise RuntimeError(\"Found multiple pods for ti %s: %s\", ti, pod_list)",
          "219:                 res = client.read_namespaced_pod_log(",
          "220:                     name=pod_list[0].metadata.name,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "273:         return log, {\"end_of_log\": end_of_log, \"log_pos\": log_pos}",
          "275:     @staticmethod",
          "276:     def _get_log_retrieval_url(ti: TaskInstance, log_relative_path: str) -> str:",
          "277:         url = urljoin(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "289:     @staticmethod",
          "290:     def _get_pod_namespace(ti: TaskInstance):",
          "291:         pod_override = ti.executor_config.get(\"pod_override\")",
          "292:         namespace = None",
          "293:         with suppress(Exception):",
          "294:             namespace = pod_override.metadata.namespace",
          "295:         return namespace or conf.get(\"kubernetes_executor\", \"namespace\", fallback=\"default\")",
          "",
          "---------------"
        ],
        "tests/kubernetes/test_kubernetes_helper_functions.py||tests/kubernetes/test_kubernetes_helper_functions.py": [
          "File: tests/kubernetes/test_kubernetes_helper_functions.py -> tests/kubernetes/test_kubernetes_helper_functions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "89:     def test_create_pod_id_dag_too_long_with_suffix(self, create_pod_id):",
          "90:         actual = create_pod_id(\"0\" * 254)",
          "93:         assert re.match(pod_name_regex, actual)",
          "95:     def test_create_pod_id_dag_too_long_non_unique(self, create_pod_id):",
          "96:         actual = create_pod_id(\"0\" * 254, unique=False)",
          "99:         assert re.match(pod_name_regex, actual)",
          "101:     @pytest.mark.parametrize(\"unique\", [True, False])",
          "",
          "[Removed Lines]",
          "91:         assert len(actual) == 63",
          "92:         assert re.match(r\"0{54}-[a-z0-9]{8}\", actual)",
          "97:         assert len(actual) == 63",
          "98:         assert re.match(r\"0{63}\", actual)",
          "",
          "[Added Lines]",
          "91:         assert len(actual) == 80",
          "92:         assert re.match(r\"0{71}-[a-z0-9]{8}\", actual)",
          "97:         assert len(actual) == 80",
          "98:         assert re.match(r\"0{80}\", actual)",
          "",
          "---------------"
        ],
        "tests/kubernetes/test_pod_generator.py||tests/kubernetes/test_pod_generator.py": [
          "File: tests/kubernetes/test_pod_generator.py -> tests/kubernetes/test_pod_generator.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: from unittest import mock",
          "23: from unittest.mock import MagicMock",
          "25: import pytest",
          "26: from dateutil import parser",
          "27: from kubernetes.client import ApiClient, models as k8s",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: import pendulum",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "38: )",
          "39: from airflow.kubernetes.secret import Secret",
          "42: class TestPodGenerator:",
          "43:     def setup_method(self):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42: now = pendulum.now(\"UTC\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "476:         result_dict = self.k8s_client.sanitize_for_serialization(result)",
          "477:         expected_dict = self.k8s_client.sanitize_for_serialization(expected)",
          "481:     def test_construct_pod_empty_executor_config(self):",
          "482:         path = sys.path[0] + \"/tests/kubernetes/pod_generator_base_with_secrets.yaml\"",
          "",
          "[Removed Lines]",
          "479:         assert expected_dict == result_dict",
          "",
          "[Added Lines]",
          "482:         assert result_dict == expected_dict",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "772:             PodGenerator()",
          "773:         PodGenerator(pod_template_file=\"tests/kubernetes/pod.yaml\")",
          "774:         PodGenerator(pod=k8s.V1Pod())",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "779:     @pytest.mark.parametrize(",
          "780:         \"extra, extra_expected\",",
          "781:         [",
          "782:             param(dict(), {}, id=\"base\"),",
          "783:             param(dict(airflow_worker=2), {\"airflow-worker\": \"2\"}, id=\"worker\"),",
          "784:             param(dict(map_index=2), {\"map_index\": \"2\"}, id=\"map_index\"),",
          "785:             param(dict(run_id=\"2\"), {\"run_id\": \"2\"}, id=\"run_id\"),",
          "786:             param(",
          "787:                 dict(execution_date=now),",
          "788:                 {\"execution_date\": datetime_to_label_safe_datestring(now)},",
          "789:                 id=\"date\",",
          "790:             ),",
          "791:             param(",
          "792:                 dict(airflow_worker=2, map_index=2, run_id=\"2\", execution_date=now),",
          "793:                 {",
          "794:                     \"airflow-worker\": \"2\",",
          "795:                     \"map_index\": \"2\",",
          "796:                     \"run_id\": \"2\",",
          "797:                     \"execution_date\": datetime_to_label_safe_datestring(now),",
          "798:                 },",
          "799:                 id=\"all\",",
          "800:             ),",
          "801:         ],",
          "802:     )",
          "803:     def test_build_labels_for_k8s_executor_pod(self, extra, extra_expected):",
          "804:         from airflow.version import version as airflow_version",
          "806:         kwargs = dict(",
          "807:             dag_id=\"dag*\",",
          "808:             task_id=\"task*\",",
          "809:             try_number=1,",
          "810:         )",
          "811:         expected = dict(",
          "812:             dag_id=\"dag-6b24921d4\",",
          "813:             task_id=\"task-b6aca8991\",",
          "814:             try_number=\"1\",",
          "815:             airflow_version=airflow_version,",
          "816:             kubernetes_executor=\"True\",",
          "817:         )",
          "818:         labels = PodGenerator.build_labels_for_k8s_executor_pod(**kwargs, **extra)",
          "819:         assert labels == {**expected, **extra_expected}",
          "820:         exp_selector = \",\".join([f\"{k}={v}\" for k, v in sorted(labels.items())])",
          "821:         if \"airflow_worker\" not in extra:",
          "822:             exp_selector += \",airflow-worker\"",
          "823:         assert PodGenerator.build_selector_for_k8s_executor_pod(**kwargs, **extra) == exp_selector",
          "",
          "---------------"
        ],
        "tests/utils/test_log_handlers.py||tests/utils/test_log_handlers.py": [
          "File: tests/utils/test_log_handlers.py -> tests/utils/test_log_handlers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import logging.config",
          "22: import os",
          "23: import re",
          "26: import pytest",
          "27: from kubernetes.client import models as k8s",
          "",
          "[Removed Lines]",
          "24: from unittest.mock import MagicMock, patch",
          "",
          "[Added Lines]",
          "24: from unittest.mock import patch",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "235:     def test_read_from_k8s_under_multi_namespace_mode(",
          "236:         self, mock_kube_client, pod_override, namespace_to_call",
          "237:     ):",
          "241:         def task_callable(ti):",
          "242:             ti.log.info(\"test\")",
          "245:         dagrun = dag.create_dagrun(",
          "246:             run_type=DagRunType.MANUAL,",
          "247:             state=State.RUNNING,",
          "248:             execution_date=DEFAULT_DATE,",
          "249:         )",
          "257:         ti = TaskInstance(task=task, run_id=dagrun.run_id)",
          "258:         ti.try_number = 3",
          "260:         logger = ti.log",
          "261:         ti.log.disabled = False",
          "266:         set_context(logger, ti)",
          "267:         ti.run(ignore_ti_state=True)",
          "269:         file_handler.read(ti, 3)",
          "274:             namespace=namespace_to_call,",
          "275:             container=\"base\",",
          "276:             follow=False,",
          "",
          "[Removed Lines]",
          "238:         mock_read_namespaced_pod_log = MagicMock()",
          "239:         mock_kube_client.return_value.read_namespaced_pod_log = mock_read_namespaced_pod_log",
          "244:         dag = DAG(\"dag_for_testing_file_task_handler\", start_date=DEFAULT_DATE)",
          "250:         executor_config_pod = pod_override",
          "251:         task = PythonOperator(",
          "252:             task_id=\"task_for_testing_file_log_handler\",",
          "253:             dag=dag,",
          "254:             python_callable=task_callable,",
          "255:             executor_config={\"pod_override\": executor_config_pod},",
          "256:         )",
          "263:         file_handler = next(",
          "264:             (handler for handler in logger.handlers if handler.name == FILE_TASK_HANDLER), None",
          "265:         )",
          "271:         # Check if kube_client.read_namespaced_pod_log() is called with the namespace we expect",
          "272:         mock_read_namespaced_pod_log.assert_called_once_with(",
          "273:             name=ti.hostname,",
          "",
          "[Added Lines]",
          "238:         mock_read_log = mock_kube_client.return_value.read_namespaced_pod_log",
          "239:         mock_list_pod = mock_kube_client.return_value.list_namespaced_pod",
          "244:         with DAG(\"dag_for_testing_file_task_handler\", start_date=DEFAULT_DATE) as dag:",
          "245:             task = PythonOperator(",
          "246:                 task_id=\"task_for_testing_file_log_handler\",",
          "247:                 python_callable=task_callable,",
          "248:                 executor_config={\"pod_override\": pod_override},",
          "249:             )",
          "261:         file_handler = next((h for h in logger.handlers if h.name == FILE_TASK_HANDLER), None)",
          "267:         # first we find pod name",
          "268:         mock_list_pod.assert_called_once()",
          "269:         actual_kwargs = mock_list_pod.call_args[1]",
          "270:         assert actual_kwargs[\"namespace\"] == namespace_to_call",
          "271:         actual_selector = actual_kwargs[\"label_selector\"]",
          "272:         assert re.match(",
          "273:             \",\".join(",
          "274:                 [",
          "275:                     \"airflow_version=.+?\",",
          "276:                     \"dag_id=dag_for_testing_file_task_handler\",",
          "277:                     \"kubernetes_executor=True\",",
          "278:                     \"run_id=manual__2016-01-01T0000000000-2b88d1d57\",",
          "279:                     \"task_id=task_for_testing_file_log_handler\",",
          "280:                     \"try_number=.+?\",",
          "281:                     \"airflow-worker\",",
          "282:                 ]",
          "283:             ),",
          "284:             actual_selector,",
          "285:         )",
          "287:         # then we read log",
          "288:         mock_read_log.assert_called_once_with(",
          "289:             name=mock_list_pod.return_value.items[0].metadata.name,",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "52560b87c991c9739791ca8419219b0d86debacd",
      "candidate_info": {
        "commit_hash": "52560b87c991c9739791ca8419219b0d86debacd",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/52560b87c991c9739791ca8419219b0d86debacd",
        "files": [
          "airflow/utils/log/file_task_handler.py"
        ],
        "message": "Fix proper joining of the path for logs retrieved from celery workers (#26493)\n\nThe change #26377 \"fixed\" the way how logs were retrieved from\nCelery, but it - unfortunately broke the retrieval eventually.\n\nThis PR should fix it.\n\nFixes: #26492",
        "before_after_code_files": [
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ],
          "candidate": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "212:             import httpx",
          "214:             url = urljoin(",
          "216:             )",
          "217:             log += f\"*** Log file does not exist: {location}\\n\"",
          "218:             log += f\"*** Fetching from: {url}\\n\"",
          "",
          "[Removed Lines]",
          "215:                 f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log\", log_relative_path",
          "",
          "[Added Lines]",
          "215:                 f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log/\",",
          "216:                 log_relative_path,",
          "",
          "---------------"
        ]
      }
    }
  ]
}