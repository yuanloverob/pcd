{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "7c19df6fb2f684a80ea3366fd365a6bbc13421b3",
      "candidate_info": {
        "commit_hash": "7c19df6fb2f684a80ea3366fd365a6bbc13421b3",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/7c19df6fb2f684a80ea3366fd365a6bbc13421b3",
        "files": [
          "project/SparkBuild.scala",
          "resource-managers/kubernetes/integration-tests/dev/dev-run-integration-tests.sh",
          "resource-managers/kubernetes/integration-tests/pom.xml",
          "resource-managers/kubernetes/integration-tests/src/test/java/org/apache/spark/deploy/k8s/integrationtest/YuniKornTag.java",
          "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/YuniKornSuite.scala"
        ],
        "message": "[SPARK-40302][K8S][TESTS] Add `YuniKornSuite`\n\nThis PR aims the followings.\n1. Add `YuniKornSuite` integration test suite which extends `KubernetesSuite` on Apache YuniKorn scheduler.\n2. Support `--default-exclude-tags` command to override `test.default.exclude.tags`.\n\nTo improve test coverage.\n\nNo. This is a test suite addition.\n\nSince this requires `Apache YuniKorn` installation, the test suite is disabled by default.\nSo, CI K8s integration test should pass without running this suite.\n\nIn order to run the tests, we need to override `test.default.exclude.tags` like the following.\n\n**SBT**\n```\n$ build/sbt -Psparkr -Pkubernetes -Pkubernetes-integration-tests \\\n-Dspark.kubernetes.test.deployMode=docker-desktop \"kubernetes-integration-tests/test\" \\\n-Dtest.exclude.tags=minikube,local \\\n-Dtest.default.exclude.tags=\n```\n\n**MAVEN**\n```\n$ dev/dev-run-integration-tests.sh --deploy-mode docker-desktop \\\n--exclude-tag minikube,local \\\n--default-exclude-tags ''\n```\n\nCloses #37753 from dongjoon-hyun/SPARK-40302.\n\nAuthored-by: Dongjoon Hyun <dongjoon@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit b2e38e16bfc547a62957e0a67085985b3c65d525)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "project/SparkBuild.scala||project/SparkBuild.scala",
          "resource-managers/kubernetes/integration-tests/dev/dev-run-integration-tests.sh||resource-managers/kubernetes/integration-tests/dev/dev-run-integration-tests.sh",
          "resource-managers/kubernetes/integration-tests/src/test/java/org/apache/spark/deploy/k8s/integrationtest/YuniKornTag.java||resource-managers/kubernetes/integration-tests/src/test/java/org/apache/spark/deploy/k8s/integrationtest/YuniKornTag.java",
          "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/YuniKornSuite.scala||resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/YuniKornSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "project/SparkBuild.scala||project/SparkBuild.scala": [
          "File: project/SparkBuild.scala -> project/SparkBuild.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1136: object TestSettings {",
          "1137:   import BuildCommons._",
          "1140:   lazy val settings = Seq (",
          "",
          "[Removed Lines]",
          "1138:   private val defaultExcludedTags = Seq(\"org.apache.spark.tags.ChromeUITest\")",
          "",
          "[Added Lines]",
          "1138:   private val defaultExcludedTags = Seq(\"org.apache.spark.tags.ChromeUITest\",",
          "1139:     \"org.apache.spark.deploy.k8s.integrationtest.YuniKornTag\")",
          "",
          "---------------"
        ],
        "resource-managers/kubernetes/integration-tests/dev/dev-run-integration-tests.sh||resource-managers/kubernetes/integration-tests/dev/dev-run-integration-tests.sh": [
          "File: resource-managers/kubernetes/integration-tests/dev/dev-run-integration-tests.sh -> resource-managers/kubernetes/integration-tests/dev/dev-run-integration-tests.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "37: CONTEXT=",
          "38: INCLUDE_TAGS=\"k8s\"",
          "39: EXCLUDE_TAGS=",
          "40: JAVA_VERSION=\"8\"",
          "41: BUILD_DEPENDENCIES_MVN_FLAG=\"-am\"",
          "42: HADOOP_PROFILE=\"hadoop-3\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "40: DEFAULT_EXCLUDE_TAGS=\"N/A\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "101:       EXCLUDE_TAGS=\"$2\"",
          "102:       shift",
          "103:       ;;",
          "104:     --base-image-name)",
          "105:       BASE_IMAGE_NAME=\"$2\"",
          "106:       shift",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "105:     --default-exclude-tags)",
          "106:       DEFAULT_EXCLUDE_TAGS=\"$2\"",
          "107:       shift",
          "108:       ;;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "180:   properties=( ${properties[@]} -Dtest.exclude.tags=$EXCLUDE_TAGS )",
          "181: fi",
          "183: BASE_IMAGE_NAME=${BASE_IMAGE_NAME:-spark}",
          "184: JVM_IMAGE_NAME=${JVM_IMAGE_NAME:-${BASE_IMAGE_NAME}}",
          "185: PYTHON_IMAGE_NAME=${PYTHON_IMAGE_NAME:-${BASE_IMAGE_NAME}-py}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "188: if [ \"$DEFAULT_EXCLUDE_TAGS\" != \"N/A\" ];",
          "189: then",
          "190:   properties=( ${properties[@]} -Dtest.default.exclude.tags=$DEFAULT_EXCLUDE_TAGS )",
          "191: fi",
          "",
          "---------------"
        ],
        "resource-managers/kubernetes/integration-tests/src/test/java/org/apache/spark/deploy/k8s/integrationtest/YuniKornTag.java||resource-managers/kubernetes/integration-tests/src/test/java/org/apache/spark/deploy/k8s/integrationtest/YuniKornTag.java": [
          "File: resource-managers/kubernetes/integration-tests/src/test/java/org/apache/spark/deploy/k8s/integrationtest/YuniKornTag.java -> resource-managers/kubernetes/integration-tests/src/test/java/org/apache/spark/deploy/k8s/integrationtest/YuniKornTag.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "17: package org.apache.spark.deploy.k8s.integrationtest;",
          "19: import java.lang.annotation.Retention;",
          "20: import java.lang.annotation.Target;",
          "21: import java.lang.annotation.RetentionPolicy;",
          "22: import java.lang.annotation.ElementType;",
          "24: @org.scalatest.TagAnnotation",
          "25: @Retention(RetentionPolicy.RUNTIME)",
          "26: @Target({ElementType.METHOD, ElementType.TYPE})",
          "27: public @interface YuniKornTag {}",
          "",
          "---------------"
        ],
        "resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/YuniKornSuite.scala||resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/YuniKornSuite.scala": [
          "File: resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/YuniKornSuite.scala -> resource-managers/kubernetes/integration-tests/src/test/scala/org/apache/spark/deploy/k8s/integrationtest/YuniKornSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "17: package org.apache.spark.deploy.k8s.integrationtest",
          "19: @YuniKornTag",
          "20: class YuniKornSuite extends KubernetesSuite {",
          "22:   override protected def setUpTest(): Unit = {",
          "23:     super.setUpTest()",
          "24:     sparkAppConf",
          "25:       .set(\"spark.kubernetes.scheduler.name\", \"yunikorn\")",
          "26:       .set(\"spark.kubernetes.driver.annotation.yunikorn.apache.org/app-id\", \"{{APP_ID}}\")",
          "27:       .set(\"spark.kubernetes.executor.annotation.yunikorn.apache.org/app-id\", \"{{APP_ID}}\")",
          "28:   }",
          "29: }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "00292543ab6b1f86ef920a4840dc23759e0b9e44",
      "candidate_info": {
        "commit_hash": "00292543ab6b1f86ef920a4840dc23759e0b9e44",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/00292543ab6b1f86ef920a4840dc23759e0b9e44",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManagerSuite.scala"
        ],
        "message": "[SPARK-38787][SS] Replace found value with non-null element in the remaining list for key and remove remaining null elements from values in keyWithIndexToValue store for stream-stream joins\n\n### What changes were proposed in this pull request?\n\nIn stream-stream joins, for removing old state (watermark by value), we call the `removeByValue` function with a removal condition. Within the iterator returned, if we find null at the end for matched value at non-last index, we are currently not removing and swapping the matched value. With this change, we will find the first non-null value from end and swap current index with that value and remove all elements from index + 1 to the end and then drop the last element as before.\n\n### Why are the changes needed?\n\nThis change fixes a bug where we were not replacing found/matching values for `removeByValue` when encountering nulls in the symmetric hash join code.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nAdded a unit test for this change with nulls added. Here is a sample output:\n```\nExecuting tests from //sql/core:org.apache.spark.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite-hive-2.3__hadoop-3.2\n-----------------------------------------------------------------------------\n2022-04-01 21:11:59,641 INFO  CodeGenerator - Code generated in 225.884757 ms\n2022-04-01 21:11:59,662 INFO  CodeGenerator - Code generated in 10.870786 ms\nRun starting. Expected test count is: 4\n\u2026\n===== TEST OUTPUT FOR o.a.s.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite: 'StreamingJoinStateManager V2 - all operations with nulls' =====\n\n2022-04-01 21:12:03,487 INFO  StateStore - State Store maintenance task started\n2022-04-01 21:12:03,508 INFO  CheckpointFileManager - Writing atomically to /tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292/0/0/left-keyToNumValues/_metadata/schema using temp file /tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292/0/0/left-keyToNumValues/_metadata/.schema.9bcc39c9-721e-4ee0-b369-fb4f516c4fd6.tmp\n2022-04-01 21:12:03,524 INFO  CheckpointFileManager - Renamed temp file /tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292/0/0/left-keyToNumValues/_metadata/.schema.9bcc39c9-721e-4ee0-b369-fb4f516c4fd6.tmp to /tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292/0/0/left-keyToNumValues/_metadata/schema\n2022-04-01 21:12:03,525 INFO  StateStore - Retrieved reference to StateStoreCoordinator: org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef374ccb9\n2022-04-01 21:12:03,525 INFO  StateStore - Reported that the loaded instance StateStoreProviderId(StateStoreId(/tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292,0,0,left-keyToNumValues),47925997-9891-4025-a36a-3e18bc758b50) is active\n2022-04-01 21:12:03,525 INFO  HDFSBackedStateStoreProvider - Retrieved version 0 of HDFSStateStoreProvider[id = (op=0,part=0),dir = /tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292/0/0/left-keyToNumValues] for update\n2022-04-01 21:12:03,525 INFO  SymmetricHashJoinStateManager$KeyToNumValuesStore - Loaded store StateStoreId(/tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292,0,0,left-keyToNumValues)\n2022-04-01 21:12:03,541 INFO  CheckpointFileManager - Writing atomically to /tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292/0/0/left-keyWithIndexToValue/_metadata/schema using temp file /tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292/0/0/left-keyWithIndexToValue/_metadata/.schema.fcde2229-a4fa-409b-b3eb-751572f06c08.tmp\n2022-04-01 21:12:03,556 INFO  CheckpointFileManager - Renamed temp file /tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292/0/0/left-keyWithIndexToValue/_metadata/.schema.fcde2229-a4fa-409b-b3eb-751572f06c08.tmp to /tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292/0/0/left-keyWithIndexToValue/_metadata/schema\n2022-04-01 21:12:03,558 INFO  StateStore - Retrieved reference to StateStoreCoordinator: org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef1ea930eb\n2022-04-01 21:12:03,559 INFO  StateStore - Reported that the loaded instance StateStoreProviderId(StateStoreId(/tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292,0,0,left-keyWithIndexToValue),47925997-9891-4025-a36a-3e18bc758b50) is active\n2022-04-01 21:12:03,559 INFO  HDFSBackedStateStoreProvider - Retrieved version 0 of HDFSStateStoreProvider[id = (op=0,part=0),dir = /tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292/0/0/left-keyWithIndexToValue] for update\n2022-04-01 21:12:03,559 INFO  SymmetricHashJoinStateManager$KeyWithIndexToValueStore - Loaded store StateStoreId(/tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292,0,0,left-keyWithIndexToValue)\n2022-04-01 21:12:03,564 INFO  CheckpointFileManager - Writing atomically to /tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292/0/0/left-keyWithIndexToValue/1.delta using temp file /tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292/0/0/left-keyWithIndexToValue/.1.delta.86db3ac9-aa68-4a6b-9729-df93dc4b8a45.tmp\n2022-04-01 21:12:03,568 INFO  CheckpointFileManager - Writing atomically to /tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292/0/0/left-keyToNumValues/1.delta using temp file /tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292/0/0/left-keyToNumValues/.1.delta.9673bc1b-2bbe-412d-a0af-69f237cde31e.tmp\n2022-04-01 21:12:03,572 WARN  SymmetricHashJoinStateManager - `keyWithIndexToValue` returns a null value for index 4 at current key [false,40,10.0].\n2022-04-01 21:12:03,574 WARN  SymmetricHashJoinStateManager - `keyWithIndexToValue` returns a null value for index 3 at current key [false,40,10.0].\n2022-04-01 21:12:03,576 WARN  SymmetricHashJoinStateManager - `keyWithIndexToValue` returns a null value for index 3 at current key [false,60,10.0].\n2022-04-01 21:12:03,576 WARN  SymmetricHashJoinStateManager - `keyWithIndexToValue` returns a null value for index 1 at current key [false,40,10.0].\n2022-04-01 21:12:03,577 INFO  SymmetricHashJoinStateManager$KeyToNumValuesStore - Aborted store StateStoreId(/tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292,0,0,left-keyToNumValues)\n2022-04-01 21:12:03,577 INFO  HDFSBackedStateStoreProvider - Aborted version 1 for HDFSStateStore[id=(op=0,part=0),dir=/tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292/0/0/left-keyToNumValues]\n2022-04-01 21:12:03,577 INFO  SymmetricHashJoinStateManager$KeyWithIndexToValueStore - Aborted store StateStoreId(/tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292,0,0,left-keyWithIndexToValue)\n2022-04-01 21:12:03,577 INFO  HDFSBackedStateStoreProvider - Aborted version 1 for HDFSStateStore[id=(op=0,part=0),dir=/tmp/spark-d94b9f11-e04c-4871-aeea-1d0b5c62e292/0/0/left-keyWithIndexToValue]\n2022-04-01 21:12:03,580 INFO  StateStore - StateStore stopped\n2022-04-01 21:12:03,580 INFO  SymmetricHashJoinStateManagerSuite -\n\n===== FINISHED o.a.s.sql.execution.streaming.state.SymmetricHashJoinStateManagerSuite: 'StreamingJoinStateManager V2 - all operations with nulls' =====\n\u2026\n2022-04-01 21:12:04,205 INFO  StateStore - StateStore stopped\nRun completed in 5 seconds, 908 milliseconds.\nTotal number of tests run: 4\nSuites: completed 1, aborted 0\nTests: succeeded 4, failed 0, canceled 0, ignored 0, pending 0\nAll tests passed.\n2022-04-01 21:12:04,605 INFO  ShutdownHookManager - Shutdown hook called\n2022-04-01 21:12:04,605 INFO  ShutdownHookManager - Deleting directory /tmp/spark-37347802-bee5-4e7f-bffe-acb13eda1c5c\n2022-04-01 21:12:04,608 INFO  ShutdownHookManager - Deleting directory /tmp/spark-9e79a2e1-cec7-4fbf-804a-92e63913f516\n```\n\nCloses #36073 from anishshri-db/bfix/SPARK-38787.\n\nAuthored-by: Anish Shrigondekar <anish.shrigondekar@databricks.com>\nSigned-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>\n(cherry picked from commit 6d9bfb675f3e58c6e7d9facd8cf3f22069c4cc48)\nSigned-off-by: Jungtaek Lim <kabhwan.opensource@gmail.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManagerSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManagerSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManager.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "256:         return null",
          "257:       }",
          "259:       override def getNext(): KeyToValuePair = {",
          "260:         val currentValue = findNextValueForIndex()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "263:       private def getRightMostNonNullIndex(stopIndex: Long): Option[Long] = {",
          "264:         (numValues - 1 to stopIndex by -1).find { idx =>",
          "265:           keyWithIndexToValue.get(currentKey, idx) != null",
          "266:         }",
          "267:       }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "272:         if (index != numValues - 1) {",
          "273:           val valuePairAtMaxIndex = keyWithIndexToValue.get(currentKey, numValues - 1)",
          "274:           if (valuePairAtMaxIndex != null) {",
          "275:             keyWithIndexToValue.put(currentKey, index, valuePairAtMaxIndex.value,",
          "276:               valuePairAtMaxIndex.matched)",
          "277:           } else {",
          "281:           }",
          "282:         }",
          "283:         keyWithIndexToValue.remove(currentKey, numValues - 1)",
          "",
          "[Removed Lines]",
          "278:             val projectedKey = getInternalRowOfKeyWithIndex(currentKey)",
          "279:             logWarning(s\"`keyWithIndexToValue` returns a null value for index ${numValues - 1} \" +",
          "280:               s\"at current key $projectedKey.\")",
          "",
          "[Added Lines]",
          "291:             val nonNullIndex = getRightMostNonNullIndex(index + 1).getOrElse(index)",
          "292:             if (nonNullIndex != index) {",
          "293:               val valuePair = keyWithIndexToValue.get(currentKey, nonNullIndex)",
          "294:               keyWithIndexToValue.put(currentKey, index, valuePair.value,",
          "295:                 valuePair.matched)",
          "296:             }",
          "299:             if (nonNullIndex != numValues - 1) {",
          "300:               logWarning(s\"`keyWithIndexToValue` returns a null value for indices \" +",
          "301:                 s\"with range from startIndex=${nonNullIndex + 1} \" +",
          "302:                 s\"and endIndex=${numValues - 1}.\")",
          "303:             }",
          "308:             (numValues - 1 to nonNullIndex + 1 by -1).foreach { removeIndex =>",
          "309:               keyWithIndexToValue.remove(currentKey, removeIndex)",
          "310:               numValues -= 1",
          "311:             }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "324:     )",
          "325:   }",
          "328:   =====================================================",
          "329:             Private methods and inner classes",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "363:   private[state] def updateNumValuesTestOnly(key: UnsafeRow, numValues: Long): Unit = {",
          "364:     keyToNumValues.put(key, numValues)",
          "365:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManagerSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManagerSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManagerSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/SymmetricHashJoinStateManagerSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "46:     }",
          "47:   }",
          "49:   SymmetricHashJoinStateManager.supportedVersions.foreach { version =>",
          "50:     test(s\"SPARK-35689: StreamingJoinStateManager V${version} - \" +",
          "51:         \"printable key of keyWithIndexToValue\") {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "49:   SymmetricHashJoinStateManager.supportedVersions.foreach { version =>",
          "50:     test(s\"StreamingJoinStateManager V${version} - all operations with nulls\") {",
          "51:       testAllOperationsWithNulls(version)",
          "52:     }",
          "53:   }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "99:       assert(get(30) === Seq.empty)     // should remove 30",
          "100:       assert(numRows === 0)",
          "107:       appendAndTest(40, 100, 200, 300)",
          "108:       appendAndTest(50, 125)",
          "109:       appendAndTest(60, 275)              // prepare for testing removeByValue",
          "",
          "[Removed Lines]",
          "102:       def appendAndTest(key: Int, values: Int*): Unit = {",
          "103:         values.foreach { value => append(key, value)}",
          "104:         require(get(key) === values)",
          "105:       }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "130:       assert(numRows === 0)",
          "131:     }",
          "132:   }",
          "133:   val watermarkMetadata = new MetadataBuilder().putLong(EventTimeWatermark.delayKey, 10).build()",
          "134:   val inputValueSchema = new StructType()",
          "135:     .add(StructField(\"time\", IntegerType, metadata = watermarkMetadata))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "135:   private def testAllOperationsWithNulls(stateFormatVersion: Int): Unit = {",
          "136:     withJoinStateManager(inputValueAttribs, joinKeyExprs, stateFormatVersion) { manager =>",
          "137:       implicit val mgr = manager",
          "139:       appendAndTest(40, 100, 200, 300)",
          "140:       appendAndTest(50, 125)",
          "141:       appendAndTest(60, 275)              // prepare for testing removeByValue",
          "142:       assert(numRows === 5)",
          "144:       updateNumValues(40, 5)   // update total values to 5 to create 2 nulls",
          "145:       removeByValue(125)",
          "146:       assert(get(40) === Seq(200, 300))",
          "147:       assert(get(50) === Seq.empty)",
          "148:       assert(get(60) === Seq(275))        // should remove only some values, not all and nulls",
          "149:       assert(numRows === 3)",
          "151:       append(40, 50)",
          "152:       assert(get(40) === Seq(50, 200, 300))",
          "153:       assert(numRows === 4)",
          "154:       updateNumValues(40, 4)   // update total values to 4 to create 1 null",
          "156:       removeByValue(200)",
          "157:       assert(get(40) === Seq(300))",
          "158:       assert(get(60) === Seq(275))        // should remove only some values, not all and nulls",
          "159:       assert(numRows === 2)",
          "160:       updateNumValues(40, 2)   // update total values to simulate nulls",
          "161:       updateNumValues(60, 4)",
          "163:       removeByValue(300)",
          "164:       assert(get(40) === Seq.empty)",
          "165:       assert(get(60) === Seq.empty)       // should remove all values now including nulls",
          "166:       assert(numRows === 0)",
          "167:     }",
          "168:   }",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "157:     manager.append(toJoinKeyRow(key), toInputValue(value), matched = false)",
          "158:   }",
          "160:   def get(key: Int)(implicit manager: SymmetricHashJoinStateManager): Seq[Int] = {",
          "161:     manager.get(toJoinKeyRow(key)).map(toValueInt).toSeq.sorted",
          "162:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "197:   def appendAndTest(key: Int, values: Int*)",
          "198:                    (implicit manager: SymmetricHashJoinStateManager): Unit = {",
          "199:     values.foreach { value => append(key, value)}",
          "200:     require(get(key) === values)",
          "201:   }",
          "203:   def updateNumValues(key: Int, numValues: Long)",
          "204:                      (implicit manager: SymmetricHashJoinStateManager): Unit = {",
          "205:     manager.updateNumValuesTestOnly(toJoinKeyRow(key), numValues)",
          "206:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2254240dba4a71d9a68a22ca9a83080351fa3343",
      "candidate_info": {
        "commit_hash": "2254240dba4a71d9a68a22ca9a83080351fa3343",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/2254240dba4a71d9a68a22ca9a83080351fa3343",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala"
        ],
        "message": "[SPARK-39867][SQL] Global limit should not inherit OrderPreservingUnaryNode\n\nMake GlobalLimit inherit UnaryNode rather than OrderPreservingUnaryNode\n\nGlobal limit can not promise the output ordering is same with child, it actually depend on the certain physical plan.\n\nFor all physical plan with gobal limits:\n- CollectLimitExec: it does not promise output ordering\n- GlobalLimitExec: it required all tuples so it can assume the child is shuffle or child is single partition. Then it can use output ordering of child\n- TakeOrderedAndProjectExec: it do sort inside it's implementation\n\nThis bug get worse since we pull out v1 write require ordering.\n\nyes, bug fix\n\nfix test and add test\n\nCloses #37284 from ulysses-you/sort.\n\nAuthored-by: ulysses-you <ulyssesyou18@gmail.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit e9cc1024df4d587a0f456842d495db91984ed9db)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1253:   override def output: Seq[Attribute] = child.output",
          "1254:   override def maxRows: Option[Long] = {",
          "1255:     limitExpr match {",
          "",
          "[Removed Lines]",
          "1252: case class GlobalLimit(limitExpr: Expression, child: LogicalPlan) extends OrderPreservingUnaryNode {",
          "",
          "[Added Lines]",
          "1257: case class GlobalLimit(limitExpr: Expression, child: LogicalPlan) extends UnaryNode {",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/EliminateSortsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "116:   test(\"SPARK-33183: remove redundant sort by\") {",
          "117:     val orderedPlan = testRelation.select('a, 'b).orderBy('a.asc, 'b.desc_nullsFirst)",
          "119:     val optimized = Optimize.execute(unnecessaryReordered.analyze)",
          "121:     comparePlans(optimized, correctAnswer)",
          "122:   }",
          "",
          "[Removed Lines]",
          "118:     val unnecessaryReordered = orderedPlan.limit(2).select('a).sortBy('a.asc, 'b.desc_nullsFirst)",
          "120:     val correctAnswer = orderedPlan.limit(2).select('a).analyze",
          "",
          "[Added Lines]",
          "118:     val unnecessaryReordered = LocalLimit(2, orderedPlan).select('a).sortBy('a.asc, 'b.desc_nullsFirst)",
          "120:     val correctAnswer = LocalLimit(2, orderedPlan).select('a).analyze",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "161:     comparePlans(optimized, correctAnswer)",
          "162:   }",
          "165:     val orderedPlan = testRelation.select('a, 'b).orderBy('a.asc, 'b.desc)",
          "167:     val optimized = Optimize.execute(filteredAndReordered.analyze)",
          "169:     comparePlans(optimized, correctAnswer)",
          "170:   }",
          "",
          "[Removed Lines]",
          "164:   test(\"SPARK-33183: limits should not affect order for local sort\") {",
          "166:     val filteredAndReordered = orderedPlan.limit(Literal(10)).sortBy('a.asc, 'b.desc)",
          "168:     val correctAnswer = orderedPlan.limit(Literal(10)).analyze",
          "",
          "[Added Lines]",
          "164:   test(\"SPARK-33183: local limits should not affect order for local sort\") {",
          "166:     val filteredAndReordered = LocalLimit(10, orderedPlan).sortBy('a.asc, 'b.desc)",
          "168:     val correctAnswer = LocalLimit(10, orderedPlan).analyze",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "442:       .sortBy($\"c\".asc).analyze",
          "443:     comparePlans(Optimize.execute(plan3), expected3)",
          "444:   }",
          "445: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "446:   test(\"SPARK-39867: Global limit should not inherit OrderPreservingUnaryNode\") {",
          "447:     val plan = testRelation.sortBy($\"a\".asc).limit(2).sortBy($\"a\".asc).analyze",
          "448:     comparePlans(Optimize.execute(plan), plan)",
          "449:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4c4efdc9897fa5ff137d454d085482061475b6e5",
      "candidate_info": {
        "commit_hash": "4c4efdc9897fa5ff137d454d085482061475b6e5",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/4c4efdc9897fa5ff137d454d085482061475b6e5",
        "files": [
          "dev/deps/spark-deps-hadoop-2-hive-2.3",
          "dev/deps/spark-deps-hadoop-3-hive-2.3",
          "pom.xml"
        ],
        "message": "[SPARK-39493][BUILD] Update ORC to 1.7.5\n\nThis PR aims to update ORC to version 1.7.5.\n\nORC 1.7.5 is the latest version with the following bug fixes:\n-https://orc.apache.org/news/2022/06/16/ORC-1.7.5/\n\nNo.\n\nPass the CIs.\n\nCloses #36892 from williamhyun/orc175.\n\nAuthored-by: William Hyun <william@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit 264d8fd7b8f2a653ddaa027adc7a194d017c9eda)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "dev/deps/spark-deps-hadoop-2-hive-2.3||dev/deps/spark-deps-hadoop-2-hive-2.3",
          "dev/deps/spark-deps-hadoop-3-hive-2.3||dev/deps/spark-deps-hadoop-3-hive-2.3"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/deps/spark-deps-hadoop-2-hive-2.3||dev/deps/spark-deps-hadoop-2-hive-2.3": [
          "File: dev/deps/spark-deps-hadoop-2-hive-2.3 -> dev/deps/spark-deps-hadoop-2-hive-2.3",
          "--- Hunk 1 ---",
          "[Context before]",
          "219: okhttp/3.12.12//okhttp-3.12.12.jar",
          "220: okio/1.14.0//okio-1.14.0.jar",
          "221: opencsv/2.3//opencsv-2.3.jar",
          "225: oro/2.0.8//oro-2.0.8.jar",
          "226: osgi-resource-locator/1.0.3//osgi-resource-locator-1.0.3.jar",
          "227: paranamer/2.8//paranamer-2.8.jar",
          "",
          "[Removed Lines]",
          "222: orc-core/1.7.4//orc-core-1.7.4.jar",
          "223: orc-mapreduce/1.7.4//orc-mapreduce-1.7.4.jar",
          "224: orc-shims/1.7.4//orc-shims-1.7.4.jar",
          "",
          "[Added Lines]",
          "222: orc-core/1.7.5//orc-core-1.7.5.jar",
          "223: orc-mapreduce/1.7.5//orc-mapreduce-1.7.5.jar",
          "224: orc-shims/1.7.5//orc-shims-1.7.5.jar",
          "",
          "---------------"
        ],
        "dev/deps/spark-deps-hadoop-3-hive-2.3||dev/deps/spark-deps-hadoop-3-hive-2.3": [
          "File: dev/deps/spark-deps-hadoop-3-hive-2.3 -> dev/deps/spark-deps-hadoop-3-hive-2.3",
          "--- Hunk 1 ---",
          "[Context before]",
          "208: opentracing-api/0.33.0//opentracing-api-0.33.0.jar",
          "209: opentracing-noop/0.33.0//opentracing-noop-0.33.0.jar",
          "210: opentracing-util/0.33.0//opentracing-util-0.33.0.jar",
          "214: oro/2.0.8//oro-2.0.8.jar",
          "215: osgi-resource-locator/1.0.3//osgi-resource-locator-1.0.3.jar",
          "216: paranamer/2.8//paranamer-2.8.jar",
          "",
          "[Removed Lines]",
          "211: orc-core/1.7.4//orc-core-1.7.4.jar",
          "212: orc-mapreduce/1.7.4//orc-mapreduce-1.7.4.jar",
          "213: orc-shims/1.7.4//orc-shims-1.7.4.jar",
          "",
          "[Added Lines]",
          "211: orc-core/1.7.5//orc-core-1.7.5.jar",
          "212: orc-mapreduce/1.7.5//orc-mapreduce-1.7.5.jar",
          "213: orc-shims/1.7.5//orc-shims-1.7.5.jar",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "dcaa6e0eb6d5b4c90df64b5396ec0d31e7c9f99a",
      "candidate_info": {
        "commit_hash": "dcaa6e0eb6d5b4c90df64b5396ec0d31e7c9f99a",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/dcaa6e0eb6d5b4c90df64b5396ec0d31e7c9f99a",
        "files": [
          "python/pyspark/sql/functions.py"
        ],
        "message": "[MINOR][PYTHON][DOCS] Fix broken Example section in col/column functions\n\nThis PR fixes a bug in the documentation. Trailing `'` breaks Example section in Python reference documentation. This PR removes it.\n\nTo render the documentation as intended in NumPy documentation style.\n\nYes, the documentation is updated.\n\n**Before**\n\n<img width=\"789\" alt=\"Screen Shot 2022-07-19 at 12 20 55 PM\" src=\"https://user-images.githubusercontent.com/6477701/179661216-715dec96-bff2-474f-ab48-41577bf4c15c.png\">\n\n**After**\n\n<img width=\"633\" alt=\"Screen Shot 2022-07-19 at 12 48 04 PM\" src=\"https://user-images.githubusercontent.com/6477701/179661245-72d15184-aeed-43c2-b9c9-5f3cab1ae28d.png\">\n\nManually built the documentation and tested.\n\nCloses #37223 from HyukjinKwon/minor-doc-fx.\n\nAuthored-by: Hyukjin Kwon <gurwls223@apache.org>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 2bdb5bfa48d1fc44358c49f7e379c2afc4a1a32f)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/sql/functions.py||python/pyspark/sql/functions.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/sql/functions.py||python/pyspark/sql/functions.py": [
          "File: python/pyspark/sql/functions.py -> python/pyspark/sql/functions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "140: @since(1.3)",
          "141: def col(col: str) -> Column:",
          "142:     \"\"\"",
          "144:     Examples",
          "145:     --------",
          "146:     >>> col('x')",
          "",
          "[Removed Lines]",
          "143:     Returns a :class:`~pyspark.sql.Column` based on the given column name.'",
          "",
          "[Added Lines]",
          "143:     Returns a :class:`~pyspark.sql.Column` based on the given column name.",
          "",
          "---------------"
        ]
      }
    }
  ]
}