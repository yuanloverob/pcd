{
  "cve_id": "CVE-2021-3702",
  "cve_desc": "A race condition flaw was found in ansible-runner, where an attacker could watch for rapid creation and deletion of a temporary directory, substitute their directory at that name, and then have access to ansible-runner's private_data_dir the next time ansible-runner made use of the private_data_dir. The highest Threat out of this flaw is to integrity and confidentiality.",
  "repo": "ansible/ansible-runner",
  "patch_hash": "93e95a3df9021a38010386d07df121392d249253",
  "patch_info": {
    "commit_hash": "93e95a3df9021a38010386d07df121392d249253",
    "repo": "ansible/ansible-runner",
    "commit_url": "https://github.com/ansible/ansible-runner/commit/93e95a3df9021a38010386d07df121392d249253",
    "files": [
      "ansible_runner/interface.py",
      "ansible_runner/runner.py",
      "ansible_runner/streaming.py"
    ],
    "message": "Successfully runs\n\nStreamController and StreamWorker are now fleshed out.",
    "before_after_code_files": [
      "ansible_runner/interface.py||ansible_runner/interface.py",
      "ansible_runner/runner.py||ansible_runner/runner.py",
      "ansible_runner/streaming.py||ansible_runner/streaming.py"
    ]
  },
  "patch_diff": {
    "ansible_runner/interface.py||ansible_runner/interface.py": [
      "File: ansible_runner/interface.py -> ansible_runner/interface.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "24: from ansible_runner import output",
      "25: from ansible_runner.runner_config import RunnerConfig",
      "26: from ansible_runner.runner import Runner",
      "28: from ansible_runner.utils import (",
      "29:     dump_artifacts,",
      "30:     check_isolation_executable_installed,",
      "",
      "[Removed Lines]",
      "27: from ansible_runner.streaming import StreamWorker",
      "",
      "[Added Lines]",
      "27: from ansible_runner.streaming import StreamController, StreamWorker",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "65:     event_callback_handler = kwargs.pop('event_handler', None)",
      "66:     status_callback_handler = kwargs.pop('status_handler', None)",
      "67:     cancel_callback = kwargs.pop('cancel_callback', None)",
      "69:     finished_callback = kwargs.pop('finished_callback', None)",
      "71:     control_out = kwargs.pop('control_out', None)",
      "78:     rc = RunnerConfig(**kwargs)",
      "79:     rc.prepare()",
      "",
      "[Removed Lines]",
      "68:     artifacts_callback = kwargs.pop('artifacts_callback', None)  # Currently not expected",
      "72:     if control_out is not None:",
      "73:         stream_worker = StreamWorker(control_out)",
      "74:         status_callback_handler = stream_worker.status_handler",
      "75:         event_callback_handler = stream_worker.event_handler",
      "76:         artifacts_callback = stream_worker.artifacts_callback",
      "",
      "[Added Lines]",
      "67:     artifacts_handler = kwargs.pop('artifacts_handler', None)",
      "71:     control_in = kwargs.pop('control_in', None)",
      "73:     worker_in = kwargs.pop('worker_in', None)",
      "74:     worker_out = kwargs.pop('worker_out', None)",
      "76:     if worker_in is not None and worker_out is not None:",
      "77:         stream_worker = StreamWorker(worker_in, worker_out, **kwargs)",
      "78:         return stream_worker",
      "80:     if control_in is not None and control_out is not None:",
      "81:         stream_controller = StreamController(control_in, control_out,",
      "82:                                              event_handler=event_callback_handler,",
      "83:                                              status_handler=status_callback_handler,",
      "84:                                              artifacts_handler=artifacts_handler,",
      "85:                                              cancel_callback=cancel_callback,",
      "86:                                              finished_callback=finished_callback,",
      "88:         return stream_controller",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "81:     return Runner(rc,",
      "82:                   event_handler=event_callback_handler,",
      "83:                   status_handler=status_callback_handler,",
      "84:                   cancel_callback=cancel_callback,",
      "86:                   finished_callback=finished_callback)",
      "",
      "[Removed Lines]",
      "85:                   artifacts_callback=artifacts_callback,",
      "",
      "[Added Lines]",
      "96:                   artifacts_handler=artifacts_handler,",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "124:     :param artifact_dir: The path to the directory where artifacts should live, this defaults to 'artifacts' under the private data dir",
      "125:     :param project_dir: The path to the playbook content, this defaults to 'project' within the private data dir",
      "126:     :param rotate_artifacts: Keep at most n artifact directories, disable with a value of 0 which is the default",
      "128:     :param event_handler: An optional callback that will be invoked any time an event is received by Runner itself, return True to keep the event",
      "129:     :param cancel_callback: An optional callback that can inform runner to cancel (returning True) or not (returning False)",
      "130:     :param finished_callback: An optional callback that will be invoked at shutdown after process cleanup.",
      "131:     :param status_handler: An optional callback that will be invoked any time the status changes (e.g...started, running, failed, successful, timeout)",
      "132:     :param process_isolation: Enable process isolation, using either a container engine (e.g. podman) or a sandbox (e.g. bwrap).",
      "133:     :param process_isolation_executable: Process isolation executable or container engine used to isolate execution. (default: podman)",
      "134:     :param process_isolation_path: Path that an isolated playbook run will use for staging. (default: /tmp)",
      "",
      "[Removed Lines]",
      "127:     :param control_out: A file-like object used for streaming information back to a control instance of Runner",
      "",
      "[Added Lines]",
      "139:     :param control_in: A file object used for receiving streamed data back from a worker instance of Runner",
      "140:     :param control_out: A file object used for streaming project data to a worker instance of Runner",
      "141:     :param worker_in: A file object used for streaming project data to a worker instance of Runner",
      "142:     :param worker_out: A file object used for streaming information back to a control instance of Runner",
      "147:     :param artifacts_handler: An optional callback that will be invoked at the end of the run to deal with the artifacts from the run.",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "170:     :type forks: int",
      "171:     :type quiet: bool",
      "172:     :type verbosity: int",
      "173:     :type control_out: file",
      "174:     :type event_handler: function",
      "175:     :type cancel_callback: function",
      "176:     :type finished_callback: function",
      "177:     :type status_handler: function",
      "178:     :type process_isolation: bool",
      "179:     :type process_isolation_executable: str",
      "180:     :type process_isolation_path: str",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "189:     :type control_in: file",
      "191:     :type worker_in: file",
      "192:     :type worker_out: file",
      "197:     :type artifacts_handler: function",
      "",
      "---------------"
    ],
    "ansible_runner/runner.py||ansible_runner/runner.py": [
      "File: ansible_runner/runner.py -> ansible_runner/runner.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "27: class Runner(object):",
      "29:     def __init__(self, config, cancel_callback=None, remove_partials=True, event_handler=None,",
      "31:         self.config = config",
      "32:         self.cancel_callback = cancel_callback",
      "33:         self.event_handler = event_handler",
      "35:         self.finished_callback = finished_callback",
      "36:         self.status_handler = status_handler",
      "37:         self.canceled = False",
      "",
      "[Removed Lines]",
      "30:                  artifacts_callback=None, finished_callback=None, status_handler=None):",
      "34:         self.artifacts_callback = artifacts_callback",
      "",
      "[Added Lines]",
      "30:                  artifacts_handler=None, finished_callback=None, status_handler=None):",
      "34:         self.artifacts_handler = artifacts_handler",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "284:                 logger.error('Failed to delete cgroup: {}'.format(stderr))",
      "285:                 raise RuntimeError('Failed to delete cgroup: {}'.format(stderr))",
      "288:             try:",
      "290:             except Exception as e:",
      "291:                 raise CallbackError(\"Exception in Artifact Callback: {}\".format(e))",
      "",
      "[Removed Lines]",
      "287:         if self.artifacts_callback is not None:",
      "289:                 self.artifacts_callback(self.config.artifact_dir)",
      "",
      "[Added Lines]",
      "287:         if self.artifacts_handler is not None:",
      "289:                 self.artifacts_handler(self.config.artifact_dir)",
      "",
      "---------------"
    ],
    "ansible_runner/streaming.py||ansible_runner/streaming.py": [
      "File: ansible_runner/streaming.py -> ansible_runner/streaming.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: import base64",
      "2: import io",
      "3: import json",
      "4: import os",
      "5: import zipfile",
      "10:         self.control_out = control_out",
      "14:         self.control_out.write(b'\\n')",
      "15:         self.control_out.flush()",
      "17:     def event_handler(self, event_data):",
      "23:         buf = io.BytesIO()",
      "24:         with zipfile.ZipFile(buf, 'w', compression=zipfile.ZIP_DEFLATED, allowZip64=True) as archive:",
      "25:             for dirpath, dirs, files in os.walk(artifact_dir):",
      "",
      "[Removed Lines]",
      "8: class StreamWorker(object):",
      "9:     def __init__(self, control_out):",
      "12:     def status_handler(self, status, runner_config):",
      "13:         self.control_out.write(json.dumps(status).encode('utf-8'))",
      "18:         self.control_out.write(json.dumps(event_data).encode('utf-8'))",
      "19:         self.control_out.write(b'\\n')",
      "20:         self.control_out.flush()",
      "22:     def artifacts_callback(self, artifact_dir):",
      "",
      "[Added Lines]",
      "2: import codecs",
      "6: import stat",
      "7: import tempfile",
      "8: import uuid",
      "11: import ansible_runner",
      "12: import ansible_runner.plugins",
      "15: class UUIDEncoder(json.JSONEncoder):",
      "16:     def default(self, obj):",
      "17:         if isinstance(obj, uuid.UUID):",
      "18:             return obj.hex",
      "19:         return json.JSONEncoder.default(self, obj)",
      "22: # List of kwargs options to the run method that should be sent to the remote executor.",
      "23: remote_run_options = (",
      "24:     'forks',",
      "25:     'host_pattern',",
      "26:     'ident',",
      "27:     'ignore_logging',",
      "28:     'inventory',",
      "29:     'limit',",
      "30:     'module',",
      "31:     'module_args',",
      "32:     'omit_event_data',",
      "33:     'only_failed_event_data',",
      "34:     'playbook',",
      "35:     'verbosity',",
      "36: )",
      "39: class StreamController(object):",
      "40:     def __init__(self, control_in, control_out, status_handler=None, event_handler=None,",
      "41:                  artifacts_handler=None, cancel_callback=None, finished_callback=None, **kwargs):",
      "42:         self.control_in = control_in",
      "45:         self.kwargs = kwargs",
      "46:         self.config = ansible_runner.RunnerConfig(**kwargs)",
      "47:         self.status_handler = status_handler",
      "48:         self.event_handler = event_handler",
      "49:         self.artifacts_handler = artifacts_handler",
      "51:         self.cancel_callback = cancel_callback",
      "52:         self.finished_callback = finished_callback",
      "54:         self.status = \"unstarted\"",
      "55:         self.rc = None",
      "57:     def run(self):",
      "58:         self.send_job()",
      "60:         job_events_path = os.path.join(self.config.artifact_dir, 'job_events')",
      "61:         if not os.path.exists(job_events_path):",
      "62:             os.mkdir(job_events_path, 0o700)",
      "64:         for line in self.control_in:",
      "65:             data = json.loads(line)",
      "66:             if 'status' in data:",
      "67:                 self.status_callback(data)",
      "68:             elif 'artifacts' in data:",
      "69:                 self.artifacts_callback(data)",
      "70:             elif 'eof' in data:",
      "71:                 break",
      "72:             else:",
      "73:                 self.event_callback(data)",
      "75:         if self.finished_callback is not None:",
      "76:             self.finished_callback(self)",
      "77:         return self.status, self.rc",
      "79:     def send_job(self):",
      "80:         self.config.prepare()",
      "81:         remote_options = {key: value for key, value in self.kwargs.items() if key in remote_run_options}",
      "83:         buf = io.BytesIO()",
      "84:         with zipfile.ZipFile(buf, 'w', compression=zipfile.ZIP_DEFLATED, allowZip64=True) as archive:",
      "85:             private_data_dir = self.kwargs.get('private_data_dir', None)",
      "86:             if private_data_dir:",
      "87:                 for dirpath, dirs, files in os.walk(private_data_dir):",
      "88:                     relpath = os.path.relpath(dirpath, private_data_dir)",
      "89:                     if relpath == \".\":",
      "90:                         relpath = \"\"",
      "91:                     for fname in files:",
      "92:                         archive.write(os.path.join(dirpath, fname), arcname=os.path.join(relpath, fname))",
      "94:             kwargs = json.dumps(remote_options, cls=UUIDEncoder)",
      "95:             archive.writestr('kwargs', kwargs)",
      "96:             archive.close()",
      "97:         buf.flush()",
      "99:         data = {",
      "100:             'private_data_dir': True,",
      "101:             'payload': base64.b64encode(buf.getvalue()).decode('ascii'),",
      "102:         }",
      "103:         self.control_out.write(json.dumps(data).encode('utf-8'))",
      "106:         self.control_out.close()",
      "108:     def status_callback(self, status_data):",
      "109:         self.status = status_data['status']",
      "111:         for plugin in ansible_runner.plugins:",
      "112:             ansible_runner.plugins[plugin].status_handler(self.config, status_data)",
      "113:         if self.status_handler is not None:",
      "114:             self.status_handler(status_data, runner_config=self.config)",
      "116:     def event_callback(self, event_data):",
      "117:         full_filename = os.path.join(self.config.artifact_dir,",
      "118:                                      'job_events',",
      "119:                                      '{}-{}.json'.format(event_data['counter'],",
      "120:                                                          event_data['uuid']))",
      "122:         if self.event_handler is not None:",
      "123:             should_write = self.event_handler(event_data)",
      "124:         else:",
      "125:             should_write = True",
      "126:         for plugin in ansible_runner.plugins:",
      "127:             ansible_runner.plugins[plugin].event_handler(self.config, event_data)",
      "128:         if should_write:",
      "129:             with codecs.open(full_filename, 'w', encoding='utf-8') as write_file:",
      "130:                 os.chmod(full_filename, stat.S_IRUSR | stat.S_IWUSR)",
      "131:                 json.dump(event_data, write_file)",
      "133:     def artifacts_callback(self, artifacts_data):  # FIXME",
      "134:         if self.artifacts_handler is not None:",
      "135:             self.artifacts_handler()",
      "138: class StreamWorker(object):",
      "139:     def __init__(self, worker_in, worker_out, **kwargs):",
      "140:         self.worker_in = worker_in",
      "141:         self.worker_out = worker_out",
      "143:         self.kwargs = kwargs",
      "145:         self.private_data_dir = tempfile.TemporaryDirectory().name",
      "147:     def run(self):",
      "148:         for line in self.worker_in:",
      "149:             data = json.loads(line)",
      "150:             if data.get('private_data_dir'):",
      "151:                 buf = io.BytesIO(base64.b64decode(data['payload']))",
      "152:                 with zipfile.ZipFile(buf, 'r') as archive:",
      "153:                     archive.extractall(path=self.private_data_dir)",
      "155:         kwargs_path = os.path.join(self.private_data_dir, 'kwargs')",
      "156:         if os.path.exists(kwargs_path):",
      "157:             with open(kwargs_path, \"r\") as kwf:",
      "158:                 kwargs = json.load(kwf)",
      "159:             if not isinstance(kwargs, dict):",
      "160:                 raise ValueError(\"Invalid kwargs data\")",
      "161:         else:",
      "162:             kwargs = {}",
      "164:         self.kwargs.update(kwargs)",
      "166:         self.kwargs['quiet'] = True",
      "167:         self.kwargs['private_data_dir'] = self.private_data_dir",
      "168:         self.kwargs['status_handler'] = self.status_handler",
      "169:         self.kwargs['event_handler'] = self.event_handler",
      "170:         self.kwargs['artifacts_handler'] = self.artifacts_handler",
      "171:         self.kwargs['finished_callback'] = self.finished_callback",
      "173:         ansible_runner.interface.run(**self.kwargs)",
      "175:         # FIXME: do cleanup on the tempdir",
      "177:     def status_handler(self, status, runner_config):",
      "178:         self.worker_out.write(json.dumps(status).encode('utf-8'))",
      "179:         self.worker_out.write(b'\\n')",
      "180:         self.worker_out.flush()",
      "183:         self.worker_out.write(json.dumps(event_data).encode('utf-8'))",
      "184:         self.worker_out.write(b'\\n')",
      "185:         self.worker_out.flush()",
      "187:     def artifacts_handler(self, artifact_dir):",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "34:             'artifacts': True,",
      "35:             'payload': base64.b64encode(buf.getvalue()).decode('ascii'),",
      "36:         }",
      "",
      "[Removed Lines]",
      "37:         self.control_out.write(json.dumps(data).encode('utf-8'))",
      "38:         self.control_out.write(b'\\n')",
      "39:         self.control_out.flush()",
      "40:         self.control_out.close()",
      "",
      "[Added Lines]",
      "202:         self.worker_out.write(json.dumps(data).encode('utf-8'))",
      "203:         self.worker_out.write(b'\\n')",
      "204:         self.worker_out.flush()",
      "206:     def finished_callback(self, runner_obj):",
      "207:         self.worker_out.write(json.dumps({'eof': True}).encode('utf-8'))",
      "208:         self.worker_out.write(b'\\n')",
      "209:         self.worker_out.flush()",
      "210:         self.worker_out.close()",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "a067b4906ee33076cac99be82b4ae53444e67c6b",
      "candidate_info": {
        "commit_hash": "a067b4906ee33076cac99be82b4ae53444e67c6b",
        "repo": "ansible/ansible-runner",
        "commit_url": "https://github.com/ansible/ansible-runner/commit/a067b4906ee33076cac99be82b4ae53444e67c6b",
        "files": [
          "ansible_runner/streaming.py"
        ],
        "message": "Add option for only streaming kwargs\n\nThis is helpful when the private data dir exists on the same system. Without\nthis, we unnessarily zip up data only to unpack it again on the same host.",
        "before_after_code_files": [
          "ansible_runner/streaming.py||ansible_runner/streaming.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ],
          "candidate": [
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ]
        }
      },
      "candidate_diff": {
        "ansible_runner/streaming.py||ansible_runner/streaming.py": [
          "File: ansible_runner/streaming.py -> ansible_runner/streaming.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "35:             _output = sys.stdout.buffer",
          "36:         self._output = _output",
          "37:         self.private_data_dir = os.path.abspath(kwargs.pop('private_data_dir'))",
          "38:         self.kwargs = kwargs",
          "40:         self.status = \"unstarted\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "38:         self.only_transmit_kwargs = kwargs.pop('only_transmit_kwargs', False)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "47:         self._output.write(b'\\n')",
          "48:         self._output.flush()",
          "51:         self._output.write(json.dumps({'eof': True}).encode('utf-8'))",
          "52:         self._output.write(b'\\n')",
          "53:         self._output.flush()",
          "",
          "[Removed Lines]",
          "50:         self._output.write(utils.stream_dir(self.private_data_dir))",
          "",
          "[Added Lines]",
          "51:         if not self.only_transmit_kwargs:",
          "52:             self._output.write(utils.stream_dir(self.private_data_dir))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c0f16d14c6192f7d3576df9eb64d87e5dbfd73ab",
      "candidate_info": {
        "commit_hash": "c0f16d14c6192f7d3576df9eb64d87e5dbfd73ab",
        "repo": "ansible/ansible-runner",
        "commit_url": "https://github.com/ansible/ansible-runner/commit/c0f16d14c6192f7d3576df9eb64d87e5dbfd73ab",
        "files": [
          "ansible_runner/streaming.py",
          "ansible_runner/utils/__init__.py",
          "ansible_runner/utils/base64io.py"
        ],
        "message": "Fix linter",
        "before_after_code_files": [
          "ansible_runner/streaming.py||ansible_runner/streaming.py",
          "ansible_runner/utils/__init__.py||ansible_runner/utils/__init__.py",
          "ansible_runner/utils/base64io.py||ansible_runner/utils/base64io.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ],
          "candidate": [
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ]
        }
      },
      "candidate_diff": {
        "ansible_runner/streaming.py||ansible_runner/streaming.py": [
          "File: ansible_runner/streaming.py -> ansible_runner/streaming.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "224:     def artifacts_callback(self, artifacts_data):",
          "225:         length = artifacts_data['zipfile']",
          "228:         if self.artifacts_handler is not None:",
          "229:             self.artifacts_handler(self.artifact_dir)",
          "",
          "[Removed Lines]",
          "226:         utils.unstream_dir(self._input, length, self.artifact_dir)",
          "",
          "[Added Lines]",
          "226:         unstream_dir(self._input, length, self.artifact_dir)",
          "",
          "---------------"
        ],
        "ansible_runner/utils/__init__.py||ansible_runner/utils/__init__.py": [
          "File: ansible_runner/utils/__init__.py -> ansible_runner/utils/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "14: import pipes",
          "15: import uuid",
          "16: import codecs",
          "20: try:",
          "21:     from collections.abc import Iterable, Mapping",
          "22: except ImportError:",
          "23:     from collections import Iterable, Mapping",
          "25: from six import string_types, PY2, PY3, text_type, binary_type",
          "",
          "[Removed Lines]",
          "17: import zipfile",
          "18: import math",
          "24: from io import BytesIO, StringIO",
          "",
          "[Added Lines]",
          "22: from io import StringIO",
          "",
          "---------------"
        ],
        "ansible_runner/utils/base64io.py||ansible_runner/utils/base64io.py": [
          "File: ansible_runner/utils/base64io.py -> ansible_runner/utils/base64io.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: import io",
          "18: import logging",
          "19: import string",
          "22: LOGGER_NAME = \"base64io\"",
          "",
          "[Removed Lines]",
          "20: import sys",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "31:         Literal,",
          "32:         Optional,",
          "33:         Type,",
          "36: except ImportError:  # pragma: no cover",
          "37:     # We only actually need these imports when running the mypy checks",
          "38:     pass",
          "",
          "[Removed Lines]",
          "34:         Union,",
          "35:     )  # noqa pylint: disable=unused-import",
          "",
          "[Added Lines]",
          "33:     )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "42: _LOGGER = logging.getLogger(LOGGER_NAME)",
          "64: def _to_bytes(data):",
          "65:     # type: (AnyStr) -> bytes",
          "66:     \"\"\"Convert input data from either string or bytes to bytes.",
          "",
          "[Removed Lines]",
          "45: def _py2():",
          "46:     # type: () -> bool",
          "47:     \"\"\"Determine if runtime is Python 2.",
          "49:     :returns: decision:",
          "50:     :rtype: bool",
          "51:     \"\"\"",
          "52:     return sys.version_info[0] == 2",
          "55: if not _py2():",
          "56:     # The \"file\" object does not exist in Python 3, but we need to reference",
          "57:     # it in Python 2 code paths. Defining this here accomplishes two things:",
          "58:     # First, it allows linters to accept \"file\" as a defined object in Python 3.",
          "59:     # Second, it will serve as a canary to ensure that there are no references",
          "60:     # to \"file\" in Python 3 code paths.",
          "61:     file = NotImplemented  # pylint: disable=invalid-name",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "137:             self.__write_buffer = b\"\"",
          "138:         self.closed = True",
          "141:         # type: (str, str) -> bool",
          "142:         \"\"\"Attempt to call the specified method on the wrapped stream and return the result.",
          "144:         If the method is not found on the wrapped stream, return False.",
          "150:         :param str method_name: Name of method to call",
          "152:         :rtype: bool",
          "153:         \"\"\"",
          "154:         try:",
          "155:             method = getattr(self.__wrapped, method_name)",
          "156:         except AttributeError:",
          "165:             return False",
          "166:         else:",
          "167:             return method()",
          "",
          "[Removed Lines]",
          "140:     def _passthrough_interactive_check(self, method_name, mode):",
          "146:         .. note::",
          "148:             Special Case: If wrapped stream is a Python 2 file, inspect the file mode.",
          "151:         :param str mode: Python 2 mode character",
          "157:             if (",
          "158:                 _py2()",
          "159:                 and isinstance(",
          "160:                     self.__wrapped, file",
          "161:                 )  # pylint: disable=isinstance-second-argument-not-valid-type",
          "162:                 and mode in self.__wrapped.mode",
          "163:             ):",
          "164:                 return True",
          "",
          "[Added Lines]",
          "119:     def _passthrough_interactive_check(self, method_name):",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "176:         :rtype: bool",
          "177:         \"\"\"",
          "180:     def readable(self):",
          "181:         # type: () -> bool",
          "",
          "[Removed Lines]",
          "178:         return self._passthrough_interactive_check(\"writable\", \"w\")",
          "",
          "[Added Lines]",
          "144:         return self._passthrough_interactive_check(\"writable\")",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "187:         :rtype: bool",
          "188:         \"\"\"",
          "191:     def flush(self):",
          "192:         # type: () -> None",
          "",
          "[Removed Lines]",
          "189:         return self._passthrough_interactive_check(\"readable\", \"r\")",
          "",
          "[Added Lines]",
          "155:         return self._passthrough_interactive_check(\"readable\")",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "375:         if line:",
          "376:             return line",
          "377:         raise StopIteration()",
          "",
          "[Removed Lines]",
          "379:     def next(self):",
          "380:         # type: () -> bytes",
          "381:         \"\"\"Python 2 iterator hook.\"\"\"",
          "382:         return self.__next__()",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0e9aa8a97e7832ef9a1553ef2908632a32d2b8c4",
      "candidate_info": {
        "commit_hash": "0e9aa8a97e7832ef9a1553ef2908632a32d2b8c4",
        "repo": "ansible/ansible-runner",
        "commit_url": "https://github.com/ansible/ansible-runner/commit/0e9aa8a97e7832ef9a1553ef2908632a32d2b8c4",
        "files": [
          "ansible_runner/streaming.py"
        ],
        "message": "Close a race condition with temporary files\n\nThe previous code allowed a race where an attacker could watch for\ncreation of a rapid creation and deletion of a temporary directory,\nsubstitute their own directory at that name, and then have access to\nansible-runner's private_data_dir the next time ansible-runner made\nues of the private_data_dir.\n\nThis code fixes the issue by creating the directory securely using\nmkdtemp() and not deleting it afterwards.",
        "before_after_code_files": [
          "ansible_runner/streaming.py||ansible_runner/streaming.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ],
          "candidate": [
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ]
        }
      },
      "candidate_diff": {
        "ansible_runner/streaming.py||ansible_runner/streaming.py": [
          "File: ansible_runner/streaming.py -> ansible_runner/streaming.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "74:         private_data_dir = kwargs.get('private_data_dir')",
          "75:         if private_data_dir is None:",
          "77:         self.private_data_dir = private_data_dir",
          "79:         self.status = \"unstarted\"",
          "",
          "[Removed Lines]",
          "76:             private_data_dir = tempfile.TemporaryDirectory().name",
          "",
          "[Added Lines]",
          "76:             private_data_dir = tempfile.mkdtemp()",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "165:         private_data_dir = kwargs.get('private_data_dir')",
          "166:         if private_data_dir is None:",
          "168:         self.private_data_dir = private_data_dir",
          "169:         self._loader = ArtifactLoader(self.private_data_dir)",
          "",
          "[Removed Lines]",
          "167:             private_data_dir = tempfile.TemporaryDirectory().name",
          "",
          "[Added Lines]",
          "167:             private_data_dir = tempfile.mkdtemp()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4d039deb5cc5fefa736aeeb062c9365575a393d5",
      "candidate_info": {
        "commit_hash": "4d039deb5cc5fefa736aeeb062c9365575a393d5",
        "repo": "ansible/ansible-runner",
        "commit_url": "https://github.com/ansible/ansible-runner/commit/4d039deb5cc5fefa736aeeb062c9365575a393d5",
        "files": [
          "ansible_runner/__main__.py",
          "ansible_runner/interface.py",
          "ansible_runner/streaming.py"
        ],
        "message": "Decompose into a 3-phase streaming process",
        "before_after_code_files": [
          "ansible_runner/__main__.py||ansible_runner/__main__.py",
          "ansible_runner/interface.py||ansible_runner/interface.py",
          "ansible_runner/streaming.py||ansible_runner/streaming.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "ansible_runner/interface.py||ansible_runner/interface.py",
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ],
          "candidate": [
            "ansible_runner/interface.py||ansible_runner/interface.py",
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ]
        }
      },
      "candidate_diff": {
        "ansible_runner/__main__.py||ansible_runner/__main__.py": [
          "File: ansible_runner/__main__.py -> ansible_runner/__main__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "232:             ),",
          "233:         ),",
          "234:     ),",
          "261:     \"roles_group\": (",
          "262:         (",
          "263:             (\"--roles-path\",),",
          "",
          "[Removed Lines]",
          "235:     \"streaming_group\": (",
          "236:         (",
          "237:             ('--control-in',),",
          "238:             dict(",
          "239:                 help=\"Executes runner in controller-mode, and consumes data piped into this named file or fifo.\"",
          "240:             ),",
          "241:         ),",
          "242:         (",
          "243:             ('--control-out',),",
          "244:             dict(",
          "245:                 help=\"Executes runner in controller-mode, and pipes commands through this named file or fifo.\"",
          "246:             ),",
          "247:         ),",
          "248:         (",
          "249:             ('--worker-in',),",
          "250:             dict(",
          "251:                 help=\"Executes runner in worker-mode, and consumes commands piped into this named file or fifo.\"",
          "252:             ),",
          "253:         ),",
          "254:         (",
          "255:             ('--worker-out',),",
          "256:             dict(",
          "257:                 help=\"Executes runner in worker-mode, and pipes output data through this named file or fifo.\"",
          "258:             ),",
          "259:         ),",
          "260:     ),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "649:     )",
          "650:     add_args_to_parser(isalive_subparser, DEFAULT_CLI_ARGS['positional_args'])",
          "653:     # adhoc command exec",
          "654:     adhoc_subparser = subparser.add_parser(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "625:     # streaming commands",
          "626:     transmit_subparser = subparser.add_parser(",
          "627:         'transmit',",
          "628:         help=\"Send a job to a remote ansible-runner process\"",
          "629:     )",
          "630:     add_args_to_parser(transmit_subparser, DEFAULT_CLI_ARGS['positional_args'])",
          "632:     worker_subparser = subparser.add_parser(",
          "633:         'worker',",
          "634:         help=\"Execute work streamed from a controlling instance\"",
          "635:     )",
          "637:     process_subparser = subparser.add_parser(",
          "638:         'process',",
          "639:         help=\"Receive the output of remote ansible-runner work and distribute the results\"",
          "640:     )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "662:     )",
          "663:     add_args_to_parser(adhoc_subparser, DEFAULT_CLI_ARGS['execenv_cli_group'])",
          "666:     playbook_subparser = subparser.add_parser(",
          "667:         'playbook',",
          "668:         help=\"Run ansible-playbook commands in an Execution Environment\"",
          "",
          "[Removed Lines]",
          "665:     # adhoc command exec",
          "",
          "[Added Lines]",
          "654:     # playbook command exec",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "681:     add_args_to_parser(isalive_subparser, DEFAULT_CLI_ARGS['generic_args'])",
          "682:     add_args_to_parser(adhoc_subparser, DEFAULT_CLI_ARGS['generic_args'])",
          "683:     add_args_to_parser(playbook_subparser, DEFAULT_CLI_ARGS['generic_args'])",
          "685:     # runner group",
          "686:     ansible_runner_group_options = (",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "673:     add_args_to_parser(transmit_subparser, DEFAULT_CLI_ARGS['generic_args'])",
          "674:     add_args_to_parser(worker_subparser, DEFAULT_CLI_ARGS['generic_args'])",
          "675:     add_args_to_parser(process_subparser, DEFAULT_CLI_ARGS['generic_args'])",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "693:     start_runner_group = start_subparser.add_argument_group(*ansible_runner_group_options)",
          "694:     stop_runner_group = stop_subparser.add_argument_group(*ansible_runner_group_options)",
          "695:     isalive_runner_group = isalive_subparser.add_argument_group(*ansible_runner_group_options)",
          "696:     add_args_to_parser(base_runner_group, DEFAULT_CLI_ARGS['runner_group'])",
          "697:     add_args_to_parser(run_runner_group, DEFAULT_CLI_ARGS['runner_group'])",
          "698:     add_args_to_parser(start_runner_group, DEFAULT_CLI_ARGS['runner_group'])",
          "699:     add_args_to_parser(stop_runner_group, DEFAULT_CLI_ARGS['runner_group'])",
          "700:     add_args_to_parser(isalive_runner_group, DEFAULT_CLI_ARGS['runner_group'])",
          "709:     # mutually exclusive group",
          "710:     run_mutually_exclusive_group = run_subparser.add_mutually_exclusive_group()",
          "711:     start_mutually_exclusive_group = start_subparser.add_mutually_exclusive_group()",
          "712:     stop_mutually_exclusive_group = stop_subparser.add_mutually_exclusive_group()",
          "713:     isalive_mutually_exclusive_group = isalive_subparser.add_mutually_exclusive_group()",
          "714:     add_args_to_parser(run_mutually_exclusive_group, DEFAULT_CLI_ARGS['mutually_exclusive_group'])",
          "715:     add_args_to_parser(start_mutually_exclusive_group, DEFAULT_CLI_ARGS['mutually_exclusive_group'])",
          "716:     add_args_to_parser(stop_mutually_exclusive_group, DEFAULT_CLI_ARGS['mutually_exclusive_group'])",
          "717:     add_args_to_parser(isalive_mutually_exclusive_group, DEFAULT_CLI_ARGS['mutually_exclusive_group'])",
          "719:     # ansible options",
          "720:     ansible_options = (",
          "",
          "[Removed Lines]",
          "702:     # streaming group",
          "703:     add_args_to_parser(base_runner_group, DEFAULT_CLI_ARGS['streaming_group'])",
          "704:     add_args_to_parser(run_runner_group, DEFAULT_CLI_ARGS['streaming_group'])",
          "705:     add_args_to_parser(start_runner_group, DEFAULT_CLI_ARGS['streaming_group'])",
          "706:     add_args_to_parser(stop_runner_group, DEFAULT_CLI_ARGS['streaming_group'])",
          "707:     add_args_to_parser(isalive_runner_group, DEFAULT_CLI_ARGS['streaming_group'])",
          "",
          "[Added Lines]",
          "688:     transmit_runner_group = transmit_subparser.add_argument_group(*ansible_runner_group_options)",
          "694:     add_args_to_parser(transmit_runner_group, DEFAULT_CLI_ARGS['runner_group'])",
          "701:     transmit_mutually_exclusive_group = transmit_subparser.add_mutually_exclusive_group()",
          "706:     add_args_to_parser(transmit_mutually_exclusive_group, DEFAULT_CLI_ARGS['mutually_exclusive_group'])",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "725:     start_ansible_group = start_subparser.add_argument_group(*ansible_options)",
          "726:     stop_ansible_group = stop_subparser.add_argument_group(*ansible_options)",
          "727:     isalive_ansible_group = isalive_subparser.add_argument_group(*ansible_options)",
          "728:     add_args_to_parser(run_ansible_group, DEFAULT_CLI_ARGS['ansible_group'])",
          "729:     add_args_to_parser(start_ansible_group, DEFAULT_CLI_ARGS['ansible_group'])",
          "730:     add_args_to_parser(stop_ansible_group, DEFAULT_CLI_ARGS['ansible_group'])",
          "731:     add_args_to_parser(isalive_ansible_group, DEFAULT_CLI_ARGS['ansible_group'])",
          "734:     # roles group",
          "735:     roles_group_options = (",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "717:     transmit_ansible_group = transmit_subparser.add_argument_group(*ansible_options)",
          "722:     add_args_to_parser(transmit_ansible_group, DEFAULT_CLI_ARGS['ansible_group'])",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "740:     start_roles_group = start_subparser.add_argument_group(*roles_group_options)",
          "741:     stop_roles_group = stop_subparser.add_argument_group(*roles_group_options)",
          "742:     isalive_roles_group = isalive_subparser.add_argument_group(*roles_group_options)",
          "743:     add_args_to_parser(run_roles_group, DEFAULT_CLI_ARGS['roles_group'])",
          "744:     add_args_to_parser(start_roles_group, DEFAULT_CLI_ARGS['roles_group'])",
          "745:     add_args_to_parser(stop_roles_group, DEFAULT_CLI_ARGS['roles_group'])",
          "746:     add_args_to_parser(isalive_roles_group, DEFAULT_CLI_ARGS['roles_group'])",
          "748:     # modules groups",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "733:     transmit_roles_group = transmit_subparser.add_argument_group(*roles_group_options)",
          "738:     add_args_to_parser(transmit_roles_group, DEFAULT_CLI_ARGS['roles_group'])",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "755:     start_modules_group = start_subparser.add_argument_group(*modules_group_options)",
          "756:     stop_modules_group = stop_subparser.add_argument_group(*modules_group_options)",
          "757:     isalive_modules_group = isalive_subparser.add_argument_group(*modules_group_options)",
          "758:     add_args_to_parser(run_modules_group, DEFAULT_CLI_ARGS['modules_group'])",
          "759:     add_args_to_parser(start_modules_group, DEFAULT_CLI_ARGS['modules_group'])",
          "760:     add_args_to_parser(stop_modules_group, DEFAULT_CLI_ARGS['modules_group'])",
          "761:     add_args_to_parser(isalive_modules_group, DEFAULT_CLI_ARGS['modules_group'])",
          "763:     # playbook options",
          "764:     playbook_group_options = (",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "750:     transmit_modules_group = transmit_subparser.add_argument_group(*modules_group_options)",
          "755:     add_args_to_parser(transmit_modules_group, DEFAULT_CLI_ARGS['modules_group'])",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "769:     start_playbook_group = start_subparser.add_argument_group(*playbook_group_options)",
          "770:     stop_playbook_group = stop_subparser.add_argument_group(*playbook_group_options)",
          "771:     isalive_playbook_group = isalive_subparser.add_argument_group(*playbook_group_options)",
          "772:     add_args_to_parser(run_playbook_group, DEFAULT_CLI_ARGS['playbook_group'])",
          "773:     add_args_to_parser(start_playbook_group, DEFAULT_CLI_ARGS['playbook_group'])",
          "774:     add_args_to_parser(stop_playbook_group, DEFAULT_CLI_ARGS['playbook_group'])",
          "775:     add_args_to_parser(isalive_playbook_group, DEFAULT_CLI_ARGS['playbook_group'])",
          "777:     # container group",
          "778:     container_group_options = (",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "766:     transmit_playbook_group = transmit_subparser.add_argument_group(*playbook_group_options)",
          "771:     add_args_to_parser(transmit_playbook_group, DEFAULT_CLI_ARGS['playbook_group'])",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "828:                 )",
          "829:             )",
          "832:         if vargs.get('hosts') and not (vargs.get('module') or vargs.get('role')):",
          "833:             parser.exit(status=1, message=\"The --hosts option can only be used with -m or -r\\n\")",
          "834:         if not (vargs.get('module') or vargs.get('role')) and not vargs.get('playbook'):",
          "835:             parser.exit(status=1, message=\"The -p option must be specified when not using -m or -r\\n\")",
          "844:     output.configure()",
          "846:     # enable or disable debug mode",
          "",
          "[Removed Lines]",
          "831:     if vargs.get('command') in ('start', 'run'):",
          "837:     if (vargs.get('control_in') is None) != (vargs.get('control_out') is None):",
          "838:         parser.exit(status=1, message=\"Both --control-in and --control-out must be specified\\n\")",
          "839:     if (vargs.get('worker_in') is None) != (vargs.get('worker_out') is None):",
          "840:         parser.exit(status=1, message=\"Both --worker-in and --worker-out must be specified\\n\")",
          "841:     if vargs.get('control_in') is not None and vargs.get('worker_in') is not None:",
          "842:         parser.exit(status=1, message=\"Runner may not be run in both control and worker modes.\\n\")",
          "",
          "[Added Lines]",
          "827:     if vargs.get('command') in ('start', 'run', 'transmit'):",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "868:     stderr_path = None",
          "869:     context = None",
          "871:         stderr_path = os.path.join(vargs.get('private_data_dir'), 'daemon.log')",
          "872:         if not os.path.exists(stderr_path):",
          "873:             os.close(os.open(stderr_path, os.O_CREAT, stat.S_IRUSR | stat.S_IWUSR))",
          "877:         if vargs.get('command') == 'start':",
          "878:             import daemon",
          "",
          "[Removed Lines]",
          "870:     if vargs.get('command') not in ('run', 'adhoc', 'playbook'):",
          "875:     if vargs.get('command') in ('start', 'run', 'adhoc', 'playbook'):",
          "",
          "[Added Lines]",
          "859:     if vargs.get('command') not in ('run', 'transmit', 'adhoc', 'playbook'):",
          "864:     if vargs.get('command') in ('start', 'run', 'transmit', 'adhoc', 'playbook'):",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "881:         else:",
          "882:             context = threading.Lock()",
          "884:         with context:",
          "885:             with role_manager(vargs) as vargs:",
          "886:                 run_options = dict(private_data_dir=vargs.get('private_data_dir'),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "873:         streamer = None",
          "874:         if vargs.get('command') in ('transmit', 'worker', 'process'):",
          "875:             streamer = vargs.get('command')",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "919:                                    resource_profiling_pid_poll_interval=vargs.get('resource_profiling_pid_poll_interval'),",
          "920:                                    resource_profiling_results_dir=vargs.get('resource_profiling_results_dir'),",
          "921:                                    limit=vargs.get('limit'),",
          "922:                                    cli_execenv_cmd=cli_execenv_cmd",
          "923:                                    )",
          "924:                 if vargs.get('command') in ('adhoc', 'playbook'):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "915:                                    streamer=streamer,",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "926:                     run_options['process_isolation']=True",
          "927:                     run_options['process_isolation_executable']=vargs.get('container_runtime')",
          "938:                 try:",
          "939:                     res = run(**run_options)",
          "940:                 except Exception:",
          "",
          "[Removed Lines]",
          "929:                 if vargs.get('control_in'):",
          "930:                     run_options['control_in'] = open(vargs['control_in'], 'rb')",
          "931:                 if vargs.get('worker_in'):",
          "932:                     run_options['worker_in'] = open(vargs['worker_in'], 'rb')",
          "933:                 if vargs.get('control_in'):",
          "934:                     run_options['control_out'] = open(vargs['control_out'], 'wb')",
          "935:                 if vargs.get('control_in'):",
          "936:                     run_options['worker_out'] = open(vargs['worker_out'], 'wb')",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "ansible_runner/interface.py||ansible_runner/interface.py": [
          "File: ansible_runner/interface.py -> ansible_runner/interface.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: from ansible_runner import output",
          "25: from ansible_runner.runner_config import RunnerConfig",
          "26: from ansible_runner.runner import Runner",
          "28: from ansible_runner.utils import (",
          "29:     dump_artifacts,",
          "30:     check_isolation_executable_installed,",
          "",
          "[Removed Lines]",
          "27: from ansible_runner.streaming import StreamController, StreamWorker",
          "",
          "[Added Lines]",
          "27: from ansible_runner.streaming import Transmitter, Worker, Processor",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "68:     cancel_callback = kwargs.pop('cancel_callback', None)",
          "69:     finished_callback = kwargs.pop('finished_callback', None)",
          "90:     rc = RunnerConfig(**kwargs)",
          "91:     rc.prepare()",
          "",
          "[Removed Lines]",
          "71:     control_in = kwargs.pop('control_in', None)",
          "72:     control_out = kwargs.pop('control_out', None)",
          "73:     worker_in = kwargs.pop('worker_in', None)",
          "74:     worker_out = kwargs.pop('worker_out', None)",
          "76:     if worker_in is not None and worker_out is not None:",
          "77:         stream_worker = StreamWorker(worker_in, worker_out, **kwargs)",
          "78:         return stream_worker",
          "80:     if control_in is not None and control_out is not None:",
          "81:         stream_controller = StreamController(control_in, control_out,",
          "82:                                              event_handler=event_callback_handler,",
          "83:                                              status_handler=status_callback_handler,",
          "84:                                              artifacts_handler=artifacts_handler,",
          "85:                                              cancel_callback=cancel_callback,",
          "86:                                              finished_callback=finished_callback,",
          "88:         return stream_controller",
          "",
          "[Added Lines]",
          "71:     if kwargs.get('streamer'):",
          "72:         streamer = kwargs.pop('streamer')",
          "74:         if streamer == 'transmit':",
          "75:             stream_transmitter = Transmitter(**kwargs)",
          "76:             return stream_transmitter",
          "78:         if streamer == 'worker':",
          "79:             stream_worker = Worker(**kwargs)",
          "80:             return stream_worker",
          "82:         if streamer == 'process':",
          "83:             stream_processor = Processor(event_handler=event_callback_handler,",
          "84:                                          status_handler=status_callback_handler,",
          "85:                                          artifacts_handler=artifacts_handler,",
          "86:                                          cancel_callback=cancel_callback,",
          "87:                                          finished_callback=finished_callback,",
          "89:             return stream_processor",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "136:     :param artifact_dir: The path to the directory where artifacts should live, this defaults to 'artifacts' under the private data dir",
          "137:     :param project_dir: The path to the playbook content, this defaults to 'project' within the private data dir",
          "138:     :param rotate_artifacts: Keep at most n artifact directories, disable with a value of 0 which is the default",
          "143:     :param event_handler: An optional callback that will be invoked any time an event is received by Runner itself, return True to keep the event",
          "144:     :param cancel_callback: An optional callback that can inform runner to cancel (returning True) or not (returning False)",
          "145:     :param finished_callback: An optional callback that will be invoked at shutdown after process cleanup.",
          "",
          "[Removed Lines]",
          "139:     :param control_in: A file object used for receiving streamed data back from a worker instance of Runner",
          "140:     :param control_out: A file object used for streaming project data to a worker instance of Runner",
          "141:     :param worker_in: A file object used for streaming project data to a worker instance of Runner",
          "142:     :param worker_out: A file object used for streaming information back to a control instance of Runner",
          "",
          "[Added Lines]",
          "140:     :param streamer: Optionally invoke ansible-runner as one of the steps in the streaming pipeline",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "186:     :type forks: int",
          "187:     :type quiet: bool",
          "188:     :type verbosity: int",
          "193:     :type event_handler: function",
          "194:     :type cancel_callback: function",
          "195:     :type finished_callback: function",
          "",
          "[Removed Lines]",
          "189:     :type control_in: file",
          "190:     :type control_out: file",
          "191:     :type worker_in: file",
          "192:     :type worker_out: file",
          "",
          "[Added Lines]",
          "187:     :type streamer: str",
          "",
          "---------------"
        ],
        "ansible_runner/streaming.py||ansible_runner/streaming.py": [
          "File: ansible_runner/streaming.py -> ansible_runner/streaming.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "4: import json",
          "5: import os",
          "6: import stat",
          "7: import tempfile",
          "8: import uuid",
          "9: import zipfile",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "7: import sys",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "36: )",
          "45:         self.kwargs = kwargs",
          "46:         self.config = ansible_runner.RunnerConfig(**kwargs)",
          "54:         self.status = \"unstarted\"",
          "55:         self.rc = None",
          "57:     def run(self):",
          "82:         self.config.prepare()",
          "83:         remote_options = {key: value for key, value in self.kwargs.items() if key in remote_run_options}",
          "",
          "[Removed Lines]",
          "39: class StreamController(object):",
          "40:     def __init__(self, control_in, control_out, status_handler=None, event_handler=None,",
          "41:                  artifacts_handler=None, cancel_callback=None, finished_callback=None, **kwargs):",
          "42:         self.control_in = control_in",
          "43:         self.control_out = control_out",
          "47:         self.status_handler = status_handler",
          "48:         self.event_handler = event_handler",
          "49:         self.artifacts_handler = artifacts_handler",
          "51:         self.cancel_callback = cancel_callback",
          "52:         self.finished_callback = finished_callback",
          "58:         self.send_job()",
          "60:         job_events_path = os.path.join(self.config.artifact_dir, 'job_events')",
          "61:         if not os.path.exists(job_events_path):",
          "62:             os.mkdir(job_events_path, 0o700)",
          "64:         while True:",
          "65:             line = self.control_in.readline()",
          "66:             data = json.loads(line)",
          "68:             if 'status' in data:",
          "69:                 self.status_callback(data)",
          "70:             elif 'artifacts' in data:",
          "71:                 self.artifacts_callback(self.control_in.read(data['artifacts']))",
          "72:             elif 'eof' in data:",
          "73:                 break",
          "74:             else:",
          "75:                 self.event_callback(data)",
          "77:         if self.finished_callback is not None:",
          "78:             self.finished_callback(self)",
          "79:         return self.status, self.rc",
          "81:     def send_job(self):",
          "",
          "[Added Lines]",
          "40: class Transmitter(object):",
          "41:     def __init__(self, _output=None, **kwargs):",
          "42:         if _output is None:",
          "43:             _output = sys.stdout.buffer",
          "44:         self._output = _output",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "102:             'private_data_dir': True,",
          "103:             'payload': base64.b64encode(buf.getvalue()).decode('ascii'),",
          "104:         }",
          "149:         self.kwargs = kwargs",
          "151:         self.private_data_dir = tempfile.TemporaryDirectory().name",
          "153:     def run(self):",
          "155:             data = json.loads(line)",
          "156:             if data.get('private_data_dir'):",
          "157:                 buf = io.BytesIO(base64.b64decode(data['payload']))",
          "",
          "[Removed Lines]",
          "105:         self.control_out.write(json.dumps(data).encode('utf-8'))",
          "106:         self.control_out.write(b'\\n')",
          "107:         self.control_out.flush()",
          "108:         self.control_out.close()",
          "110:     def status_callback(self, status_data):",
          "111:         self.status = status_data['status']",
          "113:         for plugin in ansible_runner.plugins:",
          "114:             ansible_runner.plugins[plugin].status_handler(self.config, status_data)",
          "115:         if self.status_handler is not None:",
          "116:             self.status_handler(status_data, runner_config=self.config)",
          "118:     def event_callback(self, event_data):",
          "119:         full_filename = os.path.join(self.config.artifact_dir,",
          "120:                                      'job_events',",
          "121:                                      '{}-{}.json'.format(event_data['counter'],",
          "122:                                                          event_data['uuid']))",
          "124:         if self.event_handler is not None:",
          "125:             should_write = self.event_handler(event_data)",
          "126:         else:",
          "127:             should_write = True",
          "128:         for plugin in ansible_runner.plugins:",
          "129:             ansible_runner.plugins[plugin].event_handler(self.config, event_data)",
          "130:         if should_write:",
          "131:             with codecs.open(full_filename, 'w', encoding='utf-8') as write_file:",
          "132:                 os.chmod(full_filename, stat.S_IRUSR | stat.S_IWUSR)",
          "133:                 json.dump(event_data, write_file)",
          "135:     def artifacts_callback(self, artifacts_data):",
          "136:         buf = io.BytesIO(artifacts_data)",
          "137:         with zipfile.ZipFile(buf, 'r') as archive:",
          "138:             archive.extractall(path=self.config.artifact_dir)",
          "140:         if self.artifacts_handler is not None:",
          "141:             self.artifacts_handler(self.config.artifact_dir)",
          "144: class StreamWorker(object):",
          "145:     def __init__(self, worker_in, worker_out, **kwargs):",
          "146:         self.worker_in = worker_in",
          "147:         self.worker_out = worker_out",
          "154:         for line in self.worker_in:",
          "",
          "[Added Lines]",
          "75:         self._output.flush()",
          "76:         self._output.write(json.dumps(data).encode('utf-8'))",
          "77:         self._output.write(b'\\n')",
          "78:         self._output.flush()",
          "79:         self._output.close()",
          "82: class Worker(object):",
          "83:     def __init__(self, _input=None, _output=None, **kwargs):",
          "84:         if _input is None:",
          "85:             _input = sys.stdin.buffer",
          "86:         if _output is None:",
          "87:             _output = sys.stdout.buffer",
          "88:         self._input = _input",
          "89:         self._output = _output",
          "96:         for line in self._input:",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "181:         # FIXME: do cleanup on the tempdir",
          "183:     def status_handler(self, status, runner_config):",
          "188:     def event_handler(self, event_data):",
          "193:     def artifacts_handler(self, artifact_dir):",
          "194:         buf = io.BytesIO()",
          "",
          "[Removed Lines]",
          "184:         self.worker_out.write(json.dumps(status).encode('utf-8'))",
          "185:         self.worker_out.write(b'\\n')",
          "186:         self.worker_out.flush()",
          "189:         self.worker_out.write(json.dumps(event_data).encode('utf-8'))",
          "190:         self.worker_out.write(b'\\n')",
          "191:         self.worker_out.flush()",
          "",
          "[Added Lines]",
          "126:         self._output.write(json.dumps(status).encode('utf-8'))",
          "127:         self._output.write(b'\\n')",
          "128:         self._output.flush()",
          "131:         self._output.write(json.dumps(event_data).encode('utf-8'))",
          "132:         self._output.write(b'\\n')",
          "133:         self._output.flush()",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "204:         payload = buf.getvalue()",
          "211:     def finished_callback(self, runner_obj):",
          "",
          "[Removed Lines]",
          "206:         self.worker_out.write(json.dumps({'artifacts': len(payload)}).encode('utf-8'))",
          "207:         self.worker_out.write(b'\\n')",
          "208:         self.worker_out.write(payload)",
          "209:         self.worker_out.flush()",
          "212:         self.worker_out.write(json.dumps({'eof': True}).encode('utf-8'))",
          "213:         self.worker_out.write(b'\\n')",
          "214:         self.worker_out.flush()",
          "215:         self.worker_out.close()",
          "",
          "[Added Lines]",
          "148:         self._output.write(json.dumps({'artifacts': len(payload)}).encode('utf-8'))",
          "149:         self._output.write(b'\\n')",
          "150:         self._output.write(payload)",
          "151:         self._output.flush()",
          "154:         self._output.write(json.dumps({'eof': True}).encode('utf-8'))",
          "155:         self._output.write(b'\\n')",
          "156:         self._output.flush()",
          "157:         self._output.close()",
          "160: class Processor(object):",
          "161:     def __init__(self, _input=None, status_handler=None, event_handler=None,",
          "162:                  artifacts_handler=None, cancel_callback=None, finished_callback=None, **kwargs):",
          "163:         if _input is None:",
          "164:             _input = sys.stdin.buffer",
          "165:         self._input = _input",
          "167:         self.kwargs = kwargs",
          "168:         self.config = ansible_runner.RunnerConfig(**kwargs)",
          "169:         self.status_handler = status_handler",
          "170:         self.event_handler = event_handler",
          "171:         self.artifacts_handler = artifacts_handler",
          "173:         self.cancel_callback = cancel_callback",
          "174:         self.finished_callback = finished_callback",
          "176:         self.status = \"unstarted\"",
          "177:         self.rc = None",
          "179:     def status_callback(self, status_data):",
          "180:         self.status = status_data['status']",
          "182:         for plugin in ansible_runner.plugins:",
          "183:             ansible_runner.plugins[plugin].status_handler(self.config, status_data)",
          "184:         if self.status_handler is not None:",
          "185:             self.status_handler(status_data, runner_config=self.config)",
          "187:     def event_callback(self, event_data):",
          "188:         full_filename = os.path.join(self.config.artifact_dir,",
          "189:                                      'job_events',",
          "190:                                      '{}-{}.json'.format(event_data['counter'],",
          "191:                                                          event_data['uuid']))",
          "193:         if self.event_handler is not None:",
          "194:             should_write = self.event_handler(event_data)",
          "195:         else:",
          "196:             should_write = True",
          "197:         for plugin in ansible_runner.plugins:",
          "198:             ansible_runner.plugins[plugin].event_handler(self.config, event_data)",
          "199:         if should_write:",
          "200:             with codecs.open(full_filename, 'w', encoding='utf-8') as write_file:",
          "201:                 os.chmod(full_filename, stat.S_IRUSR | stat.S_IWUSR)",
          "202:                 json.dump(event_data, write_file)",
          "204:     def artifacts_callback(self, artifacts_data):",
          "205:         buf = io.BytesIO(artifacts_data)",
          "206:         with zipfile.ZipFile(buf, 'r') as archive:",
          "207:             archive.extractall(path=self.config.artifact_dir)",
          "209:         if self.artifacts_handler is not None:",
          "210:             self.artifacts_handler(self.config.artifact_dir)",
          "212:     def run(self):",
          "213:         self.config.prepare()",
          "215:         job_events_path = os.path.join(self.config.artifact_dir, 'job_events')",
          "216:         if not os.path.exists(job_events_path):",
          "217:             os.mkdir(job_events_path, 0o700)",
          "219:         while True:",
          "220:             line = self._input.readline()",
          "221:             data = json.loads(line)",
          "223:             if 'status' in data:",
          "224:                 self.status_callback(data)",
          "225:             elif 'artifacts' in data:",
          "226:                 self.artifacts_callback(self._input.read(data['artifacts']))",
          "227:             elif 'eof' in data:",
          "228:                 break",
          "229:             else:",
          "230:                 self.event_callback(data)",
          "232:         if self.finished_callback is not None:",
          "233:             self.finished_callback(self)",
          "234:         return self.status, self.rc",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "56c001d8afb7b1ed459df4e0cb8eb5d42ae5ca49",
      "candidate_info": {
        "commit_hash": "56c001d8afb7b1ed459df4e0cb8eb5d42ae5ca49",
        "repo": "ansible/ansible-runner",
        "commit_url": "https://github.com/ansible/ansible-runner/commit/56c001d8afb7b1ed459df4e0cb8eb5d42ae5ca49",
        "files": [
          "ansible_runner/streaming.py"
        ],
        "message": "Switch to not base64 encoding the artifacts files",
        "before_after_code_files": [
          "ansible_runner/streaming.py||ansible_runner/streaming.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ],
          "candidate": [
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ]
        }
      },
      "candidate_diff": {
        "ansible_runner/streaming.py||ansible_runner/streaming.py": [
          "File: ansible_runner/streaming.py -> ansible_runner/streaming.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "61:         if not os.path.exists(job_events_path):",
          "62:             os.mkdir(job_events_path, 0o700)",
          "65:             data = json.loads(line)",
          "66:             if 'status' in data:",
          "67:                 self.status_callback(data)",
          "68:             elif 'artifacts' in data:",
          "71:                 break",
          "72:             else:",
          "73:                 self.event_callback(data)",
          "",
          "[Removed Lines]",
          "64:         for line in self.control_in:",
          "69:                 self.artifacts_callback(data)",
          "70:             elif 'eof' in data:",
          "",
          "[Added Lines]",
          "64:         while True:",
          "65:             line = self.control_in.readline()",
          "71:                 self.artifacts_callback(self.control_in.read())",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "131:                 json.dump(event_data, write_file)",
          "133:     def artifacts_callback(self, artifacts_data):",
          "135:         with zipfile.ZipFile(buf, 'r') as archive:",
          "136:             archive.extractall(path=self.config.artifact_dir)",
          "",
          "[Removed Lines]",
          "134:         buf = io.BytesIO(base64.b64decode(artifacts_data['payload']))",
          "",
          "[Added Lines]",
          "135:         buf = io.BytesIO(artifacts_data)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "189:         self.worker_out.flush()",
          "191:     def artifacts_handler(self, artifact_dir):",
          "192:         buf = io.BytesIO()",
          "193:         with zipfile.ZipFile(buf, 'w', compression=zipfile.ZIP_DEFLATED, allowZip64=True) as archive:",
          "194:             for dirpath, dirs, files in os.walk(artifact_dir):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "193:         self.worker_out.write(json.dumps({'artifacts': True}).encode('utf-8'))",
          "194:         self.worker_out.write(b'\\n')",
          "195:         self.worker_out.flush()",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "199:                     archive.write(os.path.join(dirpath, fname), arcname=os.path.join(relpath, fname))",
          "200:             archive.close()",
          "208:         self.worker_out.flush()",
          "210:     def finished_callback(self, runner_obj):",
          "213:         self.worker_out.flush()",
          "214:         self.worker_out.close()",
          "",
          "[Removed Lines]",
          "202:         data = {",
          "203:             'artifacts': True,",
          "204:             'payload': base64.b64encode(buf.getvalue()).decode('ascii'),",
          "205:         }",
          "206:         self.worker_out.write(json.dumps(data).encode('utf-8'))",
          "207:         self.worker_out.write(b'\\n')",
          "211:         self.worker_out.write(json.dumps({'eof': True}).encode('utf-8'))",
          "212:         self.worker_out.write(b'\\n')",
          "",
          "[Added Lines]",
          "207:         self.worker_out.write(buf.getvalue())",
          "",
          "---------------"
        ]
      }
    }
  ]
}