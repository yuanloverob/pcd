{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "18fc8e8e023868f6e7fab3422c5ce57e690d7834",
      "candidate_info": {
        "commit_hash": "18fc8e8e023868f6e7fab3422c5ce57e690d7834",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/18fc8e8e023868f6e7fab3422c5ce57e690d7834",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelation.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelationSuite.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEPropagateEmptyRelation.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala"
        ],
        "message": "[SPARK-39915][SQL][3.3] Dataset.repartition(N) may not create N partitions Non-AQE part\n\n### What changes were proposed in this pull request?\n\nbackport https://github.com/apache/spark/pull/37706 for branch-3.3\n\nSkip optimize the root user-specified repartition in `PropagateEmptyRelation`.\n\n### Why are the changes needed?\n\nSpark should preserve the final repatition which can affect the final output partition which is user-specified.\n\nFor example:\n\n```scala\nspark.sql(\"select * from values(1) where 1 < rand()\").repartition(1)\n\n// before:\n== Optimized Logical Plan ==\nLocalTableScan <empty>, [col1#0]\n\n// after:\n== Optimized Logical Plan ==\nRepartition 1, true\n+- LocalRelation <empty>, [col1#0]\n```\n\n### Does this PR introduce _any_ user-facing change?\n\nyes, the empty plan may change\n\n### How was this patch tested?\n\nadd test\n\nCloses #37730 from ulysses-you/empty-3.3.\n\nAuthored-by: ulysses-you <ulyssesyou18@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelation.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelation.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelationSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelationSuite.scala",
          "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEPropagateEmptyRelation.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEPropagateEmptyRelation.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "491:       def repartition(num: Integer): LogicalPlan =",
          "492:         Repartition(num, shuffle = true, logicalPlan)",
          "494:       def distribute(exprs: Expression*)(n: Int): LogicalPlan =",
          "495:         RepartitionByExpression(exprs, logicalPlan, numPartitions = n)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "494:       def repartition(): LogicalPlan =",
          "495:         RepartitionByExpression(Seq.empty, logicalPlan, None)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelation.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelation.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelation.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelation.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import org.apache.spark.sql.catalyst.plans._",
          "24: import org.apache.spark.sql.catalyst.plans.logical._",
          "25: import org.apache.spark.sql.catalyst.rules._",
          "26: import org.apache.spark.sql.catalyst.trees.TreePattern.{LOCAL_RELATION, TRUE_OR_FALSE_LITERAL}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: import org.apache.spark.sql.catalyst.trees.TreeNodeTag",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "46: abstract class PropagateEmptyRelationBase extends Rule[LogicalPlan] with CastSupport {",
          "47:   protected def isEmpty(plan: LogicalPlan): Boolean = plan match {",
          "48:     case p: LocalRelation => p.data.isEmpty",
          "49:     case _ => false",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "49:   private[sql] val ROOT_REPARTITION = TreeNodeTag[Unit](\"ROOT_REPARTITION\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "136:       case _: Sort => empty(p)",
          "137:       case _: GlobalLimit if !p.isStreaming => empty(p)",
          "138:       case _: LocalLimit if !p.isStreaming => empty(p)",
          "141:       case _: RebalancePartitions => empty(p)",
          "",
          "[Removed Lines]",
          "139:       case _: Repartition => empty(p)",
          "140:       case _: RepartitionByExpression => empty(p)",
          "",
          "[Added Lines]",
          "143:       case _: RepartitionOperation =>",
          "144:         if (p.getTagValue(ROOT_REPARTITION).isEmpty) {",
          "145:           empty(p)",
          "146:         } else {",
          "147:           p.unsetTagValue(ROOT_REPARTITION)",
          "148:           p",
          "149:         }",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "160:       case _ => p",
          "161:     }",
          "162:   }",
          "163: }",
          "168: object PropagateEmptyRelation extends PropagateEmptyRelationBase {",
          "170:     _.containsAnyPattern(LOCAL_RELATION, TRUE_OR_FALSE_LITERAL), ruleId) {",
          "171:     commonApplyFunc",
          "172:   }",
          "",
          "[Removed Lines]",
          "169:   override def apply(plan: LogicalPlan): LogicalPlan = plan.transformUpWithPruning(",
          "",
          "[Added Lines]",
          "173:   protected def userSpecifiedRepartition(p: LogicalPlan): Boolean = p match {",
          "174:     case _: Repartition => true",
          "175:     case r: RepartitionByExpression",
          "176:       if r.optNumPartitions.isDefined || r.partitionExpressions.nonEmpty => true",
          "177:     case _ => false",
          "178:   }",
          "180:   protected def applyInternal(plan: LogicalPlan): LogicalPlan",
          "186:   private def addTagForRootRepartition(plan: LogicalPlan): LogicalPlan = plan match {",
          "187:     case p: Project => p.mapChildren(addTagForRootRepartition)",
          "188:     case f: Filter => f.mapChildren(addTagForRootRepartition)",
          "189:     case r if userSpecifiedRepartition(r) =>",
          "190:       r.setTagValue(ROOT_REPARTITION, ())",
          "191:       r",
          "192:     case _ => plan",
          "193:   }",
          "195:   override def apply(plan: LogicalPlan): LogicalPlan = {",
          "196:     val planWithTag = addTagForRootRepartition(plan)",
          "197:     applyInternal(planWithTag)",
          "198:   }",
          "205:   override protected def applyInternal(p: LogicalPlan): LogicalPlan = p.transformUpWithPruning(",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelationSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelationSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelationSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelationSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "309:     val optimized2 = Optimize.execute(plan2)",
          "310:     comparePlans(optimized2, expected)",
          "311:   }",
          "312: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "313:   test(\"Propagate empty relation with repartition\") {",
          "314:     val emptyRelation = LocalRelation($\"a\".int, $\"b\".int)",
          "315:     comparePlans(Optimize.execute(",
          "316:       emptyRelation.repartition(1).sortBy($\"a\".asc).analyze",
          "317:     ), emptyRelation.analyze)",
          "319:     comparePlans(Optimize.execute(",
          "320:       emptyRelation.distribute($\"a\")(1).sortBy($\"a\".asc).analyze",
          "321:     ), emptyRelation.analyze)",
          "323:     comparePlans(Optimize.execute(",
          "324:       emptyRelation.repartition().analyze",
          "325:     ), emptyRelation.analyze)",
          "327:     comparePlans(Optimize.execute(",
          "328:       emptyRelation.repartition(1).sortBy($\"a\".asc).repartition().analyze",
          "329:     ), emptyRelation.analyze)",
          "330:   }",
          "332:   test(\"SPARK-39915: Dataset.repartition(N) may not create N partitions\") {",
          "333:     val emptyRelation = LocalRelation($\"a\".int, $\"b\".int)",
          "334:     val p1 = emptyRelation.repartition(1).analyze",
          "335:     comparePlans(Optimize.execute(p1), p1)",
          "337:     val p2 = emptyRelation.repartition(1).select($\"a\").analyze",
          "338:     comparePlans(Optimize.execute(p2), p2)",
          "340:     val p3 = emptyRelation.repartition(1).where($\"a\" > rand(1)).analyze",
          "341:     comparePlans(Optimize.execute(p3), p3)",
          "343:     val p4 = emptyRelation.repartition(1).where($\"a\" > rand(1)).select($\"a\").analyze",
          "344:     comparePlans(Optimize.execute(p4), p4)",
          "346:     val p5 = emptyRelation.sortBy(\"$a\".asc).repartition().limit(1).repartition(1).analyze",
          "347:     val expected5 = emptyRelation.repartition(1).analyze",
          "348:     comparePlans(Optimize.execute(p5), expected5)",
          "349:   }",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEPropagateEmptyRelation.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEPropagateEmptyRelation.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEPropagateEmptyRelation.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AQEPropagateEmptyRelation.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "69:       empty(j)",
          "70:   }",
          "",
          "[Removed Lines]",
          "72:   def apply(plan: LogicalPlan): LogicalPlan = plan.transformUpWithPruning(",
          "",
          "[Added Lines]",
          "72:   override protected def applyInternal(p: LogicalPlan): LogicalPlan = p.transformUpWithPruning(",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "3281:       Row(java.sql.Date.valueOf(\"2020-02-01\"), java.sql.Date.valueOf(\"2020-02-01\")) ::",
          "3282:         Row(java.sql.Date.valueOf(\"2020-01-01\"), java.sql.Date.valueOf(\"2020-01-02\")) :: Nil)",
          "3283:   }",
          "3284: }",
          "3286: case class GroupByKey(a: Int, b: Int)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3285:   test(\"SPARK-39915: Dataset.repartition(N) may not create N partitions\") {",
          "3286:     withSQLConf(SQLConf.ADAPTIVE_EXECUTION_ENABLED.key -> \"false\") {",
          "3287:       val df = spark.sql(\"select * from values(1) where 1 < rand()\").repartition(2)",
          "3288:       assert(df.queryExecution.executedPlan.execute().getNumPartitions == 2)",
          "3289:     }",
          "3290:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "1a49de67e3fa0d25e84540313688cde82d6001df",
      "candidate_info": {
        "commit_hash": "1a49de67e3fa0d25e84540313688cde82d6001df",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/1a49de67e3fa0d25e84540313688cde82d6001df",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ],
        "message": "[SPARK-39177][SQL] Provide query context on map key not exists error when WSCG is off\n\n### What changes were proposed in this pull request?\n\nSimilar to https://github.com/apache/spark/pull/36525, this PR provides query context for \"map key not exists\" runtime error when WSCG is off.\n\n### Why are the changes needed?\n\nEnhance the runtime error query context for \"map key not exists\" runtime error. After changes, it works when the whole stage codegen is not available.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nUT\n\nCloses #36538 from gengliangwang/fixMapKeyContext.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>\n(cherry picked from commit 1afddf407436c3b315ec601fab5a4a1b2028e672)\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2262:   override protected def withNewChildrenInternal(",
          "2263:     newLeft: Expression, newRight: Expression): ElementAt = copy(left = newLeft, right = newRight)",
          "2264: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2265:   override def initQueryContext(): String = if (failOnError) {",
          "2266:     origin.context",
          "2267:   } else {",
          "2268:     \"\"",
          "2269:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/complexTypeExtractors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "345:   def getValueEval(",
          "",
          "[Removed Lines]",
          "342: trait GetMapValueUtil extends BinaryExpression with ImplicitCastInputTypes {",
          "",
          "[Added Lines]",
          "342: trait GetMapValueUtil",
          "343:   extends BinaryExpression with ImplicitCastInputTypes with SupportQueryContext {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "366:     if (!found) {",
          "367:       if (failOnError) {",
          "369:       } else {",
          "370:         null",
          "371:       }",
          "",
          "[Removed Lines]",
          "368:         throw QueryExecutionErrors.mapKeyNotExistError(ordinal, keyType, origin.context)",
          "",
          "[Added Lines]",
          "369:         throw QueryExecutionErrors.mapKeyNotExistError(ordinal, keyType, queryContext)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "398:     }",
          "400:     val keyJavaType = CodeGenerator.javaType(keyType)",
          "402:     val keyDt = ctx.addReferenceObj(\"keyType\", keyType, keyType.getClass.getName)",
          "403:     nullSafeCodeGen(ctx, ev, (eval1, eval2) => {",
          "404:       val keyNotFoundBranch = if (failOnError) {",
          "",
          "[Removed Lines]",
          "401:     lazy val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "",
          "[Added Lines]",
          "402:     lazy val errorContext = ctx.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "488:   override protected def withNewChildrenInternal(",
          "489:       newLeft: Expression, newRight: Expression): GetMapValue =",
          "490:     copy(child = newLeft, key = newRight)",
          "491: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "493:   override def initQueryContext(): String = if (failOnError) {",
          "494:     origin.context",
          "495:   } else {",
          "496:     \"\"",
          "497:   }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "4404:     }",
          "4405:   }",
          "4407:   test(\"SPARK-38589: try_avg should return null if overflow happens before merging\") {",
          "4408:     val yearMonthDf = Seq(Int.MaxValue, Int.MaxValue, 2)",
          "4409:       .map(Period.ofMonths)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4407:   test(\"SPARK-39177: Query context of getting map value should be serialized to executors\" +",
          "4408:     \" when WSCG is off\") {",
          "4409:     withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> \"false\",",
          "4410:       SQLConf.ANSI_ENABLED.key -> \"true\") {",
          "4411:       withTable(\"t\") {",
          "4412:         sql(\"create table t(m map<string, string>) using parquet\")",
          "4413:         sql(\"insert into t values map('a', 'b')\")",
          "4414:         Seq(",
          "4415:           \"select m['foo'] from t\",",
          "4416:           \"select element_at(m, 'foo') from t\").foreach { query =>",
          "4417:           val msg = intercept[SparkException] {",
          "4418:             sql(query).collect()",
          "4419:           }.getMessage",
          "4420:           assert(msg.contains(query))",
          "4421:         }",
          "4422:       }",
          "4423:     }",
          "4424:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "46e9e3b49a59b9d84a9b073d07c2bbd689fac5d9",
      "candidate_info": {
        "commit_hash": "46e9e3b49a59b9d84a9b073d07c2bbd689fac5d9",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/46e9e3b49a59b9d84a9b073d07c2bbd689fac5d9",
        "files": [
          "python/pyspark/ml/classification.py",
          "python/pyspark/ml/classification.pyi",
          "python/pyspark/ml/tests/typing/test_classification.yml"
        ],
        "message": "[SPARK-37398][PYTHON][ML] Inline type hints for pyspark.ml.classification\n\n### What changes were proposed in this pull request\n\nMigration of type hints for pyspark.ml.evaluation from stub file to inline type hints.\n\n### Why are the changes needed?\n\nPart of migration of type hints.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nExisting tests.\n\nCloses #36071 from hi-zir/SPARK-37398.\n\nAuthored-by: hi-zir <sql.alp@gmail.com>\nSigned-off-by: zero323 <mszymkiewicz@gmail.com>\n(cherry picked from commit 40cdb6d51c2befcfeac8fb5cf5faf178d1a5ee7b)\nSigned-off-by: zero323 <mszymkiewicz@gmail.com>",
        "before_after_code_files": [
          "python/pyspark/ml/classification.py||python/pyspark/ml/classification.py",
          "python/pyspark/ml/classification.pyi||python/pyspark/ml/classification.pyi"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/ml/classification.py||python/pyspark/ml/classification.py": [
          "File: python/pyspark/ml/classification.py -> python/pyspark/ml/classification.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: from abc import ABCMeta, abstractmethod",
          "24: from multiprocessing.pool import ThreadPool",
          "26: from pyspark import keyword_only, since, SparkContext, inheritable_thread_target",
          "27: from pyspark.ml import Estimator, Predictor, PredictionModel, Model",
          "28: from pyspark.ml.param.shared import (",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: from typing import (",
          "27:     Any,",
          "28:     Dict,",
          "29:     Generic,",
          "30:     Iterable,",
          "31:     List,",
          "32:     Optional,",
          "33:     Type,",
          "34:     TypeVar,",
          "35:     Union,",
          "36:     cast,",
          "37:     overload,",
          "38:     TYPE_CHECKING,",
          "39: )",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "74: )",
          "75: from pyspark.ml.wrapper import JavaParams, JavaPredictor, JavaPredictionModel, JavaWrapper",
          "76: from pyspark.ml.common import inherit_doc",
          "79: from pyspark.sql.functions import udf, when",
          "80: from pyspark.sql.types import ArrayType, DoubleType",
          "81: from pyspark.storagelevel import StorageLevel",
          "83: __all__ = [",
          "84:     \"LinearSVC\",",
          "85:     \"LinearSVCModel\",",
          "",
          "[Removed Lines]",
          "77: from pyspark.ml.linalg import Vectors, VectorUDT",
          "78: from pyspark.sql import DataFrame",
          "",
          "[Added Lines]",
          "92: from pyspark.ml.linalg import Matrix, Vector, Vectors, VectorUDT",
          "93: from pyspark.sql import DataFrame, Row",
          "99: if TYPE_CHECKING:",
          "100:     from pyspark.ml._typing import P, ParamMap",
          "101:     from py4j.java_gateway import JavaObject",
          "104: T = TypeVar(\"T\")",
          "105: JPM = TypeVar(\"JPM\", bound=JavaPredictionModel)",
          "106: CM = TypeVar(\"CM\", bound=\"ClassificationModel\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "129: @inherit_doc",
          "131:     \"\"\"",
          "132:     Classifier for classification tasks.",
          "133:     Classes are indexed {0, 1, ..., numClasses - 1}.",
          "134:     \"\"\"",
          "136:     @since(\"3.0.0\")",
          "138:         \"\"\"",
          "139:         Sets the value of :py:attr:`rawPredictionCol`.",
          "140:         \"\"\"",
          "",
          "[Removed Lines]",
          "130: class Classifier(Predictor, _ClassifierParams, metaclass=ABCMeta):",
          "137:     def setRawPredictionCol(self, value):",
          "",
          "[Added Lines]",
          "155: class Classifier(Predictor[CM], _ClassifierParams, Generic[CM], metaclass=ABCMeta):",
          "162:     def setRawPredictionCol(self: \"P\", value: str) -> \"P\":",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "149:     \"\"\"",
          "151:     @since(\"3.0.0\")",
          "153:         \"\"\"",
          "154:         Sets the value of :py:attr:`rawPredictionCol`.",
          "155:         \"\"\"",
          "156:         return self._set(rawPredictionCol=value)",
          "159:     @abstractmethod",
          "160:     @since(\"2.1.0\")",
          "162:         \"\"\"",
          "163:         Number of classes (values which the label can take).",
          "164:         \"\"\"",
          "",
          "[Removed Lines]",
          "152:     def setRawPredictionCol(self, value):",
          "158:     @property",
          "161:     def numClasses(self):",
          "",
          "[Added Lines]",
          "177:     def setRawPredictionCol(self: \"P\", value: str) -> \"P\":",
          "183:     @property  # type: ignore[misc]",
          "186:     def numClasses(self) -> int:",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "167:     @abstractmethod",
          "168:     @since(\"3.0.0\")",
          "170:         \"\"\"",
          "171:         Raw prediction for each possible label.",
          "172:         \"\"\"",
          "",
          "[Removed Lines]",
          "169:     def predictRaw(self, value):",
          "",
          "[Added Lines]",
          "194:     def predictRaw(self, value: Vector) -> Vector:",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "191:     \"\"\"",
          "193:     @since(\"3.0.0\")",
          "195:         \"\"\"",
          "196:         Sets the value of :py:attr:`probabilityCol`.",
          "197:         \"\"\"",
          "198:         return self._set(probabilityCol=value)",
          "200:     @since(\"3.0.0\")",
          "202:         \"\"\"",
          "203:         Sets the value of :py:attr:`thresholds`.",
          "204:         \"\"\"",
          "",
          "[Removed Lines]",
          "194:     def setProbabilityCol(self, value):",
          "201:     def setThresholds(self, value):",
          "",
          "[Added Lines]",
          "219:     def setProbabilityCol(self: \"P\", value: str) -> \"P\":",
          "226:     def setThresholds(self: \"P\", value: List[float]) -> \"P\":",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "214:     \"\"\"",
          "216:     @since(\"3.0.0\")",
          "218:         \"\"\"",
          "219:         Sets the value of :py:attr:`probabilityCol`.",
          "220:         \"\"\"",
          "221:         return self._set(probabilityCol=value)",
          "223:     @since(\"3.0.0\")",
          "225:         \"\"\"",
          "226:         Sets the value of :py:attr:`thresholds`.",
          "227:         \"\"\"",
          "",
          "[Removed Lines]",
          "217:     def setProbabilityCol(self, value):",
          "224:     def setThresholds(self, value):",
          "",
          "[Added Lines]",
          "242:     def setProbabilityCol(self: CM, value: str) -> CM:",
          "249:     def setThresholds(self: CM, value: List[float]) -> CM:",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "230:     @abstractmethod",
          "231:     @since(\"3.0.0\")",
          "233:         \"\"\"",
          "234:         Predict the probability of each class given the features.",
          "235:         \"\"\"",
          "",
          "[Removed Lines]",
          "232:     def predictProbability(self, value):",
          "",
          "[Added Lines]",
          "257:     def predictProbability(self, value: Vector) -> Vector:",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "239: @inherit_doc",
          "241:     \"\"\"",
          "242:     Java Classifier for classification tasks.",
          "243:     Classes are indexed {0, 1, ..., numClasses - 1}.",
          "244:     \"\"\"",
          "246:     @since(\"3.0.0\")",
          "248:         \"\"\"",
          "249:         Sets the value of :py:attr:`rawPredictionCol`.",
          "250:         \"\"\"",
          "",
          "[Removed Lines]",
          "240: class _JavaClassifier(Classifier, JavaPredictor, metaclass=ABCMeta):",
          "247:     def setRawPredictionCol(self, value):",
          "",
          "[Added Lines]",
          "265: class _JavaClassifier(Classifier, JavaPredictor[JPM], Generic[JPM], metaclass=ABCMeta):",
          "272:     def setRawPredictionCol(self: \"P\", value: str) -> \"P\":",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "254: @inherit_doc",
          "256:     \"\"\"",
          "257:     Java Model produced by a ``Classifier``.",
          "258:     Classes are indexed {0, 1, ..., numClasses - 1}.",
          "259:     To be mixed in with :class:`pyspark.ml.JavaModel`",
          "260:     \"\"\"",
          "263:     @since(\"2.1.0\")",
          "265:         \"\"\"",
          "266:         Number of classes (values which the label can take).",
          "267:         \"\"\"",
          "268:         return self._call_java(\"numClasses\")",
          "270:     @since(\"3.0.0\")",
          "272:         \"\"\"",
          "273:         Raw prediction for each possible label.",
          "274:         \"\"\"",
          "",
          "[Removed Lines]",
          "255: class _JavaClassificationModel(ClassificationModel, JavaPredictionModel):",
          "262:     @property",
          "264:     def numClasses(self):",
          "271:     def predictRaw(self, value):",
          "",
          "[Added Lines]",
          "280: class _JavaClassificationModel(ClassificationModel, JavaPredictionModel[T]):",
          "287:     @property  # type: ignore[misc]",
          "289:     def numClasses(self) -> int:",
          "296:     def predictRaw(self, value: Vector) -> Vector:",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "278: @inherit_doc",
          "280:     \"\"\"",
          "281:     Java Probabilistic Classifier for classification tasks.",
          "282:     \"\"\"",
          "",
          "[Removed Lines]",
          "279: class _JavaProbabilisticClassifier(ProbabilisticClassifier, _JavaClassifier, metaclass=ABCMeta):",
          "",
          "[Added Lines]",
          "304: class _JavaProbabilisticClassifier(",
          "305:     ProbabilisticClassifier, _JavaClassifier[JPM], Generic[JPM], metaclass=ABCMeta",
          "306: ):",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "287: @inherit_doc",
          "288: class _JavaProbabilisticClassificationModel(",
          "290: ):",
          "291:     \"\"\"",
          "292:     Java Model produced by a ``ProbabilisticClassifier``.",
          "293:     \"\"\"",
          "295:     @since(\"3.0.0\")",
          "297:         \"\"\"",
          "298:         Predict the probability of each class given the features.",
          "299:         \"\"\"",
          "",
          "[Removed Lines]",
          "289:     ProbabilisticClassificationModel, _JavaClassificationModel",
          "296:     def predictProbability(self, value):",
          "",
          "[Added Lines]",
          "316:     ProbabilisticClassificationModel, _JavaClassificationModel[T]",
          "323:     def predictProbability(self, value: Vector) -> Vector:",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "308:     .. versionadded:: 3.1.0",
          "309:     \"\"\"",
          "312:     @since(\"3.1.0\")",
          "314:         \"\"\"",
          "315:         Dataframe outputted by the model's `transform` method.",
          "316:         \"\"\"",
          "317:         return self._call_java(\"predictions\")",
          "320:     @since(\"3.1.0\")",
          "322:         \"\"\"",
          "323:         Field in \"predictions\" which gives the prediction of each class.",
          "324:         \"\"\"",
          "325:         return self._call_java(\"predictionCol\")",
          "328:     @since(\"3.1.0\")",
          "330:         \"\"\"",
          "331:         Field in \"predictions\" which gives the true label of each",
          "332:         instance.",
          "333:         \"\"\"",
          "334:         return self._call_java(\"labelCol\")",
          "337:     @since(\"3.1.0\")",
          "339:         \"\"\"",
          "340:         Field in \"predictions\" which gives the weight of each instance",
          "341:         as a vector.",
          "",
          "[Removed Lines]",
          "311:     @property",
          "313:     def predictions(self):",
          "319:     @property",
          "321:     def predictionCol(self):",
          "327:     @property",
          "329:     def labelCol(self):",
          "336:     @property",
          "338:     def weightCol(self):",
          "",
          "[Added Lines]",
          "338:     @property  # type: ignore[misc]",
          "340:     def predictions(self) -> DataFrame:",
          "346:     @property  # type: ignore[misc]",
          "348:     def predictionCol(self) -> str:",
          "354:     @property  # type: ignore[misc]",
          "356:     def labelCol(self) -> str:",
          "363:     @property  # type: ignore[misc]",
          "365:     def weightCol(self) -> str:",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "343:         return self._call_java(\"weightCol\")",
          "345:     @property",
          "347:         \"\"\"",
          "348:         Returns the sequence of labels in ascending order. This order matches the order used",
          "349:         in metrics which are specified as arrays over labels, e.g., truePositiveRateByLabel.",
          "",
          "[Removed Lines]",
          "346:     def labels(self):",
          "",
          "[Added Lines]",
          "373:     def labels(self) -> List[str]:",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "359:         \"\"\"",
          "360:         return self._call_java(\"labels\")",
          "363:     @since(\"3.1.0\")",
          "365:         \"\"\"",
          "366:         Returns true positive rate for each label (category).",
          "367:         \"\"\"",
          "368:         return self._call_java(\"truePositiveRateByLabel\")",
          "371:     @since(\"3.1.0\")",
          "373:         \"\"\"",
          "374:         Returns false positive rate for each label (category).",
          "375:         \"\"\"",
          "376:         return self._call_java(\"falsePositiveRateByLabel\")",
          "379:     @since(\"3.1.0\")",
          "381:         \"\"\"",
          "382:         Returns precision for each label (category).",
          "383:         \"\"\"",
          "384:         return self._call_java(\"precisionByLabel\")",
          "387:     @since(\"3.1.0\")",
          "389:         \"\"\"",
          "390:         Returns recall for each label (category).",
          "391:         \"\"\"",
          "392:         return self._call_java(\"recallByLabel\")",
          "394:     @since(\"3.1.0\")",
          "396:         \"\"\"",
          "397:         Returns f-measure for each label (category).",
          "398:         \"\"\"",
          "399:         return self._call_java(\"fMeasureByLabel\", beta)",
          "402:     @since(\"3.1.0\")",
          "404:         \"\"\"",
          "405:         Returns accuracy.",
          "406:         (equals to the total number of correctly classified instances",
          "",
          "[Removed Lines]",
          "362:     @property",
          "364:     def truePositiveRateByLabel(self):",
          "370:     @property",
          "372:     def falsePositiveRateByLabel(self):",
          "378:     @property",
          "380:     def precisionByLabel(self):",
          "386:     @property",
          "388:     def recallByLabel(self):",
          "395:     def fMeasureByLabel(self, beta=1.0):",
          "401:     @property",
          "403:     def accuracy(self):",
          "",
          "[Added Lines]",
          "389:     @property  # type: ignore[misc]",
          "391:     def truePositiveRateByLabel(self) -> List[float]:",
          "397:     @property  # type: ignore[misc]",
          "399:     def falsePositiveRateByLabel(self) -> List[float]:",
          "405:     @property  # type: ignore[misc]",
          "407:     def precisionByLabel(self) -> List[float]:",
          "413:     @property  # type: ignore[misc]",
          "415:     def recallByLabel(self) -> List[float]:",
          "422:     def fMeasureByLabel(self, beta: float = 1.0) -> List[float]:",
          "428:     @property  # type: ignore[misc]",
          "430:     def accuracy(self) -> float:",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "408:         \"\"\"",
          "409:         return self._call_java(\"accuracy\")",
          "412:     @since(\"3.1.0\")",
          "414:         \"\"\"",
          "415:         Returns weighted true positive rate.",
          "416:         (equals to precision, recall and f-measure)",
          "417:         \"\"\"",
          "418:         return self._call_java(\"weightedTruePositiveRate\")",
          "421:     @since(\"3.1.0\")",
          "423:         \"\"\"",
          "424:         Returns weighted false positive rate.",
          "425:         \"\"\"",
          "426:         return self._call_java(\"weightedFalsePositiveRate\")",
          "429:     @since(\"3.1.0\")",
          "431:         \"\"\"",
          "432:         Returns weighted averaged recall.",
          "433:         (equals to precision, recall and f-measure)",
          "434:         \"\"\"",
          "435:         return self._call_java(\"weightedRecall\")",
          "438:     @since(\"3.1.0\")",
          "440:         \"\"\"",
          "441:         Returns weighted averaged precision.",
          "442:         \"\"\"",
          "443:         return self._call_java(\"weightedPrecision\")",
          "445:     @since(\"3.1.0\")",
          "447:         \"\"\"",
          "448:         Returns weighted averaged f-measure.",
          "449:         \"\"\"",
          "",
          "[Removed Lines]",
          "411:     @property",
          "413:     def weightedTruePositiveRate(self):",
          "420:     @property",
          "422:     def weightedFalsePositiveRate(self):",
          "428:     @property",
          "430:     def weightedRecall(self):",
          "437:     @property",
          "439:     def weightedPrecision(self):",
          "446:     def weightedFMeasure(self, beta=1.0):",
          "",
          "[Added Lines]",
          "438:     @property  # type: ignore[misc]",
          "440:     def weightedTruePositiveRate(self) -> float:",
          "447:     @property  # type: ignore[misc]",
          "449:     def weightedFalsePositiveRate(self) -> float:",
          "455:     @property  # type: ignore[misc]",
          "457:     def weightedRecall(self) -> float:",
          "464:     @property  # type: ignore[misc]",
          "466:     def weightedPrecision(self) -> float:",
          "473:     def weightedFMeasure(self, beta: float = 1.0) -> float:",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "458:     .. versionadded:: 3.1.0",
          "459:     \"\"\"",
          "462:     @since(\"3.1.0\")",
          "464:         \"\"\"",
          "465:         Objective function (scaled loss + regularization) at each",
          "466:         iteration. It contains one more element, the initial state,",
          "",
          "[Removed Lines]",
          "461:     @property",
          "463:     def objectiveHistory(self):",
          "",
          "[Added Lines]",
          "488:     @property  # type: ignore[misc]",
          "490:     def objectiveHistory(self) -> List[float]:",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "468:         \"\"\"",
          "469:         return self._call_java(\"objectiveHistory\")",
          "472:     @since(\"3.1.0\")",
          "474:         \"\"\"",
          "475:         Number of training iterations until termination.",
          "476:         \"\"\"",
          "",
          "[Removed Lines]",
          "471:     @property",
          "473:     def totalIterations(self):",
          "",
          "[Added Lines]",
          "498:     @property  # type: ignore[misc]",
          "500:     def totalIterations(self) -> int:",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "485:     .. versionadded:: 3.1.0",
          "486:     \"\"\"",
          "489:     @since(\"3.1.0\")",
          "491:         \"\"\"",
          "492:         Field in \"predictions\" which gives the probability or raw prediction",
          "493:         of each class as a vector.",
          "",
          "[Removed Lines]",
          "488:     @property",
          "490:     def scoreCol(self):",
          "",
          "[Added Lines]",
          "515:     @property  # type: ignore[misc]",
          "517:     def scoreCol(self) -> str:",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "495:         return self._call_java(\"scoreCol\")",
          "497:     @property",
          "499:         \"\"\"",
          "500:         Returns the receiver operating characteristic (ROC) curve,",
          "501:         which is a Dataframe having two fields (FPR, TPR) with",
          "",
          "[Removed Lines]",
          "498:     def roc(self):",
          "",
          "[Added Lines]",
          "525:     def roc(self) -> DataFrame:",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "509:         \"\"\"",
          "510:         return self._call_java(\"roc\")",
          "513:     @since(\"3.1.0\")",
          "515:         \"\"\"",
          "516:         Computes the area under the receiver operating characteristic",
          "517:         (ROC) curve.",
          "518:         \"\"\"",
          "519:         return self._call_java(\"areaUnderROC\")",
          "522:     @since(\"3.1.0\")",
          "524:         \"\"\"",
          "525:         Returns the precision-recall curve, which is a Dataframe",
          "526:         containing two fields recall, precision with (0.0, 1.0) prepended",
          "",
          "[Removed Lines]",
          "512:     @property",
          "514:     def areaUnderROC(self):",
          "521:     @property",
          "523:     def pr(self):",
          "",
          "[Added Lines]",
          "539:     @property  # type: ignore[misc]",
          "541:     def areaUnderROC(self) -> float:",
          "548:     @property  # type: ignore[misc]",
          "550:     def pr(self) -> DataFrame:",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "528:         \"\"\"",
          "529:         return self._call_java(\"pr\")",
          "532:     @since(\"3.1.0\")",
          "534:         \"\"\"",
          "535:         Returns a dataframe with two fields (threshold, F-Measure) curve",
          "536:         with beta = 1.0.",
          "537:         \"\"\"",
          "538:         return self._call_java(\"fMeasureByThreshold\")",
          "541:     @since(\"3.1.0\")",
          "543:         \"\"\"",
          "544:         Returns a dataframe with two fields (threshold, precision) curve.",
          "545:         Every possible probability obtained in transforming the dataset",
          "",
          "[Removed Lines]",
          "531:     @property",
          "533:     def fMeasureByThreshold(self):",
          "540:     @property",
          "542:     def precisionByThreshold(self):",
          "",
          "[Added Lines]",
          "558:     @property  # type: ignore[misc]",
          "560:     def fMeasureByThreshold(self) -> DataFrame:",
          "567:     @property  # type: ignore[misc]",
          "569:     def precisionByThreshold(self) -> DataFrame:",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "547:         \"\"\"",
          "548:         return self._call_java(\"precisionByThreshold\")",
          "551:     @since(\"3.1.0\")",
          "553:         \"\"\"",
          "554:         Returns a dataframe with two fields (threshold, recall) curve.",
          "555:         Every possible probability obtained in transforming the dataset",
          "",
          "[Removed Lines]",
          "550:     @property",
          "552:     def recallByThreshold(self):",
          "",
          "[Added Lines]",
          "577:     @property  # type: ignore[misc]",
          "579:     def recallByThreshold(self) -> DataFrame:",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "576:     .. versionadded:: 3.0.0",
          "577:     \"\"\"",
          "580:         Params._dummy(),",
          "581:         \"threshold\",",
          "582:         \"The threshold in binary classification applied to the linear model\"",
          "",
          "[Removed Lines]",
          "579:     threshold = Param(",
          "",
          "[Added Lines]",
          "606:     threshold: Param[float] = Param(",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "585:         typeConverter=TypeConverters.toFloat,",
          "586:     )",
          "589:         super(_LinearSVCParams, self).__init__(*args)",
          "590:         self._setDefault(",
          "591:             maxIter=100,",
          "",
          "[Removed Lines]",
          "588:     def __init__(self, *args):",
          "",
          "[Added Lines]",
          "615:     def __init__(self, *args: Any) -> None:",
          "",
          "---------------",
          "--- Hunk 26 ---",
          "[Context before]",
          "602: @inherit_doc",
          "604:     \"\"\"",
          "605:     This binary classifier optimizes the Hinge Loss using the OWLQN optimizer.",
          "606:     Only supports L2 regularization currently.",
          "",
          "[Removed Lines]",
          "603: class LinearSVC(_JavaClassifier, _LinearSVCParams, JavaMLWritable, JavaMLReadable):",
          "",
          "[Added Lines]",
          "630: class LinearSVC(",
          "631:     _JavaClassifier[\"LinearSVCModel\"],",
          "632:     _LinearSVCParams,",
          "633:     JavaMLWritable,",
          "634:     JavaMLReadable[\"LinearSVC\"],",
          "635: ):",
          "",
          "---------------",
          "--- Hunk 27 ---",
          "[Context before]",
          "676:     True",
          "677:     \"\"\"",
          "679:     @keyword_only",
          "680:     def __init__(",
          "681:         self,",
          "696:     ):",
          "697:         \"\"\"",
          "698:         __init__(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\",
          "",
          "[Removed Lines]",
          "683:         featuresCol=\"features\",",
          "684:         labelCol=\"label\",",
          "685:         predictionCol=\"prediction\",",
          "686:         maxIter=100,",
          "687:         regParam=0.0,",
          "688:         tol=1e-6,",
          "689:         rawPredictionCol=\"rawPrediction\",",
          "690:         fitIntercept=True,",
          "691:         standardization=True,",
          "692:         threshold=0.0,",
          "693:         weightCol=None,",
          "694:         aggregationDepth=2,",
          "695:         maxBlockSizeInMB=0.0,",
          "",
          "[Added Lines]",
          "711:     _input_kwargs: Dict[str, Any]",
          "717:         featuresCol: str = \"features\",",
          "718:         labelCol: str = \"label\",",
          "719:         predictionCol: str = \"prediction\",",
          "720:         maxIter: int = 100,",
          "721:         regParam: float = 0.0,",
          "722:         tol: float = 1e-6,",
          "723:         rawPredictionCol: str = \"rawPrediction\",",
          "724:         fitIntercept: bool = True,",
          "725:         standardization: bool = True,",
          "726:         threshold: float = 0.0,",
          "727:         weightCol: Optional[str] = None,",
          "728:         aggregationDepth: int = 2,",
          "729:         maxBlockSizeInMB: float = 0.0,",
          "",
          "---------------",
          "--- Hunk 28 ---",
          "[Context before]",
          "712:     def setParams(",
          "713:         self,",
          "729:         \"\"\"",
          "730:         setParams(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\",
          "731:                   maxIter=100, regParam=0.0, tol=1e-6, rawPredictionCol=\"rawPrediction\", \\",
          "",
          "[Removed Lines]",
          "715:         featuresCol=\"features\",",
          "716:         labelCol=\"label\",",
          "717:         predictionCol=\"prediction\",",
          "718:         maxIter=100,",
          "719:         regParam=0.0,",
          "720:         tol=1e-6,",
          "721:         rawPredictionCol=\"rawPrediction\",",
          "722:         fitIntercept=True,",
          "723:         standardization=True,",
          "724:         threshold=0.0,",
          "725:         weightCol=None,",
          "726:         aggregationDepth=2,",
          "727:         maxBlockSizeInMB=0.0,",
          "728:     ):",
          "",
          "[Added Lines]",
          "749:         featuresCol: str = \"features\",",
          "750:         labelCol: str = \"label\",",
          "751:         predictionCol: str = \"prediction\",",
          "752:         maxIter: int = 100,",
          "753:         regParam: float = 0.0,",
          "754:         tol: float = 1e-6,",
          "755:         rawPredictionCol: str = \"rawPrediction\",",
          "756:         fitIntercept: bool = True,",
          "757:         standardization: bool = True,",
          "758:         threshold: float = 0.0,",
          "759:         weightCol: Optional[str] = None,",
          "760:         aggregationDepth: int = 2,",
          "761:         maxBlockSizeInMB: float = 0.0,",
          "762:     ) -> \"LinearSVC\":",
          "",
          "---------------",
          "--- Hunk 29 ---",
          "[Context before]",
          "736:         kwargs = self._input_kwargs",
          "737:         return self._set(**kwargs)",
          "740:         return LinearSVCModel(java_model)",
          "742:     @since(\"2.2.0\")",
          "744:         \"\"\"",
          "745:         Sets the value of :py:attr:`maxIter`.",
          "746:         \"\"\"",
          "747:         return self._set(maxIter=value)",
          "749:     @since(\"2.2.0\")",
          "751:         \"\"\"",
          "752:         Sets the value of :py:attr:`regParam`.",
          "753:         \"\"\"",
          "754:         return self._set(regParam=value)",
          "756:     @since(\"2.2.0\")",
          "758:         \"\"\"",
          "759:         Sets the value of :py:attr:`tol`.",
          "760:         \"\"\"",
          "761:         return self._set(tol=value)",
          "763:     @since(\"2.2.0\")",
          "765:         \"\"\"",
          "766:         Sets the value of :py:attr:`fitIntercept`.",
          "767:         \"\"\"",
          "768:         return self._set(fitIntercept=value)",
          "770:     @since(\"2.2.0\")",
          "772:         \"\"\"",
          "773:         Sets the value of :py:attr:`standardization`.",
          "774:         \"\"\"",
          "775:         return self._set(standardization=value)",
          "777:     @since(\"2.2.0\")",
          "779:         \"\"\"",
          "780:         Sets the value of :py:attr:`threshold`.",
          "781:         \"\"\"",
          "782:         return self._set(threshold=value)",
          "784:     @since(\"2.2.0\")",
          "786:         \"\"\"",
          "787:         Sets the value of :py:attr:`weightCol`.",
          "788:         \"\"\"",
          "789:         return self._set(weightCol=value)",
          "791:     @since(\"2.2.0\")",
          "793:         \"\"\"",
          "794:         Sets the value of :py:attr:`aggregationDepth`.",
          "795:         \"\"\"",
          "796:         return self._set(aggregationDepth=value)",
          "798:     @since(\"3.1.0\")",
          "800:         \"\"\"",
          "801:         Sets the value of :py:attr:`maxBlockSizeInMB`.",
          "802:         \"\"\"",
          "",
          "[Removed Lines]",
          "739:     def _create_model(self, java_model):",
          "743:     def setMaxIter(self, value):",
          "750:     def setRegParam(self, value):",
          "757:     def setTol(self, value):",
          "764:     def setFitIntercept(self, value):",
          "771:     def setStandardization(self, value):",
          "778:     def setThreshold(self, value):",
          "785:     def setWeightCol(self, value):",
          "792:     def setAggregationDepth(self, value):",
          "799:     def setMaxBlockSizeInMB(self, value):",
          "",
          "[Added Lines]",
          "773:     def _create_model(self, java_model: \"JavaObject\") -> \"LinearSVCModel\":",
          "777:     def setMaxIter(self, value: int) -> \"LinearSVC\":",
          "784:     def setRegParam(self, value: float) -> \"LinearSVC\":",
          "791:     def setTol(self, value: float) -> \"LinearSVC\":",
          "798:     def setFitIntercept(self, value: bool) -> \"LinearSVC\":",
          "805:     def setStandardization(self, value: bool) -> \"LinearSVC\":",
          "812:     def setThreshold(self, value: float) -> \"LinearSVC\":",
          "819:     def setWeightCol(self, value: str) -> \"LinearSVC\":",
          "826:     def setAggregationDepth(self, value: int) -> \"LinearSVC\":",
          "833:     def setMaxBlockSizeInMB(self, value: float) -> \"LinearSVC\":",
          "",
          "---------------",
          "--- Hunk 30 ---",
          "[Context before]",
          "806: class LinearSVCModel(",
          "808: ):",
          "809:     \"\"\"",
          "810:     Model fitted by LinearSVC.",
          "",
          "[Removed Lines]",
          "807:     _JavaClassificationModel, _LinearSVCParams, JavaMLWritable, JavaMLReadable, HasTrainingSummary",
          "",
          "[Added Lines]",
          "841:     _JavaClassificationModel[Vector],",
          "842:     _LinearSVCParams,",
          "843:     JavaMLWritable,",
          "844:     JavaMLReadable[\"LinearSVCModel\"],",
          "845:     HasTrainingSummary[\"LinearSVCTrainingSummary\"],",
          "",
          "---------------",
          "--- Hunk 31 ---",
          "[Context before]",
          "813:     \"\"\"",
          "815:     @since(\"3.0.0\")",
          "817:         \"\"\"",
          "818:         Sets the value of :py:attr:`threshold`.",
          "819:         \"\"\"",
          "820:         return self._set(threshold=value)",
          "823:     @since(\"2.2.0\")",
          "825:         \"\"\"",
          "826:         Model coefficients of Linear SVM Classifier.",
          "827:         \"\"\"",
          "828:         return self._call_java(\"coefficients\")",
          "831:     @since(\"2.2.0\")",
          "833:         \"\"\"",
          "834:         Model intercept of Linear SVM Classifier.",
          "835:         \"\"\"",
          "836:         return self._call_java(\"intercept\")",
          "838:     @since(\"3.1.0\")",
          "840:         \"\"\"",
          "841:         Gets summary (accuracy/precision/recall, objective history, total iterations) of model",
          "842:         trained on the training set. An exception is thrown if `trainingSummary is None`.",
          "",
          "[Removed Lines]",
          "816:     def setThreshold(self, value):",
          "822:     @property",
          "824:     def coefficients(self):",
          "830:     @property",
          "832:     def intercept(self):",
          "839:     def summary(self):",
          "",
          "[Added Lines]",
          "854:     def setThreshold(self, value: float) -> \"LinearSVCModel\":",
          "860:     @property  # type: ignore[misc]",
          "862:     def coefficients(self) -> Vector:",
          "868:     @property  # type: ignore[misc]",
          "870:     def intercept(self) -> float:",
          "877:     def summary(self) -> \"LinearSVCTrainingSummary\":",
          "",
          "---------------",
          "--- Hunk 32 ---",
          "[Context before]",
          "848:                 \"No training summary available for this %s\" % self.__class__.__name__",
          "849:             )",
          "852:         \"\"\"",
          "853:         Evaluates the model on a test dataset.",
          "",
          "[Removed Lines]",
          "851:     def evaluate(self, dataset):",
          "",
          "[Added Lines]",
          "889:     def evaluate(self, dataset: DataFrame) -> \"LinearSVCSummary\":",
          "",
          "---------------",
          "--- Hunk 33 ---",
          "[Context before]",
          "905:     .. versionadded:: 3.0.0",
          "906:     \"\"\"",
          "909:         Params._dummy(),",
          "910:         \"threshold\",",
          "911:         \"Threshold in binary classification prediction, in range [0, 1].\"",
          "",
          "[Removed Lines]",
          "908:     threshold = Param(",
          "",
          "[Added Lines]",
          "946:     threshold: Param[float] = Param(",
          "",
          "---------------",
          "--- Hunk 34 ---",
          "[Context before]",
          "914:         typeConverter=TypeConverters.toFloat,",
          "915:     )",
          "918:         Params._dummy(),",
          "919:         \"family\",",
          "920:         \"The name of family which is a description of the label distribution to \"",
          "",
          "[Removed Lines]",
          "917:     family = Param(",
          "",
          "[Added Lines]",
          "955:     family: Param[str] = Param(",
          "",
          "---------------",
          "--- Hunk 35 ---",
          "[Context before]",
          "922:         typeConverter=TypeConverters.toString,",
          "923:     )",
          "926:         Params._dummy(),",
          "927:         \"lowerBoundsOnCoefficients\",",
          "928:         \"The lower bounds on coefficients if fitting under bound \"",
          "",
          "[Removed Lines]",
          "925:     lowerBoundsOnCoefficients = Param(",
          "",
          "[Added Lines]",
          "963:     lowerBoundsOnCoefficients: Param[Matrix] = Param(",
          "",
          "---------------",
          "--- Hunk 36 ---",
          "[Context before]",
          "934:         typeConverter=TypeConverters.toMatrix,",
          "935:     )",
          "938:         Params._dummy(),",
          "939:         \"upperBoundsOnCoefficients\",",
          "940:         \"The upper bounds on coefficients if fitting under bound \"",
          "",
          "[Removed Lines]",
          "937:     upperBoundsOnCoefficients = Param(",
          "",
          "[Added Lines]",
          "975:     upperBoundsOnCoefficients: Param[Matrix] = Param(",
          "",
          "---------------",
          "--- Hunk 37 ---",
          "[Context before]",
          "946:         typeConverter=TypeConverters.toMatrix,",
          "947:     )",
          "950:         Params._dummy(),",
          "951:         \"lowerBoundsOnIntercepts\",",
          "952:         \"The lower bounds on intercepts if fitting under bound \"",
          "",
          "[Removed Lines]",
          "949:     lowerBoundsOnIntercepts = Param(",
          "",
          "[Added Lines]",
          "987:     lowerBoundsOnIntercepts: Param[Vector] = Param(",
          "",
          "---------------",
          "--- Hunk 38 ---",
          "[Context before]",
          "956:         typeConverter=TypeConverters.toVector,",
          "957:     )",
          "960:         Params._dummy(),",
          "961:         \"upperBoundsOnIntercepts\",",
          "962:         \"The upper bounds on intercepts if fitting under bound \"",
          "",
          "[Removed Lines]",
          "959:     upperBoundsOnIntercepts = Param(",
          "",
          "[Added Lines]",
          "997:     upperBoundsOnIntercepts: Param[Vector] = Param(",
          "",
          "---------------",
          "--- Hunk 39 ---",
          "[Context before]",
          "966:         typeConverter=TypeConverters.toVector,",
          "967:     )",
          "970:         super(_LogisticRegressionParams, self).__init__(*args)",
          "971:         self._setDefault(",
          "972:             maxIter=100, regParam=0.0, tol=1e-6, threshold=0.5, family=\"auto\", maxBlockSizeInMB=0.0",
          "973:         )",
          "975:     @since(\"1.4.0\")",
          "977:         \"\"\"",
          "978:         Sets the value of :py:attr:`threshold`.",
          "979:         Clears value of :py:attr:`thresholds` if it has been set.",
          "980:         \"\"\"",
          "981:         self._set(threshold=value)",
          "983:         return self",
          "985:     @since(\"1.4.0\")",
          "987:         \"\"\"",
          "988:         Get threshold for binary classification.",
          "",
          "[Removed Lines]",
          "969:     def __init__(self, *args):",
          "976:     def setThreshold(self, value):",
          "982:         self.clear(self.thresholds)",
          "986:     def getThreshold(self):",
          "",
          "[Added Lines]",
          "1007:     def __init__(self, *args: Any):",
          "1014:     def setThreshold(self: \"P\", value: float) -> \"P\":",
          "1020:         self.clear(self.thresholds)  # type: ignore[attr-defined]",
          "1024:     def getThreshold(self) -> float:",
          "",
          "---------------",
          "--- Hunk 40 ---",
          "[Context before]",
          "1006:             return self.getOrDefault(self.threshold)",
          "1008:     @since(\"1.5.0\")",
          "1010:         \"\"\"",
          "1011:         Sets the value of :py:attr:`thresholds`.",
          "1012:         Clears value of :py:attr:`threshold` if it has been set.",
          "1013:         \"\"\"",
          "1014:         self._set(thresholds=value)",
          "1016:         return self",
          "1018:     @since(\"1.5.0\")",
          "1020:         \"\"\"",
          "1021:         If :py:attr:`thresholds` is set, return its value.",
          "1022:         Otherwise, if :py:attr:`threshold` is set, return the equivalent thresholds for binary",
          "",
          "[Removed Lines]",
          "1009:     def setThresholds(self, value):",
          "1015:         self.clear(self.threshold)",
          "1019:     def getThresholds(self):",
          "",
          "[Added Lines]",
          "1047:     def setThresholds(self: \"P\", value: List[float]) -> \"P\":",
          "1053:         self.clear(self.threshold)  # type: ignore[attr-defined]",
          "1057:     def getThresholds(self) -> List[float]:",
          "",
          "---------------",
          "--- Hunk 41 ---",
          "[Context before]",
          "1030:         else:",
          "1031:             return self.getOrDefault(self.thresholds)",
          "1034:         if self.isSet(self.threshold) and self.isSet(self.thresholds):",
          "1035:             ts = self.getOrDefault(self.thresholds)",
          "1036:             if len(ts) != 2:",
          "",
          "[Removed Lines]",
          "1033:     def _checkThresholdConsistency(self):",
          "",
          "[Added Lines]",
          "1071:     def _checkThresholdConsistency(self) -> None:",
          "",
          "---------------",
          "--- Hunk 42 ---",
          "[Context before]",
          "1048:                 )",
          "1050:     @since(\"2.1.0\")",
          "1052:         \"\"\"",
          "1053:         Gets the value of :py:attr:`family` or its default value.",
          "1054:         \"\"\"",
          "1055:         return self.getOrDefault(self.family)",
          "1057:     @since(\"2.3.0\")",
          "1059:         \"\"\"",
          "1060:         Gets the value of :py:attr:`lowerBoundsOnCoefficients`",
          "1061:         \"\"\"",
          "1062:         return self.getOrDefault(self.lowerBoundsOnCoefficients)",
          "1064:     @since(\"2.3.0\")",
          "1066:         \"\"\"",
          "1067:         Gets the value of :py:attr:`upperBoundsOnCoefficients`",
          "1068:         \"\"\"",
          "1069:         return self.getOrDefault(self.upperBoundsOnCoefficients)",
          "1071:     @since(\"2.3.0\")",
          "1073:         \"\"\"",
          "1074:         Gets the value of :py:attr:`lowerBoundsOnIntercepts`",
          "1075:         \"\"\"",
          "1076:         return self.getOrDefault(self.lowerBoundsOnIntercepts)",
          "1078:     @since(\"2.3.0\")",
          "1080:         \"\"\"",
          "1081:         Gets the value of :py:attr:`upperBoundsOnIntercepts`",
          "1082:         \"\"\"",
          "",
          "[Removed Lines]",
          "1051:     def getFamily(self):",
          "1058:     def getLowerBoundsOnCoefficients(self):",
          "1065:     def getUpperBoundsOnCoefficients(self):",
          "1072:     def getLowerBoundsOnIntercepts(self):",
          "1079:     def getUpperBoundsOnIntercepts(self):",
          "",
          "[Added Lines]",
          "1089:     def getFamily(self) -> str:",
          "1096:     def getLowerBoundsOnCoefficients(self) -> Matrix:",
          "1103:     def getUpperBoundsOnCoefficients(self) -> Matrix:",
          "1110:     def getLowerBoundsOnIntercepts(self) -> Vector:",
          "1117:     def getUpperBoundsOnIntercepts(self) -> Vector:",
          "",
          "---------------",
          "--- Hunk 43 ---",
          "[Context before]",
          "1086: @inherit_doc",
          "1087: class LogisticRegression(",
          "1089: ):",
          "1090:     \"\"\"",
          "1091:     Logistic regression.",
          "",
          "[Removed Lines]",
          "1088:     _JavaProbabilisticClassifier, _LogisticRegressionParams, JavaMLWritable, JavaMLReadable",
          "",
          "[Added Lines]",
          "1126:     _JavaProbabilisticClassifier[\"LogisticRegressionModel\"],",
          "1127:     _LogisticRegressionParams,",
          "1128:     JavaMLWritable,",
          "1129:     JavaMLReadable[\"LogisticRegression\"],",
          "",
          "---------------",
          "--- Hunk 44 ---",
          "[Context before]",
          "1180:     True",
          "1181:     \"\"\"",
          "1184:     def __init__(",
          "1185:         self,",
          "1208:     ):",
          "1210:         \"\"\"",
          "1211:         __init__(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\",
          "1212:                  maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, \\",
          "",
          "[Removed Lines]",
          "1183:     @keyword_only",
          "1187:         featuresCol=\"features\",",
          "1188:         labelCol=\"label\",",
          "1189:         predictionCol=\"prediction\",",
          "1190:         maxIter=100,",
          "1191:         regParam=0.0,",
          "1192:         elasticNetParam=0.0,",
          "1193:         tol=1e-6,",
          "1194:         fitIntercept=True,",
          "1195:         threshold=0.5,",
          "1196:         thresholds=None,",
          "1197:         probabilityCol=\"probability\",",
          "1198:         rawPredictionCol=\"rawPrediction\",",
          "1199:         standardization=True,",
          "1200:         weightCol=None,",
          "1201:         aggregationDepth=2,",
          "1202:         family=\"auto\",",
          "1203:         lowerBoundsOnCoefficients=None,",
          "1204:         upperBoundsOnCoefficients=None,",
          "1205:         lowerBoundsOnIntercepts=None,",
          "1206:         upperBoundsOnIntercepts=None,",
          "1207:         maxBlockSizeInMB=0.0,",
          "",
          "[Added Lines]",
          "1224:     _input_kwargs: Dict[str, Any]",
          "1226:     @overload",
          "1227:     def __init__(",
          "1228:         self,",
          "1230:         featuresCol: str = ...,",
          "1231:         labelCol: str = ...,",
          "1232:         predictionCol: str = ...,",
          "1233:         maxIter: int = ...,",
          "1234:         regParam: float = ...,",
          "1235:         elasticNetParam: float = ...,",
          "1236:         tol: float = ...,",
          "1237:         fitIntercept: bool = ...,",
          "1238:         threshold: float = ...,",
          "1239:         probabilityCol: str = ...,",
          "1240:         rawPredictionCol: str = ...,",
          "1241:         standardization: bool = ...,",
          "1242:         weightCol: Optional[str] = ...,",
          "1243:         aggregationDepth: int = ...,",
          "1244:         family: str = ...,",
          "1245:         lowerBoundsOnCoefficients: Optional[Matrix] = ...,",
          "1246:         upperBoundsOnCoefficients: Optional[Matrix] = ...,",
          "1247:         lowerBoundsOnIntercepts: Optional[Vector] = ...,",
          "1248:         upperBoundsOnIntercepts: Optional[Vector] = ...,",
          "1249:         maxBlockSizeInMB: float = ...,",
          "1250:     ):",
          "1251:         ...",
          "1253:     @overload",
          "1257:         featuresCol: str = ...,",
          "1258:         labelCol: str = ...,",
          "1259:         predictionCol: str = ...,",
          "1260:         maxIter: int = ...,",
          "1261:         regParam: float = ...,",
          "1262:         elasticNetParam: float = ...,",
          "1263:         tol: float = ...,",
          "1264:         fitIntercept: bool = ...,",
          "1265:         thresholds: Optional[List[float]] = ...,",
          "1266:         probabilityCol: str = ...,",
          "1267:         rawPredictionCol: str = ...,",
          "1268:         standardization: bool = ...,",
          "1269:         weightCol: Optional[str] = ...,",
          "1270:         aggregationDepth: int = ...,",
          "1271:         family: str = ...,",
          "1272:         lowerBoundsOnCoefficients: Optional[Matrix] = ...,",
          "1273:         upperBoundsOnCoefficients: Optional[Matrix] = ...,",
          "1274:         lowerBoundsOnIntercepts: Optional[Vector] = ...,",
          "1275:         upperBoundsOnIntercepts: Optional[Vector] = ...,",
          "1276:         maxBlockSizeInMB: float = ...,",
          "1278:         ...",
          "1280:     @keyword_only",
          "1281:     def __init__(",
          "1282:         self,",
          "1284:         featuresCol: str = \"features\",",
          "1285:         labelCol: str = \"label\",",
          "1286:         predictionCol: str = \"prediction\",",
          "1287:         maxIter: int = 100,",
          "1288:         regParam: float = 0.0,",
          "1289:         elasticNetParam: float = 0.0,",
          "1290:         tol: float = 1e-6,",
          "1291:         fitIntercept: bool = True,",
          "1292:         threshold: float = 0.5,",
          "1293:         thresholds: Optional[List[float]] = None,",
          "1294:         probabilityCol: str = \"probability\",",
          "1295:         rawPredictionCol: str = \"rawPrediction\",",
          "1296:         standardization: bool = True,",
          "1297:         weightCol: Optional[str] = None,",
          "1298:         aggregationDepth: int = 2,",
          "1299:         family: str = \"auto\",",
          "1300:         lowerBoundsOnCoefficients: Optional[Matrix] = None,",
          "1301:         upperBoundsOnCoefficients: Optional[Matrix] = None,",
          "1302:         lowerBoundsOnIntercepts: Optional[Vector] = None,",
          "1303:         upperBoundsOnIntercepts: Optional[Vector] = None,",
          "1304:         maxBlockSizeInMB: float = 0.0,",
          "1305:     ):",
          "",
          "---------------",
          "--- Hunk 45 ---",
          "[Context before]",
          "1226:         self.setParams(**kwargs)",
          "1227:         self._checkThresholdConsistency()",
          "1229:     @keyword_only",
          "1230:     @since(\"1.3.0\")",
          "1231:     def setParams(",
          "1232:         self,",
          "1256:         \"\"\"",
          "1257:         setParams(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\",
          "1258:                   maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, \\",
          "",
          "[Removed Lines]",
          "1234:         featuresCol=\"features\",",
          "1235:         labelCol=\"label\",",
          "1236:         predictionCol=\"prediction\",",
          "1237:         maxIter=100,",
          "1238:         regParam=0.0,",
          "1239:         elasticNetParam=0.0,",
          "1240:         tol=1e-6,",
          "1241:         fitIntercept=True,",
          "1242:         threshold=0.5,",
          "1243:         thresholds=None,",
          "1244:         probabilityCol=\"probability\",",
          "1245:         rawPredictionCol=\"rawPrediction\",",
          "1246:         standardization=True,",
          "1247:         weightCol=None,",
          "1248:         aggregationDepth=2,",
          "1249:         family=\"auto\",",
          "1250:         lowerBoundsOnCoefficients=None,",
          "1251:         upperBoundsOnCoefficients=None,",
          "1252:         lowerBoundsOnIntercepts=None,",
          "1253:         upperBoundsOnIntercepts=None,",
          "1254:         maxBlockSizeInMB=0.0,",
          "1255:     ):",
          "",
          "[Added Lines]",
          "1325:     @overload",
          "1326:     def setParams(",
          "1327:         self,",
          "1329:         featuresCol: str = ...,",
          "1330:         labelCol: str = ...,",
          "1331:         predictionCol: str = ...,",
          "1332:         maxIter: int = ...,",
          "1333:         regParam: float = ...,",
          "1334:         elasticNetParam: float = ...,",
          "1335:         tol: float = ...,",
          "1336:         fitIntercept: bool = ...,",
          "1337:         threshold: float = ...,",
          "1338:         probabilityCol: str = ...,",
          "1339:         rawPredictionCol: str = ...,",
          "1340:         standardization: bool = ...,",
          "1341:         weightCol: Optional[str] = ...,",
          "1342:         aggregationDepth: int = ...,",
          "1343:         family: str = ...,",
          "1344:         lowerBoundsOnCoefficients: Optional[Matrix] = ...,",
          "1345:         upperBoundsOnCoefficients: Optional[Matrix] = ...,",
          "1346:         lowerBoundsOnIntercepts: Optional[Vector] = ...,",
          "1347:         upperBoundsOnIntercepts: Optional[Vector] = ...,",
          "1348:         maxBlockSizeInMB: float = ...,",
          "1349:     ) -> \"LogisticRegression\":",
          "1350:         ...",
          "1352:     @overload",
          "1353:     def setParams(",
          "1354:         self,",
          "1356:         featuresCol: str = ...,",
          "1357:         labelCol: str = ...,",
          "1358:         predictionCol: str = ...,",
          "1359:         maxIter: int = ...,",
          "1360:         regParam: float = ...,",
          "1361:         elasticNetParam: float = ...,",
          "1362:         tol: float = ...,",
          "1363:         fitIntercept: bool = ...,",
          "1364:         thresholds: Optional[List[float]] = ...,",
          "1365:         probabilityCol: str = ...,",
          "1366:         rawPredictionCol: str = ...,",
          "1367:         standardization: bool = ...,",
          "1368:         weightCol: Optional[str] = ...,",
          "1369:         aggregationDepth: int = ...,",
          "1370:         family: str = ...,",
          "1371:         lowerBoundsOnCoefficients: Optional[Matrix] = ...,",
          "1372:         upperBoundsOnCoefficients: Optional[Matrix] = ...,",
          "1373:         lowerBoundsOnIntercepts: Optional[Vector] = ...,",
          "1374:         upperBoundsOnIntercepts: Optional[Vector] = ...,",
          "1375:         maxBlockSizeInMB: float = ...,",
          "1376:     ) -> \"LogisticRegression\":",
          "1377:         ...",
          "1384:         featuresCol: str = \"features\",",
          "1385:         labelCol: str = \"label\",",
          "1386:         predictionCol: str = \"prediction\",",
          "1387:         maxIter: int = 100,",
          "1388:         regParam: float = 0.0,",
          "1389:         elasticNetParam: float = 0.0,",
          "1390:         tol: float = 1e-6,",
          "1391:         fitIntercept: bool = True,",
          "1392:         threshold: float = 0.5,",
          "1393:         thresholds: Optional[List[float]] = None,",
          "1394:         probabilityCol: str = \"probability\",",
          "1395:         rawPredictionCol: str = \"rawPrediction\",",
          "1396:         standardization: bool = True,",
          "1397:         weightCol: Optional[str] = None,",
          "1398:         aggregationDepth: int = 2,",
          "1399:         family: str = \"auto\",",
          "1400:         lowerBoundsOnCoefficients: Optional[Matrix] = None,",
          "1401:         upperBoundsOnCoefficients: Optional[Matrix] = None,",
          "1402:         lowerBoundsOnIntercepts: Optional[Vector] = None,",
          "1403:         upperBoundsOnIntercepts: Optional[Vector] = None,",
          "1404:         maxBlockSizeInMB: float = 0.0,",
          "1405:     ) -> \"LogisticRegression\":",
          "",
          "---------------",
          "--- Hunk 46 ---",
          "[Context before]",
          "1270:         self._checkThresholdConsistency()",
          "1271:         return self",
          "1274:         return LogisticRegressionModel(java_model)",
          "1276:     @since(\"2.1.0\")",
          "1278:         \"\"\"",
          "1279:         Sets the value of :py:attr:`family`.",
          "1280:         \"\"\"",
          "1281:         return self._set(family=value)",
          "1283:     @since(\"2.3.0\")",
          "1285:         \"\"\"",
          "1286:         Sets the value of :py:attr:`lowerBoundsOnCoefficients`",
          "1287:         \"\"\"",
          "1288:         return self._set(lowerBoundsOnCoefficients=value)",
          "1290:     @since(\"2.3.0\")",
          "1292:         \"\"\"",
          "1293:         Sets the value of :py:attr:`upperBoundsOnCoefficients`",
          "1294:         \"\"\"",
          "1295:         return self._set(upperBoundsOnCoefficients=value)",
          "1297:     @since(\"2.3.0\")",
          "1299:         \"\"\"",
          "1300:         Sets the value of :py:attr:`lowerBoundsOnIntercepts`",
          "1301:         \"\"\"",
          "1302:         return self._set(lowerBoundsOnIntercepts=value)",
          "1304:     @since(\"2.3.0\")",
          "1306:         \"\"\"",
          "1307:         Sets the value of :py:attr:`upperBoundsOnIntercepts`",
          "1308:         \"\"\"",
          "1309:         return self._set(upperBoundsOnIntercepts=value)",
          "1312:         \"\"\"",
          "1313:         Sets the value of :py:attr:`maxIter`.",
          "1314:         \"\"\"",
          "1315:         return self._set(maxIter=value)",
          "1318:         \"\"\"",
          "1319:         Sets the value of :py:attr:`regParam`.",
          "1320:         \"\"\"",
          "1321:         return self._set(regParam=value)",
          "1324:         \"\"\"",
          "1325:         Sets the value of :py:attr:`tol`.",
          "1326:         \"\"\"",
          "1327:         return self._set(tol=value)",
          "1330:         \"\"\"",
          "1331:         Sets the value of :py:attr:`elasticNetParam`.",
          "1332:         \"\"\"",
          "1333:         return self._set(elasticNetParam=value)",
          "1336:         \"\"\"",
          "1337:         Sets the value of :py:attr:`fitIntercept`.",
          "1338:         \"\"\"",
          "1339:         return self._set(fitIntercept=value)",
          "1342:         \"\"\"",
          "1343:         Sets the value of :py:attr:`standardization`.",
          "1344:         \"\"\"",
          "1345:         return self._set(standardization=value)",
          "1348:         \"\"\"",
          "1349:         Sets the value of :py:attr:`weightCol`.",
          "1350:         \"\"\"",
          "1351:         return self._set(weightCol=value)",
          "1354:         \"\"\"",
          "1355:         Sets the value of :py:attr:`aggregationDepth`.",
          "1356:         \"\"\"",
          "1357:         return self._set(aggregationDepth=value)",
          "1359:     @since(\"3.1.0\")",
          "1361:         \"\"\"",
          "1362:         Sets the value of :py:attr:`maxBlockSizeInMB`.",
          "1363:         \"\"\"",
          "",
          "[Removed Lines]",
          "1273:     def _create_model(self, java_model):",
          "1277:     def setFamily(self, value):",
          "1284:     def setLowerBoundsOnCoefficients(self, value):",
          "1291:     def setUpperBoundsOnCoefficients(self, value):",
          "1298:     def setLowerBoundsOnIntercepts(self, value):",
          "1305:     def setUpperBoundsOnIntercepts(self, value):",
          "1311:     def setMaxIter(self, value):",
          "1317:     def setRegParam(self, value):",
          "1323:     def setTol(self, value):",
          "1329:     def setElasticNetParam(self, value):",
          "1335:     def setFitIntercept(self, value):",
          "1341:     def setStandardization(self, value):",
          "1347:     def setWeightCol(self, value):",
          "1353:     def setAggregationDepth(self, value):",
          "1360:     def setMaxBlockSizeInMB(self, value):",
          "",
          "[Added Lines]",
          "1423:     def _create_model(self, java_model: \"JavaObject\") -> \"LogisticRegressionModel\":",
          "1427:     def setFamily(self, value: str) -> \"LogisticRegression\":",
          "1434:     def setLowerBoundsOnCoefficients(self, value: Matrix) -> \"LogisticRegression\":",
          "1441:     def setUpperBoundsOnCoefficients(self, value: Matrix) -> \"LogisticRegression\":",
          "1448:     def setLowerBoundsOnIntercepts(self, value: Vector) -> \"LogisticRegression\":",
          "1455:     def setUpperBoundsOnIntercepts(self, value: Vector) -> \"LogisticRegression\":",
          "1461:     def setMaxIter(self, value: int) -> \"LogisticRegression\":",
          "1467:     def setRegParam(self, value: float) -> \"LogisticRegression\":",
          "1473:     def setTol(self, value: float) -> \"LogisticRegression\":",
          "1479:     def setElasticNetParam(self, value: float) -> \"LogisticRegression\":",
          "1485:     def setFitIntercept(self, value: bool) -> \"LogisticRegression\":",
          "1491:     def setStandardization(self, value: bool) -> \"LogisticRegression\":",
          "1497:     def setWeightCol(self, value: str) -> \"LogisticRegression\":",
          "1503:     def setAggregationDepth(self, value: int) -> \"LogisticRegression\":",
          "1510:     def setMaxBlockSizeInMB(self, value: float) -> \"LogisticRegression\":",
          "",
          "---------------",
          "--- Hunk 47 ---",
          "[Context before]",
          "1367: class LogisticRegressionModel(",
          "1369:     _LogisticRegressionParams,",
          "1370:     JavaMLWritable,",
          "1373: ):",
          "1374:     \"\"\"",
          "1375:     Model fitted by LogisticRegression.",
          "",
          "[Removed Lines]",
          "1368:     _JavaProbabilisticClassificationModel,",
          "1371:     JavaMLReadable,",
          "1372:     HasTrainingSummary,",
          "",
          "[Added Lines]",
          "1518:     _JavaProbabilisticClassificationModel[Vector],",
          "1521:     JavaMLReadable[\"LogisticRegressionModel\"],",
          "1522:     HasTrainingSummary[\"LogisticRegressionTrainingSummary\"],",
          "",
          "---------------",
          "--- Hunk 48 ---",
          "[Context before]",
          "1377:     .. versionadded:: 1.3.0",
          "1378:     \"\"\"",
          "1381:     @since(\"2.0.0\")",
          "1383:         \"\"\"",
          "1384:         Model coefficients of binomial logistic regression.",
          "1385:         An exception is thrown in the case of multinomial logistic regression.",
          "1386:         \"\"\"",
          "1387:         return self._call_java(\"coefficients\")",
          "1390:     @since(\"1.4.0\")",
          "1392:         \"\"\"",
          "1393:         Model intercept of binomial logistic regression.",
          "1394:         An exception is thrown in the case of multinomial logistic regression.",
          "1395:         \"\"\"",
          "1396:         return self._call_java(\"intercept\")",
          "1399:     @since(\"2.1.0\")",
          "1401:         \"\"\"",
          "1402:         Model coefficients.",
          "1403:         \"\"\"",
          "1404:         return self._call_java(\"coefficientMatrix\")",
          "1407:     @since(\"2.1.0\")",
          "1409:         \"\"\"",
          "1410:         Model intercept.",
          "1411:         \"\"\"",
          "1412:         return self._call_java(\"interceptVector\")",
          "1415:     @since(\"2.0.0\")",
          "1417:         \"\"\"",
          "1418:         Gets summary (accuracy/precision/recall, objective history, total iterations) of model",
          "1419:         trained on the training set. An exception is thrown if `trainingSummary is None`.",
          "",
          "[Removed Lines]",
          "1380:     @property",
          "1382:     def coefficients(self):",
          "1389:     @property",
          "1391:     def intercept(self):",
          "1398:     @property",
          "1400:     def coefficientMatrix(self):",
          "1406:     @property",
          "1408:     def interceptVector(self):",
          "1414:     @property",
          "1416:     def summary(self):",
          "",
          "[Added Lines]",
          "1530:     @property  # type: ignore[misc]",
          "1532:     def coefficients(self) -> Vector:",
          "1539:     @property  # type: ignore[misc]",
          "1541:     def intercept(self) -> float:",
          "1548:     @property  # type: ignore[misc]",
          "1550:     def coefficientMatrix(self) -> Matrix:",
          "1556:     @property  # type: ignore[misc]",
          "1558:     def interceptVector(self) -> Vector:",
          "1564:     @property  # type: ignore[misc]",
          "1566:     def summary(self) -> \"LogisticRegressionTrainingSummary\":",
          "",
          "---------------",
          "--- Hunk 49 ---",
          "[Context before]",
          "1432:                 \"No training summary available for this %s\" % self.__class__.__name__",
          "1433:             )",
          "1436:         \"\"\"",
          "1437:         Evaluates the model on a test dataset.",
          "",
          "[Removed Lines]",
          "1435:     def evaluate(self, dataset):",
          "",
          "[Added Lines]",
          "1585:     def evaluate(self, dataset: DataFrame) -> \"LogisticRegressionSummary\":",
          "",
          "---------------",
          "--- Hunk 50 ---",
          "[Context before]",
          "1459:     .. versionadded:: 2.0.0",
          "1460:     \"\"\"",
          "1463:     @since(\"2.0.0\")",
          "1465:         \"\"\"",
          "1466:         Field in \"predictions\" which gives the probability",
          "1467:         of each class as a vector.",
          "1468:         \"\"\"",
          "1469:         return self._call_java(\"probabilityCol\")",
          "1472:     @since(\"2.0.0\")",
          "1474:         \"\"\"",
          "1475:         Field in \"predictions\" which gives the features of each instance",
          "1476:         as a vector.",
          "",
          "[Removed Lines]",
          "1462:     @property",
          "1464:     def probabilityCol(self):",
          "1471:     @property",
          "1473:     def featuresCol(self):",
          "",
          "[Added Lines]",
          "1612:     @property  # type: ignore[misc]",
          "1614:     def probabilityCol(self) -> str:",
          "1621:     @property  # type: ignore[misc]",
          "1623:     def featuresCol(self) -> str:",
          "",
          "---------------",
          "--- Hunk 51 ---",
          "[Context before]",
          "1519:     Params for :py:class:`DecisionTreeClassifier` and :py:class:`DecisionTreeClassificationModel`.",
          "1520:     \"\"\"",
          "1523:         super(_DecisionTreeClassifierParams, self).__init__(*args)",
          "1524:         self._setDefault(",
          "1525:             maxDepth=5,",
          "",
          "[Removed Lines]",
          "1522:     def __init__(self, *args):",
          "",
          "[Added Lines]",
          "1672:     def __init__(self, *args: Any):",
          "",
          "---------------",
          "--- Hunk 52 ---",
          "[Context before]",
          "1538: @inherit_doc",
          "1539: class DecisionTreeClassifier(",
          "1541: ):",
          "1542:     \"\"\"",
          "1543:     `Decision tree <http://en.wikipedia.org/wiki/Decision_tree_learning>`_",
          "",
          "[Removed Lines]",
          "1540:     _JavaProbabilisticClassifier, _DecisionTreeClassifierParams, JavaMLWritable, JavaMLReadable",
          "",
          "[Added Lines]",
          "1690:     _JavaProbabilisticClassifier[\"DecisionTreeClassificationModel\"],",
          "1691:     _DecisionTreeClassifierParams,",
          "1692:     JavaMLWritable,",
          "1693:     JavaMLReadable[\"DecisionTreeClassifier\"],",
          "",
          "---------------",
          "--- Hunk 53 ---",
          "[Context before]",
          "1619:     DecisionTreeClassificationModel...depth=1, numNodes=3...",
          "1620:     \"\"\"",
          "1622:     @keyword_only",
          "1623:     def __init__(",
          "1624:         self,",
          "1643:     ):",
          "1644:         \"\"\"",
          "1645:         __init__(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\",
          "",
          "[Removed Lines]",
          "1626:         featuresCol=\"features\",",
          "1627:         labelCol=\"label\",",
          "1628:         predictionCol=\"prediction\",",
          "1629:         probabilityCol=\"probability\",",
          "1630:         rawPredictionCol=\"rawPrediction\",",
          "1631:         maxDepth=5,",
          "1632:         maxBins=32,",
          "1633:         minInstancesPerNode=1,",
          "1634:         minInfoGain=0.0,",
          "1635:         maxMemoryInMB=256,",
          "1636:         cacheNodeIds=False,",
          "1637:         checkpointInterval=10,",
          "1638:         impurity=\"gini\",",
          "1639:         seed=None,",
          "1640:         weightCol=None,",
          "1641:         leafCol=\"\",",
          "1642:         minWeightFractionPerNode=0.0,",
          "",
          "[Added Lines]",
          "1775:     _input_kwargs: Dict[str, Any]",
          "1781:         featuresCol: str = \"features\",",
          "1782:         labelCol: str = \"label\",",
          "1783:         predictionCol: str = \"prediction\",",
          "1784:         probabilityCol: str = \"probability\",",
          "1785:         rawPredictionCol: str = \"rawPrediction\",",
          "1786:         maxDepth: int = 5,",
          "1787:         maxBins: int = 32,",
          "1788:         minInstancesPerNode: int = 1,",
          "1789:         minInfoGain: float = 0.0,",
          "1790:         maxMemoryInMB: int = 256,",
          "1791:         cacheNodeIds: bool = False,",
          "1792:         checkpointInterval: int = 10,",
          "1793:         impurity: str = \"gini\",",
          "1794:         seed: Optional[int] = None,",
          "1795:         weightCol: Optional[str] = None,",
          "1796:         leafCol: str = \"\",",
          "1797:         minWeightFractionPerNode: float = 0.0,",
          "",
          "---------------",
          "--- Hunk 54 ---",
          "[Context before]",
          "1660:     def setParams(",
          "1661:         self,",
          "1681:         \"\"\"",
          "1682:         setParams(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\",
          "1683:                   probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\", \\",
          "",
          "[Removed Lines]",
          "1663:         featuresCol=\"features\",",
          "1664:         labelCol=\"label\",",
          "1665:         predictionCol=\"prediction\",",
          "1666:         probabilityCol=\"probability\",",
          "1667:         rawPredictionCol=\"rawPrediction\",",
          "1668:         maxDepth=5,",
          "1669:         maxBins=32,",
          "1670:         minInstancesPerNode=1,",
          "1671:         minInfoGain=0.0,",
          "1672:         maxMemoryInMB=256,",
          "1673:         cacheNodeIds=False,",
          "1674:         checkpointInterval=10,",
          "1675:         impurity=\"gini\",",
          "1676:         seed=None,",
          "1677:         weightCol=None,",
          "1678:         leafCol=\"\",",
          "1679:         minWeightFractionPerNode=0.0,",
          "1680:     ):",
          "",
          "[Added Lines]",
          "1818:         featuresCol: str = \"features\",",
          "1819:         labelCol: str = \"label\",",
          "1820:         predictionCol: str = \"prediction\",",
          "1821:         probabilityCol: str = \"probability\",",
          "1822:         rawPredictionCol: str = \"rawPrediction\",",
          "1823:         maxDepth: int = 5,",
          "1824:         maxBins: int = 32,",
          "1825:         minInstancesPerNode: int = 1,",
          "1826:         minInfoGain: float = 0.0,",
          "1827:         maxMemoryInMB: int = 256,",
          "1828:         cacheNodeIds: bool = False,",
          "1829:         checkpointInterval: int = 10,",
          "1830:         impurity: str = \"gini\",",
          "1831:         seed: Optional[int] = None,",
          "1832:         weightCol: Optional[str] = None,",
          "1833:         leafCol: str = \"\",",
          "1834:         minWeightFractionPerNode: float = 0.0,",
          "1835:     ) -> \"DecisionTreeClassifier\":",
          "",
          "---------------",
          "--- Hunk 55 ---",
          "[Context before]",
          "1689:         kwargs = self._input_kwargs",
          "1690:         return self._set(**kwargs)",
          "1693:         return DecisionTreeClassificationModel(java_model)",
          "1696:         \"\"\"",
          "1697:         Sets the value of :py:attr:`maxDepth`.",
          "1698:         \"\"\"",
          "1699:         return self._set(maxDepth=value)",
          "1702:         \"\"\"",
          "1703:         Sets the value of :py:attr:`maxBins`.",
          "1704:         \"\"\"",
          "1705:         return self._set(maxBins=value)",
          "1708:         \"\"\"",
          "1709:         Sets the value of :py:attr:`minInstancesPerNode`.",
          "1710:         \"\"\"",
          "1711:         return self._set(minInstancesPerNode=value)",
          "1713:     @since(\"3.0.0\")",
          "1715:         \"\"\"",
          "1716:         Sets the value of :py:attr:`minWeightFractionPerNode`.",
          "1717:         \"\"\"",
          "1718:         return self._set(minWeightFractionPerNode=value)",
          "1721:         \"\"\"",
          "1722:         Sets the value of :py:attr:`minInfoGain`.",
          "1723:         \"\"\"",
          "1724:         return self._set(minInfoGain=value)",
          "1727:         \"\"\"",
          "1728:         Sets the value of :py:attr:`maxMemoryInMB`.",
          "1729:         \"\"\"",
          "1730:         return self._set(maxMemoryInMB=value)",
          "1733:         \"\"\"",
          "1734:         Sets the value of :py:attr:`cacheNodeIds`.",
          "1735:         \"\"\"",
          "1736:         return self._set(cacheNodeIds=value)",
          "1738:     @since(\"1.4.0\")",
          "1740:         \"\"\"",
          "1741:         Sets the value of :py:attr:`impurity`.",
          "1742:         \"\"\"",
          "1743:         return self._set(impurity=value)",
          "1745:     @since(\"1.4.0\")",
          "1747:         \"\"\"",
          "1748:         Sets the value of :py:attr:`checkpointInterval`.",
          "1749:         \"\"\"",
          "1750:         return self._set(checkpointInterval=value)",
          "1753:         \"\"\"",
          "1754:         Sets the value of :py:attr:`seed`.",
          "1755:         \"\"\"",
          "1756:         return self._set(seed=value)",
          "1758:     @since(\"3.0.0\")",
          "1760:         \"\"\"",
          "1761:         Sets the value of :py:attr:`weightCol`.",
          "1762:         \"\"\"",
          "",
          "[Removed Lines]",
          "1692:     def _create_model(self, java_model):",
          "1695:     def setMaxDepth(self, value):",
          "1701:     def setMaxBins(self, value):",
          "1707:     def setMinInstancesPerNode(self, value):",
          "1714:     def setMinWeightFractionPerNode(self, value):",
          "1720:     def setMinInfoGain(self, value):",
          "1726:     def setMaxMemoryInMB(self, value):",
          "1732:     def setCacheNodeIds(self, value):",
          "1739:     def setImpurity(self, value):",
          "1746:     def setCheckpointInterval(self, value):",
          "1752:     def setSeed(self, value):",
          "1759:     def setWeightCol(self, value):",
          "",
          "[Added Lines]",
          "1847:     def _create_model(self, java_model: \"JavaObject\") -> \"DecisionTreeClassificationModel\":",
          "1850:     def setMaxDepth(self, value: int) -> \"DecisionTreeClassifier\":",
          "1856:     def setMaxBins(self, value: int) -> \"DecisionTreeClassifier\":",
          "1862:     def setMinInstancesPerNode(self, value: int) -> \"DecisionTreeClassifier\":",
          "1869:     def setMinWeightFractionPerNode(self, value: float) -> \"DecisionTreeClassifier\":",
          "1875:     def setMinInfoGain(self, value: float) -> \"DecisionTreeClassifier\":",
          "1881:     def setMaxMemoryInMB(self, value: int) -> \"DecisionTreeClassifier\":",
          "1887:     def setCacheNodeIds(self, value: bool) -> \"DecisionTreeClassifier\":",
          "1894:     def setImpurity(self, value: str) -> \"DecisionTreeClassifier\":",
          "1901:     def setCheckpointInterval(self, value: int) -> \"DecisionTreeClassifier\":",
          "1907:     def setSeed(self, value: int) -> \"DecisionTreeClassifier\":",
          "1914:     def setWeightCol(self, value: str) -> \"DecisionTreeClassifier\":",
          "",
          "---------------",
          "--- Hunk 56 ---",
          "[Context before]",
          "1766: @inherit_doc",
          "1767: class DecisionTreeClassificationModel(",
          "1768:     _DecisionTreeModel,",
          "1770:     _DecisionTreeClassifierParams,",
          "1771:     JavaMLWritable,",
          "1773: ):",
          "1774:     \"\"\"",
          "1775:     Model fitted by DecisionTreeClassifier.",
          "",
          "[Removed Lines]",
          "1769:     _JavaProbabilisticClassificationModel,",
          "1772:     JavaMLReadable,",
          "",
          "[Added Lines]",
          "1924:     _JavaProbabilisticClassificationModel[Vector],",
          "1927:     JavaMLReadable[\"DecisionTreeClassificationModel\"],",
          "",
          "---------------",
          "--- Hunk 57 ---",
          "[Context before]",
          "1778:     \"\"\"",
          "1780:     @property",
          "1782:         \"\"\"",
          "1783:         Estimate of the importance of each feature.",
          "",
          "[Removed Lines]",
          "1781:     def featureImportances(self):",
          "",
          "[Added Lines]",
          "1936:     def featureImportances(self) -> Vector:",
          "",
          "---------------",
          "--- Hunk 58 ---",
          "[Context before]",
          "1808:     Params for :py:class:`RandomForestClassifier` and :py:class:`RandomForestClassificationModel`.",
          "1809:     \"\"\"",
          "1812:         super(_RandomForestClassifierParams, self).__init__(*args)",
          "1813:         self._setDefault(",
          "1814:             maxDepth=5,",
          "",
          "[Removed Lines]",
          "1811:     def __init__(self, *args):",
          "",
          "[Added Lines]",
          "1966:     def __init__(self, *args: Any):",
          "",
          "---------------",
          "--- Hunk 59 ---",
          "[Context before]",
          "1831: @inherit_doc",
          "1832: class RandomForestClassifier(",
          "1834: ):",
          "1835:     \"\"\"",
          "1836:     `Random Forest <http://en.wikipedia.org/wiki/Random_forest>`_",
          "",
          "[Removed Lines]",
          "1833:     _JavaProbabilisticClassifier, _RandomForestClassifierParams, JavaMLWritable, JavaMLReadable",
          "",
          "[Added Lines]",
          "1988:     _JavaProbabilisticClassifier[\"RandomForestClassificationModel\"],",
          "1989:     _RandomForestClassifierParams,",
          "1990:     JavaMLWritable,",
          "1991:     JavaMLReadable[\"RandomForestClassifier\"],",
          "",
          "---------------",
          "--- Hunk 60 ---",
          "[Context before]",
          "1906:     True",
          "1907:     \"\"\"",
          "1909:     @keyword_only",
          "1910:     def __init__(",
          "1911:         self,",
          "1934:     ):",
          "1935:         \"\"\"",
          "1936:         __init__(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\",
          "",
          "[Removed Lines]",
          "1913:         featuresCol=\"features\",",
          "1914:         labelCol=\"label\",",
          "1915:         predictionCol=\"prediction\",",
          "1916:         probabilityCol=\"probability\",",
          "1917:         rawPredictionCol=\"rawPrediction\",",
          "1918:         maxDepth=5,",
          "1919:         maxBins=32,",
          "1920:         minInstancesPerNode=1,",
          "1921:         minInfoGain=0.0,",
          "1922:         maxMemoryInMB=256,",
          "1923:         cacheNodeIds=False,",
          "1924:         checkpointInterval=10,",
          "1925:         impurity=\"gini\",",
          "1926:         numTrees=20,",
          "1927:         featureSubsetStrategy=\"auto\",",
          "1928:         seed=None,",
          "1929:         subsamplingRate=1.0,",
          "1930:         leafCol=\"\",",
          "1931:         minWeightFractionPerNode=0.0,",
          "1932:         weightCol=None,",
          "1933:         bootstrap=True,",
          "",
          "[Added Lines]",
          "2067:     _input_kwargs: Dict[str, Any]",
          "2073:         featuresCol: str = \"features\",",
          "2074:         labelCol: str = \"label\",",
          "2075:         predictionCol: str = \"prediction\",",
          "2076:         probabilityCol: str = \"probability\",",
          "2077:         rawPredictionCol: str = \"rawPrediction\",",
          "2078:         maxDepth: int = 5,",
          "2079:         maxBins: int = 32,",
          "2080:         minInstancesPerNode: int = 1,",
          "2081:         minInfoGain: float = 0.0,",
          "2082:         maxMemoryInMB: int = 256,",
          "2083:         cacheNodeIds: bool = False,",
          "2084:         checkpointInterval: int = 10,",
          "2085:         impurity: str = \"gini\",",
          "2086:         numTrees: int = 20,",
          "2087:         featureSubsetStrategy: str = \"auto\",",
          "2088:         seed: Optional[int] = None,",
          "2089:         subsamplingRate: float = 1.0,",
          "2090:         leafCol: str = \"\",",
          "2091:         minWeightFractionPerNode: float = 0.0,",
          "2092:         weightCol: Optional[str] = None,",
          "2093:         bootstrap: Optional[bool] = True,",
          "",
          "---------------",
          "--- Hunk 61 ---",
          "[Context before]",
          "1952:     def setParams(",
          "1953:         self,",
          "1977:         \"\"\"",
          "1978:         setParams(self, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\",
          "1979:                  probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\", \\",
          "",
          "[Removed Lines]",
          "1955:         featuresCol=\"features\",",
          "1956:         labelCol=\"label\",",
          "1957:         predictionCol=\"prediction\",",
          "1958:         probabilityCol=\"probability\",",
          "1959:         rawPredictionCol=\"rawPrediction\",",
          "1960:         maxDepth=5,",
          "1961:         maxBins=32,",
          "1962:         minInstancesPerNode=1,",
          "1963:         minInfoGain=0.0,",
          "1964:         maxMemoryInMB=256,",
          "1965:         cacheNodeIds=False,",
          "1966:         checkpointInterval=10,",
          "1967:         seed=None,",
          "1968:         impurity=\"gini\",",
          "1969:         numTrees=20,",
          "1970:         featureSubsetStrategy=\"auto\",",
          "1971:         subsamplingRate=1.0,",
          "1972:         leafCol=\"\",",
          "1973:         minWeightFractionPerNode=0.0,",
          "1974:         weightCol=None,",
          "1975:         bootstrap=True,",
          "1976:     ):",
          "",
          "[Added Lines]",
          "2115:         featuresCol: str = \"features\",",
          "2116:         labelCol: str = \"label\",",
          "2117:         predictionCol: str = \"prediction\",",
          "2118:         probabilityCol: str = \"probability\",",
          "2119:         rawPredictionCol: str = \"rawPrediction\",",
          "2120:         maxDepth: int = 5,",
          "2121:         maxBins: int = 32,",
          "2122:         minInstancesPerNode: int = 1,",
          "2123:         minInfoGain: float = 0.0,",
          "2124:         maxMemoryInMB: int = 256,",
          "2125:         cacheNodeIds: bool = False,",
          "2126:         checkpointInterval: int = 10,",
          "2127:         impurity: str = \"gini\",",
          "2128:         numTrees: int = 20,",
          "2129:         featureSubsetStrategy: str = \"auto\",",
          "2130:         seed: Optional[int] = None,",
          "2131:         subsamplingRate: float = 1.0,",
          "2132:         leafCol: str = \"\",",
          "2133:         minWeightFractionPerNode: float = 0.0,",
          "2134:         weightCol: Optional[str] = None,",
          "2135:         bootstrap: Optional[bool] = True,",
          "2136:     ) -> \"RandomForestClassifier\":",
          "",
          "---------------",
          "--- Hunk 62 ---",
          "[Context before]",
          "1986:         kwargs = self._input_kwargs",
          "1987:         return self._set(**kwargs)",
          "1990:         return RandomForestClassificationModel(java_model)",
          "1993:         \"\"\"",
          "1994:         Sets the value of :py:attr:`maxDepth`.",
          "1995:         \"\"\"",
          "1996:         return self._set(maxDepth=value)",
          "1999:         \"\"\"",
          "2000:         Sets the value of :py:attr:`maxBins`.",
          "2001:         \"\"\"",
          "2002:         return self._set(maxBins=value)",
          "2005:         \"\"\"",
          "2006:         Sets the value of :py:attr:`minInstancesPerNode`.",
          "2007:         \"\"\"",
          "2008:         return self._set(minInstancesPerNode=value)",
          "2011:         \"\"\"",
          "2012:         Sets the value of :py:attr:`minInfoGain`.",
          "2013:         \"\"\"",
          "2014:         return self._set(minInfoGain=value)",
          "2017:         \"\"\"",
          "2018:         Sets the value of :py:attr:`maxMemoryInMB`.",
          "2019:         \"\"\"",
          "2020:         return self._set(maxMemoryInMB=value)",
          "2023:         \"\"\"",
          "2024:         Sets the value of :py:attr:`cacheNodeIds`.",
          "2025:         \"\"\"",
          "2026:         return self._set(cacheNodeIds=value)",
          "2028:     @since(\"1.4.0\")",
          "2030:         \"\"\"",
          "2031:         Sets the value of :py:attr:`impurity`.",
          "2032:         \"\"\"",
          "2033:         return self._set(impurity=value)",
          "2035:     @since(\"1.4.0\")",
          "2037:         \"\"\"",
          "2038:         Sets the value of :py:attr:`numTrees`.",
          "2039:         \"\"\"",
          "2040:         return self._set(numTrees=value)",
          "2042:     @since(\"3.0.0\")",
          "2044:         \"\"\"",
          "2045:         Sets the value of :py:attr:`bootstrap`.",
          "2046:         \"\"\"",
          "2047:         return self._set(bootstrap=value)",
          "2049:     @since(\"1.4.0\")",
          "2051:         \"\"\"",
          "2052:         Sets the value of :py:attr:`subsamplingRate`.",
          "2053:         \"\"\"",
          "2054:         return self._set(subsamplingRate=value)",
          "2056:     @since(\"2.4.0\")",
          "2058:         \"\"\"",
          "2059:         Sets the value of :py:attr:`featureSubsetStrategy`.",
          "2060:         \"\"\"",
          "2061:         return self._set(featureSubsetStrategy=value)",
          "2064:         \"\"\"",
          "2065:         Sets the value of :py:attr:`seed`.",
          "2066:         \"\"\"",
          "2067:         return self._set(seed=value)",
          "2070:         \"\"\"",
          "2071:         Sets the value of :py:attr:`checkpointInterval`.",
          "2072:         \"\"\"",
          "2073:         return self._set(checkpointInterval=value)",
          "2075:     @since(\"3.0.0\")",
          "2077:         \"\"\"",
          "2078:         Sets the value of :py:attr:`weightCol`.",
          "2079:         \"\"\"",
          "2080:         return self._set(weightCol=value)",
          "2082:     @since(\"3.0.0\")",
          "2084:         \"\"\"",
          "2085:         Sets the value of :py:attr:`minWeightFractionPerNode`.",
          "2086:         \"\"\"",
          "",
          "[Removed Lines]",
          "1989:     def _create_model(self, java_model):",
          "1992:     def setMaxDepth(self, value):",
          "1998:     def setMaxBins(self, value):",
          "2004:     def setMinInstancesPerNode(self, value):",
          "2010:     def setMinInfoGain(self, value):",
          "2016:     def setMaxMemoryInMB(self, value):",
          "2022:     def setCacheNodeIds(self, value):",
          "2029:     def setImpurity(self, value):",
          "2036:     def setNumTrees(self, value):",
          "2043:     def setBootstrap(self, value):",
          "2050:     def setSubsamplingRate(self, value):",
          "2057:     def setFeatureSubsetStrategy(self, value):",
          "2063:     def setSeed(self, value):",
          "2069:     def setCheckpointInterval(self, value):",
          "2076:     def setWeightCol(self, value):",
          "2083:     def setMinWeightFractionPerNode(self, value):",
          "",
          "[Added Lines]",
          "2149:     def _create_model(self, java_model: \"JavaObject\") -> \"RandomForestClassificationModel\":",
          "2152:     def setMaxDepth(self, value: int) -> \"RandomForestClassifier\":",
          "2158:     def setMaxBins(self, value: int) -> \"RandomForestClassifier\":",
          "2164:     def setMinInstancesPerNode(self, value: int) -> \"RandomForestClassifier\":",
          "2170:     def setMinInfoGain(self, value: float) -> \"RandomForestClassifier\":",
          "2176:     def setMaxMemoryInMB(self, value: int) -> \"RandomForestClassifier\":",
          "2182:     def setCacheNodeIds(self, value: bool) -> \"RandomForestClassifier\":",
          "2189:     def setImpurity(self, value: str) -> \"RandomForestClassifier\":",
          "2196:     def setNumTrees(self, value: int) -> \"RandomForestClassifier\":",
          "2203:     def setBootstrap(self, value: bool) -> \"RandomForestClassifier\":",
          "2210:     def setSubsamplingRate(self, value: float) -> \"RandomForestClassifier\":",
          "2217:     def setFeatureSubsetStrategy(self, value: str) -> \"RandomForestClassifier\":",
          "2223:     def setSeed(self, value: int) -> \"RandomForestClassifier\":",
          "2229:     def setCheckpointInterval(self, value: int) -> \"RandomForestClassifier\":",
          "2236:     def setWeightCol(self, value: str) -> \"RandomForestClassifier\":",
          "2243:     def setMinWeightFractionPerNode(self, value: float) -> \"RandomForestClassifier\":",
          "",
          "---------------",
          "--- Hunk 63 ---",
          "[Context before]",
          "2090: class RandomForestClassificationModel(",
          "2091:     _TreeEnsembleModel,",
          "2093:     _RandomForestClassifierParams,",
          "2094:     JavaMLWritable,",
          "2097: ):",
          "2098:     \"\"\"",
          "2099:     Model fitted by RandomForestClassifier.",
          "",
          "[Removed Lines]",
          "2092:     _JavaProbabilisticClassificationModel,",
          "2095:     JavaMLReadable,",
          "2096:     HasTrainingSummary,",
          "",
          "[Added Lines]",
          "2252:     _JavaProbabilisticClassificationModel[Vector],",
          "2255:     JavaMLReadable[\"RandomForestClassificationModel\"],",
          "2256:     HasTrainingSummary[\"RandomForestClassificationTrainingSummary\"],",
          "",
          "---------------",
          "--- Hunk 64 ---",
          "[Context before]",
          "2102:     \"\"\"",
          "2104:     @property",
          "2106:         \"\"\"",
          "2107:         Estimate of the importance of each feature.",
          "",
          "[Removed Lines]",
          "2105:     def featureImportances(self):",
          "",
          "[Added Lines]",
          "2265:     def featureImportances(self) -> Vector:",
          "",
          "---------------",
          "--- Hunk 65 ---",
          "[Context before]",
          "2119:         \"\"\"",
          "2120:         return self._call_java(\"featureImportances\")",
          "2123:     @since(\"2.0.0\")",
          "2125:         \"\"\"Trees in this ensemble. Warning: These have null parent Estimators.\"\"\"",
          "2126:         return [DecisionTreeClassificationModel(m) for m in list(self._call_java(\"trees\"))]",
          "2129:     @since(\"3.1.0\")",
          "2131:         \"\"\"",
          "2132:         Gets summary (accuracy/precision/recall, objective history, total iterations) of model",
          "2133:         trained on the training set. An exception is thrown if `trainingSummary is None`.",
          "",
          "[Removed Lines]",
          "2122:     @property",
          "2124:     def trees(self):",
          "2128:     @property",
          "2130:     def summary(self):",
          "",
          "[Added Lines]",
          "2282:     @property  # type: ignore[misc]",
          "2284:     def trees(self) -> List[DecisionTreeClassificationModel]:",
          "2288:     @property  # type: ignore[misc]",
          "2290:     def summary(self) -> \"RandomForestClassificationTrainingSummary\":",
          "",
          "---------------",
          "--- Hunk 66 ---",
          "[Context before]",
          "2146:                 \"No training summary available for this %s\" % self.__class__.__name__",
          "2147:             )",
          "2150:         \"\"\"",
          "2151:         Evaluates the model on a test dataset.",
          "",
          "[Removed Lines]",
          "2149:     def evaluate(self, dataset):",
          "",
          "[Added Lines]",
          "2309:     def evaluate(",
          "2310:         self, dataset: DataFrame",
          "2311:     ) -> Union[\"BinaryRandomForestClassificationSummary\", \"RandomForestClassificationSummary\"]:",
          "",
          "---------------",
          "--- Hunk 67 ---",
          "[Context before]",
          "2220:     .. versionadded:: 3.0.0",
          "2221:     \"\"\"",
          "2226:         Params._dummy(),",
          "2227:         \"lossType\",",
          "2228:         \"Loss function which GBT tries to minimize (case-insensitive). \"",
          "",
          "[Removed Lines]",
          "2223:     supportedLossTypes = [\"logistic\"]",
          "2225:     lossType = Param(",
          "",
          "[Added Lines]",
          "2385:     supportedLossTypes: List[str] = [\"logistic\"]",
          "2387:     lossType: Param[str] = Param(",
          "",
          "---------------",
          "--- Hunk 68 ---",
          "[Context before]",
          "2231:         typeConverter=TypeConverters.toString,",
          "2232:     )",
          "2235:         super(_GBTClassifierParams, self).__init__(*args)",
          "2236:         self._setDefault(",
          "2237:             maxDepth=5,",
          "",
          "[Removed Lines]",
          "2234:     def __init__(self, *args):",
          "",
          "[Added Lines]",
          "2396:     def __init__(self, *args: Any):",
          "",
          "---------------",
          "--- Hunk 69 ---",
          "[Context before]",
          "2253:         )",
          "2255:     @since(\"1.4.0\")",
          "2257:         \"\"\"",
          "2258:         Gets the value of lossType or its default value.",
          "2259:         \"\"\"",
          "",
          "[Removed Lines]",
          "2256:     def getLossType(self):",
          "",
          "[Added Lines]",
          "2418:     def getLossType(self) -> str:",
          "",
          "---------------",
          "--- Hunk 70 ---",
          "[Context before]",
          "2263: @inherit_doc",
          "2264: class GBTClassifier(",
          "2266: ):",
          "2267:     \"\"\"",
          "2268:     `Gradient-Boosted Trees (GBTs) <http://en.wikipedia.org/wiki/Gradient_boosting>`_",
          "",
          "[Removed Lines]",
          "2265:     _JavaProbabilisticClassifier, _GBTClassifierParams, JavaMLWritable, JavaMLReadable",
          "",
          "[Added Lines]",
          "2427:     _JavaProbabilisticClassifier[\"GBTClassificationModel\"],",
          "2428:     _GBTClassifierParams,",
          "2429:     JavaMLWritable,",
          "2430:     JavaMLReadable[\"GBTClassifier\"],",
          "",
          "---------------",
          "--- Hunk 71 ---",
          "[Context before]",
          "2368:     0.01",
          "2369:     \"\"\"",
          "2371:     @keyword_only",
          "2372:     def __init__(",
          "2373:         self,",
          "2397:     ):",
          "2398:         \"\"\"",
          "2399:         __init__(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\",
          "",
          "[Removed Lines]",
          "2375:         featuresCol=\"features\",",
          "2376:         labelCol=\"label\",",
          "2377:         predictionCol=\"prediction\",",
          "2378:         maxDepth=5,",
          "2379:         maxBins=32,",
          "2380:         minInstancesPerNode=1,",
          "2381:         minInfoGain=0.0,",
          "2382:         maxMemoryInMB=256,",
          "2383:         cacheNodeIds=False,",
          "2384:         checkpointInterval=10,",
          "2385:         lossType=\"logistic\",",
          "2386:         maxIter=20,",
          "2387:         stepSize=0.1,",
          "2388:         seed=None,",
          "2389:         subsamplingRate=1.0,",
          "2390:         impurity=\"variance\",",
          "2391:         featureSubsetStrategy=\"all\",",
          "2392:         validationTol=0.01,",
          "2393:         validationIndicatorCol=None,",
          "2394:         leafCol=\"\",",
          "2395:         minWeightFractionPerNode=0.0,",
          "2396:         weightCol=None,",
          "",
          "[Added Lines]",
          "2536:     _input_kwargs: Dict[str, Any]",
          "2542:         featuresCol: str = \"features\",",
          "2543:         labelCol: str = \"label\",",
          "2544:         predictionCol: str = \"prediction\",",
          "2545:         maxDepth: int = 5,",
          "2546:         maxBins: int = 32,",
          "2547:         minInstancesPerNode: int = 1,",
          "2548:         minInfoGain: float = 0.0,",
          "2549:         maxMemoryInMB: int = 256,",
          "2550:         cacheNodeIds: bool = False,",
          "2551:         checkpointInterval: int = 10,",
          "2552:         lossType: str = \"logistic\",",
          "2553:         maxIter: int = 20,",
          "2554:         stepSize: float = 0.1,",
          "2555:         seed: Optional[int] = None,",
          "2556:         subsamplingRate: float = 1.0,",
          "2557:         impurity: str = \"variance\",",
          "2558:         featureSubsetStrategy: str = \"all\",",
          "2559:         validationTol: float = 0.01,",
          "2560:         validationIndicatorCol: Optional[str] = None,",
          "2561:         leafCol: str = \"\",",
          "2562:         minWeightFractionPerNode: float = 0.0,",
          "2563:         weightCol: Optional[str] = None,",
          "",
          "---------------",
          "--- Hunk 72 ---",
          "[Context before]",
          "2416:     def setParams(",
          "2417:         self,",
          "2442:         \"\"\"",
          "2443:         setParams(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\",
          "2444:                   maxDepth=5, maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, \\",
          "",
          "[Removed Lines]",
          "2419:         featuresCol=\"features\",",
          "2420:         labelCol=\"label\",",
          "2421:         predictionCol=\"prediction\",",
          "2422:         maxDepth=5,",
          "2423:         maxBins=32,",
          "2424:         minInstancesPerNode=1,",
          "2425:         minInfoGain=0.0,",
          "2426:         maxMemoryInMB=256,",
          "2427:         cacheNodeIds=False,",
          "2428:         checkpointInterval=10,",
          "2429:         lossType=\"logistic\",",
          "2430:         maxIter=20,",
          "2431:         stepSize=0.1,",
          "2432:         seed=None,",
          "2433:         subsamplingRate=1.0,",
          "2434:         impurity=\"variance\",",
          "2435:         featureSubsetStrategy=\"all\",",
          "2436:         validationTol=0.01,",
          "2437:         validationIndicatorCol=None,",
          "2438:         leafCol=\"\",",
          "2439:         minWeightFractionPerNode=0.0,",
          "2440:         weightCol=None,",
          "2441:     ):",
          "",
          "[Added Lines]",
          "2586:         featuresCol: str = \"features\",",
          "2587:         labelCol: str = \"label\",",
          "2588:         predictionCol: str = \"prediction\",",
          "2589:         maxDepth: int = 5,",
          "2590:         maxBins: int = 32,",
          "2591:         minInstancesPerNode: int = 1,",
          "2592:         minInfoGain: float = 0.0,",
          "2593:         maxMemoryInMB: int = 256,",
          "2594:         cacheNodeIds: bool = False,",
          "2595:         checkpointInterval: int = 10,",
          "2596:         lossType: str = \"logistic\",",
          "2597:         maxIter: int = 20,",
          "2598:         stepSize: float = 0.1,",
          "2599:         seed: Optional[int] = None,",
          "2600:         subsamplingRate: float = 1.0,",
          "2601:         impurity: str = \"variance\",",
          "2602:         featureSubsetStrategy: str = \"all\",",
          "2603:         validationTol: float = 0.01,",
          "2604:         validationIndicatorCol: Optional[str] = None,",
          "2605:         leafCol: str = \"\",",
          "2606:         minWeightFractionPerNode: float = 0.0,",
          "2607:         weightCol: Optional[str] = None,",
          "2608:     ) -> \"GBTClassifier\":",
          "",
          "---------------",
          "--- Hunk 73 ---",
          "[Context before]",
          "2452:         kwargs = self._input_kwargs",
          "2453:         return self._set(**kwargs)",
          "2456:         return GBTClassificationModel(java_model)",
          "2459:         \"\"\"",
          "2460:         Sets the value of :py:attr:`maxDepth`.",
          "2461:         \"\"\"",
          "2462:         return self._set(maxDepth=value)",
          "2465:         \"\"\"",
          "2466:         Sets the value of :py:attr:`maxBins`.",
          "2467:         \"\"\"",
          "2468:         return self._set(maxBins=value)",
          "2471:         \"\"\"",
          "2472:         Sets the value of :py:attr:`minInstancesPerNode`.",
          "2473:         \"\"\"",
          "2474:         return self._set(minInstancesPerNode=value)",
          "2477:         \"\"\"",
          "2478:         Sets the value of :py:attr:`minInfoGain`.",
          "2479:         \"\"\"",
          "2480:         return self._set(minInfoGain=value)",
          "2483:         \"\"\"",
          "2484:         Sets the value of :py:attr:`maxMemoryInMB`.",
          "2485:         \"\"\"",
          "2486:         return self._set(maxMemoryInMB=value)",
          "2489:         \"\"\"",
          "2490:         Sets the value of :py:attr:`cacheNodeIds`.",
          "2491:         \"\"\"",
          "2492:         return self._set(cacheNodeIds=value)",
          "2494:     @since(\"1.4.0\")",
          "2496:         \"\"\"",
          "2497:         Sets the value of :py:attr:`impurity`.",
          "2498:         \"\"\"",
          "2499:         return self._set(impurity=value)",
          "2501:     @since(\"1.4.0\")",
          "2503:         \"\"\"",
          "2504:         Sets the value of :py:attr:`lossType`.",
          "2505:         \"\"\"",
          "2506:         return self._set(lossType=value)",
          "2508:     @since(\"1.4.0\")",
          "2510:         \"\"\"",
          "2511:         Sets the value of :py:attr:`subsamplingRate`.",
          "2512:         \"\"\"",
          "2513:         return self._set(subsamplingRate=value)",
          "2515:     @since(\"2.4.0\")",
          "2517:         \"\"\"",
          "2518:         Sets the value of :py:attr:`featureSubsetStrategy`.",
          "2519:         \"\"\"",
          "2520:         return self._set(featureSubsetStrategy=value)",
          "2522:     @since(\"3.0.0\")",
          "2524:         \"\"\"",
          "2525:         Sets the value of :py:attr:`validationIndicatorCol`.",
          "2526:         \"\"\"",
          "2527:         return self._set(validationIndicatorCol=value)",
          "2529:     @since(\"1.4.0\")",
          "2531:         \"\"\"",
          "2532:         Sets the value of :py:attr:`maxIter`.",
          "2533:         \"\"\"",
          "2534:         return self._set(maxIter=value)",
          "2536:     @since(\"1.4.0\")",
          "2538:         \"\"\"",
          "2539:         Sets the value of :py:attr:`checkpointInterval`.",
          "2540:         \"\"\"",
          "2541:         return self._set(checkpointInterval=value)",
          "2543:     @since(\"1.4.0\")",
          "2545:         \"\"\"",
          "2546:         Sets the value of :py:attr:`seed`.",
          "2547:         \"\"\"",
          "2548:         return self._set(seed=value)",
          "2550:     @since(\"1.4.0\")",
          "2552:         \"\"\"",
          "2553:         Sets the value of :py:attr:`stepSize`.",
          "2554:         \"\"\"",
          "2555:         return self._set(stepSize=value)",
          "2557:     @since(\"3.0.0\")",
          "2559:         \"\"\"",
          "2560:         Sets the value of :py:attr:`weightCol`.",
          "2561:         \"\"\"",
          "2562:         return self._set(weightCol=value)",
          "2564:     @since(\"3.0.0\")",
          "2566:         \"\"\"",
          "2567:         Sets the value of :py:attr:`minWeightFractionPerNode`.",
          "2568:         \"\"\"",
          "",
          "[Removed Lines]",
          "2455:     def _create_model(self, java_model):",
          "2458:     def setMaxDepth(self, value):",
          "2464:     def setMaxBins(self, value):",
          "2470:     def setMinInstancesPerNode(self, value):",
          "2476:     def setMinInfoGain(self, value):",
          "2482:     def setMaxMemoryInMB(self, value):",
          "2488:     def setCacheNodeIds(self, value):",
          "2495:     def setImpurity(self, value):",
          "2502:     def setLossType(self, value):",
          "2509:     def setSubsamplingRate(self, value):",
          "2516:     def setFeatureSubsetStrategy(self, value):",
          "2523:     def setValidationIndicatorCol(self, value):",
          "2530:     def setMaxIter(self, value):",
          "2537:     def setCheckpointInterval(self, value):",
          "2544:     def setSeed(self, value):",
          "2551:     def setStepSize(self, value):",
          "2558:     def setWeightCol(self, value):",
          "2565:     def setMinWeightFractionPerNode(self, value):",
          "",
          "[Added Lines]",
          "2622:     def _create_model(self, java_model: \"JavaObject\") -> \"GBTClassificationModel\":",
          "2625:     def setMaxDepth(self, value: int) -> \"GBTClassifier\":",
          "2631:     def setMaxBins(self, value: int) -> \"GBTClassifier\":",
          "2637:     def setMinInstancesPerNode(self, value: int) -> \"GBTClassifier\":",
          "2643:     def setMinInfoGain(self, value: float) -> \"GBTClassifier\":",
          "2649:     def setMaxMemoryInMB(self, value: int) -> \"GBTClassifier\":",
          "2655:     def setCacheNodeIds(self, value: bool) -> \"GBTClassifier\":",
          "2662:     def setImpurity(self, value: str) -> \"GBTClassifier\":",
          "2669:     def setLossType(self, value: str) -> \"GBTClassifier\":",
          "2676:     def setSubsamplingRate(self, value: float) -> \"GBTClassifier\":",
          "2683:     def setFeatureSubsetStrategy(self, value: str) -> \"GBTClassifier\":",
          "2690:     def setValidationIndicatorCol(self, value: str) -> \"GBTClassifier\":",
          "2697:     def setMaxIter(self, value: int) -> \"GBTClassifier\":",
          "2704:     def setCheckpointInterval(self, value: int) -> \"GBTClassifier\":",
          "2711:     def setSeed(self, value: int) -> \"GBTClassifier\":",
          "2718:     def setStepSize(self, value: int) -> \"GBTClassifier\":",
          "2725:     def setWeightCol(self, value: str) -> \"GBTClassifier\":",
          "2732:     def setMinWeightFractionPerNode(self, value: float) -> \"GBTClassifier\":",
          "",
          "---------------",
          "--- Hunk 74 ---",
          "[Context before]",
          "2572: class GBTClassificationModel(",
          "2573:     _TreeEnsembleModel,",
          "2575:     _GBTClassifierParams,",
          "2576:     JavaMLWritable,",
          "2578: ):",
          "2579:     \"\"\"",
          "2580:     Model fitted by GBTClassifier.",
          "",
          "[Removed Lines]",
          "2574:     _JavaProbabilisticClassificationModel,",
          "2577:     JavaMLReadable,",
          "",
          "[Added Lines]",
          "2741:     _JavaProbabilisticClassificationModel[Vector],",
          "2744:     JavaMLReadable[\"GBTClassificationModel\"],",
          "",
          "---------------",
          "--- Hunk 75 ---",
          "[Context before]",
          "2583:     \"\"\"",
          "2585:     @property",
          "2587:         \"\"\"",
          "2588:         Estimate of the importance of each feature.",
          "",
          "[Removed Lines]",
          "2586:     def featureImportances(self):",
          "",
          "[Added Lines]",
          "2753:     def featureImportances(self) -> Vector:",
          "",
          "---------------",
          "--- Hunk 76 ---",
          "[Context before]",
          "2600:         \"\"\"",
          "2601:         return self._call_java(\"featureImportances\")",
          "2604:     @since(\"2.0.0\")",
          "2606:         \"\"\"Trees in this ensemble. Warning: These have null parent Estimators.\"\"\"",
          "2607:         return [DecisionTreeRegressionModel(m) for m in list(self._call_java(\"trees\"))]",
          "2610:         \"\"\"",
          "2611:         Method to compute error or loss for every iteration of gradient boosting.",
          "",
          "[Removed Lines]",
          "2603:     @property",
          "2605:     def trees(self):",
          "2609:     def evaluateEachIteration(self, dataset):",
          "",
          "[Added Lines]",
          "2770:     @property  # type: ignore[misc]",
          "2772:     def trees(self) -> List[DecisionTreeRegressionModel]:",
          "2776:     def evaluateEachIteration(self, dataset: DataFrame) -> List[float]:",
          "",
          "---------------",
          "--- Hunk 77 ---",
          "[Context before]",
          "2627:     .. versionadded:: 3.0.0",
          "2628:     \"\"\"",
          "2631:         Params._dummy(),",
          "2632:         \"smoothing\",",
          "2633:         \"The smoothing parameter, should be >= 0, \" + \"default is 1.0\",",
          "2634:         typeConverter=TypeConverters.toFloat,",
          "2635:     )",
          "2637:         Params._dummy(),",
          "2638:         \"modelType\",",
          "2639:         \"The model type which is a string \"",
          "",
          "[Removed Lines]",
          "2630:     smoothing = Param(",
          "2636:     modelType = Param(",
          "",
          "[Added Lines]",
          "2797:     smoothing: Param[float] = Param(",
          "2803:     modelType: Param[str] = Param(",
          "",
          "---------------",
          "--- Hunk 78 ---",
          "[Context before]",
          "2642:         typeConverter=TypeConverters.toString,",
          "2643:     )",
          "2646:         super(_NaiveBayesParams, self).__init__(*args)",
          "2647:         self._setDefault(smoothing=1.0, modelType=\"multinomial\")",
          "2649:     @since(\"1.5.0\")",
          "2651:         \"\"\"",
          "2652:         Gets the value of smoothing or its default value.",
          "2653:         \"\"\"",
          "2654:         return self.getOrDefault(self.smoothing)",
          "2656:     @since(\"1.5.0\")",
          "2658:         \"\"\"",
          "2659:         Gets the value of modelType or its default value.",
          "2660:         \"\"\"",
          "",
          "[Removed Lines]",
          "2645:     def __init__(self, *args):",
          "2650:     def getSmoothing(self):",
          "2657:     def getModelType(self):",
          "",
          "[Added Lines]",
          "2812:     def __init__(self, *args: Any):",
          "2817:     def getSmoothing(self) -> float:",
          "2824:     def getModelType(self) -> str:",
          "",
          "---------------",
          "--- Hunk 79 ---",
          "[Context before]",
          "2664: @inherit_doc",
          "2665: class NaiveBayes(",
          "2667:     _NaiveBayesParams,",
          "2668:     HasThresholds,",
          "2669:     HasWeightCol,",
          "2670:     JavaMLWritable,",
          "2672: ):",
          "2673:     \"\"\"",
          "2674:     Naive Bayes Classifiers.",
          "",
          "[Removed Lines]",
          "2666:     _JavaProbabilisticClassifier,",
          "2671:     JavaMLReadable,",
          "",
          "[Added Lines]",
          "2833:     _JavaProbabilisticClassifier[\"NaiveBayesModel\"],",
          "2838:     JavaMLReadable[\"NaiveBayes\"],",
          "",
          "---------------",
          "--- Hunk 80 ---",
          "[Context before]",
          "2763:     DenseMatrix(0, 0, [...], ...)",
          "2764:     \"\"\"",
          "2766:     @keyword_only",
          "2767:     def __init__(",
          "2768:         self,",
          "2779:     ):",
          "2780:         \"\"\"",
          "2781:         __init__(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\",
          "",
          "[Removed Lines]",
          "2770:         featuresCol=\"features\",",
          "2771:         labelCol=\"label\",",
          "2772:         predictionCol=\"prediction\",",
          "2773:         probabilityCol=\"probability\",",
          "2774:         rawPredictionCol=\"rawPrediction\",",
          "2775:         smoothing=1.0,",
          "2776:         modelType=\"multinomial\",",
          "2777:         thresholds=None,",
          "2778:         weightCol=None,",
          "",
          "[Added Lines]",
          "2933:     _input_kwargs: Dict[str, Any]",
          "2939:         featuresCol: str = \"features\",",
          "2940:         labelCol: str = \"label\",",
          "2941:         predictionCol: str = \"prediction\",",
          "2942:         probabilityCol: str = \"probability\",",
          "2943:         rawPredictionCol: str = \"rawPrediction\",",
          "2944:         smoothing: float = 1.0,",
          "2945:         modelType: str = \"multinomial\",",
          "2946:         thresholds: Optional[List[float]] = None,",
          "2947:         weightCol: Optional[str] = None,",
          "",
          "---------------",
          "--- Hunk 81 ---",
          "[Context before]",
          "2794:     def setParams(",
          "2795:         self,",
          "2807:         \"\"\"",
          "2808:         setParams(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\",
          "2809:                   probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\", smoothing=1.0, \\",
          "",
          "[Removed Lines]",
          "2797:         featuresCol=\"features\",",
          "2798:         labelCol=\"label\",",
          "2799:         predictionCol=\"prediction\",",
          "2800:         probabilityCol=\"probability\",",
          "2801:         rawPredictionCol=\"rawPrediction\",",
          "2802:         smoothing=1.0,",
          "2803:         modelType=\"multinomial\",",
          "2804:         thresholds=None,",
          "2805:         weightCol=None,",
          "2806:     ):",
          "",
          "[Added Lines]",
          "2966:         featuresCol: str = \"features\",",
          "2967:         labelCol: str = \"label\",",
          "2968:         predictionCol: str = \"prediction\",",
          "2969:         probabilityCol: str = \"probability\",",
          "2970:         rawPredictionCol: str = \"rawPrediction\",",
          "2971:         smoothing: float = 1.0,",
          "2972:         modelType: str = \"multinomial\",",
          "2973:         thresholds: Optional[List[float]] = None,",
          "2974:         weightCol: Optional[str] = None,",
          "2975:     ) -> \"NaiveBayes\":",
          "",
          "---------------",
          "--- Hunk 82 ---",
          "[Context before]",
          "2813:         kwargs = self._input_kwargs",
          "2814:         return self._set(**kwargs)",
          "2817:         return NaiveBayesModel(java_model)",
          "2819:     @since(\"1.5.0\")",
          "2821:         \"\"\"",
          "2822:         Sets the value of :py:attr:`smoothing`.",
          "2823:         \"\"\"",
          "2824:         return self._set(smoothing=value)",
          "2826:     @since(\"1.5.0\")",
          "2828:         \"\"\"",
          "2829:         Sets the value of :py:attr:`modelType`.",
          "2830:         \"\"\"",
          "2831:         return self._set(modelType=value)",
          "2834:         \"\"\"",
          "2835:         Sets the value of :py:attr:`weightCol`.",
          "2836:         \"\"\"",
          "",
          "[Removed Lines]",
          "2816:     def _create_model(self, java_model):",
          "2820:     def setSmoothing(self, value):",
          "2827:     def setModelType(self, value):",
          "2833:     def setWeightCol(self, value):",
          "",
          "[Added Lines]",
          "2985:     def _create_model(self, java_model: \"JavaObject\") -> \"NaiveBayesModel\":",
          "2989:     def setSmoothing(self, value: float) -> \"NaiveBayes\":",
          "2996:     def setModelType(self, value: str) -> \"NaiveBayes\":",
          "3002:     def setWeightCol(self, value: str) -> \"NaiveBayes\":",
          "",
          "---------------",
          "--- Hunk 83 ---",
          "[Context before]",
          "2840: class NaiveBayesModel(",
          "2842: ):",
          "2843:     \"\"\"",
          "2844:     Model fitted by NaiveBayes.",
          "",
          "[Removed Lines]",
          "2841:     _JavaProbabilisticClassificationModel, _NaiveBayesParams, JavaMLWritable, JavaMLReadable",
          "",
          "[Added Lines]",
          "3010:     _JavaProbabilisticClassificationModel[Vector],",
          "3011:     _NaiveBayesParams,",
          "3012:     JavaMLWritable,",
          "3013:     JavaMLReadable[\"NaiveBayesModel\"],",
          "",
          "---------------",
          "--- Hunk 84 ---",
          "[Context before]",
          "2846:     .. versionadded:: 1.5.0",
          "2847:     \"\"\"",
          "2850:     @since(\"2.0.0\")",
          "2852:         \"\"\"",
          "2853:         log of class priors.",
          "2854:         \"\"\"",
          "2855:         return self._call_java(\"pi\")",
          "2858:     @since(\"2.0.0\")",
          "2860:         \"\"\"",
          "2861:         log of class conditional probabilities.",
          "2862:         \"\"\"",
          "2863:         return self._call_java(\"theta\")",
          "2866:     @since(\"3.0.0\")",
          "2868:         \"\"\"",
          "2869:         variance of each feature.",
          "2870:         \"\"\"",
          "",
          "[Removed Lines]",
          "2849:     @property",
          "2851:     def pi(self):",
          "2857:     @property",
          "2859:     def theta(self):",
          "2865:     @property",
          "2867:     def sigma(self):",
          "",
          "[Added Lines]",
          "3021:     @property  # type: ignore[misc]",
          "3023:     def pi(self) -> Vector:",
          "3029:     @property  # type: ignore[misc]",
          "3031:     def theta(self) -> Matrix:",
          "3037:     @property  # type: ignore[misc]",
          "3039:     def sigma(self) -> Matrix:",
          "",
          "---------------",
          "--- Hunk 85 ---",
          "[Context before]",
          "2886:     .. versionadded:: 3.0.0",
          "2887:     \"\"\"",
          "2890:         Params._dummy(),",
          "2891:         \"layers\",",
          "2892:         \"Sizes of layers from input layer to output layer \"",
          "",
          "[Removed Lines]",
          "2889:     layers = Param(",
          "",
          "[Added Lines]",
          "3061:     layers: Param[List[int]] = Param(",
          "",
          "---------------",
          "--- Hunk 86 ---",
          "[Context before]",
          "2894:         + \"neurons and output layer of 10 neurons.\",",
          "2895:         typeConverter=TypeConverters.toListInt,",
          "2896:     )",
          "2898:         Params._dummy(),",
          "2899:         \"solver\",",
          "2900:         \"The solver algorithm for optimization. Supported \" + \"options: l-bfgs, gd.\",",
          "2901:         typeConverter=TypeConverters.toString,",
          "2902:     )",
          "2904:         Params._dummy(),",
          "2905:         \"initialWeights\",",
          "2906:         \"The initial weights of the model.\",",
          "2907:         typeConverter=TypeConverters.toVector,",
          "2908:     )",
          "2911:         super(_MultilayerPerceptronParams, self).__init__(*args)",
          "2912:         self._setDefault(maxIter=100, tol=1e-6, blockSize=128, stepSize=0.03, solver=\"l-bfgs\")",
          "2914:     @since(\"1.6.0\")",
          "2916:         \"\"\"",
          "2917:         Gets the value of layers or its default value.",
          "2918:         \"\"\"",
          "2919:         return self.getOrDefault(self.layers)",
          "2921:     @since(\"2.0.0\")",
          "2923:         \"\"\"",
          "2924:         Gets the value of initialWeights or its default value.",
          "2925:         \"\"\"",
          "",
          "[Removed Lines]",
          "2897:     solver = Param(",
          "2903:     initialWeights = Param(",
          "2910:     def __init__(self, *args):",
          "2915:     def getLayers(self):",
          "2922:     def getInitialWeights(self):",
          "",
          "[Added Lines]",
          "3069:     solver: Param[str] = Param(",
          "3075:     initialWeights: Param[Vector] = Param(",
          "3082:     def __init__(self, *args: Any):",
          "3087:     def getLayers(self) -> List[int]:",
          "3094:     def getInitialWeights(self) -> Vector:",
          "",
          "---------------",
          "--- Hunk 87 ---",
          "[Context before]",
          "2929: @inherit_doc",
          "2930: class MultilayerPerceptronClassifier(",
          "2932: ):",
          "2933:     \"\"\"",
          "2934:     Classifier trainer based on the Multilayer Perceptron.",
          "",
          "[Removed Lines]",
          "2931:     _JavaProbabilisticClassifier, _MultilayerPerceptronParams, JavaMLWritable, JavaMLReadable",
          "",
          "[Added Lines]",
          "3103:     _JavaProbabilisticClassifier[\"MultilayerPerceptronClassificationModel\"],",
          "3104:     _MultilayerPerceptronParams,",
          "3105:     JavaMLWritable,",
          "3106:     JavaMLReadable[\"MultilayerPerceptronClassifier\"],",
          "",
          "---------------",
          "--- Hunk 88 ---",
          "[Context before]",
          "3005:     True",
          "3006:     \"\"\"",
          "3008:     @keyword_only",
          "3009:     def __init__(",
          "3010:         self,",
          "3025:     ):",
          "3026:         \"\"\"",
          "3027:         __init__(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\",
          "",
          "[Removed Lines]",
          "3012:         featuresCol=\"features\",",
          "3013:         labelCol=\"label\",",
          "3014:         predictionCol=\"prediction\",",
          "3015:         maxIter=100,",
          "3016:         tol=1e-6,",
          "3017:         seed=None,",
          "3018:         layers=None,",
          "3019:         blockSize=128,",
          "3020:         stepSize=0.03,",
          "3021:         solver=\"l-bfgs\",",
          "3022:         initialWeights=None,",
          "3023:         probabilityCol=\"probability\",",
          "3024:         rawPredictionCol=\"rawPrediction\",",
          "",
          "[Added Lines]",
          "3183:     _input_kwargs: Dict[str, Any]",
          "3189:         featuresCol: str = \"features\",",
          "3190:         labelCol: str = \"label\",",
          "3191:         predictionCol: str = \"prediction\",",
          "3192:         maxIter: int = 100,",
          "3193:         tol: float = 1e-6,",
          "3194:         seed: Optional[int] = None,",
          "3195:         layers: Optional[List[int]] = None,",
          "3196:         blockSize: int = 128,",
          "3197:         stepSize: float = 0.03,",
          "3198:         solver: str = \"l-bfgs\",",
          "3199:         initialWeights: Optional[Vector] = None,",
          "3200:         probabilityCol: str = \"probability\",",
          "3201:         rawPredictionCol: str = \"rawPrediction\",",
          "",
          "---------------",
          "--- Hunk 89 ---",
          "[Context before]",
          "3041:     def setParams(",
          "3042:         self,",
          "3058:         \"\"\"",
          "3059:         setParams(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\",
          "3060:                   maxIter=100, tol=1e-6, seed=None, layers=None, blockSize=128, stepSize=0.03, \\",
          "",
          "[Removed Lines]",
          "3044:         featuresCol=\"features\",",
          "3045:         labelCol=\"label\",",
          "3046:         predictionCol=\"prediction\",",
          "3047:         maxIter=100,",
          "3048:         tol=1e-6,",
          "3049:         seed=None,",
          "3050:         layers=None,",
          "3051:         blockSize=128,",
          "3052:         stepSize=0.03,",
          "3053:         solver=\"l-bfgs\",",
          "3054:         initialWeights=None,",
          "3055:         probabilityCol=\"probability\",",
          "3056:         rawPredictionCol=\"rawPrediction\",",
          "3057:     ):",
          "",
          "[Added Lines]",
          "3221:         featuresCol: str = \"features\",",
          "3222:         labelCol: str = \"label\",",
          "3223:         predictionCol: str = \"prediction\",",
          "3224:         maxIter: int = 100,",
          "3225:         tol: float = 1e-6,",
          "3226:         seed: Optional[int] = None,",
          "3227:         layers: Optional[List[int]] = None,",
          "3228:         blockSize: int = 128,",
          "3229:         stepSize: float = 0.03,",
          "3230:         solver: str = \"l-bfgs\",",
          "3231:         initialWeights: Optional[Vector] = None,",
          "3232:         probabilityCol: str = \"probability\",",
          "3233:     ) -> \"MultilayerPerceptronClassifier\":",
          "",
          "---------------",
          "--- Hunk 90 ---",
          "[Context before]",
          "3065:         kwargs = self._input_kwargs",
          "3066:         return self._set(**kwargs)",
          "3069:         return MultilayerPerceptronClassificationModel(java_model)",
          "3071:     @since(\"1.6.0\")",
          "3073:         \"\"\"",
          "3074:         Sets the value of :py:attr:`layers`.",
          "3075:         \"\"\"",
          "3076:         return self._set(layers=value)",
          "3078:     @since(\"1.6.0\")",
          "3080:         \"\"\"",
          "3081:         Sets the value of :py:attr:`blockSize`.",
          "3082:         \"\"\"",
          "3083:         return self._set(blockSize=value)",
          "3085:     @since(\"2.0.0\")",
          "3087:         \"\"\"",
          "3088:         Sets the value of :py:attr:`initialWeights`.",
          "3089:         \"\"\"",
          "3090:         return self._set(initialWeights=value)",
          "3093:         \"\"\"",
          "3094:         Sets the value of :py:attr:`maxIter`.",
          "3095:         \"\"\"",
          "3096:         return self._set(maxIter=value)",
          "3099:         \"\"\"",
          "3100:         Sets the value of :py:attr:`seed`.",
          "3101:         \"\"\"",
          "3102:         return self._set(seed=value)",
          "3105:         \"\"\"",
          "3106:         Sets the value of :py:attr:`tol`.",
          "3107:         \"\"\"",
          "3108:         return self._set(tol=value)",
          "3110:     @since(\"2.0.0\")",
          "3112:         \"\"\"",
          "3113:         Sets the value of :py:attr:`stepSize`.",
          "3114:         \"\"\"",
          "3115:         return self._set(stepSize=value)",
          "3118:         \"\"\"",
          "3119:         Sets the value of :py:attr:`solver`.",
          "3120:         \"\"\"",
          "",
          "[Removed Lines]",
          "3068:     def _create_model(self, java_model):",
          "3072:     def setLayers(self, value):",
          "3079:     def setBlockSize(self, value):",
          "3086:     def setInitialWeights(self, value):",
          "3092:     def setMaxIter(self, value):",
          "3098:     def setSeed(self, value):",
          "3104:     def setTol(self, value):",
          "3111:     def setStepSize(self, value):",
          "3117:     def setSolver(self, value):",
          "",
          "[Added Lines]",
          "3244:     def _create_model(self, java_model: \"JavaObject\") -> \"MultilayerPerceptronClassificationModel\":",
          "3248:     def setLayers(self, value: List[int]) -> \"MultilayerPerceptronClassifier\":",
          "3255:     def setBlockSize(self, value: int) -> \"MultilayerPerceptronClassifier\":",
          "3262:     def setInitialWeights(self, value: Vector) -> \"MultilayerPerceptronClassifier\":",
          "3268:     def setMaxIter(self, value: int) -> \"MultilayerPerceptronClassifier\":",
          "3274:     def setSeed(self, value: int) -> \"MultilayerPerceptronClassifier\":",
          "3280:     def setTol(self, value: float) -> \"MultilayerPerceptronClassifier\":",
          "3287:     def setStepSize(self, value: float) -> \"MultilayerPerceptronClassifier\":",
          "3293:     def setSolver(self, value: str) -> \"MultilayerPerceptronClassifier\":",
          "",
          "---------------",
          "--- Hunk 91 ---",
          "[Context before]",
          "3124: class MultilayerPerceptronClassificationModel(",
          "3126:     _MultilayerPerceptronParams,",
          "3127:     JavaMLWritable,",
          "3130: ):",
          "3131:     \"\"\"",
          "3132:     Model fitted by MultilayerPerceptronClassifier.",
          "",
          "[Removed Lines]",
          "3125:     _JavaProbabilisticClassificationModel,",
          "3128:     JavaMLReadable,",
          "3129:     HasTrainingSummary,",
          "",
          "[Added Lines]",
          "3301:     _JavaProbabilisticClassificationModel[Vector],",
          "3304:     JavaMLReadable[\"MultilayerPerceptronClassificationModel\"],",
          "3305:     HasTrainingSummary[\"MultilayerPerceptronClassificationTrainingSummary\"],",
          "",
          "---------------",
          "--- Hunk 92 ---",
          "[Context before]",
          "3134:     .. versionadded:: 1.6.0",
          "3135:     \"\"\"",
          "3138:     @since(\"2.0.0\")",
          "3140:         \"\"\"",
          "3141:         the weights of layers.",
          "3142:         \"\"\"",
          "3143:         return self._call_java(\"weights\")",
          "3145:     @since(\"3.1.0\")",
          "3147:         \"\"\"",
          "3148:         Gets summary (accuracy/precision/recall, objective history, total iterations) of model",
          "3149:         trained on the training set. An exception is thrown if `trainingSummary is None`.",
          "",
          "[Removed Lines]",
          "3137:     @property",
          "3139:     def weights(self):",
          "3146:     def summary(self):",
          "",
          "[Added Lines]",
          "3313:     @property  # type: ignore[misc]",
          "3315:     def weights(self) -> Vector:",
          "3322:     def summary(self) -> \"MultilayerPerceptronClassificationTrainingSummary\":",
          "",
          "---------------",
          "--- Hunk 93 ---",
          "[Context before]",
          "3157:                 \"No training summary available for this %s\" % self.__class__.__name__",
          "3158:             )",
          "3161:         \"\"\"",
          "3162:         Evaluates the model on a test dataset.",
          "",
          "[Removed Lines]",
          "3160:     def evaluate(self, dataset):",
          "",
          "[Added Lines]",
          "3336:     def evaluate(self, dataset: DataFrame) -> \"MultilayerPerceptronClassificationSummary\":",
          "",
          "---------------",
          "--- Hunk 94 ---",
          "[Context before]",
          "3202:     Params for :py:class:`OneVsRest` and :py:class:`OneVsRestModelModel`.",
          "3203:     \"\"\"",
          "3207:     @since(\"2.0.0\")",
          "3209:         \"\"\"",
          "3210:         Gets the value of classifier or its default value.",
          "3211:         \"\"\"",
          "",
          "[Removed Lines]",
          "3205:     classifier = Param(Params._dummy(), \"classifier\", \"base binary classifier\")",
          "3208:     def getClassifier(self):",
          "",
          "[Added Lines]",
          "3381:     classifier: Param[Classifier] = Param(Params._dummy(), \"classifier\", \"base binary classifier\")",
          "3384:     def getClassifier(self) -> Classifier:",
          "",
          "---------------",
          "--- Hunk 95 ---",
          "[Context before]",
          "3215: @inherit_doc",
          "3217:     \"\"\"",
          "3218:     Reduction of Multiclass Classification to Binary Classification.",
          "3219:     Performs reduction using one against all strategy.",
          "",
          "[Removed Lines]",
          "3216: class OneVsRest(Estimator, _OneVsRestParams, HasParallelism, MLReadable, MLWritable):",
          "",
          "[Added Lines]",
          "3392: class OneVsRest(",
          "3393:     Estimator[\"OneVsRestModel\"],",
          "3394:     _OneVsRestParams,",
          "3395:     HasParallelism,",
          "3396:     MLReadable[\"OneVsRest\"],",
          "3397:     MLWritable,",
          "3398:     Generic[CM],",
          "3399: ):",
          "",
          "---------------",
          "--- Hunk 96 ---",
          "[Context before]",
          "3264:     ['features', 'rawPrediction', 'newPrediction']",
          "3265:     \"\"\"",
          "3267:     @keyword_only",
          "3268:     def __init__(",
          "3269:         self,",
          "3278:     ):",
          "3279:         \"\"\"",
          "3280:         __init__(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\",
          "",
          "[Removed Lines]",
          "3271:         featuresCol=\"features\",",
          "3272:         labelCol=\"label\",",
          "3273:         predictionCol=\"prediction\",",
          "3274:         rawPredictionCol=\"rawPrediction\",",
          "3275:         classifier=None,",
          "3276:         weightCol=None,",
          "3277:         parallelism=1,",
          "",
          "[Added Lines]",
          "3450:     _input_kwargs: Dict[str, Any]",
          "3456:         featuresCol: str = \"features\",",
          "3457:         labelCol: str = \"label\",",
          "3458:         predictionCol: str = \"prediction\",",
          "3459:         rawPredictionCol: str = \"rawPrediction\",",
          "3460:         classifier: Optional[Classifier[CM]] = None,",
          "3461:         weightCol: Optional[str] = None,",
          "3462:         parallelism: int = 1,",
          "",
          "---------------",
          "--- Hunk 97 ---",
          "[Context before]",
          "3290:     def setParams(",
          "3291:         self,",
          "3301:         \"\"\"",
          "3302:         setParams(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\",
          "3303:                   rawPredictionCol=\"rawPrediction\", classifier=None, weightCol=None, parallelism=1):",
          "",
          "[Removed Lines]",
          "3293:         featuresCol=\"features\",",
          "3294:         labelCol=\"label\",",
          "3295:         predictionCol=\"prediction\",",
          "3296:         rawPredictionCol=\"rawPrediction\",",
          "3297:         classifier=None,",
          "3298:         weightCol=None,",
          "3299:         parallelism=1,",
          "3300:     ):",
          "",
          "[Added Lines]",
          "3478:         featuresCol: str = \"features\",",
          "3479:         labelCol: str = \"label\",",
          "3480:         predictionCol: str = \"prediction\",",
          "3481:         rawPredictionCol: str = \"rawPrediction\",",
          "3482:         classifier: Optional[Classifier[CM]] = None,",
          "3483:         weightCol: Optional[str] = None,",
          "3484:         parallelism: int = 1,",
          "3485:     ) -> \"OneVsRest\":",
          "",
          "---------------",
          "--- Hunk 98 ---",
          "[Context before]",
          "3307:         return self._set(**kwargs)",
          "3309:     @since(\"2.0.0\")",
          "3311:         \"\"\"",
          "3312:         Sets the value of :py:attr:`classifier`.",
          "3313:         \"\"\"",
          "3314:         return self._set(classifier=value)",
          "3317:         \"\"\"",
          "3318:         Sets the value of :py:attr:`labelCol`.",
          "3319:         \"\"\"",
          "3320:         return self._set(labelCol=value)",
          "3323:         \"\"\"",
          "3324:         Sets the value of :py:attr:`featuresCol`.",
          "3325:         \"\"\"",
          "3326:         return self._set(featuresCol=value)",
          "3329:         \"\"\"",
          "3330:         Sets the value of :py:attr:`predictionCol`.",
          "3331:         \"\"\"",
          "3332:         return self._set(predictionCol=value)",
          "3335:         \"\"\"",
          "3336:         Sets the value of :py:attr:`rawPredictionCol`.",
          "3337:         \"\"\"",
          "3338:         return self._set(rawPredictionCol=value)",
          "3341:         \"\"\"",
          "3342:         Sets the value of :py:attr:`weightCol`.",
          "3343:         \"\"\"",
          "3344:         return self._set(weightCol=value)",
          "3347:         \"\"\"",
          "3348:         Sets the value of :py:attr:`parallelism`.",
          "3349:         \"\"\"",
          "3350:         return self._set(parallelism=value)",
          "3353:         labelCol = self.getLabelCol()",
          "3354:         featuresCol = self.getFeaturesCol()",
          "3355:         predictionCol = self.getPredictionCol()",
          "3356:         classifier = self.getClassifier()",
          "3360:         weightCol = None",
          "3361:         if self.isDefined(self.weightCol) and self.getWeightCol():",
          "",
          "[Removed Lines]",
          "3310:     def setClassifier(self, value):",
          "3316:     def setLabelCol(self, value):",
          "3322:     def setFeaturesCol(self, value):",
          "3328:     def setPredictionCol(self, value):",
          "3334:     def setRawPredictionCol(self, value):",
          "3340:     def setWeightCol(self, value):",
          "3346:     def setParallelism(self, value):",
          "3352:     def _fit(self, dataset):",
          "3358:         numClasses = int(dataset.agg({labelCol: \"max\"}).head()[\"max(\" + labelCol + \")\"]) + 1",
          "",
          "[Added Lines]",
          "3495:     def setClassifier(self, value: Classifier[CM]) -> \"OneVsRest\":",
          "3501:     def setLabelCol(self, value: str) -> \"OneVsRest\":",
          "3507:     def setFeaturesCol(self, value: str) -> \"OneVsRest\":",
          "3513:     def setPredictionCol(self, value: str) -> \"OneVsRest\":",
          "3519:     def setRawPredictionCol(self, value: str) -> \"OneVsRest\":",
          "3525:     def setWeightCol(self, value: str) -> \"OneVsRest\":",
          "3531:     def setParallelism(self, value: int) -> \"OneVsRest\":",
          "3537:     def _fit(self, dataset: DataFrame) -> \"OneVsRestModel\":",
          "3543:         numClasses = (",
          "3544:             int(cast(Row, dataset.agg({labelCol: \"max\"}).head())[\"max(\" + labelCol + \")\"]) + 1",
          "3545:         )",
          "",
          "---------------",
          "--- Hunk 99 ---",
          "[Context before]",
          "3376:         if handlePersistence:",
          "3377:             multiclassLabeled.persist(StorageLevel.MEMORY_AND_DISK)",
          "3380:             binaryLabelCol = \"mc2b$\" + str(index)",
          "3381:             trainingDataset = multiclassLabeled.withColumn(",
          "3382:                 binaryLabelCol,",
          "",
          "[Removed Lines]",
          "3379:         def trainSingleClass(index):",
          "",
          "[Added Lines]",
          "3566:         def trainSingleClass(index: int) -> CM:",
          "",
          "---------------",
          "--- Hunk 100 ---",
          "[Context before]",
          "3390:                 ]",
          "3391:             )",
          "3392:             if weightCol:",
          "3394:             return classifier.fit(trainingDataset, paramMap)",
          "3396:         pool = ThreadPool(processes=min(self.getParallelism(), numClasses))",
          "",
          "[Removed Lines]",
          "3393:                 paramMap[classifier.weightCol] = weightCol",
          "",
          "[Added Lines]",
          "3580:                 paramMap[cast(HasWeightCol, classifier).weightCol] = weightCol",
          "",
          "---------------",
          "--- Hunk 101 ---",
          "[Context before]",
          "3403:         return self._copyValues(OneVsRestModel(models=models))",
          "3406:         \"\"\"",
          "3407:         Creates a copy of this instance with a randomly generated uid",
          "3408:         and some extra params. This creates a deep copy of the embedded paramMap,",
          "",
          "[Removed Lines]",
          "3405:     def copy(self, extra=None):",
          "",
          "[Added Lines]",
          "3592:     def copy(self, extra: Optional[\"ParamMap\"] = None) -> \"OneVsRest\":",
          "",
          "---------------",
          "--- Hunk 102 ---",
          "[Context before]",
          "3428:         return newOvr",
          "3430:     @classmethod",
          "3432:         \"\"\"",
          "3433:         Given a Java OneVsRest, create and return a Python wrapper of it.",
          "3434:         Used for ML persistence.",
          "",
          "[Removed Lines]",
          "3431:     def _from_java(cls, java_stage):",
          "",
          "[Added Lines]",
          "3618:     def _from_java(cls, java_stage: \"JavaObject\") -> \"OneVsRest\":",
          "",
          "---------------",
          "--- Hunk 103 ---",
          "[Context before]",
          "3437:         labelCol = java_stage.getLabelCol()",
          "3438:         predictionCol = java_stage.getPredictionCol()",
          "3439:         rawPredictionCol = java_stage.getRawPredictionCol()",
          "3441:         parallelism = java_stage.getParallelism()",
          "3442:         py_stage = cls(",
          "3443:             featuresCol=featuresCol,",
          "",
          "[Removed Lines]",
          "3440:         classifier = JavaParams._from_java(java_stage.getClassifier())",
          "",
          "[Added Lines]",
          "3627:         classifier: Classifier = JavaParams._from_java(java_stage.getClassifier())",
          "",
          "---------------",
          "--- Hunk 104 ---",
          "[Context before]",
          "3452:         py_stage._resetUid(java_stage.uid())",
          "3453:         return py_stage",
          "3456:         \"\"\"",
          "3457:         Transfer this instance to a Java OneVsRest. Used for ML persistence.",
          "",
          "[Removed Lines]",
          "3455:     def _to_java(self):",
          "",
          "[Added Lines]",
          "3642:     def _to_java(self) -> \"JavaObject\":",
          "",
          "---------------",
          "--- Hunk 105 ---",
          "[Context before]",
          "3464:         _java_obj = JavaParams._new_java_obj(",
          "3465:             \"org.apache.spark.ml.classification.OneVsRest\", self.uid",
          "3466:         )",
          "3468:         _java_obj.setParallelism(self.getParallelism())",
          "3469:         _java_obj.setFeaturesCol(self.getFeaturesCol())",
          "3470:         _java_obj.setLabelCol(self.getLabelCol())",
          "",
          "[Removed Lines]",
          "3467:         _java_obj.setClassifier(self.getClassifier()._to_java())",
          "",
          "[Added Lines]",
          "3654:         _java_obj.setClassifier(cast(_JavaClassifier, self.getClassifier())._to_java())",
          "",
          "---------------",
          "--- Hunk 106 ---",
          "[Context before]",
          "3475:         return _java_obj",
          "3477:     @classmethod",
          "3479:         return OneVsRestReader(cls)",
          "3482:         if isinstance(self.getClassifier(), JavaMLWritable):",
          "3484:         else:",
          "3485:             return OneVsRestWriter(self)",
          "3488: class _OneVsRestSharedReadWrite:",
          "3489:     @staticmethod",
          "3491:         skipParams = [\"classifier\"]",
          "3492:         jsonParams = DefaultParamsWriter.extractJsonParams(instance, skipParams)",
          "3493:         DefaultParamsWriter.saveMetadata(",
          "3494:             instance, path, sc, paramMap=jsonParams, extraMetadata=extraMetadata",
          "3495:         )",
          "3496:         classifierPath = os.path.join(path, \"classifier\")",
          "3499:     @staticmethod",
          "3501:         classifierPath = os.path.join(path, \"classifier\")",
          "3502:         return DefaultParamsReader.loadParamsInstance(classifierPath, sc)",
          "3504:     @staticmethod",
          "3507:         if isinstance(instance, OneVsRestModel):",
          "3508:             elems_to_check.extend(instance.models)",
          "",
          "[Removed Lines]",
          "3478:     def read(cls):",
          "3481:     def write(self):",
          "3483:             return JavaMLWriter(self)",
          "3490:     def saveImpl(instance, sc, path, extraMetadata=None):",
          "3497:         instance.getClassifier().save(classifierPath)",
          "3500:     def loadClassifier(path, sc):",
          "3505:     def validateParams(instance):",
          "3506:         elems_to_check = [instance.getClassifier()]",
          "",
          "[Added Lines]",
          "3665:     def read(cls) -> \"OneVsRestReader\":",
          "3668:     def write(self) -> MLWriter:",
          "3670:             return JavaMLWriter(self)  # type: ignore[arg-type]",
          "3677:     def saveImpl(",
          "3678:         instance: Union[OneVsRest, \"OneVsRestModel\"],",
          "3679:         sc: SparkContext,",
          "3680:         path: str,",
          "3681:         extraMetadata: Optional[Dict[str, Any]] = None,",
          "3682:     ) -> None:",
          "3689:         cast(MLWritable, instance.getClassifier()).save(classifierPath)",
          "3692:     def loadClassifier(path: str, sc: SparkContext) -> Union[OneVsRest, \"OneVsRestModel\"]:",
          "3697:     def validateParams(instance: Union[OneVsRest, \"OneVsRestModel\"]) -> None:",
          "3698:         elems_to_check: List[Params] = [instance.getClassifier()]",
          "",
          "---------------",
          "--- Hunk 107 ---",
          "[Context before]",
          "3518: @inherit_doc",
          "3521:         super(OneVsRestReader, self).__init__()",
          "3522:         self.cls = cls",
          "3525:         metadata = DefaultParamsReader.loadMetadata(path, self.sc)",
          "3526:         if not DefaultParamsReader.isPythonParamsInstance(metadata):",
          "3528:         else:",
          "3531:             DefaultParamsReader.getAndSetParams(ova, metadata, skipParams=[\"classifier\"])",
          "3532:             return ova",
          "3535: @inherit_doc",
          "3536: class OneVsRestWriter(MLWriter):",
          "3538:         super(OneVsRestWriter, self).__init__()",
          "3539:         self.instance = instance",
          "3542:         _OneVsRestSharedReadWrite.validateParams(self.instance)",
          "3543:         _OneVsRestSharedReadWrite.saveImpl(self.instance, self.sc, path)",
          "3547:     \"\"\"",
          "3548:     Model fitted by OneVsRest.",
          "3549:     This stores the models resulting from training k binary classifiers: one for each class.",
          "",
          "[Removed Lines]",
          "3519: class OneVsRestReader(MLReader):",
          "3520:     def __init__(self, cls):",
          "3524:     def load(self, path):",
          "3527:             return JavaMLReader(self.cls).load(path)",
          "3529:             classifier = _OneVsRestSharedReadWrite.loadClassifier(path, self.sc)",
          "3530:             ova = OneVsRest(classifier=classifier)._resetUid(metadata[\"uid\"])",
          "3537:     def __init__(self, instance):",
          "3541:     def saveImpl(self, path):",
          "3546: class OneVsRestModel(Model, _OneVsRestParams, MLReadable, MLWritable):",
          "",
          "[Added Lines]",
          "3711: class OneVsRestReader(MLReader[OneVsRest]):",
          "3712:     def __init__(self, cls: Type[OneVsRest]) -> None:",
          "3716:     def load(self, path: str) -> OneVsRest:",
          "3719:             return JavaMLReader(self.cls).load(path)  # type: ignore[arg-type]",
          "3721:             classifier = cast(Classifier, _OneVsRestSharedReadWrite.loadClassifier(path, self.sc))",
          "3722:             ova: OneVsRest = OneVsRest(classifier=classifier)._resetUid(metadata[\"uid\"])",
          "3729:     def __init__(self, instance: OneVsRest):",
          "3733:     def saveImpl(self, path: str) -> None:",
          "3738: class OneVsRestModel(",
          "3739:     Model,",
          "3740:     _OneVsRestParams,",
          "3741:     MLReadable[\"OneVsRestModel\"],",
          "3742:     MLWritable,",
          "3743: ):",
          "",
          "---------------",
          "--- Hunk 108 ---",
          "[Context before]",
          "3553:     .. versionadded:: 2.0.0",
          "3554:     \"\"\"",
          "3557:         \"\"\"",
          "3558:         Sets the value of :py:attr:`featuresCol`.",
          "3559:         \"\"\"",
          "3560:         return self._set(featuresCol=value)",
          "3563:         \"\"\"",
          "3564:         Sets the value of :py:attr:`predictionCol`.",
          "3565:         \"\"\"",
          "3566:         return self._set(predictionCol=value)",
          "3569:         \"\"\"",
          "3570:         Sets the value of :py:attr:`rawPredictionCol`.",
          "3571:         \"\"\"",
          "3572:         return self._set(rawPredictionCol=value)",
          "3575:         super(OneVsRestModel, self).__init__()",
          "3576:         self.models = models",
          "3577:         if not isinstance(models[0], JavaMLWritable):",
          "3578:             return",
          "3579:         # set java instance",
          "3581:         sc = SparkContext._active_spark_context",
          "3582:         java_models_array = JavaWrapper._new_java_array(",
          "3583:             java_models, sc._gateway.jvm.org.apache.spark.ml.classification.ClassificationModel",
          "3584:         )",
          "",
          "[Removed Lines]",
          "3556:     def setFeaturesCol(self, value):",
          "3562:     def setPredictionCol(self, value):",
          "3568:     def setRawPredictionCol(self, value):",
          "3574:     def __init__(self, models):",
          "3580:         java_models = [model._to_java() for model in self.models]",
          "",
          "[Added Lines]",
          "3753:     def setFeaturesCol(self, value: str) -> \"OneVsRestModel\":",
          "3759:     def setPredictionCol(self, value: str) -> \"OneVsRestModel\":",
          "3765:     def setRawPredictionCol(self, value: str) -> \"OneVsRestModel\":",
          "3771:     def __init__(self, models: List[ClassificationModel]):",
          "3777:         java_models = [cast(_JavaClassificationModel, model)._to_java() for model in self.models]",
          "3779:         assert sc is not None and sc._gateway is not None",
          "",
          "---------------",
          "--- Hunk 109 ---",
          "[Context before]",
          "3591:             java_models_array,",
          "3592:         )",
          "3595:         # determine the input columns: these need to be passed through",
          "3596:         origCols = dataset.columns",
          "",
          "[Removed Lines]",
          "3594:     def _transform(self, dataset):",
          "",
          "[Added Lines]",
          "3793:     def _transform(self, dataset: DataFrame) -> DataFrame:",
          "",
          "---------------",
          "--- Hunk 110 ---",
          "[Context before]",
          "3636:         if self.getRawPredictionCol():",
          "3640:                 for x in predictions:",
          "3641:                     predArray.append(x)",
          "3642:                 return Vectors.dense(predArray)",
          "",
          "[Removed Lines]",
          "3638:             def func(predictions):",
          "3639:                 predArray = []",
          "",
          "[Added Lines]",
          "3837:             def func(predictions: Iterable[float]) -> Vector:",
          "3838:                 predArray: List[float] = []",
          "",
          "---------------",
          "--- Hunk 111 ---",
          "[Context before]",
          "3659:             )",
          "3660:         return aggregatedDataset.drop(accColName)",
          "3663:         \"\"\"",
          "3664:         Creates a copy of this instance with a randomly generated uid",
          "3665:         and some extra params. This creates a deep copy of the embedded paramMap,",
          "",
          "[Removed Lines]",
          "3662:     def copy(self, extra=None):",
          "",
          "[Added Lines]",
          "3861:     def copy(self, extra: Optional[\"ParamMap\"] = None) -> \"OneVsRestModel\":",
          "",
          "---------------",
          "--- Hunk 112 ---",
          "[Context before]",
          "3684:         return newModel",
          "3686:     @classmethod",
          "3688:         \"\"\"",
          "3689:         Given a Java OneVsRestModel, create and return a Python wrapper of it.",
          "3690:         Used for ML persistence.",
          "",
          "[Removed Lines]",
          "3687:     def _from_java(cls, java_stage):",
          "",
          "[Added Lines]",
          "3886:     def _from_java(cls, java_stage: \"JavaObject\") -> \"OneVsRestModel\":",
          "",
          "---------------",
          "--- Hunk 113 ---",
          "[Context before]",
          "3692:         featuresCol = java_stage.getFeaturesCol()",
          "3693:         labelCol = java_stage.getLabelCol()",
          "3694:         predictionCol = java_stage.getPredictionCol()",
          "3697:         py_stage = cls(models=models).setPredictionCol(predictionCol).setFeaturesCol(featuresCol)",
          "3698:         py_stage._set(labelCol=labelCol)",
          "3699:         if java_stage.isDefined(java_stage.getParam(\"weightCol\")):",
          "",
          "[Removed Lines]",
          "3695:         classifier = JavaParams._from_java(java_stage.getClassifier())",
          "3696:         models = [JavaParams._from_java(model) for model in java_stage.models()]",
          "",
          "[Added Lines]",
          "3894:         classifier: Classifier = JavaParams._from_java(java_stage.getClassifier())",
          "3895:         models: List[ClassificationModel] = [",
          "3896:             JavaParams._from_java(model) for model in java_stage.models()",
          "3897:         ]",
          "",
          "---------------",
          "--- Hunk 114 ---",
          "[Context before]",
          "3702:         py_stage._resetUid(java_stage.uid())",
          "3703:         return py_stage",
          "3706:         \"\"\"",
          "3707:         Transfer this instance to a Java OneVsRestModel. Used for ML persistence.",
          "",
          "[Removed Lines]",
          "3705:     def _to_java(self):",
          "",
          "[Added Lines]",
          "3906:     def _to_java(self) -> \"JavaObject\":",
          "",
          "---------------",
          "--- Hunk 115 ---",
          "[Context before]",
          "3712:             Java object equivalent to this instance.",
          "3713:         \"\"\"",
          "3714:         sc = SparkContext._active_spark_context",
          "3716:         java_models_array = JavaWrapper._new_java_array(",
          "3717:             java_models, sc._gateway.jvm.org.apache.spark.ml.classification.ClassificationModel",
          "3718:         )",
          "",
          "[Removed Lines]",
          "3715:         java_models = [model._to_java() for model in self.models]",
          "",
          "[Added Lines]",
          "3916:         assert sc is not None and sc._gateway is not None",
          "3918:         java_models = [cast(_JavaClassificationModel, model)._to_java() for model in self.models]",
          "",
          "---------------",
          "--- Hunk 116 ---",
          "[Context before]",
          "3723:             metadata.empty(),",
          "3724:             java_models_array,",
          "3725:         )",
          "3727:         _java_obj.set(\"featuresCol\", self.getFeaturesCol())",
          "3728:         _java_obj.set(\"labelCol\", self.getLabelCol())",
          "3729:         _java_obj.set(\"predictionCol\", self.getPredictionCol())",
          "",
          "[Removed Lines]",
          "3726:         _java_obj.set(\"classifier\", self.getClassifier()._to_java())",
          "",
          "[Added Lines]",
          "3929:         _java_obj.set(\"classifier\", cast(_JavaClassifier, self.getClassifier())._to_java())",
          "",
          "---------------",
          "--- Hunk 117 ---",
          "[Context before]",
          "3732:         return _java_obj",
          "3734:     @classmethod",
          "3736:         return OneVsRestModelReader(cls)",
          "3739:         if all(",
          "3741:         ):",
          "3743:         else:",
          "3744:             return OneVsRestModelWriter(self)",
          "3747: @inherit_doc",
          "3750:         super(OneVsRestModelReader, self).__init__()",
          "3751:         self.cls = cls",
          "3754:         metadata = DefaultParamsReader.loadMetadata(path, self.sc)",
          "3755:         if not DefaultParamsReader.isPythonParamsInstance(metadata):",
          "3757:         else:",
          "3758:             classifier = _OneVsRestSharedReadWrite.loadClassifier(path, self.sc)",
          "3759:             numClasses = metadata[\"numClasses\"]",
          "",
          "[Removed Lines]",
          "3735:     def read(cls):",
          "3738:     def write(self):",
          "3740:             map(lambda elem: isinstance(elem, JavaMLWritable), [self.getClassifier()] + self.models)",
          "3742:             return JavaMLWriter(self)",
          "3748: class OneVsRestModelReader(MLReader):",
          "3749:     def __init__(self, cls):",
          "3753:     def load(self, path):",
          "3756:             return JavaMLReader(self.cls).load(path)",
          "",
          "[Added Lines]",
          "3938:     def read(cls) -> \"OneVsRestModelReader\":",
          "3941:     def write(self) -> MLWriter:",
          "3943:             map(",
          "3944:                 lambda elem: isinstance(elem, JavaMLWritable),",
          "3945:                 [self.getClassifier()] + self.models,  # type: ignore[operator]",
          "3946:             )",
          "3948:             return JavaMLWriter(self)  # type: ignore[arg-type]",
          "3954: class OneVsRestModelReader(MLReader[OneVsRestModel]):",
          "3955:     def __init__(self, cls: Type[OneVsRestModel]):",
          "3959:     def load(self, path: str) -> OneVsRestModel:",
          "3962:             return JavaMLReader(self.cls).load(path)  # type: ignore[arg-type]",
          "",
          "---------------",
          "--- Hunk 118 ---",
          "[Context before]",
          "3761:             for idx in range(numClasses):",
          "3762:                 subModelPath = os.path.join(path, f\"model_{idx}\")",
          "3763:                 subModels[idx] = DefaultParamsReader.loadParamsInstance(subModelPath, self.sc)",
          "3765:             ovaModel.set(ovaModel.classifier, classifier)",
          "3766:             DefaultParamsReader.getAndSetParams(ovaModel, metadata, skipParams=[\"classifier\"])",
          "3767:             return ovaModel",
          "",
          "[Removed Lines]",
          "3764:             ovaModel = OneVsRestModel(subModels)._resetUid(metadata[\"uid\"])",
          "",
          "[Added Lines]",
          "3970:             ovaModel = OneVsRestModel(cast(List[ClassificationModel], subModels))._resetUid(",
          "3971:                 metadata[\"uid\"]",
          "3972:             )",
          "",
          "---------------",
          "--- Hunk 119 ---",
          "[Context before]",
          "3770: @inherit_doc",
          "3771: class OneVsRestModelWriter(MLWriter):",
          "3773:         super(OneVsRestModelWriter, self).__init__()",
          "3774:         self.instance = instance",
          "3777:         _OneVsRestSharedReadWrite.validateParams(self.instance)",
          "3778:         instance = self.instance",
          "3779:         numClasses = len(instance.models)",
          "",
          "[Removed Lines]",
          "3772:     def __init__(self, instance):",
          "3776:     def saveImpl(self, path):",
          "",
          "[Added Lines]",
          "3980:     def __init__(self, instance: OneVsRestModel):",
          "3984:     def saveImpl(self, path: str) -> None:",
          "",
          "---------------",
          "--- Hunk 120 ---",
          "[Context before]",
          "3781:         _OneVsRestSharedReadWrite.saveImpl(instance, self.sc, path, extraMetadata=extraMetadata)",
          "3782:         for idx in range(numClasses):",
          "3783:             subModelPath = os.path.join(path, f\"model_{idx}\")",
          "3787: @inherit_doc",
          "3788: class FMClassifier(",
          "3790: ):",
          "3791:     \"\"\"",
          "3792:     Factorization Machines learning algorithm for classification.",
          "",
          "[Removed Lines]",
          "3784:             instance.models[idx].save(subModelPath)",
          "3789:     _JavaProbabilisticClassifier, _FactorizationMachinesParams, JavaMLWritable, JavaMLReadable",
          "",
          "[Added Lines]",
          "3992:             cast(MLWritable, instance.models[idx]).save(subModelPath)",
          "3997:     _JavaProbabilisticClassifier[\"FMClassificationModel\"],",
          "3998:     _FactorizationMachinesParams,",
          "3999:     JavaMLWritable,",
          "4000:     JavaMLReadable[\"FMClassifier\"],",
          "",
          "---------------",
          "--- Hunk 121 ---",
          "[Context before]",
          "3849:     True",
          "3850:     \"\"\"",
          "3852:     @keyword_only",
          "3853:     def __init__(",
          "3854:         self,",
          "3873:     ):",
          "3874:         \"\"\"",
          "3875:         __init__(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\",
          "",
          "[Removed Lines]",
          "3856:         featuresCol=\"features\",",
          "3857:         labelCol=\"label\",",
          "3858:         predictionCol=\"prediction\",",
          "3859:         probabilityCol=\"probability\",",
          "3860:         rawPredictionCol=\"rawPrediction\",",
          "3861:         factorSize=8,",
          "3862:         fitIntercept=True,",
          "3863:         fitLinear=True,",
          "3864:         regParam=0.0,",
          "3865:         miniBatchFraction=1.0,",
          "3866:         initStd=0.01,",
          "3867:         maxIter=100,",
          "3868:         stepSize=1.0,",
          "3869:         tol=1e-6,",
          "3870:         solver=\"adamW\",",
          "3871:         thresholds=None,",
          "3872:         seed=None,",
          "",
          "[Added Lines]",
          "4063:     _input_kwargs: Dict[str, Any]",
          "4069:         featuresCol: str = \"features\",",
          "4070:         labelCol: str = \"label\",",
          "4071:         predictionCol: str = \"prediction\",",
          "4072:         probabilityCol: str = \"probability\",",
          "4073:         rawPredictionCol: str = \"rawPrediction\",",
          "4074:         factorSize: int = 8,",
          "4075:         fitIntercept: bool = True,",
          "4076:         fitLinear: bool = True,",
          "4077:         regParam: float = 0.0,",
          "4078:         miniBatchFraction: float = 1.0,",
          "4079:         initStd: float = 0.01,",
          "4080:         maxIter: int = 100,",
          "4081:         stepSize: float = 1.0,",
          "4082:         tol: float = 1e-6,",
          "4083:         solver: str = \"adamW\",",
          "4084:         thresholds: Optional[List[float]] = None,",
          "4085:         seed: Optional[int] = None,",
          "",
          "---------------",
          "--- Hunk 122 ---",
          "[Context before]",
          "3890:     def setParams(",
          "3891:         self,",
          "3911:         \"\"\"",
          "3912:         setParams(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\",
          "3913:                   probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\", \\",
          "",
          "[Removed Lines]",
          "3893:         featuresCol=\"features\",",
          "3894:         labelCol=\"label\",",
          "3895:         predictionCol=\"prediction\",",
          "3896:         probabilityCol=\"probability\",",
          "3897:         rawPredictionCol=\"rawPrediction\",",
          "3898:         factorSize=8,",
          "3899:         fitIntercept=True,",
          "3900:         fitLinear=True,",
          "3901:         regParam=0.0,",
          "3902:         miniBatchFraction=1.0,",
          "3903:         initStd=0.01,",
          "3904:         maxIter=100,",
          "3905:         stepSize=1.0,",
          "3906:         tol=1e-6,",
          "3907:         solver=\"adamW\",",
          "3908:         thresholds=None,",
          "3909:         seed=None,",
          "3910:     ):",
          "",
          "[Added Lines]",
          "4106:         featuresCol: str = \"features\",",
          "4107:         labelCol: str = \"label\",",
          "4108:         predictionCol: str = \"prediction\",",
          "4109:         probabilityCol: str = \"probability\",",
          "4110:         rawPredictionCol: str = \"rawPrediction\",",
          "4111:         factorSize: int = 8,",
          "4112:         fitIntercept: bool = True,",
          "4113:         fitLinear: bool = True,",
          "4114:         regParam: float = 0.0,",
          "4115:         miniBatchFraction: float = 1.0,",
          "4116:         initStd: float = 0.01,",
          "4117:         maxIter: int = 100,",
          "4118:         stepSize: float = 1.0,",
          "4119:         tol: float = 1e-6,",
          "4120:         solver: str = \"adamW\",",
          "4121:         thresholds: Optional[List[float]] = None,",
          "4122:         seed: Optional[int] = None,",
          "4123:     ) -> \"FMClassifier\":",
          "",
          "---------------",
          "--- Hunk 123 ---",
          "[Context before]",
          "3919:         kwargs = self._input_kwargs",
          "3920:         return self._set(**kwargs)",
          "3923:         return FMClassificationModel(java_model)",
          "3925:     @since(\"3.0.0\")",
          "3927:         \"\"\"",
          "3928:         Sets the value of :py:attr:`factorSize`.",
          "3929:         \"\"\"",
          "3930:         return self._set(factorSize=value)",
          "3932:     @since(\"3.0.0\")",
          "3934:         \"\"\"",
          "3935:         Sets the value of :py:attr:`fitLinear`.",
          "3936:         \"\"\"",
          "3937:         return self._set(fitLinear=value)",
          "3939:     @since(\"3.0.0\")",
          "3941:         \"\"\"",
          "3942:         Sets the value of :py:attr:`miniBatchFraction`.",
          "3943:         \"\"\"",
          "3944:         return self._set(miniBatchFraction=value)",
          "3946:     @since(\"3.0.0\")",
          "3948:         \"\"\"",
          "3949:         Sets the value of :py:attr:`initStd`.",
          "3950:         \"\"\"",
          "3951:         return self._set(initStd=value)",
          "3953:     @since(\"3.0.0\")",
          "3955:         \"\"\"",
          "3956:         Sets the value of :py:attr:`maxIter`.",
          "3957:         \"\"\"",
          "3958:         return self._set(maxIter=value)",
          "3960:     @since(\"3.0.0\")",
          "3962:         \"\"\"",
          "3963:         Sets the value of :py:attr:`stepSize`.",
          "3964:         \"\"\"",
          "3965:         return self._set(stepSize=value)",
          "3967:     @since(\"3.0.0\")",
          "3969:         \"\"\"",
          "3970:         Sets the value of :py:attr:`tol`.",
          "3971:         \"\"\"",
          "3972:         return self._set(tol=value)",
          "3974:     @since(\"3.0.0\")",
          "3976:         \"\"\"",
          "3977:         Sets the value of :py:attr:`solver`.",
          "3978:         \"\"\"",
          "3979:         return self._set(solver=value)",
          "3981:     @since(\"3.0.0\")",
          "3983:         \"\"\"",
          "3984:         Sets the value of :py:attr:`seed`.",
          "3985:         \"\"\"",
          "3986:         return self._set(seed=value)",
          "3988:     @since(\"3.0.0\")",
          "3990:         \"\"\"",
          "3991:         Sets the value of :py:attr:`fitIntercept`.",
          "3992:         \"\"\"",
          "3993:         return self._set(fitIntercept=value)",
          "3995:     @since(\"3.0.0\")",
          "3997:         \"\"\"",
          "3998:         Sets the value of :py:attr:`regParam`.",
          "3999:         \"\"\"",
          "",
          "[Removed Lines]",
          "3922:     def _create_model(self, java_model):",
          "3926:     def setFactorSize(self, value):",
          "3933:     def setFitLinear(self, value):",
          "3940:     def setMiniBatchFraction(self, value):",
          "3947:     def setInitStd(self, value):",
          "3954:     def setMaxIter(self, value):",
          "3961:     def setStepSize(self, value):",
          "3968:     def setTol(self, value):",
          "3975:     def setSolver(self, value):",
          "3982:     def setSeed(self, value):",
          "3989:     def setFitIntercept(self, value):",
          "3996:     def setRegParam(self, value):",
          "",
          "[Added Lines]",
          "4135:     def _create_model(self, java_model: \"JavaObject\") -> \"FMClassificationModel\":",
          "4139:     def setFactorSize(self, value: int) -> \"FMClassifier\":",
          "4146:     def setFitLinear(self, value: bool) -> \"FMClassifier\":",
          "4153:     def setMiniBatchFraction(self, value: float) -> \"FMClassifier\":",
          "4160:     def setInitStd(self, value: float) -> \"FMClassifier\":",
          "4167:     def setMaxIter(self, value: int) -> \"FMClassifier\":",
          "4174:     def setStepSize(self, value: float) -> \"FMClassifier\":",
          "4181:     def setTol(self, value: float) -> \"FMClassifier\":",
          "4188:     def setSolver(self, value: str) -> \"FMClassifier\":",
          "4195:     def setSeed(self, value: int) -> \"FMClassifier\":",
          "4202:     def setFitIntercept(self, value: bool) -> \"FMClassifier\":",
          "4209:     def setRegParam(self, value: float) -> \"FMClassifier\":",
          "",
          "---------------",
          "--- Hunk 124 ---",
          "[Context before]",
          "4003: class FMClassificationModel(",
          "4005:     _FactorizationMachinesParams,",
          "4006:     JavaMLWritable,",
          "4008:     HasTrainingSummary,",
          "4009: ):",
          "4010:     \"\"\"",
          "",
          "[Removed Lines]",
          "4004:     _JavaProbabilisticClassificationModel,",
          "4007:     JavaMLReadable,",
          "",
          "[Added Lines]",
          "4217:     _JavaProbabilisticClassificationModel[Vector],",
          "4220:     JavaMLReadable[\"FMClassificationModel\"],",
          "",
          "---------------",
          "--- Hunk 125 ---",
          "[Context before]",
          "4013:     .. versionadded:: 3.0.0",
          "4014:     \"\"\"",
          "4017:     @since(\"3.0.0\")",
          "4019:         \"\"\"",
          "4020:         Model intercept.",
          "4021:         \"\"\"",
          "4022:         return self._call_java(\"intercept\")",
          "4025:     @since(\"3.0.0\")",
          "4027:         \"\"\"",
          "4028:         Model linear term.",
          "4029:         \"\"\"",
          "4030:         return self._call_java(\"linear\")",
          "4033:     @since(\"3.0.0\")",
          "4035:         \"\"\"",
          "4036:         Model factor term.",
          "4037:         \"\"\"",
          "4038:         return self._call_java(\"factors\")",
          "4040:     @since(\"3.1.0\")",
          "4042:         \"\"\"",
          "4043:         Gets summary (accuracy/precision/recall, objective history, total iterations) of model",
          "4044:         trained on the training set. An exception is thrown if `trainingSummary is None`.",
          "",
          "[Removed Lines]",
          "4016:     @property",
          "4018:     def intercept(self):",
          "4024:     @property",
          "4026:     def linear(self):",
          "4032:     @property",
          "4034:     def factors(self):",
          "4041:     def summary(self):",
          "",
          "[Added Lines]",
          "4229:     @property  # type: ignore[misc]",
          "4231:     def intercept(self) -> float:",
          "4237:     @property  # type: ignore[misc]",
          "4239:     def linear(self) -> Vector:",
          "4245:     @property  # type: ignore[misc]",
          "4247:     def factors(self) -> Matrix:",
          "4254:     def summary(self) -> \"FMClassificationTrainingSummary\":",
          "",
          "---------------",
          "--- Hunk 126 ---",
          "[Context before]",
          "4050:                 \"No training summary available for this %s\" % self.__class__.__name__",
          "4051:             )",
          "4054:         \"\"\"",
          "4055:         Evaluates the model on a test dataset.",
          "",
          "[Removed Lines]",
          "4053:     def evaluate(self, dataset):",
          "",
          "[Added Lines]",
          "4266:     def evaluate(self, dataset: DataFrame) -> \"FMClassificationSummary\":",
          "",
          "---------------"
        ],
        "python/pyspark/ml/classification.pyi||python/pyspark/ml/classification.pyi": [
          "File: python/pyspark/ml/classification.pyi -> python/pyspark/ml/classification.pyi",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "609efe1515fc03d04ebcfc4b7f25872c2cfbe185",
      "candidate_info": {
        "commit_hash": "609efe1515fc03d04ebcfc4b7f25872c2cfbe185",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/609efe1515fc03d04ebcfc4b7f25872c2cfbe185",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2StrategySuite.scala"
        ],
        "message": "[SPARK-39857][SQL][3.3] V2ExpressionBuilder uses the wrong LiteralValue data type for In predicate\n\n### What changes were proposed in this pull request?\n\nWhen building V2 In `Predicate` in `V2ExpressionBuilder`, `InSet.dataType` (which is BooleanType) is used to build the `LiteralValue`, `InSet.child.dataType `should be used instead.\n\nback port https://github.com/apache/spark/pull/37271 to 3.3\n\n### Why are the changes needed?\nbug fix\n\n### Does this PR introduce _any_ user-facing change?\nno\n\n### How was this patch tested?\nnew test\n\nCloses #37324 from huaxingao/backport.\n\nAuthored-by: huaxingao <huaxin_gao@apple.com>\nSigned-off-by: huaxingao <huaxin_gao@apple.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala||sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2StrategySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2StrategySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala||sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala -> sql/core/src/main/scala/org/apache/spark/sql/catalyst/util/V2ExpressionBuilder.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "52:       } else {",
          "53:         Some(ref)",
          "54:       }",
          "56:       generateExpression(child).map { v =>",
          "57:         val children =",
          "59:         new V2Predicate(\"IN\", children)",
          "60:       }",
          "",
          "[Removed Lines]",
          "55:     case in @ InSet(child, hset) =>",
          "58:           (v +: hset.toSeq.map(elem => LiteralValue(elem, in.dataType))).toArray[V2Expression]",
          "",
          "[Added Lines]",
          "55:     case InSet(child, hset) =>",
          "58:           (v +: hset.toSeq.map(elem => LiteralValue(elem, child.dataType))).toArray[V2Expression]",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2StrategySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2StrategySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2StrategySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2StrategySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.execution.datasources.v2",
          "20: import org.apache.spark.sql.catalyst.dsl.expressions._",
          "22: import org.apache.spark.sql.catalyst.plans.PlanTest",
          "23: import org.apache.spark.sql.connector.expressions.{FieldReference, LiteralValue}",
          "25: import org.apache.spark.sql.test.SharedSparkSession",
          "28: class DataSourceV2StrategySuite extends PlanTest with SharedSparkSession {",
          "29:   test(\"SPARK-36644: Push down boolean column filter\") {",
          "31:       Some(new Predicate(\"=\", Array(FieldReference(\"col\"), LiteralValue(true, BooleanType)))))",
          "32:   }",
          "",
          "[Removed Lines]",
          "21: import org.apache.spark.sql.catalyst.expressions.Expression",
          "24: import org.apache.spark.sql.connector.expressions.filter.Predicate",
          "26: import org.apache.spark.sql.types.BooleanType",
          "30:     testTranslateFilter(Symbol(\"col\").boolean,",
          "",
          "[Added Lines]",
          "21: import org.apache.spark.sql.catalyst.expressions._",
          "24: import org.apache.spark.sql.connector.expressions.filter.{And => V2And, Not => V2Not, Or => V2Or, Predicate}",
          "26: import org.apache.spark.sql.types.{BooleanType, IntegerType, StringType, StructField, StructType}",
          "27: import org.apache.spark.unsafe.types.UTF8String",
          "30:   val attrInts = Seq(",
          "31:     $\"cint\".int,",
          "32:     $\"c.int\".int,",
          "33:     GetStructField($\"a\".struct(StructType(",
          "34:       StructField(\"cstr\", StringType, nullable = true) ::",
          "35:         StructField(\"cint\", IntegerType, nullable = true) :: Nil)), 1, None),",
          "36:     GetStructField($\"a\".struct(StructType(",
          "37:       StructField(\"c.int\", IntegerType, nullable = true) ::",
          "38:         StructField(\"cstr\", StringType, nullable = true) :: Nil)), 0, None),",
          "39:     GetStructField($\"a.b\".struct(StructType(",
          "40:       StructField(\"cstr1\", StringType, nullable = true) ::",
          "41:         StructField(\"cstr2\", StringType, nullable = true) ::",
          "42:         StructField(\"cint\", IntegerType, nullable = true) :: Nil)), 2, None),",
          "43:     GetStructField($\"a.b\".struct(StructType(",
          "44:       StructField(\"c.int\", IntegerType, nullable = true) :: Nil)), 0, None),",
          "45:     GetStructField(GetStructField($\"a\".struct(StructType(",
          "46:       StructField(\"cstr1\", StringType, nullable = true) ::",
          "47:         StructField(\"b\", StructType(StructField(\"cint\", IntegerType, nullable = true) ::",
          "48:           StructField(\"cstr2\", StringType, nullable = true) :: Nil)) :: Nil)), 1, None), 0, None)",
          "49:   ).zip(Seq(",
          "50:     \"cint\",",
          "51:     \"`c.int`\", // single level field that contains `dot` in name",
          "52:     \"a.cint\", // two level nested field",
          "53:     \"a.`c.int`\", // two level nested field, and nested level contains `dot`",
          "54:     \"`a.b`.cint\", // two level nested field, and top level contains `dot`",
          "55:     \"`a.b`.`c.int`\", // two level nested field, and both levels contain `dot`",
          "56:     \"a.b.cint\" // three level nested field",
          "57:   ))",
          "59:   val attrStrs = Seq(",
          "60:     $\"cstr\".string,",
          "61:     $\"c.str\".string,",
          "62:     GetStructField($\"a\".struct(StructType(",
          "63:       StructField(\"cint\", IntegerType, nullable = true) ::",
          "64:         StructField(\"cstr\", StringType, nullable = true) :: Nil)), 1, None),",
          "65:     GetStructField($\"a\".struct(StructType(",
          "66:       StructField(\"c.str\", StringType, nullable = true) ::",
          "67:         StructField(\"cint\", IntegerType, nullable = true) :: Nil)), 0, None),",
          "68:     GetStructField($\"a.b\".struct(StructType(",
          "69:       StructField(\"cint1\", IntegerType, nullable = true) ::",
          "70:         StructField(\"cint2\", IntegerType, nullable = true) ::",
          "71:         StructField(\"cstr\", StringType, nullable = true) :: Nil)), 2, None),",
          "72:     GetStructField($\"a.b\".struct(StructType(",
          "73:       StructField(\"c.str\", StringType, nullable = true) :: Nil)), 0, None),",
          "74:     GetStructField(GetStructField($\"a\".struct(StructType(",
          "75:       StructField(\"cint1\", IntegerType, nullable = true) ::",
          "76:         StructField(\"b\", StructType(StructField(\"cstr\", StringType, nullable = true) ::",
          "77:           StructField(\"cint2\", IntegerType, nullable = true) :: Nil)) :: Nil)), 1, None), 0, None)",
          "78:   ).zip(Seq(",
          "79:     \"cstr\",",
          "80:     \"`c.str`\", // single level field that contains `dot` in name",
          "81:     \"a.cstr\", // two level nested field",
          "82:     \"a.`c.str`\", // two level nested field, and nested level contains `dot`",
          "83:     \"`a.b`.cstr\", // two level nested field, and top level contains `dot`",
          "84:     \"`a.b`.`c.str`\", // two level nested field, and both levels contain `dot`",
          "85:     \"a.b.cstr\" // three level nested field",
          "86:   ))",
          "88:   test(\"translate simple expression\") { attrInts.zip(attrStrs)",
          "89:     .foreach { case ((attrInt, intColName), (attrStr, strColName)) =>",
          "90:       testTranslateFilter(EqualTo(attrInt, 1),",
          "91:         Some(new Predicate(\"=\", Array(FieldReference(intColName), LiteralValue(1, IntegerType)))))",
          "93:       testTranslateFilter(EqualNullSafe(attrInt, 1),",
          "94:         Some(new Predicate(\"<=>\", Array(FieldReference(intColName), LiteralValue(1, IntegerType)))))",
          "96:       testTranslateFilter(GreaterThan(attrInt, 1),",
          "97:         Some(new Predicate(\">\", Array(FieldReference(intColName), LiteralValue(1, IntegerType)))))",
          "99:       testTranslateFilter(LessThan(attrInt, 1),",
          "100:         Some(new Predicate(\"<\", Array(FieldReference(intColName), LiteralValue(1, IntegerType)))))",
          "102:       testTranslateFilter(GreaterThanOrEqual(attrInt, 1),",
          "103:         Some(new Predicate(\">=\", Array(FieldReference(intColName), LiteralValue(1, IntegerType)))))",
          "105:       testTranslateFilter(LessThanOrEqual(attrInt, 1),",
          "106:         Some(new Predicate(\"<=\", Array(FieldReference(intColName), LiteralValue(1, IntegerType)))))",
          "108:       testTranslateFilter(IsNull(attrInt),",
          "109:         Some(new Predicate(\"IS_NULL\", Array(FieldReference(intColName)))))",
          "110:       testTranslateFilter(IsNotNull(attrInt),",
          "111:         Some(new Predicate(\"IS_NOT_NULL\", Array(FieldReference(intColName)))))",
          "113:       testTranslateFilter(InSet(attrInt, Set(1, 2, 3)),",
          "114:         Some(new Predicate(\"IN\", Array(FieldReference(intColName),",
          "115:           LiteralValue(1, IntegerType), LiteralValue(2, IntegerType),",
          "116:           LiteralValue(3, IntegerType)))))",
          "118:       testTranslateFilter(In(attrInt, Seq(1, 2, 3)),",
          "119:         Some(new Predicate(\"IN\", Array(FieldReference(intColName),",
          "120:           LiteralValue(1, IntegerType), LiteralValue(2, IntegerType),",
          "121:           LiteralValue(3, IntegerType)))))",
          "124:       testTranslateFilter(And(",
          "125:         GreaterThan(attrInt, 1),",
          "126:         LessThan(attrInt, 10)),",
          "127:         Some(new V2And(",
          "128:           new Predicate(\">\", Array(FieldReference(intColName), LiteralValue(1, IntegerType))),",
          "129:           new Predicate(\"<\", Array(FieldReference(intColName), LiteralValue(10, IntegerType))))))",
          "132:       testTranslateFilter(Or(",
          "133:         GreaterThanOrEqual(attrInt, 8),",
          "134:         LessThanOrEqual(attrInt, 2)),",
          "135:         Some(new V2Or(",
          "136:           new Predicate(\">=\", Array(FieldReference(intColName), LiteralValue(8, IntegerType))),",
          "137:           new Predicate(\"<=\", Array(FieldReference(intColName), LiteralValue(2, IntegerType))))))",
          "139:       testTranslateFilter(Not(GreaterThanOrEqual(attrInt, 8)),",
          "140:         Some(new V2Not(new Predicate(\">=\", Array(FieldReference(intColName),",
          "141:           LiteralValue(8, IntegerType))))))",
          "143:       testTranslateFilter(StartsWith(attrStr, \"a\"),",
          "144:         Some(new Predicate(\"STARTS_WITH\", Array(FieldReference(strColName),",
          "145:           LiteralValue(UTF8String.fromString(\"a\"), StringType)))))",
          "147:       testTranslateFilter(EndsWith(attrStr, \"a\"),",
          "148:         Some(new Predicate(\"ENDS_WITH\", Array(FieldReference(strColName),",
          "149:           LiteralValue(UTF8String.fromString(\"a\"), StringType)))))",
          "151:       testTranslateFilter(Contains(attrStr, \"a\"),",
          "152:         Some(new Predicate(\"CONTAINS\", Array(FieldReference(strColName),",
          "153:           LiteralValue(UTF8String.fromString(\"a\"), StringType)))))",
          "154:     }",
          "155:   }",
          "157:   test(\"translate complex expression\") {",
          "158:     attrInts.foreach { case (attrInt, intColName) =>",
          "161:       testTranslateFilter(LessThanOrEqual(",
          "164:         Subtract(Abs(attrInt), 2), 1), None)",
          "167:       testTranslateFilter(Or(",
          "168:         And(",
          "169:           GreaterThan(attrInt, 1),",
          "170:           LessThan(attrInt, 10)",
          "171:         ),",
          "172:         And(",
          "173:           GreaterThan(attrInt, 50),",
          "174:           LessThan(attrInt, 100))),",
          "175:         Some(new V2Or(",
          "176:           new V2And(",
          "177:             new Predicate(\">\", Array(FieldReference(intColName), LiteralValue(1, IntegerType))),",
          "178:             new Predicate(\"<\", Array(FieldReference(intColName), LiteralValue(10, IntegerType)))),",
          "179:           new V2And(",
          "180:             new Predicate(\">\", Array(FieldReference(intColName), LiteralValue(50, IntegerType))),",
          "181:             new Predicate(\"<\", Array(FieldReference(intColName),",
          "182:               LiteralValue(100, IntegerType)))))",
          "183:         )",
          "184:       )",
          "187:       testTranslateFilter(Or(",
          "188:         And(",
          "189:           GreaterThan(attrInt, 1),",
          "191:           LessThan(Abs(attrInt), 10)",
          "192:         ),",
          "193:         And(",
          "194:           GreaterThan(attrInt, 50),",
          "195:           LessThan(attrInt, 100))), None)",
          "198:       testTranslateFilter(Not(And(",
          "199:         Or(",
          "200:           LessThanOrEqual(attrInt, 1),",
          "202:           GreaterThanOrEqual(Abs(attrInt), 10)",
          "203:         ),",
          "204:         Or(",
          "205:           LessThanOrEqual(attrInt, 50),",
          "206:           GreaterThanOrEqual(attrInt, 100)))), None)",
          "209:       testTranslateFilter(Or(",
          "210:         Or(",
          "211:           EqualTo(attrInt, 1),",
          "212:           EqualTo(attrInt, 10)",
          "213:         ),",
          "214:         Or(",
          "215:           GreaterThan(attrInt, 0),",
          "216:           LessThan(attrInt, -10))),",
          "217:         Some(new V2Or(",
          "218:           new V2Or(",
          "219:             new Predicate(\"=\", Array(FieldReference(intColName), LiteralValue(1, IntegerType))),",
          "220:             new Predicate(\"=\", Array(FieldReference(intColName), LiteralValue(10, IntegerType)))),",
          "221:           new V2Or(",
          "222:             new Predicate(\">\", Array(FieldReference(intColName), LiteralValue(0, IntegerType))),",
          "223:             new Predicate(\"<\", Array(FieldReference(intColName), LiteralValue(-10, IntegerType)))))",
          "224:         )",
          "225:       )",
          "228:       testTranslateFilter(Or(",
          "229:         Or(",
          "230:           EqualTo(attrInt, 1),",
          "232:           EqualTo(Abs(attrInt), 10)",
          "233:         ),",
          "234:         Or(",
          "235:           GreaterThan(attrInt, 0),",
          "236:           LessThan(attrInt, -10))), None)",
          "242:       testTranslateFilter(And(",
          "243:         And(",
          "244:           GreaterThan(attrInt, 1),",
          "245:           LessThan(attrInt, 10)",
          "246:         ),",
          "247:         And(",
          "248:           EqualTo(attrInt, 6),",
          "249:           IsNotNull(attrInt))),",
          "250:         Some(new V2And(",
          "251:           new V2And(",
          "252:             new Predicate(\">\", Array(FieldReference(intColName), LiteralValue(1, IntegerType))),",
          "253:             new Predicate(\"<\", Array(FieldReference(intColName), LiteralValue(10, IntegerType)))),",
          "254:           new V2And(",
          "255:             new Predicate(\"=\", Array(FieldReference(intColName), LiteralValue(6, IntegerType))),",
          "256:             new Predicate(\"IS_NOT_NULL\", Array(FieldReference(intColName)))))",
          "257:         )",
          "258:       )",
          "261:       testTranslateFilter(And(",
          "262:         And(",
          "263:           GreaterThan(attrInt, 1),",
          "264:           LessThan(attrInt, 10)",
          "265:         ),",
          "266:         And(",
          "268:           EqualTo(Abs(attrInt), 6),",
          "269:           IsNotNull(attrInt))), None)",
          "272:       testTranslateFilter(And(",
          "273:         Or(",
          "274:           GreaterThan(attrInt, 1),",
          "275:           LessThan(attrInt, 10)",
          "276:         ),",
          "277:         Or(",
          "278:           EqualTo(attrInt, 6),",
          "279:           IsNotNull(attrInt))),",
          "280:         Some(new V2And(",
          "281:           new V2Or(",
          "282:             new Predicate(\">\", Array(FieldReference(intColName), LiteralValue(1, IntegerType))),",
          "283:             new Predicate(\"<\", Array(FieldReference(intColName), LiteralValue(10, IntegerType)))),",
          "284:           new V2Or(",
          "285:             new Predicate(\"=\", Array(FieldReference(intColName), LiteralValue(6, IntegerType))),",
          "286:             new Predicate(\"IS_NOT_NULL\", Array(FieldReference(intColName)))))",
          "287:         )",
          "288:       )",
          "291:       testTranslateFilter(And(",
          "292:         Or(",
          "293:           GreaterThan(attrInt, 1),",
          "294:           LessThan(attrInt, 10)",
          "295:         ),",
          "296:         Or(",
          "298:           EqualTo(Abs(attrInt), 6),",
          "299:           IsNotNull(attrInt))), None)",
          "300:     }",
          "301:   }",
          "304:     testTranslateFilter($\"col\".boolean,",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e3f6b6d1e15378860b5e30fb4c40168215b16eea",
      "candidate_info": {
        "commit_hash": "e3f6b6d1e15378860b5e30fb4c40168215b16eea",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/e3f6b6d1e15378860b5e30fb4c40168215b16eea",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala"
        ],
        "message": "[SPARK-40212][SQL] SparkSQL castPartValue does not properly handle byte, short, or float\n\nThe `castPartValueToDesiredType` function now returns byte for ByteType and short for ShortType, rather than ints; also floats for FloatType rather than double.\n\nPreviously, attempting to read back in a file partitioned on one of these column types would result in a ClassCastException at runtime (for Byte, `java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Byte`). I can't think this is anything but a bug, as returning the correct data type prevents the crash.\n\nYes: it changes the observed behavior when reading in a byte/short/float-partitioned file.\n\nAdded unit test. Without the `castPartValueToDesiredType` updates, the test fails with the stated exception.\n\n===\nI'll note that I'm not familiar enough with the spark repo to know if this will have ripple effects elsewhere, but tests pass on my fork and since the very similar https://github.com/apache/spark/pull/36344/files only needed to touch these two files I expect this change is self-contained as well.\n\nCloses #37659 from BrennanStein/spark40212.\n\nAuthored-by: Brennan Stein <brennan.stein@ekata.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 146f187342140635b83bfe775b6c327755edfbe1)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "530:     case _ if value == DEFAULT_PARTITION_NAME => null",
          "531:     case NullType => null",
          "532:     case StringType => UTF8String.fromString(unescapePathName(value))",
          "534:     case LongType => JLong.parseLong(value)",
          "536:     case _: DecimalType => Literal(new JBigDecimal(value)).value",
          "537:     case DateType =>",
          "538:       Cast(Literal(value), DateType, Some(zoneId.getId)).eval()",
          "",
          "[Removed Lines]",
          "533:     case ByteType | ShortType | IntegerType => Integer.parseInt(value)",
          "535:     case FloatType | DoubleType => JDouble.parseDouble(value)",
          "",
          "[Added Lines]",
          "533:     case ByteType => Integer.parseInt(value).toByte",
          "534:     case ShortType => Integer.parseInt(value).toShort",
          "535:     case IntegerType => Integer.parseInt(value)",
          "537:     case FloatType => JDouble.parseDouble(value).toFloat",
          "538:     case DoubleType => JDouble.parseDouble(value)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1095:       checkAnswer(readback, Row(0, \"AA\") :: Row(1, \"-0\") :: Nil)",
          "1096:     }",
          "1097:   }",
          "1098: }",
          "1100: class ParquetV1PartitionDiscoverySuite extends ParquetPartitionDiscoverySuite {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1099:   test(\"SPARK-40212: SparkSQL castPartValue does not properly handle byte, short, float\") {",
          "1100:     withTempDir { dir =>",
          "1101:       val data = Seq[(Int, Byte, Short, Float)](",
          "1102:         (1, 2, 3, 4.0f)",
          "1103:       )",
          "1104:       data.toDF(\"a\", \"b\", \"c\", \"d\")",
          "1105:         .write",
          "1106:         .mode(\"overwrite\")",
          "1107:         .partitionBy(\"b\", \"c\", \"d\")",
          "1108:         .parquet(dir.getCanonicalPath)",
          "1109:       val res = spark.read",
          "1110:         .schema(\"a INT, b BYTE, c SHORT, d FLOAT\")",
          "1111:         .parquet(dir.getCanonicalPath)",
          "1112:       checkAnswer(res, Seq(Row(1, 2, 3, 4.0f)))",
          "1113:     }",
          "1114:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}