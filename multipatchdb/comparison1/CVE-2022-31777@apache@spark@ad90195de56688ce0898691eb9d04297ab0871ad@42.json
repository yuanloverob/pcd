{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "aba523c66086a8990d491cf5e9f27aadf39379a0",
      "candidate_info": {
        "commit_hash": "aba523c66086a8990d491cf5e9f27aadf39379a0",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/aba523c66086a8990d491cf5e9f27aadf39379a0",
        "files": [
          "core/src/main/resources/error/error-classes.json",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/HigherOrderFunctionsSuite.scala"
        ],
        "message": "[SPARK-39419][SQL][3.3] Fix ArraySort to throw an exception when the comparator returns null\n\n### What changes were proposed in this pull request?\n\nBackport of #36812.\n\nFixes `ArraySort` to throw an exception when the comparator returns `null`.\n\nAlso updates the doc to follow the corrected behavior.\n\n### Why are the changes needed?\n\nWhen the comparator of `ArraySort` returns `null`, currently\u00a0it handles it as `0` (equal).\n\nAccording to the doc,\n\n```\nIt returns -1, 0, or 1 as the first element is less than, equal to, or greater than\nthe second element. If the comparator function returns other\nvalues (including null), the function will fail and raise an error.\n```\n\nIt's fine to return non -1, 0, 1 integers to follow the Java convention (still need to update the doc, though), but it should throw an exception for `null` result.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, if a user uses a comparator that returns `null`, it will throw an error after this PR.\n\nThe legacy flag `spark.sql.legacy.allowNullComparisonResultInArraySort` can be used to restore the legacy behavior that handles `null` as `0` (equal).\n\n### How was this patch tested?\n\nAdded some tests.\n\nCloses #36834 from ueshin/issues/SPARK-39419/3.3/array_sort.\n\nAuthored-by: Takuya UESHIN <ueshin@databricks.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/HigherOrderFunctionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/HigherOrderFunctionsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "357:     Since 3.0.0 this function also sorts and returns the array based on the",
          "358:     given comparator function. The comparator will take two arguments representing",
          "359:     two elements of the array.",
          "363:     \"\"\",",
          "364:   examples = \"\"\"",
          "365:     Examples:",
          "",
          "[Removed Lines]",
          "360:     It returns -1, 0, or 1 as the first element is less than, equal to, or greater",
          "361:     than the second element. If the comparator function returns other",
          "362:     values (including null), the function will fail and raise an error.",
          "",
          "[Added Lines]",
          "360:     It returns a negative integer, 0, or a positive integer as the first element is less than,",
          "361:     equal to, or greater than the second element. If the comparator function returns null,",
          "362:     the function will fail and raise an error.",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "376: case class ArraySort(",
          "377:     argument: Expression,",
          "379:   extends ArrayBasedSimpleHigherOrderFunction with CodegenFallback {",
          "381:   def this(argument: Expression) = this(argument, ArraySort.defaultComparator)",
          "383:   @transient lazy val elementType: DataType =",
          "",
          "[Removed Lines]",
          "378:     function: Expression)",
          "",
          "[Added Lines]",
          "378:     function: Expression,",
          "379:     allowNullComparisonResult: Boolean)",
          "382:   def this(argument: Expression, function: Expression) = {",
          "383:     this(",
          "384:       argument,",
          "385:       function,",
          "386:       SQLConf.get.getConf(SQLConf.LEGACY_ALLOW_NULL_COMPARISON_RESULT_IN_ARRAY_SORT))",
          "387:   }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "416:     (o1: Any, o2: Any) => {",
          "417:       firstElemVar.value.set(o1)",
          "418:       secondElemVar.value.set(o2)",
          "420:     }",
          "421:   }",
          "",
          "[Removed Lines]",
          "419:       f.eval(inputRow).asInstanceOf[Int]",
          "",
          "[Added Lines]",
          "427:       val cmp = f.eval(inputRow)",
          "428:       if (!allowNullComparisonResult && cmp == null) {",
          "429:         throw QueryExecutionErrors.nullComparisonResultError()",
          "430:       }",
          "431:       cmp.asInstanceOf[Int]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "438: object ArraySort {",
          "440:   def comparator(left: Expression, right: Expression): Expression = {",
          "441:     val lit0 = Literal(0)",
          "442:     val lit1 = Literal(1)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "452:   def apply(argument: Expression, function: Expression): ArraySort = {",
          "453:     new ArraySort(argument, function)",
          "454:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2065:         s\"add ${toSQLValue(amount, IntegerType)} $unit to \" +",
          "2066:         s\"${toSQLValue(DateTimeUtils.microsToInstant(micros), TimestampType)}\"))",
          "2067:   }",
          "2068: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2069:   def nullComparisonResultError(): Throwable = {",
          "2070:     new SparkException(errorClass = \"NULL_COMPARISON_RESULT\",",
          "2071:       messageParameters = Array(), cause = null)",
          "2072:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "3748:       .booleanConf",
          "3749:       .createWithDefault(false)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3751:   val LEGACY_ALLOW_NULL_COMPARISON_RESULT_IN_ARRAY_SORT =",
          "3752:     buildConf(\"spark.sql.legacy.allowNullComparisonResultInArraySort\")",
          "3753:       .internal()",
          "3754:       .doc(\"When set to false, `array_sort` function throws an error \" +",
          "3755:         \"if the comparator function returns null. \" +",
          "3756:         \"If set to true, it restores the legacy behavior that handles null as zero (equal).\")",
          "3757:       .version(\"3.2.2\")",
          "3758:       .booleanConf",
          "3759:       .createWithDefault(false)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/HigherOrderFunctionsSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/HigherOrderFunctionsSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/HigherOrderFunctionsSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/HigherOrderFunctionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: package org.apache.spark.sql.catalyst.expressions",
          "21: import org.apache.spark.sql.catalyst.analysis.TypeCheckResult",
          "22: import org.apache.spark.sql.internal.SQLConf",
          "23: import org.apache.spark.sql.types._",
          "",
          "[Removed Lines]",
          "20: import org.apache.spark.SparkFunSuite",
          "",
          "[Added Lines]",
          "20: import org.apache.spark.{SparkException, SparkFunSuite}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "838:       Literal.create(Seq(Double.NaN, 1d, 2d, null), ArrayType(DoubleType))),",
          "839:       Seq(1d, 2d, Double.NaN, null))",
          "840:   }",
          "841: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "842:   test(\"SPARK-39419: ArraySort should throw an exception when the comparator returns null\") {",
          "843:     val comparator = {",
          "844:       val comp = ArraySort.comparator _",
          "845:       (left: Expression, right: Expression) =>",
          "846:         If(comp(left, right) === 0, Literal.create(null, IntegerType), comp(left, right))",
          "847:     }",
          "849:     withSQLConf(",
          "850:         SQLConf.LEGACY_ALLOW_NULL_COMPARISON_RESULT_IN_ARRAY_SORT.key -> \"false\") {",
          "851:       checkExceptionInExpression[SparkException](",
          "852:         arraySort(Literal.create(Seq(3, 1, 1, 2)), comparator), \"The comparison result is null\")",
          "853:     }",
          "855:     withSQLConf(",
          "856:         SQLConf.LEGACY_ALLOW_NULL_COMPARISON_RESULT_IN_ARRAY_SORT.key -> \"true\") {",
          "857:       checkEvaluation(arraySort(Literal.create(Seq(3, 1, 1, 2)), comparator),",
          "858:         Seq(1, 1, 2, 3))",
          "859:     }",
          "860:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "b9a514ea0519e2da21efe2201c7f888be2640458",
      "candidate_info": {
        "commit_hash": "b9a514ea0519e2da21efe2201c7f888be2640458",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/b9a514ea0519e2da21efe2201c7f888be2640458",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala"
        ],
        "message": "[SPARK-40470][SQL] Handle GetArrayStructFields and GetMapValue in \"arrays_zip\" function\n\n### What changes were proposed in this pull request?\n\nThis is a follow-up for https://github.com/apache/spark/pull/37833.\n\nThe PR fixes column names in `arrays_zip` function for the cases when `GetArrayStructFields` and `GetMapValue` expressions are used (see unit tests for more details).\n\nBefore the patch, the column names would be indexes or an AnalysisException would be thrown in the case of `GetArrayStructFields` example.\n\n### Why are the changes needed?\n\nFixes an inconsistency issue in Spark 3.2 and onwards where the fields would be labeled as indexes instead of column names.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nI added unit tests that reproduce the issue and confirmed that the patch fixes them.\n\nCloses #37911 from sadikovi/SPARK-40470.\n\nAuthored-by: Ivan Sadikov <ivan.sadikov@databricks.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 9b0f979141ba2c4124d96bc5da69ea5cac51df0d)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "267:         case (u: UnresolvedAttribute, _) => Literal(u.nameParts.last)",
          "268:         case (e: NamedExpression, _) if e.resolved => Literal(e.name)",
          "269:         case (e: NamedExpression, _) => NamePlaceholder",
          "271:         case (_, idx) => Literal(idx.toString)",
          "272:       })",
          "273:   }",
          "",
          "[Removed Lines]",
          "270:         case (e: GetStructField, _) => Literal(e.extractFieldName)",
          "",
          "[Added Lines]",
          "270:         case (g: GetStructField, _) => Literal(g.extractFieldName)",
          "271:         case (g: GetArrayStructFields, _) => Literal(g.field.name)",
          "272:         case (g: GetMapValue, _) => Literal(g.key)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "740:     assert(fieldNames.toSeq === Seq(\"arr_1\", \"arr_2\", \"arr_3\"))",
          "741:   }",
          "743:   def testSizeOfMap(sizeOfNull: Any): Unit = {",
          "744:     val df = Seq(",
          "745:       (Map[Int, Int](1 -> 1, 2 -> 2), \"x\"),",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "743:   test(\"SPARK-40470: array_zip should return field names in GetArrayStructFields\") {",
          "744:     val df = spark.read.json(Seq(",
          "745:       \"\"\"",
          "746:       {",
          "747:         \"arr\": [",
          "748:           {",
          "749:             \"obj\": {",
          "750:               \"nested\": {",
          "751:                 \"field1\": [1],",
          "752:                 \"field2\": [2]",
          "753:               }",
          "754:             }",
          "755:           }",
          "756:         ]",
          "757:       }",
          "758:       \"\"\").toDS())",
          "760:     val res = df",
          "761:       .selectExpr(\"arrays_zip(arr.obj.nested.field1, arr.obj.nested.field2) as arr\")",
          "762:       .select(col(\"arr.field1\"), col(\"arr.field2\"))",
          "764:     val fieldNames = res.schema.fieldNames",
          "765:     assert(fieldNames.toSeq === Seq(\"field1\", \"field2\"))",
          "767:     checkAnswer(res, Row(Seq(Seq(1)), Seq(Seq(2))) :: Nil)",
          "768:   }",
          "770:   test(\"SPARK-40470: arrays_zip should return field names in GetMapValue\") {",
          "771:     val df = spark.sql(\"\"\"",
          "772:       select",
          "773:         map(",
          "774:           'arr_1', array(1, 2),",
          "775:           'arr_2', array(3, 4)",
          "776:         ) as map_obj",
          "777:       \"\"\")",
          "779:     val res = df.selectExpr(\"arrays_zip(map_obj.arr_1, map_obj.arr_2) as arr\")",
          "781:     val fieldNames = res.schema.head.dataType.asInstanceOf[ArrayType]",
          "782:       .elementType.asInstanceOf[StructType].fieldNames",
          "783:     assert(fieldNames.toSeq === Seq(\"arr_1\", \"arr_2\"))",
          "785:     checkAnswer(res, Row(Seq(Row(1, 3), Row(2, 4))))",
          "786:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "72eb58ae224efb0f5bd3912073ff133116c0d05e",
      "candidate_info": {
        "commit_hash": "72eb58ae224efb0f5bd3912073ff133116c0d05e",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/72eb58ae224efb0f5bd3912073ff133116c0d05e",
        "files": [
          "python/pyspark/sql/utils.py",
          "sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala"
        ],
        "message": "[SPARK-39215][PYTHON] Reduce Py4J calls in pyspark.sql.utils.is_timestamp_ntz_preferred\n\n### What changes were proposed in this pull request?\n\nThis PR proposes to reduce the number of Py4J calls at `pyspark.sql.utils.is_timestamp_ntz_preferred` by having a single method to check.\n\n### Why are the changes needed?\n\nFor better performance, and simplicity in the code.\n\n### Does this PR introduce _any_ user-facing change?\n\nYes, the number of Py4J calls will be reduced, and the driver side access will become faster.\n\n### How was this patch tested?\n\nExisting tests should cover.\n\nCloses #36587 from HyukjinKwon/SPARK-39215.\n\nLead-authored-by: Hyukjin Kwon <gurwls223@apache.org>\nCo-authored-by: Hyukjin Kwon <gurwls223@gmail.com>\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>\n(cherry picked from commit 28e7764bbe6949b2a68ef1466e210ca6418a3018)\nSigned-off-by: Hyukjin Kwon <gurwls223@apache.org>",
        "before_after_code_files": [
          "python/pyspark/sql/utils.py||python/pyspark/sql/utils.py",
          "sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/sql/utils.py||python/pyspark/sql/utils.py": [
          "File: python/pyspark/sql/utils.py -> python/pyspark/sql/utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "293:     Return a bool if TimestampNTZType is preferred according to the SQL configuration set.",
          "294:     \"\"\"",
          "295:     jvm = SparkContext._jvm",
          "",
          "[Removed Lines]",
          "296:     return (",
          "297:         jvm is not None",
          "298:         and getattr(getattr(jvm.org.apache.spark.sql.internal, \"SQLConf$\"), \"MODULE$\")",
          "299:         .get()",
          "300:         .timestampType()",
          "301:         .typeName()",
          "302:         == \"timestamp_ntz\"",
          "303:     )",
          "",
          "[Added Lines]",
          "296:     return jvm is not None and jvm.PythonSQLUtils.isTimestampNTZPreferred()",
          "",
          "---------------"
        ],
        "sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala -> sql/core/src/main/scala/org/apache/spark/sql/api/python/PythonSQLUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "55:     listAllSQLConfigs().filter(p => SQLConf.isStaticConfigKey(p._1)).toArray",
          "56:   }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "58:   def isTimestampNTZPreferred: Boolean =",
          "59:     SQLConf.get.timestampType == org.apache.spark.sql.types.TimestampNTZType",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5c3ef79544814473e49d356f36cf100aca9afe57",
      "candidate_info": {
        "commit_hash": "5c3ef79544814473e49d356f36cf100aca9afe57",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/5c3ef79544814473e49d356f36cf100aca9afe57",
        "files": [
          "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala"
        ],
        "message": "[SPARK-37960][SQL][FOLLOWUP] Make the testing CASE WHEN query more reasonable\n\n### What changes were proposed in this pull request?\nSome testing CASE WHEN queries are not carefully written and do not make sense. In the future, the optimizer may get smarter and get rid of the CASE WHEN completely, and then we loose test coverage.\n\nThis PR updates some CASE WHEN queries to make them more reasonable.\n\n### Why are the changes needed?\nfuture-proof test coverage.\n\n### Does this PR introduce _any_ user-facing change?\n'No'.\n\n### How was this patch tested?\nN/A\n\nCloses #36125 from beliefer/SPARK-37960_followup3.\n\nAuthored-by: Jiaan Geng <beliefer@163.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 4e118383f58d23d5515ce6b00d3935e3ac51fb03)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala -> sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCV2Suite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "889:         |  COUNT(CASE WHEN SALARY >= 12000 OR SALARY < 9000 THEN SALARY ELSE 0 END),",
          "890:         |  COUNT(CASE WHEN SALARY >= 12000 OR NOT(SALARY >= 9000) THEN SALARY ELSE 0 END),",
          "891:         |  MAX(CASE WHEN NOT(SALARY > 10000) AND SALARY >= 8000 THEN SALARY ELSE 0 END),",
          "893:         |  MAX(CASE WHEN NOT(SALARY > 10000) AND NOT(SALARY < 8000) THEN SALARY ELSE 0 END),",
          "894:         |  MAX(CASE WHEN NOT(SALARY != 0) OR NOT(SALARY < 8000) THEN SALARY ELSE 0 END),",
          "896:         |  MIN(CASE WHEN NOT(SALARY > 8000 OR SALARY IS NULL) THEN SALARY ELSE 0 END),",
          "897:         |  SUM(CASE WHEN SALARY > 10000 THEN 2 WHEN SALARY > 8000 THEN 1 END),",
          "898:         |  AVG(CASE WHEN NOT(SALARY > 8000 OR SALARY IS NOT NULL) THEN SALARY ELSE 0 END)",
          "",
          "[Removed Lines]",
          "892:         |  MAX(CASE WHEN NOT(SALARY > 10000) OR SALARY > 8000 THEN SALARY ELSE 0 END),",
          "895:         |  MAX(CASE WHEN NOT(SALARY > 8000 AND SALARY > 8000) THEN 0 ELSE SALARY END),",
          "",
          "[Added Lines]",
          "892:         |  MAX(CASE WHEN NOT(SALARY > 9000) OR SALARY > 10000 THEN SALARY ELSE 0 END),",
          "895:         |  MAX(CASE WHEN NOT(SALARY > 8000 AND SALARY < 10000) THEN 0 ELSE SALARY END),",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "904:       \" THEN SALARY ELSE 0.00 END), COUNT(CAS..., \" +",
          "905:       \"PushedFilters: [], \" +",
          "906:       \"PushedGroupByColumns: [DEPT], \")",
          "910:   }",
          "912:   test(\"scan with aggregate push-down: aggregate function with binary arithmetic\") {",
          "",
          "[Removed Lines]",
          "907:     checkAnswer(df, Seq(Row(1, 1, 1, 1, 1, 0d, 12000d, 0d, 12000d, 12000d, 0d, 2, 0d),",
          "908:       Row(2, 2, 2, 2, 2, 10000d, 10000d, 10000d, 10000d, 10000d, 0d, 2, 0d),",
          "909:       Row(2, 2, 2, 2, 2, 10000d, 12000d, 10000d, 12000d, 12000d, 0d, 3, 0d)))",
          "",
          "[Added Lines]",
          "907:     checkAnswer(df, Seq(Row(1, 1, 1, 1, 1, 0d, 12000d, 0d, 12000d, 0d, 0d, 2, 0d),",
          "908:       Row(2, 2, 2, 2, 2, 10000d, 12000d, 10000d, 12000d, 0d, 0d, 3, 0d),",
          "909:       Row(2, 2, 2, 2, 2, 10000d, 9000d, 10000d, 10000d, 9000d, 0d, 2, 0d)))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c358ee6761539b4a4d12dbe36a4dd1a632a0efeb",
      "candidate_info": {
        "commit_hash": "c358ee6761539b4a4d12dbe36a4dd1a632a0efeb",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/c358ee6761539b4a4d12dbe36a4dd1a632a0efeb",
        "files": [
          "core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala",
          "core/src/test/scala/org/apache/spark/serializer/GenericAvroSerializerSuite.scala",
          "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDataToCatalyst.scala",
          "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroOptions.scala",
          "external/avro/src/main/scala/org/apache/spark/sql/avro/CatalystDataToAvro.scala",
          "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroFunctionsSuite.scala"
        ],
        "message": "[SPARK-39775][CORE][AVRO] Disable validate default values when parsing Avro schemas\n\n### What changes were proposed in this pull request?\n\nThis PR disables validate default values when parsing Avro schemas.\n\n### Why are the changes needed?\n\nSpark will throw exception if upgrade to Spark 3.2. We have fixed the Hive serde tables before: SPARK-34512.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nUnit test.\n\nCloses #37191 from wangyum/SPARK-39775.\n\nAuthored-by: Yuming Wang <yumwang@ebay.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 5c1b99f441ec5e178290637a9a9e7902aaa116e1)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala||core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala",
          "core/src/test/scala/org/apache/spark/serializer/GenericAvroSerializerSuite.scala||core/src/test/scala/org/apache/spark/serializer/GenericAvroSerializerSuite.scala",
          "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDataToCatalyst.scala||external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDataToCatalyst.scala",
          "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroOptions.scala||external/avro/src/main/scala/org/apache/spark/sql/avro/AvroOptions.scala",
          "external/avro/src/main/scala/org/apache/spark/sql/avro/CatalystDataToAvro.scala||external/avro/src/main/scala/org/apache/spark/sql/avro/CatalystDataToAvro.scala",
          "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroFunctionsSuite.scala||external/avro/src/test/scala/org/apache/spark/sql/avro/AvroFunctionsSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala||core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala": [
          "File: core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala -> core/src/main/scala/org/apache/spark/serializer/GenericAvroSerializer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "97:     } {",
          "98:       in.close()",
          "99:     }",
          "101:   })",
          "",
          "[Removed Lines]",
          "100:     new Schema.Parser().parse(new String(bytes, StandardCharsets.UTF_8))",
          "",
          "[Added Lines]",
          "100:     new Schema.Parser().setValidateDefaults(false).parse(new String(bytes, StandardCharsets.UTF_8))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "137:         val fingerprint = input.readLong()",
          "138:         schemaCache.getOrElseUpdate(fingerprint, {",
          "139:           schemas.get(fingerprint) match {",
          "141:             case None =>",
          "142:               throw new SparkException(",
          "143:                 \"Error reading attempting to read avro data -- encountered an unknown \" +",
          "",
          "[Removed Lines]",
          "140:             case Some(s) => new Schema.Parser().parse(s)",
          "",
          "[Added Lines]",
          "140:             case Some(s) => new Schema.Parser().setValidateDefaults(false).parse(s)",
          "",
          "---------------"
        ],
        "core/src/test/scala/org/apache/spark/serializer/GenericAvroSerializerSuite.scala||core/src/test/scala/org/apache/spark/serializer/GenericAvroSerializerSuite.scala": [
          "File: core/src/test/scala/org/apache/spark/serializer/GenericAvroSerializerSuite.scala -> core/src/test/scala/org/apache/spark/serializer/GenericAvroSerializerSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "110:       assert(rdd.collect() sameElements Array.fill(10)(datum))",
          "111:     }",
          "112:   }",
          "113: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "114:   test(\"SPARK-39775: Disable validate default values when parsing Avro schemas\") {",
          "115:     val avroTypeStruct = s\"\"\"",
          "116:       |{",
          "117:       |  \"type\": \"record\",",
          "118:       |  \"name\": \"struct\",",
          "119:       |  \"fields\": [",
          "120:       |    {\"name\": \"id\", \"type\": \"long\", \"default\": null}",
          "121:       |  ]",
          "122:       |}",
          "123:     \"\"\".stripMargin",
          "124:     val schema = new Schema.Parser().setValidateDefaults(false).parse(avroTypeStruct)",
          "126:     val genericSer = new GenericAvroSerializer(conf.getAvroSchema)",
          "127:     assert(schema === genericSer.decompress(ByteBuffer.wrap(genericSer.compress(schema))))",
          "128:   }",
          "",
          "---------------"
        ],
        "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDataToCatalyst.scala||external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDataToCatalyst.scala": [
          "File: external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDataToCatalyst.scala -> external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDataToCatalyst.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "54:   private lazy val avroOptions = AvroOptions(options)",
          "58:   @transient private lazy val expectedSchema = avroOptions.schema.getOrElse(actualSchema)",
          "",
          "[Removed Lines]",
          "56:   @transient private lazy val actualSchema = new Schema.Parser().parse(jsonFormatSchema)",
          "",
          "[Added Lines]",
          "56:   @transient private lazy val actualSchema =",
          "57:     new Schema.Parser().setValidateDefaults(false).parse(jsonFormatSchema)",
          "",
          "---------------"
        ],
        "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroOptions.scala||external/avro/src/main/scala/org/apache/spark/sql/avro/AvroOptions.scala": [
          "File: external/avro/src/main/scala/org/apache/spark/sql/avro/AvroOptions.scala -> external/avro/src/main/scala/org/apache/spark/sql/avro/AvroOptions.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "54:   val schema: Option[Schema] = {",
          "56:       val avroUrlSchema = parameters.get(\"avroSchemaUrl\").map(url => {",
          "57:         log.debug(\"loading avro schema from url: \" + url)",
          "58:         val fs = FileSystem.get(new URI(url), conf)",
          "59:         val in = fs.open(new Path(url))",
          "60:         try {",
          "62:         } finally {",
          "63:           in.close()",
          "64:         }",
          "",
          "[Removed Lines]",
          "55:     parameters.get(\"avroSchema\").map(new Schema.Parser().parse).orElse({",
          "61:           new Schema.Parser().parse(in)",
          "",
          "[Added Lines]",
          "55:     parameters.get(\"avroSchema\").map(new Schema.Parser().setValidateDefaults(false).parse).orElse({",
          "61:           new Schema.Parser().setValidateDefaults(false).parse(in)",
          "",
          "---------------"
        ],
        "external/avro/src/main/scala/org/apache/spark/sql/avro/CatalystDataToAvro.scala||external/avro/src/main/scala/org/apache/spark/sql/avro/CatalystDataToAvro.scala": [
          "File: external/avro/src/main/scala/org/apache/spark/sql/avro/CatalystDataToAvro.scala -> external/avro/src/main/scala/org/apache/spark/sql/avro/CatalystDataToAvro.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "36:   @transient private lazy val avroType =",
          "37:     jsonFormatSchema",
          "39:       .getOrElse(SchemaConverters.toAvroType(child.dataType, child.nullable))",
          "41:   @transient private lazy val serializer =",
          "",
          "[Removed Lines]",
          "38:       .map(new Schema.Parser().parse)",
          "",
          "[Added Lines]",
          "38:       .map(new Schema.Parser().setValidateDefaults(false).parse)",
          "",
          "---------------"
        ],
        "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroFunctionsSuite.scala||external/avro/src/test/scala/org/apache/spark/sql/avro/AvroFunctionsSuite.scala": [
          "File: external/avro/src/test/scala/org/apache/spark/sql/avro/AvroFunctionsSuite.scala -> external/avro/src/test/scala/org/apache/spark/sql/avro/AvroFunctionsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: import org.apache.spark.sql.functions.{col, lit, struct}",
          "32: import org.apache.spark.sql.internal.SQLConf",
          "33: import org.apache.spark.sql.test.SharedSparkSession",
          "35: class AvroFunctionsSuite extends QueryTest with SharedSparkSession {",
          "36:   import testImplicits._",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "34: import org.apache.spark.sql.types.StructType",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "238:       assert(message.contains(\"Only UNION of a null type and a non-null type is supported\"))",
          "239:     }",
          "240:   }",
          "241: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "243:   test(\"SPARK-39775: Disable validate default values when parsing Avro schemas\") {",
          "244:     val avroTypeStruct = s\"\"\"",
          "245:       |{",
          "246:       |  \"type\": \"record\",",
          "247:       |  \"name\": \"struct\",",
          "248:       |  \"fields\": [",
          "249:       |    {\"name\": \"id\", \"type\": \"long\", \"default\": null}",
          "250:       |  ]",
          "251:       |}",
          "252:     \"\"\".stripMargin",
          "253:     val avroSchema = AvroOptions(Map(\"avroSchema\" -> avroTypeStruct)).schema.get",
          "254:     val sparkSchema = SchemaConverters.toSqlType(avroSchema).dataType.asInstanceOf[StructType]",
          "256:     val df = spark.range(5).select($\"id\")",
          "257:     val structDf = df.select(struct($\"id\").as(\"struct\"))",
          "258:     val avroStructDF = structDf.select(functions.to_avro('struct, avroTypeStruct).as(\"avro\"))",
          "259:     checkAnswer(avroStructDF.select(functions.from_avro('avro, avroTypeStruct)), structDf)",
          "261:     withTempPath { dir =>",
          "262:       df.write.format(\"avro\").save(dir.getCanonicalPath)",
          "263:       checkAnswer(spark.read.schema(sparkSchema).format(\"avro\").load(dir.getCanonicalPath), df)",
          "265:       val msg = intercept[SparkException] {",
          "266:         spark.read.option(\"avroSchema\", avroTypeStruct).format(\"avro\")",
          "267:           .load(dir.getCanonicalPath)",
          "268:           .collect()",
          "269:       }.getCause.getMessage",
          "270:       assert(msg.contains(\"Invalid default for field id: null not a \\\"long\\\"\"))",
          "271:     }",
          "272:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}