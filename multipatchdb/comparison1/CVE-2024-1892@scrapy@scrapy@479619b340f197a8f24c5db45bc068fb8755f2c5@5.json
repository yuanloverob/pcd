{
  "cve_id": "CVE-2024-1892",
  "cve_desc": "A Regular Expression Denial of Service (ReDoS) vulnerability exists in the XMLFeedSpider class of the scrapy/scrapy project, specifically in the parsing of XML content. By crafting malicious XML content that exploits inefficient regular expression complexity used in the parsing process, an attacker can cause a denial-of-service (DoS) condition. This vulnerability allows for the system to hang and consume significant resources, potentially rendering services that utilize Scrapy for XML processing unresponsive.",
  "repo": "scrapy/scrapy",
  "patch_hash": "479619b340f197a8f24c5db45bc068fb8755f2c5",
  "patch_info": {
    "commit_hash": "479619b340f197a8f24c5db45bc068fb8755f2c5",
    "repo": "scrapy/scrapy",
    "commit_url": "https://github.com/scrapy/scrapy/commit/479619b340f197a8f24c5db45bc068fb8755f2c5",
    "files": [
      "docs/faq.rst",
      "docs/news.rst",
      "docs/topics/debug.rst",
      "scrapy/spiders/feed.py",
      "scrapy/utils/iterators.py",
      "scrapy/utils/response.py",
      "tests/test_spider.py",
      "tests/test_utils_iterators.py",
      "tests/test_utils_response.py"
    ],
    "message": "Merge branch '2.11-redos' into 2.11",
    "before_after_code_files": [
      "scrapy/spiders/feed.py||scrapy/spiders/feed.py",
      "scrapy/utils/iterators.py||scrapy/utils/iterators.py",
      "scrapy/utils/response.py||scrapy/utils/response.py",
      "tests/test_spider.py||tests/test_spider.py",
      "tests/test_utils_iterators.py||tests/test_utils_iterators.py",
      "tests/test_utils_response.py||tests/test_utils_response.py"
    ]
  },
  "patch_diff": {
    "scrapy/spiders/feed.py||scrapy/spiders/feed.py": [
      "File: scrapy/spiders/feed.py -> scrapy/spiders/feed.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "7: from scrapy.exceptions import NotConfigured, NotSupported",
      "8: from scrapy.selector import Selector",
      "9: from scrapy.spiders import Spider",
      "11: from scrapy.utils.spider import iterate_spider_output",
      "",
      "[Removed Lines]",
      "10: from scrapy.utils.iterators import csviter, xmliter",
      "",
      "[Added Lines]",
      "10: from scrapy.utils.iterators import csviter, xmliter_lxml",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "84:         return self.parse_nodes(response, nodes)",
      "86:     def _iternodes(self, response):",
      "88:             self._register_namespaces(node)",
      "89:             yield node",
      "",
      "[Removed Lines]",
      "87:         for node in xmliter(response, self.itertag):",
      "",
      "[Added Lines]",
      "87:         for node in xmliter_lxml(response, self.itertag):",
      "",
      "---------------"
    ],
    "scrapy/utils/iterators.py||scrapy/utils/iterators.py": [
      "File: scrapy/utils/iterators.py -> scrapy/utils/iterators.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "16:     cast,",
      "17:     overload,",
      "18: )",
      "20: from scrapy.http import Response, TextResponse",
      "21: from scrapy.selector import Selector",
      "22: from scrapy.utils.python import re_rsearch, to_unicode",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "19: from warnings import warn",
      "21: from lxml import etree",
      "23: from scrapy.exceptions import ScrapyDeprecationWarning",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "38:     - a unicode string",
      "39:     - a string encoded as utf-8",
      "40:     \"\"\"",
      "41:     nodename_patt = re.escape(nodename)",
      "43:     DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]+>\\s*\", re.S)",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "45:     warn(",
      "46:         (",
      "47:             \"xmliter is deprecated and its use strongly discouraged because \"",
      "48:             \"it is vulnerable to ReDoS attacks. Use xmliter_lxml instead. See \"",
      "49:             \"https://github.com/scrapy/scrapy/security/advisories/GHSA-cc65-xxvf-f7r9\"",
      "50:         ),",
      "51:         ScrapyDeprecationWarning,",
      "52:         stacklevel=2,",
      "53:     )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "81:     namespace: Optional[str] = None,",
      "82:     prefix: str = \"x\",",
      "83: ) -> Generator[Selector, Any, None]:",
      "86:     reader = _StreamReader(obj)",
      "87:     tag = f\"{{{namespace}}}{nodename}\" if namespace else nodename",
      "88:     iterable = etree.iterparse(",
      "90:     )",
      "91:     selxpath = \"//\" + (f\"{prefix}:{nodename}\" if namespace else nodename)",
      "93:         nodetext = etree.tostring(node, encoding=\"unicode\")",
      "94:         node.clear()",
      "95:         xs = Selector(text=nodetext, type=\"xml\")",
      "",
      "[Removed Lines]",
      "84:     from lxml import etree",
      "89:         cast(\"SupportsReadClose[bytes]\", reader), tag=tag, encoding=reader.encoding",
      "92:     for _, node in iterable:",
      "",
      "[Added Lines]",
      "101:         cast(\"SupportsReadClose[bytes]\", reader),",
      "102:         encoding=reader.encoding,",
      "103:         events=(\"end\", \"start-ns\"),",
      "104:         huge_tree=True,",
      "107:     needs_namespace_resolution = not namespace and \":\" in nodename",
      "108:     if needs_namespace_resolution:",
      "109:         prefix, nodename = nodename.split(\":\", maxsplit=1)",
      "110:     for event, data in iterable:",
      "111:         if event == \"start-ns\":",
      "112:             assert isinstance(data, tuple)",
      "113:             if needs_namespace_resolution:",
      "114:                 _prefix, _namespace = data",
      "115:                 if _prefix != prefix:",
      "116:                     continue",
      "117:                 namespace = _namespace",
      "118:                 needs_namespace_resolution = False",
      "119:                 selxpath = f\"//{prefix}:{nodename}\"",
      "120:                 tag = f\"{{{namespace}}}{nodename}\"",
      "121:             continue",
      "122:         assert isinstance(data, etree._Element)",
      "123:         node = data",
      "124:         if node.tag != tag:",
      "125:             continue",
      "",
      "---------------"
    ],
    "scrapy/utils/response.py||scrapy/utils/response.py": [
      "File: scrapy/utils/response.py -> scrapy/utils/response.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "74:     return b\"\".join(values)",
      "77: def open_in_browser(",
      "78:     response: Union[",
      "79:         \"scrapy.http.response.html.HtmlResponse\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "77: def _remove_html_comments(body):",
      "78:     start = body.find(b\"<!--\")",
      "79:     while start != -1:",
      "80:         end = body.find(b\"-->\", start + 1)",
      "81:         if end == -1:",
      "82:             return body[:start]",
      "83:         else:",
      "84:             body = body[:start] + body[end + 3 :]",
      "85:             start = body.find(b\"<!--\")",
      "86:     return body",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "81:     ],",
      "82:     _openfunc: Callable[[str], Any] = webbrowser.open,",
      "83: ) -> Any:",
      "86:     \"\"\"",
      "87:     from scrapy.http import HtmlResponse, TextResponse",
      "",
      "[Removed Lines]",
      "84:     \"\"\"Open the given response in a local web browser, populating the <base>",
      "85:     tag for external links to work",
      "",
      "[Added Lines]",
      "96:     \"\"\"Open *response* in a local web browser, adjusting the `base tag`_ for",
      "97:     external links to work, e.g. so that images and styles are displayed.",
      "99:     .. _base tag: https://www.w3schools.com/tags/tag_base.asp",
      "101:     For example:",
      "103:     .. code-block:: python",
      "105:         from scrapy.utils.response import open_in_browser",
      "108:         def parse_details(self, response):",
      "109:             if \"item name\" not in response.body:",
      "110:                 open_in_browser(response)",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "90:     body = response.body",
      "91:     if isinstance(response, HtmlResponse):",
      "92:         if b\"<base\" not in body:",
      "96:         ext = \".html\"",
      "97:     elif isinstance(response, TextResponse):",
      "98:         ext = \".txt\"",
      "",
      "[Removed Lines]",
      "93:             repl = rf'\\1<base href=\"{response.url}\">'",
      "94:             body = re.sub(b\"<!--.*?-->\", b\"\", body, flags=re.DOTALL)",
      "95:             body = re.sub(rb\"(<head(?:>|\\s.*?>))\", to_bytes(repl), body)",
      "",
      "[Added Lines]",
      "118:             _remove_html_comments(body)",
      "119:             repl = rf'\\0<base href=\"{response.url}\">'",
      "120:             body = re.sub(rb\"<head(?:[^<>]*?>)\", to_bytes(repl), body, count=1)",
      "",
      "---------------"
    ],
    "tests/test_spider.py||tests/test_spider.py": [
      "File: tests/test_spider.py -> tests/test_spider.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "151:         body = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>",
      "152:         <urlset xmlns:x=\"http://www.google.com/schemas/sitemap/0.84\"",
      "153:                 xmlns:y=\"http://www.example.com/schemas/extras/1.0\">",
      "155:             <other value=\"bar\" y:custom=\"fuu\"/>",
      "156:         </url>",
      "158:         </urlset>\"\"\"",
      "159:         response = XmlResponse(url=\"http://example.com/sitemap.xml\", body=body)",
      "",
      "[Removed Lines]",
      "154:         <url><x:loc>http://www.example.com/Special-Offers.html</loc><y:updated>2009-08-16</updated>",
      "157:         <url><loc>http://www.example.com/</loc><y:updated>2009-08-16</updated><other value=\"foo\"/></url>",
      "",
      "[Added Lines]",
      "154:         <url><x:loc>http://www.example.com/Special-Offers.html</x:loc><y:updated>2009-08-16</y:updated>",
      "157:         <url><loc>http://www.example.com/</loc><y:updated>2009-08-16</y:updated><other value=\"foo\"/></url>",
      "",
      "---------------"
    ],
    "tests/test_utils_iterators.py||tests/test_utils_iterators.py": [
      "File: tests/test_utils_iterators.py -> tests/test_utils_iterators.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "2: from twisted.trial import unittest",
      "4: from scrapy.http import Response, TextResponse, XmlResponse",
      "5: from scrapy.utils.iterators import _body_or_str, csviter, xmliter, xmliter_lxml",
      "6: from tests import get_testdata",
      "12:     def test_xmliter(self):",
      "13:         body = b\"\"\"",
      "14:             <?xml version=\"1.0\" encoding=\"UTF-8\"?>",
      "",
      "[Removed Lines]",
      "1: from pytest import mark",
      "9: class XmliterTestCase(unittest.TestCase):",
      "10:     xmliter = staticmethod(xmliter)",
      "",
      "[Added Lines]",
      "1: import pytest",
      "4: from scrapy.exceptions import ScrapyDeprecationWarning",
      "10: class XmliterBaseTestCase:",
      "11:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "40:             attrs, [(\"001\", [\"Name 1\"], [\"Type 1\"]), (\"002\", [\"Name 2\"], [\"Type 2\"])]",
      "41:         )",
      "43:     def test_xmliter_unusual_node(self):",
      "44:         body = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>",
      "45:             <root>",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "43:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "53:         ]",
      "54:         self.assertEqual(nodenames, [[\"matchme...\"]])",
      "56:     def test_xmliter_unicode(self):",
      "57:         # example taken from https://github.com/scrapy/scrapy/issues/1665",
      "58:         body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "57:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "112:                 [(\"26\", [\"-\"], [\"80\"]), (\"21\", [\"Ab\"], [\"76\"]), (\"27\", [\"A\"], [\"27\"])],",
      "113:             )",
      "115:     def test_xmliter_text(self):",
      "116:         body = (",
      "117:             '<?xml version=\"1.0\" encoding=\"UTF-8\"?>'",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "117:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "123:             [[\"one\"], [\"two\"]],",
      "124:         )",
      "126:     def test_xmliter_namespaces(self):",
      "127:         body = b\"\"\"",
      "128:             <?xml version=\"1.0\" encoding=\"UTF-8\"?>",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "129:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "162:         self.assertEqual(node.xpath(\"id/text()\").getall(), [])",
      "163:         self.assertEqual(node.xpath(\"price/text()\").getall(), [])",
      "165:     def test_xmliter_namespaced_nodename(self):",
      "166:         body = b\"\"\"",
      "167:             <?xml version=\"1.0\" encoding=\"UTF-8\"?>",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "169:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
      "",
      "---------------",
      "--- Hunk 7 ---",
      "[Context before]",
      "190:             [\"http://www.mydummycompany.com/images/item1.jpg\"],",
      "191:         )",
      "193:     def test_xmliter_namespaced_nodename_missing(self):",
      "194:         body = b\"\"\"",
      "195:             <?xml version=\"1.0\" encoding=\"UTF-8\"?>",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "198:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
      "",
      "---------------",
      "--- Hunk 8 ---",
      "[Context before]",
      "214:         with self.assertRaises(StopIteration):",
      "215:             next(my_iter)",
      "217:     def test_xmliter_exception(self):",
      "218:         body = (",
      "219:             '<?xml version=\"1.0\" encoding=\"UTF-8\"?>'",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "223:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
      "",
      "---------------",
      "--- Hunk 9 ---",
      "[Context before]",
      "227:         self.assertRaises(StopIteration, next, iter)",
      "229:     def test_xmliter_objtype_exception(self):",
      "230:         i = self.xmliter(42, \"product\")",
      "231:         self.assertRaises(TypeError, next, i)",
      "233:     def test_xmliter_encoding(self):",
      "234:         body = (",
      "235:             b'<?xml version=\"1.0\" encoding=\"ISO-8859-9\"?>\\n'",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "236:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
      "241:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
      "",
      "---------------",
      "--- Hunk 10 ---",
      "[Context before]",
      "244:         )",
      "254:     def test_xmliter_iterate_namespace(self):",
      "255:         body = b\"\"\"",
      "",
      "[Removed Lines]",
      "247: class LxmlXmliterTestCase(XmliterTestCase):",
      "248:     xmliter = staticmethod(xmliter_lxml)",
      "250:     @mark.xfail(reason=\"known bug of the current implementation\")",
      "251:     def test_xmliter_namespaced_nodename(self):",
      "252:         super().test_xmliter_namespaced_nodename()",
      "",
      "[Added Lines]",
      "256: class XmliterTestCase(XmliterBaseTestCase, unittest.TestCase):",
      "257:     xmliter = staticmethod(xmliter)",
      "259:     def test_deprecation(self):",
      "260:         body = b\"\"\"",
      "261:             <?xml version=\"1.0\" encoding=\"UTF-8\"?>",
      "262:             <products>",
      "263:               <product></product>",
      "264:             </products>",
      "265:         \"\"\"",
      "266:         with pytest.warns(",
      "267:             ScrapyDeprecationWarning,",
      "268:             match=\"xmliter\",",
      "269:         ):",
      "270:             next(self.xmliter(body, \"product\"))",
      "273: class LxmlXmliterTestCase(XmliterBaseTestCase, unittest.TestCase):",
      "274:     xmliter = staticmethod(xmliter_lxml)",
      "",
      "---------------"
    ],
    "tests/test_utils_response.py||tests/test_utils_response.py": [
      "File: tests/test_utils_response.py -> tests/test_utils_response.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: import unittest",
      "2: import warnings",
      "3: from pathlib import Path",
      "4: from urllib.parse import urlparse",
      "6: from scrapy.exceptions import ScrapyDeprecationWarning",
      "7: from scrapy.http import HtmlResponse, Response, TextResponse",
      "8: from scrapy.utils.python import to_bytes",
      "9: from scrapy.utils.response import (",
      "10:     get_base_url,",
      "11:     get_meta_refresh,",
      "12:     open_in_browser,",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "4: from time import process_time",
      "7: import pytest",
      "13:     _remove_html_comments,",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "198:         assert open_in_browser(",
      "199:             r5, _openfunc=check_base_url",
      "200:         ), \"Inject unique base url with conditional comment\"",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "206:     def test_open_in_browser_redos_comment(self):",
      "207:         MAX_CPU_TIME = 0.001",
      "209:         # Exploit input from",
      "210:         # https://makenowjust-labs.github.io/recheck/playground/",
      "211:         # for /<!--.*?-->/ (old pattern to remove comments).",
      "212:         body = b\"-><!--\\x00\" * 25_000 + b\"->\\n<!---->\"",
      "214:         response = HtmlResponse(\"https://example.com\", body=body)",
      "216:         start_time = process_time()",
      "218:         open_in_browser(response, lambda url: True)",
      "220:         end_time = process_time()",
      "221:         self.assertLess(end_time - start_time, MAX_CPU_TIME)",
      "223:     def test_open_in_browser_redos_head(self):",
      "224:         MAX_CPU_TIME = 0.001",
      "226:         # Exploit input from",
      "227:         # https://makenowjust-labs.github.io/recheck/playground/",
      "228:         # for /(<head(?:>|\\s.*?>))/ (old pattern to find the head element).",
      "229:         body = b\"<head\\t\" * 8_000",
      "231:         response = HtmlResponse(\"https://example.com\", body=body)",
      "233:         start_time = process_time()",
      "235:         open_in_browser(response, lambda url: True)",
      "237:         end_time = process_time()",
      "238:         self.assertLess(end_time - start_time, MAX_CPU_TIME)",
      "241: @pytest.mark.parametrize(",
      "242:     \"input_body,output_body\",",
      "243:     (",
      "244:         (",
      "245:             b\"a<!--\",",
      "246:             b\"a\",",
      "247:         ),",
      "248:         (",
      "249:             b\"a<!---->b\",",
      "250:             b\"ab\",",
      "251:         ),",
      "252:         (",
      "253:             b\"a<!--b-->c\",",
      "254:             b\"ac\",",
      "255:         ),",
      "256:         (",
      "257:             b\"a<!--b-->c<!--\",",
      "258:             b\"ac\",",
      "259:         ),",
      "260:         (",
      "261:             b\"a<!--b-->c<!--d\",",
      "262:             b\"ac\",",
      "263:         ),",
      "264:         (",
      "265:             b\"a<!--b-->c<!---->d\",",
      "266:             b\"acd\",",
      "267:         ),",
      "268:         (",
      "269:             b\"a<!--b--><!--c-->d\",",
      "270:             b\"ad\",",
      "271:         ),",
      "272:     ),",
      "273: )",
      "274: def test_remove_html_comments(input_body, output_body):",
      "275:     assert (",
      "276:         _remove_html_comments(input_body) == output_body",
      "277:     ), f\"{_remove_html_comments(input_body)=} == {output_body=}\"",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "150d96764b5a455c75315596ca8ba5ded0f416dd",
      "candidate_info": {
        "commit_hash": "150d96764b5a455c75315596ca8ba5ded0f416dd",
        "repo": "scrapy/scrapy",
        "commit_url": "https://github.com/scrapy/scrapy/commit/150d96764b5a455c75315596ca8ba5ded0f416dd",
        "files": [
          "docs/faq.rst",
          "docs/news.rst",
          "scrapy/spiders/feed.py",
          "scrapy/utils/iterators.py",
          "tests/test_utils_iterators.py",
          "tests/test_utils_response.py"
        ],
        "message": "Deprecate xmliter in favor of xmliter_lxml",
        "before_after_code_files": [
          "scrapy/spiders/feed.py||scrapy/spiders/feed.py",
          "scrapy/utils/iterators.py||scrapy/utils/iterators.py",
          "tests/test_utils_iterators.py||tests/test_utils_iterators.py",
          "tests/test_utils_response.py||tests/test_utils_response.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/scrapy/scrapy/pull/6222"
        ],
        "olp_code_files": {
          "patch": [
            "scrapy/spiders/feed.py||scrapy/spiders/feed.py",
            "scrapy/utils/iterators.py||scrapy/utils/iterators.py",
            "tests/test_utils_iterators.py||tests/test_utils_iterators.py",
            "tests/test_utils_response.py||tests/test_utils_response.py"
          ],
          "candidate": [
            "scrapy/spiders/feed.py||scrapy/spiders/feed.py",
            "scrapy/utils/iterators.py||scrapy/utils/iterators.py",
            "tests/test_utils_iterators.py||tests/test_utils_iterators.py",
            "tests/test_utils_response.py||tests/test_utils_response.py"
          ]
        }
      },
      "candidate_diff": {
        "scrapy/spiders/feed.py||scrapy/spiders/feed.py": [
          "File: scrapy/spiders/feed.py -> scrapy/spiders/feed.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "7: from scrapy.exceptions import NotConfigured, NotSupported",
          "8: from scrapy.selector import Selector",
          "9: from scrapy.spiders import Spider",
          "11: from scrapy.utils.spider import iterate_spider_output",
          "",
          "[Removed Lines]",
          "10: from scrapy.utils.iterators import csviter, xmliter",
          "",
          "[Added Lines]",
          "10: from scrapy.utils.iterators import csviter, xmliter_lxml",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "84:         return self.parse_nodes(response, nodes)",
          "86:     def _iternodes(self, response):",
          "88:             self._register_namespaces(node)",
          "89:             yield node",
          "",
          "[Removed Lines]",
          "87:         for node in xmliter(response, self.itertag):",
          "",
          "[Added Lines]",
          "87:         for node in xmliter_lxml(response, self.itertag):",
          "",
          "---------------"
        ],
        "scrapy/utils/iterators.py||scrapy/utils/iterators.py": [
          "File: scrapy/utils/iterators.py -> scrapy/utils/iterators.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17:     cast,",
          "18:     overload,",
          "19: )",
          "21: from lxml import etree",
          "23: from scrapy.http import Response, TextResponse",
          "24: from scrapy.selector import Selector",
          "25: from scrapy.utils.python import re_rsearch, to_unicode",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: from warnings import warn",
          "23: from packaging.version import Version",
          "25: from scrapy.exceptions import ScrapyDeprecationWarning",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "30: logger = logging.getLogger(__name__)",
          "33: def xmliter(",
          "34:     obj: Union[Response, str, bytes], nodename: str",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "35: _LXML_VERSION = Version(etree.__version__)",
          "36: _LXML_HUGE_TREE_VERSION = Version(\"4.2\")",
          "37: _ITERPARSE_KWARGS = {}",
          "38: if _LXML_VERSION >= _LXML_HUGE_TREE_VERSION:",
          "39:     _ITERPARSE_KWARGS[\"huge_tree\"] = True",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "41:     - a unicode string",
          "42:     - a string encoded as utf-8",
          "43:     \"\"\"",
          "44:     nodename_patt = re.escape(nodename)",
          "46:     DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]+>\\s*\", re.S)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "53:     warn(",
          "54:         (",
          "55:             \"xmliter is deprecated and its use strongly discouraged because \"",
          "56:             \"it is vulnerable to ReDoS attacks. Use xmliter_lxml instead. See \"",
          "57:             \"https://github.com/scrapy/scrapy/security/advisories/GHSA-cc65-xxvf-f7r9\"",
          "58:         ),",
          "59:         ScrapyDeprecationWarning,",
          "60:         stacklevel=2,",
          "61:     )",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "87:         reader,",
          "88:         encoding=reader.encoding,",
          "89:         events=(\"start-ns\",),",
          "91:     )",
          "92:     for event, (_prefix, _namespace) in ns_iterator:",
          "93:         if _prefix != node_prefix:",
          "",
          "[Removed Lines]",
          "90:         huge_tree=True,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "111:         reader,",
          "112:         tag=tag,",
          "113:         encoding=reader.encoding,",
          "115:     )",
          "116:     selxpath = \"//\" + (f\"{prefix}:{nodename}\" if namespace else nodename)",
          "117:     for _, node in iterable:",
          "",
          "[Removed Lines]",
          "114:         huge_tree=True,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/test_utils_iterators.py||tests/test_utils_iterators.py": [
          "File: tests/test_utils_iterators.py -> tests/test_utils_iterators.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: from twisted.trial import unittest",
          "3: from scrapy.http import Response, TextResponse, XmlResponse",
          "4: from scrapy.utils.iterators import _body_or_str, csviter, xmliter, xmliter_lxml",
          "5: from tests import get_testdata",
          "11:     def test_xmliter(self):",
          "12:         body = b\"\"\"",
          "13:             <?xml version=\"1.0\" encoding=\"UTF-8\"?>",
          "",
          "[Removed Lines]",
          "8: class XmliterTestCase(unittest.TestCase):",
          "9:     xmliter = staticmethod(xmliter)",
          "",
          "[Added Lines]",
          "1: import pytest",
          "4: from scrapy.exceptions import ScrapyDeprecationWarning",
          "10: class XmliterBaseTestCase:",
          "11:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39:             attrs, [(\"001\", [\"Name 1\"], [\"Type 1\"]), (\"002\", [\"Name 2\"], [\"Type 2\"])]",
          "40:         )",
          "42:     def test_xmliter_unusual_node(self):",
          "43:         body = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>",
          "44:             <root>",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "43:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "52:         ]",
          "53:         self.assertEqual(nodenames, [[\"matchme...\"]])",
          "55:     def test_xmliter_unicode(self):",
          "56:         # example taken from https://github.com/scrapy/scrapy/issues/1665",
          "57:         body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "57:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "111:                 [(\"26\", [\"-\"], [\"80\"]), (\"21\", [\"Ab\"], [\"76\"]), (\"27\", [\"A\"], [\"27\"])],",
          "112:             )",
          "114:     def test_xmliter_text(self):",
          "115:         body = (",
          "116:             '<?xml version=\"1.0\" encoding=\"UTF-8\"?>'",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "117:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "122:             [[\"one\"], [\"two\"]],",
          "123:         )",
          "125:     def test_xmliter_namespaces(self):",
          "126:         body = b\"\"\"",
          "127:             <?xml version=\"1.0\" encoding=\"UTF-8\"?>",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "129:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "161:         self.assertEqual(node.xpath(\"id/text()\").getall(), [])",
          "162:         self.assertEqual(node.xpath(\"price/text()\").getall(), [])",
          "164:     def test_xmliter_namespaced_nodename(self):",
          "165:         body = b\"\"\"",
          "166:             <?xml version=\"1.0\" encoding=\"UTF-8\"?>",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "169:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "189:             [\"http://www.mydummycompany.com/images/item1.jpg\"],",
          "190:         )",
          "192:     def test_xmliter_namespaced_nodename_missing(self):",
          "193:         body = b\"\"\"",
          "194:             <?xml version=\"1.0\" encoding=\"UTF-8\"?>",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "198:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "213:         with self.assertRaises(StopIteration):",
          "214:             next(my_iter)",
          "216:     def test_xmliter_exception(self):",
          "217:         body = (",
          "218:             '<?xml version=\"1.0\" encoding=\"UTF-8\"?>'",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "223:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "226:         self.assertRaises(StopIteration, next, iter)",
          "228:     def test_xmliter_objtype_exception(self):",
          "229:         i = self.xmliter(42, \"product\")",
          "230:         self.assertRaises(TypeError, next, i)",
          "232:     def test_xmliter_encoding(self):",
          "233:         body = (",
          "234:             b'<?xml version=\"1.0\" encoding=\"ISO-8859-9\"?>\\n'",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "236:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
          "241:     @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "243:         )",
          "247:     xmliter = staticmethod(xmliter_lxml)",
          "249:     def test_xmliter_iterate_namespace(self):",
          "",
          "[Removed Lines]",
          "246: class LxmlXmliterTestCase(XmliterTestCase):",
          "",
          "[Added Lines]",
          "256: class XmliterTestCase(XmliterBaseTestCase, unittest.TestCase):",
          "257:     xmliter = staticmethod(xmliter)",
          "259:     def test_deprecation(self):",
          "260:         body = b\"\"\"",
          "261:             <?xml version=\"1.0\" encoding=\"UTF-8\"?>",
          "262:             <products>",
          "263:               <product></product>",
          "264:             </products>",
          "265:         \"\"\"",
          "266:         with pytest.warns(",
          "267:             ScrapyDeprecationWarning,",
          "268:             match=\"xmliter\",",
          "269:         ):",
          "270:             next(self.xmliter(body, \"product\"))",
          "273: class LxmlXmliterTestCase(XmliterBaseTestCase, unittest.TestCase):",
          "",
          "---------------"
        ],
        "tests/test_utils_response.py||tests/test_utils_response.py": [
          "File: tests/test_utils_response.py -> tests/test_utils_response.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "203:             r5, _openfunc=check_base_url",
          "204:         ), \"Inject unique base url with conditional comment\"",
          "",
          "[Removed Lines]",
          "207: @pytest.mark.slow",
          "208: def test_open_in_browser_redos_comment():",
          "209:     MAX_CPU_TIME = 30",
          "211:     # Exploit input from",
          "212:     # https://makenowjust-labs.github.io/recheck/playground/",
          "213:     # for /<!--.*?-->/ (old pattern to remove comments).",
          "214:     body = b\"-><!--\\x00\" * (int(DOWNLOAD_MAXSIZE / 7) - 10) + b\"->\\n<!---->\"",
          "216:     response = HtmlResponse(\"https://example.com\", body=body)",
          "218:     start_time = process_time()",
          "220:     open_in_browser(response, lambda url: True)",
          "222:     end_time = process_time()",
          "223:     assert (end_time - start_time) < MAX_CPU_TIME",
          "226: @pytest.mark.slow",
          "227: def test_open_in_browser_redos_head():",
          "228:     MAX_CPU_TIME = 15",
          "230:     # Exploit input from",
          "231:     # https://makenowjust-labs.github.io/recheck/playground/",
          "232:     # for /(<head(?:>|\\s.*?>))/ (old pattern to find the head element).",
          "233:     body = b\"<head\\t\" * int(DOWNLOAD_MAXSIZE / 6)",
          "235:     response = HtmlResponse(\"https://example.com\", body=body)",
          "237:     start_time = process_time()",
          "239:     open_in_browser(response, lambda url: True)",
          "241:     end_time = process_time()",
          "242:     assert (end_time - start_time) < MAX_CPU_TIME",
          "",
          "[Added Lines]",
          "206:     @pytest.mark.slow",
          "207:     def test_open_in_browser_redos_comment(self):",
          "208:         MAX_CPU_TIME = 30",
          "210:         # Exploit input from",
          "211:         # https://makenowjust-labs.github.io/recheck/playground/",
          "212:         # for /<!--.*?-->/ (old pattern to remove comments).",
          "213:         body = b\"-><!--\\x00\" * (int(DOWNLOAD_MAXSIZE / 7) - 10) + b\"->\\n<!---->\"",
          "215:         response = HtmlResponse(\"https://example.com\", body=body)",
          "217:         start_time = process_time()",
          "219:         open_in_browser(response, lambda url: True)",
          "221:         end_time = process_time()",
          "222:         self.assertLess(end_time - start_time, MAX_CPU_TIME)",
          "224:     @pytest.mark.slow",
          "225:     def test_open_in_browser_redos_head(self):",
          "226:         MAX_CPU_TIME = 15",
          "228:         # Exploit input from",
          "229:         # https://makenowjust-labs.github.io/recheck/playground/",
          "230:         # for /(<head(?:>|\\s.*?>))/ (old pattern to find the head element).",
          "231:         body = b\"<head\\t\" * int(DOWNLOAD_MAXSIZE / 6)",
          "233:         response = HtmlResponse(\"https://example.com\", body=body)",
          "235:         start_time = process_time()",
          "237:         open_in_browser(response, lambda url: True)",
          "239:         end_time = process_time()",
          "240:         self.assertLess(end_time - start_time, MAX_CPU_TIME)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8a73c6c90c5984292d39a0a9d4be86e792d81cb4",
      "candidate_info": {
        "commit_hash": "8a73c6c90c5984292d39a0a9d4be86e792d81cb4",
        "repo": "scrapy/scrapy",
        "commit_url": "https://github.com/scrapy/scrapy/commit/8a73c6c90c5984292d39a0a9d4be86e792d81cb4",
        "files": [
          "scrapy/downloadermiddlewares/httpcompression.py"
        ],
        "message": "Fix HttpCompressionMiddleware backward compatibility",
        "before_after_code_files": [
          "scrapy/downloadermiddlewares/httpcompression.py||scrapy/downloadermiddlewares/httpcompression.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/scrapy/scrapy/pull/6222"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scrapy/downloadermiddlewares/httpcompression.py||scrapy/downloadermiddlewares/httpcompression.py": [
          "File: scrapy/downloadermiddlewares/httpcompression.py -> scrapy/downloadermiddlewares/httpcompression.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "40:     def __init__(self, stats=None, *, crawler=None):",
          "41:         if not crawler:",
          "44:             return",
          "45:         self.stats = crawler.stats",
          "46:         self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")",
          "",
          "[Removed Lines]",
          "42:             if stats:",
          "43:                 self.stats = stats",
          "",
          "[Added Lines]",
          "42:             self.stats = stats",
          "43:             self._max_size = 1073741824",
          "44:             self._warn_size = 33554432",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a49c8762dd163b60cc73c4486a662471cfa7ac7d",
      "candidate_info": {
        "commit_hash": "a49c8762dd163b60cc73c4486a662471cfa7ac7d",
        "repo": "scrapy/scrapy",
        "commit_url": "https://github.com/scrapy/scrapy/commit/a49c8762dd163b60cc73c4486a662471cfa7ac7d",
        "files": [
          "scrapy/utils/iterators.py"
        ],
        "message": "Avoid calling iterparse twice",
        "before_after_code_files": [
          "scrapy/utils/iterators.py||scrapy/utils/iterators.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/scrapy/scrapy/pull/6222"
        ],
        "olp_code_files": {
          "patch": [
            "scrapy/utils/iterators.py||scrapy/utils/iterators.py"
          ],
          "candidate": [
            "scrapy/utils/iterators.py||scrapy/utils/iterators.py"
          ]
        }
      },
      "candidate_diff": {
        "scrapy/utils/iterators.py||scrapy/utils/iterators.py": [
          "File: scrapy/utils/iterators.py -> scrapy/utils/iterators.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "12:     List,",
          "13:     Literal,",
          "14:     Optional,",
          "16:     Union,",
          "17:     cast,",
          "18:     overload,",
          "",
          "[Removed Lines]",
          "15:     Tuple,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "97:         yield Selector(text=nodetext, type=\"xml\")",
          "118: def xmliter_lxml(",
          "119:     obj: Union[Response, str, bytes],",
          "120:     nodename: str,",
          "121:     namespace: Optional[str] = None,",
          "122:     prefix: str = \"x\",",
          "123: ) -> Generator[Selector, Any, None]:",
          "127:     reader: \"SupportsReadClose[bytes]\" = _StreamReader(obj)",
          "128:     tag = f\"{{{namespace}}}{nodename}\" if namespace else nodename",
          "129:     iterable = etree.iterparse(",
          "130:         reader,",
          "132:         encoding=reader.encoding,",
          "134:     )",
          "135:     selxpath = \"//\" + (f\"{prefix}:{nodename}\" if namespace else nodename)",
          "137:         nodetext = etree.tostring(node, encoding=\"unicode\")",
          "138:         node.clear()",
          "139:         xs = Selector(text=nodetext, type=\"xml\")",
          "",
          "[Removed Lines]",
          "100: def _resolve_xml_namespace(element_name: str, data: bytes) -> Tuple[str, str]:",
          "101:     if \":\" not in element_name:",
          "102:         return element_name, None, None",
          "103:     reader: \"SupportsReadClose[bytes]\" = _StreamReader(data)",
          "104:     input_prefix, element_name = element_name.split(\":\", maxsplit=1)",
          "105:     ns_iterator = etree.iterparse(",
          "106:         reader,",
          "107:         encoding=reader.encoding,",
          "108:         events=(\"start-ns\",),",
          "110:     )",
          "111:     for event, (prefix, namespace) in ns_iterator:",
          "112:         if prefix != input_prefix:",
          "113:             continue",
          "114:         return element_name, prefix, namespace",
          "115:     return f\"{input_prefix}:{element_name}\", None, None",
          "124:     if not namespace:",
          "125:         nodename, prefix, namespace = _resolve_xml_namespace(nodename, obj)",
          "131:         tag=tag,",
          "136:     for _, node in iterable:",
          "",
          "[Added Lines]",
          "110:         events=(\"end\", \"start-ns\"),",
          "114:     needs_namespace_resolution = not namespace and \":\" in nodename",
          "115:     if needs_namespace_resolution:",
          "116:         prefix, nodename = nodename.split(\":\", maxsplit=1)",
          "117:     for event, data in iterable:",
          "118:         if event == \"start-ns\":",
          "119:             if needs_namespace_resolution:",
          "120:                 _prefix, _namespace = data",
          "121:                 if _prefix != prefix:",
          "122:                     continue",
          "123:                 namespace = _namespace",
          "124:                 needs_namespace_resolution = False",
          "125:                 selxpath = f\"//{prefix}:{nodename}\"",
          "126:                 tag = f\"{{{namespace}}}{nodename}\"",
          "127:             continue",
          "128:         node = data",
          "129:         if node.tag != tag:",
          "130:             continue",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bb74badd1bd66c59a63268c52342507c76d290b8",
      "candidate_info": {
        "commit_hash": "bb74badd1bd66c59a63268c52342507c76d290b8",
        "repo": "scrapy/scrapy",
        "commit_url": "https://github.com/scrapy/scrapy/commit/bb74badd1bd66c59a63268c52342507c76d290b8",
        "files": [
          "scrapy/downloadermiddlewares/httpcompression.py"
        ],
        "message": "spider \u2192 mw",
        "before_after_code_files": [
          "scrapy/downloadermiddlewares/httpcompression.py||scrapy/downloadermiddlewares/httpcompression.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/scrapy/scrapy/pull/6222"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scrapy/downloadermiddlewares/httpcompression.py||scrapy/downloadermiddlewares/httpcompression.py": [
          "File: scrapy/downloadermiddlewares/httpcompression.py -> scrapy/downloadermiddlewares/httpcompression.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "61:                 \"reimplement their 'from_crawler' method.\",",
          "62:                 ScrapyDeprecationWarning,",
          "63:             )",
          "71:     def open_spider(self, spider):",
          "72:         if hasattr(spider, \"download_maxsize\"):",
          "",
          "[Removed Lines]",
          "64:             spider = cls()",
          "65:             spider.stats = crawler.stats",
          "66:             spider._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")",
          "67:             spider._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")",
          "68:             crawler.signals.connect(spider.open_spider, signals.spider_opened)",
          "69:             return spider",
          "",
          "[Added Lines]",
          "64:             mw = cls()",
          "65:             mw.stats = crawler.stats",
          "66:             mw._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")",
          "67:             mw._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")",
          "68:             crawler.signals.connect(mw.open_spider, signals.spider_opened)",
          "69:             return mw",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "538192916f496eb21846d797a6feff5c05f501cf",
      "candidate_info": {
        "commit_hash": "538192916f496eb21846d797a6feff5c05f501cf",
        "repo": "scrapy/scrapy",
        "commit_url": "https://github.com/scrapy/scrapy/commit/538192916f496eb21846d797a6feff5c05f501cf",
        "files": [
          "scrapy/crawler.py",
          "scrapy/utils/ossignal.py",
          "scrapy/utils/testproc.py",
          "setup.py",
          "tests/CrawlerProcess/sleeping.py",
          "tests/requirements.txt",
          "tests/test_command_shell.py",
          "tests/test_crawler.py"
        ],
        "message": "Merge pull request #6064 from wRAR/signals-proper\n\nRefactor installing signals.",
        "before_after_code_files": [
          "scrapy/crawler.py||scrapy/crawler.py",
          "scrapy/utils/ossignal.py||scrapy/utils/ossignal.py",
          "scrapy/utils/testproc.py||scrapy/utils/testproc.py",
          "setup.py||setup.py",
          "tests/CrawlerProcess/sleeping.py||tests/CrawlerProcess/sleeping.py",
          "tests/test_command_shell.py||tests/test_command_shell.py",
          "tests/test_crawler.py||tests/test_crawler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/scrapy/scrapy/pull/6222"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scrapy/crawler.py||scrapy/crawler.py": [
          "File: scrapy/crawler.py -> scrapy/crawler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "404:         :param bool stop_after_crawl: stop or not the reactor when all",
          "405:             crawlers have finished",
          "409:         \"\"\"",
          "410:         from twisted.internet import reactor",
          "",
          "[Removed Lines]",
          "407:         :param bool install_signal_handlers: whether to install the shutdown",
          "408:             handlers (default: True)",
          "",
          "[Added Lines]",
          "407:         :param bool install_signal_handlers: whether to install the OS signal",
          "408:             handlers from Twisted and Scrapy (default: True)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "416:                 return",
          "417:             d.addBoth(self._stop_reactor)",
          "421:         resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])",
          "422:         resolver = create_instance(resolver_class, self.settings, self, reactor=reactor)",
          "423:         resolver.install_on_reactor()",
          "424:         tp = reactor.getThreadPool()",
          "425:         tp.adjustPoolsize(maxthreads=self.settings.getint(\"REACTOR_THREADPOOL_MAXSIZE\"))",
          "426:         reactor.addSystemEventTrigger(\"before\", \"shutdown\", self.stop)",
          "429:     def _graceful_stop_reactor(self) -> Deferred:",
          "430:         d = self.stop()",
          "",
          "[Removed Lines]",
          "419:         if install_signal_handlers:",
          "420:             install_shutdown_handlers(self._signal_shutdown)",
          "427:         reactor.run(installSignalHandlers=False)  # blocking call",
          "",
          "[Added Lines]",
          "425:         if install_signal_handlers:",
          "426:             reactor.addSystemEventTrigger(",
          "427:                 \"after\", \"startup\", install_shutdown_handlers, self._signal_shutdown",
          "428:             )",
          "429:         reactor.run(installSignalHandlers=install_signal_handlers)  # blocking call",
          "",
          "---------------"
        ],
        "scrapy/utils/ossignal.py||scrapy/utils/ossignal.py": [
          "File: scrapy/utils/ossignal.py -> scrapy/utils/ossignal.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19:     function: SignalHandlerT, override_sigint: bool = True",
          "20: ) -> None:",
          "21:     \"\"\"Install the given function as a signal handler for all common shutdown",
          "25:     \"\"\"",
          "29:     signal.signal(signal.SIGTERM, function)",
          "30:     if signal.getsignal(signal.SIGINT) == signal.default_int_handler or override_sigint:",
          "31:         signal.signal(signal.SIGINT, function)",
          "",
          "[Removed Lines]",
          "22:     signals (such as SIGINT, SIGTERM, etc). If override_sigint is ``False`` the",
          "23:     SIGINT handler won't be install if there is already a handler in place",
          "24:     (e.g.  Pdb)",
          "26:     from twisted.internet import reactor",
          "28:     reactor._handleSignals()",
          "",
          "[Added Lines]",
          "22:     signals (such as SIGINT, SIGTERM, etc). If ``override_sigint`` is ``False`` the",
          "23:     SIGINT handler won't be installed if there is already a handler in place",
          "24:     (e.g. Pdb)",
          "",
          "---------------"
        ],
        "scrapy/utils/testproc.py||scrapy/utils/testproc.py": [
          "File: scrapy/utils/testproc.py -> scrapy/utils/testproc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "3: import os",
          "4: import sys",
          "7: from twisted.internet.defer import Deferred",
          "8: from twisted.internet.error import ProcessTerminated",
          "",
          "[Removed Lines]",
          "5: from typing import Iterable, Optional, Tuple, cast",
          "",
          "[Added Lines]",
          "5: from typing import Iterable, List, Optional, Tuple, cast",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "26:         env = os.environ.copy()",
          "27:         if settings is not None:",
          "28:             env[\"SCRAPY_SETTINGS_MODULE\"] = settings",
          "29:         cmd = self.prefix + [self.command] + list(args)",
          "30:         pp = TestProcessProtocol()",
          "32:         reactor.spawnProcess(pp, cmd[0], cmd, env=env, path=self.cwd)",
          "33:         return pp.deferred",
          "35:     def _process_finished(",
          "37:     ) -> Tuple[int, bytes, bytes]:",
          "38:         if pp.exitcode and check_code:",
          "39:             msg = f\"process {cmd} exit with code {pp.exitcode}\"",
          "",
          "[Removed Lines]",
          "31:         pp.deferred.addBoth(self._process_finished, cmd, check_code)",
          "36:         self, pp: TestProcessProtocol, cmd: str, check_code: bool",
          "",
          "[Added Lines]",
          "29:         assert self.command",
          "32:         pp.deferred.addCallback(self._process_finished, cmd, check_code)",
          "37:         self, pp: TestProcessProtocol, cmd: List[str], check_code: bool",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "8: install_requires = [",
          "11:     \"cryptography>=36.0.0\",",
          "12:     \"cssselect>=0.9.1\",",
          "13:     \"itemloaders>=1.0.1\",",
          "",
          "[Removed Lines]",
          "9:     # 23.8.0 incompatibility: https://github.com/scrapy/scrapy/issues/6024",
          "10:     \"Twisted>=18.9.0,<23.8.0\",",
          "",
          "[Added Lines]",
          "9:     \"Twisted>=18.9.0\",",
          "",
          "---------------"
        ],
        "tests/CrawlerProcess/sleeping.py||tests/CrawlerProcess/sleeping.py": [
          "File: tests/CrawlerProcess/sleeping.py -> tests/CrawlerProcess/sleeping.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: from twisted.internet.defer import Deferred",
          "3: import scrapy",
          "4: from scrapy.crawler import CrawlerProcess",
          "5: from scrapy.utils.defer import maybe_deferred_to_future",
          "8: class SleepingSpider(scrapy.Spider):",
          "9:     name = \"sleeping\"",
          "11:     start_urls = [\"data:,;\"]",
          "13:     async def parse(self, response):",
          "14:         from twisted.internet import reactor",
          "16:         d = Deferred()",
          "17:         reactor.callLater(3, d.callback, None)",
          "18:         await maybe_deferred_to_future(d)",
          "21: process = CrawlerProcess(settings={})",
          "23: process.crawl(SleepingSpider)",
          "24: process.start()",
          "",
          "---------------"
        ],
        "tests/test_command_shell.py||tests/test_command_shell.py": [
          "File: tests/test_command_shell.py -> tests/test_command_shell.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: from pathlib import Path",
          "3: from twisted.internet import defer",
          "4: from twisted.trial import unittest",
          "6: from scrapy.utils.testproc import ProcessTest",
          "7: from scrapy.utils.testsite import SiteTest",
          "8: from tests import NON_EXISTING_RESOLVABLE, tests_datadir",
          "11: class ShellTest(ProcessTest, SiteTest, unittest.TestCase):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: import sys",
          "2: from io import BytesIO",
          "5: from pexpect.popen_spawn import PopenSpawn",
          "12: from tests.mockserver import MockServer",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "133:         args = [\"-c\", code, \"--set\", f\"TWISTED_REACTOR={reactor_path}\"]",
          "134:         _, _, err = yield self.execute(args, check_code=True)",
          "135:         self.assertNotIn(b\"RuntimeError: There is no current event loop in thread\", err)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "142: class InteractiveShellTest(unittest.TestCase):",
          "143:     def test_fetch(self):",
          "144:         args = (",
          "145:             sys.executable,",
          "146:             \"-m\",",
          "147:             \"scrapy.cmdline\",",
          "148:             \"shell\",",
          "149:         )",
          "150:         logfile = BytesIO()",
          "151:         p = PopenSpawn(args, timeout=5)",
          "152:         p.logfile_read = logfile",
          "153:         p.expect_exact(\"Available Scrapy objects\")",
          "154:         with MockServer() as mockserver:",
          "155:             p.sendline(f\"fetch('{mockserver.url('/')}')\")",
          "156:             p.sendline(\"type(response)\")",
          "157:             p.expect_exact(\"HtmlResponse\")",
          "158:         p.sendeof()",
          "159:         p.wait()",
          "160:         logfile.seek(0)",
          "161:         self.assertNotIn(\"Traceback\", logfile.read().decode())",
          "",
          "---------------"
        ],
        "tests/test_crawler.py||tests/test_crawler.py": [
          "File: tests/test_crawler.py -> tests/test_crawler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: import logging",
          "2: import os",
          "3: import platform",
          "4: import subprocess",
          "5: import sys",
          "6: import warnings",
          "7: from pathlib import Path",
          "9: import pytest",
          "10: from packaging.version import parse as parse_version",
          "11: from pytest import mark, raises",
          "12: from twisted.internet import defer",
          "13: from twisted.trial import unittest",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4: import signal",
          "9: from typing import List",
          "13: from pexpect.popen_spawn import PopenSpawn",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "289:     script_dir: Path",
          "290:     cwd = os.getcwd()",
          "293:         script_path = self.script_dir / script_name",
          "295:         p = subprocess.Popen(",
          "296:             args,",
          "297:             env=get_mockserver_env(),",
          "",
          "[Removed Lines]",
          "292:     def run_script(self, script_name: str, *script_args):",
          "294:         args = [sys.executable, str(script_path)] + list(script_args)",
          "",
          "[Added Lines]",
          "295:     def get_script_args(self, script_name: str, *script_args: str) -> List[str]:",
          "297:         return [sys.executable, str(script_path)] + list(script_args)",
          "299:     def run_script(self, script_name: str, *script_args: str) -> str:",
          "300:         args = self.get_script_args(script_name, *script_args)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "517:         self.assertIn(\"Spider closed (finished)\", log)",
          "518:         self.assertIn(\"The value of FOO is 42\", log)",
          "521: class CrawlerRunnerSubprocess(ScriptRunnerMixin, unittest.TestCase):",
          "522:     script_dir = Path(__file__).parent.resolve() / \"CrawlerRunner\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "526:     def test_shutdown_graceful(self):",
          "527:         sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK",
          "528:         args = self.get_script_args(\"sleeping.py\")",
          "529:         p = PopenSpawn(args, timeout=5)",
          "530:         p.expect_exact(\"Spider opened\")",
          "531:         p.expect_exact(\"Crawled (200)\")",
          "532:         p.kill(sig)",
          "533:         p.expect_exact(\"shutting down gracefully\")",
          "534:         p.expect_exact(\"Spider closed (shutdown)\")",
          "535:         p.wait()",
          "537:     def test_shutdown_forced(self):",
          "538:         sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK",
          "539:         args = self.get_script_args(\"sleeping.py\")",
          "540:         p = PopenSpawn(args, timeout=5)",
          "541:         p.expect_exact(\"Spider opened\")",
          "542:         p.expect_exact(\"Crawled (200)\")",
          "543:         p.kill(sig)",
          "544:         p.expect_exact(\"shutting down gracefully\")",
          "545:         p.kill(sig)",
          "546:         p.expect_exact(\"forcing unclean shutdown\")",
          "547:         p.wait()",
          "",
          "---------------"
        ]
      }
    }
  ]
}