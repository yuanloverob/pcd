{
  "cve_id": "CVE-2021-3702",
  "cve_desc": "A race condition flaw was found in ansible-runner, where an attacker could watch for rapid creation and deletion of a temporary directory, substitute their directory at that name, and then have access to ansible-runner's private_data_dir the next time ansible-runner made use of the private_data_dir. The highest Threat out of this flaw is to integrity and confidentiality.",
  "repo": "ansible/ansible-runner",
  "patch_hash": "93e95a3df9021a38010386d07df121392d249253",
  "patch_info": {
    "commit_hash": "93e95a3df9021a38010386d07df121392d249253",
    "repo": "ansible/ansible-runner",
    "commit_url": "https://github.com/ansible/ansible-runner/commit/93e95a3df9021a38010386d07df121392d249253",
    "files": [
      "ansible_runner/interface.py",
      "ansible_runner/runner.py",
      "ansible_runner/streaming.py"
    ],
    "message": "Successfully runs\n\nStreamController and StreamWorker are now fleshed out.",
    "before_after_code_files": [
      "ansible_runner/interface.py||ansible_runner/interface.py",
      "ansible_runner/runner.py||ansible_runner/runner.py",
      "ansible_runner/streaming.py||ansible_runner/streaming.py"
    ]
  },
  "patch_diff": {
    "ansible_runner/interface.py||ansible_runner/interface.py": [
      "File: ansible_runner/interface.py -> ansible_runner/interface.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "24: from ansible_runner import output",
      "25: from ansible_runner.runner_config import RunnerConfig",
      "26: from ansible_runner.runner import Runner",
      "28: from ansible_runner.utils import (",
      "29:     dump_artifacts,",
      "30:     check_isolation_executable_installed,",
      "",
      "[Removed Lines]",
      "27: from ansible_runner.streaming import StreamWorker",
      "",
      "[Added Lines]",
      "27: from ansible_runner.streaming import StreamController, StreamWorker",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "65:     event_callback_handler = kwargs.pop('event_handler', None)",
      "66:     status_callback_handler = kwargs.pop('status_handler', None)",
      "67:     cancel_callback = kwargs.pop('cancel_callback', None)",
      "69:     finished_callback = kwargs.pop('finished_callback', None)",
      "71:     control_out = kwargs.pop('control_out', None)",
      "78:     rc = RunnerConfig(**kwargs)",
      "79:     rc.prepare()",
      "",
      "[Removed Lines]",
      "68:     artifacts_callback = kwargs.pop('artifacts_callback', None)  # Currently not expected",
      "72:     if control_out is not None:",
      "73:         stream_worker = StreamWorker(control_out)",
      "74:         status_callback_handler = stream_worker.status_handler",
      "75:         event_callback_handler = stream_worker.event_handler",
      "76:         artifacts_callback = stream_worker.artifacts_callback",
      "",
      "[Added Lines]",
      "67:     artifacts_handler = kwargs.pop('artifacts_handler', None)",
      "71:     control_in = kwargs.pop('control_in', None)",
      "73:     worker_in = kwargs.pop('worker_in', None)",
      "74:     worker_out = kwargs.pop('worker_out', None)",
      "76:     if worker_in is not None and worker_out is not None:",
      "77:         stream_worker = StreamWorker(worker_in, worker_out, **kwargs)",
      "78:         return stream_worker",
      "80:     if control_in is not None and control_out is not None:",
      "81:         stream_controller = StreamController(control_in, control_out,",
      "82:                                              event_handler=event_callback_handler,",
      "83:                                              status_handler=status_callback_handler,",
      "84:                                              artifacts_handler=artifacts_handler,",
      "85:                                              cancel_callback=cancel_callback,",
      "86:                                              finished_callback=finished_callback,",
      "88:         return stream_controller",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "81:     return Runner(rc,",
      "82:                   event_handler=event_callback_handler,",
      "83:                   status_handler=status_callback_handler,",
      "84:                   cancel_callback=cancel_callback,",
      "86:                   finished_callback=finished_callback)",
      "",
      "[Removed Lines]",
      "85:                   artifacts_callback=artifacts_callback,",
      "",
      "[Added Lines]",
      "96:                   artifacts_handler=artifacts_handler,",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "124:     :param artifact_dir: The path to the directory where artifacts should live, this defaults to 'artifacts' under the private data dir",
      "125:     :param project_dir: The path to the playbook content, this defaults to 'project' within the private data dir",
      "126:     :param rotate_artifacts: Keep at most n artifact directories, disable with a value of 0 which is the default",
      "128:     :param event_handler: An optional callback that will be invoked any time an event is received by Runner itself, return True to keep the event",
      "129:     :param cancel_callback: An optional callback that can inform runner to cancel (returning True) or not (returning False)",
      "130:     :param finished_callback: An optional callback that will be invoked at shutdown after process cleanup.",
      "131:     :param status_handler: An optional callback that will be invoked any time the status changes (e.g...started, running, failed, successful, timeout)",
      "132:     :param process_isolation: Enable process isolation, using either a container engine (e.g. podman) or a sandbox (e.g. bwrap).",
      "133:     :param process_isolation_executable: Process isolation executable or container engine used to isolate execution. (default: podman)",
      "134:     :param process_isolation_path: Path that an isolated playbook run will use for staging. (default: /tmp)",
      "",
      "[Removed Lines]",
      "127:     :param control_out: A file-like object used for streaming information back to a control instance of Runner",
      "",
      "[Added Lines]",
      "139:     :param control_in: A file object used for receiving streamed data back from a worker instance of Runner",
      "140:     :param control_out: A file object used for streaming project data to a worker instance of Runner",
      "141:     :param worker_in: A file object used for streaming project data to a worker instance of Runner",
      "142:     :param worker_out: A file object used for streaming information back to a control instance of Runner",
      "147:     :param artifacts_handler: An optional callback that will be invoked at the end of the run to deal with the artifacts from the run.",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "170:     :type forks: int",
      "171:     :type quiet: bool",
      "172:     :type verbosity: int",
      "173:     :type control_out: file",
      "174:     :type event_handler: function",
      "175:     :type cancel_callback: function",
      "176:     :type finished_callback: function",
      "177:     :type status_handler: function",
      "178:     :type process_isolation: bool",
      "179:     :type process_isolation_executable: str",
      "180:     :type process_isolation_path: str",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "189:     :type control_in: file",
      "191:     :type worker_in: file",
      "192:     :type worker_out: file",
      "197:     :type artifacts_handler: function",
      "",
      "---------------"
    ],
    "ansible_runner/runner.py||ansible_runner/runner.py": [
      "File: ansible_runner/runner.py -> ansible_runner/runner.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "27: class Runner(object):",
      "29:     def __init__(self, config, cancel_callback=None, remove_partials=True, event_handler=None,",
      "31:         self.config = config",
      "32:         self.cancel_callback = cancel_callback",
      "33:         self.event_handler = event_handler",
      "35:         self.finished_callback = finished_callback",
      "36:         self.status_handler = status_handler",
      "37:         self.canceled = False",
      "",
      "[Removed Lines]",
      "30:                  artifacts_callback=None, finished_callback=None, status_handler=None):",
      "34:         self.artifacts_callback = artifacts_callback",
      "",
      "[Added Lines]",
      "30:                  artifacts_handler=None, finished_callback=None, status_handler=None):",
      "34:         self.artifacts_handler = artifacts_handler",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "284:                 logger.error('Failed to delete cgroup: {}'.format(stderr))",
      "285:                 raise RuntimeError('Failed to delete cgroup: {}'.format(stderr))",
      "288:             try:",
      "290:             except Exception as e:",
      "291:                 raise CallbackError(\"Exception in Artifact Callback: {}\".format(e))",
      "",
      "[Removed Lines]",
      "287:         if self.artifacts_callback is not None:",
      "289:                 self.artifacts_callback(self.config.artifact_dir)",
      "",
      "[Added Lines]",
      "287:         if self.artifacts_handler is not None:",
      "289:                 self.artifacts_handler(self.config.artifact_dir)",
      "",
      "---------------"
    ],
    "ansible_runner/streaming.py||ansible_runner/streaming.py": [
      "File: ansible_runner/streaming.py -> ansible_runner/streaming.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "1: import base64",
      "2: import io",
      "3: import json",
      "4: import os",
      "5: import zipfile",
      "10:         self.control_out = control_out",
      "14:         self.control_out.write(b'\\n')",
      "15:         self.control_out.flush()",
      "17:     def event_handler(self, event_data):",
      "23:         buf = io.BytesIO()",
      "24:         with zipfile.ZipFile(buf, 'w', compression=zipfile.ZIP_DEFLATED, allowZip64=True) as archive:",
      "25:             for dirpath, dirs, files in os.walk(artifact_dir):",
      "",
      "[Removed Lines]",
      "8: class StreamWorker(object):",
      "9:     def __init__(self, control_out):",
      "12:     def status_handler(self, status, runner_config):",
      "13:         self.control_out.write(json.dumps(status).encode('utf-8'))",
      "18:         self.control_out.write(json.dumps(event_data).encode('utf-8'))",
      "19:         self.control_out.write(b'\\n')",
      "20:         self.control_out.flush()",
      "22:     def artifacts_callback(self, artifact_dir):",
      "",
      "[Added Lines]",
      "2: import codecs",
      "6: import stat",
      "7: import tempfile",
      "8: import uuid",
      "11: import ansible_runner",
      "12: import ansible_runner.plugins",
      "15: class UUIDEncoder(json.JSONEncoder):",
      "16:     def default(self, obj):",
      "17:         if isinstance(obj, uuid.UUID):",
      "18:             return obj.hex",
      "19:         return json.JSONEncoder.default(self, obj)",
      "22: # List of kwargs options to the run method that should be sent to the remote executor.",
      "23: remote_run_options = (",
      "24:     'forks',",
      "25:     'host_pattern',",
      "26:     'ident',",
      "27:     'ignore_logging',",
      "28:     'inventory',",
      "29:     'limit',",
      "30:     'module',",
      "31:     'module_args',",
      "32:     'omit_event_data',",
      "33:     'only_failed_event_data',",
      "34:     'playbook',",
      "35:     'verbosity',",
      "36: )",
      "39: class StreamController(object):",
      "40:     def __init__(self, control_in, control_out, status_handler=None, event_handler=None,",
      "41:                  artifacts_handler=None, cancel_callback=None, finished_callback=None, **kwargs):",
      "42:         self.control_in = control_in",
      "45:         self.kwargs = kwargs",
      "46:         self.config = ansible_runner.RunnerConfig(**kwargs)",
      "47:         self.status_handler = status_handler",
      "48:         self.event_handler = event_handler",
      "49:         self.artifacts_handler = artifacts_handler",
      "51:         self.cancel_callback = cancel_callback",
      "52:         self.finished_callback = finished_callback",
      "54:         self.status = \"unstarted\"",
      "55:         self.rc = None",
      "57:     def run(self):",
      "58:         self.send_job()",
      "60:         job_events_path = os.path.join(self.config.artifact_dir, 'job_events')",
      "61:         if not os.path.exists(job_events_path):",
      "62:             os.mkdir(job_events_path, 0o700)",
      "64:         for line in self.control_in:",
      "65:             data = json.loads(line)",
      "66:             if 'status' in data:",
      "67:                 self.status_callback(data)",
      "68:             elif 'artifacts' in data:",
      "69:                 self.artifacts_callback(data)",
      "70:             elif 'eof' in data:",
      "71:                 break",
      "72:             else:",
      "73:                 self.event_callback(data)",
      "75:         if self.finished_callback is not None:",
      "76:             self.finished_callback(self)",
      "77:         return self.status, self.rc",
      "79:     def send_job(self):",
      "80:         self.config.prepare()",
      "81:         remote_options = {key: value for key, value in self.kwargs.items() if key in remote_run_options}",
      "83:         buf = io.BytesIO()",
      "84:         with zipfile.ZipFile(buf, 'w', compression=zipfile.ZIP_DEFLATED, allowZip64=True) as archive:",
      "85:             private_data_dir = self.kwargs.get('private_data_dir', None)",
      "86:             if private_data_dir:",
      "87:                 for dirpath, dirs, files in os.walk(private_data_dir):",
      "88:                     relpath = os.path.relpath(dirpath, private_data_dir)",
      "89:                     if relpath == \".\":",
      "90:                         relpath = \"\"",
      "91:                     for fname in files:",
      "92:                         archive.write(os.path.join(dirpath, fname), arcname=os.path.join(relpath, fname))",
      "94:             kwargs = json.dumps(remote_options, cls=UUIDEncoder)",
      "95:             archive.writestr('kwargs', kwargs)",
      "96:             archive.close()",
      "97:         buf.flush()",
      "99:         data = {",
      "100:             'private_data_dir': True,",
      "101:             'payload': base64.b64encode(buf.getvalue()).decode('ascii'),",
      "102:         }",
      "103:         self.control_out.write(json.dumps(data).encode('utf-8'))",
      "106:         self.control_out.close()",
      "108:     def status_callback(self, status_data):",
      "109:         self.status = status_data['status']",
      "111:         for plugin in ansible_runner.plugins:",
      "112:             ansible_runner.plugins[plugin].status_handler(self.config, status_data)",
      "113:         if self.status_handler is not None:",
      "114:             self.status_handler(status_data, runner_config=self.config)",
      "116:     def event_callback(self, event_data):",
      "117:         full_filename = os.path.join(self.config.artifact_dir,",
      "118:                                      'job_events',",
      "119:                                      '{}-{}.json'.format(event_data['counter'],",
      "120:                                                          event_data['uuid']))",
      "122:         if self.event_handler is not None:",
      "123:             should_write = self.event_handler(event_data)",
      "124:         else:",
      "125:             should_write = True",
      "126:         for plugin in ansible_runner.plugins:",
      "127:             ansible_runner.plugins[plugin].event_handler(self.config, event_data)",
      "128:         if should_write:",
      "129:             with codecs.open(full_filename, 'w', encoding='utf-8') as write_file:",
      "130:                 os.chmod(full_filename, stat.S_IRUSR | stat.S_IWUSR)",
      "131:                 json.dump(event_data, write_file)",
      "133:     def artifacts_callback(self, artifacts_data):  # FIXME",
      "134:         if self.artifacts_handler is not None:",
      "135:             self.artifacts_handler()",
      "138: class StreamWorker(object):",
      "139:     def __init__(self, worker_in, worker_out, **kwargs):",
      "140:         self.worker_in = worker_in",
      "141:         self.worker_out = worker_out",
      "143:         self.kwargs = kwargs",
      "145:         self.private_data_dir = tempfile.TemporaryDirectory().name",
      "147:     def run(self):",
      "148:         for line in self.worker_in:",
      "149:             data = json.loads(line)",
      "150:             if data.get('private_data_dir'):",
      "151:                 buf = io.BytesIO(base64.b64decode(data['payload']))",
      "152:                 with zipfile.ZipFile(buf, 'r') as archive:",
      "153:                     archive.extractall(path=self.private_data_dir)",
      "155:         kwargs_path = os.path.join(self.private_data_dir, 'kwargs')",
      "156:         if os.path.exists(kwargs_path):",
      "157:             with open(kwargs_path, \"r\") as kwf:",
      "158:                 kwargs = json.load(kwf)",
      "159:             if not isinstance(kwargs, dict):",
      "160:                 raise ValueError(\"Invalid kwargs data\")",
      "161:         else:",
      "162:             kwargs = {}",
      "164:         self.kwargs.update(kwargs)",
      "166:         self.kwargs['quiet'] = True",
      "167:         self.kwargs['private_data_dir'] = self.private_data_dir",
      "168:         self.kwargs['status_handler'] = self.status_handler",
      "169:         self.kwargs['event_handler'] = self.event_handler",
      "170:         self.kwargs['artifacts_handler'] = self.artifacts_handler",
      "171:         self.kwargs['finished_callback'] = self.finished_callback",
      "173:         ansible_runner.interface.run(**self.kwargs)",
      "175:         # FIXME: do cleanup on the tempdir",
      "177:     def status_handler(self, status, runner_config):",
      "178:         self.worker_out.write(json.dumps(status).encode('utf-8'))",
      "179:         self.worker_out.write(b'\\n')",
      "180:         self.worker_out.flush()",
      "183:         self.worker_out.write(json.dumps(event_data).encode('utf-8'))",
      "184:         self.worker_out.write(b'\\n')",
      "185:         self.worker_out.flush()",
      "187:     def artifacts_handler(self, artifact_dir):",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "34:             'artifacts': True,",
      "35:             'payload': base64.b64encode(buf.getvalue()).decode('ascii'),",
      "36:         }",
      "",
      "[Removed Lines]",
      "37:         self.control_out.write(json.dumps(data).encode('utf-8'))",
      "38:         self.control_out.write(b'\\n')",
      "39:         self.control_out.flush()",
      "40:         self.control_out.close()",
      "",
      "[Added Lines]",
      "202:         self.worker_out.write(json.dumps(data).encode('utf-8'))",
      "203:         self.worker_out.write(b'\\n')",
      "204:         self.worker_out.flush()",
      "206:     def finished_callback(self, runner_obj):",
      "207:         self.worker_out.write(json.dumps({'eof': True}).encode('utf-8'))",
      "208:         self.worker_out.write(b'\\n')",
      "209:         self.worker_out.flush()",
      "210:         self.worker_out.close()",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "b5e997ac8254aec47462f48769102890d0a7a599",
      "candidate_info": {
        "commit_hash": "b5e997ac8254aec47462f48769102890d0a7a599",
        "repo": "ansible/ansible-runner",
        "commit_url": "https://github.com/ansible/ansible-runner/commit/b5e997ac8254aec47462f48769102890d0a7a599",
        "files": [
          "ansible_runner/streaming.py"
        ],
        "message": "Properly reconstruct paths in worker phase",
        "before_after_code_files": [
          "ansible_runner/streaming.py||ansible_runner/streaming.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ],
          "candidate": [
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ]
        }
      },
      "candidate_diff": {
        "ansible_runner/streaming.py||ansible_runner/streaming.py": [
          "File: ansible_runner/streaming.py -> ansible_runner/streaming.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "34:         if _output is None:",
          "35:             _output = sys.stdout.buffer",
          "36:         self._output = _output",
          "38:         self.kwargs = kwargs",
          "40:         self.status = \"unstarted\"",
          "",
          "[Removed Lines]",
          "37:         self.private_data_dir = kwargs.pop('private_data_dir')",
          "",
          "[Added Lines]",
          "37:         self.private_data_dir = os.path.abspath(kwargs.pop('private_data_dir'))",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "75:         self.status = \"unstarted\"",
          "76:         self.rc = None",
          "78:     def run(self):",
          "79:         while True:",
          "80:             line = self._input.readline()",
          "81:             data = json.loads(line)",
          "83:             if 'kwargs' in data:",
          "85:             elif 'zipfile' in data:",
          "86:                 zip_data = self._input.read(data['zipfile'])",
          "87:                 utils.unstream_dir(zip_data, self.private_data_dir)",
          "",
          "[Removed Lines]",
          "84:                 self.job_kwargs = data['kwargs']",
          "",
          "[Added Lines]",
          "78:     def update_paths(self, kwargs):",
          "79:         if kwargs.get('envvars'):",
          "80:             roles_path = kwargs['envvars']['ANSIBLE_ROLES_PATH']",
          "81:             roles_dir = os.path.join(self.private_data_dir, 'roles')",
          "82:             kwargs['envvars']['ANSIBLE_ROLES_PATH'] = os.path.join(roles_dir, roles_path)",
          "83:         if kwargs.get('inventory'):",
          "84:             kwargs['inventory'] = os.path.join(self.private_data_dir, kwargs['inventory'])",
          "86:         return kwargs",
          "94:                 self.job_kwargs = self.update_paths(data['kwargs'])",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ad8ea0bb976be8889874d21f6acaabbe4959bd0d",
      "candidate_info": {
        "commit_hash": "ad8ea0bb976be8889874d21f6acaabbe4959bd0d",
        "repo": "ansible/ansible-runner",
        "commit_url": "https://github.com/ansible/ansible-runner/commit/ad8ea0bb976be8889874d21f6acaabbe4959bd0d",
        "files": [
          "ansible_runner/streaming.py"
        ],
        "message": "Hook up the artifact callback properly",
        "before_after_code_files": [
          "ansible_runner/streaming.py||ansible_runner/streaming.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ],
          "candidate": [
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ]
        }
      },
      "candidate_diff": {
        "ansible_runner/streaming.py||ansible_runner/streaming.py": [
          "File: ansible_runner/streaming.py -> ansible_runner/streaming.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "130:                 os.chmod(full_filename, stat.S_IRUSR | stat.S_IWUSR)",
          "131:                 json.dump(event_data, write_file)",
          "134:         if self.artifacts_handler is not None:",
          "138: class StreamWorker(object):",
          "",
          "[Removed Lines]",
          "133:     def artifacts_callback(self, artifacts_data):  # FIXME",
          "135:             self.artifacts_handler()",
          "",
          "[Added Lines]",
          "133:     def artifacts_callback(self, artifacts_data):",
          "134:         buf = io.BytesIO(base64.b64decode(artifacts_data['payload']))",
          "135:         with zipfile.ZipFile(buf, 'r') as archive:",
          "136:             archive.extractall(path=self.config.artifact_dir)",
          "139:             self.artifacts_handler(self.config.artifact_dir)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ca3bc272b94801913912a76c7dec021d9387edad",
      "candidate_info": {
        "commit_hash": "ca3bc272b94801913912a76c7dec021d9387edad",
        "repo": "ansible/ansible-runner",
        "commit_url": "https://github.com/ansible/ansible-runner/commit/ca3bc272b94801913912a76c7dec021d9387edad",
        "files": [
          "ansible_runner/__main__.py",
          "ansible_runner/interface.py",
          "ansible_runner/runner.py",
          "ansible_runner/streaming.py"
        ],
        "message": "Fix a few issues",
        "before_after_code_files": [
          "ansible_runner/__main__.py||ansible_runner/__main__.py",
          "ansible_runner/interface.py||ansible_runner/interface.py",
          "ansible_runner/runner.py||ansible_runner/runner.py",
          "ansible_runner/streaming.py||ansible_runner/streaming.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "ansible_runner/interface.py||ansible_runner/interface.py",
            "ansible_runner/runner.py||ansible_runner/runner.py",
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ],
          "candidate": [
            "ansible_runner/interface.py||ansible_runner/interface.py",
            "ansible_runner/runner.py||ansible_runner/runner.py",
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ]
        }
      },
      "candidate_diff": {
        "ansible_runner/__main__.py||ansible_runner/__main__.py": [
          "File: ansible_runner/__main__.py -> ansible_runner/__main__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "638:         'process',",
          "639:         help=\"Receive the output of remote ansible-runner work and distribute the results\"",
          "640:     )",
          "642:     # adhoc command exec",
          "643:     adhoc_subparser = subparser.add_parser(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "641:     add_args_to_parser(process_subparser, DEFAULT_CLI_ARGS['positional_args'])",
          "",
          "---------------"
        ],
        "ansible_runner/interface.py||ansible_runner/interface.py": [
          "File: ansible_runner/interface.py -> ansible_runner/interface.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "68:     cancel_callback = kwargs.pop('cancel_callback', None)",
          "69:     finished_callback = kwargs.pop('finished_callback', None)",
          "72:     if streamer:",
          "73:         if streamer == 'transmit':",
          "74:             stream_transmitter = Transmitter(**kwargs)",
          "",
          "[Removed Lines]",
          "71:     streamer = kwargs.pop('streamer')",
          "",
          "[Added Lines]",
          "71:     streamer = kwargs.pop('streamer', None)",
          "",
          "---------------"
        ],
        "ansible_runner/runner.py||ansible_runner/runner.py": [
          "File: ansible_runner/runner.py -> ansible_runner/runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "82:     def status_callback(self, status):",
          "83:         self.status = status",
          "88:         for plugin in ansible_runner.plugins:",
          "89:             ansible_runner.plugins[plugin].status_handler(self.config, status_data)",
          "90:         if self.status_handler is not None:",
          "",
          "[Removed Lines]",
          "84:         status_data = {",
          "85:             'status': status, 'runner_ident': str(self.config.ident),",
          "86:             'command': self.config.command, 'env': self.config.env, 'cwd': self.config.cwd",
          "87:         }",
          "",
          "[Added Lines]",
          "84:         status_data = {'status': status, 'runner_ident': str(self.config.ident)}",
          "85:         if status == 'starting':",
          "86:             status_data.update({'command': self.config.command, 'env': self.config.env, 'cwd': self.config.cwd})",
          "",
          "---------------"
        ],
        "ansible_runner/streaming.py||ansible_runner/streaming.py": [
          "File: ansible_runner/streaming.py -> ansible_runner/streaming.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "13:     from collections import Mapping",
          "15: import ansible_runner",
          "16: from ansible_runner.loader import ArtifactLoader",
          "17: import ansible_runner.plugins",
          "18: from ansible_runner import utils",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "16: from ansible_runner.exceptions import ConfigurationError",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "44:         self._output.write(",
          "45:             json.dumps({'kwargs': self.kwargs}, cls=UUIDEncoder).encode('utf-8')",
          "46:         )",
          "47:         self._output.flush()",
          "49:         private_data_dir = self.kwargs.get('private_data_dir')",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "48:         self._output.write(b'\\n')",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "93:         self.kwargs.update(self.job_kwargs)",
          "94:         self.kwargs['quiet'] = True",
          "95:         self.kwargs['private_data_dir'] = self.private_data_dir",
          "96:         self.kwargs['status_handler'] = self.status_handler",
          "97:         self.kwargs['event_handler'] = self.event_handler",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "97:         self.kwargs['suppress_ansible_output'] = True",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "142:         settings = kwargs.get('settings')",
          "143:         if settings is None:",
          "145:         self.config = MockConfig(settings)",
          "147:         artifact_dir = kwargs.get('artifact_dir')",
          "",
          "[Removed Lines]",
          "144:             settings = self._loader.load_file('env/settings', Mapping)",
          "",
          "[Added Lines]",
          "147:             try:",
          "148:                 settings = self._loader.load_file('env/settings', Mapping)",
          "149:             except ConfigurationError:",
          "150:                 settings = {}",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "175:                                      'job_events',",
          "176:                                      '{}-{}.json'.format(event_data['counter'],",
          "177:                                                          event_data['uuid']))",
          "179:         if self.event_handler is not None:",
          "180:             should_write = self.event_handler(event_data)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "184:         if 'stdout' in event_data:",
          "185:             print(event_data['stdout'])",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "198:     def run(self):",
          "199:         job_events_path = os.path.join(self.artifact_dir, 'job_events')",
          "200:         if not os.path.exists(job_events_path):",
          "203:         while True:",
          "204:             line = self._input.readline()",
          "",
          "[Removed Lines]",
          "201:             os.mkdir(job_events_path, 0o700)",
          "",
          "[Added Lines]",
          "209:             os.makedirs(job_events_path, 0o700, exist_ok=True)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "139165a4bc5091b7dec73e18cfb05e2cf324745f",
      "candidate_info": {
        "commit_hash": "139165a4bc5091b7dec73e18cfb05e2cf324745f",
        "repo": "ansible/ansible-runner",
        "commit_url": "https://github.com/ansible/ansible-runner/commit/139165a4bc5091b7dec73e18cfb05e2cf324745f",
        "files": [
          "ansible_runner/interface.py",
          "ansible_runner/streaming.py",
          "ansible_runner/utils.py",
          "test/integration/test_transmit_worker_process.py"
        ],
        "message": "Socket test fixups to get to full functionality\n\nCombine the two transmit worker process tests\n  into a single pytest class\n\nPatch up from Zuul tests\n\nMake test a little more brief",
        "before_after_code_files": [
          "ansible_runner/interface.py||ansible_runner/interface.py",
          "ansible_runner/streaming.py||ansible_runner/streaming.py",
          "ansible_runner/utils.py||ansible_runner/utils.py",
          "test/integration/test_transmit_worker_process.py||test/integration/test_transmit_worker_process.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "ansible_runner/interface.py||ansible_runner/interface.py",
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ],
          "candidate": [
            "ansible_runner/interface.py||ansible_runner/interface.py",
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ]
        }
      },
      "candidate_diff": {
        "ansible_runner/interface.py||ansible_runner/interface.py": [
          "File: ansible_runner/interface.py -> ansible_runner/interface.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:     See parameters given to :py:func:`ansible_runner.interface.run`",
          "45:     '''",
          "47:         dump_artifacts(kwargs)",
          "49:     if kwargs.get('streamer'):",
          "",
          "[Removed Lines]",
          "46:     if not kwargs.get('cli_execenv_cmd'):",
          "",
          "[Added Lines]",
          "46:     # If running via the transmit-worker-process method, we must only extract things as read-only",
          "47:     # inside of one of these commands. That could be either transmit or worker.",
          "48:     if not kwargs.get('cli_execenv_cmd') and (kwargs.get('streamer') not in ('worker', 'process')):",
          "",
          "---------------"
        ],
        "ansible_runner/streaming.py||ansible_runner/streaming.py": [
          "File: ansible_runner/streaming.py -> ansible_runner/streaming.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "5: import sys",
          "6: import tempfile",
          "7: import uuid",
          "8: try:",
          "9:     from collections.abc import Mapping",
          "10: except ImportError:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "8: import traceback",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "95:                 line = self._input.readline()",
          "96:                 data = json.loads(line)",
          "97:             except (json.decoder.JSONDecodeError, IOError):",
          "99:                 return self.status, self.rc",
          "101:             if 'kwargs' in data:",
          "102:                 self.job_kwargs = self.update_paths(data['kwargs'])",
          "103:             elif 'zipfile' in data:",
          "104:                 zip_data = self._input.read(data['zipfile'])",
          "107:             elif 'eof' in data:",
          "108:                 break",
          "",
          "[Removed Lines]",
          "98:                 self.status_handler({'status': 'error'}, None)",
          "105:                 if not utils.unstream_dir(zip_data, self.private_data_dir):",
          "106:                     break",
          "",
          "[Added Lines]",
          "99:                 self.status_handler({'status': 'error', 'job_explanation': 'Failed to JSON parse a line from transmit stream.'}, None)",
          "100:                 self.finished_callback(None)  # send eof line",
          "107:                 try:",
          "108:                     utils.unstream_dir(zip_data, self.private_data_dir)",
          "109:                 except Exception:",
          "110:                     self.status_handler({",
          "111:                         'status': 'error',",
          "112:                         'job_explanation': 'Failed to extract private data directory on worker.',",
          "113:                         'result_traceback': traceback.format_exc()",
          "114:                     }, None)",
          "115:                     self.finished_callback(None)  # send eof line",
          "116:                     return self.status, self.rc",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "215:     def artifacts_callback(self, artifacts_data):",
          "216:         zip_data = self._input.read(artifacts_data['zipfile'])",
          "218:             return  # FIXME?",
          "220:         if self.artifacts_handler is not None:",
          "",
          "[Removed Lines]",
          "217:         if not utils.unstream_dir(zip_data, self.artifact_dir):",
          "",
          "[Added Lines]",
          "228:         try:",
          "229:             utils.unstream_dir(zip_data, self.artifact_dir)",
          "230:         except Exception:",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "230:                 line = self._input.readline()",
          "231:                 data = json.loads(line)",
          "232:             except (json.decoder.JSONDecodeError, IOError):",
          "236:             if 'status' in data:",
          "237:                 self.status_callback(data)",
          "",
          "[Removed Lines]",
          "233:                 self.status_callback({'status': 'error'})",
          "234:                 return self.status, self.rc",
          "",
          "[Added Lines]",
          "246:                 self.status_callback({'status': 'error', 'job_explanation': 'Failed to JSON parse a line from worker stream.'})",
          "247:                 break",
          "",
          "---------------"
        ],
        "ansible_runner/utils.py||ansible_runner/utils.py": [
          "File: ansible_runner/utils.py -> ansible_runner/utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "102: def unstream_dir(data, directory):",
          "103:     buf = BytesIO(data)",
          "119: def dump_artifact(obj, path, filename=None):",
          "",
          "[Removed Lines]",
          "104:     try:",
          "105:         with zipfile.ZipFile(buf, 'r') as archive:",
          "106:             # Fancy extraction in order to preserve permissions",
          "107:             # https://www.burgundywall.com/post/preserving-file-perms-with-python-zipfile-module",
          "108:             for info in archive.infolist():",
          "109:                 archive.extract(info.filename, path=directory)",
          "110:                 out_path = os.path.join(directory, info.filename)",
          "111:                 perm = info.external_attr >> 16",
          "112:                 os.chmod(out_path, perm)",
          "113:     except zipfile.BadZipFile:",
          "114:         return False",
          "116:     return True",
          "",
          "[Added Lines]",
          "103:     # NOTE: caller needs to process exceptions",
          "105:     with zipfile.ZipFile(buf, 'r') as archive:",
          "106:         # Fancy extraction in order to preserve permissions",
          "107:         # https://www.burgundywall.com/post/preserving-file-perms-with-python-zipfile-module",
          "108:         for info in archive.infolist():",
          "109:             archive.extract(info.filename, path=directory)",
          "110:             out_path = os.path.join(directory, info.filename)",
          "111:             perm = info.external_attr >> 16",
          "112:             os.chmod(out_path, perm)",
          "",
          "---------------"
        ],
        "test/integration/test_transmit_worker_process.py||test/integration/test_transmit_worker_process.py": [
          "File: test/integration/test_transmit_worker_process.py -> test/integration/test_transmit_worker_process.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1: import io",
          "2: import os",
          "3: import socket",
          "5: import time",
          "8: import pytest",
          "9: import json",
          "",
          "[Removed Lines]",
          "4: import concurrent",
          "6: import traceback",
          "",
          "[Added Lines]",
          "4: import concurrent.futures",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "14: import ansible_runner.interface  # AWX import pattern",
          "199: def test_missing_private_dir_transmit(tmpdir):",
          "",
          "[Removed Lines]",
          "17: @pytest.mark.parametrize(\"job_type\", ['run', 'adhoc'])",
          "18: def test_remote_job_interface(tmpdir, test_data_dir, job_type):",
          "19:     transmit_dir = os.path.join(test_data_dir, 'debug')",
          "20:     worker_dir = str(tmpdir.mkdir('for_worker'))",
          "21:     process_dir = str(tmpdir.mkdir('for_process'))",
          "23:     outgoing_buffer = io.BytesIO()",
          "25:     # Intended AWX and Tower use case",
          "26:     if job_type == 'run':",
          "27:         job_kwargs = dict(playbook='debug.yml')",
          "28:     else:",
          "29:         job_kwargs = dict(module='setup', host_pattern='localhost')",
          "30:     # also test use of user env vars",
          "31:     job_kwargs['envvars'] = dict(MY_ENV_VAR='bogus')",
          "33:     transmitter = Transmitter(",
          "34:         _output=outgoing_buffer,",
          "35:         private_data_dir=transmit_dir,",
          "37:     )",
          "39:     for key, value in job_kwargs.items():",
          "40:         assert transmitter.kwargs.get(key, '') == value",
          "42:     status, rc = transmitter.run()",
          "43:     assert rc in (None, 0)",
          "44:     assert status == 'unstarted'",
          "46:     outgoing_buffer.seek(0)  # rewind so we can start reading",
          "48:     sent = outgoing_buffer.getvalue()",
          "49:     assert sent  # should not be blank at least",
          "50:     assert b'zipfile' in sent",
          "52:     incoming_buffer = io.BytesIO()",
          "54:     worker = Worker(",
          "55:         _input=outgoing_buffer,",
          "56:         _output=incoming_buffer,",
          "57:         private_data_dir=worker_dir",
          "58:     )",
          "59:     worker.run()",
          "61:     assert set(os.listdir(worker_dir)) == {'artifacts', 'inventory', 'project', 'env'}, outgoing_buffer.getvalue()",
          "63:     incoming_buffer.seek(0)  # again, be kind, rewind",
          "65:     processor = Processor(",
          "66:         _input=incoming_buffer,",
          "67:         private_data_dir=process_dir",
          "68:     )",
          "69:     processor.run()",
          "71:     assert set(os.listdir(process_dir)) == {'artifacts',}, outgoing_buffer.getvalue()",
          "73:     events_dir = os.path.join(process_dir, 'artifacts', 'job_events')",
          "74:     events = []",
          "75:     for file in os.listdir(events_dir):",
          "76:         with open(os.path.join(events_dir, file), 'r') as f:",
          "77:             if file in ('status', 'rc'):",
          "78:                 continue",
          "79:             content = f.read()",
          "80:             events.append(json.loads(content))",
          "81:     stdout = '\\n'.join(event['stdout'] for event in events)",
          "83:     if job_type == 'run':",
          "84:         assert 'Hello world!' in stdout",
          "85:     else:",
          "86:         assert '\"ansible_facts\"' in stdout",
          "89: @pytest.mark.parametrize(\"job_type\", ['run', 'adhoc'])",
          "90: def test_remote_job_by_sockets(tmpdir, test_data_dir, job_type):",
          "91:     \"\"\"This test case is intended to be close to how the AWX use case works",
          "92:     the process interacts with receptorctl with sockets",
          "93:     sockets are used here, but worker is manually called instead of invoked by receptor",
          "94:     \"\"\"",
          "95:     transmit_dir = os.path.join(test_data_dir, 'debug')",
          "96:     worker_dir = str(tmpdir.mkdir('for_worker'))",
          "97:     process_dir = str(tmpdir.mkdir('for_process'))",
          "99:     # Intended AWX and Tower use case",
          "100:     if job_type == 'run':",
          "101:         job_kwargs = dict(playbook='debug.yml')",
          "102:     else:",
          "103:         job_kwargs = dict(module='setup', host_pattern='localhost')",
          "104:     # also test use of user env vars",
          "105:     job_kwargs['envvars'] = dict(MY_ENV_VAR='bogus')",
          "108:     def transmit_method(transmit_sockfile_write):",
          "109:         ansible_runner.interface.run(",
          "110:             streamer='transmit',",
          "111:             _output=transmit_sockfile_write,",
          "112:             private_data_dir=transmit_dir,",
          "114:         )",
          "117:     def worker_method(transmit_sockfile_read, results_sockfile_write):",
          "118:         # ThreadPoolExecutor does not handle tracebacks nicely",
          "119:         try:",
          "120:             ansible_runner.interface.run(",
          "121:                 streamer='worker',",
          "122:                 _input=transmit_sockfile_read,",
          "123:                 _output=results_sockfile_write,",
          "124:                 private_data_dir=worker_dir,",
          "126:             )",
          "127:         except Exception:",
          "128:             traceback.print_exc()",
          "129:             raise",
          "132:     def process_method(results_sockfile_read):",
          "133:         ansible_runner.interface.run(",
          "134:             streamer='process',",
          "135:             quiet=True,",
          "136:             _input=results_sockfile_read,",
          "137:             private_data_dir=process_dir,",
          "139:         )",
          "141:     transmit_socket_write, transmit_socket_read = socket.socketpair()",
          "142:     results_socket_write, results_socket_read = socket.socketpair()",
          "144:     with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:",
          "145:         transmit_future = executor.submit(transmit_method, transmit_socket_write.makefile('wb'))",
          "146:         # we will not make assertions on, or manage, worker directly",
          "147:         worker_future = executor.submit(worker_method, transmit_socket_read.makefile('rb'), results_socket_write.makefile('wb'))",
          "149:         while True:",
          "150:             # In AWX this loop is where the cancel callback is checked, but here we just check transmit",
          "151:             transmit_finished = transmit_future.done()",
          "152:             if transmit_finished:",
          "153:                 break",
          "154:             time.sleep(0.05)",
          "156:         process_future = executor.submit(process_method, results_socket_read.makefile('rb'))",
          "158:         while True:",
          "159:             worker_finished = worker_future.done()",
          "160:             if worker_finished:",
          "161:                 break",
          "162:             time.sleep(0.05)",
          "164:         while True:",
          "165:             # this is the second cancel loop, which is still pretty similar to the first",
          "166:             process_finished = process_future.done()",
          "167:             if process_finished:",
          "168:                 # process_result = process_future.result()",
          "169:                 break",
          "170:             time.sleep(0.05)",
          "173:     # close all the sockets",
          "174:     transmit_socket_write.close()",
          "175:     transmit_socket_read.close()",
          "176:     results_socket_write.close()",
          "177:     results_socket_read.close()",
          "179:     assert set(os.listdir(worker_dir)) == {'artifacts', 'inventory', 'project', 'env'}",
          "181:     assert set(os.listdir(process_dir)) == {'artifacts',}",
          "183:     events_dir = os.path.join(process_dir, 'artifacts', 'job_events')",
          "184:     events = []",
          "185:     for file in os.listdir(events_dir):",
          "186:         with open(os.path.join(events_dir, file), 'r') as f:",
          "187:             if file in ('status', 'rc'):",
          "188:                 continue",
          "189:             content = f.read()",
          "190:             events.append(json.loads(content))",
          "191:     stdout = '\\n'.join(event['stdout'] for event in events)",
          "193:     if job_type == 'run':",
          "194:         assert 'Hello world!' in stdout",
          "195:     else:",
          "196:         assert '\"ansible_facts\"' in stdout",
          "",
          "[Added Lines]",
          "16: class TestStreamingUsage:",
          "18:     @pytest.fixture(autouse=True)",
          "19:     def reset_self_props(self):",
          "20:         self.status_data = None",
          "22:     def status_handler(self, status_data, runner_config=None):",
          "23:         self.status_data = status_data",
          "25:     def get_job_kwargs(self, job_type):",
          "26:         \"\"\"For this test scenaro, the ansible-runner interface kwargs\"\"\"",
          "27:         if job_type == 'run':",
          "28:             job_kwargs = dict(playbook='debug.yml')",
          "29:         else:",
          "30:             job_kwargs = dict(module='setup', host_pattern='localhost')",
          "31:         # also test use of user env vars",
          "32:         job_kwargs['envvars'] = dict(MY_ENV_VAR='bogus')",
          "33:         return job_kwargs",
          "35:     def check_artifacts(self, process_dir, job_type):",
          "37:         assert set(os.listdir(process_dir)) == {'artifacts',}",
          "39:         events_dir = os.path.join(process_dir, 'artifacts', 'job_events')",
          "40:         events = []",
          "41:         for file in os.listdir(events_dir):",
          "42:             with open(os.path.join(events_dir, file), 'r') as f:",
          "43:                 if file in ('status', 'rc'):",
          "44:                     continue",
          "45:                 content = f.read()",
          "46:                 events.append(json.loads(content))",
          "47:         stdout = '\\n'.join(event['stdout'] for event in events)",
          "49:         if job_type == 'run':",
          "50:             assert 'Hello world!' in stdout",
          "51:         else:",
          "52:             assert '\"ansible_facts\"' in stdout",
          "54:     @pytest.mark.parametrize(\"job_type\", ['run', 'adhoc'])",
          "55:     def test_remote_job_interface(self, tmpdir, test_data_dir, job_type):",
          "56:         transmit_dir = os.path.join(test_data_dir, 'debug')",
          "57:         worker_dir = str(tmpdir.mkdir('for_worker'))",
          "58:         process_dir = str(tmpdir.mkdir('for_process'))",
          "59:         job_kwargs = self.get_job_kwargs(job_type)",
          "61:         outgoing_buffer = io.BytesIO()",
          "63:         transmitter = Transmitter(_output=outgoing_buffer, private_data_dir=transmit_dir, **job_kwargs)",
          "65:         for key, value in job_kwargs.items():",
          "66:             assert transmitter.kwargs.get(key, '') == value",
          "68:         status, rc = transmitter.run()",
          "69:         assert rc in (None, 0)",
          "70:         assert status == 'unstarted'",
          "72:         outgoing_buffer.seek(0)  # rewind so we can start reading",
          "74:         sent = outgoing_buffer.getvalue()",
          "75:         assert sent  # should not be blank at least",
          "76:         assert b'zipfile' in sent",
          "78:         incoming_buffer = io.BytesIO()",
          "80:         worker = Worker(_input=outgoing_buffer, _output=incoming_buffer, private_data_dir=worker_dir)",
          "81:         worker.run()",
          "83:         assert set(os.listdir(worker_dir)) == {'artifacts', 'inventory', 'project', 'env'}, outgoing_buffer.getvalue()",
          "85:         incoming_buffer.seek(0)  # again, be kind, rewind",
          "87:         processor = Processor(_input=incoming_buffer, private_data_dir=process_dir)",
          "88:         processor.run()",
          "90:         self.check_artifacts(process_dir, job_type)",
          "93:     @pytest.mark.parametrize(\"job_type\", ['run', 'adhoc'])",
          "94:     def test_remote_job_by_sockets(self, tmpdir, test_data_dir, container_runtime_installed, job_type):",
          "95:         \"\"\"This test case is intended to be close to how the AWX use case works",
          "96:         the process interacts with receptorctl with sockets",
          "97:         sockets are used here, but worker is manually called instead of invoked by receptor",
          "98:         \"\"\"",
          "99:         transmit_dir = os.path.join(test_data_dir, 'debug')",
          "100:         worker_dir = str(tmpdir.mkdir('for_worker'))",
          "101:         process_dir = str(tmpdir.mkdir('for_process'))",
          "102:         job_kwargs = self.get_job_kwargs(job_type)",
          "104:         def transmit_method(transmit_sockfile_write):",
          "105:             return ansible_runner.interface.run(",
          "106:                 streamer='transmit',",
          "107:                 _output=transmit_sockfile_write,",
          "108:                 private_data_dir=transmit_dir, **job_kwargs)",
          "110:         def worker_method(transmit_sockfile_read, results_sockfile_write):",
          "111:             return ansible_runner.interface.run(",
          "112:                 streamer='worker',",
          "113:                 _input=transmit_sockfile_read, _output=results_sockfile_write,",
          "114:                 private_data_dir=worker_dir, **job_kwargs)",
          "116:         def process_method(results_sockfile_read):",
          "117:             return ansible_runner.interface.run(",
          "118:                 streamer='process', quiet=True,",
          "119:                 _input=results_sockfile_read,",
          "120:                 private_data_dir=process_dir, status_handler=self.status_handler, **job_kwargs)",
          "122:         transmit_socket_write, transmit_socket_read = socket.socketpair()",
          "123:         results_socket_write, results_socket_read = socket.socketpair()",
          "125:         with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:",
          "126:             transmit_future = executor.submit(transmit_method, transmit_socket_write.makefile('wb'))",
          "127:             # In real AWX implementation, worker is done via receptorctl",
          "128:             executor.submit(worker_method, transmit_socket_read.makefile('rb'), results_socket_write.makefile('wb'))",
          "130:             while True:",
          "131:                 if transmit_future.done():",
          "132:                     break",
          "133:                 time.sleep(0.05)  # additionally, AWX calls cancel_callback()",
          "135:             res = transmit_future.result()",
          "136:             assert res.rc in (None, 0)",
          "137:             assert res.status == 'unstarted'",
          "139:             process_future = executor.submit(process_method, results_socket_read.makefile('rb'))",
          "141:             while True:",
          "142:                 if process_future.done():",
          "143:                     break",
          "144:                 time.sleep(0.05)  # additionally, AWX calls cancel_callback()",
          "146:         for s in (transmit_socket_write, transmit_socket_read, results_socket_write, results_socket_read):",
          "147:             s.close()",
          "149:         assert self.status_data is not None",
          "150:         if 'result_traceback' in self.status_data:",
          "151:             raise Exception(self.status_data['result_traceback'])",
          "152:         assert self.status_data.get('status') == 'successful'",
          "154:         assert set(os.listdir(worker_dir)) == {'artifacts', 'inventory', 'project', 'env'}",
          "156:         self.check_artifacts(process_dir, job_type)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "225:         private_data_dir=worker_dir,",
          "226:     )",
          "227:     sent = outgoing_buffer.getvalue()",
          "231: def test_unparsable_private_dir_worker(tmpdir):",
          "",
          "[Removed Lines]",
          "228:     assert b'\"status\": \"failed\"' in sent",
          "",
          "[Added Lines]",
          "188:     assert b'\"status\": \"error\"' in sent",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "be7ed61bd2c73723e239a3faa530975da4e4f399",
      "candidate_info": {
        "commit_hash": "be7ed61bd2c73723e239a3faa530975da4e4f399",
        "repo": "ansible/ansible-runner",
        "commit_url": "https://github.com/ansible/ansible-runner/commit/be7ed61bd2c73723e239a3faa530975da4e4f399",
        "files": [
          "ansible_runner/runner.py",
          "ansible_runner/streaming.py"
        ],
        "message": "Add new callback functions to stream output and artifacts\n\nback to a control instance of runner.",
        "before_after_code_files": [
          "ansible_runner/runner.py||ansible_runner/runner.py",
          "ansible_runner/streaming.py||ansible_runner/streaming.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "ansible_runner/runner.py||ansible_runner/runner.py",
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ],
          "candidate": [
            "ansible_runner/runner.py||ansible_runner/runner.py",
            "ansible_runner/streaming.py||ansible_runner/streaming.py"
          ]
        }
      },
      "candidate_diff": {
        "ansible_runner/runner.py||ansible_runner/runner.py": [
          "File: ansible_runner/runner.py -> ansible_runner/runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: class Runner(object):",
          "31:         self.config = config",
          "32:         self.cancel_callback = cancel_callback",
          "33:         self.event_handler = event_handler",
          "34:         self.finished_callback = finished_callback",
          "35:         self.status_handler = status_handler",
          "36:         self.canceled = False",
          "",
          "[Removed Lines]",
          "29:     def __init__(self, config, cancel_callback=None, remove_partials=True,",
          "30:                  event_handler=None, finished_callback=None, status_handler=None):",
          "",
          "[Added Lines]",
          "29:     def __init__(self, config, cancel_callback=None, remove_partials=True, event_handler=None,",
          "30:                  artifacts_callback=None, finished_callback=None, status_handler=None):",
          "34:         self.artifacts_callback = artifacts_callback",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "283:                 logger.error('Failed to delete cgroup: {}'.format(stderr))",
          "284:                 raise RuntimeError('Failed to delete cgroup: {}'.format(stderr))",
          "286:         if self.finished_callback is not None:",
          "287:             try:",
          "288:                 self.finished_callback(self)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "287:         if self.artifacts_callback is not None:",
          "288:             try:",
          "289:                 self.artifacts_callback(self.config.artifact_dir)",
          "290:             except Exception as e:",
          "291:                 raise CallbackError(\"Exception in Artifact Callback: {}\".format(e))",
          "",
          "---------------"
        ],
        "ansible_runner/streaming.py||ansible_runner/streaming.py": [
          "File: ansible_runner/streaming.py -> ansible_runner/streaming.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: import io",
          "2: import json",
          "3: import os",
          "4: import zipfile",
          "7: class StreamWorker(object):",
          "8:     def __init__(self, control_out):",
          "9:         self.control_out = control_out",
          "11:     def status_handler(self, status):",
          "12:         json.dump(status, self.control_out)",
          "13:         self.control_out.flush()",
          "15:     def event_handler(self, event_data):",
          "16:         json.dump(event_data, self.control_out)",
          "17:         self.control_out.flush()",
          "19:     def artifacts_callback(self, artifact_dir):",
          "20:         buf = io.BytesIO()",
          "21:         with zipfile.ZipFile(buf, 'w', compression=zipfile.ZIP_DEFLATED, allowZip64=True) as archive:",
          "22:             for dirpath, dirs, files in os.walk(artifact_dir):",
          "23:                 relpath = os.path.relpath(dirpath, artifact_dir)",
          "24:                 if relpath == \".\":",
          "25:                     relpath = \"\"",
          "26:                 for fname in files:",
          "27:                     archive.write(os.path.join(dirpath, fname), arcname=os.path.join(relpath, fname))",
          "28:             archive.close()",
          "30:         self.control_out.write(buf.getvalue())",
          "31:         self.control_out.flush()",
          "32:         self.control_out.close()",
          "",
          "---------------"
        ]
      }
    }
  ]
}