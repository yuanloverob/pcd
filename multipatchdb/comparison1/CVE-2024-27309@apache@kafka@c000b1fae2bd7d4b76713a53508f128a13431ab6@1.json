{
  "cve_id": "CVE-2024-27309",
  "cve_desc": "While an Apache Kafka cluster is being migrated from ZooKeeper mode to KRaft mode, in some cases ACLs will not be correctly enforced.\n\nTwo preconditions are needed to trigger the bug:\n1. The administrator decides to remove an ACL\n2. The resource associated with the removed ACL continues to have two or more other ACLs associated with it after the removal.\n\nWhen those two preconditions are met, Kafka will treat the resource as if it had only one ACL associated with it after the removal, rather than the two or more that would be correct.\n\nThe incorrect condition is cleared by removing all brokers in ZK mode, or by adding a new ACL to the affected resource. Once the migration is completed, there is no metadata loss (the ACLs all remain).\n\nThe full impact depends on the ACLs in use. If only ALLOW ACLs were configured during the migration, the impact would be limited to availability impact. if DENY ACLs were configured, the impact could include confidentiality and integrity impact depending on the ACLs configured, as the DENY ACLs might be ignored due to this vulnerability during the migration period.",
  "repo": "apache/kafka",
  "patch_hash": "c000b1fae2bd7d4b76713a53508f128a13431ab6",
  "patch_info": {
    "commit_hash": "c000b1fae2bd7d4b76713a53508f128a13431ab6",
    "repo": "apache/kafka",
    "commit_url": "https://github.com/apache/kafka/commit/c000b1fae2bd7d4b76713a53508f128a13431ab6",
    "files": [
      "core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala",
      "core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala",
      "core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala",
      "metadata/src/main/java/org/apache/kafka/image/AclsDelta.java",
      "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
      "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java",
      "metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java"
    ],
    "message": "MINOR: Fix some MetadataDelta handling issues during ZK migration (#15327)\n\nReviewers: Colin P. McCabe <cmccabe@apache.org>",
    "before_after_code_files": [
      "core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala",
      "core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala",
      "core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala",
      "metadata/src/main/java/org/apache/kafka/image/AclsDelta.java||metadata/src/main/java/org/apache/kafka/image/AclsDelta.java",
      "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
      "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java",
      "metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java||metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java"
    ]
  },
  "patch_diff": {
    "core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala": [
      "File: core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala -> core/src/test/scala/unit/kafka/zk/migration/ZkAclMigrationClientTest.scala",
      "--- Hunk 1 ---",
      "[Context before]",
      "21: import kafka.utils.TestUtils",
      "22: import org.apache.kafka.common.Uuid",
      "23: import org.apache.kafka.common.acl._",
      "25: import org.apache.kafka.common.resource.{PatternType, ResourcePattern, ResourcePatternFilter, ResourceType}",
      "26: import org.apache.kafka.common.security.auth.KafkaPrincipal",
      "27: import org.apache.kafka.common.utils.SecurityUtils",
      "28: import org.apache.kafka.image.{MetadataDelta, MetadataImage, MetadataProvenance}",
      "29: import org.apache.kafka.metadata.migration.KRaftMigrationZkWriter",
      "30: import org.apache.kafka.server.common.ApiMessageAndVersion",
      "32: import org.junit.jupiter.api.Test",
      "34: import scala.collection.mutable",
      "",
      "[Removed Lines]",
      "24: import org.apache.kafka.common.metadata.AccessControlEntryRecord",
      "31: import org.junit.jupiter.api.Assertions.{assertEquals, assertTrue}",
      "",
      "[Added Lines]",
      "24: import org.apache.kafka.common.metadata.{AccessControlEntryRecord, RemoveAccessControlEntryRecord}",
      "31: import org.junit.jupiter.api.Assertions.{assertEquals, assertTrue, fail}",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "169:     val image = delta.apply(MetadataProvenance.EMPTY)",
      "173:     kraftMigrationZkWriter.handleSnapshot(image, (_, _, operation) => { migrationState = operation.apply(migrationState) })",
      "",
      "[Removed Lines]",
      "172:     val kraftMigrationZkWriter = new KRaftMigrationZkWriter(migrationClient)",
      "",
      "[Added Lines]",
      "172:     val kraftMigrationZkWriter = new KRaftMigrationZkWriter(migrationClient, fail(_))",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "189:         AclPermissionType.fromCode(acl1Resource3.permissionType())),",
      "190:       resource3AclsInZk.head.ace)",
      "191:   }",
      "192: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "193:   def user(user: String): String = {",
      "194:     new KafkaPrincipal(KafkaPrincipal.USER_TYPE, user).toString",
      "195:   }",
      "197:   def acl(resourceName: String,",
      "198:           resourceType: ResourceType,",
      "199:           resourcePattern: PatternType,",
      "200:           principal: String,",
      "201:           host: String = \"*\",",
      "202:           operation: AclOperation = AclOperation.READ,",
      "203:           permissionType: AclPermissionType = AclPermissionType.ALLOW",
      "204:   ): AccessControlEntryRecord = {",
      "205:     new AccessControlEntryRecord()",
      "206:       .setId(Uuid.randomUuid())",
      "207:       .setHost(host)",
      "208:       .setOperation(operation.code())",
      "209:       .setPrincipal(principal)",
      "210:       .setPermissionType(permissionType.code())",
      "211:       .setPatternType(resourcePattern.code())",
      "212:       .setResourceName(resourceName)",
      "213:       .setResourceType(resourceType.code())",
      "214:   }",
      "216:   @Test",
      "217:   def testDeleteOneAclOfMany(): Unit = {",
      "218:     zkClient.createAclPaths()",
      "219:     val topicName = \"topic-\" + Uuid.randomUuid()",
      "220:     val resource = new ResourcePattern(ResourceType.TOPIC, topicName, PatternType.LITERAL)",
      "223:     val delta = new MetadataDelta(MetadataImage.EMPTY)",
      "224:     val acl1 = acl(topicName, ResourceType.TOPIC, PatternType.LITERAL, user(\"alice\"))",
      "225:     val acl2 = acl(topicName, ResourceType.TOPIC, PatternType.LITERAL, user(\"bob\"))",
      "226:     val acl3 = acl(topicName, ResourceType.TOPIC, PatternType.LITERAL, user(\"carol\"))",
      "227:     delta.replay(acl1)",
      "228:     delta.replay(acl2)",
      "229:     delta.replay(acl3)",
      "230:     val image = delta.apply(MetadataProvenance.EMPTY)",
      "233:     val errorLogs = mutable.Buffer[String]()",
      "234:     val kraftMigrationZkWriter = new KRaftMigrationZkWriter(migrationClient, errorLogs.append)",
      "235:     kraftMigrationZkWriter.handleSnapshot(image, (_, _, operation) => {",
      "236:       migrationState = operation.apply(migrationState)",
      "237:     })",
      "240:     val aclsInZk = zkClient.getVersionedAclsForResource(resource).acls",
      "241:     assertEquals(3, aclsInZk.size)",
      "244:     val delta2 = new MetadataDelta.Builder()",
      "245:       .setImage(image)",
      "246:       .build()",
      "247:     delta2.replay(new RemoveAccessControlEntryRecord().setId(acl3.id()))",
      "248:     val image2 = delta2.apply(MetadataProvenance.EMPTY)",
      "249:     kraftMigrationZkWriter.handleDelta(image, image2, delta2, (_, _, operation) => {",
      "250:       migrationState = operation.apply(migrationState)",
      "251:     })",
      "254:     val aclsInZk2 = zkClient.getVersionedAclsForResource(resource).acls",
      "255:     assertEquals(2, aclsInZk2.size)",
      "256:     assertEquals(0, errorLogs.size)",
      "259:     val acl4 = acl(topicName, ResourceType.TOPIC, PatternType.LITERAL, user(\"carol\"))",
      "260:     delta2.replay(acl4)",
      "261:     val image3 = delta2.apply(MetadataProvenance.EMPTY)",
      "265:     kraftMigrationZkWriter.handleDelta(image3, image3, delta2, (_, _, operation) => {",
      "266:       migrationState = operation.apply(migrationState)",
      "267:     })",
      "269:     val aclsInZk3 = zkClient.getVersionedAclsForResource(resource).acls",
      "270:     assertEquals(3, aclsInZk3.size)",
      "271:     assertEquals(1, errorLogs.size)",
      "272:     assertEquals(s\"Cannot delete ACL ${acl3.id()} from ZK since it is missing from previous AclImage\", errorLogs.head)",
      "273:   }",
      "275:   @Test",
      "276:   def testAclUpdateAndDelete(): Unit = {",
      "277:     zkClient.createAclPaths()",
      "278:     val errorLogs = mutable.Buffer[String]()",
      "279:     val kraftMigrationZkWriter = new KRaftMigrationZkWriter(migrationClient, errorLogs.append)",
      "281:     val topicName = \"topic-\" + Uuid.randomUuid()",
      "282:     val otherName = \"other-\" + Uuid.randomUuid()",
      "283:     val literalResource = new ResourcePattern(ResourceType.TOPIC, topicName, PatternType.LITERAL)",
      "284:     val prefixedResource = new ResourcePattern(ResourceType.TOPIC, topicName, PatternType.PREFIXED)",
      "285:     val otherResource = new ResourcePattern(ResourceType.TOPIC, otherName, PatternType.LITERAL)",
      "288:     val acl1 = acl(topicName, ResourceType.TOPIC, PatternType.LITERAL, user(\"alice\"))",
      "289:     val acl2 = acl(topicName, ResourceType.TOPIC, PatternType.LITERAL, user(\"bob\"))",
      "290:     val acl3 = acl(topicName, ResourceType.TOPIC, PatternType.LITERAL, user(\"carol\"))",
      "291:     val acl4 = acl(topicName, ResourceType.TOPIC, PatternType.LITERAL, user(\"dave\"))",
      "293:     val delta1 = new MetadataDelta(MetadataImage.EMPTY)",
      "294:     delta1.replay(acl1)",
      "295:     delta1.replay(acl2)",
      "296:     delta1.replay(acl3)",
      "297:     delta1.replay(acl4)",
      "299:     val image1 = delta1.apply(MetadataProvenance.EMPTY)",
      "300:     kraftMigrationZkWriter.handleDelta(MetadataImage.EMPTY, image1, delta1, (_, _, operation) => {",
      "301:       migrationState = operation.apply(migrationState)",
      "302:     })",
      "303:     assertEquals(4, zkClient.getVersionedAclsForResource(literalResource).acls.size)",
      "304:     assertEquals(0, zkClient.getVersionedAclsForResource(prefixedResource).acls.size)",
      "305:     assertEquals(0, zkClient.getVersionedAclsForResource(otherResource).acls.size)",
      "306:     assertEquals(0, errorLogs.size)",
      "308:     val acl5 = acl(topicName, ResourceType.TOPIC, PatternType.PREFIXED, user(\"alice\"))",
      "309:     val acl6 = acl(topicName, ResourceType.TOPIC, PatternType.PREFIXED, user(\"bob\"))",
      "310:     val acl7 = acl(otherName, ResourceType.TOPIC, PatternType.LITERAL, user(\"carol\"))",
      "311:     val acl8 = acl(otherName, ResourceType.TOPIC, PatternType.LITERAL, user(\"dave\"))",
      "314:     val delta2 = new MetadataDelta.Builder().setImage(image1).build()",
      "315:     delta2.replay(acl5)",
      "316:     delta2.replay(acl6)",
      "317:     delta2.replay(acl7)",
      "318:     delta2.replay(acl8)",
      "319:     delta2.replay(new RemoveAccessControlEntryRecord().setId(acl1.id()))",
      "321:     val image2 = delta2.apply(MetadataProvenance.EMPTY)",
      "322:     kraftMigrationZkWriter.handleDelta(image1, image2, delta2, (_, _, operation) => {",
      "323:       migrationState = operation.apply(migrationState)",
      "324:     })",
      "325:     assertEquals(3, zkClient.getVersionedAclsForResource(literalResource).acls.size)",
      "326:     assertEquals(2, zkClient.getVersionedAclsForResource(prefixedResource).acls.size)",
      "327:     assertEquals(2, zkClient.getVersionedAclsForResource(otherResource).acls.size)",
      "328:     assertEquals(0, errorLogs.size)",
      "331:     val acl9 = acl(otherName, ResourceType.TOPIC, PatternType.LITERAL, user(\"eve\"))",
      "332:     val delta3 = new MetadataDelta.Builder().setImage(image2).build()",
      "333:     delta3.replay(acl1)",
      "334:     delta3.replay(new RemoveAccessControlEntryRecord().setId(acl2.id()))",
      "335:     delta3.replay(new RemoveAccessControlEntryRecord().setId(acl5.id()))",
      "336:     delta3.replay(new RemoveAccessControlEntryRecord().setId(acl6.id()))",
      "337:     delta3.replay(acl9)",
      "339:     val image3 = delta3.apply(MetadataProvenance.EMPTY)",
      "340:     kraftMigrationZkWriter.handleDelta(image2, image3, delta3, (_, _, operation) => {",
      "341:       migrationState = operation.apply(migrationState)",
      "342:     })",
      "343:     assertEquals(3, zkClient.getVersionedAclsForResource(literalResource).acls.size)",
      "344:     assertEquals(0, zkClient.getVersionedAclsForResource(prefixedResource).acls.size)",
      "345:     assertEquals(3, zkClient.getVersionedAclsForResource(otherResource).acls.size)",
      "346:     assertEquals(0, errorLogs.size)",
      "347:   }",
      "",
      "---------------"
    ],
    "core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala": [
      "File: core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala -> core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala",
      "--- Hunk 1 ---",
      "[Context before]",
      "40: import org.apache.kafka.server.common.ApiMessageAndVersion",
      "41: import org.apache.kafka.server.config.ConfigType",
      "42: import org.apache.kafka.server.util.MockRandom",
      "44: import org.junit.jupiter.api.Test",
      "46: import java.util",
      "",
      "[Removed Lines]",
      "43: import org.junit.jupiter.api.Assertions.{assertEquals, assertTrue}",
      "",
      "[Added Lines]",
      "43: import org.junit.jupiter.api.Assertions.{assertEquals, assertTrue, fail}",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "326:     val image = delta.apply(MetadataProvenance.EMPTY)",
      "330:     kraftMigrationZkWriter.handleSnapshot(image, (_, _, operation) => {",
      "331:       migrationState = operation.apply(migrationState)",
      "332:     })",
      "",
      "[Removed Lines]",
      "329:     val kraftMigrationZkWriter = new KRaftMigrationZkWriter(migrationClient)",
      "",
      "[Added Lines]",
      "329:     val kraftMigrationZkWriter = new KRaftMigrationZkWriter(migrationClient, fail(_))",
      "",
      "---------------"
    ],
    "core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala": [
      "File: core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala -> core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala",
      "--- Hunk 1 ---",
      "[Context before]",
      "318:   @Test",
      "319:   def testTopicAndBrokerConfigsMigrationWithSnapshots(): Unit = {",
      "323:     val topicName = \"testTopic\"",
      "",
      "[Removed Lines]",
      "320:     val kraftWriter = new KRaftMigrationZkWriter(migrationClient)",
      "",
      "[Added Lines]",
      "320:     val kraftWriter = new KRaftMigrationZkWriter(migrationClient, fail(_))",
      "",
      "---------------"
    ],
    "metadata/src/main/java/org/apache/kafka/image/AclsDelta.java||metadata/src/main/java/org/apache/kafka/image/AclsDelta.java": [
      "File: metadata/src/main/java/org/apache/kafka/image/AclsDelta.java -> metadata/src/main/java/org/apache/kafka/image/AclsDelta.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "25: import org.apache.kafka.server.common.MetadataVersion;",
      "27: import java.util.HashMap;",
      "29: import java.util.LinkedHashMap;",
      "30: import java.util.Map;",
      "31: import java.util.Map.Entry;",
      "32: import java.util.Optional;",
      "34: import java.util.stream.Collectors;",
      "",
      "[Removed Lines]",
      "28: import java.util.HashSet;",
      "33: import java.util.Set;",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "40: public final class AclsDelta {",
      "41:     private final AclsImage image;",
      "42:     private final Map<Uuid, Optional<StandardAcl>> changes = new LinkedHashMap<>();",
      "45:     public AclsDelta(AclsImage image) {",
      "46:         this.image = image;",
      "",
      "[Removed Lines]",
      "43:     private final Set<StandardAcl> deleted = new HashSet<>();",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "56:         return changes;",
      "57:     }",
      "68:     void finishSnapshot() {",
      "69:         for (Entry<Uuid, StandardAcl> entry : image.acls().entrySet()) {",
      "70:             if (!changes.containsKey(entry.getKey())) {",
      "",
      "[Removed Lines]",
      "64:     public Set<StandardAcl> deleted() {",
      "65:         return deleted;",
      "66:     }",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "93:     public void replay(RemoveAccessControlEntryRecord record) {",
      "94:         if (image.acls().containsKey(record.id())) {",
      "95:             changes.put(record.id(), Optional.empty());",
      "97:         } else if (changes.containsKey(record.id())) {",
      "98:             changes.remove(record.id());",
      "",
      "[Removed Lines]",
      "96:             deleted.add(image.acls().get(record.id()));",
      "",
      "[Added Lines]",
      "[None]",
      "",
      "---------------"
    ],
    "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java": [
      "File: metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "134:         this.time = time;",
      "135:         LogContext logContext = new LogContext(\"[KRaftMigrationDriver id=\" + nodeId + \"] \");",
      "136:         this.controllerMetrics = controllerMetrics;",
      "138:         this.migrationState = MigrationDriverState.UNINITIALIZED;",
      "139:         this.migrationLeadershipState = ZkMigrationLeadershipState.EMPTY;",
      "140:         this.eventQueue = new KafkaEventQueue(Time.SYSTEM, logContext, \"controller-\" + nodeId + \"-migration-driver-\");",
      "",
      "[Removed Lines]",
      "137:         this.log = logContext.logger(KRaftMigrationDriver.class);",
      "",
      "[Added Lines]",
      "137:         Logger log = logContext.logger(KRaftMigrationDriver.class);",
      "138:         this.log = log;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "144:         this.initialZkLoadHandler = initialZkLoadHandler;",
      "145:         this.faultHandler = faultHandler;",
      "146:         this.quorumFeatures = quorumFeatures;",
      "148:         this.recordRedactor = new RecordRedactor(configSchema);",
      "149:         this.minBatchSize = minBatchSize;",
      "150:     }",
      "",
      "[Removed Lines]",
      "147:         this.zkMetadataWriter = new KRaftMigrationZkWriter(zkMigrationClient);",
      "",
      "[Added Lines]",
      "148:         this.zkMetadataWriter = new KRaftMigrationZkWriter(zkMigrationClient, log::error);",
      "",
      "---------------"
    ],
    "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java": [
      "File: metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "60: import java.util.Optional;",
      "61: import java.util.Set;",
      "62: import java.util.function.BiConsumer;",
      "63: import java.util.function.Function;",
      "66: public class KRaftMigrationZkWriter {",
      "",
      "[Removed Lines]",
      "64: import java.util.stream.Collectors;",
      "",
      "[Added Lines]",
      "63: import java.util.function.Consumer;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "84:     private final MigrationClient migrationClient;",
      "86:     public KRaftMigrationZkWriter(",
      "88:     ) {",
      "89:         this.migrationClient = migrationClient;",
      "90:     }",
      "92:     public void handleSnapshot(MetadataImage image, KRaftMigrationOperationConsumer operationConsumer) {",
      "",
      "[Removed Lines]",
      "87:         MigrationClient migrationClient",
      "",
      "[Added Lines]",
      "85:     private final Consumer<String> errorLogger;",
      "88:         MigrationClient migrationClient,",
      "89:         Consumer<String> errorLogger",
      "92:         this.errorLogger = errorLogger;",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "122:             updated = true;",
      "123:         }",
      "124:         if (delta.aclsDelta() != null) {",
      "126:             updated = true;",
      "127:         }",
      "128:         if (delta.delegationTokenDelta() != null) {",
      "",
      "[Removed Lines]",
      "125:             handleAclsDelta(image.acls(), delta.aclsDelta(), operationConsumer);",
      "",
      "[Added Lines]",
      "128:             handleAclsDelta(previousImage.acls(), image.acls(), delta.aclsDelta(), operationConsumer);",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "612:         });",
      "613:     }",
      "630:         Map<ResourcePattern, List<AccessControlEntry>> aclsToWrite = new HashMap<>();",
      "639:             }",
      "640:         });",
      "646:         });",
      "648:         aclsToWrite.forEach((resourcePattern, accessControlEntries) -> {",
      "652:         });",
      "653:     }",
      "",
      "[Removed Lines]",
      "615:     void handleAclsDelta(AclsImage image, AclsDelta delta, KRaftMigrationOperationConsumer operationConsumer) {",
      "617:         Set<ResourcePattern> resourcesWithChangedAcls = delta.changes().values()",
      "618:             .stream()",
      "619:             .filter(Optional::isPresent)",
      "620:             .map(Optional::get)",
      "621:             .map(this::resourcePatternFromAcl)",
      "622:             .collect(Collectors.toSet());",
      "624:         Set<ResourcePattern> resourcesWithDeletedAcls = delta.deleted()",
      "625:             .stream()",
      "626:             .map(this::resourcePatternFromAcl)",
      "627:             .collect(Collectors.toSet());",
      "631:         image.acls().forEach((uuid, standardAcl) -> {",
      "632:             ResourcePattern resourcePattern = resourcePatternFromAcl(standardAcl);",
      "633:             boolean removed = resourcesWithDeletedAcls.remove(resourcePattern);",
      "635:             if (resourcesWithChangedAcls.contains(resourcePattern) || removed) {",
      "636:                 aclsToWrite.computeIfAbsent(resourcePattern, __ -> new ArrayList<>()).add(",
      "637:                     new AccessControlEntry(standardAcl.principal(), standardAcl.host(), standardAcl.operation(), standardAcl.permissionType())",
      "638:                 );",
      "642:         resourcesWithDeletedAcls.forEach(deletedResource -> {",
      "643:             String name = \"Deleting resource \" + deletedResource + \" which has no more ACLs\";",
      "644:             operationConsumer.accept(DELETE_ACL, name, migrationState ->",
      "645:                 migrationClient.aclClient().deleteResource(deletedResource, migrationState));",
      "649:             String name = \"Writing \" + accessControlEntries.size() + \" for resource \" + resourcePattern;",
      "650:             operationConsumer.accept(UPDATE_ACL, name, migrationState ->",
      "651:                 migrationClient.aclClient().writeResourceAcls(resourcePattern, accessControlEntries, migrationState));",
      "",
      "[Added Lines]",
      "619:     void handleAclsDelta(AclsImage prevImage, AclsImage image, AclsDelta delta, KRaftMigrationOperationConsumer operationConsumer) {",
      "622:         delta.changes().forEach((aclId, aclChange) -> {",
      "623:             if (aclChange.isPresent()) {",
      "624:                 ResourcePattern resourcePattern = resourcePatternFromAcl(aclChange.get());",
      "625:                 aclsToWrite.put(resourcePattern, new ArrayList<>());",
      "626:             } else {",
      "628:                 StandardAcl deletedAcl = prevImage.acls().get(aclId);",
      "629:                 if (deletedAcl == null) {",
      "630:                     errorLogger.accept(\"Cannot delete ACL \" + aclId + \" from ZK since it is missing from previous AclImage\");",
      "631:                 } else {",
      "632:                     ResourcePattern resourcePattern = resourcePatternFromAcl(deletedAcl);",
      "633:                     aclsToWrite.put(resourcePattern, new ArrayList<>());",
      "634:                 }",
      "639:         image.acls().forEach((uuid, standardAcl) -> {",
      "640:             ResourcePattern resourcePattern = resourcePatternFromAcl(standardAcl);",
      "641:             List<AccessControlEntry> entries = aclsToWrite.get(resourcePattern);",
      "642:             if (entries != null) {",
      "643:                 entries.add(new AccessControlEntry(standardAcl.principal(), standardAcl.host(), standardAcl.operation(), standardAcl.permissionType()));",
      "644:             }",
      "649:             if (accessControlEntries.isEmpty()) {",
      "650:                 String name = \"Deleting resource \" + resourcePattern + \" which has no more ACLs\";",
      "651:                 operationConsumer.accept(DELETE_ACL, name, migrationState ->",
      "652:                     migrationClient.aclClient().deleteResource(resourcePattern, migrationState));",
      "653:             } else {",
      "654:                 String name = \"Writing \" + accessControlEntries.size() + \" for resource \" + resourcePattern;",
      "655:                 operationConsumer.accept(UPDATE_ACL, name, migrationState ->",
      "656:                     migrationClient.aclClient().writeResourceAcls(resourcePattern, accessControlEntries, migrationState));",
      "657:             }",
      "",
      "---------------"
    ],
    "metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java||metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java": [
      "File: metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java -> metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriterTest.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "80:             .setConfigMigrationClient(configClient)",
      "81:             .build();",
      "85:         MetadataImage image = new MetadataImage(",
      "86:             MetadataProvenance.EMPTY,",
      "",
      "[Removed Lines]",
      "83:         KRaftMigrationZkWriter writer = new KRaftMigrationZkWriter(migrationClient);",
      "",
      "[Added Lines]",
      "83:         KRaftMigrationZkWriter writer = new KRaftMigrationZkWriter(migrationClient, __ -> { });",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "120:             .setAclMigrationClient(aclClient)",
      "121:             .build();",
      "125:         MetadataImage image = new MetadataImage(",
      "126:             MetadataProvenance.EMPTY,",
      "",
      "[Removed Lines]",
      "123:         KRaftMigrationZkWriter writer = new KRaftMigrationZkWriter(migrationClient);",
      "",
      "[Added Lines]",
      "123:         KRaftMigrationZkWriter writer = new KRaftMigrationZkWriter(migrationClient, __ -> { });",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "179:             .setAclMigrationClient(aclClient)",
      "180:             .build();",
      "184:         MetadataImage image = new MetadataImage(",
      "185:             MetadataProvenance.EMPTY,",
      "",
      "[Removed Lines]",
      "182:         KRaftMigrationZkWriter writer = new KRaftMigrationZkWriter(migrationClient);",
      "",
      "[Added Lines]",
      "182:         KRaftMigrationZkWriter writer = new KRaftMigrationZkWriter(migrationClient, __ -> { });",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "a900794ace4dcf1f9dadee27fbd8b63979532a18",
      "candidate_info": {
        "commit_hash": "a900794ace4dcf1f9dadee27fbd8b63979532a18",
        "repo": "apache/kafka",
        "commit_url": "https://github.com/apache/kafka/commit/a900794ace4dcf1f9dadee27fbd8b63979532a18",
        "files": [
          "core/src/main/scala/kafka/controller/KafkaController.scala",
          "metadata/src/main/java/org/apache/kafka/controller/QuorumController.java",
          "metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetrics.java",
          "metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsPublisher.java",
          "metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetricsChanges.java",
          "metadata/src/main/java/org/apache/kafka/controller/metrics/QuorumControllerMetrics.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/ZkMigrationState.java",
          "metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsTest.java",
          "metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetricsChangesTest.java",
          "metadata/src/test/java/org/apache/kafka/controller/metrics/QuorumControllerMetricsTest.java"
        ],
        "message": "KAFKA-15196 Additional ZK migration metrics (#14028)\n\nThis patch adds several metrics defined in KIP-866:\n\n* MigratingZkBrokerCount: the number of zk brokers registered with KRaft\n* ZkWriteDeltaTimeMs: time spent writing MetadataDelta to ZK\n* ZkWriteSnapshotTimeMs: time spent writing MetadataImage to ZK\n* Adds value 4 for \"ZK\" to ZkMigrationState\n\nAlso fixes a typo in the metric name introduced in #14009 (ZKWriteBehindLag -> ZkWriteBehindLag)\n\nReviewers: Luke Chen <showuon@gmail.com>, Colin P. McCabe <cmccabe@apache.org>",
        "before_after_code_files": [
          "core/src/main/scala/kafka/controller/KafkaController.scala||core/src/main/scala/kafka/controller/KafkaController.scala",
          "metadata/src/main/java/org/apache/kafka/controller/QuorumController.java||metadata/src/main/java/org/apache/kafka/controller/QuorumController.java",
          "metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetrics.java||metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetrics.java",
          "metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsPublisher.java||metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsPublisher.java",
          "metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetricsChanges.java||metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetricsChanges.java",
          "metadata/src/main/java/org/apache/kafka/controller/metrics/QuorumControllerMetrics.java||metadata/src/main/java/org/apache/kafka/controller/metrics/QuorumControllerMetrics.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/ZkMigrationState.java||metadata/src/main/java/org/apache/kafka/metadata/migration/ZkMigrationState.java",
          "metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsTest.java||metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsTest.java",
          "metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetricsChangesTest.java||metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetricsChangesTest.java",
          "metadata/src/test/java/org/apache/kafka/controller/metrics/QuorumControllerMetricsTest.java||metadata/src/test/java/org/apache/kafka/controller/metrics/QuorumControllerMetricsTest.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
            "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java"
          ],
          "candidate": [
            "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
            "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java"
          ]
        }
      },
      "candidate_diff": {
        "core/src/main/scala/kafka/controller/KafkaController.scala||core/src/main/scala/kafka/controller/KafkaController.scala": [
          "File: core/src/main/scala/kafka/controller/KafkaController.scala -> core/src/main/scala/kafka/controller/KafkaController.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "22: import kafka.api._",
          "23: import kafka.common._",
          "24: import kafka.cluster.Broker",
          "26: import kafka.coordinator.transaction.ZkProducerIdManager",
          "27: import kafka.server._",
          "28: import kafka.server.metadata.ZkFinalizedFeatureCache",
          "",
          "[Removed Lines]",
          "25: import kafka.controller.KafkaController.{ActiveBrokerCountMetricName, ActiveControllerCountMetricName, AlterReassignmentsCallback, ControllerStateMetricName, ElectLeadersCallback, FencedBrokerCountMetricName, GlobalPartitionCountMetricName, GlobalTopicCountMetricName, ListReassignmentsCallback, OfflinePartitionsCountMetricName, PreferredReplicaImbalanceCountMetricName, ReplicasIneligibleToDeleteCountMetricName, ReplicasToDeleteCountMetricName, TopicsIneligibleToDeleteCountMetricName, TopicsToDeleteCountMetricName, UpdateFeaturesCallback}",
          "",
          "[Added Lines]",
          "25: import kafka.controller.KafkaController.{ActiveBrokerCountMetricName, ActiveControllerCountMetricName, AlterReassignmentsCallback, ControllerStateMetricName, ElectLeadersCallback, FencedBrokerCountMetricName, GlobalPartitionCountMetricName, GlobalTopicCountMetricName, ListReassignmentsCallback, OfflinePartitionsCountMetricName, PreferredReplicaImbalanceCountMetricName, ReplicasIneligibleToDeleteCountMetricName, ReplicasToDeleteCountMetricName, TopicsIneligibleToDeleteCountMetricName, TopicsToDeleteCountMetricName, UpdateFeaturesCallback, ZkMigrationStateMetricName}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "44: import org.apache.kafka.common.requests.{AbstractControlRequest, ApiError, LeaderAndIsrResponse, UpdateFeaturesRequest, UpdateMetadataResponse}",
          "45: import org.apache.kafka.common.utils.{Time, Utils}",
          "46: import org.apache.kafka.metadata.LeaderRecoveryState",
          "47: import org.apache.kafka.server.common.{AdminOperationException, ProducerIdsBlock}",
          "48: import org.apache.kafka.server.metrics.KafkaMetricsGroup",
          "49: import org.apache.kafka.server.util.KafkaScheduler",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "47: import org.apache.kafka.metadata.migration.ZkMigrationState",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "81:   private val ReplicasIneligibleToDeleteCountMetricName = \"ReplicasIneligibleToDeleteCount\"",
          "82:   private val ActiveBrokerCountMetricName = \"ActiveBrokerCount\"",
          "83:   private val FencedBrokerCountMetricName = \"FencedBrokerCount\"",
          "86:   private[controller] val MetricNames = Set(",
          "87:     ActiveControllerCountMetricName,",
          "88:     OfflinePartitionsCountMetricName,",
          "89:     PreferredReplicaImbalanceCountMetricName,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "85:   private val ZkMigrationStateMetricName = \"ZkMigrationState\"",
          "89:     ZkMigrationStateMetricName,",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "173:   private val tokenCleanScheduler = new KafkaScheduler(1, true, \"delegation-token-cleaner\")",
          "175:   metricsGroup.newGauge(ActiveControllerCountMetricName, () => if (isActive) 1 else 0)",
          "176:   metricsGroup.newGauge(OfflinePartitionsCountMetricName, () => offlinePartitionCount)",
          "177:   metricsGroup.newGauge(PreferredReplicaImbalanceCountMetricName, () => preferredReplicaImbalanceCount)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "178:   metricsGroup.newGauge(ZkMigrationStateMetricName, () => ZkMigrationState.ZK)",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/controller/QuorumController.java||metadata/src/main/java/org/apache/kafka/controller/QuorumController.java": [
          "File: metadata/src/main/java/org/apache/kafka/controller/QuorumController.java -> metadata/src/main/java/org/apache/kafka/controller/QuorumController.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "1273:                                 \"has been completed.\");",
          "1274:                         }",
          "1275:                         break;",
          "1276:                 }",
          "1277:             } else {",
          "1278:                 if (zkMigrationEnabled) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1276:                     default:",
          "1277:                         throw new IllegalStateException(\"Unsupported ZkMigrationState \" + featureControl.zkMigrationState());",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetrics.java||metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetrics.java": [
          "File: metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetrics.java -> metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetrics.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "39:         \"KafkaController\", \"FencedBrokerCount\");",
          "40:     private final static MetricName ACTIVE_BROKER_COUNT = getMetricName(",
          "41:         \"KafkaController\", \"ActiveBrokerCount\");",
          "42:     private final static MetricName GLOBAL_TOPIC_COUNT = getMetricName(",
          "43:         \"KafkaController\", \"GlobalTopicCount\");",
          "44:     private final static MetricName GLOBAL_PARTITION_COUNT = getMetricName(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42:     private final static MetricName MIGRATING_ZK_BROKER_COUNT = getMetricName(",
          "43:         \"KafkaController\", \"MigratingZkBrokerCount\");",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "55:     private final Optional<MetricsRegistry> registry;",
          "56:     private final AtomicInteger fencedBrokerCount = new AtomicInteger(0);",
          "57:     private final AtomicInteger activeBrokerCount = new AtomicInteger(0);",
          "58:     private final AtomicInteger globalTopicCount = new AtomicInteger(0);",
          "59:     private final AtomicInteger globalPartitionCount = new AtomicInteger(0);",
          "60:     private final AtomicInteger offlinePartitionCount = new AtomicInteger(0);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "60:     private final AtomicInteger migratingZkBrokerCount = new AtomicInteger(0);",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "117:                 return (int) zkMigrationState();",
          "118:             }",
          "119:         }));",
          "120:     }",
          "122:     public void setFencedBrokerCount(int brokerCount) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "124:         registry.ifPresent(r -> r.newGauge(MIGRATING_ZK_BROKER_COUNT, new Gauge<Integer>() {",
          "125:             @Override",
          "126:             public Integer value() {",
          "127:                 return migratingZkBrokerCount();",
          "128:             }",
          "129:         }));",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "143:         return this.activeBrokerCount.get();",
          "144:     }",
          "146:     public void setGlobalTopicCount(int topicCount) {",
          "147:         this.globalTopicCount.set(topicCount);",
          "148:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "157:     public void setMigratingZkBrokerCount(int brokerCount) {",
          "158:         this.migratingZkBrokerCount.set(brokerCount);",
          "159:     }",
          "161:     public void addToMigratingZkBrokerCount(int brokerCountDelta) {",
          "162:         this.migratingZkBrokerCount.addAndGet(brokerCountDelta);",
          "163:     }",
          "165:     public int migratingZkBrokerCount() {",
          "166:         return this.migratingZkBrokerCount.get();",
          "167:     }",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "212:         registry.ifPresent(r -> Arrays.asList(",
          "213:             FENCED_BROKER_COUNT,",
          "214:             ACTIVE_BROKER_COUNT,",
          "215:             GLOBAL_TOPIC_COUNT,",
          "216:             GLOBAL_PARTITION_COUNT,",
          "217:             OFFLINE_PARTITION_COUNT,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "238:             MIGRATING_ZK_BROKER_COUNT,",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsPublisher.java||metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsPublisher.java": [
          "File: metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsPublisher.java -> metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsPublisher.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "124:         metrics.setGlobalTopicCount(newImage.topics().topicsById().size());",
          "125:         int fencedBrokers = 0;",
          "126:         int activeBrokers = 0;",
          "127:         for (BrokerRegistration broker : newImage.cluster().brokers().values()) {",
          "128:             if (broker.fenced()) {",
          "129:                 fencedBrokers++;",
          "130:             } else {",
          "131:                 activeBrokers++;",
          "132:             }",
          "133:         }",
          "134:         metrics.setFencedBrokerCount(fencedBrokers);",
          "135:         metrics.setActiveBrokerCount(activeBrokers);",
          "136:         int totalPartitions = 0;",
          "137:         int offlinePartitions = 0;",
          "138:         int partitionsWithoutPreferredLeader = 0;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "127:         int zkBrokers = 0;",
          "134:             if (broker.isMigratingZkBroker()) {",
          "135:                 zkBrokers++;",
          "136:             }",
          "140:         metrics.setMigratingZkBrokerCount(zkBrokers);",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetricsChanges.java||metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetricsChanges.java": [
          "File: metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetricsChanges.java -> metadata/src/main/java/org/apache/kafka/controller/metrics/ControllerMetricsChanges.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:     private int fencedBrokersChange = 0;",
          "45:     private int activeBrokersChange = 0;",
          "46:     private int globalTopicsChange = 0;",
          "47:     private int globalPartitionsChange = 0;",
          "48:     private int offlinePartitionsChange = 0;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "46:     private int migratingZkBrokersChange = 0;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "56:         return activeBrokersChange;",
          "57:     }",
          "59:     public int globalTopicsChange() {",
          "60:         return globalTopicsChange;",
          "61:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "60:     public int migratingZkBrokersChange() {",
          "61:         return migratingZkBrokersChange;",
          "62:     }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "75:     void handleBrokerChange(BrokerRegistration prev, BrokerRegistration next) {",
          "76:         boolean wasFenced = false;",
          "77:         boolean wasActive = false;",
          "78:         if (prev != null) {",
          "79:             wasFenced = prev.fenced();",
          "80:             wasActive = !prev.fenced();",
          "81:         }",
          "82:         boolean isFenced = false;",
          "83:         boolean isActive = false;",
          "84:         if (next != null) {",
          "85:             isFenced = next.fenced();",
          "86:             isActive = !next.fenced();",
          "87:         }",
          "88:         fencedBrokersChange += delta(wasFenced, isFenced);",
          "89:         activeBrokersChange += delta(wasActive, isActive);",
          "90:     }",
          "92:     void handleDeletedTopic(TopicImage deletedTopic) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "83:         boolean wasZk = false;",
          "87:             wasZk = prev.isMigratingZkBroker();",
          "91:         boolean isZk = false;",
          "95:             isZk = next.isMigratingZkBroker();",
          "99:         migratingZkBrokersChange += delta(wasZk, isZk);",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "141:         if (activeBrokersChange != 0) {",
          "142:             metrics.addToActiveBrokerCount(activeBrokersChange);",
          "143:         }",
          "144:         if (globalTopicsChange != 0) {",
          "145:             metrics.addToGlobalTopicCount(globalTopicsChange);",
          "146:         }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "154:         if (migratingZkBrokersChange != 0) {",
          "155:             metrics.addToMigratingZkBrokerCount(migratingZkBrokersChange);",
          "156:         }",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/controller/metrics/QuorumControllerMetrics.java||metadata/src/main/java/org/apache/kafka/controller/metrics/QuorumControllerMetrics.java": [
          "File: metadata/src/main/java/org/apache/kafka/controller/metrics/QuorumControllerMetrics.java -> metadata/src/main/java/org/apache/kafka/controller/metrics/QuorumControllerMetrics.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "45:     private final static MetricName EVENT_QUEUE_PROCESSING_TIME_MS = getMetricName(",
          "46:         \"ControllerEventManager\", \"EventQueueProcessingTimeMs\");",
          "47:     private final static MetricName ZK_WRITE_BEHIND_LAG = getMetricName(",
          "49:     private final static MetricName LAST_APPLIED_RECORD_OFFSET = getMetricName(",
          "50:         \"KafkaController\", \"LastAppliedRecordOffset\");",
          "51:     private final static MetricName LAST_COMMITTED_RECORD_OFFSET = getMetricName(",
          "",
          "[Removed Lines]",
          "48:         \"KafkaController\", \"ZKWriteBehindLag\");",
          "",
          "[Added Lines]",
          "48:         \"KafkaController\", \"ZkWriteBehindLag\");",
          "49:     private final static MetricName ZK_WRITE_SNAPSHOT_TIME_MS = getMetricName(",
          "50:         \"KafkaController\", \"ZkWriteSnapshotTimeMs\");",
          "51:     private final static MetricName ZK_WRITE_DELTA_TIME_MS = getMetricName(",
          "52:         \"KafkaController\", \"ZkWriteDeltaTimeMs\");",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "71:     private final AtomicLong dualWriteOffset = new AtomicLong(0);",
          "72:     private final Consumer<Long> eventQueueTimeUpdater;",
          "73:     private final Consumer<Long> eventQueueProcessingTimeUpdater;",
          "74:     private final AtomicLong timedOutHeartbeats = new AtomicLong(0);",
          "75:     private final AtomicLong operationsStarted = new AtomicLong(0);",
          "76:     private final AtomicLong operationsTimedOut = new AtomicLong(0);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "78:     private final Consumer<Long> zkWriteSnapshotTimeHandler;",
          "79:     private final Consumer<Long> zkWriteDeltaTimeHandler;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "88:     public QuorumControllerMetrics(",
          "89:         Optional<MetricsRegistry> registry,",
          "90:         Time time,",
          "92:     ) {",
          "93:         this.registry = registry;",
          "94:         this.active = false;",
          "",
          "[Removed Lines]",
          "91:         boolean zkMigrationState",
          "",
          "[Added Lines]",
          "98:         boolean zkMigrationEnabled",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "148:                 return newActiveControllers();",
          "149:             }",
          "150:         }));",
          "152:             registry.ifPresent(r -> r.newGauge(ZK_WRITE_BEHIND_LAG, new Gauge<Long>() {",
          "153:                 @Override",
          "154:                 public Long value() {",
          "",
          "[Removed Lines]",
          "151:         if (zkMigrationState) {",
          "",
          "[Added Lines]",
          "159:         if (zkMigrationEnabled) {",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "158:                     else return lastCommittedRecordOffset() - dualWriteOffset();",
          "159:                 }",
          "160:             }));",
          "161:         }",
          "162:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "169:             this.zkWriteSnapshotTimeHandler = newHistogram(ZK_WRITE_SNAPSHOT_TIME_MS, true);",
          "170:             this.zkWriteDeltaTimeHandler = newHistogram(ZK_WRITE_DELTA_TIME_MS, true);",
          "171:         } else {",
          "172:             this.zkWriteSnapshotTimeHandler = __ -> { };",
          "173:             this.zkWriteDeltaTimeHandler = __ -> { };",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "177:         eventQueueProcessingTimeUpdater.accept(durationMs);",
          "178:     }",
          "180:     public void setLastAppliedRecordOffset(long offset) {",
          "181:         lastAppliedRecordOffset.set(offset);",
          "182:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "193:     public void updateZkWriteSnapshotTimeMs(long durationMs) {",
          "194:         zkWriteSnapshotTimeHandler.accept(durationMs);",
          "195:     }",
          "197:     public void updateZkWriteDeltaTimeMs(long durationMs) {",
          "198:         zkWriteDeltaTimeHandler.accept(durationMs);",
          "199:     }",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "255:             EVENT_QUEUE_OPERATIONS_STARTED_COUNT,",
          "256:             EVENT_QUEUE_OPERATIONS_TIMED_OUT_COUNT,",
          "257:             NEW_ACTIVE_CONTROLLERS_COUNT,",
          "259:         ).forEach(r::removeMetric));",
          "260:     }",
          "",
          "[Removed Lines]",
          "258:             ZK_WRITE_BEHIND_LAG",
          "",
          "[Added Lines]",
          "279:             ZK_WRITE_BEHIND_LAG,",
          "280:             ZK_WRITE_SNAPSHOT_TIME_MS,",
          "281:             ZK_WRITE_DELTA_TIME_MS",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java": [
          "File: metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "475:             }",
          "477:             Map<String, Integer> dualWriteCounts = new TreeMap<>();",
          "478:             if (isSnapshot) {",
          "479:                 zkMetadataWriter.handleSnapshot(image, countingOperationConsumer(",
          "481:             } else {",
          "484:             }",
          "485:             if (dualWriteCounts.isEmpty()) {",
          "486:                 log.trace(\"Did not make any ZK writes when handling KRaft {}\", isSnapshot ? \"snapshot\" : \"delta\");",
          "",
          "[Removed Lines]",
          "480:                         dualWriteCounts, KRaftMigrationDriver.this::applyMigrationOperation));",
          "482:                 zkMetadataWriter.handleDelta(prevImage, image, delta, countingOperationConsumer(",
          "483:                         dualWriteCounts, KRaftMigrationDriver.this::applyMigrationOperation));",
          "",
          "[Added Lines]",
          "478:             long startTime = time.nanoseconds();",
          "481:                     dualWriteCounts, KRaftMigrationDriver.this::applyMigrationOperation));",
          "482:                 controllerMetrics.updateZkWriteSnapshotTimeMs(NANOSECONDS.toMillis(time.nanoseconds() - startTime));",
          "484:                 if (zkMetadataWriter.handleDelta(prevImage, image, delta, countingOperationConsumer(",
          "485:                       dualWriteCounts, KRaftMigrationDriver.this::applyMigrationOperation))) {",
          "487:                     controllerMetrics.updateZkWriteDeltaTimeMs(NANOSECONDS.toMillis(time.nanoseconds() - startTime));",
          "488:                 }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "556:                         log.error(\"KRaft controller indicates a completed migration, but the migration driver is somehow active.\");",
          "557:                         transitionTo(MigrationDriverState.INACTIVE);",
          "558:                         break;",
          "559:                 }",
          "560:             }",
          "561:         }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "564:                     default:",
          "565:                         throw new IllegalStateException(\"Unsupported ZkMigrationState \" + zkMigrationState);",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "658:             if (migrationState == MigrationDriverState.SYNC_KRAFT_TO_ZK) {",
          "659:                 log.info(\"Performing a full metadata sync from KRaft to ZK.\");",
          "660:                 Map<String, Integer> dualWriteCounts = new TreeMap<>();",
          "661:                 zkMetadataWriter.handleSnapshot(image, countingOperationConsumer(",
          "663:                 log.info(\"Made the following ZK writes when reconciling with KRaft state: {}\", dualWriteCounts);",
          "664:                 transitionTo(MigrationDriverState.KRAFT_CONTROLLER_TO_BROKER_COMM);",
          "665:             }",
          "",
          "[Removed Lines]",
          "662:                         dualWriteCounts, KRaftMigrationDriver.this::applyMigrationOperation));",
          "",
          "[Added Lines]",
          "668:                 long startTime = time.nanoseconds();",
          "670:                     dualWriteCounts, KRaftMigrationDriver.this::applyMigrationOperation));",
          "671:                 long endTime = time.nanoseconds();",
          "672:                 controllerMetrics.updateZkWriteSnapshotTimeMs(NANOSECONDS.toMillis(startTime - endTime));",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java": [
          "File: metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "93:         handleAclsSnapshot(image.acls(), operationConsumer);",
          "94:     }",
          "97:         MetadataImage previousImage,",
          "98:         MetadataImage image,",
          "99:         MetadataDelta delta,",
          "100:         KRaftMigrationOperationConsumer operationConsumer",
          "101:     ) {",
          "102:         if (delta.topicsDelta() != null) {",
          "103:             handleTopicsDelta(previousImage.topics().topicIdToNameView()::get, image.topics(), delta.topicsDelta(), operationConsumer);",
          "104:         }",
          "105:         if (delta.configsDelta() != null) {",
          "106:             handleConfigsDelta(image.configs(), delta.configsDelta(), operationConsumer);",
          "107:         }",
          "108:         if ((delta.clientQuotasDelta() != null) || (delta.scramDelta() != null)) {",
          "109:             handleClientQuotasDelta(image, delta, operationConsumer);",
          "110:         }",
          "111:         if (delta.producerIdsDelta() != null) {",
          "112:             handleProducerIdDelta(delta.producerIdsDelta(), operationConsumer);",
          "113:         }",
          "114:         if (delta.aclsDelta() != null) {",
          "115:             handleAclsDelta(image.acls(), delta.aclsDelta(), operationConsumer);",
          "116:         }",
          "117:     }",
          "",
          "[Removed Lines]",
          "96:     public void handleDelta(",
          "",
          "[Added Lines]",
          "96:     public boolean handleDelta(",
          "102:         boolean updated = false;",
          "105:             updated = true;",
          "109:             updated = true;",
          "113:             updated = true;",
          "117:             updated = true;",
          "121:             updated = true;",
          "123:         return updated;",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/metadata/migration/ZkMigrationState.java||metadata/src/main/java/org/apache/kafka/metadata/migration/ZkMigrationState.java": [
          "File: metadata/src/main/java/org/apache/kafka/metadata/migration/ZkMigrationState.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/ZkMigrationState.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "67:     private final byte value;",
          "",
          "[Removed Lines]",
          "65:     POST_MIGRATION((byte) 3);",
          "",
          "[Added Lines]",
          "65:     POST_MIGRATION((byte) 3),",
          "72:     ZK((byte) 4);",
          "",
          "---------------"
        ],
        "metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsTest.java||metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsTest.java": [
          "File: metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsTest.java -> metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetadataMetricsTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "41:                     new HashSet<>(Arrays.asList(",
          "42:                         \"kafka.controller:type=KafkaController,name=ActiveBrokerCount\",",
          "43:                         \"kafka.controller:type=KafkaController,name=FencedBrokerCount\",",
          "44:                         \"kafka.controller:type=KafkaController,name=GlobalPartitionCount\",",
          "45:                         \"kafka.controller:type=KafkaController,name=GlobalTopicCount\",",
          "46:                         \"kafka.controller:type=KafkaController,name=MetadataErrorCount\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "44:                         \"kafka.controller:type=KafkaController,name=MigratingZkBrokerCount\",",
          "",
          "---------------"
        ],
        "metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetricsChangesTest.java||metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetricsChangesTest.java": [
          "File: metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetricsChangesTest.java -> metadata/src/test/java/org/apache/kafka/controller/metrics/ControllerMetricsChangesTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "51:         boolean fenced",
          "52:     ) {",
          "53:         return new BrokerRegistration(brokerId,",
          "61:     }",
          "63:     @Test",
          "",
          "[Removed Lines]",
          "54:                 100L,",
          "55:                 Uuid.fromString(\"Pxi6QwS2RFuN8VSKjqJZyQ\"),",
          "56:                 Collections.emptyList(),",
          "57:                 Collections.emptyMap(),",
          "58:                 Optional.empty(),",
          "59:                 fenced,",
          "60:                 false);",
          "",
          "[Added Lines]",
          "54:             100L,",
          "55:             Uuid.fromString(\"Pxi6QwS2RFuN8VSKjqJZyQ\"),",
          "56:             Collections.emptyList(),",
          "57:             Collections.emptyMap(),",
          "58:             Optional.empty(),",
          "59:             fenced,",
          "60:             false);",
          "61:     }",
          "63:     private static BrokerRegistration zkBrokerRegistration(",
          "64:         int brokerId",
          "65:     ) {",
          "66:         return new BrokerRegistration(brokerId,",
          "67:             100L,",
          "68:             Uuid.fromString(\"Pxi6QwS2RFuN8VSKjqJZyQ\"),",
          "69:             Collections.emptyList(),",
          "70:             Collections.emptyMap(),",
          "71:             Optional.empty(),",
          "72:             false,",
          "73:             false,",
          "74:             true);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "103:         assertEquals(1, changes.activeBrokersChange());",
          "104:     }",
          "106:     @Test",
          "107:     public void testHandleDeletedTopic() {",
          "108:         ControllerMetricsChanges changes = new ControllerMetricsChanges();",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "120:     @Test",
          "121:     public void testHandleZkBroker() {",
          "122:         ControllerMetricsChanges changes = new ControllerMetricsChanges();",
          "123:         changes.handleBrokerChange(null, zkBrokerRegistration(1));",
          "124:         assertEquals(1, changes.migratingZkBrokersChange());",
          "125:         changes.handleBrokerChange(null, zkBrokerRegistration(2));",
          "126:         changes.handleBrokerChange(null, zkBrokerRegistration(3));",
          "127:         assertEquals(3, changes.migratingZkBrokersChange());",
          "129:         changes.handleBrokerChange(zkBrokerRegistration(3), brokerRegistration(3, true));",
          "130:         changes.handleBrokerChange(brokerRegistration(3, true), brokerRegistration(3, false));",
          "131:         assertEquals(2, changes.migratingZkBrokersChange());",
          "132:     }",
          "",
          "---------------"
        ],
        "metadata/src/test/java/org/apache/kafka/controller/metrics/QuorumControllerMetricsTest.java||metadata/src/test/java/org/apache/kafka/controller/metrics/QuorumControllerMetricsTest.java": [
          "File: metadata/src/test/java/org/apache/kafka/controller/metrics/QuorumControllerMetricsTest.java -> metadata/src/test/java/org/apache/kafka/controller/metrics/QuorumControllerMetricsTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "59:                     \"kafka.controller:type=KafkaController,name=TimedOutBrokerHeartbeatCount\"",
          "60:                 ));",
          "61:                 if (inMigration) {",
          "63:                 }",
          "64:                 ControllerMetricsTestUtils.assertMetricsForTypeEqual(registry, \"kafka.controller\", expected);",
          "65:             }",
          "",
          "[Removed Lines]",
          "62:                     expected.add(\"kafka.controller:type=KafkaController,name=ZKWriteBehindLag\");",
          "",
          "[Added Lines]",
          "62:                     expected.add(\"kafka.controller:type=KafkaController,name=ZkWriteBehindLag\");",
          "63:                     expected.add(\"kafka.controller:type=KafkaController,name=ZkWriteSnapshotTimeMs\");",
          "64:                     expected.add(\"kafka.controller:type=KafkaController,name=ZkWriteDeltaTimeMs\");",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "144:             @SuppressWarnings(\"unchecked\")",
          "145:             Gauge<Long> zkWriteBehindLag = (Gauge<Long>) registry",
          "146:                     .allMetrics()",
          "148:             assertEquals(10L, zkWriteBehindLag.value());",
          "150:             @SuppressWarnings(\"unchecked\")",
          "",
          "[Removed Lines]",
          "147:                     .get(metricName(\"KafkaController\", \"ZKWriteBehindLag\"));",
          "",
          "[Added Lines]",
          "149:                     .get(metricName(\"KafkaController\", \"ZkWriteBehindLag\"));",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "184:             metrics.updateDualWriteOffset(0);",
          "185:             @SuppressWarnings(\"unchecked\")",
          "186:             Gauge<Long> zkWriteBehindLag = (Gauge<Long>) registry",
          "189:             assertEquals(0, zkWriteBehindLag.value());",
          "190:         } finally {",
          "191:             registry.shutdown();",
          "",
          "[Removed Lines]",
          "187:                     .allMetrics()",
          "188:                     .get(metricName(\"KafkaController\", \"ZKWriteBehindLag\"));",
          "",
          "[Added Lines]",
          "189:                 .allMetrics()",
          "190:                 .get(metricName(\"KafkaController\", \"ZkWriteBehindLag\"));",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "197:             metrics.setLastCommittedRecordOffset(100);",
          "198:             @SuppressWarnings(\"unchecked\")",
          "199:             Gauge<Long> zkWriteBehindLag = (Gauge<Long>) registry",
          "202:             assertEquals(10, zkWriteBehindLag.value());",
          "203:         } finally {",
          "204:             registry.shutdown();",
          "",
          "[Removed Lines]",
          "200:                     .allMetrics()",
          "201:                     .get(metricName(\"KafkaController\", \"ZKWriteBehindLag\"));",
          "",
          "[Added Lines]",
          "202:                 .allMetrics()",
          "203:                 .get(metricName(\"KafkaController\", \"ZkWriteBehindLag\"));",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cc6b919212ae62d75850214ae2c93379b78ff325",
      "candidate_info": {
        "commit_hash": "cc6b919212ae62d75850214ae2c93379b78ff325",
        "repo": "apache/kafka",
        "commit_url": "https://github.com/apache/kafka/commit/cc6b919212ae62d75850214ae2c93379b78ff325",
        "files": [
          "core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala"
        ],
        "message": "KAFKA-16435 Add test for KAFKA-16428 (#15635)\n\nReviewers: Chia-Ping Tsai <chia7712@gmail.com>",
        "before_after_code_files": [
          "core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala"
          ],
          "candidate": [
            "core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala"
          ]
        }
      },
      "candidate_diff": {
        "core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala": [
          "File: core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala -> core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "40: import org.apache.kafka.server.common.ApiMessageAndVersion",
          "41: import org.apache.kafka.server.config.ConfigType",
          "42: import org.apache.kafka.server.util.MockRandom",
          "44: import org.junit.jupiter.api.Test",
          "46: import java.util",
          "47: import java.util.Properties",
          "48: import scala.collection.Map",
          "",
          "[Removed Lines]",
          "43: import org.junit.jupiter.api.Assertions.{assertEquals, assertTrue, fail}",
          "",
          "[Added Lines]",
          "43: import org.junit.jupiter.api.Assertions.{assertEquals, assertFalse, assertTrue, fail}",
          "46: import java.nio.charset.StandardCharsets.UTF_8",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "112:         assertEquals(newProps.get(key), value)",
          "113:       }",
          "114:     }",
          "116:     migrationState = migrationClient.configClient().deleteConfigs(",
          "117:       new ConfigResource(ConfigResource.Type.BROKER, \"1\"), migrationState)",
          "118:     assertEquals(0, zkClient.getEntityConfigs(ConfigType.BROKER, \"1\").size())",
          "119:   }",
          "121:   @Test",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "116:     assertPathExistenceAndData(\"/config/changes/config_change_0000000000\", \"\"\"{\"version\":2,\"entity_path\":\"brokers/1\"}\"\"\")",
          "121:     assertPathExistenceAndData(\"/config/changes/config_change_0000000001\", \"\"\"{\"version\":2,\"entity_path\":\"brokers/1\"}\"\"\")",
          "124:     assertFalse(zkClient.pathExists(\"/config/changes/config_change_0000000002\"))",
          "125:   }",
          "127:   private def assertPathExistenceAndData(expectedPath: String, data: String): Unit = {",
          "128:     assertTrue(zkClient.pathExists(expectedPath))",
          "129:     assertEquals(Some(data), zkClient.getDataAndStat(expectedPath)._1.map(new String(_, UTF_8)))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5dcdf71dec4124b67f13fcd1561faef3dac38d55",
      "candidate_info": {
        "commit_hash": "5dcdf71dec4124b67f13fcd1561faef3dac38d55",
        "repo": "apache/kafka",
        "commit_url": "https://github.com/apache/kafka/commit/5dcdf71dec4124b67f13fcd1561faef3dac38d55",
        "files": [
          "core/src/main/scala/kafka/zk/ZkMigrationClient.scala",
          "core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClient.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClientAuthException.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClientException.java",
          "metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationDriverTest.java"
        ],
        "message": "MINOR: Improved error handling in ZK migration (#13372)\n\nThis patch fixes many small issues to improve error handling and logging during the ZK migration. A test was added\nto simulate a ZK session expiration to ensure the correctness of the migration driver.\n\nWith this change, ZK errors thrown during the migration will not hit the fault handler registered with with\nKRaftMigrationDriver, but they will be logged.\n\nReviewers: Colin P. McCabe <cmccabe@apache.org>",
        "before_after_code_files": [
          "core/src/main/scala/kafka/zk/ZkMigrationClient.scala||core/src/main/scala/kafka/zk/ZkMigrationClient.scala",
          "core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala||core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClient.java||metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClient.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClientAuthException.java||metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClientAuthException.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClientException.java||metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClientException.java",
          "metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationDriverTest.java||metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationDriverTest.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java"
          ],
          "candidate": [
            "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java"
          ]
        }
      },
      "candidate_diff": {
        "core/src/main/scala/kafka/zk/ZkMigrationClient.scala||core/src/main/scala/kafka/zk/ZkMigrationClient.scala": [
          "File: core/src/main/scala/kafka/zk/ZkMigrationClient.scala -> core/src/main/scala/kafka/zk/ZkMigrationClient.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "28: import org.apache.kafka.common.metadata._",
          "29: import org.apache.kafka.common.quota.ClientQuotaEntity",
          "30: import org.apache.kafka.common.{TopicPartition, Uuid}",
          "32: import org.apache.kafka.metadata.{LeaderRecoveryState, PartitionRegistration}",
          "34: import org.apache.kafka.server.common.{ApiMessageAndVersion, ProducerIdsBlock}",
          "36: import org.apache.zookeeper.{CreateMode, KeeperException}",
          "38: import java.util",
          "",
          "[Removed Lines]",
          "31: import org.apache.kafka.image.{MetadataDelta, MetadataImage}",
          "33: import org.apache.kafka.metadata.migration.{MigrationClient, ZkMigrationLeadershipState}",
          "35: import org.apache.zookeeper.KeeperException.Code",
          "",
          "[Added Lines]",
          "32: import org.apache.kafka.metadata.migration.{MigrationClient, MigrationClientAuthException, MigrationClientException, ZkMigrationLeadershipState}",
          "34: import org.apache.zookeeper.KeeperException.{AuthFailedException, Code, NoAuthException, SessionClosedRequireAuthException}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "48: class ZkMigrationClient(",
          "49:   zkClient: KafkaZkClient,",
          "50:   zkConfigEncoder: PasswordEncoder",
          "51: ) extends MigrationClient with Logging {",
          "56:   }",
          "59:     zkClient.updateMigrationState(state)",
          "60:   }",
          "63:     zkClient.tryRegisterKRaftControllerAsActiveController(state.kraftControllerId(), state.kraftControllerEpoch()) match {",
          "64:       case SuccessfulRegistrationResult(controllerEpoch, controllerEpochZkVersion) =>",
          "65:         state.withZkController(controllerEpoch, controllerEpochZkVersion)",
          "",
          "[Removed Lines]",
          "53:   override def getOrCreateMigrationRecoveryState(initialState: ZkMigrationLeadershipState): ZkMigrationLeadershipState = {",
          "54:     zkClient.createTopLevelPaths()",
          "55:     zkClient.getOrCreateMigrationState(initialState)",
          "58:   override def setMigrationRecoveryState(state: ZkMigrationLeadershipState): ZkMigrationLeadershipState = {",
          "62:   override def claimControllerLeadership(state: ZkMigrationLeadershipState): ZkMigrationLeadershipState = {",
          "",
          "[Added Lines]",
          "58:   @throws(classOf[MigrationClientException])",
          "59:   private def wrapZkException[T](fn: => T): T = {",
          "60:     try {",
          "61:       fn",
          "62:     } catch {",
          "63:       case e @ (_: MigrationClientException | _: MigrationClientAuthException) => throw e",
          "64:       case e @ (_: AuthFailedException | _: NoAuthException | _: SessionClosedRequireAuthException) =>",
          "66:         throw new MigrationClientAuthException(e)",
          "67:       case e: KeeperException => throw new MigrationClientException(e)",
          "68:     }",
          "71:   override def getOrCreateMigrationRecoveryState(",
          "72:     initialState: ZkMigrationLeadershipState",
          "73:   ): ZkMigrationLeadershipState = wrapZkException {",
          "74:       zkClient.createTopLevelPaths()",
          "75:       zkClient.getOrCreateMigrationState(initialState)",
          "76:     }",
          "78:   override def setMigrationRecoveryState(",
          "79:     state: ZkMigrationLeadershipState",
          "80:   ): ZkMigrationLeadershipState = wrapZkException {",
          "84:   override def claimControllerLeadership(",
          "85:     state: ZkMigrationLeadershipState",
          "86:   ): ZkMigrationLeadershipState = wrapZkException {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "67:     }",
          "68:   }",
          "71:     try {",
          "72:       zkClient.deleteController(state.zkControllerEpochZkVersion())",
          "73:       state.withUnknownZkController()",
          "",
          "[Removed Lines]",
          "70:   override def releaseControllerLeadership(state: ZkMigrationLeadershipState): ZkMigrationLeadershipState = {",
          "",
          "[Added Lines]",
          "94:   override def releaseControllerLeadership(",
          "95:     state: ZkMigrationLeadershipState",
          "96:   ): ZkMigrationLeadershipState = wrapZkException {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "77:         state.withUnknownZkController()",
          "78:       case t: Throwable =>",
          "80:     }",
          "81:   }",
          "83:   def migrateTopics(",
          "84:     recordConsumer: Consumer[util.List[ApiMessageAndVersion]],",
          "85:     brokerIdConsumer: Consumer[Integer]",
          "87:     val topics = zkClient.getAllTopicsInCluster()",
          "88:     val topicConfigs = zkClient.getEntitiesConfigs(ConfigType.Topic, topics)",
          "89:     val replicaAssignmentAndTopicIds = zkClient.getReplicaAssignmentAndTopicIdForTopics(topics)",
          "",
          "[Removed Lines]",
          "79:         throw new RuntimeException(\"Could not release controller leadership due to underlying error\", t)",
          "86:   ): Unit = {",
          "",
          "[Added Lines]",
          "105:         throw new MigrationClientException(\"Could not release controller leadership due to underlying error\", t)",
          "112:   ): Unit = wrapZkException {",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "137:     }",
          "138:   }",
          "141:     val batch = new util.ArrayList[ApiMessageAndVersion]()",
          "143:     val brokerEntities = zkClient.getAllEntitiesWithConfig(ConfigType.Broker)",
          "",
          "[Removed Lines]",
          "140:   def migrateBrokerConfigs(recordConsumer: Consumer[util.List[ApiMessageAndVersion]]): Unit = {",
          "",
          "[Added Lines]",
          "166:   def migrateBrokerConfigs(",
          "167:     recordConsumer: Consumer[util.List[ApiMessageAndVersion]]",
          "168:   ): Unit = wrapZkException {",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "165:     }",
          "166:   }",
          "169:     val adminZkClient = new AdminZkClient(zkClient)",
          "171:     def migrateEntityType(entityType: String): Unit = {",
          "",
          "[Removed Lines]",
          "168:   def migrateClientQuotas(recordConsumer: Consumer[util.List[ApiMessageAndVersion]]): Unit = {",
          "",
          "[Added Lines]",
          "196:   def migrateClientQuotas(",
          "197:     recordConsumer: Consumer[util.List[ApiMessageAndVersion]]",
          "198:   ): Unit = wrapZkException {",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "207:     migrateEntityType(ConfigType.Ip)",
          "208:   }",
          "211:     val (dataOpt, _) = zkClient.getDataAndVersion(ProducerIdBlockZNode.path)",
          "212:     dataOpt match {",
          "213:       case Some(data) =>",
          "",
          "[Removed Lines]",
          "210:   def migrateProducerId(recordConsumer: Consumer[util.List[ApiMessageAndVersion]]): Unit = {",
          "",
          "[Added Lines]",
          "240:   def migrateProducerId(",
          "241:     recordConsumer: Consumer[util.List[ApiMessageAndVersion]]",
          "242:   ): Unit = wrapZkException {",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "220:     }",
          "221:   }",
          "225:     migrateTopics(batchConsumer, brokerIdConsumer)",
          "226:     migrateBrokerConfigs(batchConsumer)",
          "227:     migrateClientQuotas(batchConsumer)",
          "228:     migrateProducerId(batchConsumer)",
          "229:   }",
          "233:   }",
          "236:     val topics = zkClient.getAllTopicsInCluster()",
          "237:     val replicaAssignmentAndTopicIds = zkClient.getReplicaAssignmentAndTopicIdForTopics(topics)",
          "238:     val brokersWithAssignments = new util.HashSet[Integer]()",
          "",
          "[Removed Lines]",
          "223:   override def readAllMetadata(batchConsumer: Consumer[util.List[ApiMessageAndVersion]],",
          "224:                                brokerIdConsumer: Consumer[Integer]): Unit = {",
          "231:   override def readBrokerIds(): util.Set[Integer] = {",
          "232:     zkClient.getSortedBrokerList.map(Integer.valueOf).toSet.asJava",
          "235:   override def readBrokerIdsFromTopicAssignments(): util.Set[Integer] = {",
          "",
          "[Added Lines]",
          "255:   override def readAllMetadata(",
          "256:     batchConsumer: Consumer[util.List[ApiMessageAndVersion]],",
          "257:     brokerIdConsumer: Consumer[Integer]",
          "258:   ): Unit = {",
          "265:   override def readBrokerIds(): util.Set[Integer] = wrapZkException {",
          "266:     new util.HashSet[Integer](zkClient.getSortedBrokerList.map(Integer.valueOf).toSet.asJava)",
          "269:   override def readBrokerIdsFromTopicAssignments(): util.Set[Integer] = wrapZkException {",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "244:     brokersWithAssignments",
          "245:   }",
          "251:     val assignments = partitions.asScala.map { case (partitionId, partition) =>",
          "252:       new TopicPartition(topicName, partitionId) ->",
          "253:         ReplicaAssignment(partition.replicas, partition.addingReplicas, partition.removingReplicas)",
          "",
          "[Removed Lines]",
          "247:   override def createTopic(topicName: String,",
          "248:                            topicId: Uuid,",
          "249:                            partitions: util.Map[Integer, PartitionRegistration],",
          "250:                            state: ZkMigrationLeadershipState): ZkMigrationLeadershipState = {",
          "",
          "[Added Lines]",
          "281:   override def createTopic(",
          "282:     topicName: String,",
          "283:     topicId: Uuid,",
          "284:     partitions: util.Map[Integer, PartitionRegistration],",
          "285:     state: ZkMigrationLeadershipState",
          "286:   ): ZkMigrationLeadershipState = wrapZkException {",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "289:       state.withMigrationZkVersion(migrationZkVersion)",
          "290:     } else {",
          "293:     }",
          "294:   }",
          "297:     val path = TopicPartitionZNode.path(topicPartition)",
          "298:     CreateRequest(path, null, zkClient.defaultAcls(path), CreateMode.PERSISTENT, Some(topicPartition))",
          "299:   }",
          "304:     val path = TopicPartitionStateZNode.path(topicPartition)",
          "305:     val data = TopicPartitionStateZNode.encode(LeaderIsrAndControllerEpoch(new LeaderAndIsr(",
          "306:       partitionRegistration.leader,",
          "",
          "[Removed Lines]",
          "292:       throw new RuntimeException(s\"Failed to create or update topic $topicName. ZK operation had results $resultCodes\")",
          "296:   private def createTopicPartition(topicPartition: TopicPartition): CreateRequest = {",
          "301:   private def partitionStatePathAndData(topicPartition: TopicPartition,",
          "302:                                         partitionRegistration: PartitionRegistration,",
          "303:                                         controllerEpoch: Int): (String, Array[Byte]) = {",
          "",
          "[Added Lines]",
          "328:       throw new MigrationClientException(s\"Failed to create or update topic $topicName. ZK operation had results $resultCodes\")",
          "332:   private def createTopicPartition(",
          "333:     topicPartition: TopicPartition",
          "334:   ): CreateRequest = wrapZkException {",
          "339:   private def partitionStatePathAndData(",
          "340:     topicPartition: TopicPartition,",
          "341:     partitionRegistration: PartitionRegistration,",
          "342:     controllerEpoch: Int",
          "343:   ): (String, Array[Byte]) = {",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "311:     (path, data)",
          "312:   }",
          "317:     val (path, data) = partitionStatePathAndData(topicPartition, partitionRegistration, controllerEpoch)",
          "318:     CreateRequest(path, data, zkClient.defaultAcls(path), CreateMode.PERSISTENT, Some(topicPartition))",
          "319:   }",
          "324:     val (path, data) = partitionStatePathAndData(topicPartition, partitionRegistration, controllerEpoch)",
          "325:     SetDataRequest(path, data, ZkVersion.MatchAnyVersion, Some(topicPartition))",
          "326:   }",
          "330:     val requests = topicPartitions.asScala.flatMap { case (topicName, partitionRegistrations) =>",
          "331:       partitionRegistrations.asScala.flatMap { case (partitionId, partitionRegistration) =>",
          "332:         val topicPartition = new TopicPartition(topicName, partitionId)",
          "",
          "[Removed Lines]",
          "314:   private def createTopicPartitionState(topicPartition: TopicPartition,",
          "315:                                         partitionRegistration: PartitionRegistration,",
          "316:                                         controllerEpoch: Int): CreateRequest = {",
          "321:   private def updateTopicPartitionState(topicPartition: TopicPartition,",
          "322:                                         partitionRegistration: PartitionRegistration,",
          "323:                                         controllerEpoch: Int): SetDataRequest = {",
          "328:   override def updateTopicPartitions(topicPartitions: util.Map[String, util.Map[Integer, PartitionRegistration]],",
          "329:                                      state: ZkMigrationLeadershipState): ZkMigrationLeadershipState = {",
          "",
          "[Added Lines]",
          "354:   private def createTopicPartitionState(",
          "355:     topicPartition: TopicPartition,",
          "356:     partitionRegistration: PartitionRegistration,",
          "357:     controllerEpoch: Int",
          "358:   ): CreateRequest = {",
          "363:   private def updateTopicPartitionState(",
          "364:     topicPartition: TopicPartition,",
          "365:     partitionRegistration: PartitionRegistration,",
          "366:     controllerEpoch: Int",
          "367:   ): SetDataRequest = {",
          "372:   override def updateTopicPartitions(",
          "373:     topicPartitions: util.Map[String, util.Map[Integer, PartitionRegistration]],",
          "374:     state: ZkMigrationLeadershipState",
          "375:   ): ZkMigrationLeadershipState = wrapZkException {",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "341:       if (resultCodes.forall { case (_, code) => code.equals(Code.OK) } ) {",
          "342:         state.withMigrationZkVersion(migrationZkVersion)",
          "343:       } else {",
          "345:       }",
          "346:     }",
          "347:   }",
          "356:     val configData = ConfigEntityZNode.encode(props)",
          "358:     val requests = if (create) {",
          "",
          "[Removed Lines]",
          "344:         throw new RuntimeException(s\"Failed to update partition states: $topicPartitions. ZK transaction had results $resultCodes\")",
          "351:   def tryWriteEntityConfig(entityType: String,",
          "352:                            path: String,",
          "353:                            props: Properties,",
          "354:                            create: Boolean,",
          "355:                            state: ZkMigrationLeadershipState): Option[ZkMigrationLeadershipState] = {",
          "",
          "[Added Lines]",
          "390:         throw new MigrationClientException(s\"Failed to update partition states: $topicPartitions. ZK transaction had results $resultCodes\")",
          "397:   def tryWriteEntityConfig(",
          "398:     entityType: String,",
          "399:     path: String,",
          "400:     props: Properties,",
          "401:     create: Boolean,",
          "402:     state: ZkMigrationLeadershipState",
          "403:   ): Option[ZkMigrationLeadershipState] = wrapZkException {",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "375:     entity: util.Map[String, String],",
          "376:     quotas: util.Map[String, java.lang.Double],",
          "377:     state: ZkMigrationLeadershipState",
          "380:     val entityMap = entity.asScala",
          "381:     val hasUser = entityMap.contains(ClientQuotaEntity.USER)",
          "382:     val hasClient = entityMap.contains(ClientQuotaEntity.CLIENT_ID)",
          "",
          "[Removed Lines]",
          "378:   ): ZkMigrationLeadershipState = {",
          "",
          "[Added Lines]",
          "426:   ): ZkMigrationLeadershipState = wrapZkException {",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "419:         tryWriteEntityConfig(configType.get, path.get, props, create=true, state) match {",
          "420:           case Some(newStateSecondTry) => newStateSecondTry",
          "422:             s\"Could not write client quotas for $entity on second attempt when using Create instead of SetData\")",
          "423:         }",
          "424:     }",
          "425:   }",
          "428:     val newProducerIdBlockData = ProducerIdBlockZNode.generateProducerIdBlockJson(",
          "429:       new ProducerIdsBlock(-1, nextProducerId, ProducerIdsBlock.PRODUCER_ID_BLOCK_SIZE))",
          "",
          "[Removed Lines]",
          "421:           case None => throw new RuntimeException(",
          "427:   override def writeProducerId(nextProducerId: Long, state: ZkMigrationLeadershipState): ZkMigrationLeadershipState = {",
          "",
          "[Added Lines]",
          "468:           case None => throw new MigrationClientException(",
          "474:   override def writeProducerId(",
          "475:     nextProducerId: Long,",
          "476:     state: ZkMigrationLeadershipState",
          "477:   ): ZkMigrationLeadershipState = wrapZkException {",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "433:     state.withMigrationZkVersion(migrationZkVersion)",
          "434:   }",
          "439:     val configType = resource.`type`() match {",
          "440:       case ConfigResource.Type.BROKER => Some(ConfigType.Broker)",
          "441:       case ConfigResource.Type.TOPIC => Some(ConfigType.Topic)",
          "",
          "[Removed Lines]",
          "436:   override def writeConfigs(resource: ConfigResource,",
          "437:                             configs: util.Map[String, String],",
          "438:                             state: ZkMigrationLeadershipState): ZkMigrationLeadershipState = {",
          "",
          "[Added Lines]",
          "486:   override def writeConfigs(",
          "487:     resource: ConfigResource,",
          "488:     configs: util.Map[String, String],",
          "489:     state: ZkMigrationLeadershipState",
          "490:   ): ZkMigrationLeadershipState = wrapZkException {",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "457:           tryWriteEntityConfig(configType.get, configName, props, create=true, state) match {",
          "458:             case Some(newStateSecondTry) => newStateSecondTry",
          "460:               s\"Could not write ${configType.get} configs on second attempt when using Create instead of SetData.\")",
          "461:           }",
          "462:       }",
          "",
          "[Removed Lines]",
          "459:             case None => throw new RuntimeException(",
          "",
          "[Added Lines]",
          "511:             case None => throw new MigrationClientException(",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "465:       state",
          "466:     }",
          "467:   }",
          "474: }",
          "",
          "[Removed Lines]",
          "469:   override def writeMetadataDeltaToZookeeper(delta: MetadataDelta,",
          "470:                                              image: MetadataImage,",
          "471:                                              state: ZkMigrationLeadershipState): ZkMigrationLeadershipState = {",
          "472:     state",
          "473:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala||core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala": [
          "File: core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala -> core/src/test/scala/integration/kafka/zk/ZkMigrationIntegrationTest.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: import org.apache.kafka.raft.RaftConfig",
          "35: import org.apache.kafka.server.common.{ApiMessageAndVersion, MetadataVersion, ProducerIdsBlock}",
          "36: import org.junit.jupiter.api.Assertions.{assertEquals, assertFalse, assertNotNull, assertTrue}",
          "37: import org.junit.jupiter.api.extension.ExtendWith",
          "38: import org.slf4j.LoggerFactory",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "37: import org.junit.jupiter.api.Timeout",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "43: import scala.collection.Seq",
          "44: import scala.jdk.CollectionConverters._",
          "46: @ExtendWith(value = Array(classOf[ClusterTestExtensions]))",
          "47: class ZkMigrationIntegrationTest {",
          "49:   val log = LoggerFactory.getLogger(classOf[ZkMigrationIntegrationTest])",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "49: @Timeout(120)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "85:     quotas.add(new ClientQuotaAlteration(",
          "86:       new ClientQuotaEntity(Map(\"ip\" -> \"8.8.8.8\").asJava),",
          "87:       List(new ClientQuotaAlteration.Op(\"connection_creation_rate\", 10.0)).asJava))",
          "90:     val zkClient = clusterInstance.asInstanceOf[ZkClusterInstance].getUnderlying().zkClient",
          "91:     val kafkaConfig = clusterInstance.asInstanceOf[ZkClusterInstance].getUnderlying.servers.head.config",
          "",
          "[Removed Lines]",
          "88:     admin.alterClientQuotas(quotas)",
          "",
          "[Added Lines]",
          "91:     admin.alterClientQuotas(quotas).all().get(60, TimeUnit.SECONDS)",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java": [
          "File: metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: import org.apache.kafka.server.common.ApiMessageAndVersion;",
          "35: import org.apache.kafka.server.fault.FaultHandler;",
          "36: import org.slf4j.Logger;",
          "39: import java.util.Collection;",
          "40: import java.util.Collections;",
          "",
          "[Removed Lines]",
          "37: import org.slf4j.LoggerFactory;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "90:         this.zkMigrationClient = zkMigrationClient;",
          "91:         this.propagator = propagator;",
          "92:         this.time = Time.SYSTEM;",
          "94:         this.migrationState = MigrationDriverState.UNINITIALIZED;",
          "95:         this.migrationLeadershipState = ZkMigrationLeadershipState.EMPTY;",
          "97:         this.image = MetadataImage.EMPTY;",
          "98:         this.leaderAndEpoch = LeaderAndEpoch.UNKNOWN;",
          "99:         this.initialZkLoadHandler = initialZkLoadHandler;",
          "",
          "[Removed Lines]",
          "93:         this.log = LoggerFactory.getLogger(KRaftMigrationDriver.class);",
          "96:         this.eventQueue = new KafkaEventQueue(Time.SYSTEM, new LogContext(\"KRaftMigrationDriver\"), \"kraft-migration\");",
          "",
          "[Added Lines]",
          "92:         LogContext logContext = new LogContext(String.format(\"[KRaftMigrationDriver nodeId=%d] \", nodeId));",
          "93:         this.log = logContext.logger(KRaftMigrationDriver.class);",
          "96:         this.eventQueue = new KafkaEventQueue(Time.SYSTEM, logContext, \"kraft-migration\");",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "132:         return true;",
          "133:     }",
          "135:     private boolean areZkBrokersReadyForMigration() {",
          "136:         if (image == MetadataImage.EMPTY) {",
          "138:             log.info(\"Waiting for initial metadata publish before checking if Zk brokers are registered.\");",
          "139:             return false;",
          "140:         }",
          "146:         }",
          "151:             return false;",
          "152:         }",
          "153:     }",
          "155:     private void apply(String name, Function<ZkMigrationLeadershipState, ZkMigrationLeadershipState> stateMutator) {",
          "156:         ZkMigrationLeadershipState beforeState = this.migrationLeadershipState;",
          "157:         ZkMigrationLeadershipState afterState = stateMutator.apply(beforeState);",
          "",
          "[Removed Lines]",
          "141:         Set<Integer> zkRegisteredZkBrokers = zkMigrationClient.readBrokerIdsFromTopicAssignments();",
          "142:         for (BrokerRegistration broker : image.cluster().brokers().values()) {",
          "143:             if (broker.isMigratingZkBroker()) {",
          "144:                 zkRegisteredZkBrokers.remove(broker.id());",
          "145:             }",
          "147:         if (zkRegisteredZkBrokers.isEmpty()) {",
          "148:             return true;",
          "149:         } else {",
          "150:             log.info(\"Still waiting for ZK brokers {} to register with KRaft.\", zkRegisteredZkBrokers);",
          "",
          "[Added Lines]",
          "135:     private boolean imageDoesNotContainAllBrokers(MetadataImage image, Set<Integer> brokerIds) {",
          "136:         for (BrokerRegistration broker : image.cluster().brokers().values()) {",
          "137:             if (broker.isMigratingZkBroker()) {",
          "138:                 brokerIds.remove(broker.id());",
          "139:             }",
          "140:         }",
          "141:         return !brokerIds.isEmpty();",
          "142:     }",
          "152:         Set<Integer> zkBrokerRegistrations = zkMigrationClient.readBrokerIds();",
          "153:         if (imageDoesNotContainAllBrokers(image, zkBrokerRegistrations)) {",
          "154:             log.info(\"Still waiting for ZK brokers {} to register with KRaft.\", zkBrokerRegistrations);",
          "155:             return false;",
          "159:         Set<Integer> zkBrokersWithAssignments = zkMigrationClient.readBrokerIdsFromTopicAssignments();",
          "160:         if (imageDoesNotContainAllBrokers(image, zkBrokersWithAssignments)) {",
          "161:             log.info(\"Still waiting for ZK brokers {} to register with KRaft.\", zkBrokersWithAssignments);",
          "165:         return true;",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "271:     abstract class MigrationEvent implements EventQueue.Event {",
          "272:         @Override",
          "273:         public void handleException(Throwable e) {",
          "276:             } else {",
          "279:             }",
          "280:         }",
          "281:     }",
          "283:     class PollEvent extends MigrationEvent {",
          "",
          "[Removed Lines]",
          "274:             if (e instanceof RejectedExecutionException) {",
          "275:                 log.info(\"Not processing {} because the event queue is closed.\", this);",
          "277:                 KRaftMigrationDriver.this.faultHandler.handleFault(",
          "278:                     \"Unhandled error in \" + this.getClass().getSimpleName(), e);",
          "",
          "[Added Lines]",
          "291:         @SuppressWarnings(\"ThrowableNotThrown\")",
          "294:             if (e instanceof MigrationClientAuthException) {",
          "295:                 KRaftMigrationDriver.this.faultHandler.handleFault(\"Encountered ZooKeeper authentication in \" + this, e);",
          "296:             } else if (e instanceof MigrationClientException) {",
          "297:                 log.info(String.format(\"Encountered ZooKeeper error during event %s. Will retry.\", this), e.getCause());",
          "298:             } else if (e instanceof RejectedExecutionException) {",
          "299:                 log.debug(\"Not processing {} because the event queue is closed.\", this);",
          "301:                 KRaftMigrationDriver.this.faultHandler.handleFault(\"Unhandled error in \" + this, e);",
          "305:         @Override",
          "306:         public String toString() {",
          "307:             return this.getClass().getSimpleName();",
          "308:         }",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "381:     class BecomeZkControllerEvent extends MigrationEvent {",
          "382:         @Override",
          "383:         public void run() throws Exception {",
          "390:                     } else {",
          "396:                     }",
          "401:             }",
          "402:         }",
          "403:     }",
          "",
          "[Removed Lines]",
          "384:             switch (migrationState) {",
          "385:                 case BECOME_CONTROLLER:",
          "387:                     apply(\"BecomeZkLeaderEvent\", zkMigrationClient::claimControllerLeadership);",
          "388:                     if (migrationLeadershipState.zkControllerEpochZkVersion() == -1) {",
          "391:                         if (!migrationLeadershipState.zkMigrationComplete()) {",
          "392:                             transitionTo(MigrationDriverState.ZK_MIGRATION);",
          "393:                         } else {",
          "394:                             transitionTo(MigrationDriverState.KRAFT_CONTROLLER_TO_BROKER_COMM);",
          "395:                         }",
          "397:                     break;",
          "398:                 default:",
          "400:                     break;",
          "",
          "[Added Lines]",
          "412:             if (migrationState == MigrationDriverState.BECOME_CONTROLLER) {",
          "413:                 apply(\"BecomeZkLeaderEvent\", zkMigrationClient::claimControllerLeadership);",
          "414:                 if (migrationLeadershipState.zkControllerEpochZkVersion() == -1) {",
          "415:                     log.debug(\"Unable to claim leadership, will retry until we learn of a different KRaft leader\");",
          "416:                 } else {",
          "417:                     if (!migrationLeadershipState.zkMigrationComplete()) {",
          "418:                         transitionTo(MigrationDriverState.ZK_MIGRATION);",
          "420:                         transitionTo(MigrationDriverState.KRAFT_CONTROLLER_TO_BROKER_COMM);",
          "422:                 }",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClient.java||metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClient.java": [
          "File: metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClient.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClient.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: import org.apache.kafka.common.Uuid;",
          "20: import org.apache.kafka.common.config.ConfigResource;",
          "23: import org.apache.kafka.metadata.PartitionRegistration;",
          "24: import org.apache.kafka.server.common.ApiMessageAndVersion;",
          "",
          "[Removed Lines]",
          "21: import org.apache.kafka.image.MetadataDelta;",
          "22: import org.apache.kafka.image.MetadataImage;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "108:     Set<Integer> readBrokerIds();",
          "110:     Set<Integer> readBrokerIdsFromTopicAssignments();",
          "122: }",
          "",
          "[Removed Lines]",
          "119:     ZkMigrationLeadershipState writeMetadataDeltaToZookeeper(MetadataDelta delta,",
          "120:                                                              MetadataImage image,",
          "121:                                                              ZkMigrationLeadershipState state);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClientAuthException.java||metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClientAuthException.java": [
          "File: metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClientAuthException.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClientAuthException.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "17: package org.apache.kafka.metadata.migration;",
          "22: public class MigrationClientAuthException extends MigrationClientException {",
          "23:     public MigrationClientAuthException(Throwable t) {",
          "24:         super(t);",
          "25:     }",
          "26: }",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClientException.java||metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClientException.java": [
          "File: metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClientException.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/MigrationClientException.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "17: package org.apache.kafka.metadata.migration;",
          "19: import org.apache.kafka.common.KafkaException;",
          "26: public class MigrationClientException extends KafkaException {",
          "27:     public MigrationClientException(String message, Throwable t) {",
          "28:         super(message, t);",
          "29:     }",
          "31:     public MigrationClientException(Throwable t) {",
          "32:         super(t);",
          "33:     }",
          "35:     public MigrationClientException(String message) {",
          "36:         super(message);",
          "37:     }",
          "38: }",
          "",
          "---------------"
        ],
        "metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationDriverTest.java||metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationDriverTest.java": [
          "File: metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationDriverTest.java -> metadata/src/test/java/org/apache/kafka/metadata/migration/KRaftMigrationDriverTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "36: import org.apache.kafka.test.TestUtils;",
          "37: import org.junit.jupiter.api.Assertions;",
          "38: import org.junit.jupiter.api.Test;",
          "40: import java.util.Arrays;",
          "41: import java.util.HashMap;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "39: import org.junit.jupiter.params.ParameterizedTest;",
          "40: import org.junit.jupiter.params.provider.ValueSource;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "45: import java.util.OptionalInt;",
          "46: import java.util.Set;",
          "47: import java.util.concurrent.CompletableFuture;",
          "48: import java.util.concurrent.TimeUnit;",
          "49: import java.util.function.Consumer;",
          "51: public class KRaftMigrationDriverTest {",
          "53:         @Override",
          "54:         public void beginMigration() {",
          "",
          "[Removed Lines]",
          "52:     class NoOpRecordConsumer implements ZkRecordConsumer {",
          "",
          "[Added Lines]",
          "50: import java.util.concurrent.CountDownLatch;",
          "55:     static class NoOpRecordConsumer implements ZkRecordConsumer {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "71:         }",
          "72:     }",
          "76:         private final Set<Integer> brokerIds;",
          "77:         public final Map<ConfigResource, Map<String, String>> capturedConfigs = new HashMap<>();",
          "",
          "[Removed Lines]",
          "74:     class CapturingMigrationClient implements MigrationClient {",
          "",
          "[Added Lines]",
          "77:     static class CapturingMigrationClient implements MigrationClient {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "162:         public Set<Integer> readBrokerIdsFromTopicAssignments() {",
          "163:             return brokerIds;",
          "164:         }",
          "174:     }",
          "178:         public int deltas = 0;",
          "179:         public int images = 0;",
          "",
          "[Removed Lines]",
          "166:         @Override",
          "167:         public ZkMigrationLeadershipState writeMetadataDeltaToZookeeper(",
          "168:             MetadataDelta delta,",
          "169:             MetadataImage image,",
          "170:             ZkMigrationLeadershipState state",
          "171:         ) {",
          "172:             return state;",
          "173:         }",
          "176:     class CountingMetadataPropagator implements LegacyPropagator {",
          "",
          "[Added Lines]",
          "170:     static class CountingMetadataPropagator implements LegacyPropagator {",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "317:         driver.close();",
          "318:     }",
          "",
          "[Removed Lines]",
          "319: }",
          "",
          "[Added Lines]",
          "314:     @ParameterizedTest",
          "315:     @ValueSource(booleans = {true, false})",
          "316:     public void testMigrationWithClientException(boolean authException) throws Exception {",
          "317:         CountingMetadataPropagator metadataPropagator = new CountingMetadataPropagator();",
          "318:         CountDownLatch claimLeaderAttempts = new CountDownLatch(3);",
          "319:         CapturingMigrationClient migrationClient = new CapturingMigrationClient(new HashSet<>(Arrays.asList(1, 2, 3))) {",
          "320:             @Override",
          "321:             public ZkMigrationLeadershipState claimControllerLeadership(ZkMigrationLeadershipState state) {",
          "322:                 if (claimLeaderAttempts.getCount() == 0) {",
          "323:                     return super.claimControllerLeadership(state);",
          "324:                 } else {",
          "325:                     claimLeaderAttempts.countDown();",
          "326:                     if (authException) {",
          "327:                         throw new MigrationClientAuthException(new RuntimeException(\"Some kind of ZK auth error!\"));",
          "328:                     } else {",
          "329:                         throw new MigrationClientException(\"Some kind of ZK error!\");",
          "330:                     }",
          "331:                 }",
          "333:             }",
          "334:         };",
          "335:         MockFaultHandler faultHandler = new MockFaultHandler(\"testMigrationClientExpiration\");",
          "336:         try (KRaftMigrationDriver driver = new KRaftMigrationDriver(",
          "337:             3000,",
          "338:             new NoOpRecordConsumer(),",
          "339:             migrationClient,",
          "340:             metadataPropagator,",
          "341:             metadataPublisher -> { },",
          "342:             faultHandler",
          "343:         )) {",
          "344:             MetadataImage image = MetadataImage.EMPTY;",
          "345:             MetadataDelta delta = new MetadataDelta(image);",
          "347:             driver.start();",
          "348:             delta.replay(zkBrokerRecord(1));",
          "349:             delta.replay(zkBrokerRecord(2));",
          "350:             delta.replay(zkBrokerRecord(3));",
          "351:             MetadataProvenance provenance = new MetadataProvenance(100, 1, 1);",
          "352:             image = delta.apply(provenance);",
          "355:             driver.onControllerChange(new LeaderAndEpoch(OptionalInt.of(3000), 1));",
          "357:             driver.onMetadataUpdate(delta, image, new LogDeltaManifest(provenance,",
          "358:                 new LeaderAndEpoch(OptionalInt.of(3000), 1), 1, 100, 42));",
          "359:             Assertions.assertTrue(claimLeaderAttempts.await(1, TimeUnit.MINUTES));",
          "360:             TestUtils.waitForCondition(() -> driver.migrationState().get(1, TimeUnit.MINUTES).equals(MigrationDriverState.ZK_MIGRATION),",
          "361:                 \"Waiting for KRaftMigrationDriver to enter ZK_MIGRATION state\");",
          "363:             if (authException) {",
          "364:                 Assertions.assertEquals(MigrationClientAuthException.class, faultHandler.firstException().getCause().getClass());",
          "365:             } else {",
          "366:                 Assertions.assertNull(faultHandler.firstException());",
          "367:             }",
          "368:         }",
          "369:     }",
          "370: }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ddd652c672b503de52b6cb2be20c29c7a5e6816f",
      "candidate_info": {
        "commit_hash": "ddd652c672b503de52b6cb2be20c29c7a5e6816f",
        "repo": "apache/kafka",
        "commit_url": "https://github.com/apache/kafka/commit/ddd652c672b503de52b6cb2be20c29c7a5e6816f",
        "files": [
          "core/src/main/scala/kafka/Kafka.scala",
          "core/src/main/scala/kafka/raft/RaftManager.scala",
          "core/src/main/scala/kafka/server/AlterPartitionManager.scala",
          "core/src/main/scala/kafka/server/BrokerLifecycleManager.scala",
          "core/src/main/scala/kafka/server/BrokerServer.scala",
          "core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala",
          "core/src/main/scala/kafka/server/ControllerServer.scala",
          "core/src/main/scala/kafka/server/KafkaConfig.scala",
          "core/src/main/scala/kafka/server/KafkaRaftServer.scala",
          "core/src/main/scala/kafka/server/KafkaServer.scala",
          "core/src/main/scala/kafka/server/SharedServer.scala",
          "core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java",
          "core/src/test/java/kafka/testkit/KafkaClusterTestKit.java",
          "core/src/test/java/kafka/testkit/TestKitNodes.java",
          "core/src/test/scala/integration/kafka/server/KRaftClusterTest.scala",
          "core/src/test/scala/integration/kafka/server/QuorumTestHarness.scala",
          "core/src/test/scala/unit/kafka/KafkaConfigTest.scala",
          "core/src/test/scala/unit/kafka/server/BrokerLifecycleManagerTest.scala",
          "core/src/test/scala/unit/kafka/server/BrokerRegistrationRequestTest.scala",
          "metadata/src/main/java/org/apache/kafka/controller/QuorumController.java",
          "metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java",
          "metadata/src/main/java/org/apache/kafka/image/publisher/SnapshotGenerator.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
          "server-common/src/main/java/org/apache/kafka/queue/KafkaEventQueue.java",
          "tests/kafkatest/sanity_checks/test_bounce.py",
          "tests/kafkatest/services/kafka/kafka.py",
          "tests/kafkatest/services/kafka/quorum.py",
          "tests/kafkatest/tests/core/kraft_upgrade_test.py",
          "tests/kafkatest/tests/core/snapshot_test.py"
        ],
        "message": "MINOR: Standardize KRaft logging, thread names, and terminology (#13390)\n\nStandardize KRaft thread names.\n\n- Always use kebab case. That is, \"my-thread-name\".\n\n- Thread prefixes are just strings, not Option[String] or Optional<String>.\n  If you don't want a prefix, use the empty string.\n\n- Thread prefixes end in a dash (except the empty prefix). Then you can\n  calculate thread names as $prefix + \"my-thread-name\"\n\n- Broker-only components get \"broker-$id-\" as a thread name prefix. For example, \"broker-1-\"\n\n- Controller-only components get \"controller-$id-\" as a thread name prefix. For example, \"controller-1-\"\n\n- Shared components get \"kafka-$id-\" as a thread name prefix. For example, \"kafka-0-\"\n\n- Always pass a prefix to KafkaEventQueue, so that threads have names like\n  \"broker-0-metadata-loader-event-handler\" rather than \"event-handler\". Prior to this PR, we had\n  several threads just named \"EventHandler\" which was not helpful for debugging.\n\n- QuorumController thread name is \"quorum-controller-123-event-handler\"\n\n- Don't set a thread prefix for replication threads started by ReplicaManager. They run only on the\n  broker, and already include the broker ID.\n\nStandardize KRaft slf4j log prefixes.\n\n- Names should be of the form \"[ComponentName id=$id] \". So for a ControllerServer with ID 123, we\n  will have \"[ControllerServer id=123] \"\n\n- For the QuorumController class, use the prefix \"[QuorumController id=$id] \" rather than\n  \"[Controller <nodeId] \", to make it clearer that this is a KRaft controller.\n\n- In BrokerLifecycleManager, add isZkBroker=true to the log prefix for the migration case.\n\nStandardize KRaft terminology.\n\n- All synonyms of combined mode (colocated, coresident, etc.) should be replaced by \"combined\"\n\n- All synonyms of isolated mode (remote, non-colocated, distributed, etc.) should be replaced by\n  \"isolated\".",
        "before_after_code_files": [
          "core/src/main/scala/kafka/Kafka.scala||core/src/main/scala/kafka/Kafka.scala",
          "core/src/main/scala/kafka/raft/RaftManager.scala||core/src/main/scala/kafka/raft/RaftManager.scala",
          "core/src/main/scala/kafka/server/AlterPartitionManager.scala||core/src/main/scala/kafka/server/AlterPartitionManager.scala",
          "core/src/main/scala/kafka/server/BrokerLifecycleManager.scala||core/src/main/scala/kafka/server/BrokerLifecycleManager.scala",
          "core/src/main/scala/kafka/server/BrokerServer.scala||core/src/main/scala/kafka/server/BrokerServer.scala",
          "core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala||core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala",
          "core/src/main/scala/kafka/server/ControllerServer.scala||core/src/main/scala/kafka/server/ControllerServer.scala",
          "core/src/main/scala/kafka/server/KafkaConfig.scala||core/src/main/scala/kafka/server/KafkaConfig.scala",
          "core/src/main/scala/kafka/server/KafkaRaftServer.scala||core/src/main/scala/kafka/server/KafkaRaftServer.scala",
          "core/src/main/scala/kafka/server/KafkaServer.scala||core/src/main/scala/kafka/server/KafkaServer.scala",
          "core/src/main/scala/kafka/server/SharedServer.scala||core/src/main/scala/kafka/server/SharedServer.scala",
          "core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java||core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java",
          "core/src/test/java/kafka/testkit/KafkaClusterTestKit.java||core/src/test/java/kafka/testkit/KafkaClusterTestKit.java",
          "core/src/test/java/kafka/testkit/TestKitNodes.java||core/src/test/java/kafka/testkit/TestKitNodes.java",
          "core/src/test/scala/integration/kafka/server/KRaftClusterTest.scala||core/src/test/scala/integration/kafka/server/KRaftClusterTest.scala",
          "core/src/test/scala/integration/kafka/server/QuorumTestHarness.scala||core/src/test/scala/integration/kafka/server/QuorumTestHarness.scala",
          "core/src/test/scala/unit/kafka/KafkaConfigTest.scala||core/src/test/scala/unit/kafka/KafkaConfigTest.scala",
          "core/src/test/scala/unit/kafka/server/BrokerLifecycleManagerTest.scala||core/src/test/scala/unit/kafka/server/BrokerLifecycleManagerTest.scala",
          "core/src/test/scala/unit/kafka/server/BrokerRegistrationRequestTest.scala||core/src/test/scala/unit/kafka/server/BrokerRegistrationRequestTest.scala",
          "metadata/src/main/java/org/apache/kafka/controller/QuorumController.java||metadata/src/main/java/org/apache/kafka/controller/QuorumController.java",
          "metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java||metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java",
          "metadata/src/main/java/org/apache/kafka/image/publisher/SnapshotGenerator.java||metadata/src/main/java/org/apache/kafka/image/publisher/SnapshotGenerator.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
          "server-common/src/main/java/org/apache/kafka/queue/KafkaEventQueue.java||server-common/src/main/java/org/apache/kafka/queue/KafkaEventQueue.java",
          "tests/kafkatest/sanity_checks/test_bounce.py||tests/kafkatest/sanity_checks/test_bounce.py",
          "tests/kafkatest/services/kafka/kafka.py||tests/kafkatest/services/kafka/kafka.py",
          "tests/kafkatest/services/kafka/quorum.py||tests/kafkatest/services/kafka/quorum.py",
          "tests/kafkatest/tests/core/kraft_upgrade_test.py||tests/kafkatest/tests/core/kraft_upgrade_test.py",
          "tests/kafkatest/tests/core/snapshot_test.py||tests/kafkatest/tests/core/snapshot_test.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java"
          ],
          "candidate": [
            "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java"
          ]
        }
      },
      "candidate_diff": {
        "core/src/main/scala/kafka/Kafka.scala||core/src/main/scala/kafka/Kafka.scala": [
          "File: core/src/main/scala/kafka/Kafka.scala -> core/src/main/scala/kafka/Kafka.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "81:       new KafkaRaftServer(",
          "82:         config,",
          "83:         Time.SYSTEM,",
          "85:       )",
          "86:     }",
          "87:   }",
          "",
          "[Removed Lines]",
          "84:         threadNamePrefix = None",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "core/src/main/scala/kafka/raft/RaftManager.scala||core/src/main/scala/kafka/raft/RaftManager.scala": [
          "File: core/src/main/scala/kafka/raft/RaftManager.scala -> core/src/main/scala/kafka/raft/RaftManager.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "143:   val apiVersions = new ApiVersions()",
          "144:   private val raftConfig = new RaftConfig(config)",
          "145:   private val threadNamePrefix = threadNamePrefixOpt.getOrElse(\"kafka-raft\")",
          "147:   this.logIdent = logContext.logPrefix()",
          "149:   private val scheduler = new KafkaScheduler(1, true, threadNamePrefix + \"-scheduler\")",
          "",
          "[Removed Lines]",
          "146:   private val logContext = new LogContext(s\"[RaftManager nodeId=${config.nodeId}] \")",
          "",
          "[Added Lines]",
          "146:   private val logContext = new LogContext(s\"[RaftManager id=${config.nodeId}] \")",
          "",
          "---------------"
        ],
        "core/src/main/scala/kafka/server/AlterPartitionManager.scala||core/src/main/scala/kafka/server/AlterPartitionManager.scala": [
          "File: core/src/main/scala/kafka/server/AlterPartitionManager.scala -> core/src/main/scala/kafka/server/AlterPartitionManager.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "81:     controllerNodeProvider: ControllerNodeProvider,",
          "82:     time: Time,",
          "83:     metrics: Metrics,",
          "85:     brokerEpochSupplier: () => Long,",
          "86:   ): AlterPartitionManager = {",
          "87:     val channelManager = BrokerToControllerChannelManager(",
          "",
          "[Removed Lines]",
          "84:     threadNamePrefix: Option[String],",
          "",
          "[Added Lines]",
          "84:     threadNamePrefix: String,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "89:       time = time,",
          "90:       metrics = metrics,",
          "91:       config = config,",
          "93:       threadNamePrefix = threadNamePrefix,",
          "94:       retryTimeoutMs = Long.MaxValue",
          "95:     )",
          "",
          "[Removed Lines]",
          "92:       channelName = \"alterPartition\",",
          "",
          "[Added Lines]",
          "92:       channelName = \"alter-partition\",",
          "",
          "---------------"
        ],
        "core/src/main/scala/kafka/server/BrokerLifecycleManager.scala||core/src/main/scala/kafka/server/BrokerLifecycleManager.scala": [
          "File: core/src/main/scala/kafka/server/BrokerLifecycleManager.scala -> core/src/main/scala/kafka/server/BrokerLifecycleManager.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "54: class BrokerLifecycleManager(",
          "55:   val config: KafkaConfig,",
          "56:   val time: Time,",
          "58:   val isZkBroker: Boolean",
          "59: ) extends Logging {",
          "63:   this.logIdent = logContext.logPrefix()",
          "",
          "[Removed Lines]",
          "57:   val threadNamePrefix: Option[String],",
          "61:   val logContext = new LogContext(s\"[BrokerLifecycleManager id=${config.nodeId}] \")",
          "",
          "[Added Lines]",
          "57:   val threadNamePrefix: String,",
          "61:   private def logPrefix(): String = {",
          "62:     val builder = new StringBuilder(\"[BrokerLifecycleManager\")",
          "63:     builder.append(\" id=\").append(config.nodeId)",
          "64:     if (isZkBroker) {",
          "65:       builder.append(\" isZkBroker=true\")",
          "66:     }",
          "67:     builder.append(\"]\")",
          "68:     builder.toString()",
          "69:   }",
          "71:   val logContext = new LogContext(logPrefix())",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "183:   private[server] val eventQueue = new KafkaEventQueue(time,",
          "184:     logContext,",
          "186:     new ShutdownEvent())",
          "",
          "[Removed Lines]",
          "185:     threadNamePrefix.getOrElse(\"\"),",
          "",
          "[Added Lines]",
          "195:     threadNamePrefix + \"lifecycle-manager-\",",
          "",
          "---------------"
        ],
        "core/src/main/scala/kafka/server/BrokerServer.scala||core/src/main/scala/kafka/server/BrokerServer.scala": [
          "File: core/src/main/scala/kafka/server/BrokerServer.scala -> core/src/main/scala/kafka/server/BrokerServer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "64:   val sharedServer: SharedServer,",
          "65:   val initialOfflineDirs: Seq[String],",
          "66: ) extends KafkaBroker {",
          "68:   val config = sharedServer.brokerConfig",
          "69:   val time = sharedServer.time",
          "70:   def metrics = sharedServer.metrics",
          "",
          "[Removed Lines]",
          "67:   val threadNamePrefix = sharedServer.threadNamePrefix",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "173:       lifecycleManager = new BrokerLifecycleManager(config,",
          "174:         time,",
          "176:         isZkBroker = false)",
          "",
          "[Removed Lines]",
          "175:         threadNamePrefix,",
          "",
          "[Added Lines]",
          "174:         s\"broker-${config.nodeId}-\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "183:       brokerTopicStats = new BrokerTopicStats",
          "187:       logDirFailureChannel = new LogDirFailureChannel(config.logDirs.size)",
          "",
          "[Removed Lines]",
          "185:       quotaManagers = QuotaFactory.instantiate(config, metrics, time, threadNamePrefix.getOrElse(\"\"))",
          "",
          "[Added Lines]",
          "185:       quotaManagers = QuotaFactory.instantiate(config, metrics, time, s\"broker-${config.nodeId}-\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "213:         metrics,",
          "214:         config,",
          "215:         channelName = \"forwarding\",",
          "217:         retryTimeoutMs = 60000",
          "218:       )",
          "219:       clientToControllerChannelManager.start()",
          "",
          "[Removed Lines]",
          "216:         threadNamePrefix,",
          "",
          "[Added Lines]",
          "216:         s\"broker-${config.nodeId}-\",",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "242:         controllerNodeProvider,",
          "243:         time = time,",
          "244:         metrics,",
          "246:         brokerEpochSupplier = () => lifecycleManager.brokerEpoch",
          "247:       )",
          "248:       alterPartitionManager.start()",
          "",
          "[Removed Lines]",
          "245:         threadNamePrefix,",
          "",
          "[Added Lines]",
          "245:         s\"broker-${config.nodeId}-\",",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "261:         brokerTopicStats = brokerTopicStats,",
          "262:         isShuttingDown = isShuttingDown,",
          "263:         zkClient = None,",
          "268:       if (config.tokenAuthEnabled) {",
          "",
          "[Removed Lines]",
          "264:         threadNamePrefix = threadNamePrefix,",
          "265:         brokerEpochSupplier = () => lifecycleManager.brokerEpoch)",
          "",
          "[Added Lines]",
          "264:         threadNamePrefix = None, // The ReplicaManager only runs on the broker, and already includes the ID in thread names.",
          "265:         brokerEpochSupplier = () => lifecycleManager.brokerEpoch",
          "266:       )",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "321:         metrics,",
          "322:         config,",
          "323:         \"heartbeat\",",
          "325:         config.brokerSessionTimeoutMs / 2 // KAFKA-14392",
          "326:       )",
          "327:       lifecycleManager.start(",
          "",
          "[Removed Lines]",
          "324:         threadNamePrefix,",
          "",
          "[Added Lines]",
          "325:         s\"broker-${config.nodeId}-\",",
          "",
          "---------------"
        ],
        "core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala||core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala": [
          "File: core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala -> core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "136:     metrics: Metrics,",
          "137:     config: KafkaConfig,",
          "138:     channelName: String,",
          "140:     retryTimeoutMs: Long",
          "141:   ): BrokerToControllerChannelManager = {",
          "142:     new BrokerToControllerChannelManagerImpl(",
          "",
          "[Removed Lines]",
          "139:     threadNamePrefix: Option[String],",
          "",
          "[Added Lines]",
          "139:     threadNamePrefix: String,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "174:   metrics: Metrics,",
          "175:   config: KafkaConfig,",
          "176:   channelName: String,",
          "178:   retryTimeoutMs: Long",
          "179: ) extends BrokerToControllerChannelManager with Logging {",
          "181:   private val manualMetadataUpdater = new ManualMetadataUpdater()",
          "182:   private val apiVersions = new ApiVersions()",
          "183:   private val requestThread = newRequestThread",
          "",
          "[Removed Lines]",
          "177:   threadNamePrefix: Option[String],",
          "180:   private val logContext = new LogContext(s\"[BrokerToControllerChannelManager broker=${config.brokerId} name=$channelName] \")",
          "",
          "[Added Lines]",
          "177:   threadNamePrefix: String,",
          "180:   private val logContext = new LogContext(s\"[BrokerToControllerChannelManager id=${config.brokerId} name=${channelName}] \")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "236:         logContext",
          "237:       )",
          "238:     }",
          "244:     val controllerInformation = controllerNodeProvider.getControllerInfo()",
          "245:     new BrokerToControllerRequestThread(",
          "",
          "[Removed Lines]",
          "239:     val threadName = threadNamePrefix match {",
          "240:       case None => s\"BrokerToControllerChannelManager broker=${config.brokerId} name=$channelName\"",
          "241:       case Some(name) => s\"$name:BrokerToControllerChannelManager broker=${config.brokerId} name=$channelName\"",
          "242:     }",
          "",
          "[Added Lines]",
          "239:     val threadName = s\"${threadNamePrefix}to-controller-${channelName}-channel-manager\"",
          "",
          "---------------"
        ],
        "core/src/main/scala/kafka/server/ControllerServer.scala||core/src/main/scala/kafka/server/ControllerServer.scala": [
          "File: core/src/main/scala/kafka/server/ControllerServer.scala -> core/src/main/scala/kafka/server/ControllerServer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "90:   val config = sharedServer.controllerConfig",
          "91:   val time = sharedServer.time",
          "92:   def metrics = sharedServer.metrics",
          "94:   def raftManager: KafkaRaftManager[ApiMessageAndVersion] = sharedServer.raftManager",
          "96:   val lock = new ReentrantLock()",
          "",
          "[Removed Lines]",
          "93:   val threadNamePrefix = sharedServer.threadNamePrefix.getOrElse(\"\")",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "131:     if (!maybeChangeStatus(SHUTDOWN, STARTING)) return",
          "132:     val startupDeadline = Deadline.fromDelay(time, config.serverMaxStartupTimeMs, TimeUnit.MILLISECONDS)",
          "133:     try {",
          "134:       info(\"Starting controller\")",
          "135:       config.dynamicConfig.initialize(zkClientOpt = None)",
          "137:       maybeChangeStatus(STARTING, STARTED)",
          "140:       metricsGroup.newGauge(\"ClusterId\", () => clusterId)",
          "141:       metricsGroup.newGauge(\"yammer-metrics-count\", () =>  KafkaYammerMetrics.defaultRegistry.allMetrics.size)",
          "",
          "[Removed Lines]",
          "138:       this.logIdent = new LogContext(s\"[ControllerServer id=${config.nodeId}] \").logPrefix()",
          "",
          "[Added Lines]",
          "133:       this.logIdent = new LogContext(s\"[ControllerServer id=${config.nodeId}] \").logPrefix()",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "219:         new QuorumController.Builder(config.nodeId, sharedServer.metaProps.clusterId).",
          "220:           setTime(time).",
          "222:           setConfigSchema(configSchema).",
          "223:           setRaftClient(raftManager.client).",
          "224:           setQuorumFeatures(quorumFeatures).",
          "",
          "[Removed Lines]",
          "221:           setThreadNamePrefix(threadNamePrefix).",
          "",
          "[Added Lines]",
          "220:           setThreadNamePrefix(s\"quorum-controller-${config.nodeId}-\").",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "274:       quotaManagers = QuotaFactory.instantiate(config,",
          "275:         metrics,",
          "276:         time,",
          "278:       clientQuotaMetadataManager = new ClientQuotaMetadataManager(quotaManagers, socketServer.connectionQuotas)",
          "279:       controllerApis = new ControllerApis(socketServer.dataPlaneRequestChannel,",
          "280:         authorizer,",
          "",
          "[Removed Lines]",
          "277:         threadNamePrefix)",
          "",
          "[Added Lines]",
          "276:         s\"controller-${config.nodeId}-\")",
          "",
          "---------------"
        ],
        "core/src/main/scala/kafka/server/KafkaConfig.scala||core/src/main/scala/kafka/server/KafkaConfig.scala": [
          "File: core/src/main/scala/kafka/server/KafkaConfig.scala -> core/src/main/scala/kafka/server/KafkaConfig.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1718:     distinctRoles",
          "1719:   }",
          "1722:     processRoles == Set(BrokerRole, ControllerRole)",
          "1723:   }",
          "",
          "[Removed Lines]",
          "1721:   def isKRaftCoResidentMode: Boolean = {",
          "",
          "[Added Lines]",
          "1721:   def isKRaftCombinedMode: Boolean = {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2280:       validateControllerQuorumVotersMustContainNodeIdForKRaftController()",
          "2281:       validateControllerListenerExistsForKRaftController()",
          "2282:       validateControllerListenerNamesMustAppearInListenersForKRaftController()",
          "2285:       validateNonEmptyQuorumVotersForKRaft()",
          "2286:       validateControlPlaneListenerEmptyForKRaft()",
          "2287:       validateAdvertisedListenersDoesNotContainControllerListenersForKRaftBroker()",
          "",
          "[Removed Lines]",
          "2283:     } else if (isKRaftCoResidentMode) {",
          "",
          "[Added Lines]",
          "2283:     } else if (isKRaftCombinedMode) {",
          "",
          "---------------"
        ],
        "core/src/main/scala/kafka/server/KafkaRaftServer.scala||core/src/main/scala/kafka/server/KafkaRaftServer.scala": [
          "File: core/src/main/scala/kafka/server/KafkaRaftServer.scala -> core/src/main/scala/kafka/server/KafkaRaftServer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "48: class KafkaRaftServer(",
          "49:   config: KafkaConfig,",
          "50:   time: Time,",
          "52: ) extends Server with Logging {",
          "54:   this.logIdent = s\"[KafkaRaftServer nodeId=${config.nodeId}] \"",
          "",
          "[Removed Lines]",
          "51:   threadNamePrefix: Option[String]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "71:     metaProps,",
          "72:     time,",
          "73:     metrics,",
          "75:     controllerQuorumVotersFuture,",
          "76:     new StandardFaultHandlerFactory(),",
          "77:   )",
          "",
          "[Removed Lines]",
          "74:     threadNamePrefix,",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "core/src/main/scala/kafka/server/KafkaServer.scala||core/src/main/scala/kafka/server/KafkaServer.scala": [
          "File: core/src/main/scala/kafka/server/KafkaServer.scala -> core/src/main/scala/kafka/server/KafkaServer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "311:           metrics = metrics,",
          "312:           config = config,",
          "313:           channelName = \"forwarding\",",
          "315:           retryTimeoutMs = config.requestTimeoutMs.longValue",
          "316:         )",
          "317:         clientToControllerChannelManager.start()",
          "",
          "[Removed Lines]",
          "314:           threadNamePrefix = threadNamePrefix,",
          "",
          "[Added Lines]",
          "314:           s\"zk-broker-${config.nodeId}-\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "348:             controllerNodeProvider,",
          "349:             time = time,",
          "350:             metrics = metrics,",
          "352:             brokerEpochSupplier = brokerEpochSupplier",
          "353:           )",
          "354:         } else {",
          "",
          "[Removed Lines]",
          "351:             threadNamePrefix = threadNamePrefix,",
          "",
          "[Added Lines]",
          "351:             s\"zk-broker-${config.nodeId}-\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "379:           logger.info(\"Starting up additional components for ZooKeeper migration\")",
          "380:           lifecycleManager = new BrokerLifecycleManager(config,",
          "381:             time,",
          "383:             isZkBroker = true)",
          "",
          "[Removed Lines]",
          "382:             threadNamePrefix,",
          "",
          "[Added Lines]",
          "382:             s\"zk-broker-${config.nodeId}-\",",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "406:             metrics = metrics,",
          "407:             config = config,",
          "408:             channelName = \"quorum\",",
          "410:             retryTimeoutMs = config.requestTimeoutMs.longValue",
          "411:           )",
          "",
          "[Removed Lines]",
          "409:             threadNamePrefix = threadNamePrefix,",
          "",
          "[Added Lines]",
          "409:             s\"zk-broker-${config.nodeId}-\",",
          "",
          "---------------"
        ],
        "core/src/main/scala/kafka/server/SharedServer.scala||core/src/main/scala/kafka/server/SharedServer.scala": [
          "File: core/src/main/scala/kafka/server/SharedServer.scala -> core/src/main/scala/kafka/server/SharedServer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "88:   val metaProps: MetaProperties,",
          "89:   val time: Time,",
          "90:   private val _metrics: Metrics,",
          "92:   val controllerQuorumVotersFuture: CompletableFuture[util.Map[Integer, AddressSpec]],",
          "93:   val faultHandlerFactory: FaultHandlerFactory",
          "94: ) extends Logging {",
          "",
          "[Removed Lines]",
          "91:   val threadNamePrefix: Option[String],",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "243:           KafkaRaftServer.MetadataTopicId,",
          "244:           time,",
          "245:           metrics,",
          "247:           controllerQuorumVotersFuture,",
          "248:           raftManagerFaultHandler",
          "249:         )",
          "",
          "[Removed Lines]",
          "246:           threadNamePrefix,",
          "",
          "[Added Lines]",
          "245:           Some(s\"kafka-${sharedServerConfig.nodeId}-raft\"), // No dash expected at the end",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "252:         val loaderBuilder = new MetadataLoader.Builder().",
          "253:           setNodeId(metaProps.nodeId).",
          "254:           setTime(time).",
          "256:           setFaultHandler(metadataLoaderFaultHandler).",
          "257:           setHighWaterMarkAccessor(() => raftManager.client.highWatermark())",
          "258:         if (brokerMetrics != null) {",
          "",
          "[Removed Lines]",
          "255:           setThreadNamePrefix(threadNamePrefix.getOrElse(\"\")).",
          "",
          "[Added Lines]",
          "254:           setThreadNamePrefix(s\"kafka-${sharedServerConfig.nodeId}-\").",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "270:           setMaxBytesSinceLastSnapshot(sharedServerConfig.metadataSnapshotMaxNewRecordBytes).",
          "271:           setMaxTimeSinceLastSnapshotNs(TimeUnit.MILLISECONDS.toNanos(sharedServerConfig.metadataSnapshotMaxIntervalMs)).",
          "272:           setDisabledReason(snapshotsDiabledReason).",
          "273:           build()",
          "274:         raftManager.register(loader)",
          "275:         try {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "272:           setThreadNamePrefix(s\"kafka-${sharedServerConfig.nodeId}-\").",
          "",
          "---------------"
        ],
        "core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java||core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java": [
          "File: core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java -> core/src/test/java/kafka/test/junit/RaftClusterInvocationContext.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "68:     private final ClusterConfig clusterConfig;",
          "69:     private final AtomicReference<KafkaClusterTestKit> clusterReference;",
          "70:     private final AtomicReference<EmbeddedZookeeper> zkReference;",
          "74:         this.clusterConfig = clusterConfig;",
          "75:         this.clusterReference = new AtomicReference<>();",
          "76:         this.zkReference = new AtomicReference<>();",
          "78:     }",
          "80:     @Override",
          "",
          "[Removed Lines]",
          "71:     private final boolean isCoResident;",
          "73:     public RaftClusterInvocationContext(ClusterConfig clusterConfig, boolean isCoResident) {",
          "77:         this.isCoResident = isCoResident;",
          "",
          "[Added Lines]",
          "71:     private final boolean isCombined;",
          "73:     public RaftClusterInvocationContext(ClusterConfig clusterConfig, boolean isCombined) {",
          "77:         this.isCombined = isCombined;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "82:         String clusterDesc = clusterConfig.nameTags().entrySet().stream()",
          "83:             .map(Object::toString)",
          "84:             .collect(Collectors.joining(\", \"));",
          "86:     }",
          "88:     @Override",
          "",
          "[Removed Lines]",
          "85:         return String.format(\"[%d] Type=Raft-%s, %s\", invocationIndex, isCoResident ? \"CoReside\" : \"Distributed\", clusterDesc);",
          "",
          "[Added Lines]",
          "85:         return String.format(\"[%d] Type=Raft-%s, %s\", invocationIndex, isCombined ? \"Combined\" : \"Isolated\", clusterDesc);",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "92:             (BeforeTestExecutionCallback) context -> {",
          "93:                 TestKitNodes nodes = new TestKitNodes.Builder().",
          "94:                         setBootstrapMetadataVersion(clusterConfig.metadataVersion()).",
          "96:                         setNumBrokerNodes(clusterConfig.numBrokers()).",
          "97:                         setNumControllerNodes(clusterConfig.numControllers()).build();",
          "98:                 nodes.brokerNodes().forEach((brokerId, brokerNode) -> {",
          "",
          "[Removed Lines]",
          "95:                         setCoResident(isCoResident).",
          "",
          "[Added Lines]",
          "95:                         setCombined(isCombined).",
          "",
          "---------------"
        ],
        "core/src/test/java/kafka/testkit/KafkaClusterTestKit.java||core/src/test/java/kafka/testkit/KafkaClusterTestKit.java": [
          "File: core/src/test/java/kafka/testkit/KafkaClusterTestKit.java -> core/src/test/java/kafka/testkit/KafkaClusterTestKit.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "218:                 baseDirectory = TestUtils.tempDirectory();",
          "219:                 nodes = nodes.copyWithAbsolutePaths(baseDirectory.getAbsolutePath());",
          "220:                 executorService = Executors.newFixedThreadPool(numOfExecutorThreads,",
          "222:                 for (ControllerNode node : nodes.controllerNodes().values()) {",
          "223:                     setupNodeDirectories(baseDirectory, node.metadataDirectory(), Collections.emptyList());",
          "224:                     BootstrapMetadata bootstrapMetadata = BootstrapMetadata.",
          "225:                         fromVersion(nodes.bootstrapMetadataVersion(), \"testkit\");",
          "229:                     SharedServer sharedServer = new SharedServer(createNodeConfig(node),",
          "230:                             MetaProperties.apply(nodes.clusterId().toString(), node.id()),",
          "231:                             Time.SYSTEM,",
          "232:                             new Metrics(),",
          "234:                             connectFutureManager.future,",
          "235:                             faultHandlerFactory);",
          "236:                     ControllerServer controller = null;",
          "",
          "[Removed Lines]",
          "221:                     ThreadUtils.createThreadFactory(\"KafkaClusterTestKit%d\", false));",
          "226:                     String threadNamePrefix = (nodes.brokerNodes().containsKey(node.id())) ?",
          "227:                             String.format(\"colocated%d\", node.id()) :",
          "228:                             String.format(\"controller%d\", node.id());",
          "233:                             Option.apply(threadNamePrefix),",
          "",
          "[Added Lines]",
          "221:                     ThreadUtils.createThreadFactory(\"kafka-cluster-test-kit-executor-%d\", false));",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "261:                             MetaProperties.apply(nodes.clusterId().toString(), id),",
          "262:                             Time.SYSTEM,",
          "263:                             new Metrics(),",
          "265:                             connectFutureManager.future,",
          "266:                             faultHandlerFactory));",
          "267:                     BrokerServer broker = null;",
          "",
          "[Removed Lines]",
          "264:                             Option.apply(String.format(\"broker%d_\", id)),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "304:         }",
          "306:         private String listeners(int node) {",
          "308:                 return \"EXTERNAL://localhost:0,CONTROLLER://localhost:0\";",
          "309:             }",
          "310:             if (nodes.controllerNodes().containsKey(node)) {",
          "",
          "[Removed Lines]",
          "307:             if (nodes.isCoResidentNode(node)) {",
          "",
          "[Added Lines]",
          "302:             if (nodes.isCombined(node)) {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "314:         }",
          "316:         private String roles(int node) {",
          "318:                 return \"broker,controller\";",
          "319:             }",
          "320:             if (nodes.controllerNodes().containsKey(node)) {",
          "",
          "[Removed Lines]",
          "317:             if (nodes.isCoResidentNode(node)) {",
          "",
          "[Added Lines]",
          "312:             if (nodes.isCombined(node)) {",
          "",
          "---------------"
        ],
        "core/src/test/java/kafka/testkit/TestKitNodes.java||core/src/test/java/kafka/testkit/TestKitNodes.java": [
          "File: core/src/test/java/kafka/testkit/TestKitNodes.java -> core/src/test/java/kafka/testkit/TestKitNodes.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: public class TestKitNodes {",
          "35:     public static class Builder {",
          "37:         private Uuid clusterId = null;",
          "38:         private MetadataVersion bootstrapMetadataVersion = null;",
          "39:         private final NavigableMap<Integer, ControllerNode> controllerNodes = new TreeMap<>();",
          "",
          "[Removed Lines]",
          "36:         private boolean coResident = false;",
          "",
          "[Added Lines]",
          "36:         private boolean combined = false;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "49:             return this;",
          "50:         }",
          "54:             return this;",
          "55:         }",
          "",
          "[Removed Lines]",
          "52:         public Builder setCoResident(boolean coResident) {",
          "53:             this.coResident = coResident;",
          "",
          "[Added Lines]",
          "52:         public Builder setCombined(boolean combined) {",
          "53:             this.combined = combined;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "127:         }",
          "129:         private int startControllerId() {",
          "131:                 return startBrokerId();",
          "132:             }",
          "133:             return startBrokerId() + 3000;",
          "",
          "[Removed Lines]",
          "130:             if (coResident) {",
          "",
          "[Added Lines]",
          "130:             if (combined) {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "139:     private final NavigableMap<Integer, ControllerNode> controllerNodes;",
          "140:     private final NavigableMap<Integer, BrokerNode> brokerNodes;",
          "143:         return controllerNodes.containsKey(node) && brokerNodes.containsKey(node);",
          "144:     }",
          "",
          "[Removed Lines]",
          "142:     public boolean isCoResidentNode(int node) {",
          "",
          "[Added Lines]",
          "142:     public boolean isCombined(int node) {",
          "",
          "---------------"
        ],
        "core/src/test/scala/integration/kafka/server/KRaftClusterTest.scala||core/src/test/scala/integration/kafka/server/KRaftClusterTest.scala": [
          "File: core/src/test/scala/integration/kafka/server/KRaftClusterTest.scala -> core/src/test/scala/integration/kafka/server/KRaftClusterTest.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "980:     val cluster = new KafkaClusterTestKit.Builder(",
          "981:       new TestKitNodes.Builder().",
          "982:         setNumBrokerNodes(1).",
          "984:         setNumControllerNodes(1).build()).",
          "985:       setConfigProp(\"client.quota.callback.class\", classOf[DummyClientQuotaCallback].getName).",
          "986:       setConfigProp(DummyClientQuotaCallback.dummyClientQuotaCallbackValueConfigKey, \"0\").",
          "",
          "[Removed Lines]",
          "983:         setCoResident(combinedController).",
          "",
          "[Added Lines]",
          "983:         setCombined(combinedController).",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1022:     val cluster = new KafkaClusterTestKit.Builder(",
          "1023:       new TestKitNodes.Builder().",
          "1024:         setNumBrokerNodes(1).",
          "1026:         setNumControllerNodes(1).build()).",
          "1027:       setConfigProp(\"authorizer.class.name\", classOf[FakeConfigurableAuthorizer].getName).",
          "1028:       build()",
          "",
          "[Removed Lines]",
          "1025:         setCoResident(combinedMode).",
          "",
          "[Added Lines]",
          "1025:         setCombined(combinedMode).",
          "",
          "---------------"
        ],
        "core/src/test/scala/integration/kafka/server/QuorumTestHarness.scala||core/src/test/scala/integration/kafka/server/QuorumTestHarness.scala": [
          "File: core/src/test/scala/integration/kafka/server/QuorumTestHarness.scala -> core/src/test/scala/integration/kafka/server/QuorumTestHarness.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "98:       new MetaProperties(clusterId, config.nodeId),",
          "99:       Time.SYSTEM,",
          "100:       new Metrics(),",
          "102:       controllerQuorumVotersFuture,",
          "103:       faultHandlerFactory)",
          "104:     var broker: BrokerServer = null",
          "",
          "[Removed Lines]",
          "101:       Option(\"Broker%02d_\".format(config.nodeId)),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "316:       metaProperties,",
          "317:       Time.SYSTEM,",
          "318:       new Metrics(),",
          "320:       controllerQuorumVotersFuture,",
          "321:       faultHandlerFactory)",
          "322:     var controllerServer: ControllerServer = null",
          "",
          "[Removed Lines]",
          "319:       Option(\"Controller_\" + testInfo.getDisplayName),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "core/src/test/scala/unit/kafka/KafkaConfigTest.scala||core/src/test/scala/unit/kafka/KafkaConfigTest.scala": [
          "File: core/src/test/scala/unit/kafka/KafkaConfigTest.scala -> core/src/test/scala/unit/kafka/KafkaConfigTest.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "112:   }",
          "114:   @Test",
          "117:     val propertiesFile = new Properties",
          "118:     propertiesFile.setProperty(KafkaConfig.ProcessRolesProp, \"controller,broker\")",
          "119:     propertiesFile.setProperty(KafkaConfig.NodeIdProp, \"1\")",
          "",
          "[Removed Lines]",
          "115:   def testColocatedRoleNodeIdValidation(): Unit = {",
          "",
          "[Added Lines]",
          "115:   def testCombinedRoleNodeIdValidation(): Unit = {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "127:     KafkaConfig.fromProps(propertiesFile)",
          "128:   }",
          "130:   @Test",
          "131:   def testMustContainQuorumVotersIfUsingProcessRoles(): Unit = {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "130:   @Test",
          "131:   def testIsKRaftCombinedMode(): Unit = {",
          "132:     val propertiesFile = new Properties",
          "133:     propertiesFile.setProperty(KafkaConfig.ProcessRolesProp, \"controller,broker\")",
          "134:     propertiesFile.setProperty(KafkaConfig.NodeIdProp, \"1\")",
          "135:     propertiesFile.setProperty(KafkaConfig.QuorumVotersProp, \"1@localhost:9092\")",
          "136:     val config = KafkaConfig.fromProps(propertiesFile)",
          "137:     assertTrue(config.isKRaftCombinedMode)",
          "138:   }",
          "",
          "---------------"
        ],
        "core/src/test/scala/unit/kafka/server/BrokerLifecycleManagerTest.scala||core/src/test/scala/unit/kafka/server/BrokerLifecycleManagerTest.scala": [
          "File: core/src/test/scala/unit/kafka/server/BrokerLifecycleManagerTest.scala -> core/src/test/scala/unit/kafka/server/BrokerLifecycleManagerTest.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "98:   @Test",
          "99:   def testCreateAndClose(): Unit = {",
          "100:     val context = new BrokerLifecycleManagerTestContext(configProperties)",
          "102:     manager.close()",
          "103:   }",
          "105:   @Test",
          "106:   def testCreateStartAndClose(): Unit = {",
          "107:     val context = new BrokerLifecycleManagerTestContext(configProperties)",
          "109:     assertEquals(BrokerState.NOT_RUNNING, manager.state)",
          "110:     manager.start(() => context.highestMetadataOffset.get(),",
          "111:       context.mockChannelManager, context.clusterId, context.advertisedListeners,",
          "",
          "[Removed Lines]",
          "101:     val manager = new BrokerLifecycleManager(context.config, context.time, None, isZkBroker = false)",
          "108:     val manager = new BrokerLifecycleManager(context.config, context.time, None, isZkBroker = false)",
          "",
          "[Added Lines]",
          "101:     val manager = new BrokerLifecycleManager(context.config, context.time, \"create-and-close-\", isZkBroker = false)",
          "108:     val manager = new BrokerLifecycleManager(context.config, context.time, \"create-start-and-close-\", isZkBroker = false)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "120:   @Test",
          "121:   def testSuccessfulRegistration(): Unit = {",
          "122:     val context = new BrokerLifecycleManagerTestContext(configProperties)",
          "124:     val controllerNode = new Node(3000, \"localhost\", 8021)",
          "125:     context.controllerNodeProvider.node.set(controllerNode)",
          "126:     context.mockClient.prepareResponseFrom(new BrokerRegistrationResponse(",
          "",
          "[Removed Lines]",
          "123:     val manager = new BrokerLifecycleManager(context.config, context.time, None, isZkBroker = false)",
          "",
          "[Added Lines]",
          "123:     val manager = new BrokerLifecycleManager(context.config, context.time, \"successful-registration-\", isZkBroker = false)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "140:   def testRegistrationTimeout(): Unit = {",
          "141:     val context = new BrokerLifecycleManagerTestContext(configProperties)",
          "142:     val controllerNode = new Node(3000, \"localhost\", 8021)",
          "144:     context.controllerNodeProvider.node.set(controllerNode)",
          "145:     def newDuplicateRegistrationResponse(): Unit = {",
          "146:       context.mockClient.prepareResponseFrom(new BrokerRegistrationResponse(",
          "",
          "[Removed Lines]",
          "143:     val manager = new BrokerLifecycleManager(context.config, context.time, None, isZkBroker = false)",
          "",
          "[Added Lines]",
          "143:     val manager = new BrokerLifecycleManager(context.config, context.time, \"registration-timeout-\", isZkBroker = false)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "181:   @Test",
          "182:   def testControlledShutdown(): Unit = {",
          "183:     val context = new BrokerLifecycleManagerTestContext(configProperties)",
          "185:     val controllerNode = new Node(3000, \"localhost\", 8021)",
          "186:     context.controllerNodeProvider.node.set(controllerNode)",
          "187:     context.mockClient.prepareResponseFrom(new BrokerRegistrationResponse(",
          "",
          "[Removed Lines]",
          "184:     val manager = new BrokerLifecycleManager(context.config, context.time, None, isZkBroker = false)",
          "",
          "[Added Lines]",
          "184:     val manager = new BrokerLifecycleManager(context.config, context.time, \"controlled-shutdown-\", isZkBroker = false)",
          "",
          "---------------"
        ],
        "core/src/test/scala/unit/kafka/server/BrokerRegistrationRequestTest.scala||core/src/test/scala/unit/kafka/server/BrokerRegistrationRequestTest.scala": [
          "File: core/src/test/scala/unit/kafka/server/BrokerRegistrationRequestTest.scala -> core/src/test/scala/unit/kafka/server/BrokerRegistrationRequestTest.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "69:       new Metrics(),",
          "70:       clusterInstance.anyControllerSocketServer().config,",
          "71:       \"heartbeat\",",
          "73:       10000",
          "74:     )",
          "75:   }",
          "",
          "[Removed Lines]",
          "72:       Some(\"heartbeat\"),",
          "",
          "[Added Lines]",
          "72:       \"test-heartbeat-\",",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/controller/QuorumController.java||metadata/src/main/java/org/apache/kafka/controller/QuorumController.java": [
          "File: metadata/src/main/java/org/apache/kafka/controller/QuorumController.java -> metadata/src/main/java/org/apache/kafka/controller/QuorumController.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "316:             }",
          "318:             if (threadNamePrefix == null) {",
          "320:             }",
          "321:             if (logContext == null) {",
          "323:             }",
          "324:             if (controllerMetrics == null) {",
          "325:                 controllerMetrics = (ControllerMetrics) Class.forName(",
          "",
          "[Removed Lines]",
          "319:                 threadNamePrefix = String.format(\"Node%d_\", nodeId);",
          "322:                 logContext = new LogContext(String.format(\"[Controller %d] \", nodeId));",
          "",
          "[Added Lines]",
          "319:                 threadNamePrefix = String.format(\"quorum-controller-%d-\", nodeId);",
          "322:                 logContext = new LogContext(String.format(\"[QuorumController id=%d] \", nodeId));",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "329:             KafkaEventQueue queue = null;",
          "330:             try {",
          "332:                 return new QuorumController(",
          "333:                     fatalFaultHandler,",
          "334:                     logContext,",
          "",
          "[Removed Lines]",
          "331:                 queue = new KafkaEventQueue(time, logContext, threadNamePrefix + \"QuorumController\");",
          "",
          "[Added Lines]",
          "331:                 queue = new KafkaEventQueue(time, logContext, threadNamePrefix);",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java||metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java": [
          "File: metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java -> metadata/src/main/java/org/apache/kafka/image/loader/MetadataLoader.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "64: public class MetadataLoader implements RaftClient.Listener<ApiMessageAndVersion>, AutoCloseable {",
          "65:     public static class Builder {",
          "66:         private int nodeId = -1;",
          "67:         private Time time = Time.SYSTEM;",
          "68:         private LogContext logContext = null;",
          "70:         private FaultHandler faultHandler = (m, e) -> new FaultHandlerException(m, e);",
          "71:         private MetadataLoaderMetrics metrics = new MetadataLoaderMetrics() {",
          "72:             private volatile long lastAppliedOffset = -1L;",
          "",
          "[Removed Lines]",
          "69:         private String threadNamePrefix = \"\";",
          "",
          "[Added Lines]",
          "67:         private String threadNamePrefix = \"\";",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "97:             return this;",
          "98:         }",
          "102:             return this;",
          "103:         }",
          "107:             return this;",
          "108:         }",
          "",
          "[Removed Lines]",
          "100:         public Builder setTime(Time time) {",
          "101:             this.time = time;",
          "105:         public Builder setThreadNamePrefix(String threadNamePrefix) {",
          "106:             this.threadNamePrefix = threadNamePrefix;",
          "",
          "[Added Lines]",
          "100:         public Builder setThreadNamePrefix(String threadNamePrefix) {",
          "101:             this.threadNamePrefix = threadNamePrefix;",
          "105:         public Builder setTime(Time time) {",
          "106:             this.time = time;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "125:         public MetadataLoader build() {",
          "126:             if (logContext == null) {",
          "128:             }",
          "129:             if (highWaterMarkAccessor == null) {",
          "130:                 throw new RuntimeException(\"You must set the high water mark accessor.\");",
          "",
          "[Removed Lines]",
          "127:                 logContext = new LogContext(\"[MetadataLoader \" + nodeId + \"] \");",
          "",
          "[Added Lines]",
          "127:                 logContext = new LogContext(\"[MetadataLoader id=\" + nodeId + \"] \");",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "132:             return new MetadataLoader(",
          "133:                 time,",
          "134:                 logContext,",
          "135:                 threadNamePrefix,",
          "136:                 faultHandler,",
          "137:                 metrics,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "135:                 nodeId,",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "197:     private MetadataLoader(",
          "198:         Time time,",
          "199:         LogContext logContext,",
          "200:         String threadNamePrefix,",
          "201:         FaultHandler faultHandler,",
          "202:         MetadataLoaderMetrics metrics,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "201:         int nodeId,",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "210:         this.uninitializedPublishers = new LinkedHashMap<>();",
          "211:         this.publishers = new LinkedHashMap<>();",
          "212:         this.image = MetadataImage.EMPTY;",
          "214:     }",
          "216:     private boolean stillNeedToCatchUp(long offset) {",
          "",
          "[Removed Lines]",
          "213:         this.eventQueue = new KafkaEventQueue(time, logContext, threadNamePrefix, new ShutdownEvent());",
          "",
          "[Added Lines]",
          "215:         this.eventQueue = new KafkaEventQueue(time, logContext,",
          "216:                 threadNamePrefix + \"metadata-loader-\",",
          "217:                 new ShutdownEvent());",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/image/publisher/SnapshotGenerator.java||metadata/src/main/java/org/apache/kafka/image/publisher/SnapshotGenerator.java": [
          "File: metadata/src/main/java/org/apache/kafka/image/publisher/SnapshotGenerator.java -> metadata/src/main/java/org/apache/kafka/image/publisher/SnapshotGenerator.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "45:         private long maxBytesSinceLastSnapshot = 100 * 1024L * 1024L;",
          "46:         private long maxTimeSinceLastSnapshotNs = TimeUnit.DAYS.toNanos(1);",
          "47:         private AtomicReference<String> disabledReason = null;",
          "49:         public Builder(Emitter emitter) {",
          "50:             this.emitter = emitter;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "48:         private String threadNamePrefix = \"\";",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "80:             return this;",
          "81:         }",
          "83:         public SnapshotGenerator build() {",
          "84:             if (disabledReason == null) {",
          "85:                 disabledReason = new AtomicReference<>();",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "84:         public Builder setThreadNamePrefix(String threadNamePrefix) {",
          "85:             this.threadNamePrefix = threadNamePrefix;",
          "86:             return this;",
          "87:         }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "91:                 faultHandler,",
          "92:                 maxBytesSinceLastSnapshot,",
          "93:                 maxTimeSinceLastSnapshotNs,",
          "95:             );",
          "96:         }",
          "97:     }",
          "",
          "[Removed Lines]",
          "94:                 disabledReason",
          "",
          "[Added Lines]",
          "100:                 disabledReason,",
          "101:                 threadNamePrefix",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "174:         FaultHandler faultHandler,",
          "175:         long maxBytesSinceLastSnapshot,",
          "176:         long maxTimeSinceLastSnapshotNs,",
          "178:     ) {",
          "179:         this.nodeId = nodeId;",
          "180:         this.time = time;",
          "",
          "[Removed Lines]",
          "177:         AtomicReference<String> disabledReason",
          "",
          "[Added Lines]",
          "184:         AtomicReference<String> disabledReason,",
          "185:         String threadNamePrefix",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "182:         this.faultHandler = faultHandler;",
          "183:         this.maxBytesSinceLastSnapshot = maxBytesSinceLastSnapshot;",
          "184:         this.maxTimeSinceLastSnapshotNs = maxTimeSinceLastSnapshotNs;",
          "186:         this.log = logContext.logger(SnapshotGenerator.class);",
          "187:         this.disabledReason = disabledReason;",
          "189:         resetSnapshotCounters();",
          "190:         log.debug(\"Starting SnapshotGenerator.\");",
          "191:     }",
          "",
          "[Removed Lines]",
          "185:         LogContext logContext = new LogContext(\"[SnapshotGenerator \" + nodeId + \"] \");",
          "188:         this.eventQueue = new KafkaEventQueue(time, logContext, \"SnapshotGenerator\" + nodeId);",
          "",
          "[Added Lines]",
          "193:         LogContext logContext = new LogContext(\"[SnapshotGenerator id=\" + nodeId + \"] \");",
          "196:         this.eventQueue = new KafkaEventQueue(time, logContext, threadNamePrefix + \"snapshot-generator-\");",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java": [
          "File: metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "89:         this.zkMigrationClient = zkMigrationClient;",
          "90:         this.propagator = propagator;",
          "91:         this.time = Time.SYSTEM;",
          "93:         this.log = logContext.logger(KRaftMigrationDriver.class);",
          "94:         this.migrationState = MigrationDriverState.UNINITIALIZED;",
          "95:         this.migrationLeadershipState = ZkMigrationLeadershipState.EMPTY;",
          "97:         this.image = MetadataImage.EMPTY;",
          "98:         this.leaderAndEpoch = LeaderAndEpoch.UNKNOWN;",
          "99:         this.initialZkLoadHandler = initialZkLoadHandler;",
          "",
          "[Removed Lines]",
          "92:         LogContext logContext = new LogContext(String.format(\"[KRaftMigrationDriver nodeId=%d] \", nodeId));",
          "96:         this.eventQueue = new KafkaEventQueue(Time.SYSTEM, logContext, \"kraft-migration\");",
          "",
          "[Added Lines]",
          "92:         LogContext logContext = new LogContext(\"[KRaftMigrationDriver id=\" + nodeId + \"] \");",
          "96:         this.eventQueue = new KafkaEventQueue(Time.SYSTEM, logContext, \"controller-\" + nodeId + \"-migration-driver-\");",
          "",
          "---------------"
        ],
        "server-common/src/main/java/org/apache/kafka/queue/KafkaEventQueue.java||server-common/src/main/java/org/apache/kafka/queue/KafkaEventQueue.java": [
          "File: server-common/src/main/java/org/apache/kafka/queue/KafkaEventQueue.java -> server-common/src/main/java/org/apache/kafka/queue/KafkaEventQueue.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "453:         this.lock = new ReentrantLock();",
          "454:         this.log = logContext.logger(KafkaEventQueue.class);",
          "455:         this.eventHandler = new EventHandler();",
          "457:             this.eventHandler, false);",
          "458:         this.shuttingDown = false;",
          "459:         this.interrupted = false;",
          "",
          "[Removed Lines]",
          "456:         this.eventHandlerThread = new KafkaThread(threadNamePrefix + \"EventHandler\",",
          "",
          "[Added Lines]",
          "456:         this.eventHandlerThread = new KafkaThread(threadNamePrefix + \"event-handler\",",
          "",
          "---------------"
        ],
        "tests/kafkatest/sanity_checks/test_bounce.py||tests/kafkatest/sanity_checks/test_bounce.py": [
          "File: tests/kafkatest/sanity_checks/test_bounce.py -> tests/kafkatest/sanity_checks/test_bounce.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "36:             raise Exception(\"Illegal %s value provided for the test: %s\" % (quorum_size_arg_name, quorum_size))",
          "37:         self.topic = \"topic\"",
          "38:         self.zk = ZookeeperService(test_context, num_nodes=quorum_size) if quorum.for_test(test_context) == quorum.zk else None",
          "40:         self.kafka = KafkaService(test_context, num_nodes=num_kafka_nodes, zk=self.zk,",
          "41:                                   topics={self.topic: {\"partitions\": 1, \"replication-factor\": 1}},",
          "42:                                   controller_num_nodes_override=quorum_size)",
          "",
          "[Removed Lines]",
          "39:         num_kafka_nodes = quorum_size if quorum.for_test(test_context) == quorum.colocated_kraft else 1",
          "",
          "[Added Lines]",
          "39:         num_kafka_nodes = quorum_size if quorum.for_test(test_context) == quorum.combined_kraft else 1",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "53:     # ZooKeeper and KRaft, quorum size = 1",
          "54:     @cluster(num_nodes=4)",
          "55:     @matrix(metadata_quorum=quorum.all, quorum_size=[1])",
          "57:     @cluster(num_nodes=6)",
          "58:     @matrix(metadata_quorum=quorum.all_kraft, quorum_size=[3])",
          "59:     def test_simple_run(self, metadata_quorum, quorum_size):",
          "",
          "[Removed Lines]",
          "56:     # Remote and Co-located KRaft, quorum size = 3",
          "",
          "[Added Lines]",
          "56:     # Isolated and Combined KRaft, quorum size = 3",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "73:             assert num_produced == self.num_messages, \"num_produced: %d, num_messages: %d\" % (num_produced, self.num_messages)",
          "74:             if first_time:",
          "75:                 self.producer.stop()",
          "78:                 self.kafka.restart_cluster()",
          "",
          "[Removed Lines]",
          "76:                 if self.kafka.quorum_info.using_kraft and self.kafka.remote_controller_quorum:",
          "77:                     self.kafka.remote_controller_quorum.restart_cluster()",
          "",
          "[Added Lines]",
          "76:                 if self.kafka.quorum_info.using_kraft and self.kafka.isolated_controller_quorum:",
          "77:                     self.kafka.isolated_controller_quorum.restart_cluster()",
          "",
          "---------------"
        ],
        "tests/kafkatest/services/kafka/kafka.py||tests/kafkatest/services/kafka/kafka.py": [
          "File: tests/kafkatest/services/kafka/kafka.py -> tests/kafkatest/services/kafka/kafka.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "77:         in process.roles (0 when using Zookeeper)",
          "78:     controller_quorum : KafkaService",
          "79:         None when using ZooKeeper, otherwise the Kafka service for the",
          "86:     Kafka Security Protocols",
          "87:     ------------------------",
          "",
          "[Removed Lines]",
          "80:         co-located case or the remote controller quorum service",
          "81:         instance for the remote case",
          "82:     remote_controller_quorum : KafkaService",
          "83:         None for the co-located case or when using ZooKeeper, otherwise",
          "84:         the remote controller quorum service instance",
          "",
          "[Added Lines]",
          "80:         combined case or the isolated controller quorum service",
          "81:         instance for the isolated case",
          "82:     isolated_controller_quorum : KafkaService",
          "83:         None for the combined case or when using ZooKeeper, otherwise",
          "84:         the isolated controller quorum service instance",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "107:     KRaft Quorums",
          "108:     ------------",
          "110:     Do not instantiate a ZookeeperService instance.",
          "113:     automatically start first.  Explicitly stopping Kafka does not stop",
          "115:     tearing down the test (it will stop Kafka first).",
          "117:     KRaft Security Protocols",
          "",
          "[Removed Lines]",
          "109:     Set metadata_quorum accordingly (to COLOCATED_KRAFT or REMOTE_KRAFT).",
          "112:     Starting Kafka will cause any remote controller quorum to",
          "114:     any remote controller quorum, but Ducktape will stop both when",
          "",
          "[Added Lines]",
          "109:     Set metadata_quorum accordingly (to COMBINED_KRAFT or ISOLATED_KRAFT).",
          "112:     Starting Kafka will cause any isolated controller quorum to",
          "114:     any isolated controller quorum, but Ducktape will stop both when",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "119:     The broker-to-controller and inter-controller security protocols",
          "120:     will both initially be set to the inter-broker security protocol.",
          "121:     The broker-to-controller and inter-controller security protocols",
          "123:     thrown when trying to start the service if they are not identical).",
          "124:     The broker-to-controller and inter-controller security protocols",
          "128:     when starting each node:",
          "130:     controller_security_protocol : str",
          "",
          "[Removed Lines]",
          "122:     must be identical for the co-located case (an exception will be",
          "125:     can differ in the remote case.",
          "127:     Set these attributes for the co-located case.  Changes take effect",
          "",
          "[Added Lines]",
          "122:     must be identical for the combined case (an exception will be",
          "125:     can differ in the isolated case.",
          "127:     Set these attributes for the combined case.  Changes take effect",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "136:     intercontroller_sasl_mechanism : str",
          "137:         default GSSAPI, ignored unless using SASL_PLAINTEXT or SASL_SSL",
          "140:     when starting each quorum node), but you must first obtain the",
          "143:     defined above.",
          "145:     \"\"\"",
          "",
          "[Removed Lines]",
          "139:     Set the same attributes for the remote case (changes take effect",
          "141:     service instance for the remote quorum via one of the",
          "142:     'controller_quorum' or 'remote_controller_quorum' attributes as",
          "",
          "[Added Lines]",
          "139:     Set the same attributes for the isolated case (changes take effect",
          "141:     service instance for the isolated quorum via one of the",
          "142:     'controller_quorum' or 'isolated_controller_quorum' attributes as",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "200:                  zk_client_secure=False,",
          "201:                  listener_security_config=ListenerSecurityConfig(), per_node_server_prop_overrides=None,",
          "202:                  extra_kafka_opts=\"\", tls_version=None,",
          "204:                  controller_num_nodes_override=0,",
          "205:                  allow_zk_with_kraft=False,",
          "206:                  quorum_info_provider=None",
          "",
          "[Removed Lines]",
          "203:                  remote_kafka=None,",
          "",
          "[Added Lines]",
          "203:                  isolated_kafka=None,",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "211:             1) Zookeeper quorum:",
          "212:                 The number of brokers is defined by this parameter.",
          "213:                 The broker.id values will be 1..num_nodes.",
          "215:                 The number of nodes having a broker role is defined by this parameter.",
          "216:                 The node.id values will be 1..num_nodes",
          "217:                 The number of nodes having a controller role will by default be 1, 3, or 5 depending on num_nodes",
          "",
          "[Removed Lines]",
          "214:             2) Co-located KRaft quorum:",
          "",
          "[Added Lines]",
          "214:             2) Combined KRaft quorum:",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "231:                     broker having node.id=1: broker.roles=broker+controller",
          "232:                     broker having node.id=2: broker.roles=broker",
          "233:                     broker having node.id=3: broker.roles=broker",
          "235:                 The number of nodes, all of which will have broker.roles=broker, is defined by this parameter.",
          "236:                 The node.id values will be 1..num_nodes",
          "238:                 The number of nodes, all of which will have broker.roles=controller, is defined by this parameter.",
          "239:                 The node.id values will be 3001..(3000 + num_nodes)",
          "240:                 The value passed in is determined by the broker service when that is instantiated, and it uses the",
          "",
          "[Removed Lines]",
          "234:             3) Remote KRaft quorum when instantiating the broker service:",
          "237:             4) Remote KRaft quorum when instantiating the controller service:",
          "",
          "[Added Lines]",
          "234:             3) Isolated KRaft quorum when instantiating the broker service:",
          "237:             4) Isolated KRaft quorum when instantiating the controller service:",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "260:         :param dict per_node_server_prop_overrides: overrides for kafka.properties file keyed by 1-based node number",
          "261:             e.g: {1: [[\"config1\", \"true\"], [\"config2\", \"1000\"]], 2: [[\"config1\", \"false\"], [\"config2\", \"0\"]]}",
          "262:         :param str extra_kafka_opts: jvm args to add to KAFKA_OPTS variable",
          "265:         :param bool allow_zk_with_kraft: if True, then allow a KRaft broker or controller to also use ZooKeeper",
          "266:         :param quorum_info_provider: A function that takes this KafkaService as an argument and returns a ServiceQuorumInfo. If this is None, then the ServiceQuorumInfo is generated from the test context",
          "267:         \"\"\"",
          "269:         self.zk = zk",
          "271:         self.allow_zk_with_kraft = allow_zk_with_kraft",
          "272:         if quorum_info_provider is None:",
          "273:             self.quorum_info = quorum.ServiceQuorumInfo.from_test_context(self, context)",
          "274:         else:",
          "275:             self.quorum_info = quorum_info_provider(self)",
          "276:         self.controller_quorum = None # will define below if necessary",
          "278:         self.configured_for_zk_migration = False",
          "280:         if num_nodes < 1:",
          "",
          "[Removed Lines]",
          "263:         :param KafkaService remote_kafka: process.roles=controller for this cluster when not None; ignored when using ZooKeeper",
          "264:         :param int controller_num_nodes_override: the number of nodes to use in the cluster, instead of 5, 3, or 1 based on num_nodes, if positive, not using ZooKeeper, and remote_kafka is not None; ignored otherwise",
          "270:         self.remote_kafka = remote_kafka",
          "277:         self.remote_controller_quorum = None # will define below if necessary",
          "",
          "[Added Lines]",
          "263:         :param KafkaService isolated_kafka: process.roles=controller for this cluster when not None; ignored when using ZooKeeper",
          "264:         :param int controller_num_nodes_override: the number of nodes to use in the cluster, instead of 5, 3, or 1 based on num_nodes, if positive, not using ZooKeeper, and isolated_kafka is not None; ignored otherwise",
          "270:         self.isolated_kafka = isolated_kafka",
          "277:         self.isolated_controller_quorum = None # will define below if necessary",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "287:                 num_nodes_broker_role = num_nodes",
          "288:                 if self.quorum_info.has_controllers:",
          "289:                     self.num_nodes_controller_role = self.num_kraft_controllers(num_nodes_broker_role, controller_num_nodes_override)",
          "292:             else:",
          "293:                 self.num_nodes_controller_role = num_nodes",
          "297:             # Initially use the inter-broker security protocol for both",
          "298:             # broker-to-controller and inter-controller communication. Both can be explicitly changed later if desired.",
          "301:             self.controller_security_protocol = interbroker_security_protocol",
          "302:             self.controller_sasl_mechanism = interbroker_sasl_mechanism",
          "303:             self.intercontroller_security_protocol = interbroker_security_protocol",
          "304:             self.intercontroller_sasl_mechanism = interbroker_sasl_mechanism",
          "306:             # Ducktape tears down services in the reverse order in which they are created,",
          "308:             # invoking Service.__init__(), so that Ducktape will tear down the quorum last; otherwise",
          "309:             # Ducktape will tear down the controller quorum first, which could lead to problems in",
          "310:             # Kafka and delays in tearing it down (and who knows what else -- it's simply better",
          "312:             if self.quorum_info.has_controllers:",
          "313:                 self.controller_quorum = self",
          "314:             else:",
          "318:                     interbroker_security_protocol=self.intercontroller_security_protocol,",
          "319:                     client_sasl_mechanism=self.controller_sasl_mechanism, interbroker_sasl_mechanism=self.intercontroller_sasl_mechanism,",
          "320:                     authorizer_class_name=authorizer_class_name, version=version, jmx_object_names=jmx_object_names,",
          "321:                     jmx_attributes=jmx_attributes,",
          "322:                     listener_security_config=listener_security_config,",
          "323:                     extra_kafka_opts=extra_kafka_opts, tls_version=tls_version,",
          "325:                     server_prop_overrides=server_prop_overrides",
          "326:                 )",
          "329:         Service.__init__(self, context, num_nodes)",
          "330:         JmxMixin.__init__(self, num_nodes=num_nodes, jmx_object_names=jmx_object_names, jmx_attributes=(jmx_attributes or []),",
          "",
          "[Removed Lines]",
          "290:                     if self.remote_kafka:",
          "291:                         raise Exception(\"Must not specify remote Kafka service with co-located Controller quorum\")",
          "294:                 if not self.remote_kafka:",
          "295:                     raise Exception(\"Must specify remote Kafka service when instantiating remote Controller service (should not happen)\")",
          "299:             # Note, however, that the two must the same if the controller quorum is co-located with the",
          "300:             # brokers.  Different security protocols for the two are only supported with a remote controller quorum.",
          "307:             # so create a service for the remote controller quorum (if we need one) first, before",
          "311:             # to correctly tear down Kafka first, before tearing down the remote controller).",
          "315:                 num_remote_controller_nodes = self.num_kraft_controllers(num_nodes, controller_num_nodes_override)",
          "316:                 self.remote_controller_quorum = KafkaService(",
          "317:                     context, num_remote_controller_nodes, self.zk, security_protocol=self.controller_security_protocol,",
          "324:                     remote_kafka=self, allow_zk_with_kraft=self.allow_zk_with_kraft,",
          "327:                 self.controller_quorum = self.remote_controller_quorum",
          "",
          "[Added Lines]",
          "290:                     if self.isolated_kafka:",
          "291:                         raise Exception(\"Must not specify isolated Kafka service with combined Controller quorum\")",
          "294:                 if not self.isolated_kafka:",
          "295:                     raise Exception(\"Must specify isolated Kafka service when instantiating isolated Controller service (should not happen)\")",
          "299:             # Note, however, that the two must the same if the controller quorum is combined with the",
          "300:             # brokers.  Different security protocols for the two are only supported with a isolated controller quorum.",
          "307:             # so create a service for the isolated controller quorum (if we need one) first, before",
          "311:             # to correctly tear down Kafka first, before tearing down the isolated controller).",
          "315:                 num_isolated_controller_nodes = self.num_kraft_controllers(num_nodes, controller_num_nodes_override)",
          "316:                 self.isolated_controller_quorum = KafkaService(",
          "317:                     context, num_isolated_controller_nodes, self.zk, security_protocol=self.controller_security_protocol,",
          "324:                     isolated_kafka=self, allow_zk_with_kraft=self.allow_zk_with_kraft,",
          "327:                 self.controller_quorum = self.isolated_controller_quorum",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "429:                     node.config = KafkaConfig(**kraft_broker_plus_zk_configs)",
          "430:                 else:",
          "431:                     node.config = KafkaConfig(**kraft_broker_configs)",
          "433:         self.nodes_to_start = self.nodes",
          "435:     def reconfigure_zk_for_migration(self, kraft_quorum):",
          "",
          "[Removed Lines]",
          "432:         self.colocated_nodes_started = 0",
          "",
          "[Added Lines]",
          "432:         self.combined_nodes_started = 0",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "459:         self.server_prop_overrides.extend(props)",
          "460:         del self.port_mappings[kraft_quorum.controller_listener_names]",
          "465:         self.controller_quorum = kraft_quorum",
          "467:     def num_kraft_controllers(self, num_nodes_broker_role, controller_num_nodes_override):",
          "468:         if controller_num_nodes_override < 0:",
          "469:             raise Exception(\"controller_num_nodes_override must not be negative: %i\" % controller_num_nodes_override)",
          "472:                             (controller_num_nodes_override, num_nodes_broker_role))",
          "473:         if controller_num_nodes_override:",
          "474:             return controller_num_nodes_override",
          "",
          "[Removed Lines]",
          "462:         # Set the quorum info to remote KRaft",
          "463:         self.quorum_info = quorum.ServiceQuorumInfo(quorum.remote_kraft, self)",
          "464:         self.remote_controller_quorum = kraft_quorum",
          "470:         if controller_num_nodes_override > num_nodes_broker_role and self.quorum_info.quorum_type == quorum.colocated_kraft:",
          "471:             raise Exception(\"controller_num_nodes_override must not exceed the service's node count in the co-located case: %i > %i\" %",
          "",
          "[Added Lines]",
          "462:         # Set the quorum info to isolated KRaft",
          "463:         self.quorum_info = quorum.ServiceQuorumInfo(quorum.isolated_kraft, self)",
          "464:         self.isolated_controller_quorum = kraft_quorum",
          "470:         if controller_num_nodes_override > num_nodes_broker_role and self.quorum_info.quorum_type == quorum.combined_kraft:",
          "471:             raise Exception(\"controller_num_nodes_override must not exceed the service's node count in the combined case: %i > %i\" %",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "511:     @property",
          "512:     def security_config(self):",
          "513:         if not self._security_config:",
          "515:             # those security protocols are irrelevant there and we don't want to falsely indicate the use of SASL or TLS",
          "516:             security_protocol_to_use=self.security_protocol",
          "517:             interbroker_security_protocol_to_use=self.interbroker_security_protocol",
          "",
          "[Removed Lines]",
          "514:             # we will later change the security protocols to PLAINTEXT if this is a remote KRaft controller case since",
          "",
          "[Added Lines]",
          "514:             # we will later change the security protocols to PLAINTEXT if this is an isolated KRaft controller case since",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "525:             if self.quorum_info.has_controllers:",
          "526:                 if self.intercontroller_security_protocol in SecurityConfig.SASL_SECURITY_PROTOCOLS:",
          "527:                     serves_intercontroller_sasl_mechanism = self.intercontroller_sasl_mechanism",
          "529:                 if self.controller_security_protocol in SecurityConfig.SASL_SECURITY_PROTOCOLS:",
          "530:                     serves_controller_sasl_mechanism = self.controller_sasl_mechanism",
          "531:             # determine if KRaft uses TLS",
          "",
          "[Removed Lines]",
          "528:                     uses_controller_sasl_mechanism = self.intercontroller_sasl_mechanism # won't change from above in co-located case",
          "",
          "[Added Lines]",
          "528:                     uses_controller_sasl_mechanism = self.intercontroller_sasl_mechanism # won't change from above in combined case",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "534:                 # KRaft broker only",
          "535:                 kraft_tls = self.controller_quorum.controller_security_protocol in SecurityConfig.SSL_SECURITY_PROTOCOLS",
          "536:             if self.quorum_info.has_controllers:",
          "538:                 kraft_tls = self.controller_security_protocol in SecurityConfig.SSL_SECURITY_PROTOCOLS \\",
          "539:                            or self.intercontroller_security_protocol in SecurityConfig.SSL_SECURITY_PROTOCOLS",
          "541:             if self.quorum_info.has_controllers and not self.quorum_info.has_brokers:",
          "542:                 security_protocol_to_use=SecurityConfig.PLAINTEXT",
          "543:                 interbroker_security_protocol_to_use=SecurityConfig.PLAINTEXT",
          "",
          "[Removed Lines]",
          "537:                 # remote or co-located KRaft controller",
          "540:             # clear irrelevant security protocols of SASL/TLS implications for remote controller quorum case",
          "",
          "[Added Lines]",
          "537:                 # isolated or combined KRaft controller",
          "540:             # clear irrelevant security protocols of SASL/TLS implications for the isolated controller quorum case",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "556:         self._security_config.properties['security.protocol'] = self.security_protocol",
          "557:         self._security_config.properties['sasl.mechanism'] = self.client_sasl_mechanism",
          "558:         # Ensure we have the right inter-broker security protocol because it may have been mutated",
          "560:         # inter-broker security protocol is not used there).",
          "561:         if (self.quorum_info.using_zk or self.quorum_info.has_brokers):",
          "562:             # in case inter-broker SASL mechanism has changed without changing the inter-broker security protocol",
          "",
          "[Removed Lines]",
          "559:         # since we cached our security config (ignore if this is a remote KRaft controller quorum case; the",
          "",
          "[Added Lines]",
          "559:         # since we cached our security config (ignore if this is an isolated KRaft controller quorum case; the",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "587:         has_sasl = self.security_config.has_sasl",
          "588:         if has_sasl:",
          "589:             if self.minikdc is None:",
          "591:                 if not other_service or not other_service.minikdc:",
          "592:                     nodes_for_kdc = self.nodes.copy()",
          "593:                     if other_service and other_service != self:",
          "",
          "[Removed Lines]",
          "590:                 other_service = self.remote_kafka if self.remote_kafka else self.controller_quorum if self.quorum_info.using_kraft else None",
          "",
          "[Added Lines]",
          "590:                 other_service = self.isolated_kafka if self.isolated_kafka else self.controller_quorum if self.quorum_info.using_kraft else None",
          "",
          "---------------",
          "--- Hunk 17 ---",
          "[Context before]",
          "598:             self.minikdc = None",
          "599:             if self.quorum_info.using_kraft:",
          "600:                 self.controller_quorum.minikdc = None",
          "604:     def alive(self, node):",
          "605:         return len(self.pids(node)) > 0",
          "",
          "[Removed Lines]",
          "601:                 if self.remote_kafka:",
          "602:                     self.remote_kafka.minikdc = None",
          "",
          "[Added Lines]",
          "601:                 if self.isolated_kafka:",
          "602:                     self.isolated_kafka.minikdc = None",
          "",
          "---------------",
          "--- Hunk 18 ---",
          "[Context before]",
          "621:             # This is not supported because both the broker and the controller take the first entry from",
          "622:             # controller.listener.names and the value from sasl.mechanism.controller.protocol;",
          "623:             # they share a single config, so they must both see/use identical values.",
          "625:                             (self.controller_security_protocol, self.controller_sasl_mechanism,",
          "626:                              self.intercontroller_security_protocol, self.intercontroller_sasl_mechanism))",
          "627:         if self.quorum_info.using_zk or self.quorum_info.has_brokers:",
          "",
          "[Removed Lines]",
          "624:             raise Exception(\"Co-located KRaft Brokers (%s/%s) and Controllers (%s/%s) cannot talk to Controllers via different security protocols\" %",
          "",
          "[Added Lines]",
          "624:             raise Exception(\"Combined KRaft Brokers (%s/%s) and Controllers (%s/%s) cannot talk to Controllers via different security protocols\" %",
          "",
          "---------------",
          "--- Hunk 19 ---",
          "[Context before]",
          "629:             self.interbroker_listener.open = True",
          "630:         # we have to wait to decide whether to open the controller port(s)",
          "631:         # because it could be dependent on the particular node in the",
          "633:         # than the number of nodes in the service",
          "635:         self.start_minikdc_if_necessary(add_principals)",
          "",
          "[Removed Lines]",
          "632:         # co-located case where the number of controllers could be less",
          "",
          "[Added Lines]",
          "632:         # combined case where the number of controllers could be less",
          "",
          "---------------",
          "--- Hunk 20 ---",
          "[Context before]",
          "641:         if self.quorum_info.using_zk:",
          "642:             self._ensure_zk_chroot()",
          "647:         Service.start(self, **kwargs)",
          "",
          "[Removed Lines]",
          "644:         if self.remote_controller_quorum:",
          "645:             self.remote_controller_quorum.start()",
          "",
          "[Added Lines]",
          "644:         if self.isolated_controller_quorum:",
          "645:             self.isolated_controller_quorum.start()",
          "",
          "---------------",
          "--- Hunk 21 ---",
          "[Context before]",
          "729:                 if not port.name in controller_listener_names:",
          "730:                     advertised_listeners.append(port.advertised_listener(node))",
          "731:                 protocol_map.append(port.listener_security_protocol())",
          "733:             else self.controller_security_protocol if self.quorum_info.has_brokers_and_controllers and not quorum.NodeQuorumInfo(self.quorum_info, node).has_controller_role \\",
          "734:             else None",
          "735:         if controller_sec_protocol:",
          "",
          "[Removed Lines]",
          "732:         controller_sec_protocol = self.remote_controller_quorum.controller_security_protocol if self.remote_controller_quorum \\",
          "",
          "[Added Lines]",
          "732:         controller_sec_protocol = self.isolated_controller_quorum.controller_security_protocol if self.isolated_controller_quorum \\",
          "",
          "---------------",
          "--- Hunk 22 ---",
          "[Context before]",
          "826:             for controller_listener in self.controller_listener_name_list(node):",
          "827:                 if self.node_quorum_info.has_controller_role:",
          "828:                     self.open_port(controller_listener)",
          "830:                     self.close_port(controller_listener)",
          "832:         self.security_config.setup_node(node)",
          "",
          "[Removed Lines]",
          "829:                 else: # co-located case where node doesn't have a controller",
          "",
          "[Added Lines]",
          "829:                 else: # combined case where node doesn't have a controller",
          "",
          "---------------",
          "--- Hunk 23 ---",
          "[Context before]",
          "845:                                                       for node in self.controller_quorum.nodes[:self.controller_quorum.num_nodes_controller_role]])",
          "846:             # define controller.listener.names",
          "847:             self.controller_listener_names = ','.join(self.controller_listener_name_list(node))",
          "852:         prop_file = self.prop_file(node)",
          "853:         self.logger.info(\"kafka.properties:\")",
          "",
          "[Removed Lines]",
          "848:             # define sasl.mechanism.controller.protocol to match remote quorum if one exists",
          "849:             if self.remote_controller_quorum:",
          "850:                 self.controller_sasl_mechanism = self.remote_controller_quorum.controller_sasl_mechanism",
          "",
          "[Added Lines]",
          "848:             # define sasl.mechanism.controller.protocol to match the isolated quorum if one exists",
          "849:             if self.isolated_controller_quorum:",
          "850:                 self.controller_sasl_mechanism = self.isolated_controller_quorum.controller_sasl_mechanism",
          "",
          "---------------",
          "--- Hunk 24 ---",
          "[Context before]",
          "866:         self.logger.debug(\"Attempting to start KafkaService %s on %s with command: %s\" %\\",
          "867:                           (\"concurrently\" if self.concurrent_start else \"serially\", str(node.account), cmd))",
          "868:         if self.node_quorum_info.has_controller_role and self.node_quorum_info.has_broker_role:",
          "870:         if self.concurrent_start:",
          "871:             node.account.ssh(cmd) # and then don't wait for the startup message",
          "872:         else:",
          "",
          "[Removed Lines]",
          "869:             self.colocated_nodes_started += 1",
          "",
          "[Added Lines]",
          "869:             self.combined_nodes_started += 1",
          "",
          "---------------",
          "--- Hunk 25 ---",
          "[Context before]",
          "938:     def stop_node(self, node, clean_shutdown=True, timeout_sec=60):",
          "939:         pids = self.pids(node)",
          "950:         for pid in pids:",
          "951:             node.account.signal(pid, sig, allow_fail=False)",
          "953:         node_quorum_info = quorum.NodeQuorumInfo(self.quorum_info, node)",
          "958:         try:",
          "959:             wait_until(lambda: len(self.pids(node)) == 0, timeout_sec=timeout_sec,",
          "960:                        err_msg=\"Kafka node failed to stop in %d seconds\" % timeout_sec)",
          "961:         except Exception:",
          "963:                 # it didn't stop",
          "965:             self.thread_dump(node)",
          "966:             raise",
          "",
          "[Removed Lines]",
          "940:         cluster_has_colocated_controllers = self.quorum_info.has_brokers and self.quorum_info.has_controllers",
          "941:         force_sigkill_due_to_too_few_colocated_controllers =\\",
          "942:             clean_shutdown and cluster_has_colocated_controllers\\",
          "943:             and self.colocated_nodes_started < self.controllers_required_for_quorum()",
          "944:         if force_sigkill_due_to_too_few_colocated_controllers:",
          "945:             self.logger.info(\"Forcing node to stop via SIGKILL due to too few co-located KRaft controllers: %i/%i\" %\\",
          "946:                              (self.colocated_nodes_started, self.num_nodes_controller_role))",
          "948:         sig = signal.SIGTERM if clean_shutdown and not force_sigkill_due_to_too_few_colocated_controllers else signal.SIGKILL",
          "954:         node_has_colocated_controllers = node_quorum_info.has_controller_role and node_quorum_info.has_broker_role",
          "955:         if pids and node_has_colocated_controllers:",
          "956:             self.colocated_nodes_started -= 1",
          "962:             if node_has_colocated_controllers:",
          "964:                 self.colocated_nodes_started += 1",
          "",
          "[Added Lines]",
          "940:         cluster_has_combined_controllers = self.quorum_info.has_brokers and self.quorum_info.has_controllers",
          "941:         force_sigkill_due_to_too_few_combined_controllers =\\",
          "942:             clean_shutdown and cluster_has_combined_controllers\\",
          "943:             and self.combined_nodes_started < self.controllers_required_for_quorum()",
          "944:         if force_sigkill_due_to_too_few_combined_controllers:",
          "945:             self.logger.info(\"Forcing node to stop via SIGKILL due to too few combined KRaft controllers: %i/%i\" %\\",
          "946:                              (self.combined_nodes_started, self.num_nodes_controller_role))",
          "948:         sig = signal.SIGTERM if clean_shutdown and not force_sigkill_due_to_too_few_combined_controllers else signal.SIGKILL",
          "954:         node_has_combined_controllers = node_quorum_info.has_controller_role and node_quorum_info.has_broker_role",
          "955:         if pids and node_has_combined_controllers:",
          "956:             self.combined_nodes_started -= 1",
          "962:             if node_has_combined_controllers:",
          "964:                 self.combined_nodes_started += 1",
          "",
          "---------------",
          "--- Hunk 26 ---",
          "[Context before]",
          "1501:         return missing",
          "1503:     def restart_cluster(self, clean_shutdown=True, timeout_sec=60, after_each_broker_restart=None, *args):",
          "1505:         # This is not widely used -- it typically appears in rolling upgrade tests --",
          "1507:         for node in self.nodes:",
          "1508:             self.restart_node(node, clean_shutdown=clean_shutdown, timeout_sec=timeout_sec)",
          "1509:             if after_each_broker_restart is not None:",
          "",
          "[Removed Lines]",
          "1504:         # We do not restart the remote controller quorum if it exists.",
          "1506:         # so we will let tests explicitly decide if/when to restart any remote controller quorum.",
          "",
          "[Added Lines]",
          "1504:         # We do not restart the isolated controller quorum if it exists.",
          "1506:         # so we will let tests explicitly decide if/when to restart any isolated controller quorum.",
          "",
          "---------------"
        ],
        "tests/kafkatest/services/kafka/quorum.py||tests/kafkatest/services/kafka/quorum.py": [
          "File: tests/kafkatest/services/kafka/quorum.py -> tests/kafkatest/services/kafka/quorum.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # the types of metadata quorums we support",
          "17: zk = 'ZK' # ZooKeeper, used before/during the KIP-500 bridge release(s)",
          "21: # How we will parameterize tests that exercise all quorum styles",
          "25: # How we will parameterize tests that exercise all KRaft quorum styles",
          "27: # How we will parameterize tests that are unrelated to upgrades:",
          "28: #   [\u201cZK\u201d] before the KIP-500 bridge release(s)",
          "33: def for_test(test_context):",
          "34:     # A test uses ZooKeeper if it doesn't specify a metadata quorum or if it explicitly specifies ZooKeeper",
          "",
          "[Removed Lines]",
          "18: colocated_kraft = 'COLOCATED_KRAFT' # co-located Controllers in KRaft mode, used during/after the KIP-500 bridge release(s)",
          "19: remote_kraft = 'REMOTE_KRAFT' # separate Controllers in KRaft mode, used during/after the KIP-500 bridge release(s)",
          "22: #   [\u201cZK\u201d, \u201cREMOTE_KRAFT\u201d, \"COLOCATED_KRAFT\"] during the KIP-500 bridge release(s)",
          "23: #   [\u201cREMOTE_KRAFT\u201d, \"COLOCATED_KRAFT\u201d] after the KIP-500 bridge release(s)",
          "24: all = [zk, remote_kraft, colocated_kraft]",
          "26: all_kraft = [remote_kraft, colocated_kraft]",
          "29: #   [\u201cZK\u201d, \u201cREMOTE_KRAFT\u201d] during the KIP-500 bridge release(s) and in preview releases",
          "30: #   [\u201cREMOTE_KRAFT\u201d] after the KIP-500 bridge release(s)",
          "31: all_non_upgrade = [zk, remote_kraft]",
          "",
          "[Added Lines]",
          "18: combined_kraft = 'COMBINED_KRAFT' # combined Controllers in KRaft mode, used during/after the KIP-500 bridge release(s)",
          "19: isolated_kraft = 'ISOLATED_KRAFT' # isolated Controllers in KRaft mode, used during/after the KIP-500 bridge release(s)",
          "22: #   [\u201cZK\u201d, \u201cISOLATED_KRAFT\u201d, \"COMBINED_KRAFT\"] during the KIP-500 bridge release(s)",
          "23: #   [\u201cISOLATED_KRAFT\u201d, \"COMBINED_KRAFT\u201d] after the KIP-500 bridge release(s)",
          "24: all = [zk, isolated_kraft, combined_kraft]",
          "26: all_kraft = [isolated_kraft, combined_kraft]",
          "29: #   [\u201cZK\u201d, \u201cISOLATED_KRAFT\u201d] during the KIP-500 bridge release(s) and in preview releases",
          "30: #   [\u201cISOLATED_KRAFT\u201d] after the KIP-500 bridge release(s)",
          "31: all_non_upgrade = [zk, isolated_kraft]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "44:     Exposes quorum-related information for a KafkaService",
          "46:     Kafka can use either ZooKeeper or a KRaft (Kafka Raft) Controller quorum for",
          "49:     the 'metadata_quorum' parameter defined for the system test: if it",
          "50:     is not explicitly defined, or if it is set to 'ZK', then ZooKeeper",
          "55:     Attributes",
          "56:     ----------",
          "",
          "[Removed Lines]",
          "47:     its metadata.  KRaft Controllers can either be co-located with Kafka in",
          "48:     the same JVM or remote in separate JVMs.  The choice is made via",
          "51:     is used.  If it is explicitly set to 'COLOCATED_KRAFT' then KRaft",
          "52:     controllers will be co-located with the brokers; the value",
          "53:     `REMOTE_KRAFT` indicates remote controllers.",
          "",
          "[Added Lines]",
          "47:     its metadata.  KRaft Controllers can either be combined with Kafka in",
          "48:     the same JVM or isolated in separate JVMs.  The choice is made via",
          "51:     is used.  If it is explicitly set to 'COMBINED_KRAFT' then KRaft",
          "52:     controllers will be combined with the brokers; the value",
          "53:     `ISOLATED_KRAFT` indicates isolated controllers.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "59:         The service for which this instance exposes quorum-related",
          "60:         information",
          "61:     quorum_type : str",
          "63:     using_zk : bool",
          "64:         True iff quorum_type==ZK",
          "65:     using_kraft : bool",
          "",
          "[Removed Lines]",
          "62:         COLOCATED_KRAFT, REMOTE_KRAFT, or ZK",
          "",
          "[Added Lines]",
          "62:         COMBINED_KRAFT, ISOLATED_KRAFT, or ZK",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "67:     has_brokers : bool",
          "68:         Whether there is at least one node with process.roles",
          "69:         containing 'broker'.  True iff using_kraft and the Kafka",
          "72:     has_controllers : bool",
          "73:         Whether there is at least one node with process.roles",
          "74:         containing 'controller'.  True iff quorum_type ==",
          "77:     has_brokers_and_controllers :",
          "79:     \"\"\"",
          "81:     def __init__(self, quorum_type, kafka):",
          "82:         \"\"\"",
          "84:         :param quorum_type : str",
          "86:         :param context : TestContext",
          "87:             The test context within which the this instance and the",
          "88:             given Kafka service is being instantiated",
          "",
          "[Removed Lines]",
          "70:         service doesn't itself have a remote Kafka service (meaning",
          "71:         it is not a remote controller quorum).",
          "75:         COLOCATED_KRAFT or the Kafka service itself has a remote Kafka",
          "76:         service (meaning it is a remote controller quorum).",
          "78:         True iff quorum_type==COLOCATED_KRAFT",
          "85:             The type of quorum being used. Either \"ZK\", \"COLOCATED_KRAFT\", or \"REMOTE_KRAFT\"",
          "",
          "[Added Lines]",
          "70:         service doesn't itself have an isolated Kafka service (meaning",
          "71:         it is not an isolated controller quorum).",
          "75:         COMBINED_KRAFT or the Kafka service itself has an isolated Kafka",
          "76:         service (meaning it is an isolated controller quorum).",
          "78:         True iff quorum_type==COMBINED_KRAFT",
          "85:             The type of quorum being used. Either \"ZK\", \"COMBINED_KRAFT\", or \"ISOLATED_KRAFT\"",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "91:         if quorum_type != zk and kafka.zk and not kafka.allow_zk_with_kraft:",
          "92:             raise Exception(\"Cannot use ZooKeeper while specifying a KRaft metadata quorum unless explicitly allowing it\")",
          "96:         self.kafka = kafka",
          "97:         self.quorum_type = quorum_type",
          "98:         self.using_zk = quorum_type == zk",
          "99:         self.using_kraft = not self.using_zk",
          "104:     @staticmethod",
          "105:     def from_test_context(kafka, context):",
          "",
          "[Removed Lines]",
          "93:         if kafka.remote_kafka and quorum_type != remote_kraft:",
          "94:             raise Exception(\"Cannot specify a remote Kafka service unless using a remote KRaft metadata quorum (should not happen)\")",
          "100:         self.has_brokers = self.using_kraft and not kafka.remote_kafka",
          "101:         self.has_controllers = quorum_type == colocated_kraft or kafka.remote_kafka",
          "102:         self.has_brokers_and_controllers = quorum_type == colocated_kraft",
          "",
          "[Added Lines]",
          "93:         if kafka.isolated_kafka and quorum_type != isolated_kraft:",
          "94:             raise Exception(\"Cannot specify an isolated Kafka service unless using an isolated KRaft metadata quorum (should not happen)\")",
          "100:         self.has_brokers = self.using_kraft and not kafka.isolated_kafka",
          "101:         self.has_controllers = quorum_type == combined_kraft or kafka.isolated_kafka",
          "102:         self.has_brokers_and_controllers = quorum_type == combined_kraft",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "127:         belongs",
          "128:     has_broker_role : bool",
          "129:         True iff using_kraft and the Kafka service doesn't itself have",
          "131:     has_controller_role : bool",
          "133:         the first N in the cluster where N is the number of nodes",
          "136:         quorum).",
          "137:     has_combined_broker_and_controller_roles :",
          "138:         True iff has_broker_role==True and has_controller_role==true",
          "",
          "[Removed Lines]",
          "130:         a remote Kafka service (meaning it is not a remote controller)",
          "132:         True iff quorum_type==COLOCATED_KRAFT and the node is one of",
          "134:         that have a controller role; or the Kafka service itself has a",
          "135:         remote Kafka service (meaning it is a remote controller",
          "",
          "[Added Lines]",
          "130:         an isolated Kafka service (meaning it is not an isolated controller)",
          "132:         True iff quorum_type==COMBINED_KRAFT and the node is one of",
          "134:         that have a controller role; or the Kafka service itself has an",
          "135:         isolated Kafka service (meaning it is an isolated controller",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "145:             belongs",
          "146:         :param node : Node",
          "147:             The particular node for which this information applies.",
          "149:             process.roles contains 'controller' may vary based on the",
          "150:             particular node if the number of controller nodes is less",
          "151:             than the number of nodes in the service.",
          "",
          "[Removed Lines]",
          "148:             In the co-located case, whether or not a node's broker's",
          "",
          "[Added Lines]",
          "148:             In the combined case, whether or not a node's broker's",
          "",
          "---------------"
        ],
        "tests/kafkatest/tests/core/kraft_upgrade_test.py||tests/kafkatest/tests/core/kraft_upgrade_test.py": [
          "File: tests/kafkatest/tests/core/kraft_upgrade_test.py -> tests/kafkatest/tests/core/kraft_upgrade_test.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from ducktape.utils.util import wait_until",
          "19: from kafkatest.services.console_consumer import ConsoleConsumer",
          "20: from kafkatest.services.kafka import KafkaService",
          "22: from kafkatest.services.verifiable_producer import VerifiableProducer",
          "23: from kafkatest.tests.produce_consume_validate import ProduceConsumeValidateTest",
          "24: from kafkatest.utils import is_int",
          "",
          "[Removed Lines]",
          "21: from kafkatest.services.kafka.quorum import remote_kraft, colocated_kraft",
          "",
          "[Added Lines]",
          "21: from kafkatest.services.kafka.quorum import isolated_kraft, combined_kraft",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "109:         assert self.kafka.check_protocol_errors(self)",
          "111:     @cluster(num_nodes=5)",
          "115:         self.run_upgrade(from_kafka_version)",
          "117:     @cluster(num_nodes=8)",
          "121:         self.run_upgrade(from_kafka_version)",
          "",
          "[Removed Lines]",
          "112:     @parametrize(from_kafka_version=str(LATEST_3_1), metadata_quorum=colocated_kraft)",
          "113:     @parametrize(from_kafka_version=str(LATEST_3_2), metadata_quorum=colocated_kraft)",
          "114:     def test_colocated_upgrade(self, from_kafka_version, metadata_quorum):",
          "118:     @parametrize(from_kafka_version=str(LATEST_3_1), metadata_quorum=remote_kraft)",
          "119:     @parametrize(from_kafka_version=str(LATEST_3_2), metadata_quorum=remote_kraft)",
          "120:     def test_non_colocated_upgrade(self, from_kafka_version, metadata_quorum):",
          "",
          "[Added Lines]",
          "112:     @parametrize(from_kafka_version=str(LATEST_3_1), metadata_quorum=combined_kraft)",
          "113:     @parametrize(from_kafka_version=str(LATEST_3_2), metadata_quorum=combined_kraft)",
          "114:     def test_combined_mode_upgrade(self, from_kafka_version, metadata_quorum):",
          "118:     @parametrize(from_kafka_version=str(LATEST_3_1), metadata_quorum=isolated_kraft)",
          "119:     @parametrize(from_kafka_version=str(LATEST_3_2), metadata_quorum=isolated_kraft)",
          "120:     def test_isolated_mode_upgrade(self, from_kafka_version, metadata_quorum):",
          "",
          "---------------"
        ],
        "tests/kafkatest/tests/core/snapshot_test.py||tests/kafkatest/tests/core/snapshot_test.py": [
          "File: tests/kafkatest/tests/core/snapshot_test.py -> tests/kafkatest/tests/core/snapshot_test.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "71:         topic_count = 10",
          "72:         self.topics_created += self.create_n_topics(topic_count)",
          "76:         else:",
          "77:             self.controller_nodes = self.kafka.nodes[:self.kafka.num_nodes_controller_role]",
          "",
          "[Removed Lines]",
          "74:         if self.kafka.remote_controller_quorum:",
          "75:             self.controller_nodes = self.kafka.remote_controller_quorum.nodes",
          "",
          "[Added Lines]",
          "74:         if self.kafka.isolated_controller_quorum:",
          "75:             self.controller_nodes = self.kafka.isolated_controller_quorum.nodes",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "146:     @cluster(num_nodes=9)",
          "147:     @matrix(metadata_quorum=quorum.all_kraft)",
          "149:         \"\"\" Test the ability of a broker to consume metadata snapshots",
          "150:         and to recover the cluster metadata state using them",
          "",
          "[Removed Lines]",
          "148:     def test_broker(self, metadata_quorum=quorum.colocated_kraft):",
          "",
          "[Added Lines]",
          "148:     def test_broker(self, metadata_quorum=quorum.combined_kraft):",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "206:     @cluster(num_nodes=9)",
          "207:     @matrix(metadata_quorum=quorum.all_kraft)",
          "209:         \"\"\" Test the ability of controllers to consume metadata snapshots",
          "210:         and to recover the cluster metadata state using them",
          "",
          "[Removed Lines]",
          "208:     def test_controller(self, metadata_quorum=quorum.colocated_kraft):",
          "",
          "[Added Lines]",
          "208:     def test_controller(self, metadata_quorum=quorum.combined_kraft):",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7a679af687a47dcfee365808301aa99ede1719d0",
      "candidate_info": {
        "commit_hash": "7a679af687a47dcfee365808301aa99ede1719d0",
        "repo": "apache/kafka",
        "commit_url": "https://github.com/apache/kafka/commit/7a679af687a47dcfee365808301aa99ede1719d0",
        "files": [
          "core/src/main/scala/kafka/zk/ZkMigrationClient.scala",
          "core/src/main/scala/kafka/zk/migration/ZkConfigMigrationClient.scala",
          "core/src/main/scala/kafka/zk/migration/ZkTopicMigrationClient.scala",
          "core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala",
          "core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala",
          "core/src/test/scala/unit/kafka/zk/migration/ZkMigrationTestHarness.scala",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/ConfigMigrationClient.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/TopicMigrationClient.java",
          "metadata/src/test/java/org/apache/kafka/metadata/migration/CapturingConfigMigrationClient.java"
        ],
        "message": "KAFKA-15004: Fix configuration dual-write during migration (#13767)\n\nThis patch fixes several small bugs with configuration dual-write during migration.\n\n* Topic configs are not written back to ZK while handling snapshot.\n* New broker/topic configs in KRaft that did not exist in ZK will not be written to ZK.\n* The sensitive configs are not encoded while writing them to Zookeeper.\n* Handle topic configs in ConfigMigrationClient and KRaftMigrationZkWriter#handleConfigsSnapshot\n\nAdded tests to ensure we no longer have the above mentioned issues.\n\nCo-authored-by: Akhilesh Chaganti <akhileshchg@users.noreply.github.com>\nReviewers: Colin P. McCabe <cmccabe@apache.org>",
        "before_after_code_files": [
          "core/src/main/scala/kafka/zk/ZkMigrationClient.scala||core/src/main/scala/kafka/zk/ZkMigrationClient.scala",
          "core/src/main/scala/kafka/zk/migration/ZkConfigMigrationClient.scala||core/src/main/scala/kafka/zk/migration/ZkConfigMigrationClient.scala",
          "core/src/main/scala/kafka/zk/migration/ZkTopicMigrationClient.scala||core/src/main/scala/kafka/zk/migration/ZkTopicMigrationClient.scala",
          "core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala",
          "core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala",
          "core/src/test/scala/unit/kafka/zk/migration/ZkMigrationTestHarness.scala||core/src/test/scala/unit/kafka/zk/migration/ZkMigrationTestHarness.scala",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/ConfigMigrationClient.java||metadata/src/main/java/org/apache/kafka/metadata/migration/ConfigMigrationClient.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java",
          "metadata/src/main/java/org/apache/kafka/metadata/migration/TopicMigrationClient.java||metadata/src/main/java/org/apache/kafka/metadata/migration/TopicMigrationClient.java",
          "metadata/src/test/java/org/apache/kafka/metadata/migration/CapturingConfigMigrationClient.java||metadata/src/test/java/org/apache/kafka/metadata/migration/CapturingConfigMigrationClient.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_branch_evolution": 1,
        "olp_code_files": {
          "patch": [
            "core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala",
            "core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala",
            "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
            "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java"
          ],
          "candidate": [
            "core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala",
            "core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala",
            "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java",
            "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java"
          ]
        }
      },
      "candidate_diff": {
        "core/src/main/scala/kafka/zk/ZkMigrationClient.scala||core/src/main/scala/kafka/zk/ZkMigrationClient.scala": [
          "File: core/src/main/scala/kafka/zk/ZkMigrationClient.scala -> core/src/main/scala/kafka/zk/ZkMigrationClient.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "37: import org.apache.zookeeper.KeeperException.{AuthFailedException, NoAuthException, SessionClosedRequireAuthException}",
          "39: import java.{lang, util}",
          "41: import java.util.function.Consumer",
          "42: import scala.collection.Seq",
          "43: import scala.jdk.CollectionConverters._",
          "",
          "[Removed Lines]",
          "40: import java.util.Properties",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "145:     topicClient.iterateTopics(",
          "146:       util.EnumSet.allOf(classOf[TopicVisitorInterest]),",
          "147:       new TopicVisitor() {",
          "152:         }",
          "184:       }",
          "187:     if (!topicBatch.isEmpty) {",
          "188:       recordConsumer.accept(topicBatch)",
          "",
          "[Removed Lines]",
          "148:       override def visitTopic(topicName: String, topicId: Uuid, assignments: util.Map[Integer, util.List[Integer]]): Unit = {",
          "149:         if (!topicBatch.isEmpty) {",
          "150:           recordConsumer.accept(topicBatch)",
          "151:           topicBatch = new util.ArrayList[ApiMessageAndVersion]()",
          "154:         topicBatch.add(new ApiMessageAndVersion(new TopicRecord()",
          "155:           .setName(topicName)",
          "156:           .setTopicId(topicId), 0.toShort))",
          "157:       }",
          "159:       override def visitPartition(topicIdPartition: TopicIdPartition, partitionRegistration: PartitionRegistration): Unit = {",
          "160:         val record = new PartitionRecord()",
          "161:           .setTopicId(topicIdPartition.topicId())",
          "162:           .setPartitionId(topicIdPartition.partition())",
          "163:           .setReplicas(partitionRegistration.replicas.map(Integer.valueOf).toList.asJava)",
          "164:           .setAddingReplicas(partitionRegistration.addingReplicas.map(Integer.valueOf).toList.asJava)",
          "165:           .setRemovingReplicas(partitionRegistration.removingReplicas.map(Integer.valueOf).toList.asJava)",
          "166:           .setIsr(partitionRegistration.isr.map(Integer.valueOf).toList.asJava)",
          "167:           .setLeader(partitionRegistration.leader)",
          "168:           .setLeaderEpoch(partitionRegistration.leaderEpoch)",
          "169:           .setPartitionEpoch(partitionRegistration.partitionEpoch)",
          "170:           .setLeaderRecoveryState(partitionRegistration.leaderRecoveryState.value())",
          "171:         partitionRegistration.replicas.foreach(brokerIdConsumer.accept(_))",
          "172:         partitionRegistration.addingReplicas.foreach(brokerIdConsumer.accept(_))",
          "173:         topicBatch.add(new ApiMessageAndVersion(record, 0.toShort))",
          "174:       }",
          "176:       override def visitConfigs(topicName: String, topicProps: Properties): Unit = {",
          "177:         topicProps.forEach((key: Any, value: Any) => {",
          "178:           topicBatch.add(new ApiMessageAndVersion(new ConfigRecord()",
          "179:             .setResourceType(ConfigResource.Type.TOPIC.id)",
          "180:             .setResourceName(topicName)",
          "181:             .setName(key.toString)",
          "182:             .setValue(value.toString), 0.toShort))",
          "183:         })",
          "185:     })",
          "",
          "[Added Lines]",
          "147:         override def visitTopic(topicName: String, topicId: Uuid, assignments: util.Map[Integer, util.List[Integer]]): Unit = {",
          "148:           if (!topicBatch.isEmpty) {",
          "149:             recordConsumer.accept(topicBatch)",
          "150:             topicBatch = new util.ArrayList[ApiMessageAndVersion]()",
          "151:           }",
          "153:           topicBatch.add(new ApiMessageAndVersion(new TopicRecord()",
          "154:             .setName(topicName)",
          "155:             .setTopicId(topicId), 0.toShort))",
          "159:           configClient.readTopicConfigs(topicName, (topicConfigs: util.Map[String, String]) => {",
          "160:             topicConfigs.forEach((key: Any, value: Any) => {",
          "161:               topicBatch.add(new ApiMessageAndVersion(new ConfigRecord()",
          "162:                 .setResourceType(ConfigResource.Type.TOPIC.id)",
          "163:                 .setResourceName(topicName)",
          "164:                 .setName(key.toString)",
          "165:                 .setValue(value.toString), 0.toShort))",
          "166:             })",
          "167:           })",
          "170:         override def visitPartition(topicIdPartition: TopicIdPartition, partitionRegistration: PartitionRegistration): Unit = {",
          "171:           val record = new PartitionRecord()",
          "172:             .setTopicId(topicIdPartition.topicId())",
          "173:             .setPartitionId(topicIdPartition.partition())",
          "174:             .setReplicas(partitionRegistration.replicas.map(Integer.valueOf).toList.asJava)",
          "175:             .setAddingReplicas(partitionRegistration.addingReplicas.map(Integer.valueOf).toList.asJava)",
          "176:             .setRemovingReplicas(partitionRegistration.removingReplicas.map(Integer.valueOf).toList.asJava)",
          "177:             .setIsr(partitionRegistration.isr.map(Integer.valueOf).toList.asJava)",
          "178:             .setLeader(partitionRegistration.leader)",
          "179:             .setLeaderEpoch(partitionRegistration.leaderEpoch)",
          "180:             .setPartitionEpoch(partitionRegistration.partitionEpoch)",
          "181:             .setLeaderRecoveryState(partitionRegistration.leaderRecoveryState.value())",
          "182:           partitionRegistration.replicas.foreach(brokerIdConsumer.accept(_))",
          "183:           partitionRegistration.addingReplicas.foreach(brokerIdConsumer.accept(_))",
          "184:           topicBatch.add(new ApiMessageAndVersion(record, 0.toShort))",
          "185:         }",
          "187:     )",
          "",
          "---------------"
        ],
        "core/src/main/scala/kafka/zk/migration/ZkConfigMigrationClient.scala||core/src/main/scala/kafka/zk/migration/ZkConfigMigrationClient.scala": [
          "File: core/src/main/scala/kafka/zk/migration/ZkConfigMigrationClient.scala -> core/src/main/scala/kafka/zk/migration/ZkConfigMigrationClient.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import kafka.zk._",
          "24: import kafka.zookeeper.{CreateRequest, DeleteRequest, SetDataRequest}",
          "25: import org.apache.kafka.clients.admin.ScramMechanism",
          "26: import org.apache.kafka.common.config.{ConfigDef, ConfigResource}",
          "27: import org.apache.kafka.common.errors.InvalidRequestException",
          "28: import org.apache.kafka.common.metadata.ClientQuotaRecord.EntityData",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: import org.apache.kafka.common.config.types.Password",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "36: import java.{lang, util}",
          "37: import java.util.Properties",
          "39: import scala.collection.Seq",
          "40: import scala.jdk.CollectionConverters._",
          "",
          "[Removed Lines]",
          "38: import java.util.function.BiConsumer",
          "",
          "[Added Lines]",
          "39: import java.util.function.{BiConsumer, Consumer}",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "145:     }",
          "146:   }",
          "148:   override def writeConfigs(",
          "149:     configResource: ConfigResource,",
          "150:     configMap: util.Map[String, String],",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "149:   override def iterateTopicConfigs(configConsumer: BiConsumer[String, util.Map[String, String]]): Unit = {",
          "150:     val topicEntities = zkClient.getAllEntitiesWithConfig(ConfigType.Topic)",
          "151:     topicEntities.foreach { topic =>",
          "152:       readTopicConfigs(topic, props => configConsumer.accept(topic, props))",
          "153:     }",
          "154:   }",
          "156:   override def readTopicConfigs(topicName: String, configConsumer: Consumer[util.Map[String, String]]): Unit = {",
          "157:     val topicResource = fromZkEntityName(topicName)",
          "158:     val props = zkClient.getEntityConfigs(ConfigType.Topic, topicResource)",
          "159:     val decodedProps = props.asScala.map { case (key, value) =>",
          "160:       if (DynamicBrokerConfig.isPasswordConfig(key))",
          "161:         key -> passwordEncoder.decode(value).value",
          "162:       else",
          "163:         key -> value",
          "164:     }.toMap.asJava",
          "166:     logAndRethrow(this, s\"Error in topic config consumer. Topic was $topicResource.\") {",
          "167:       configConsumer.accept(decodedProps)",
          "168:     }",
          "169:   }",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "159:     val configName = toZkEntityName(configResource.name())",
          "160:     if (configType.isDefined) {",
          "161:       val props = new Properties()",
          "163:       tryWriteEntityConfig(configType.get, configName, props, create = false, state) match {",
          "164:         case Some(newState) =>",
          "165:           newState",
          "",
          "[Removed Lines]",
          "162:       configMap.forEach { case (key, value) => props.put(key, value) }",
          "",
          "[Added Lines]",
          "185:       configMap.forEach { case (key, value) =>",
          "186:         if (DynamicBrokerConfig.isPasswordConfig(key)) {",
          "187:           props.put(key, passwordEncoder.encode(new Password(value)))",
          "188:         } else",
          "189:           props.put(key, value)",
          "190:       }",
          "",
          "---------------"
        ],
        "core/src/main/scala/kafka/zk/migration/ZkTopicMigrationClient.scala||core/src/main/scala/kafka/zk/migration/ZkTopicMigrationClient.scala": [
          "File: core/src/main/scala/kafka/zk/migration/ZkTopicMigrationClient.scala -> core/src/main/scala/kafka/zk/migration/ZkTopicMigrationClient.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "48:       throw new IllegalArgumentException(\"Must specify at least TOPICS in topic visitor interests.\")",
          "49:     }",
          "50:     val topics = zkClient.getAllTopicsInCluster()",
          "52:     val replicaAssignmentAndTopicIds = zkClient.getReplicaAssignmentAndTopicIdForTopics(topics)",
          "53:     replicaAssignmentAndTopicIds.foreach { case TopicIdReplicaAssignment(topic, topicIdOpt, partitionAssignments) =>",
          "54:       val topicAssignment = partitionAssignments.map { case (partition, assignment) =>",
          "",
          "[Removed Lines]",
          "51:     val topicConfigs = zkClient.getEntitiesConfigs(ConfigType.Topic, topics)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "91:           }",
          "92:         }",
          "93:       }",
          "100:     }",
          "101:   }",
          "",
          "[Removed Lines]",
          "94:       if (interests.contains(TopicVisitorInterest.CONFIGS)) {",
          "95:         val props = topicConfigs(topic)",
          "96:         logAndRethrow(this, s\"Error in topic config consumer. Topic was $topic.\") {",
          "97:           visitor.visitConfigs(topic, props)",
          "98:         }",
          "99:       }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala": [
          "File: core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala -> core/src/test/scala/unit/kafka/zk/migration/ZkConfigMigrationClientTest.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "33: import org.junit.jupiter.api.Assertions.{assertEquals, assertTrue}",
          "34: import org.junit.jupiter.api.Test",
          "36: import java.util.Properties",
          "37: import scala.collection.Map",
          "38: import scala.jdk.CollectionConverters._",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "36: import java.util",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "68:       }",
          "69:     })",
          "71:     migrationState = migrationClient.configClient().deleteConfigs(",
          "72:       new ConfigResource(ConfigResource.Type.BROKER, \"1\"), migrationState)",
          "73:     assertEquals(0, zkClient.getEntityConfigs(ConfigType.Broker, \"1\").size())",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "74:     val newProps = new util.HashMap[String, String]()",
          "75:     newProps.put(KafkaConfig.DefaultReplicationFactorProp, \"2\") // normal config",
          "76:     newProps.put(KafkaConfig.SslKeystorePasswordProp, NEW_SECRET) // sensitive config",
          "77:     migrationState = migrationClient.configClient().writeConfigs(",
          "78:       new ConfigResource(ConfigResource.Type.BROKER, \"1\"), newProps, migrationState)",
          "79:     val actualPropsInZk = zkClient.getEntityConfigs(ConfigType.Broker, \"1\")",
          "80:     assertEquals(2, actualPropsInZk.size())",
          "81:     actualPropsInZk.forEach { case (key, value) =>",
          "82:       if (key == KafkaConfig.SslKeystorePasswordProp) {",
          "83:         assertEquals(NEW_SECRET, encoder.decode(value.toString).value)",
          "84:       } else {",
          "85:         assertEquals(newProps.get(key), value)",
          "86:       }",
          "87:     }",
          "",
          "---------------"
        ],
        "core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala||core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala": [
          "File: core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala -> core/src/test/scala/unit/kafka/zk/migration/ZkMigrationClientTest.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: package kafka.zk.migration",
          "19: import kafka.api.LeaderAndIsr",
          "21: import kafka.coordinator.transaction.ProducerIdManager",
          "24: import org.apache.kafka.common.errors.ControllerMovedException",
          "26: import org.apache.kafka.common.{TopicPartition, Uuid}",
          "28: import org.apache.kafka.metadata.{LeaderRecoveryState, PartitionRegistration}",
          "29: import org.apache.kafka.server.common.ApiMessageAndVersion",
          "30: import org.junit.jupiter.api.Assertions.{assertEquals, assertThrows, assertTrue, fail}",
          "",
          "[Removed Lines]",
          "20: import kafka.controller.LeaderIsrAndControllerEpoch",
          "22: import kafka.zk.migration.ZkMigrationTestHarness",
          "23: import org.apache.kafka.common.config.TopicConfig",
          "25: import org.apache.kafka.common.metadata.{ConfigRecord, MetadataRecordType, ProducerIdsRecord}",
          "27: import org.apache.kafka.metadata.migration.ZkMigrationLeadershipState",
          "",
          "[Added Lines]",
          "20: import kafka.controller.{LeaderIsrAndControllerEpoch, ReplicaAssignment}",
          "22: import kafka.server.{ConfigType, KafkaConfig}",
          "23: import org.apache.kafka.common.config.{ConfigResource, TopicConfig}",
          "25: import org.apache.kafka.common.metadata.{ConfigRecord, MetadataRecordType, PartitionRecord, ProducerIdsRecord, TopicRecord}",
          "27: import org.apache.kafka.image.{MetadataDelta, MetadataImage, MetadataProvenance}",
          "28: import org.apache.kafka.metadata.migration.{KRaftMigrationZkWriter, ZkMigrationLeadershipState}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "252:       .map {_.message() }",
          "253:       .filter(message => MetadataRecordType.fromId(message.apiKey()).equals(MetadataRecordType.CONFIG_RECORD))",
          "254:       .map { _.asInstanceOf[ConfigRecord] }",
          "256:     assertEquals(2, configs.size)",
          "261:   }",
          "262: }",
          "",
          "[Removed Lines]",
          "255:       .toSeq",
          "257:     assertEquals(TopicConfig.FLUSH_MS_CONFIG, configs.head.name())",
          "258:     assertEquals(\"60000\", configs.head.value())",
          "259:     assertEquals(TopicConfig.RETENTION_MS_CONFIG, configs.last.name())",
          "260:     assertEquals(\"300000\", configs.last.value())",
          "",
          "[Added Lines]",
          "256:       .map { record => record.name() -> record.value()}",
          "257:       .toMap",
          "259:     assertTrue(configs.contains(TopicConfig.FLUSH_MS_CONFIG))",
          "260:     assertEquals(\"60000\", configs(TopicConfig.FLUSH_MS_CONFIG))",
          "261:     assertTrue(configs.contains(TopicConfig.RETENTION_MS_CONFIG))",
          "262:     assertEquals(\"300000\", configs(TopicConfig.RETENTION_MS_CONFIG))",
          "263:   }",
          "265:   @Test",
          "266:   def testTopicAndBrokerConfigsMigrationWithSnapshots(): Unit = {",
          "267:     val kraftWriter = new KRaftMigrationZkWriter(migrationClient, (_, operation) => {",
          "268:       migrationState = operation.apply(migrationState)",
          "269:     })",
          "272:     val topicName = \"testTopic\"",
          "273:     val partition = 0",
          "274:     val tp = new TopicPartition(topicName, partition)",
          "275:     val leaderPartition = 1",
          "276:     val leaderEpoch = 100",
          "277:     val partitionEpoch = 10",
          "278:     val brokerId = \"1\"",
          "279:     val replicas = List(1, 2, 3).map(int2Integer).asJava",
          "280:     val topicId = Uuid.randomUuid()",
          "281:     val props = new Properties()",
          "282:     props.put(KafkaConfig.DefaultReplicationFactorProp, \"1\") // normal config",
          "283:     props.put(KafkaConfig.SslKeystorePasswordProp, SECRET) // sensitive config",
          "288:     val delta = new MetadataDelta(MetadataImage.EMPTY)",
          "289:     delta.replay(new TopicRecord()",
          "290:       .setTopicId(topicId)",
          "291:       .setName(topicName)",
          "292:     )",
          "293:     delta.replay(new PartitionRecord()",
          "294:       .setTopicId(topicId)",
          "295:       .setIsr(replicas)",
          "296:       .setLeader(leaderPartition)",
          "297:       .setReplicas(replicas)",
          "298:       .setAddingReplicas(List.empty.asJava)",
          "299:       .setRemovingReplicas(List.empty.asJava)",
          "300:       .setLeaderEpoch(leaderEpoch)",
          "301:       .setPartitionEpoch(partitionEpoch)",
          "302:       .setPartitionId(partition)",
          "303:       .setLeaderRecoveryState(LeaderRecoveryState.RECOVERED.value())",
          "304:     )",
          "306:     props.asScala.foreach { case (key, value) =>",
          "307:       delta.replay(new ConfigRecord()",
          "308:         .setName(key)",
          "309:         .setValue(value)",
          "310:         .setResourceName(topicName)",
          "311:         .setResourceType(ConfigResource.Type.TOPIC.id())",
          "312:       )",
          "313:       delta.replay(new ConfigRecord()",
          "314:         .setName(key)",
          "315:         .setValue(value)",
          "316:         .setResourceName(brokerId)",
          "317:         .setResourceType(ConfigResource.Type.BROKER.id())",
          "318:       )",
          "319:     }",
          "320:     val image = delta.apply(MetadataProvenance.EMPTY)",
          "323:     kraftWriter.handleLoadSnapshot(image)",
          "326:     val topicIdReplicaAssignment =",
          "327:       zkClient.getReplicaAssignmentAndTopicIdForTopics(Set(topicName))",
          "328:     assertEquals(1, topicIdReplicaAssignment.size)",
          "329:     topicIdReplicaAssignment.foreach { assignment =>",
          "330:       assertEquals(topicName, assignment.topic)",
          "331:       assertEquals(Some(topicId), assignment.topicId)",
          "332:       assertEquals(Map(tp -> ReplicaAssignment(replicas.asScala.map(Integer2int).toSeq)),",
          "333:         assignment.assignment)",
          "334:     }",
          "337:     val topicPartitionState = zkClient.getTopicPartitionState(tp)",
          "338:     assertTrue(topicPartitionState.isDefined)",
          "339:     topicPartitionState.foreach { state =>",
          "340:       assertEquals(leaderPartition, state.leaderAndIsr.leader)",
          "341:       assertEquals(leaderEpoch, state.leaderAndIsr.leaderEpoch)",
          "342:       assertEquals(LeaderRecoveryState.RECOVERED, state.leaderAndIsr.leaderRecoveryState)",
          "343:       assertEquals(replicas.asScala.map(Integer2int).toList, state.leaderAndIsr.isr)",
          "344:     }",
          "347:     val brokerProps = zkClient.getEntityConfigs(ConfigType.Broker, brokerId)",
          "348:     val topicProps = zkClient.getEntityConfigs(ConfigType.Topic, topicName)",
          "349:     assertEquals(2, brokerProps.size())",
          "351:     brokerProps.asScala.foreach { case (key, value) =>",
          "352:       if (key == KafkaConfig.SslKeystorePasswordProp) {",
          "353:         assertEquals(SECRET, encoder.decode(value).value)",
          "354:       } else {",
          "355:         assertEquals(props.getProperty(key), value)",
          "356:       }",
          "357:     }",
          "359:     topicProps.asScala.foreach { case (key, value) =>",
          "360:       if (key == KafkaConfig.SslKeystorePasswordProp) {",
          "361:         assertEquals(SECRET, encoder.decode(value).value)",
          "362:       } else {",
          "363:         assertEquals(props.getProperty(key), value)",
          "364:       }",
          "365:     }",
          "",
          "---------------"
        ],
        "core/src/test/scala/unit/kafka/zk/migration/ZkMigrationTestHarness.scala||core/src/test/scala/unit/kafka/zk/migration/ZkMigrationTestHarness.scala": [
          "File: core/src/test/scala/unit/kafka/zk/migration/ZkMigrationTestHarness.scala -> core/src/test/scala/unit/kafka/zk/migration/ZkMigrationTestHarness.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "37:   val SECRET = \"secret\"",
          "39:   val encoder: PasswordEncoder = {",
          "40:     val encoderProps = new Properties()",
          "41:     encoderProps.put(KafkaConfig.ZkConnectProp, \"localhost:1234\") // Get around the config validation",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "39:   val NEW_SECRET = \"newSecret\"",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/metadata/migration/ConfigMigrationClient.java||metadata/src/main/java/org/apache/kafka/metadata/migration/ConfigMigrationClient.java": [
          "File: metadata/src/main/java/org/apache/kafka/metadata/migration/ConfigMigrationClient.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/ConfigMigrationClient.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "25: import java.util.List;",
          "26: import java.util.Map;",
          "27: import java.util.function.BiConsumer;",
          "29: public interface ConfigMigrationClient {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "28: import java.util.function.Consumer;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "39:     void iterateBrokerConfigs(BiConsumer<String, Map<String, String>> configConsumer);",
          "41:     ZkMigrationLeadershipState writeConfigs(",
          "42:         ConfigResource configResource,",
          "43:         Map<String, String> configMap,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "42:     void iterateTopicConfigs(BiConsumer<String, Map<String, String>> configConsumer);",
          "44:     void readTopicConfigs(String topicName, Consumer<Map<String, String>> configConsumer);",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java": [
          "File: metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationDriver.java"
        ],
        "metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java||metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java": [
          "File: metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/KRaftMigrationZkWriter.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "194:     }",
          "196:     void handleConfigsSnapshot(ConfigurationsImage configsImage) {",
          "198:         migrationClient.configClient().iterateBrokerConfigs((broker, configs) -> {",
          "199:             ConfigResource brokerResource = new ConfigResource(ConfigResource.Type.BROKER, broker);",
          "203:             }",
          "204:         });",
          "208:             if (props.isEmpty()) {",
          "211:             } else {",
          "214:             }",
          "215:         });",
          "216:     }",
          "",
          "[Removed Lines]",
          "197:         Set<ConfigResource> brokersToUpdate = new HashSet<>();",
          "200:             Map<String, String> kraftProps = configsImage.configMapForResource(brokerResource);",
          "201:             if (!kraftProps.equals(configs)) {",
          "202:                 brokersToUpdate.add(brokerResource);",
          "206:         brokersToUpdate.forEach(brokerResource -> {",
          "207:             Map<String, String> props = configsImage.configMapForResource(brokerResource);",
          "209:                 operationConsumer.accept(\"Delete configs for broker \" + brokerResource.name(), migrationState ->",
          "210:                     migrationClient.configClient().deleteConfigs(brokerResource, migrationState));",
          "212:                 operationConsumer.accept(\"Update configs for broker \" + brokerResource.name(), migrationState ->",
          "213:                     migrationClient.configClient().writeConfigs(brokerResource, props, migrationState));",
          "",
          "[Added Lines]",
          "198:         Set<ConfigResource> newResources = new HashSet<>();",
          "199:         configsImage.resourceData().keySet().forEach(resource -> {",
          "200:             if (EnumSet.of(ConfigResource.Type.BROKER, ConfigResource.Type.TOPIC).contains(resource.type())) {",
          "201:                 newResources.add(resource);",
          "202:             } else {",
          "203:                 throw new RuntimeException(\"Unknown config resource type \" + resource.type());",
          "204:             }",
          "205:         });",
          "206:         Set<ConfigResource> resourcesToUpdate = new HashSet<>();",
          "207:         BiConsumer<ConfigResource, Map<String, String>> processConfigsForResource = (ConfigResource resource, Map<String, String> configs) -> {",
          "208:             newResources.remove(resource);",
          "209:             Map<String, String> kraftProps = configsImage.configMapForResource(resource);",
          "210:             if (!kraftProps.equals(configs)) {",
          "211:                 resourcesToUpdate.add(resource);",
          "212:             }",
          "213:         };",
          "217:             processConfigsForResource.accept(brokerResource, configs);",
          "218:         });",
          "219:         migrationClient.configClient().iterateTopicConfigs((topic, configs) -> {",
          "220:             ConfigResource topicResource = new ConfigResource(ConfigResource.Type.TOPIC, topic);",
          "221:             processConfigsForResource.accept(topicResource, configs);",
          "222:         });",
          "224:         newResources.forEach(resource -> {",
          "225:             Map<String, String> props = configsImage.configMapForResource(resource);",
          "226:             if (!props.isEmpty()) {",
          "227:                 operationConsumer.accept(\"Create configs for \" + resource.type().name() + \" \" + resource.name(),",
          "228:                     migrationState -> migrationClient.configClient().writeConfigs(resource, props, migrationState));",
          "232:         resourcesToUpdate.forEach(resource -> {",
          "233:             Map<String, String> props = configsImage.configMapForResource(resource);",
          "235:                 operationConsumer.accept(\"Delete configs for \" + resource.type().name() + \" \" + resource.name(),",
          "236:                     migrationState -> migrationClient.configClient().deleteConfigs(resource, migrationState));",
          "238:                 operationConsumer.accept(\"Update configs for \" + resource.type().name() + \" \" + resource.name(),",
          "239:                     migrationState -> migrationClient.configClient().writeConfigs(resource, props, migrationState));",
          "",
          "---------------"
        ],
        "metadata/src/main/java/org/apache/kafka/metadata/migration/TopicMigrationClient.java||metadata/src/main/java/org/apache/kafka/metadata/migration/TopicMigrationClient.java": [
          "File: metadata/src/main/java/org/apache/kafka/metadata/migration/TopicMigrationClient.java -> metadata/src/main/java/org/apache/kafka/metadata/migration/TopicMigrationClient.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import java.util.EnumSet;",
          "25: import java.util.List;",
          "26: import java.util.Map;",
          "29: public interface TopicMigrationClient {",
          "31:     enum TopicVisitorInterest {",
          "32:         TOPICS,",
          "35:     }",
          "37:     interface TopicVisitor {",
          "38:         void visitTopic(String topicName, Uuid topicId, Map<Integer, List<Integer>> assignments);",
          "39:         default void visitPartition(TopicIdPartition topicIdPartition, PartitionRegistration partitionRegistration) {",
          "44:         }",
          "45:     }",
          "",
          "[Removed Lines]",
          "27: import java.util.Properties;",
          "33:         PARTITIONS,",
          "34:         CONFIGS",
          "41:         }",
          "42:         default void visitConfigs(String topicName, Properties topicProps) {",
          "",
          "[Added Lines]",
          "32:         PARTITIONS",
          "",
          "---------------"
        ],
        "metadata/src/test/java/org/apache/kafka/metadata/migration/CapturingConfigMigrationClient.java||metadata/src/test/java/org/apache/kafka/metadata/migration/CapturingConfigMigrationClient.java": [
          "File: metadata/src/test/java/org/apache/kafka/metadata/migration/CapturingConfigMigrationClient.java -> metadata/src/test/java/org/apache/kafka/metadata/migration/CapturingConfigMigrationClient.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import java.util.List;",
          "25: import java.util.Map;",
          "26: import java.util.function.BiConsumer;",
          "28: public class CapturingConfigMigrationClient implements ConfigMigrationClient {",
          "29:     public List<ConfigResource> deletedResources = new ArrayList<>();",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: import java.util.function.Consumer;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "45:     }",
          "47:     @Override",
          "48:     public ZkMigrationLeadershipState writeConfigs(ConfigResource configResource, Map<String, String> configMap, ZkMigrationLeadershipState state) {",
          "49:         writtenConfigs.put(configResource, configMap);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "48:     @Override",
          "49:     public void iterateTopicConfigs(BiConsumer<String, Map<String, String>> configConsumer) {",
          "51:     }",
          "53:     @Override",
          "54:     public void readTopicConfigs(String topicName, Consumer<Map<String, String>> configConsumer) {",
          "56:     }",
          "",
          "---------------"
        ]
      }
    }
  ]
}