{
  "cve_id": "CVE-2021-45456",
  "cve_desc": "Apache kylin checks the legitimacy of the project before executing some commands with the project name passed in by the user. There is a mismatch between what is being checked and what is being used as the shell command argument in DiagnosisService. This may cause an illegal project name to pass the check and perform the following steps, resulting in a command injection vulnerability. This issue affects Apache Kylin 4.0.0.",
  "repo": "apache/kylin",
  "patch_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
  "patch_info": {
    "commit_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
    "repo": "apache/kylin",
    "commit_url": "https://github.com/apache/kylin/commit/f4daf14dde99b934c92ce2c832509f24342bc845",
    "files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "server/src/main/webapp/WEB-INF/web.xml"
    ],
    "message": "test fix",
    "before_after_code_files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java"
    ]
  },
  "patch_diff": {
    "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "3403:     public String getKerberosPrincipal() {",
      "3404:         return getOptional(\"kylin.kerberos.principal\");",
      "3405:     }",
      "3406: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "3407:     public String getEncryptCipherIvSpec() {",
      "3408:         return getOptional(\"kylin.security.encrypt.cipher.ivSpec\", \"AAAAAAAAAAAAAAAA\");",
      "3409:     }",
      "",
      "---------------"
    ],
    "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java -> core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "25: import java.security.NoSuchAlgorithmException;",
      "27: import org.apache.commons.codec.binary.Base64;",
      "29: import javax.crypto.Cipher;",
      "30: import javax.crypto.NoSuchPaddingException;",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "28: import org.apache.kylin.common.KylinConfig;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "42:             InvalidKeyException, NoSuchPaddingException, NoSuchAlgorithmException, UnsupportedEncodingException {",
      "43:         Cipher cipher = Cipher.getInstance(\"AES/CFB/PKCS5Padding\");",
      "44:         final SecretKeySpec secretKey = new SecretKeySpec(key, \"AES\");",
      "46:         cipher.init(cipherMode, secretKey, ivSpec);",
      "47:         return cipher;",
      "48:     }",
      "",
      "[Removed Lines]",
      "45:         IvParameterSpec ivSpec = new IvParameterSpec(\"AAAAAAAAAAAAAAAA\".getBytes(\"UTF-8\"));",
      "",
      "[Added Lines]",
      "46:         IvParameterSpec ivSpec = new IvParameterSpec(KylinConfig.getInstanceFromEnv().getEncryptCipherIvSpec().getBytes(\"UTF-8\"));",
      "",
      "---------------"
    ],
    "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java": [
      "File: core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java -> core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: package org.apache.kylin.common.util;",
      "21: import org.junit.Assert;",
      "22: import org.junit.Test;",
      "26:     @Test",
      "27:     public void testAESEncrypt(){",
      "",
      "[Removed Lines]",
      "24: public class EncryptUtilTest {",
      "",
      "[Added Lines]",
      "21: import org.junit.After;",
      "23: import org.junit.Before;",
      "26: public class EncryptUtilTest extends LocalFileMetadataTestCase {",
      "27:     @Before",
      "28:     public void setUp() throws Exception {",
      "29:         this.createTestMetadata();",
      "30:     }",
      "32:     @After",
      "33:     public void after() throws Exception {",
      "34:         this.cleanupTestMetadata();",
      "35:     }",
      "",
      "---------------"
    ],
    "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java": [
      "File: server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java -> server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "87:     public String dumpProjectDiagnosisInfo(String project, File exportPath) throws IOException {",
      "88:         Message msg = MsgPicker.getMsg();",
      "89:         ProjectInstance projectInstance =",
      "90:                 ProjectManager.getInstance(KylinConfig.getInstanceFromEnv())",
      "92:         if (null == projectInstance) {",
      "93:             throw new BadRequestException(",
      "95:         }",
      "96:         aclEvaluate.checkProjectOperationPermission(projectInstance);",
      "98:         runDiagnosisCLI(args);",
      "99:         return getDiagnosisPackageName(exportPath);",
      "100:     }",
      "",
      "[Removed Lines]",
      "91:                         .getProject(ValidateUtil.convertStringToBeAlphanumericUnderscore(project));",
      "94:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), project));",
      "97:         String[] args = { project, exportPath.getAbsolutePath() };",
      "",
      "[Added Lines]",
      "89:         String projectName = ValidateUtil.convertStringToBeAlphanumericUnderscore(project);",
      "92:                         .getProject(projectName);",
      "95:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), projectName));",
      "98:         String[] args = { projectName, exportPath.getAbsolutePath() };",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "a9e050d73eecbbb6c54e7adadc86e0cac3f6d241",
      "candidate_info": {
        "commit_hash": "a9e050d73eecbbb6c54e7adadc86e0cac3f6d241",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/a9e050d73eecbbb6c54e7adadc86e0cac3f6d241",
        "files": [
          "core-common/pom.xml",
          "core-common/src/main/java/org/apache/kylin/common/persistence/JDBCResourceStore.java",
          "core-common/src/main/java/org/apache/kylin/common/persistence/ResourceTool.java",
          "core-common/src/main/resources/metadata-jdbc-postgresql.properties",
          "pom.xml"
        ],
        "message": "KYLIN-5181 Support to use postgreSQL to store metadata\n\n* KYLIN-5181, support to use postgreSQL to store metadata\n\n* minor fix\n\n* minor fix, remove useless comment\n\n* minor fix, same index name can not create in a schema\n\n* resolve confict",
        "before_after_code_files": [
          "core-common/src/main/java/org/apache/kylin/common/persistence/JDBCResourceStore.java||core-common/src/main/java/org/apache/kylin/common/persistence/JDBCResourceStore.java",
          "core-common/src/main/java/org/apache/kylin/common/persistence/ResourceTool.java||core-common/src/main/java/org/apache/kylin/common/persistence/ResourceTool.java",
          "core-common/src/main/resources/metadata-jdbc-postgresql.properties||core-common/src/main/resources/metadata-jdbc-postgresql.properties"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core-common/src/main/java/org/apache/kylin/common/persistence/JDBCResourceStore.java||core-common/src/main/java/org/apache/kylin/common/persistence/JDBCResourceStore.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/persistence/JDBCResourceStore.java -> core-common/src/main/java/org/apache/kylin/common/persistence/JDBCResourceStore.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "50:     private static final String META_TABLE_KEY = \"META_TABLE_KEY\";",
          "51:     private static final String META_TABLE_TS = \"META_TABLE_TS\";",
          "52:     private static final String META_TABLE_CONTENT = \"META_TABLE_CONTENT\";",
          "53:     private static Logger logger = LoggerFactory.getLogger(JDBCResourceStore.class);",
          "54:     private JDBCConnectionManager connectionManager;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "53:     private static final String DIALECT_OF_PG = \"postgresql\";",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "124:                 try {",
          "125:                     String indexName = \"IDX_\" + META_TABLE_TS;",
          "126:                     String createIndexSql = sqls.getCreateIndexSql(indexName, tableName, META_TABLE_TS);",
          "127:                     logger.info(\"Creating index: {}\", createIndexSql);",
          "128:                     pstat = connection.prepareStatement(createIndexSql);",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "127:                     if (DIALECT_OF_PG.equals(kylinConfig.getMetadataDialect())) {",
          "128:                         indexName += System.currentTimeMillis();",
          "129:                     }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "314:         if (rs == null) {",
          "315:             return null;",
          "316:         }",
          "318:         Blob blob = rs.getBlob(META_TABLE_CONTENT);",
          "320:         if (blob == null || blob.length() == 0) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "321:         if (DIALECT_OF_PG.equals(kylinConfig.getMetadataDialect())) {",
          "322:             InputStream inputStream = rs.getBinaryStream(META_TABLE_CONTENT);",
          "323:             if (inputStream == null) {",
          "324:                 return openPushdown(resPath);",
          "325:             }",
          "326:             return inputStream;",
          "327:         }",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "355:                     if (existing) {",
          "356:                         pstat = connection.prepareStatement(sqls.getReplaceSql());",
          "357:                         pstat.setLong(1, ts);",
          "359:                         pstat.setString(3, resPath);",
          "360:                     } else {",
          "361:                         pstat = connection.prepareStatement(sqls.getInsertSql());",
          "362:                         pstat.setString(1, resPath);",
          "363:                         pstat.setLong(2, ts);",
          "365:                     }",
          "367:                     if (isContentOverflow(bytes, resPath)) {",
          "",
          "[Removed Lines]",
          "358:                         pstat.setBlob(2, new BufferedInputStream(new ByteArrayInputStream(bytes)));",
          "364:                         pstat.setBlob(3, new BufferedInputStream(new ByteArrayInputStream(bytes)));",
          "",
          "[Added Lines]",
          "368:                         pstat.setBinaryStream(2, new BufferedInputStream(new ByteArrayInputStream(bytes)));",
          "374:                         pstat.setBinaryStream(3, new BufferedInputStream(new ByteArrayInputStream(bytes)));",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "376:                         RollbackablePushdown pushdown = writePushdown(resPath, ContentWriter.create(bytes));",
          "377:                         try {",
          "378:                             int result = pstat.executeUpdate();",
          "380:                                 throw new SQLException();",
          "381:                         } catch (Exception e) {",
          "382:                             pushdown.rollback();",
          "383:                             throw e;",
          "",
          "[Removed Lines]",
          "379:                             if (result != 1)",
          "",
          "[Added Lines]",
          "389:                             if (result != 1) {",
          "391:                             }",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "414:         }",
          "416:         int maxSize = kylinConfig.getJdbcResourceStoreMaxCellSize();",
          "418:             return true;",
          "420:             return false;",
          "421:     }",
          "423:     @Override",
          "",
          "[Removed Lines]",
          "417:         if (content.length > maxSize)",
          "419:         else",
          "",
          "[Added Lines]",
          "428:         if (content.length > maxSize) {",
          "430:         } else {",
          "432:         }",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "454:                             RollbackablePushdown pushdown = writePushdown(resPath, ContentWriter.create(content));",
          "455:                             try {",
          "456:                                 int result = pstat.executeUpdate();",
          "458:                                     throw new SQLException();",
          "459:                             } catch (Throwable e) {",
          "460:                                 pushdown.rollback();",
          "461:                                 throw e;",
          "",
          "[Removed Lines]",
          "457:                                 if (result != 1)",
          "",
          "[Added Lines]",
          "469:                                 if (result != 1) {",
          "471:                                 }",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "466:                             pstat = connection.prepareStatement(sqls.getInsertSql());",
          "467:                             pstat.setString(1, resPath);",
          "468:                             pstat.setLong(2, newTS);",
          "470:                             pstat.executeUpdate();",
          "471:                         }",
          "472:                     } else {",
          "",
          "[Removed Lines]",
          "469:                             pstat.setBlob(3, new BufferedInputStream(new ByteArrayInputStream(content)));",
          "",
          "[Added Lines]",
          "482:                             pstat.setBinaryStream(3, new BufferedInputStream(new ByteArrayInputStream(content)));",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "481:                             RollbackablePushdown pushdown = writePushdown(resPath, ContentWriter.create(content));",
          "482:                             try {",
          "483:                                 int result = pstat.executeUpdate();",
          "485:                                     throw new SQLException();",
          "486:                             } catch (Throwable e) {",
          "487:                                 pushdown.rollback();",
          "488:                                 throw e;",
          "",
          "[Removed Lines]",
          "484:                                 if (result != 1)",
          "",
          "[Added Lines]",
          "497:                                 if (result != 1) {",
          "499:                                 }",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "647:         abstract public void execute(final Connection connection) throws SQLException, IOException;",
          "648:     }",
          "",
          "[Removed Lines]",
          "650: }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "core-common/src/main/java/org/apache/kylin/common/persistence/ResourceTool.java||core-common/src/main/java/org/apache/kylin/common/persistence/ResourceTool.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/persistence/ResourceTool.java -> core-common/src/main/java/org/apache/kylin/common/persistence/ResourceTool.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "70:             tool.addExcludes(exclude.split(\"\\\\s*,\\\\s*\"));",
          "71:         }",
          "72:         String group = System.getProperty(\"group\");",
          "74:             tool.parallelCopyGroupSize = Integer.parseInt(group);",
          "76:         tool.addExcludes(IMMUTABLE_PREFIX.toArray(new String[IMMUTABLE_PREFIX.size()]));",
          "",
          "[Removed Lines]",
          "73:         if (group != null)",
          "",
          "[Added Lines]",
          "73:         if (group != null) {",
          "75:         }",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "152:     private void copyParallel(KylinConfig from, KylinConfig to, String folder) throws IOException {",
          "153:         ResourceParallelCopier copier = new ResourceParallelCopier(ResourceStore.getStore(from), ResourceStore.getStore(to));",
          "155:             copier.setGroupSize(parallelCopyGroupSize);",
          "157:         Stats stats = copier.copy(folder, includes, excludes, new Stats() {",
          "",
          "[Removed Lines]",
          "154:         if (parallelCopyGroupSize > 0)",
          "",
          "[Added Lines]",
          "155:         if (parallelCopyGroupSize > 0) {",
          "157:         }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "178:         });",
          "180:         if (stats.hasError()) {",
          "182:                 System.out.println(\"Failed to copy resource group: \" + errGroup + \"*\");",
          "184:                 System.out.println(\"Failed to copy resource: \" + errResPath);",
          "185:             throw new IOException(\"Failed to copy \" + stats.errorResource.get() + \" resource\");",
          "186:         }",
          "187:     }",
          "",
          "[Removed Lines]",
          "181:             for (String errGroup : stats.errorGroups)",
          "183:             for (String errResPath : stats.errorResourcePaths)",
          "",
          "[Added Lines]",
          "183:             for (String errGroup : stats.errorGroups) {",
          "185:             }",
          "186:             for (String errResPath : stats.errorResourcePaths) {",
          "188:             }",
          "",
          "---------------"
        ],
        "core-common/src/main/resources/metadata-jdbc-postgresql.properties||core-common/src/main/resources/metadata-jdbc-postgresql.properties": [
          "File: core-common/src/main/resources/metadata-jdbc-postgresql.properties -> core-common/src/main/resources/metadata-jdbc-postgresql.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one or more",
          "3: # contributor license agreements.  See the NOTICE file distributed with",
          "4: # this work for additional information regarding copyright ownership.",
          "5: # The ASF licenses this file to You under the Apache License, Version 2.0",
          "6: # (the \"License\"); you may not use this file except in compliance with",
          "7: # the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #    http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing, software",
          "12: # distributed under the License is distributed on an \"AS IS\" BASIS,",
          "13: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
          "14: # See the License for the specific language governing permissions and",
          "15: # limitations under the License.",
          "16: #",
          "18: ###JDBC METASTORE",
          "19: format.sql.create-if-need=create table if not exists {0} ( {1} VARCHAR(255) primary key, {2} BIGINT, {3} BYTEA )",
          "20: format.sql.key-equals=select {0} from {1} where {2} = ?",
          "21: format.sql.delete-pstat=delete from {0}  where {1} = ?",
          "22: format.sql.list-resource=select {0} from {1} where {2} like ?",
          "23: format.sql.all-resource=select {0} from {1} where {2} like ? escape ''#'' and {3} >= ? and {4} < ?",
          "24: format.sql.replace=update {0} set {1} = ?,{2} = ? where {3} = ?",
          "25: format.sql.insert=insert into {0}({1},{2},{3}) values(?,?,?)",
          "26: format.sql.replace-without-content=update {0} set {1} = ? where {2} = ?",
          "27: format.sql.insert-without-content=insert into {0}({1},{2}) values(?,?)",
          "28: format.sql.update-content-ts=update {0} set {1}=?,{2} = ? where {3}=? and {4}=?",
          "29: format.sql.test.create=create table if not exists {0} (name VARCHAR(255) primary key, id BIGINT)",
          "30: format.sql.test.drop=drop table if exists {0}",
          "31: format.sql.create-index=create index {0} on {1} ({2})",
          "32: format.sql.check-table-exists=SELECT table_name FROM information_schema.tables \\",
          "33:   WHERE table_schema='''public''' AND table_type='''BASE TABLE''' AND table_name=''{0}'';",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "e603ecfd1f2535d3c3054958347423c39d4c0314",
      "candidate_info": {
        "commit_hash": "e603ecfd1f2535d3c3054958347423c39d4c0314",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/e603ecfd1f2535d3c3054958347423c39d4c0314",
        "files": [
          "core-common/src/main/java/org/apache/kylin/common/KylinConfig.java"
        ],
        "message": "improve performance of getInstanceFromEnv",
        "before_after_code_files": [
          "core-common/src/main/java/org/apache/kylin/common/KylinConfig.java||core-common/src/main/java/org/apache/kylin/common/KylinConfig.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core-common/src/main/java/org/apache/kylin/common/KylinConfig.java||core-common/src/main/java/org/apache/kylin/common/KylinConfig.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/KylinConfig.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfig.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "65:     public static final String KYLIN_CONF = \"KYLIN_CONF\";",
          "71:     private static OrderedProperties defaultOrderedProperties = new OrderedProperties();",
          "",
          "[Removed Lines]",
          "68:     private static KylinConfig SYS_ENV_INSTANCE = null;",
          "",
          "[Added Lines]",
          "68:     private static volatile KylinConfig SYS_ENV_INSTANCE = null;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "134:     }",
          "136:     public static KylinConfig getInstanceFromEnv(boolean allowConfigFileNoExist) {",
          "150:                     try {",
          "156:                         }",
          "165:                 }",
          "166:             }",
          "168:         }",
          "169:     }",
          "171:     public static KylinConfig getInstanceFromEnv() {",
          "",
          "[Removed Lines]",
          "137:         synchronized (KylinConfig.class) {",
          "138:             KylinConfig config = THREAD_ENV_INSTANCE.get();",
          "139:             if (config != null) {",
          "140:                 return config;",
          "141:             }",
          "143:             if (SYS_ENV_INSTANCE == null) {",
          "144:                 try {",
          "147:                     buildDefaultOrderedProperties();",
          "149:                     config = new KylinConfig();",
          "151:                         config.reloadKylinConfig(buildSiteProperties());",
          "152:                     } catch (KylinConfigCannotInitException e) {",
          "153:                         logger.info(\"Kylin Config Can not Init Exception\");",
          "154:                         if (!allowConfigFileNoExist) {",
          "155:                             throw e;",
          "157:                     }",
          "159:                     VersionUtil.loadKylinVersion();",
          "160:                     logger.info(\"Initialized a new KylinConfig from getInstanceFromEnv : \"",
          "161:                             + System.identityHashCode(config));",
          "162:                     SYS_ENV_INSTANCE = config;",
          "163:                 } catch (IllegalArgumentException e) {",
          "164:                     throw new IllegalStateException(\"Failed to find KylinConfig \", e);",
          "167:             return SYS_ENV_INSTANCE;",
          "",
          "[Added Lines]",
          "137:         KylinConfig config = THREAD_ENV_INSTANCE.get();",
          "138:         if (config != null) {",
          "139:             return config;",
          "140:         }",
          "142:         if (SYS_ENV_INSTANCE == null) {",
          "143:             synchronized (KylinConfig.class) {",
          "144:                 if (SYS_ENV_INSTANCE == null) {",
          "148:                         buildDefaultOrderedProperties();",
          "150:                         config = new KylinConfig();",
          "151:                         try {",
          "152:                             config.reloadKylinConfig(buildSiteProperties());",
          "153:                         } catch (KylinConfigCannotInitException e) {",
          "154:                             logger.info(\"Kylin Config Can not Init Exception\");",
          "155:                             if (!allowConfigFileNoExist) {",
          "156:                                 throw e;",
          "157:                             }",
          "160:                         VersionUtil.loadKylinVersion();",
          "161:                         logger.info(\"Initialized a new KylinConfig from getInstanceFromEnv : \"",
          "162:                                 + System.identityHashCode(config));",
          "163:                         SYS_ENV_INSTANCE = config;",
          "164:                     } catch (IllegalArgumentException e) {",
          "165:                         throw new IllegalStateException(\"Failed to find KylinConfig \", e);",
          "166:                     }",
          "170:         return SYS_ENV_INSTANCE;",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bf07880b9c634996a4f699daa4bdc7e6f8e33501",
      "candidate_info": {
        "commit_hash": "bf07880b9c634996a4f699daa4bdc7e6f8e33501",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/bf07880b9c634996a4f699daa4bdc7e6f8e33501",
        "files": [
          ".github/pull_request_template.md",
          ".travis.yml",
          "build/bin/check-env.sh"
        ],
        "message": "Fix check-env.sh",
        "before_after_code_files": [
          "build/bin/check-env.sh||build/bin/check-env.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "build/bin/check-env.sh||build/bin/check-env.sh": [
          "File: build/bin/check-env.sh -> build/bin/check-env.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "76: then",
          "77:     quit \"Please set kylin.env.hdfs-working-dir in kylin.properties\"",
          "78: fi",
          "80: then",
          "81:     WORKING_DIR=${WORKING_DIR/\"s3a\"/\"s3\"}",
          "82: fi",
          "",
          "[Removed Lines]",
          "79: if [[ ${WORKING_DIR:0:3} -eq \"s3a\" ]]",
          "",
          "[Added Lines]",
          "79: if [[ \"${WORKING_DIR:0:3}\" == \"s3a\" ]]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "94: SPARK_EVENTLOG_DIR=`bash $KYLIN_HOME/bin/get-properties.sh kylin.engine.spark-conf.spark.eventLog.dir`",
          "95: if [ -n \"$SPARK_EVENTLOG_DIR\" ]",
          "96: then",
          "98:     then",
          "99:         SPARK_EVENTLOG_DIR=${SPARK_EVENTLOG_DIR/\"s3a\"/\"s3\"}",
          "100:     fi",
          "",
          "[Removed Lines]",
          "97:     if [[ ${SPARK_EVENTLOG_DIR:0:3} -eq \"s3a\" ]]",
          "",
          "[Added Lines]",
          "97:     if [[ \"${SPARK_EVENTLOG_DIR:0:3}\" == \"s3a\" ]]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "108: SPARK_HISTORYLOG_DIR=`bash $KYLIN_HOME/bin/get-properties.sh kylin.engine.spark-conf.spark.history.fs.logDirectory`",
          "109: if [ -n \"$SPARK_HISTORYLOG_DIR\" ]",
          "110: then",
          "112:     then",
          "113:         SPARK_HISTORYLOG_DIR=${SPARK_HISTORYLOG_DIR/\"s3a\"/\"s3\"}",
          "114:     fi",
          "",
          "[Removed Lines]",
          "111:     if [[ ${SPARK_HISTORYLOG_DIR:0:3} -eq \"s3a\" ]]",
          "",
          "[Added Lines]",
          "111:     if [[ \"${SPARK_HISTORYLOG_DIR:0:3}\" == \"s3a\" ]]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f2cdedb47416de04c4f3f3594e64fa9ce46be33d",
      "candidate_info": {
        "commit_hash": "f2cdedb47416de04c4f3f3594e64fa9ce46be33d",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/f2cdedb47416de04c4f3f3594e64fa9ce46be33d",
        "files": [
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AbstractCacheFileSystem.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ConsistentHash.java"
        ],
        "message": "KYLIN-5187, Problem fix,     1. Fix exception to get more detailed message for other FileSystem     2. Fix de.thetaphi:forbiddenapis:2.3:check for forbidden api, add default charset UTF-8",
        "before_after_code_files": [
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AbstractCacheFileSystem.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AbstractCacheFileSystem.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ConsistentHash.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ConsistentHash.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AbstractCacheFileSystem.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AbstractCacheFileSystem.java": [
          "File: kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AbstractCacheFileSystem.java -> kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AbstractCacheFileSystem.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "110:             LOG.info(\"Create filesystem {} for scheme {} .\",",
          "111:                     schemeClassMap.get(uri.getScheme()), uri.getScheme());",
          "112:         } catch (ClassNotFoundException e) {",
          "114:         }",
          "115:         return fs;",
          "116:     }",
          "",
          "[Removed Lines]",
          "113:             throw new IOException(\"Can not found FileSystem Clazz for scheme: \" + uri.getScheme());",
          "",
          "[Added Lines]",
          "113:             throw new IOException(\"Can not found FileSystem Clazz for scheme: \" + uri.getScheme() + \" Exception is: \" + e.getMessage());",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ConsistentHash.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ConsistentHash.java": [
          "File: kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ConsistentHash.java -> kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ConsistentHash.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import com.google.common.hash.HashFunction;",
          "22: import com.google.common.hash.Hashing;",
          "24: import java.util.List;",
          "25: import java.util.SortedMap;",
          "26: import java.util.concurrent.ConcurrentSkipListMap;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24: import java.nio.charset.Charset;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "78:     }",
          "80:     private int getKeyHash(final String k) {",
          "82:     }",
          "84:     private int getKetamaHash(final String k) {",
          "86:     }",
          "87: }",
          "",
          "[Removed Lines]",
          "81:         return keyHash.hashBytes(k.getBytes()).asInt();",
          "85:         return nodeHash.hashBytes(k.getBytes()).asInt();",
          "",
          "[Added Lines]",
          "82:         return keyHash.hashBytes(k.getBytes(Charset.defaultCharset())).asInt();",
          "86:         return nodeHash.hashBytes(k.getBytes(Charset.defaultCharset())).asInt();",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "9706a9dd76486ea4c4fd4fa86abe1f53602edb28",
      "candidate_info": {
        "commit_hash": "9706a9dd76486ea4c4fd4fa86abe1f53602edb28",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/9706a9dd76486ea4c4fd4fa86abe1f53602edb28",
        "files": [
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "kylin-it/src/test/resources/query/test_query/query01.sql",
          "kylin-spark-project/kylin-soft-affinity-cache/pom.xml",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/KylinCacheConstants.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AbstractCacheFileSystem.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AlluxioHdfsFileInputStream.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/CacheFileInputStream.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/CacheFileSystemConstants.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/MemCacheFileInputStream.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/OnlyForTestCacheFileSystem.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/kylin/CacheAllFileSystem.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/kylin/KylinCacheFileSystem.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ConsistentHash.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ReflectionUtil.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/softaffinity/SoftAffinityConstants.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/SoftAffinityManager.scala",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/scheduler/SoftAffinityListener.scala",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/strategy/SoftAffinityAllocationTrait.scala",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/strategy/SoftAffinityStrategy.scala",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/spark31/org/apache/spark/sql/execution/datasources/CacheFilePartition.scala",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/spark31/org/apache/spark/sql/execution/datasources/CacheFileScanRDD.scala",
          "kylin-spark-project/kylin-spark-common/pom.xml",
          "kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/AbstractHdfsLogAppender.java",
          "kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/SparkDriverHdfsLogAppender.java",
          "kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/SparkExecutorHdfsAppender.java",
          "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala",
          "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/utils/SparkHadoopUtils.scala",
          "kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala",
          "kylin-spark-project/pom.xml"
        ],
        "message": "KYLIN-5187 Support soft affinity and local cache feature\n\n    1. Implement LocalDataCacheManager\n    2. base xiaoxiang's PR\n    3. Implement CacheFileScanRDD\n    4. Implement AbstractCacheFileSystem\n    5. Optimize performance\n    6. Support soft affinity for hdfs\n    7. Support ByteBuffer to read data, and avoid to read data one byte by one byte\n    8. Support to cache small files in memory : ByteBufferPageStore extends PageStore to support cache data in memory\n    9. Pre-init KylinCacheFileSystem to fix s3a issue\n   10. Upgrade alluxio client verion to 2.7.4",
        "before_after_code_files": [
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "kylin-it/src/test/resources/query/test_query/query01.sql||kylin-it/src/test/resources/query/test_query/query01.sql",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/KylinCacheConstants.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/KylinCacheConstants.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AbstractCacheFileSystem.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AbstractCacheFileSystem.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AlluxioHdfsFileInputStream.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AlluxioHdfsFileInputStream.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/CacheFileInputStream.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/CacheFileInputStream.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/CacheFileSystemConstants.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/CacheFileSystemConstants.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/MemCacheFileInputStream.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/MemCacheFileInputStream.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/OnlyForTestCacheFileSystem.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/OnlyForTestCacheFileSystem.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/kylin/CacheAllFileSystem.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/kylin/CacheAllFileSystem.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/kylin/KylinCacheFileSystem.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/kylin/KylinCacheFileSystem.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ConsistentHash.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ConsistentHash.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ReflectionUtil.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ReflectionUtil.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/softaffinity/SoftAffinityConstants.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/softaffinity/SoftAffinityConstants.java",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/SoftAffinityManager.scala||kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/SoftAffinityManager.scala",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/scheduler/SoftAffinityListener.scala||kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/scheduler/SoftAffinityListener.scala",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/strategy/SoftAffinityAllocationTrait.scala||kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/strategy/SoftAffinityAllocationTrait.scala",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/strategy/SoftAffinityStrategy.scala||kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/strategy/SoftAffinityStrategy.scala",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/spark31/org/apache/spark/sql/execution/datasources/CacheFilePartition.scala||kylin-spark-project/kylin-soft-affinity-cache/src/main/spark31/org/apache/spark/sql/execution/datasources/CacheFilePartition.scala",
          "kylin-spark-project/kylin-soft-affinity-cache/src/main/spark31/org/apache/spark/sql/execution/datasources/CacheFileScanRDD.scala||kylin-spark-project/kylin-soft-affinity-cache/src/main/spark31/org/apache/spark/sql/execution/datasources/CacheFileScanRDD.scala",
          "kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/AbstractHdfsLogAppender.java||kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/AbstractHdfsLogAppender.java",
          "kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/SparkDriverHdfsLogAppender.java||kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/SparkDriverHdfsLogAppender.java",
          "kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/SparkExecutorHdfsAppender.java||kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/SparkExecutorHdfsAppender.java",
          "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala||kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala",
          "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/utils/SparkHadoopUtils.scala||kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/utils/SparkHadoopUtils.scala",
          "kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala||kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ],
          "candidate": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ]
        }
      },
      "candidate_diff": {
        "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "38: import org.apache.commons.lang.StringUtils;",
          "39: import org.apache.commons.lang.text.StrSubstitutor;",
          "40: import org.apache.hadoop.fs.FileSystem;",
          "41: import org.apache.hadoop.fs.Path;",
          "42: import org.apache.kylin.common.annotation.ConfigTag;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "40: import org.apache.hadoop.conf.Configuration;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "295:     }",
          "297:     public String getHdfsWorkingDirectory() {",
          "298:         if (cachedHdfsWorkingDirectory != null) {",
          "299:             return cachedHdfsWorkingDirectory;",
          "300:         }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "299:         return getHdfsWorkingDirectoryInternal(HadoopUtil.getCurrentConfiguration());",
          "300:     }",
          "302:     public String getHdfsWorkingDirectoryInternal(Configuration hadoopConf) {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "306:             throw new IllegalArgumentException(\"kylin.env.hdfs-working-dir must be absolute, but got \" + root);",
          "308:         try {",
          "310:             path = fs.makeQualified(path);",
          "311:         } catch (IOException e) {",
          "312:             throw new RuntimeException(e);",
          "",
          "[Removed Lines]",
          "309:             FileSystem fs = path.getFileSystem(HadoopUtil.getCurrentConfiguration());",
          "",
          "[Added Lines]",
          "314:             FileSystem fs = path.getFileSystem(hadoopConf);",
          "",
          "---------------"
        ],
        "kylin-it/src/test/resources/query/test_query/query01.sql||kylin-it/src/test/resources/query/test_query/query01.sql": [
          "File: kylin-it/src/test/resources/query/test_query/query01.sql -> kylin-it/src/test/resources/query/test_query/query01.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: --",
          "2: -- Licensed to the Apache Software Foundation (ASF) under one",
          "3: -- or more contributor license agreements.  See the NOTICE file",
          "4: -- distributed with this work for additional information",
          "5: -- regarding copyright ownership.  The ASF licenses this file",
          "6: -- to you under the Apache License, Version 2.0 (the",
          "7: -- \"License\"); you may not use this file except in compliance",
          "8: -- with the License.  You may obtain a copy of the License at",
          "9: --",
          "10: --     http://www.apache.org/licenses/LICENSE-2.0",
          "11: --",
          "12: -- Unless required by applicable law or agreed to in writing, software",
          "13: -- distributed under the License is distributed on an \"AS IS\" BASIS,",
          "14: -- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
          "15: -- See the License for the specific language governing permissions and",
          "16: -- limitations under the License.",
          "17: --",
          "19: SELECT test_kylin_fact.cal_dt,cast(timestampdiff(DAY,date'2013-01-01',test_kylin_fact.cal_dt) as integer) as x,sum(price) as y",
          "20:  FROM TEST_KYLIN_FACT",
          "22: inner JOIN edw.test_cal_dt as test_cal_dt",
          "23:  ON test_kylin_fact.cal_dt = test_cal_dt.cal_dt",
          "24:  inner JOIN test_category_groupings",
          "25:  ON test_kylin_fact.leaf_categ_id = test_category_groupings.leaf_categ_id AND test_kylin_fact.lstg_site_id = test_category_groupings.site_id",
          "26:  inner JOIN edw.test_sites as test_sites",
          "27:  ON test_kylin_fact.lstg_site_id = test_sites.site_id",
          "28:  GROUP BY test_kylin_fact.cal_dt",
          "29:  ORDER BY test_kylin_fact.cal_dt",
          "30: ;{\"scanRowCount\":1462,\"scanBytes\":215217,\"scanFiles\":2,\"cuboidId\":262144}",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/KylinCacheConstants.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/KylinCacheConstants.java": [
          "File: kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/KylinCacheConstants.java -> kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/KylinCacheConstants.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.cache;",
          "21: public class KylinCacheConstants {",
          "23:     private KylinCacheConstants() {",
          "24:     }",
          "27:     public static final String KYLIN_CACHE_FS =",
          "28:             \"org.apache.kylin.cache.fs.kylin.KylinCacheFileSystem\";",
          "29: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AbstractCacheFileSystem.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AbstractCacheFileSystem.java": [
          "File: kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AbstractCacheFileSystem.java -> kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AbstractCacheFileSystem.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.cache.fs;",
          "21: import alluxio.client.file.CacheContext;",
          "22: import alluxio.client.file.URIStatus;",
          "23: import alluxio.client.file.cache.CacheManager;",
          "24: import alluxio.client.file.cache.LocalCacheFileInStream;",
          "25: import alluxio.conf.AlluxioConfiguration;",
          "26: import alluxio.hadoop.AlluxioHdfsInputStream;",
          "27: import alluxio.hadoop.HadoopFileOpener;",
          "28: import alluxio.hadoop.HadoopUtils;",
          "29: import alluxio.metrics.MetricsConfig;",
          "30: import alluxio.metrics.MetricsSystem;",
          "31: import alluxio.wire.FileInfo;",
          "32: import com.google.common.cache.CacheBuilder;",
          "33: import com.google.common.cache.CacheLoader;",
          "34: import com.google.common.cache.LoadingCache;",
          "35: import org.apache.hadoop.conf.Configuration;",
          "36: import org.apache.hadoop.fs.FSDataInputStream;",
          "37: import org.apache.hadoop.fs.FileStatus;",
          "38: import org.apache.hadoop.fs.FileSystem;",
          "39: import org.apache.hadoop.fs.FilterFileSystem;",
          "40: import org.apache.hadoop.fs.Path;",
          "42: import org.apache.hadoop.util.ReflectionUtils;",
          "43: import org.apache.kylin.cache.utils.ReflectionUtil;",
          "44: import org.slf4j.Logger;",
          "45: import org.slf4j.LoggerFactory;",
          "47: import java.io.FileNotFoundException;",
          "48: import java.io.IOException;",
          "49: import java.net.URI;",
          "50: import java.util.HashMap;",
          "51: import java.util.Map;",
          "52: import java.util.Properties;",
          "53: import java.util.concurrent.ExecutionException;",
          "54: import java.util.concurrent.TimeUnit;",
          "56: import static com.google.common.hash.Hashing.md5;",
          "57: import static java.nio.charset.StandardCharsets.UTF_8;",
          "59: public abstract class AbstractCacheFileSystem extends FilterFileSystem {",
          "61:     private static final Logger LOG = LoggerFactory.getLogger(AbstractCacheFileSystem.class);",
          "65:     protected URI uri;",
          "66:     protected String originalScheme;",
          "67:     protected int bufferSize = 4096;",
          "68:     protected boolean useLocalCache = false;",
          "69:     protected boolean useLegacyFileInputStream = false;",
          "70:     protected HadoopFileOpener mHadoopFileOpener;",
          "71:     protected LocalCacheFileInStream.FileInStreamOpener mAlluxioFileOpener;",
          "72:     protected CacheManager mCacheManager;",
          "73:     protected AlluxioConfiguration mAlluxioConf;",
          "75:     protected LoadingCache<Path, FileStatus> fileStatusCache;",
          "80:     protected static final Map<String, String> schemeClassMap = new HashMap<String, String>() {",
          "81:         {",
          "82:             put(\"file\", \"org.apache.hadoop.fs.LocalFileSystem\");",
          "83:             put(\"viewfs\", \"org.apache.hadoop.fs.viewfs.ViewFileSystem\");",
          "84:             put(\"s3a\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\");",
          "85:             put(\"s3\", \"org.apache.hadoop.fs.s3.S3FileSystem\");",
          "86:             put(\"s3n\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\");",
          "87:             put(\"hdfs\", \"org.apache.hadoop.hdfs.DistributedFileSystem\");",
          "88:             put(\"wasb\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\");",
          "89:             put(\"wasbs\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure\");",
          "90:             put(\"jfs\", \"io.juicefs.JuiceFileSystem\");",
          "91:             put(\"alluxio\", \"alluxio.hadoop.FileSystem\");",
          "92:         }",
          "93:     };",
          "98:     protected static FileSystem createInternalFS(URI uri, Configuration conf)",
          "99:             throws IOException {",
          "100:         if (!schemeClassMap.containsKey(uri.getScheme())) {",
          "101:             throw new IOException(\"No FileSystem for scheme: \" + uri.getScheme());",
          "102:         }",
          "103:         FileSystem fs = null;",
          "104:         try {",
          "105:             Class<? extends FileSystem> clazz =",
          "106:                     (Class<? extends FileSystem>) conf.getClassByName(",
          "107:                             schemeClassMap.get(uri.getScheme()));",
          "108:             fs = ReflectionUtils.newInstance(clazz, conf);",
          "109:             fs.initialize(uri, conf);",
          "110:             LOG.info(\"Create filesystem {} for scheme {} .\",",
          "111:                     schemeClassMap.get(uri.getScheme()), uri.getScheme());",
          "112:         } catch (ClassNotFoundException e) {",
          "113:             throw new IOException(\"Can not found FileSystem Clazz for scheme: \" + uri.getScheme());",
          "114:         }",
          "115:         return fs;",
          "116:     }",
          "118:     protected void createLocalCacheManager(URI name, Configuration conf) throws IOException{",
          "119:         mHadoopFileOpener = uriStatus -> this.fs.open(new Path(uriStatus.getPath()));",
          "120:         mAlluxioFileOpener = status -> new AlluxioHdfsInputStream(mHadoopFileOpener.open(status));",
          "122:         mAlluxioConf = HadoopUtils.toAlluxioConf(conf);",
          "124:         Properties metricsProperties = new Properties();",
          "125:         for (Map.Entry<String, String> entry : conf) {",
          "126:             metricsProperties.setProperty(entry.getKey(), entry.getValue());",
          "127:         }",
          "128:         MetricsSystem.startSinksFromConfig(new MetricsConfig(metricsProperties));",
          "129:         mCacheManager = CacheManager.Factory.get(mAlluxioConf);",
          "130:         if (mCacheManager == null) {",
          "131:             throw new IOException(\"CacheManager is null !\");",
          "132:         }",
          "133:     }",
          "135:     @Override",
          "136:     public synchronized void initialize(URI name, Configuration conf) throws IOException {",
          "137:         this.originalScheme = name.getScheme();",
          "139:         this.fs = createInternalFS(name, conf);",
          "140:         this.statistics = (FileSystem.Statistics) ReflectionUtil.getFieldValue(this.fs,",
          "141:                 \"statistics\");",
          "142:         if (null == this.statistics) {",
          "143:             LOG.info(\"======= original statistics is null.\");",
          "144:         } else {",
          "145:             LOG.info(\"======= original statistics is {} {}.\", this.statistics.getScheme(),",
          "146:                     this.statistics.toString());",
          "147:         }",
          "148:         super.initialize(name, conf);",
          "149:         this.setConf(conf);",
          "150:         LOG.info(\"======= current statistics is {} {}.\", this.statistics.getScheme(),",
          "151:                 this.statistics.toString());",
          "153:         this.bufferSize = conf.getInt(CacheFileSystemConstants.PARAMS_KEY_IO_FILE_BUFFER_SIZE,",
          "154:                 CacheFileSystemConstants.PARAMS_KEY_IO_FILE_BUFFER_SIZE_DEFAULT_VALUE);",
          "156:         this.useLocalCache = conf.getBoolean(CacheFileSystemConstants.PARAMS_KEY_USE_CACHE,",
          "157:                 CacheFileSystemConstants.PARAMS_KEY_USE_CACHE_DEFAULT_VALUE)",
          "158:                 && !this.originalScheme.equals(CacheFileSystemConstants.JUICEFS_SCHEME);",
          "160:         this.useLegacyFileInputStream = conf.getBoolean(",
          "161:                 CacheFileSystemConstants.PARAMS_KEY_USE_LEGACY_FILE_INPUTSTREAM,",
          "162:                 CacheFileSystemConstants.PARAMS_KEY_USE_LEGACY_FILE_INPUTSTREAM_DEFAULT_VALUE);",
          "165:         long fileStatusTTL =",
          "166:                 conf.getLong(CacheFileSystemConstants.PARAMS_KEY_FILE_STATUS_CACHE_TTL,",
          "167:                         CacheFileSystemConstants.PARAMS_KEY_FILE_STATUS_CACHE_TTL_DEFAULT_VALUE);",
          "168:         long fileStatusMaxSize =",
          "169:                 conf.getLong(CacheFileSystemConstants.PARAMS_KEY_FILE_STATUS_CACHE_MAX_SIZE,",
          "170:                      CacheFileSystemConstants.PARAMS_KEY_FILE_STATUS_CACHE_MAX_SIZE_DEFAULT_VALUE);",
          "171:         CacheLoader<Path, FileStatus> fileStatusCacheLoader = new CacheLoader<Path, FileStatus>() {",
          "172:             @Override",
          "173:             public FileStatus load(Path path) throws Exception {",
          "174:                 return getFileStatusForCache(path);",
          "175:             }",
          "176:         };",
          "177:         this.fileStatusCache =",
          "178:                 CacheBuilder.newBuilder()",
          "179:                         .maximumSize(fileStatusMaxSize)",
          "180:                         .expireAfterAccess(fileStatusTTL, TimeUnit.SECONDS)",
          "181:                         .recordStats()",
          "182:                         .build(fileStatusCacheLoader);",
          "185:         if (this.isUseLocalCache()) {",
          "187:             this.createLocalCacheManager(this.getUri(), conf);",
          "188:             LOG.info(\"Create LocalCacheFileSystem successfully .\");",
          "189:         }",
          "190:     }",
          "192:     protected FileStatus getFileStatusForCache(Path path) throws IOException {",
          "193:         return this.fs.getFileStatus(path);",
          "194:     }",
          "196:     @Override",
          "197:     public String getScheme() {",
          "198:         return this.originalScheme;",
          "199:     }",
          "201:     @Override",
          "202:     public FSDataInputStream open(Path f) throws IOException {",
          "203:         return open(f, bufferSize);",
          "204:     }",
          "209:     public abstract boolean isUseLocalCacheForTargetExecs();",
          "214:     public FileInfo wrapFileInfo(FileStatus fileStatus) {",
          "215:         return (new FileInfo()",
          "216:                 .setLength(fileStatus.getLen())",
          "217:                 .setPath(fileStatus.getPath().toString())",
          "218:                 .setFolder(fileStatus.isDirectory())",
          "219:                 .setBlockSizeBytes(fileStatus.getBlockSize())",
          "220:                 .setLastModificationTimeMs(fileStatus.getModificationTime())",
          "221:                 .setLastAccessTimeMs(fileStatus.getAccessTime())",
          "222:                 .setOwner(fileStatus.getOwner())",
          "223:                 .setGroup(fileStatus.getGroup()));",
          "224:     }",
          "226:     private int checkBufferSize(int size) {",
          "227:         if (size < this.bufferSize) {",
          "228:             size = this.bufferSize;",
          "229:         }",
          "234:         return size;",
          "235:     }",
          "237:     @Override",
          "238:     public FSDataInputStream open(Path p, int bufferSize) throws IOException {",
          "239:         return this.open(p, bufferSize, this.isUseLocalCacheForTargetExecs());",
          "240:     }",
          "242:     public FSDataInputStream open(Path p, int bufferSize, boolean useLocalCacheForExec) throws IOException {",
          "243:         Path f = this.fs.makeQualified(p);",
          "245:         if (this.isUseLocalCache() && this.mCacheManager != null && useLocalCacheForExec) {",
          "246:             FileStatus fileStatus = this.getFileStatus(f);",
          "247:             FileInfo fileInfo = wrapFileInfo(fileStatus);",
          "250:             CacheContext context = CacheContext.defaults().setCacheIdentifier(",
          "251:                     md5().hashString(fileStatus.getPath().toString(), UTF_8).toString());",
          "252:             URIStatus status = new URIStatus(fileInfo, context);",
          "253:             LOG.info(\"Use local cache FileSystem to open file {} .\", f);",
          "254:             if (this.useLegacyFileInputStream) {",
          "255:                 return new FSDataInputStream(new AlluxioHdfsFileInputStream(",
          "256:                         new LocalCacheFileInStream(status, mAlluxioFileOpener, mCacheManager,",
          "257:                                 mAlluxioConf), statistics));",
          "258:             }",
          "259:             return new FSDataInputStream(new CacheFileInputStream(f,",
          "260:                     new LocalCacheFileInStream(status, mAlluxioFileOpener, mCacheManager,",
          "261:                             mAlluxioConf),",
          "262:                     null, statistics, checkBufferSize(bufferSize)));",
          "263:         }",
          "264:         LOG.info(\"Use original FileSystem to open file {} .\", f);",
          "265:         return super.open(f, bufferSize);",
          "266:     }",
          "268:     @Override",
          "269:     public FileStatus getFileStatus(Path f) throws IOException {",
          "270:         this.statistics.incrementReadOps(1);",
          "271:         long start = System.currentTimeMillis();",
          "272:         FileStatus fileStatus = null;",
          "273:         Path p = this.fs.makeQualified(f);",
          "274:         try {",
          "275:             fileStatus = this.fileStatusCache.get(p);",
          "276:         } catch (ExecutionException e) {",
          "277:             if (e.getCause() instanceof FileNotFoundException)",
          "278:                 throw new FileNotFoundException(\"File does not exist: \" + p);",
          "279:             LOG.error(\"Get file status from cache error: \" + p, e);",
          "280:             return fileStatus;",
          "281:         }",
          "282:         LOG.info(\"Get file status {} from cache took: {}\", f,",
          "283:                 (System.currentTimeMillis() - start));",
          "284:         return fileStatus;",
          "285:     }",
          "287:     public CacheManager getmCacheManager() {",
          "288:         return mCacheManager;",
          "289:     }",
          "291:     public void setmCacheManager(CacheManager mCacheManager) {",
          "292:         this.mCacheManager = mCacheManager;",
          "293:     }",
          "295:     public AlluxioConfiguration getmAlluxioConf() {",
          "296:         return mAlluxioConf;",
          "297:     }",
          "299:     public void setmAlluxioConf(AlluxioConfiguration mAlluxioConf) {",
          "300:         this.mAlluxioConf = mAlluxioConf;",
          "301:     }",
          "303:     public boolean isUseLocalCache() {",
          "304:         return useLocalCache;",
          "305:     }",
          "307:     public void setUseLocalCache(boolean useLocalCache) {",
          "308:         this.useLocalCache = useLocalCache;",
          "309:     }",
          "311:     public int getBufferSize() {",
          "312:         return bufferSize;",
          "313:     }",
          "315:     public void setBufferSize(int bufferSize) {",
          "316:         this.bufferSize = bufferSize;",
          "317:     }",
          "319:     public boolean isUseLegacyFileInputStream() {",
          "320:         return useLegacyFileInputStream;",
          "321:     }",
          "323:     public void setUseLegacyFileInputStream(boolean useLegacyFileInputStream) {",
          "324:         this.useLegacyFileInputStream = useLegacyFileInputStream;",
          "325:     }",
          "327:     public LoadingCache<Path, FileStatus> getFileStatusCache() {",
          "328:         return fileStatusCache;",
          "329:     }",
          "330: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AlluxioHdfsFileInputStream.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AlluxioHdfsFileInputStream.java": [
          "File: kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AlluxioHdfsFileInputStream.java -> kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/AlluxioHdfsFileInputStream.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.cache.fs;",
          "21: import alluxio.AlluxioURI;",
          "22: import alluxio.client.file.FileInStream;",
          "23: import alluxio.client.file.FileSystem;",
          "24: import alluxio.exception.AlluxioException;",
          "25: import alluxio.exception.ExceptionMessage;",
          "26: import alluxio.exception.FileDoesNotExistException;",
          "28: import org.apache.hadoop.fs.ByteBufferReadable;",
          "29: import org.apache.hadoop.fs.FileSystem.Statistics;",
          "30: import org.apache.hadoop.fs.PositionedReadable;",
          "31: import org.apache.hadoop.fs.Seekable;",
          "33: import org.slf4j.Logger;",
          "34: import org.slf4j.LoggerFactory;",
          "36: import java.io.EOFException;",
          "37: import java.io.FileNotFoundException;",
          "38: import java.io.IOException;",
          "39: import java.io.InputStream;",
          "40: import java.nio.ByteBuffer;",
          "41: import javax.annotation.concurrent.NotThreadSafe;",
          "43: @NotThreadSafe",
          "44: public class AlluxioHdfsFileInputStream extends InputStream implements Seekable, PositionedReadable,",
          "45:         ByteBufferReadable {",
          "46:     private static final Logger LOG = LoggerFactory.getLogger(AlluxioHdfsFileInputStream.class);",
          "48:     private final Statistics mStatistics;",
          "49:     private final FileInStream mInputStream;",
          "51:     private boolean mClosed = false;",
          "60:     public AlluxioHdfsFileInputStream(FileSystem fs, AlluxioURI uri, Statistics stats)",
          "61:             throws IOException {",
          "62:         LOG.debug(\"HdfsFileInputStream({}, {})\", uri, stats);",
          "64:         mStatistics = stats;",
          "65:         try {",
          "66:             mInputStream = fs.openFile(uri);",
          "67:         } catch (FileDoesNotExistException e) {",
          "69:             throw new FileNotFoundException(ExceptionMessage.PATH_DOES_NOT_EXIST.getMessage(uri));",
          "70:         } catch (AlluxioException e) {",
          "71:             throw new IOException(e);",
          "72:         }",
          "73:     }",
          "81:     public AlluxioHdfsFileInputStream(FileInStream inputStream, Statistics stats) {",
          "82:         mInputStream = inputStream;",
          "83:         mStatistics = stats;",
          "84:     }",
          "86:     @Override",
          "87:     public int available() throws IOException {",
          "88:         if (mClosed) {",
          "89:             throw new IOException(\"Cannot query available bytes from a closed stream.\");",
          "90:         }",
          "91:         return (int) mInputStream.remaining();",
          "92:     }",
          "94:     @Override",
          "95:     public void close() throws IOException {",
          "96:         if (mClosed) {",
          "97:             return;",
          "98:         }",
          "99:         mInputStream.close();",
          "100:         mClosed = true;",
          "101:     }",
          "103:     @Override",
          "104:     public long getPos() throws IOException {",
          "105:         return mInputStream.getPos();",
          "106:     }",
          "108:     @Override",
          "109:     public int read() throws IOException {",
          "110:         if (mClosed) {",
          "111:             throw new IOException(ExceptionMessage.READ_CLOSED_STREAM.getMessage());",
          "112:         }",
          "114:         int data = mInputStream.read();",
          "115:         if (data != -1 && mStatistics != null) {",
          "116:             mStatistics.incrementBytesRead(1);",
          "117:             LOG.info(\"Read one byte.\");",
          "118:         }",
          "119:         return data;",
          "120:     }",
          "122:     @Override",
          "123:     public int read(byte[] buffer) throws IOException {",
          "124:         return read(buffer, 0, buffer.length);",
          "125:     }",
          "127:     @Override",
          "128:     public int read(byte[] buffer, int offset, int length) throws IOException {",
          "129:         if (mClosed) {",
          "130:             throw new IOException(ExceptionMessage.READ_CLOSED_STREAM.getMessage());",
          "131:         }",
          "133:         int bytesRead = mInputStream.read(buffer, offset, length);",
          "134:         if (bytesRead != -1 && mStatistics != null) {",
          "135:             mStatistics.incrementBytesRead(bytesRead);",
          "136:             LOG.info(\"Read {} bytes.\", bytesRead);",
          "137:         }",
          "138:         return bytesRead;",
          "139:     }",
          "141:     @Override",
          "142:     public int read(ByteBuffer buf) throws IOException {",
          "143:         if (mClosed) {",
          "144:             throw new IOException(ExceptionMessage.READ_CLOSED_STREAM.getMessage());",
          "145:         }",
          "146:         int bytesRead;",
          "147:         if (buf.hasArray() || !buf.isDirect()) {",
          "148:             bytesRead = mInputStream.read(buf.array(), buf.position(), buf.remaining());",
          "149:             if (bytesRead > 0) {",
          "150:                 buf.position(buf.position() + bytesRead);",
          "151:             }",
          "152:         } else {",
          "153:             bytesRead = mInputStream.read(buf);",
          "154:         }",
          "155:         if (bytesRead != -1 && mStatistics != null) {",
          "156:             mStatistics.incrementBytesRead(bytesRead);",
          "157:             LOG.info(\"Read {} byte buffer {}.\", bytesRead, buf.hasArray());",
          "158:         }",
          "159:         return bytesRead;",
          "160:     }",
          "162:     @Override",
          "163:     public int read(long position, byte[] buffer, int offset, int length) throws IOException {",
          "164:         if (mClosed) {",
          "165:             throw new IOException(ExceptionMessage.READ_CLOSED_STREAM.getMessage());",
          "166:         }",
          "168:         int bytesRead = mInputStream.positionedRead(position, buffer, offset, length);",
          "169:         if (bytesRead != -1 && mStatistics != null) {",
          "170:             mStatistics.incrementBytesRead(bytesRead);",
          "171:             LOG.info(\"Read {} {} byte buffer.\", position, bytesRead);",
          "172:         }",
          "173:         return bytesRead;",
          "174:     }",
          "176:     @Override",
          "177:     public void readFully(long position, byte[] buffer) throws IOException {",
          "178:         readFully(position, buffer, 0, buffer.length);",
          "179:     }",
          "181:     @Override",
          "182:     public void readFully(long position, byte[] buffer, int offset, int length) throws IOException {",
          "183:         int totalBytesRead = 0;",
          "184:         while (totalBytesRead < length) {",
          "185:             int bytesRead =",
          "186:                     read(position + totalBytesRead, buffer, offset + totalBytesRead, length - totalBytesRead);",
          "187:             if (bytesRead == -1) {",
          "188:                 throw new EOFException();",
          "189:             }",
          "190:             totalBytesRead += bytesRead;",
          "191:         }",
          "192:         LOG.info(\"Read fully {} {} byte buffer.\", position, totalBytesRead);",
          "193:     }",
          "195:     @Override",
          "196:     public void seek(long pos) throws IOException {",
          "197:         try {",
          "198:             mInputStream.seek(pos);",
          "199:         } catch (IllegalArgumentException e) { // convert back to IOException",
          "200:             throw new IOException(e);",
          "201:         }",
          "202:     }",
          "211:     @Override",
          "212:     public boolean seekToNewSource(long targetPos) throws IOException {",
          "213:         throw new IOException(ExceptionMessage.NOT_SUPPORTED.getMessage());",
          "214:     }",
          "216:     @Override",
          "217:     public long skip(long n) throws IOException {",
          "218:         if (mClosed) {",
          "219:             throw new IOException(\"Cannot skip bytes in a closed stream.\");",
          "220:         }",
          "221:         return mInputStream.skip(n);",
          "222:     }",
          "223: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/CacheFileInputStream.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/CacheFileInputStream.java": [
          "File: kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/CacheFileInputStream.java -> kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/CacheFileInputStream.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.cache.fs;",
          "21: import alluxio.client.file.FileInStream;",
          "22: import org.apache.hadoop.fs.ByteBufferReadable;",
          "23: import org.apache.hadoop.fs.FSExceptionMessages;",
          "24: import org.apache.hadoop.fs.FSInputStream;",
          "25: import org.apache.hadoop.fs.FileSystem.Statistics;",
          "26: import org.apache.hadoop.fs.Path;",
          "27: import org.apache.hadoop.util.DirectBufferPool;",
          "28: import org.slf4j.Logger;",
          "29: import org.slf4j.LoggerFactory;",
          "31: import java.io.EOFException;",
          "32: import java.io.IOException;",
          "33: import java.nio.ByteBuffer;",
          "35: public class CacheFileInputStream extends FSInputStream implements ByteBufferReadable {",
          "37:     public static final int EINVAL = -0x16;",
          "42:     private static final Logger LOG = LoggerFactory.getLogger(CacheFileInputStream.class);",
          "44:     private final DirectBufferPool bufferPool;",
          "45:     private final Statistics statistics;",
          "46:     private final FileInStream mInputStream;",
          "48:     private ByteBuffer buf;",
          "49:     private Path file;",
          "50:     private boolean mClosed = false;",
          "52:     public CacheFileInputStream(Path file, FileInStream inputStream,",
          "53:                                 DirectBufferPool directBufferPool,",
          "54:                                 Statistics statistics, int size) throws IOException {",
          "55:         this.file = file;",
          "56:         this.mInputStream = inputStream;",
          "57:         this.bufferPool = directBufferPool;",
          "58:         if (this.bufferPool != null) {",
          "59:             this.buf = this.bufferPool.getBuffer(size);",
          "60:         } else {",
          "61:             this.buf = ByteBuffer.allocate(size);",
          "62:         }",
          "63:         this.statistics = statistics;",
          "64:         this.buf.limit(0);",
          "65:     }",
          "67:     @Override",
          "68:     public synchronized long getPos() throws IOException {",
          "69:         if (buf == null)",
          "70:             throw new IOException(",
          "71:                     \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "72:         return mInputStream.getPos() - buf.remaining();",
          "73:     }",
          "75:     @Override",
          "76:     public boolean seekToNewSource(long targetPos) throws IOException {",
          "77:         return false;",
          "78:     }",
          "80:     @Override",
          "81:     public synchronized int available() throws IOException {",
          "82:         if (buf == null)",
          "83:             throw new IOException(",
          "84:                     \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "85:         return buf.remaining() + mInputStream.available();",
          "86:     }",
          "88:     @Override",
          "89:     public boolean markSupported() {",
          "90:         return false;",
          "91:     }",
          "93:     @Override",
          "94:     public void reset() throws IOException {",
          "95:         throw new IOException(\"Mark/reset not supported\");",
          "96:     }",
          "98:     @Override",
          "99:     public synchronized int read() throws IOException {",
          "100:         if (buf == null)",
          "101:             throw new IOException(",
          "102:                     \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "103:         if (!buf.hasRemaining() && !refill())",
          "104:             return -1; // EOF",
          "105:         assert buf.hasRemaining();",
          "106:         statistics.incrementBytesRead(1);",
          "107:         return buf.get() & 0xFF;",
          "108:     }",
          "110:     @Override",
          "111:     public synchronized int read(byte[] b, int off, int len) throws IOException {",
          "112:         if (off < 0 || len < 0 || b.length - off < len)",
          "113:             throw new IndexOutOfBoundsException();",
          "114:         if (len == 0)",
          "115:             return 0;",
          "116:         if (buf == null)",
          "117:             throw new IOException(",
          "118:                     \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "119:         if (!buf.hasRemaining() && len <= buf.capacity() && !refill())",
          "120:             return -1; // No bytes were read before EOF.",
          "122:         int read = Math.min(buf.remaining(), len);",
          "123:         if (read > 0) {",
          "124:             buf.get(b, off, read);",
          "125:             statistics.incrementBytesRead(read);",
          "126:             off += read;",
          "127:             len -= read;",
          "128:         }",
          "129:         if (len == 0)",
          "130:             return read;",
          "132:         int more = readInternal(b, off, len);",
          "133:         if (more <= 0) {",
          "134:             if (read > 0) {",
          "135:                 return read;",
          "136:             } else {",
          "137:                 return -1;",
          "138:             }",
          "139:         }",
          "141:         buf.position(0);",
          "142:         buf.limit(0);",
          "143:         return read + more;",
          "144:     }",
          "146:     protected synchronized int readInternal(byte[] b, int off, int len) throws IOException {",
          "147:         if (len == 0)",
          "148:             return 0;",
          "149:         if (buf == null)",
          "150:             throw new IOException(",
          "151:                     \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "152:         if (b == null || off < 0 || len < 0 || b.length - off < len) {",
          "153:             throw new IllegalArgumentException(",
          "154:                     \"Reading file \" + this.file.toString() + \" error, invalid arguments: \" +",
          "155:                             off + \" \" + len);",
          "156:         }",
          "157:         int got = mInputStream.read(b, off, len);",
          "158:         if (got == 0)",
          "159:             return -1;",
          "160:         if (got == EINVAL)",
          "161:             throw new IOException(",
          "162:                     \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "163:         if (got < 0)",
          "164:             throw new IOException(",
          "165:                     \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "166:         statistics.incrementBytesRead(got);",
          "167:         return got;",
          "168:     }",
          "170:     private boolean refill() throws IOException {",
          "171:         buf.clear();",
          "172:         int read = readInternal(buf);",
          "173:         if (read <= 0) {",
          "174:             buf.limit(0);",
          "175:             return false; // EOF",
          "176:         }",
          "177:         statistics.incrementBytesRead(-read);",
          "178:         buf.flip();",
          "179:         return true;",
          "180:     }",
          "182:     @Override",
          "183:     public synchronized int read(long pos, byte[] b, int off, int len) throws IOException {",
          "184:         if (len == 0)",
          "185:             return 0;",
          "186:         if (pos < 0)",
          "187:             throw new EOFException(",
          "188:                     \"Reading file \" + this.file.toString() + \" error, position is negative\");",
          "189:         if (b == null || off < 0 || len < 0 || b.length - off < len) {",
          "190:             throw new IllegalArgumentException(",
          "191:                     \"Reading file \" + this.file.toString() + \" error, invalid arguments: \" +",
          "192:                             off + \" \" + len);",
          "193:         }",
          "194:         long oldPos = mInputStream.getPos();",
          "195:         mInputStream.seek(pos);",
          "196:         int got = -1;",
          "197:         try {",
          "198:             got = mInputStream.read(b, off, len);",
          "199:             if (got == 0)",
          "200:                 return -1;",
          "201:             if (got == EINVAL)",
          "202:                 throw new IOException(",
          "203:                         \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "204:             if (got < 0)",
          "205:                 throw new IOException(",
          "206:                         \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "207:         } finally {",
          "208:             mInputStream.seek(oldPos);",
          "209:         }",
          "210:         statistics.incrementBytesRead(got);",
          "211:         return got;",
          "212:     }",
          "214:     @Override",
          "215:     public synchronized int read(ByteBuffer b) throws IOException {",
          "216:         if (!b.hasRemaining())",
          "217:             return 0;",
          "218:         if (buf == null)",
          "219:             throw new IOException(",
          "220:                     \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "221:         if (!buf.hasRemaining() && b.remaining() <= buf.capacity() && !refill()) {",
          "222:             return -1;",
          "223:         }",
          "224:         int got = Math.min(b.remaining(), buf.remaining());",
          "225:         if (got > 0) {",
          "226:             byte[] readedBytes = new byte[got];",
          "227:             buf.get(readedBytes, 0, got);",
          "228:             b.put(readedBytes, 0, got);",
          "229:             statistics.incrementBytesRead(got);",
          "230:         }",
          "231:         if (!b.hasRemaining())",
          "232:             return got;",
          "233:         int more = readInternal(b);",
          "234:         if (more <= 0)",
          "235:             return got > 0 ? got : -1;",
          "236:         buf.position(0);",
          "237:         buf.limit(0);",
          "238:         return got + more;",
          "239:     }",
          "241:     protected synchronized int readInternal(ByteBuffer b) throws IOException {",
          "242:         if (!b.hasRemaining())",
          "243:             return 0;",
          "244:         int got;",
          "245:         if (b.hasArray()) {",
          "247:             got = readInternal(b.array(), b.position(), b.remaining());",
          "248:             if (got <= 0)",
          "249:                 return got;",
          "250:             b.position(b.position() + got);",
          "251:         } else {",
          "252:             assert b.isDirect();",
          "253:             got = mInputStream.read(b, b.position(), b.remaining());",
          "254:             if (got == EINVAL)",
          "255:                 throw new IOException(",
          "256:                         \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "257:             if (got < 0)",
          "258:                 throw new IOException(",
          "259:                         \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "260:             if (got == 0)",
          "261:                 return -1;",
          "262:             statistics.incrementBytesRead(got);",
          "263:         }",
          "264:         return got;",
          "265:     }",
          "267:     @Override",
          "268:     public synchronized void seek(long p) throws IOException {",
          "269:         if (p < 0) {",
          "270:             throw new EOFException(FSExceptionMessages.NEGATIVE_SEEK);",
          "271:         }",
          "272:         if (buf == null)",
          "273:             throw new IOException(",
          "274:                     \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "275:         if (p < mInputStream.getPos() && p >= mInputStream.getPos() - buf.limit()) {",
          "276:             buf.position((int) (p - (mInputStream.getPos() - buf.limit())));",
          "277:         } else {",
          "278:             buf.position(0);",
          "279:             buf.limit(0);",
          "280:             mInputStream.seek(p);",
          "281:         }",
          "282:     }",
          "284:     @Override",
          "285:     public synchronized long skip(long n) throws IOException {",
          "286:         if (n < 0)",
          "287:             return -1;",
          "288:         if (buf == null)",
          "289:             throw new IOException(",
          "290:                     \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "291:         if (n < buf.remaining()) {",
          "292:             buf.position(buf.position() + (int) n);",
          "293:         } else {",
          "294:             mInputStream.skip(n - buf.remaining());",
          "295:             buf.position(0);",
          "296:             buf.limit(0);",
          "297:         }",
          "298:         return n;",
          "299:     }",
          "301:     @Override",
          "302:     public synchronized void close() throws IOException {",
          "303:         if (!mClosed) {",
          "304:             mInputStream.close();",
          "305:             mClosed = true;",
          "306:         }",
          "307:         if (this.bufferPool != null && buf != null) {",
          "308:             this.bufferPool.returnBuffer(buf);",
          "309:         }",
          "310:         buf = null;",
          "311:     }",
          "312: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/CacheFileSystemConstants.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/CacheFileSystemConstants.java": [
          "File: kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/CacheFileSystemConstants.java -> kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/CacheFileSystemConstants.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.cache.fs;",
          "21: public class CacheFileSystemConstants {",
          "23:     private CacheFileSystemConstants() {",
          "24:     }",
          "26:     public static final String PARAMS_KEY_USE_CACHE =",
          "27:             \"spark.kylin.local-cache.enabled\";",
          "29:     public static final boolean PARAMS_KEY_USE_CACHE_DEFAULT_VALUE = false;",
          "31:     public static final String PARAMS_KEY_IO_FILE_BUFFER_SIZE = \"io.file.buffer.size\";",
          "33:     public static final int PARAMS_KEY_IO_FILE_BUFFER_SIZE_DEFAULT_VALUE = 65536;",
          "35:     public static final String PARAMS_KEY_FILE_STATUS_CACHE_TTL =",
          "36:             \"spark.kylin.local-cache.filestatus.cache.ttl\";",
          "38:     public static final long PARAMS_KEY_FILE_STATUS_CACHE_TTL_DEFAULT_VALUE = 3600L;",
          "40:     public static final String PARAMS_KEY_FILE_STATUS_CACHE_MAX_SIZE =",
          "41:             \"spark.kylin.local-cache.filestatus.cache.max-size\";",
          "43:     public static final long PARAMS_KEY_FILE_STATUS_CACHE_MAX_SIZE_DEFAULT_VALUE = 10000L;",
          "45:     public static final String PARAMS_KEY_USE_LEGACY_FILE_INPUTSTREAM =",
          "46:             \"spark.kylin.local-cache.use.legacy.file.input-stream\";",
          "48:     public static final boolean PARAMS_KEY_USE_LEGACY_FILE_INPUTSTREAM_DEFAULT_VALUE = false;",
          "50:     public static final String PARAMS_KEY_LOCAL_CACHE_FOR_CURRENT_FILES =",
          "51:             \"spark.kylin.local-cache.for.current.files\";",
          "53:     public static final String JUICEFS_SCHEME = \"jfs\";",
          "54: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/MemCacheFileInputStream.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/MemCacheFileInputStream.java": [
          "File: kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/MemCacheFileInputStream.java -> kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/MemCacheFileInputStream.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.cache.fs;",
          "21: import org.apache.hadoop.fs.ByteBufferReadable;",
          "22: import org.apache.hadoop.fs.FSExceptionMessages;",
          "23: import org.apache.hadoop.fs.FSInputStream;",
          "24: import org.apache.hadoop.fs.FileSystem.Statistics;",
          "25: import org.apache.hadoop.fs.Path;",
          "26: import org.slf4j.Logger;",
          "27: import org.slf4j.LoggerFactory;",
          "29: import java.io.EOFException;",
          "30: import java.io.IOException;",
          "31: import java.nio.ByteBuffer;",
          "33: public class MemCacheFileInputStream extends FSInputStream implements ByteBufferReadable {",
          "35:     private static final Logger LOG = LoggerFactory.getLogger(MemCacheFileInputStream.class);",
          "37:     private final Statistics statistics;",
          "38:     private ByteBuffer buf;",
          "39:     private Path file;",
          "40:     private int fileLength;",
          "42:     public MemCacheFileInputStream(Path file, ByteBuffer buf, int fileLength,",
          "43:                                    Statistics statistics) throws IOException {",
          "44:         this.file = file;",
          "45:         this.buf = buf;",
          "46:         this.fileLength = fileLength;",
          "47:         this.statistics = statistics;",
          "48:         assert this.fileLength == buf.capacity();",
          "51:         this.buf.flip();",
          "52:         this.buf.limit(fileLength);",
          "53:     }",
          "55:     @Override",
          "56:     public synchronized long getPos() throws IOException {",
          "57:         if (buf == null)",
          "58:             throw new IOException(",
          "59:                     \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "60:         return buf.position();",
          "61:     }",
          "63:     @Override",
          "64:     public boolean seekToNewSource(long targetPos) throws IOException {",
          "65:         return false;",
          "66:     }",
          "68:     @Override",
          "69:     public synchronized int available() throws IOException {",
          "70:         if (buf == null)",
          "71:             throw new IOException(",
          "72:                     \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "73:         return buf.remaining();",
          "74:     }",
          "76:     @Override",
          "77:     public boolean markSupported() {",
          "78:         return false;",
          "79:     }",
          "81:     @Override",
          "82:     public void reset() throws IOException {",
          "83:         throw new IOException(\"Mark/reset not supported\");",
          "84:     }",
          "86:     public synchronized int read() throws IOException {",
          "87:         if (buf == null)",
          "88:             throw new IOException(",
          "89:                     \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "90:         if (!buf.hasRemaining())",
          "91:             return -1; // EOF",
          "92:         statistics.incrementBytesRead(1);",
          "93:         return buf.get() & 0xFF;",
          "94:     }",
          "96:     @Override",
          "97:     public synchronized int read(byte[] b, int off, int len) throws IOException {",
          "98:         if (off < 0 || len < 0 || b.length - off < len)",
          "99:             throw new IndexOutOfBoundsException();",
          "100:         if (len == 0)",
          "101:             return 0;",
          "102:         if (buf == null)",
          "103:             throw new IOException(",
          "104:                     \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "105:         if (!buf.hasRemaining())",
          "106:             return -1; // No bytes were read before EOF.",
          "108:         int read = Math.min(buf.remaining(), len);",
          "109:         if (read > 0) {",
          "110:             buf.get(b, off, read);",
          "111:             statistics.incrementBytesRead(read);",
          "112:         }",
          "113:         return read;",
          "114:     }",
          "116:     @Override",
          "117:     public synchronized int read(long pos, byte[] b, int off, int len) throws IOException {",
          "118:         if (len == 0)",
          "119:             return 0;",
          "120:         if (pos < 0 || pos > buf.limit())",
          "121:             throw new EOFException(",
          "122:                     \"Reading file \" + this.file.toString() + \" error, position is negative\");",
          "123:         if (b == null || off < 0 || len < 0 || b.length - off < len) {",
          "124:             throw new IllegalArgumentException(",
          "125:                     \"Reading file \" + this.file.toString() + \" error, invalid arguments: \" +",
          "126:                             off + \" \" + len);",
          "127:         }",
          "128:         int oldPos = buf.position();",
          "129:         buf.position((int)pos);",
          "130:         int got = Math.min(buf.remaining(), len);",
          "131:         try {",
          "132:             if (got > 0) {",
          "133:                 buf.get(b, off, len);",
          "134:                 statistics.incrementBytesRead(got);",
          "135:             }",
          "136:         } finally {",
          "137:             buf.position(oldPos);",
          "138:         }",
          "139:         return got;",
          "140:     }",
          "142:     @Override",
          "143:     public synchronized int read(ByteBuffer b) throws IOException {",
          "144:         if (!b.hasRemaining())",
          "145:             return 0;",
          "146:         if (buf == null)",
          "147:             throw new IOException(",
          "148:                     \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "149:         if (!buf.hasRemaining()) {",
          "150:             return -1;",
          "151:         }",
          "152:         int got = Math.min(b.remaining(), buf.remaining());",
          "153:         if (got > 0) {",
          "154:             byte[] readedBytes = new byte[got];",
          "155:             buf.get(readedBytes, 0, got);",
          "156:             b.put(readedBytes, 0, got);",
          "157:             statistics.incrementBytesRead(got);",
          "158:         }",
          "159:         return got;",
          "160:     }",
          "162:     @Override",
          "163:     public synchronized void seek(long p) throws IOException {",
          "164:         if (p < 0) {",
          "165:             throw new EOFException(FSExceptionMessages.NEGATIVE_SEEK);",
          "166:         }",
          "167:         if (p > buf.limit()) {",
          "168:             throw new EOFException(FSExceptionMessages.CANNOT_SEEK_PAST_EOF);",
          "169:         }",
          "170:         if (buf == null)",
          "171:             throw new IOException(",
          "172:                     \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "173:         buf.position((int)p);",
          "174:     }",
          "176:     @Override",
          "177:     public synchronized long skip(long n) throws IOException {",
          "178:         if (n < 0)",
          "179:             return -1;",
          "180:         if (buf == null)",
          "181:             throw new IOException(",
          "182:                     \"Reading file \" + this.file.toString() + \" error, stream was closed\");",
          "183:         if (n > buf.remaining()) {",
          "184:             throw new EOFException(\"Attempted to skip past the end of the file\");",
          "185:         }",
          "186:         buf.position(buf.position() + (int) n);",
          "187:         return n;",
          "188:     }",
          "190:     @Override",
          "191:     public synchronized void close() throws IOException {",
          "192:         if (buf != null) {",
          "193:             buf = null;",
          "194:         }",
          "195:     }",
          "197:     public Statistics getStatistics() {",
          "198:         return statistics;",
          "199:     }",
          "201:     public ByteBuffer getBuf() {",
          "202:         return buf;",
          "203:     }",
          "205:     public void setBuf(ByteBuffer buf) {",
          "206:         this.buf = buf;",
          "207:     }",
          "209:     public Path getFile() {",
          "210:         return file;",
          "211:     }",
          "213:     public void setFile(Path file) {",
          "214:         this.file = file;",
          "215:     }",
          "216: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/OnlyForTestCacheFileSystem.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/OnlyForTestCacheFileSystem.java": [
          "File: kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/OnlyForTestCacheFileSystem.java -> kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/OnlyForTestCacheFileSystem.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.cache.fs;",
          "21: public class OnlyForTestCacheFileSystem  extends AbstractCacheFileSystem {",
          "26:     @Override",
          "27:     public boolean isUseLocalCacheForTargetExecs() {",
          "28:         return true;",
          "29:     }",
          "30: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/kylin/CacheAllFileSystem.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/kylin/CacheAllFileSystem.java": [
          "File: kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/kylin/CacheAllFileSystem.java -> kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/kylin/CacheAllFileSystem.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.cache.fs.kylin;",
          "21: import org.apache.kylin.cache.fs.AbstractCacheFileSystem;",
          "23: public class CacheAllFileSystem extends AbstractCacheFileSystem {",
          "28:     @Override",
          "29:     public boolean isUseLocalCacheForTargetExecs() {",
          "30:         return true;",
          "31:     }",
          "32: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/kylin/KylinCacheFileSystem.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/kylin/KylinCacheFileSystem.java": [
          "File: kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/kylin/KylinCacheFileSystem.java -> kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/fs/kylin/KylinCacheFileSystem.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.cache.fs.kylin;",
          "21: import org.apache.commons.lang3.StringUtils;",
          "22: import org.apache.kylin.cache.fs.AbstractCacheFileSystem;",
          "23: import org.apache.kylin.cache.fs.CacheFileSystemConstants;",
          "24: import org.apache.spark.TaskContext;",
          "25: import org.slf4j.Logger;",
          "26: import org.slf4j.LoggerFactory;",
          "28: public class KylinCacheFileSystem extends AbstractCacheFileSystem {",
          "30:     private static final Logger LOG = LoggerFactory.getLogger(KylinCacheFileSystem.class);",
          "35:     @Override",
          "36:     public boolean isUseLocalCacheForTargetExecs() {",
          "37:         if (null == TaskContext.get()) {",
          "38:             LOG.warn(\"Task Context is null.\");",
          "39:             return false;",
          "40:         }",
          "41:         String localCacheForCurrExecutor =",
          "42:                 TaskContext.get()",
          "43:                         .getLocalProperty(CacheFileSystemConstants.PARAMS_KEY_LOCAL_CACHE_FOR_CURRENT_FILES);",
          "44:         LOG.info(\"Cache for current executor is {}\", localCacheForCurrExecutor);",
          "45:         return (StringUtils.isNotBlank(localCacheForCurrExecutor) && Boolean.valueOf(localCacheForCurrExecutor));",
          "46:     }",
          "47: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ConsistentHash.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ConsistentHash.java": [
          "File: kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ConsistentHash.java -> kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ConsistentHash.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.cache.utils;",
          "21: import com.google.common.hash.HashFunction;",
          "22: import com.google.common.hash.Hashing;",
          "24: import java.util.List;",
          "25: import java.util.SortedMap;",
          "26: import java.util.concurrent.ConcurrentSkipListMap;",
          "28: public class ConsistentHash<T> {",
          "30:     private final int numberOfVirtualNodeReplicas;",
          "31:     private final SortedMap<Integer, T> circle = new ConcurrentSkipListMap<>();",
          "32:     private final HashFunction nodeHash = Hashing.murmur3_32();",
          "33:     private final HashFunction keyHash = Hashing.murmur3_32();",
          "35:     public ConsistentHash(int numberOfVirtualNodeReplicas, List<T> nodes) {",
          "36:         this.numberOfVirtualNodeReplicas = numberOfVirtualNodeReplicas;",
          "37:         addNode(nodes);",
          "38:     }",
          "40:     public ConsistentHash(int numberOfVirtualNodeReplicas) {",
          "41:         this.numberOfVirtualNodeReplicas = numberOfVirtualNodeReplicas;",
          "42:     }",
          "44:     public void addNode(List<T> nodes) {",
          "45:         for (T node : nodes) {",
          "46:             addNode(node);",
          "47:         }",
          "48:     }",
          "50:     public void addNode(T node) {",
          "51:         for (int i = 0; i < numberOfVirtualNodeReplicas; i++) {",
          "52:             circle.put(getKetamaHash(i + \"\" + node), node);",
          "53:         }",
          "54:     }",
          "56:     public void remove(List<T> nodes) {",
          "57:         for (T node : nodes) {",
          "58:             remove(node);",
          "59:         }",
          "60:     }",
          "62:     public void remove(T node) {",
          "63:         for (int i = 0; i < numberOfVirtualNodeReplicas; i++) {",
          "64:             circle.remove(getKetamaHash(i + \"\" + node));",
          "65:         }",
          "66:     }",
          "68:     public T get(Object key) {",
          "69:         if (circle.isEmpty()) {",
          "70:             return null;",
          "71:         }",
          "72:         int hash = getKeyHash(key.toString());",
          "73:         if (!circle.containsKey(hash)) {",
          "74:             SortedMap<Integer, T> tailMap = circle.tailMap(hash);",
          "75:             hash = tailMap.isEmpty() ? circle.firstKey() : tailMap.firstKey();",
          "76:         }",
          "77:         return circle.get(hash);",
          "78:     }",
          "80:     private int getKeyHash(final String k) {",
          "81:         return keyHash.hashBytes(k.getBytes()).asInt();",
          "82:     }",
          "84:     private int getKetamaHash(final String k) {",
          "85:         return nodeHash.hashBytes(k.getBytes()).asInt();",
          "86:     }",
          "87: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ReflectionUtil.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ReflectionUtil.java": [
          "File: kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ReflectionUtil.java -> kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/cache/utils/ReflectionUtil.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.cache.utils;",
          "21: import org.slf4j.Logger;",
          "22: import org.slf4j.LoggerFactory;",
          "24: import java.lang.reflect.Field;",
          "25: import java.lang.reflect.InvocationTargetException;",
          "26: import java.lang.reflect.Method;",
          "27: import java.lang.reflect.Modifier;",
          "28: import java.lang.reflect.ParameterizedType;",
          "29: import java.lang.reflect.Type;",
          "31: public class ReflectionUtil {",
          "33:     private static final Logger LOG = LoggerFactory.getLogger(ReflectionUtil.class);",
          "35:     private ReflectionUtil() {",
          "36:     }",
          "42:     public static Object getFieldValue(Object object, String fieldName) {",
          "43:         Field field = getDeclaredField(object, fieldName);",
          "44:         if (field == null) {",
          "45:             throw new IllegalArgumentException(",
          "46:                     \"Could not find field [\" + fieldName + \"] on target [\" + object + \"]\");",
          "47:         }",
          "48:         makeAccessible(field);",
          "50:         Object result = null;",
          "51:         try {",
          "52:             result = field.get(object);",
          "53:         } catch (IllegalAccessException e) {",
          "54:             LOG.error(\"Get field value error:\", e);",
          "55:         }",
          "57:         return result;",
          "58:     }",
          "64:     public static void setFieldValue(Object object, String fieldName, Object value) {",
          "65:         Field field = getDeclaredField(object, fieldName);",
          "66:         if (field == null) {",
          "67:             throw new IllegalArgumentException(",
          "68:                     \"Could not find field [\" + fieldName + \"] on target [\" + object + \"]\");",
          "69:         }",
          "70:         makeAccessible(field);",
          "72:         try {",
          "73:             field.set(object, value);",
          "74:         } catch (IllegalAccessException e) {",
          "75:             LOG.error(\"Set field value error:\", e);",
          "76:         }",
          "77:     }",
          "82:     @SuppressWarnings(\"unchecked\")",
          "83:     public static Class getSuperClassGenricType(Class clazz, int index) {",
          "84:         Type genType = clazz.getGenericSuperclass();",
          "85:         if (!(genType instanceof ParameterizedType)) {",
          "86:             return Object.class;",
          "87:         }",
          "89:         Type[] params = ((ParameterizedType) genType).getActualTypeArguments();",
          "90:         if (index >= params.length || index < 0) {",
          "91:             return Object.class;",
          "92:         }",
          "93:         if (!(params[index] instanceof Class)) {",
          "94:             return Object.class;",
          "95:         }",
          "97:         return (Class) params[index];",
          "98:     }",
          "103:     @SuppressWarnings(\"unchecked\")",
          "104:     public static <T> Class<T> getSuperGenericType(Class clazz) {",
          "105:         return getSuperClassGenricType(clazz, 0);",
          "106:     }",
          "111:     public static Method getDeclaredMethod(Object object, String methodName, Class<?>[] parameterTypes) {",
          "112:         for (Class<?> superClass = object.getClass(); superClass != Object.class;",
          "113:              superClass = superClass.getSuperclass()) {",
          "114:             try {",
          "115:                 return superClass.getDeclaredMethod(methodName, parameterTypes);",
          "116:             } catch (NoSuchMethodException e) {",
          "117:             }",
          "118:         }",
          "119:         LOG.error(\"Can not find method \" + methodName);",
          "120:         return null;",
          "121:     }",
          "126:     public static void makeAccessible(Field field) {",
          "127:         if (!Modifier.isPublic(field.getModifiers())) {",
          "128:             field.setAccessible(true);",
          "129:         }",
          "130:     }",
          "135:     public static Field getDeclaredField(Object object, String filedName) {",
          "136:         for (Class<?> superClass = object.getClass(); superClass != Object.class;",
          "137:              superClass = superClass.getSuperclass()) {",
          "138:             try {",
          "139:                 return superClass.getDeclaredField(filedName);",
          "140:             } catch (NoSuchFieldException e) {",
          "141:             }",
          "142:         }",
          "143:         LOG.error(\"Can not find field \" + filedName);",
          "144:         return null;",
          "145:     }",
          "151:     public static Object invokeMethod(Object object, String methodName, Class<?>[] parameterTypes,",
          "152:                                       Object[] parameters) throws InvocationTargetException {",
          "153:         Method method = getDeclaredMethod(object, methodName, parameterTypes);",
          "154:         if (method == null) {",
          "155:             throw new IllegalArgumentException(",
          "156:                     \"Could not find method [\" + methodName + \"] on target [\" + object + \"]\");",
          "157:         }",
          "158:         method.setAccessible(true);",
          "160:         try {",
          "161:             return method.invoke(object, parameters);",
          "162:         } catch (IllegalAccessException e) {",
          "163:             LOG.error(\"Invoke method \" + methodName + \" error: \", e);",
          "164:         }",
          "166:         return null;",
          "167:     }",
          "168: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/softaffinity/SoftAffinityConstants.java||kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/softaffinity/SoftAffinityConstants.java": [
          "File: kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/softaffinity/SoftAffinityConstants.java -> kylin-spark-project/kylin-soft-affinity-cache/src/main/java/org/apache/kylin/softaffinity/SoftAffinityConstants.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.softaffinity;",
          "21: public class SoftAffinityConstants {",
          "23:     private SoftAffinityConstants() {",
          "24:     }",
          "26:     public static final String PARAMS_KEY_SOFT_AFFINITY_ENABLED =",
          "27:             \"spark.kylin.soft-affinity.enabled\";",
          "29:     public static final boolean PARAMS_KEY_SOFT_AFFINITY_ENABLED_DEFAULT_VALUE = false;",
          "31:     public static final String PARAMS_KEY_SOFT_AFFINITY_REPLICATIONS_NUM =",
          "32:             \"spark.kylin.soft-affinity.replications.num\";",
          "34:     public static final int PARAMS_KEY_SOFT_AFFINITY_REPLICATIONS_NUM_DEFAULT_VALUE = 2;",
          "37:     public static final String PARAMS_KEY_SOFT_AFFINITY_MIN_TARGET_HOSTS =",
          "38:             \"spark.kylin.soft-affinity.min.target-hosts\";",
          "40:     public static final int PARAMS_KEY_SOFT_AFFINITY_MIN_TARGET_HOSTS_DEFAULT_VALUE = 1;",
          "41: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/SoftAffinityManager.scala||kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/SoftAffinityManager.scala": [
          "File: kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/SoftAffinityManager.scala -> kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/SoftAffinityManager.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.softaffinity",
          "21: import org.apache.kylin.softaffinity.strategy.SoftAffinityStrategy",
          "23: import java.util.concurrent.locks.ReentrantReadWriteLock",
          "24: import scala.collection.mutable",
          "25: import org.apache.spark.SparkEnv",
          "26: import org.apache.spark.internal.Logging",
          "28: import java.util.concurrent.atomic.AtomicInteger",
          "30: object SoftAffinityManager extends Logging {",
          "32:   val resourceRWLock = new ReentrantReadWriteLock(true)",
          "34:   val softAffinityAllocation = new SoftAffinityStrategy",
          "36:   lazy val minOnTargetHosts = SparkEnv.get.conf.getInt(",
          "37:     SoftAffinityConstants.PARAMS_KEY_SOFT_AFFINITY_MIN_TARGET_HOSTS,",
          "38:     SoftAffinityConstants.PARAMS_KEY_SOFT_AFFINITY_MIN_TARGET_HOSTS_DEFAULT_VALUE",
          "39:   )",
          "42:   val fixedIdForExecutors = new mutable.ListBuffer[Option[(String, String)]]()",
          "44:   val nodesExecutorsMap = new mutable.HashMap[String, mutable.HashSet[String]]()",
          "46:   protected val totalRegisteredExecutors = new AtomicInteger(0)",
          "48:   lazy val usingSoftAffinity = SparkEnv.get.conf.getBoolean(",
          "49:     SoftAffinityConstants.PARAMS_KEY_SOFT_AFFINITY_ENABLED,",
          "50:     SoftAffinityConstants.PARAMS_KEY_SOFT_AFFINITY_ENABLED_DEFAULT_VALUE",
          "51:   )",
          "53:   def totalExecutors(): Int = totalRegisteredExecutors.intValue()",
          "55:   def handleExecutorAdded(execHostId: (String, String)): Unit = {",
          "56:     resourceRWLock.writeLock().lock()",
          "57:     try {",
          "59:       if (!fixedIdForExecutors.exists( exec => {",
          "60:         exec.isDefined && exec.get._1.equals(execHostId._1)",
          "61:       })) {",
          "62:         val executorsSet = nodesExecutorsMap.getOrElseUpdate(execHostId._2,",
          "63:           new mutable.HashSet[String]())",
          "64:         executorsSet.add(execHostId._1)",
          "65:         if (fixedIdForExecutors.exists(_.isEmpty)) {",
          "67:           val replaceIdx = fixedIdForExecutors.indexWhere(_.isEmpty)",
          "68:           fixedIdForExecutors(replaceIdx) = Option(execHostId)",
          "69:         } else {",
          "70:           fixedIdForExecutors += Option(execHostId)",
          "71:         }",
          "72:         totalRegisteredExecutors.addAndGet(1)",
          "73:       }",
          "74:       logInfo(s\"After adding executor ${execHostId._1} on host ${execHostId._2}, \" +",
          "75:         s\"fixedIdForExecutors is ${fixedIdForExecutors.mkString(\",\")}, \" +",
          "76:         s\"nodesExecutorsMap is ${nodesExecutorsMap.keySet.mkString(\",\")}, \" +",
          "77:         s\"actual executors count is ${totalRegisteredExecutors.intValue()}.\"",
          "78:       )",
          "79:     } finally {",
          "80:       resourceRWLock.writeLock().unlock()",
          "81:     }",
          "82:   }",
          "84:   def handleExecutorRemoved(execId: String): Unit = {",
          "85:     resourceRWLock.writeLock().lock()",
          "86:     try {",
          "87:       val execIdx = fixedIdForExecutors.indexWhere( execHost => {",
          "88:         if (execHost.isDefined) {",
          "89:           execHost.get._1.equals(execId)",
          "90:         } else {",
          "91:           false",
          "92:         }",
          "93:       })",
          "94:       if (execIdx != -1) {",
          "95:         val findedExecId = fixedIdForExecutors(execIdx)",
          "96:         fixedIdForExecutors(execIdx) = None",
          "97:         val nodeExecs = nodesExecutorsMap.get(findedExecId.get._2).get",
          "98:         nodeExecs -= findedExecId.get._1",
          "99:         if (nodeExecs.isEmpty) {",
          "101:           nodesExecutorsMap.remove(findedExecId.get._2)",
          "102:         }",
          "103:         totalRegisteredExecutors.addAndGet(-1)",
          "104:       }",
          "105:       logInfo(s\"After removing executor ${execId}, \" +",
          "106:         s\"fixedIdForExecutors is ${fixedIdForExecutors.mkString(\",\")}, \" +",
          "107:         s\"nodesExecutorsMap is ${nodesExecutorsMap.keySet.mkString(\",\")}, \" +",
          "108:         s\"actual executors count is ${totalRegisteredExecutors.intValue()}.\"",
          "109:       )",
          "110:     } finally {",
          "111:       resourceRWLock.writeLock().unlock()",
          "112:     }",
          "113:   }",
          "115:   def checkTargetHosts(hosts: Array[String]): Boolean = {",
          "116:     resourceRWLock.readLock().lock()",
          "117:     try {",
          "118:       if (hosts.length < 1) {",
          "120:         false",
          "121:       } else if (nodesExecutorsMap.size < 1) {",
          "122:         true",
          "123:       } else {",
          "125:         val minHostsNum = Math.min(minOnTargetHosts, hosts.length)",
          "127:         nodesExecutorsMap.map(_._1).toArray.intersect(hosts).size >= minHostsNum",
          "128:       }",
          "129:     } finally {",
          "130:       resourceRWLock.readLock().unlock()",
          "131:     }",
          "132:   }",
          "134:   def askExecutors(file: String): Array[(String, String)] = {",
          "135:     resourceRWLock.readLock().lock()",
          "136:     try {",
          "137:       if (nodesExecutorsMap.size < 1) {",
          "138:         Array.empty",
          "139:       } else {",
          "140:         softAffinityAllocation.allocateExecs(file, fixedIdForExecutors)",
          "141:       }",
          "142:     } finally {",
          "143:       resourceRWLock.readLock().unlock()",
          "144:     }",
          "145:   }",
          "146: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/scheduler/SoftAffinityListener.scala||kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/scheduler/SoftAffinityListener.scala": [
          "File: kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/scheduler/SoftAffinityListener.scala -> kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/scheduler/SoftAffinityListener.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.softaffinity.scheduler",
          "21: import org.apache.kylin.softaffinity.SoftAffinityManager",
          "23: import org.apache.spark.internal.Logging",
          "24: import org.apache.spark.scheduler.{SparkListener, SparkListenerExecutorAdded, SparkListenerExecutorRemoved}",
          "26: class SoftAffinityListener extends SparkListener with Logging {",
          "28:   override def onExecutorAdded(executorAdded: SparkListenerExecutorAdded): Unit = {",
          "29:     val execId = executorAdded.executorId",
          "30:     val host = executorAdded.executorInfo.executorHost",
          "31:     SoftAffinityManager.handleExecutorAdded((execId, host))",
          "32:   }",
          "34:   override def onExecutorRemoved(executorRemoved: SparkListenerExecutorRemoved): Unit = {",
          "35:     val execId = executorRemoved.executorId",
          "36:     SoftAffinityManager.handleExecutorRemoved(execId)",
          "37:   }",
          "38: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/strategy/SoftAffinityAllocationTrait.scala||kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/strategy/SoftAffinityAllocationTrait.scala": [
          "File: kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/strategy/SoftAffinityAllocationTrait.scala -> kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/strategy/SoftAffinityAllocationTrait.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.softaffinity.strategy",
          "21: import org.apache.kylin.softaffinity.SoftAffinityConstants",
          "23: import scala.collection.mutable.ListBuffer",
          "24: import org.apache.spark.SparkEnv",
          "26: trait SoftAffinityAllocationTrait {",
          "28:   lazy val softAffinityReplicationNum = SparkEnv.get.conf.getInt(",
          "29:     SoftAffinityConstants.PARAMS_KEY_SOFT_AFFINITY_REPLICATIONS_NUM,",
          "30:     SoftAffinityConstants.PARAMS_KEY_SOFT_AFFINITY_REPLICATIONS_NUM_DEFAULT_VALUE",
          "31:   )",
          "36:   def allocateExecs(file: String,",
          "37:                     candidates: ListBuffer[Option[(String, String)]]): Array[(String, String)]",
          "38: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/strategy/SoftAffinityStrategy.scala||kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/strategy/SoftAffinityStrategy.scala": [
          "File: kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/strategy/SoftAffinityStrategy.scala -> kylin-spark-project/kylin-soft-affinity-cache/src/main/scala/org/apache/kylin/softaffinity/strategy/SoftAffinityStrategy.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.softaffinity.strategy",
          "21: import scala.collection.mutable.LinkedHashSet",
          "22: import scala.collection.mutable.ListBuffer",
          "24: import org.apache.spark.internal.Logging",
          "26: class SoftAffinityStrategy extends SoftAffinityAllocationTrait with Logging {",
          "31:   override def allocateExecs(file: String,",
          "32:       candidates: ListBuffer[Option[(String, String)]]): Array[(String, String)] = {",
          "33:     if (candidates.size < 1) {",
          "34:       Array.empty",
          "35:     } else {",
          "36:       val candidatesSize = candidates.size",
          "37:       val halfCandidatesSize = candidatesSize / softAffinityReplicationNum",
          "38:       val resultSet = new LinkedHashSet[(String, String)]",
          "40:       val mod = file.hashCode % candidatesSize",
          "41:       val c1 = if (mod < 0) (mod + candidatesSize) else mod",
          "43:       if (candidates(c1).isDefined) {",
          "44:         resultSet.add(candidates(c1).get)",
          "45:       }",
          "46:       for (i <- 1 to (softAffinityReplicationNum - 1)) {",
          "47:         val c2 = (c1 + halfCandidatesSize + i) % candidatesSize",
          "48:         if (candidates(c2).isDefined) {",
          "49:           resultSet.add(candidates(c2).get)",
          "50:         }",
          "51:       }",
          "52:       resultSet.toArray",
          "53:     }",
          "54:   }",
          "55: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-soft-affinity-cache/src/main/spark31/org/apache/spark/sql/execution/datasources/CacheFilePartition.scala||kylin-spark-project/kylin-soft-affinity-cache/src/main/spark31/org/apache/spark/sql/execution/datasources/CacheFilePartition.scala": [
          "File: kylin-spark-project/kylin-soft-affinity-cache/src/main/spark31/org/apache/spark/sql/execution/datasources/CacheFilePartition.scala -> kylin-spark-project/kylin-soft-affinity-cache/src/main/spark31/org/apache/spark/sql/execution/datasources/CacheFilePartition.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.spark.sql.execution.datasources",
          "21: import org.apache.kylin.softaffinity.SoftAffinityManager",
          "23: import org.apache.spark.Partition",
          "24: import org.apache.spark.internal.Logging",
          "25: import org.apache.spark.scheduler.ExecutorCacheTaskLocation",
          "26: import org.apache.spark.sql.connector.read.InputPartition",
          "32: case class CacheFilePartition(index: Int, files: Array[CachePartitionedFile])",
          "33:   extends Partition with InputPartition {",
          "34:   override def preferredLocations(): Array[String] = {",
          "36:     files.head.locations.map { p =>",
          "37:       if (p._1.equals(\"\")) p._2",
          "38:       else ExecutorCacheTaskLocation(p._2, p._1).toString",
          "39:     }.toArray",
          "40:   }",
          "41: }",
          "43: object CacheFilePartition extends Logging {",
          "45:   def convertFilePartitionToCache(filePartition: FilePartition): CacheFilePartition = {",
          "47:     val expectedTargets = filePartition.preferredLocations()",
          "48:     val files = filePartition.files",
          "50:     var locations = Array.empty[(String, String)]",
          "51:     if (!files.isEmpty && SoftAffinityManager.usingSoftAffinity",
          "52:       && !SoftAffinityManager.checkTargetHosts(expectedTargets)) {",
          "56:       locations = SoftAffinityManager.askExecutors(files.head.filePath)",
          "57:       if (!locations.isEmpty) {",
          "58:         logInfo(s\"SAMetrics=File ${files.head.filePath} - \" +",
          "59:           s\"the expected executors are ${locations.mkString(\"_\")} \")",
          "60:       } else {",
          "61:         locations = expectedTargets.map((\"\", _))",
          "62:       }",
          "63:     } else {",
          "64:       locations = expectedTargets.map((\"\", _))",
          "65:     }",
          "66:     CacheFilePartition(filePartition.index, filePartition.files.map(p => {",
          "67:       CachePartitionedFile(p.partitionValues, p.filePath, p.start, p.length, locations)",
          "68:     }))",
          "69:   }",
          "70: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-soft-affinity-cache/src/main/spark31/org/apache/spark/sql/execution/datasources/CacheFileScanRDD.scala||kylin-spark-project/kylin-soft-affinity-cache/src/main/spark31/org/apache/spark/sql/execution/datasources/CacheFileScanRDD.scala": [
          "File: kylin-spark-project/kylin-soft-affinity-cache/src/main/spark31/org/apache/spark/sql/execution/datasources/CacheFileScanRDD.scala -> kylin-spark-project/kylin-soft-affinity-cache/src/main/spark31/org/apache/spark/sql/execution/datasources/CacheFileScanRDD.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.spark.sql.execution.datasources",
          "21: import org.apache.kylin.cache.fs.CacheFileSystemConstants",
          "23: import org.apache.spark.{SparkEnv, TaskContext, Partition => RDDPartition}",
          "24: import org.apache.spark.sql.SparkSession",
          "25: import org.apache.spark.sql.catalyst.InternalRow",
          "27: case class CachePartitionedFile(",
          "28:                             partitionValues: InternalRow,",
          "29:                             filePath: String,",
          "30:                             start: Long,",
          "31:                             length: Long,",
          "32:                             locations: Array[(String, String)] = Array.empty) {",
          "33:   override def toString: String = {",
          "34:     s\"path: $filePath, range: $start-${start + length}, partition values: $partitionValues,\" +",
          "35:       s\" on executor locations ${locations.mkString}\"",
          "36:   }",
          "37: }",
          "39: class CacheFileScanRDD(",
          "40:     @transient private val sparkSession: SparkSession,",
          "41:     readFunction: (PartitionedFile) => Iterator[InternalRow],",
          "42:     @transient val cacheFilePartitions: Seq[CacheFilePartition])",
          "43:   extends FileScanRDD(sparkSession, readFunction, Nil) {",
          "45:   def checkCached(cacheLocations: Array[(String, String)]): Boolean = {",
          "46:     cacheLocations.map(_._1).contains(SparkEnv.get.executorId)",
          "47:   }",
          "49:   override def compute(split: RDDPartition, context: TaskContext): Iterator[InternalRow] = {",
          "51:     val start = System.currentTimeMillis()",
          "52:     val cacheFilePartition = split.asInstanceOf[CacheFilePartition]",
          "53:     val cacheSplit = FilePartition(cacheFilePartition.index, cacheFilePartition.files.map { f =>",
          "54:       PartitionedFile(f.partitionValues, f.filePath, f.start, f.length, f.locations.map(_._2))",
          "55:     })",
          "56:     var currFilePath = \"empty\"",
          "57:     val isCache = if (!cacheFilePartition.files.isEmpty) {",
          "58:       currFilePath = cacheFilePartition.files.head.filePath",
          "59:       checkCached(cacheFilePartition.files.head.locations)",
          "60:     } else {",
          "61:       false",
          "62:     }",
          "63:     logInfo(s\"SAMetrics=File ${currFilePath} running in task ${context.taskAttemptId()} \" +",
          "64:       s\"on executor ${SparkEnv.get.executorId} with cached ${isCache} , \" +",
          "65:       s\"took ${(System.currentTimeMillis() - start)}\")",
          "67:     context.getLocalProperties.setProperty(",
          "68:       CacheFileSystemConstants.PARAMS_KEY_LOCAL_CACHE_FOR_CURRENT_FILES, isCache.toString)",
          "69:     super.compute(cacheSplit, context)",
          "70:   }",
          "72:   override protected def getPartitions: Array[RDDPartition] = cacheFilePartitions.toArray",
          "74:   override protected def getPreferredLocations(split: RDDPartition): Seq[String] = {",
          "75:     split.asInstanceOf[CacheFilePartition].preferredLocations()",
          "76:   }",
          "77: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/AbstractHdfsLogAppender.java||kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/AbstractHdfsLogAppender.java": [
          "File: kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/AbstractHdfsLogAppender.java -> kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/AbstractHdfsLogAppender.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: import org.apache.log4j.AppenderSkeleton;",
          "28: import org.apache.log4j.helpers.LogLog;",
          "29: import org.apache.log4j.spi.LoggingEvent;",
          "31: import java.io.BufferedWriter;",
          "32: import java.io.IOException;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "30: import org.apache.spark.utils.SparkHadoopUtils;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "92:     public FileSystem getFileSystem() {",
          "93:         if (null == fileSystem) {",
          "95:         }",
          "96:         return fileSystem;",
          "97:     }",
          "",
          "[Removed Lines]",
          "94:             return getFileSystem(new Configuration());",
          "",
          "[Added Lines]",
          "95:             return getFileSystem(SparkHadoopUtils.newConfigurationWithSparkConf());",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/SparkDriverHdfsLogAppender.java||kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/SparkDriverHdfsLogAppender.java": [
          "File: kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/SparkDriverHdfsLogAppender.java -> kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/SparkDriverHdfsLogAppender.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import org.apache.hadoop.security.UserGroupInformation;",
          "24: import org.apache.log4j.helpers.LogLog;",
          "25: import org.apache.log4j.spi.LoggingEvent;",
          "27: import java.io.IOException;",
          "28: import java.util.List;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: import org.apache.spark.utils.SparkHadoopUtils;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "92:     public void doWriteLog(int eventSize, List<LoggingEvent> transaction)",
          "93:             throws IOException, InterruptedException {",
          "94:         if (!isWriterInited()) {",
          "96:             if (isKerberosEnable()) {",
          "97:                 UserGroupInformation.setConfiguration(conf);",
          "98:                 UserGroupInformation.loginUserFromKeytab(getKerberosPrincipal(), getKerberosKeytab());",
          "",
          "[Removed Lines]",
          "95:             Configuration conf = new Configuration();",
          "",
          "[Added Lines]",
          "96:             Configuration conf = SparkHadoopUtils.newConfigurationWithSparkConf();",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/SparkExecutorHdfsAppender.java||kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/SparkExecutorHdfsAppender.java": [
          "File: kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/SparkExecutorHdfsAppender.java -> kylin-spark-project/kylin-spark-common/src/main/java/org/apache/kylin/engine/spark/common/logging/SparkExecutorHdfsAppender.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import com.google.common.annotations.VisibleForTesting;",
          "22: import org.apache.commons.lang3.StringUtils;",
          "24: import org.apache.hadoop.fs.FileStatus;",
          "25: import org.apache.hadoop.fs.FileSystem;",
          "26: import org.apache.hadoop.fs.Path;",
          "",
          "[Removed Lines]",
          "23: import org.apache.hadoop.conf.Configuration;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "30: import org.apache.log4j.spi.LoggingEvent;",
          "31: import org.apache.spark.SparkEnv;",
          "32: import org.apache.spark.deploy.SparkHadoopUtil;",
          "33: import scala.runtime.BoxedUnit;",
          "35: import java.io.File;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "32: import org.apache.spark.utils.SparkHadoopUtils;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "160:                 SparkHadoopUtil.get().runAsSparkUser(new scala.runtime.AbstractFunction0<scala.runtime.BoxedUnit>() {",
          "161:                     @Override",
          "162:                     public BoxedUnit apply() {",
          "164:                             LogLog.error(\"Failed to init the hdfs writer!\");",
          "165:                         }",
          "166:                         try {",
          "",
          "[Removed Lines]",
          "163:                         if (!initHdfsWriter(file, new Configuration())) {",
          "",
          "[Added Lines]",
          "163:                         if (!initHdfsWriter(file, SparkHadoopUtils.newConfigurationWithSparkConf())) {",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala||kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala": [
          "File: kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala -> kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "211:   var cached = new java.util.HashMap[(Seq[Expression], Seq[Expression]), Seq[PartitionDirectory]]()",
          "213:   private def getFileStatusBySeg(seg: SegmentDirectory, fsc: FileStatusCache): SegmentDirectory = {",
          "214:     val path = new Path(toPath(seg.segmentName, seg.identifier))",
          "215:     val fs = path.getFileSystem(session.sparkContext.hadoopConfiguration)",
          "216:     if (fs.isDirectory(path) && fs.exists(path)) {",
          "217:       val maybeStatuses = fsc.getLeafFiles(path)",
          "218:       if (maybeStatuses.isDefined) {",
          "219:         SegmentDirectory(seg.segmentName, seg.identifier, maybeStatuses.get)",
          "220:       } else {",
          "221:         val statuses = fs.listStatus(path)",
          "222:         fsc.putLeafFiles(path, statuses)",
          "223:         SegmentDirectory(seg.segmentName, seg.identifier, statuses)",
          "224:       }",
          "225:     } else {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "214:     var startT = System.currentTimeMillis()",
          "217:     logInfo(s\"Get segment filesystem: ${System.currentTimeMillis() - startT}\")",
          "218:     startT = System.currentTimeMillis()",
          "222:         logInfo(s\"Get segment status from cache: ${System.currentTimeMillis() - startT}\")",
          "227:         logInfo(s\"Get segment status and cache: ${System.currentTimeMillis() - startT}\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "239:     val timePartitionFilters = getSegmentFilter(dataFilters, timePartitionColumn)",
          "240:     logInfo(s\"Applying time partition filters: ${timePartitionFilters.mkString(\",\")}\")",
          "242:     val fsc = ShardFileStatusCache.getFileStatusCache(session)",
          "245:     var selected = afterPruning(\"segment\", timePartitionFilters, segmentDirs) {",
          "246:       pruneSegments",
          "247:     }",
          "250:     selected = selected.par.map(seg => {",
          "251:       getFileStatusBySeg(seg, fsc)",
          "252:     }).filter(_.files.nonEmpty).seq",
          "255:     selected = afterPruning(\"shard\", dataFilters, selected) {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "247:     var startT = System.currentTimeMillis()",
          "249:     logInfo(s\"Get file status cache: ${System.currentTimeMillis() - startT}\")",
          "256:     startT = System.currentTimeMillis()",
          "261:     logInfo(s\"Get segment status: ${System.currentTimeMillis() - startT}\")",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/utils/SparkHadoopUtils.scala||kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/utils/SparkHadoopUtils.scala": [
          "File: kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/utils/SparkHadoopUtils.scala -> kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/utils/SparkHadoopUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.spark.utils",
          "21: import org.apache.hadoop.conf.Configuration",
          "22: import org.apache.spark.SparkEnv",
          "23: import org.apache.spark.deploy.SparkHadoopUtil",
          "25: object SparkHadoopUtils {",
          "27:   def newConfigurationWithSparkConf(): Configuration = {",
          "28:     SparkHadoopUtil.newConfiguration(SparkEnv.get.conf)",
          "29:   }",
          "30: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala||kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala": [
          "File: kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala -> kylin-spark-project/kylin-spark-common/src/main/spark31/org/apache/spark/sql/execution/KylinFileSourceScanExec.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import org.apache.hadoop.fs.{BlockLocation, FileStatus, LocatedFileStatus, Path}",
          "22: import org.apache.kylin.common.KylinConfig",
          "23: import org.apache.spark.rdd.RDD",
          "24: import org.apache.spark.sql.catalyst.expressions.{Ascending, Attribute, Expression, ExpressionUtils, SortOrder}",
          "25: import org.apache.spark.sql.catalyst.plans.physical.{HashPartitioning, Partitioning, UnknownPartitioning}",
          "26: import org.apache.spark.sql.catalyst.{InternalRow, TableIdentifier}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24: import org.apache.kylin.softaffinity.SoftAffinityManager",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "165:       FilePartition(shardId, filesToPartitionId.getOrElse(shardId, Nil).toArray)",
          "166:     }",
          "169:   }",
          "",
          "[Removed Lines]",
          "168:     new FileScanRDD(fsRelation.sparkSession, readFile, filePartitions)",
          "",
          "[Added Lines]",
          "169:     if (SoftAffinityManager.usingSoftAffinity) {",
          "170:       val start = System.currentTimeMillis()",
          "171:       val cachePartitions = filePartitions.map(CacheFilePartition.convertFilePartitionToCache(_))",
          "172:       logInfo(s\"Convert bucketed file partition took: ${(System.currentTimeMillis() - start)}\")",
          "173:       new CacheFileScanRDD(fsRelation.sparkSession, readFile, cachePartitions)",
          "174:     } else {",
          "175:       new FileScanRDD(fsRelation.sparkSession, readFile, filePartitions)",
          "176:     }",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "237:     }",
          "238:     closePartition()",
          "241:   }",
          "243:   private def getBlockLocations(file: FileStatus): Array[BlockLocation] = file match {",
          "",
          "[Removed Lines]",
          "240:     new FileScanRDD(fsRelation.sparkSession, readFile, partitions)",
          "",
          "[Added Lines]",
          "248:     if (SoftAffinityManager.usingSoftAffinity) {",
          "249:       val start = System.currentTimeMillis()",
          "250:       val cachePartitions = partitions.map(CacheFilePartition.convertFilePartitionToCache(_))",
          "251:       logInfo(s\"Convert file partition took: ${(System.currentTimeMillis() - start)}\")",
          "252:       new CacheFileScanRDD(fsRelation.sparkSession, readFile, cachePartitions)",
          "253:     } else {",
          "254:       new FileScanRDD(fsRelation.sparkSession, readFile, partitions)",
          "255:     }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala": [
          "File: kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala -> kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: package org.apache.spark.sql",
          "34: import org.apache.commons.io.FileUtils",
          "35: import org.apache.kylin.common.KylinConfig",
          "36: import org.apache.kylin.common.util.ToolUtil",
          "37: import org.apache.kylin.query.monitor.SparderContextCanary",
          "38: import org.apache.kylin.spark.classloader.ClassLoaderUtils",
          "39: import org.apache.spark.deploy.StandaloneAppClient",
          "42: import org.apache.spark.sql.execution.datasource.{KylinSourceStrategy, ShardFileStatusCache}",
          "43: import org.apache.spark.sql.metrics.SparderMetricsListener",
          "44: import org.apache.spark.utils.YarnInfoFetcherUtils",
          "47: object SparderContext extends Logging {",
          "",
          "[Removed Lines]",
          "21: import java.lang.{Boolean => JBoolean, String => JString}",
          "22: import java.nio.file.Paths",
          "24: import org.apache.spark.memory.MonitorEnv",
          "25: import org.apache.spark.util.Utils",
          "26: import org.apache.spark.scheduler.{SparkListener, SparkListenerEvent}",
          "27: import org.apache.kylin.query.UdfManager",
          "28: import org.apache.spark.internal.Logging",
          "29: import org.apache.spark.sql.catalyst.parser.ParseException",
          "30: import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan",
          "31: import org.apache.spark.sql.KylinSession._",
          "32: import java.util.concurrent.atomic.AtomicReference",
          "40: import org.apache.spark.sql.SparderContext.master_app_url",
          "41: import org.apache.spark.{SparkConf, SparkContext, SparkEnv}",
          "",
          "[Added Lines]",
          "24: import org.apache.kylin.query.UdfManager",
          "28: import org.apache.spark.internal.Logging",
          "29: import org.apache.spark.memory.MonitorEnv",
          "30: import org.apache.spark.scheduler.{SparkListener, SparkListenerEvent}",
          "31: import org.apache.spark.sql.KylinSession._",
          "32: import org.apache.spark.sql.catalyst.parser.ParseException",
          "33: import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan",
          "36: import org.apache.spark.util.Utils",
          "38: import org.apache.spark.{SparkConf, SparkContext, SparkEnv}",
          "40: import java.lang.{Boolean => JBoolean, String => JString}",
          "41: import java.nio.file.Paths",
          "42: import java.util.concurrent.atomic.AtomicReference",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "192:                 case _ =>",
          "193:                   master_app_url = YarnInfoFetcherUtils.getTrackingUrl(appid)",
          "194:               }",
          "195:             } catch {",
          "196:               case throwable: Throwable =>",
          "197:                 logError(\"Error for initializing spark \", throwable)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "195:               kylinConf.getHdfsWorkingDirectoryInternal(spark.sparkContext.hadoopConfiguration)",
          "",
          "---------------"
        ]
      }
    }
  ]
}