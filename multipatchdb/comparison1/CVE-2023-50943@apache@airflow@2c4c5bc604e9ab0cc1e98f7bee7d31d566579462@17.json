{
  "cve_id": "CVE-2023-50943",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows a potential attacker to poison the XCom data by bypassing the protection of \"enable_xcom_pickling=False\" configuration setting resulting in poisoned data after XCom deserialization. This vulnerability is considered low since it requires a DAG author to exploit it. Users are recommended to upgrade to version 2.8.1 or later, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
  "patch_info": {
    "commit_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "files": [
      "airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py"
    ],
    "message": "Stop deserializing pickle when enable_xcom_pickling is False (#36255)\n\n* Stop deserializing pickle when enable_xcom_pickling is False\n\n* Fix unit tests\n\n(cherry picked from commit 63e97abec5d56bc62a293c93f5227f364561e51c)",
    "before_after_code_files": [
      "airflow/models/xcom.py||airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py||tests/models/test_xcom.py"
    ]
  },
  "patch_diff": {
    "airflow/models/xcom.py||airflow/models/xcom.py": [
      "File: airflow/models/xcom.py -> airflow/models/xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "685:             except pickle.UnpicklingError:",
      "686:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "687:         else:",
      "693:     @staticmethod",
      "694:     def deserialize_value(result: XCom) -> Any:",
      "",
      "[Removed Lines]",
      "688:             try:",
      "689:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "690:             except (json.JSONDecodeError, UnicodeDecodeError):",
      "691:                 return pickle.loads(result.value)",
      "",
      "[Added Lines]",
      "688:             # Since xcom_pickling is disabled, we should only try to deserialize with JSON",
      "689:             return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "",
      "---------------"
    ],
    "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py": [
      "File: tests/api_connexion/schemas/test_xcom_schema.py -> tests/api_connexion/schemas/test_xcom_schema.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from airflow.models import DagRun, XCom",
      "31: from airflow.utils.dates import parse_execution_date",
      "32: from airflow.utils.session import create_session",
      "34: pytestmark = pytest.mark.db_test",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "188:     default_time = \"2016-04-02T21:00:00+00:00\"",
      "189:     default_time_parsed = parse_execution_date(default_time)",
      "191:     def test_serialize(self, create_xcom, session):",
      "192:         create_xcom(",
      "193:             dag_id=\"test_dag\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "192:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "208:             \"map_index\": -1,",
      "209:         }",
      "211:     def test_deserialize(self):",
      "212:         xcom_dump = {",
      "213:             \"key\": \"test_key\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "213:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------"
    ],
    "tests/models/test_xcom.py||tests/models/test_xcom.py": [
      "File: tests/models/test_xcom.py -> tests/models/test_xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "140:             ret_value = XCom.get_value(key=\"xcom_test3\", ti_key=ti_key, session=session)",
      "141:         assert ret_value == {\"key\": \"value\"}",
      "144:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"}):",
      "145:             XCom.set(",
      "146:                 key=\"xcom_test3\",",
      "",
      "[Removed Lines]",
      "143:     def test_xcom_deserialize_with_pickle_to_json_switch(self, task_instance, session):",
      "",
      "[Added Lines]",
      "143:     def test_xcom_deserialize_pickle_when_xcom_pickling_is_disabled(self, task_instance, session):",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "151:                 session=session,",
      "152:             )",
      "153:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"False\"}):",
      "163:     @conf_vars({(\"core\", \"xcom_enable_pickling\"): \"False\"})",
      "164:     def test_xcom_disable_pickle_type_fail_on_non_json(self, task_instance, session):",
      "",
      "[Removed Lines]",
      "154:             ret_value = XCom.get_one(",
      "155:                 key=\"xcom_test3\",",
      "156:                 dag_id=task_instance.dag_id,",
      "157:                 task_id=task_instance.task_id,",
      "158:                 run_id=task_instance.run_id,",
      "159:                 session=session,",
      "160:             )",
      "161:         assert ret_value == {\"key\": \"value\"}",
      "",
      "[Added Lines]",
      "154:             with pytest.raises(UnicodeDecodeError):",
      "155:                 XCom.get_one(",
      "156:                     key=\"xcom_test3\",",
      "157:                     dag_id=task_instance.dag_id,",
      "158:                     task_id=task_instance.task_id,",
      "159:                     run_id=task_instance.run_id,",
      "160:                     session=session,",
      "161:                 )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "808ed02476385c8c670ae3e64c70f19954496075",
      "candidate_info": {
        "commit_hash": "808ed02476385c8c670ae3e64c70f19954496075",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/808ed02476385c8c670ae3e64c70f19954496075",
        "files": [
          "airflow/utils/task_group.py",
          "tests/utils/test_task_group.py"
        ],
        "message": "Fix get_leaves calculation for teardown in nested group (#36456)\n\nWhen arrowing `group` >> `task`, the \"leaves\" of `group` are connected to `task`. When calculating leaves in the group, teardown tasks are ignored, and we recurse upstream to find non-teardowns.\n\nWhat was happening, and what this fixes, is you might recurse to a work task that already has another non-teardown downstream in the group.  In that case you should ignore the work task (because it already has a non-teardown descendent).\n\nResolves #36345\n\n(cherry picked from commit 949fc5788ec7a7a1e4f6bc850d2615ec0f79a57d)",
        "before_after_code_files": [
          "airflow/utils/task_group.py||airflow/utils/task_group.py",
          "tests/utils/test_task_group.py||tests/utils/test_task_group.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/utils/task_group.py||airflow/utils/task_group.py": [
          "File: airflow/utils/task_group.py -> airflow/utils/task_group.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "371:         tasks = list(self)",
          "372:         ids = {x.task_id for x in tasks}",
          "374:         def recurse_for_first_non_teardown(task):",
          "375:             for upstream_task in task.upstream_list:",
          "376:                 if upstream_task.task_id not in ids:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "374:         def has_non_teardown_downstream(task, exclude: str):",
          "375:             for down_task in task.downstream_list:",
          "376:                 if down_task.task_id == exclude:",
          "377:                     continue",
          "378:                 elif down_task.task_id not in ids:",
          "379:                     continue",
          "380:                 elif not down_task.is_teardown:",
          "381:                     return True",
          "382:             return False",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "381:                 elif task.is_teardown and upstream_task.is_setup:",
          "382:                     # don't go through the teardown-to-setup path",
          "383:                     continue",
          "385:                     yield upstream_task",
          "387:         for task in tasks:",
          "",
          "[Removed Lines]",
          "384:                 else:",
          "",
          "[Added Lines]",
          "394:                 # return unless upstream task already has non-teardown downstream in group",
          "395:                 elif not has_non_teardown_downstream(upstream_task, exclude=task.task_id):",
          "",
          "---------------"
        ],
        "tests/utils/test_task_group.py||tests/utils/test_task_group.py": [
          "File: tests/utils/test_task_group.py -> tests/utils/test_task_group.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "584:     ]",
          "587: def test_duplicate_group_id():",
          "588:     from airflow.exceptions import DuplicateTaskIdFound",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "587: def test_dag_edges_setup_teardown_nested():",
          "588:     from airflow.decorators import task, task_group",
          "589:     from airflow.models.dag import DAG",
          "590:     from airflow.operators.empty import EmptyOperator",
          "592:     execution_date = pendulum.parse(\"20200101\")",
          "594:     with DAG(dag_id=\"s_t_dag\", start_date=execution_date) as dag:",
          "596:         @task",
          "597:         def test_task():",
          "598:             print(\"Hello world!\")",
          "600:         @task_group",
          "601:         def inner():",
          "602:             inner_start = EmptyOperator(task_id=\"start\")",
          "603:             inner_end = EmptyOperator(task_id=\"end\")",
          "605:             test_task_r = test_task.override(task_id=\"work\")()",
          "606:             inner_start >> test_task_r >> inner_end.as_teardown(setups=inner_start)",
          "608:         @task_group",
          "609:         def outer():",
          "610:             outer_work = EmptyOperator(task_id=\"work\")",
          "611:             inner_group = inner()",
          "612:             inner_group >> outer_work",
          "614:         dag_start = EmptyOperator(task_id=\"dag_start\")",
          "615:         dag_end = EmptyOperator(task_id=\"dag_end\")",
          "616:         dag_start >> outer() >> dag_end",
          "618:     edges = dag_edges(dag)",
          "620:     actual = sorted((e[\"source_id\"], e[\"target_id\"], e.get(\"is_setup_teardown\")) for e in edges)",
          "621:     assert actual == [",
          "622:         (\"dag_start\", \"outer.upstream_join_id\", None),",
          "623:         (\"outer.downstream_join_id\", \"dag_end\", None),",
          "624:         (\"outer.inner.downstream_join_id\", \"outer.work\", None),",
          "625:         (\"outer.inner.start\", \"outer.inner.end\", True),",
          "626:         (\"outer.inner.start\", \"outer.inner.work\", None),",
          "627:         (\"outer.inner.work\", \"outer.inner.downstream_join_id\", None),",
          "628:         (\"outer.inner.work\", \"outer.inner.end\", None),",
          "629:         (\"outer.upstream_join_id\", \"outer.inner.start\", None),",
          "630:         (\"outer.work\", \"outer.downstream_join_id\", None),",
          "631:     ]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ce43d4abeaf8b08dfe7c9022cea46a71efaa6a78",
      "candidate_info": {
        "commit_hash": "ce43d4abeaf8b08dfe7c9022cea46a71efaa6a78",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ce43d4abeaf8b08dfe7c9022cea46a71efaa6a78",
        "files": [
          "dev/breeze/src/airflow_breeze/utils/path_utils.py",
          "dev/breeze/tests/test_find_airflow_directory.py"
        ],
        "message": "Change detection mechanism for Breeze self-upgrade (#36635)\n\nBreeze auto-detects if it should upgrade itself - based on\nfinding Airflow directory it is in and calculating the hash of\nthe pyproject.toml it uses. Finding the airflow sources to\nact on was using setup.cfg from Airflow and checking the package\nname inside, but since we are about to remove setup.cfg, and\nmove all project configuration to pyproject.toml (see #36537), this\nmechanism will stop working.\n\nThis PR changes it by just checking if `airflow` subdir is present,\nand contains `__init__.py` with \"airflow\" inside. That should be\n\"good enough\" and fast, and also it should be backwards compatible\nin case new Breeze is used in older airflow sources.\n\n(cherry picked from commit c5de0db05e6ec5aff135d03de63f4683b3141a95)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/utils/path_utils.py||dev/breeze/src/airflow_breeze/utils/path_utils.py",
          "dev/breeze/tests/test_find_airflow_directory.py||dev/breeze/tests/test_find_airflow_directory.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/utils/path_utils.py||dev/breeze/src/airflow_breeze/utils/path_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/path_utils.py -> dev/breeze/src/airflow_breeze/utils/path_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "34: from airflow_breeze.utils.reinstall import reinstall_breeze, warn_dependencies_changed, warn_non_editable",
          "35: from airflow_breeze.utils.shared_options import get_verbose, set_forced_answer",
          "40: def search_upwards_for_airflow_sources_root(start_from: Path) -> Path | None:",
          "41:     root = Path(start_from.root)",
          "42:     d = start_from",
          "43:     while d != root:",
          "47:         d = d.parent",
          "48:     return None",
          "",
          "[Removed Lines]",
          "37: AIRFLOW_CFG_FILE = \"setup.cfg\"",
          "44:         attempt = d / AIRFLOW_CFG_FILE",
          "45:         if attempt.exists() and \"name = apache-airflow\\n\" in attempt.read_text():",
          "46:             return attempt.parent",
          "",
          "[Added Lines]",
          "37: PYPROJECT_TOML_FILE = \"pyproject.toml\"",
          "44:         airflow_candidate = d / \"airflow\"",
          "45:         airflow_candidate_init_py = airflow_candidate / \"__init__.py\"",
          "46:         if (",
          "47:             airflow_candidate.is_dir()",
          "48:             and airflow_candidate_init_py.is_file()",
          "49:             and \"airflow\" in airflow_candidate_init_py.read_text().lower()",
          "50:         ):",
          "51:             return airflow_candidate.parent",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "97:     return \"NOT FOUND\"",
          "101:     try:",
          "102:         the_hash = hashlib.new(\"blake2b\")",
          "104:         return the_hash.hexdigest()",
          "105:     except FileNotFoundError as e:",
          "106:         return f\"Missing file {e.filename}\"",
          "",
          "[Removed Lines]",
          "100: def get_sources_setup_metadata_hash(sources: Path) -> str:",
          "103:         the_hash.update((sources / \"dev\" / \"breeze\" / \"pyproject.toml\").read_bytes())",
          "",
          "[Added Lines]",
          "105: def get_pyproject_toml_hash(sources: Path) -> str:",
          "108:         the_hash.update((sources / \"dev\" / \"breeze\" / PYPROJECT_TOML_FILE).read_bytes())",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "109: def get_installation_sources_config_metadata_hash() -> str:",
          "110:     \"\"\"",
          "113:     This is used in order to determine if we need to upgrade Breeze, because some",
          "114:     setup files changed. Blake2b algorithm will not be flagged by security checkers",
          "",
          "[Removed Lines]",
          "111:     Retrieves hash of setup.py and setup.cfg files from the source of installation of Breeze.",
          "",
          "[Added Lines]",
          "116:     Retrieves hash of pyproject.toml from the source of installation of Breeze.",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "118:     installation_sources = get_installation_airflow_sources()",
          "119:     if installation_sources is None:",
          "120:         return \"NOT FOUND\"",
          "124: def get_used_sources_setup_metadata_hash() -> str:",
          "125:     \"\"\"",
          "126:     Retrieves hash of setup files from the currently used sources.",
          "127:     \"\"\"",
          "131: def set_forced_answer_for_upgrade_check():",
          "",
          "[Removed Lines]",
          "121:     return get_sources_setup_metadata_hash(installation_sources)",
          "128:     return get_sources_setup_metadata_hash(get_used_airflow_sources())",
          "",
          "[Added Lines]",
          "126:     return get_pyproject_toml_hash(installation_sources)",
          "133:     return get_pyproject_toml_hash(get_used_airflow_sources())",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_find_airflow_directory.py||dev/breeze/tests/test_find_airflow_directory.py": [
          "File: dev/breeze/tests/test_find_airflow_directory.py -> dev/breeze/tests/test_find_airflow_directory.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "42:     assert output == \"\"",
          "46: @mock.patch(\"airflow_breeze.utils.path_utils.Path.cwd\")",
          "47: def test_find_airflow_root_from_installation_dir(mock_cwd, capsys):",
          "48:     mock_cwd.return_value = ROOT_PATH",
          "",
          "[Removed Lines]",
          "45: @mock.patch(\"airflow_breeze.utils.path_utils.AIRFLOW_CFG_FILE\", \"bad_name.cfg\")",
          "",
          "[Added Lines]",
          "45: @mock.patch(\"airflow_breeze.utils.path_utils.PYPROJECT_TOML_FILE\", \"bad_name.toml\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4bad5e4f95990c918e7138c83e900200524ec61f",
      "candidate_info": {
        "commit_hash": "4bad5e4f95990c918e7138c83e900200524ec61f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4bad5e4f95990c918e7138c83e900200524ec61f",
        "files": [
          "airflow/providers/apache/hdfs/provider.yaml",
          "airflow/providers/apache/kylin/provider.yaml",
          "dev/breeze/src/airflow_breeze/breeze.py"
        ],
        "message": "Fixes small issues related to suspended/removed providers (#36501)\n\nAfter speeding up breeze in #36499 there are few small fixes needed\nfor suspended/removed providers.",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/breeze.py||dev/breeze/src/airflow_breeze/breeze.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/breeze.py||dev/breeze/src/airflow_breeze/breeze.py": [
          "File: dev/breeze/src/airflow_breeze/breeze.py -> dev/breeze/src/airflow_breeze/breeze.py"
        ]
      }
    },
    {
      "candidate_hash": "d79f7a4027fd9b252005fa98f4a31d171f65c45b",
      "candidate_info": {
        "commit_hash": "d79f7a4027fd9b252005fa98f4a31d171f65c45b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/d79f7a4027fd9b252005fa98f4a31d171f65c45b",
        "files": [
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "dev/breeze/tests/test_selective_checks.py"
        ],
        "message": "Fix problems with missing selective checks on new types of unit tests (#36372)\n\nWhen the DB/NonDB tests were introduced (#35160) new test types have\nbeen added (separating various Python test types from generic\nOperator test type). However we have not added matching of the python\noperator and test files into the right selective unit test type. This\ncaused that when only `operators/python.py` and `tests/test_python` were\nchanged, then `Operators` test type was run but the specific Python *\ntest types were not run.\n\nThis PR fixes it for current test type (including also separated\nSerialization test type) and for the future - instead of matching\nselected test type we match all of them except the few that we\nnow are \"special\" (\"Always, Core, Other, PlainAsserts\").\n\n(cherry picked from commit b0db1f94ede7d316b3f63a924176e5e1eefa89c1)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "dev/breeze/tests/test_selective_checks.py||dev/breeze/tests/test_selective_checks.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py": [
          "File: dev/breeze/src/airflow_breeze/utils/selective_checks.py -> dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "223:     }",
          "224: )",
          "226: TEST_TYPE_MATCHES = HashableDict(",
          "227:     {",
          "228:         SelectiveUnitTestTypes.API: [",
          "233:         ],",
          "234:         SelectiveUnitTestTypes.CLI: [",
          "237:         ],",
          "238:         SelectiveUnitTestTypes.OPERATORS: [",
          "241:         ],",
          "242:         SelectiveUnitTestTypes.PROVIDERS: [",
          "243:             r\"^airflow/providers/\",",
          "244:             r\"^tests/system/providers/\",",
          "245:             r\"^tests/providers/\",",
          "246:         ],",
          "252:         ],",
          "253:         SelectiveUnitTestTypes.WWW: [r\"^airflow/www\", r\"^tests/www\"],",
          "254:     }",
          "255: )",
          "",
          "[Removed Lines]",
          "229:             r\"^airflow/api\",",
          "230:             r\"^airflow/api_connexion\",",
          "231:             r\"^tests/api\",",
          "232:             r\"^tests/api_connexion\",",
          "235:             r\"^airflow/cli\",",
          "236:             r\"^tests/cli\",",
          "239:             r\"^airflow/operators\",",
          "240:             r\"^tests/operators\",",
          "247:         SelectiveUnitTestTypes.PYTHON_VENV: [",
          "248:             r\"^tests/operators/test_python.py\",",
          "249:         ],",
          "250:         SelectiveUnitTestTypes.BRANCH_PYTHON_VENV: [",
          "251:             r\"^tests/operators/test_python.py\",",
          "",
          "[Added Lines]",
          "226: PYTHON_OPERATOR_FILES = [",
          "227:     r\"^airflow/operators/python.py\",",
          "228:     r\"^tests/operators/test_python.py\",",
          "229: ]",
          "234:             r\"^airflow/api/\",",
          "235:             r\"^airflow/api_connexion/\",",
          "236:             r\"^airflow/api_internal/\",",
          "237:             r\"^tests/api/\",",
          "238:             r\"^tests/api_connexion/\",",
          "239:             r\"^tests/api_internal/\",",
          "242:             r\"^airflow/cli/\",",
          "243:             r\"^tests/cli/\",",
          "246:             r\"^airflow/operators/\",",
          "247:             r\"^tests/operators/\",",
          "254:         SelectiveUnitTestTypes.SERIALIZATION: [",
          "255:             r\"^airflow/serialization/\",",
          "256:             r\"^tests/serialization/\",",
          "258:         SelectiveUnitTestTypes.PYTHON_VENV: PYTHON_OPERATOR_FILES,",
          "259:         SelectiveUnitTestTypes.BRANCH_PYTHON_VENV: PYTHON_OPERATOR_FILES,",
          "260:         SelectiveUnitTestTypes.EXTERNAL_PYTHON: PYTHON_OPERATOR_FILES,",
          "261:         SelectiveUnitTestTypes.EXTERNAL_BRANCH_PYTHON: PYTHON_OPERATOR_FILES,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "653:         candidate_test_types: set[str] = {\"Always\"}",
          "654:         matched_files: set[str] = set()",
          "671:         kubernetes_files = self._matching_files(",
          "672:             FileGroupForCi.KUBERNETES_FILES, CI_FILE_GROUP_MATCHES, CI_FILE_GROUP_EXCLUDES",
          "",
          "[Removed Lines]",
          "655:         matched_files.update(",
          "656:             self._select_test_type_if_matching(candidate_test_types, SelectiveUnitTestTypes.WWW)",
          "657:         )",
          "658:         matched_files.update(",
          "659:             self._select_test_type_if_matching(candidate_test_types, SelectiveUnitTestTypes.PROVIDERS)",
          "660:         )",
          "661:         matched_files.update(",
          "662:             self._select_test_type_if_matching(candidate_test_types, SelectiveUnitTestTypes.CLI)",
          "663:         )",
          "664:         matched_files.update(",
          "665:             self._select_test_type_if_matching(candidate_test_types, SelectiveUnitTestTypes.OPERATORS)",
          "666:         )",
          "667:         matched_files.update(",
          "668:             self._select_test_type_if_matching(candidate_test_types, SelectiveUnitTestTypes.API)",
          "669:         )",
          "",
          "[Added Lines]",
          "664:         for test_type in SelectiveUnitTestTypes:",
          "665:             if test_type not in [",
          "666:                 SelectiveUnitTestTypes.ALWAYS,",
          "667:                 SelectiveUnitTestTypes.CORE,",
          "668:                 SelectiveUnitTestTypes.OTHER,",
          "669:                 SelectiveUnitTestTypes.PLAIN_ASSERTS,",
          "670:             ]:",
          "671:                 matched_files.update(self._select_test_type_if_matching(candidate_test_types, test_type))",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_selective_checks.py||dev/breeze/tests/test_selective_checks.py": [
          "File: dev/breeze/tests/test_selective_checks.py -> dev/breeze/tests/test_selective_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "161:                 id=\"Only Operator tests and DOCS should run\",",
          "162:             )",
          "163:         ),",
          "164:         (",
          "165:             pytest.param(",
          "166:                 (",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "164:         (",
          "165:             pytest.param(",
          "166:                 (\"airflow/operators/python.py\",),",
          "167:                 {",
          "168:                     \"affected-providers-list-as-string\": None,",
          "169:                     \"all-python-versions\": \"['3.8']\",",
          "170:                     \"all-python-versions-list-as-string\": \"3.8\",",
          "171:                     \"python-versions\": \"['3.8']\",",
          "172:                     \"python-versions-list-as-string\": \"3.8\",",
          "173:                     \"ci-image-build\": \"true\",",
          "174:                     \"prod-image-build\": \"false\",",
          "175:                     \"needs-helm-tests\": \"false\",",
          "176:                     \"run-tests\": \"true\",",
          "177:                     \"run-amazon-tests\": \"false\",",
          "178:                     \"docs-build\": \"true\",",
          "179:                     \"skip-pre-commits\": \"check-provider-yaml-valid,identity,lint-helm-chart,mypy-dev,\"",
          "180:                     \"mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "181:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "182:                     \"parallel-test-types-list-as-string\": \"Always BranchExternalPython BranchPythonVenv \"",
          "183:                     \"ExternalPython Operators PythonVenv\",",
          "184:                 },",
          "185:                 id=\"Only Python tests\",",
          "186:             )",
          "187:         ),",
          "188:         (",
          "189:             pytest.param(",
          "190:                 (\"airflow/serialization/python.py\",),",
          "191:                 {",
          "192:                     \"affected-providers-list-as-string\": None,",
          "193:                     \"all-python-versions\": \"['3.8']\",",
          "194:                     \"all-python-versions-list-as-string\": \"3.8\",",
          "195:                     \"python-versions\": \"['3.8']\",",
          "196:                     \"python-versions-list-as-string\": \"3.8\",",
          "197:                     \"ci-image-build\": \"true\",",
          "198:                     \"prod-image-build\": \"false\",",
          "199:                     \"needs-helm-tests\": \"false\",",
          "200:                     \"run-tests\": \"true\",",
          "201:                     \"run-amazon-tests\": \"false\",",
          "202:                     \"docs-build\": \"true\",",
          "203:                     \"skip-pre-commits\": \"check-provider-yaml-valid,identity,lint-helm-chart,mypy-dev,\"",
          "204:                     \"mypy-docs,mypy-providers,ts-compile-format-lint-www\",",
          "205:                     \"upgrade-to-newer-dependencies\": \"false\",",
          "206:                     \"parallel-test-types-list-as-string\": \"Always Serialization\",",
          "207:                 },",
          "208:                 id=\"Only Serialization tests\",",
          "209:             )",
          "210:         ),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bf935545adbfeb24fe9449b21316c9c4024767bc",
      "candidate_info": {
        "commit_hash": "bf935545adbfeb24fe9449b21316c9c4024767bc",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/bf935545adbfeb24fe9449b21316c9c4024767bc",
        "files": [
          "airflow/decorators/base.py",
          "tests/decorators/test_python.py"
        ],
        "message": "Fix check on subclass for `typing.Union` in `_infer_multiple_outputs` for Python 3.10+ (#36728)\n\n* Fix check on subclass for `typing.Union` in `_infer_multiple_outputs` for Python 3.10+\n\n* Limit PEP 604 test by Python 3.10\n\n(cherry picked from commit f1d82971053287c27c83e1b945b774a6a37a8552)",
        "before_after_code_files": [
          "airflow/decorators/base.py||airflow/decorators/base.py",
          "tests/decorators/test_python.py||tests/decorators/test_python.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/decorators/base.py||airflow/decorators/base.py": [
          "File: airflow/decorators/base.py -> airflow/decorators/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "350:         except TypeError:  # Can't evaluate return type.",
          "351:             return False",
          "352:         ttype = getattr(return_type, \"__origin__\", return_type)",
          "355:     def __attrs_post_init__(self):",
          "356:         if \"self\" in self.function_signature.parameters:",
          "",
          "[Removed Lines]",
          "353:         return issubclass(ttype, Mapping)",
          "",
          "[Added Lines]",
          "353:         return isinstance(ttype, type) and issubclass(ttype, Mapping)",
          "",
          "---------------"
        ],
        "tests/decorators/test_python.py||tests/decorators/test_python.py": [
          "File: tests/decorators/test_python.py -> tests/decorators/test_python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "98:         assert identity_dict_with_decorator_call(5, 5).operator.multiple_outputs is True",
          "101:     def test_infer_multiple_outputs_typed_dict(self):",
          "102:         from typing import TypedDict",
          "",
          "[Removed Lines]",
          "100:     @pytest.mark.skipif(sys.version_info < (3, 8), reason=\"PEP 589 is implemented in Python 3.8\")",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "111:         assert t1().operator.multiple_outputs is True",
          "113:     def test_infer_multiple_outputs_forward_annotation(self):",
          "114:         if TYPE_CHECKING:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "112:     # We do not enable `from __future__ import annotations` for particular this test module,",
          "113:     # that mean `str | None` annotation would raise TypeError in Python 3.9 and below",
          "114:     @pytest.mark.skipif(sys.version_info < (3, 10), reason=\"PEP 604 is implemented in Python 3.10\")",
          "115:     def test_infer_multiple_outputs_pep_604_union_type(self):",
          "116:         @task_decorator",
          "117:         def t1() -> str | None:",
          "118:             # Before PEP 604 which are implemented in Python 3.10 `str | None`",
          "119:             # returns `types.UnionType` which are class and could be check in `issubclass()`.",
          "120:             # However in Python 3.10+ this construction returns object `typing.Union`",
          "121:             # which can not be used in `issubclass()`",
          "122:             return \"foo\"",
          "124:         assert t1().operator.multiple_outputs is False",
          "126:     def test_infer_multiple_outputs_union_type(self):",
          "127:         @task_decorator",
          "128:         def t1() -> Union[str, None]:",
          "129:             return \"foo\"",
          "131:         assert t1().operator.multiple_outputs is False",
          "",
          "---------------"
        ]
      }
    }
  ]
}