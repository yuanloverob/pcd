{
  "cve_id": "CVE-2023-50944",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows an authenticated user to access the source code of a DAG to which they don't have access.\u00a0This vulnerability is considered low since it requires an authenticated user to exploit it. Users are recommended to upgrade to version 2.8.1, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "8d76538d6e105947272b000581c6fabec20146b1",
  "patch_info": {
    "commit_hash": "8d76538d6e105947272b000581c6fabec20146b1",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/8d76538d6e105947272b000581c6fabec20146b1",
    "files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ],
    "message": "Check DAG read permission before accessing DAG code (#36257)\n\n(cherry picked from commit 30ea37e0d247ce54c2d25b115e807fdb0074d795)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "airflow/models/dagcode.py||airflow/models/dagcode.py",
      "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/dag_source_endpoint.py||airflow/api_connexion/endpoints/dag_source_endpoint.py": [
      "File: airflow/api_connexion/endpoints/dag_source_endpoint.py -> airflow/api_connexion/endpoints/dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "17: from __future__ import annotations",
      "19: from http import HTTPStatus",
      "21: from flask import Response, current_app, request",
      "22: from itsdangerous import BadSignature, URLSafeSerializer",
      "24: from airflow.api_connexion import security",
      "26: from airflow.api_connexion.schemas.dag_source_schema import dag_source_schema",
      "27: from airflow.auth.managers.models.resource_details import DagAccessEntity",
      "28: from airflow.models.dagcode import DagCode",
      "31: @security.requires_access_dag(\"GET\", DagAccessEntity.CODE)",
      "33:     \"\"\"Get source code using file token.\"\"\"",
      "34:     secret_key = current_app.config[\"SECRET_KEY\"]",
      "35:     auth_s = URLSafeSerializer(secret_key)",
      "36:     try:",
      "37:         path = auth_s.loads(file_token)",
      "39:     except (BadSignature, FileNotFoundError):",
      "40:         raise NotFound(\"Dag source not found\")",
      "",
      "[Removed Lines]",
      "25: from airflow.api_connexion.exceptions import NotFound",
      "32: def get_dag_source(*, file_token: str) -> Response:",
      "38:         dag_source = DagCode.code(path)",
      "",
      "[Added Lines]",
      "20: from typing import TYPE_CHECKING",
      "26: from airflow.api_connexion.exceptions import NotFound, PermissionDenied",
      "28: from airflow.api_connexion.security import get_readable_dags",
      "30: from airflow.models.dag import DagModel",
      "32: from airflow.utils.session import NEW_SESSION, provide_session",
      "34: if TYPE_CHECKING:",
      "35:     from sqlalchemy.orm import Session",
      "39: @provide_session",
      "40: def get_dag_source(*, file_token: str, session: Session = NEW_SESSION) -> Response:",
      "46:         dag_ids = session.query(DagModel.dag_id).filter(DagModel.fileloc == path).all()",
      "47:         readable_dags = get_readable_dags()",
      "48:         # Check if user has read access to all the DAGs defined in the file",
      "49:         if any(dag_id[0] not in readable_dags for dag_id in dag_ids):",
      "50:             raise PermissionDenied()",
      "51:         dag_source = DagCode.code(path, session=session)",
      "",
      "---------------"
    ],
    "airflow/models/dagcode.py||airflow/models/dagcode.py": [
      "File: airflow/models/dagcode.py -> airflow/models/dagcode.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "177:         return cls.code(fileloc)",
      "179:     @classmethod",
      "181:         \"\"\"Return source code for this DagCode object.",
      "183:         :return: source code as string",
      "184:         \"\"\"",
      "187:     @staticmethod",
      "188:     def _get_code_from_file(fileloc):",
      "",
      "[Removed Lines]",
      "180:     def code(cls, fileloc) -> str:",
      "185:         return cls._get_code_from_db(fileloc)",
      "",
      "[Added Lines]",
      "180:     @provide_session",
      "181:     def code(cls, fileloc, session: Session = NEW_SESSION) -> str:",
      "186:         return cls._get_code_from_db(fileloc, session)",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_dag_source_endpoint.py||tests/api_connexion/endpoints/test_dag_source_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_dag_source_endpoint.py -> tests/api_connexion/endpoints/test_dag_source_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "35: ROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))",
      "36: EXAMPLE_DAG_FILE = os.path.join(\"airflow\", \"example_dags\", \"example_bash_operator.py\")",
      "39: @pytest.fixture(scope=\"module\")",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "37: EXAMPLE_DAG_ID = \"example_bash_operator\"",
      "38: TEST_DAG_ID = \"latest_only\"",
      "39: NOT_READABLE_DAG_ID = \"latest_only_with_trigger\"",
      "40: TEST_MULTIPLE_DAGS_ID = \"dataset_produces_1\"",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "45:         role_name=\"Test\",",
      "46:         permissions=[(permissions.ACTION_CAN_READ, permissions.RESOURCE_DAG_CODE)],  # type: ignore",
      "47:     )",
      "48:     create_user(app, username=\"test_no_permissions\", role_name=\"TestNoPermissions\")  # type: ignore",
      "50:     yield app",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "52:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "53:         TEST_DAG_ID,",
      "54:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "55:     )",
      "56:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "57:         EXAMPLE_DAG_ID,",
      "58:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "59:     )",
      "60:     app.appbuilder.sm.sync_perm_for_dag(  # type: ignore",
      "61:         TEST_MULTIPLE_DAGS_ID,",
      "62:         access_control={\"Test\": [permissions.ACTION_CAN_READ]},",
      "63:     )",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "80:     def test_should_respond_200_text(self, url_safe_serializer):",
      "81:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "82:         dagbag.sync_to_db()",
      "87:         response = self.client.get(",
      "88:             url, headers={\"Accept\": \"text/plain\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "89:         )",
      "",
      "[Removed Lines]",
      "83:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "84:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "86:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "99:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "100:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "102:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 4 ---",
      "[Context before]",
      "95:     def test_should_respond_200_json(self, url_safe_serializer):",
      "96:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "97:         dagbag.sync_to_db()",
      "102:         response = self.client.get(",
      "103:             url, headers={\"Accept\": \"application/json\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "104:         )",
      "",
      "[Removed Lines]",
      "98:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "99:         dag_docstring = self._get_dag_file_docstring(first_dag.fileloc)",
      "101:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "114:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "115:         dag_docstring = self._get_dag_file_docstring(test_dag.fileloc)",
      "117:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 5 ---",
      "[Context before]",
      "110:     def test_should_respond_406(self, url_safe_serializer):",
      "111:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "112:         dagbag.sync_to_db()",
      "116:         response = self.client.get(",
      "117:             url, headers={\"Accept\": \"image/webp\"}, environ_overrides={\"REMOTE_USER\": \"test\"}",
      "118:         )",
      "",
      "[Removed Lines]",
      "113:         first_dag: DAG = next(iter(dagbag.dags.values()))",
      "115:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(first_dag.fileloc)}\"",
      "",
      "[Added Lines]",
      "129:         test_dag: DAG = dagbag.dags[TEST_DAG_ID]",
      "131:         url = f\"/api/v1/dagSources/{url_safe_serializer.dumps(test_dag.fileloc)}\"",
      "",
      "---------------",
      "--- Hunk 6 ---",
      "[Context before]",
      "151:             environ_overrides={\"REMOTE_USER\": \"test_no_permissions\"},",
      "152:         )",
      "153:         assert response.status_code == 403",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "171:     def test_should_respond_403_not_readable(self, url_safe_serializer):",
      "172:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "173:         dagbag.sync_to_db()",
      "174:         dag: DAG = dagbag.dags[NOT_READABLE_DAG_ID]",
      "176:         response = self.client.get(",
      "177:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "178:             headers={\"Accept\": \"text/plain\"},",
      "179:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "180:         )",
      "181:         read_dag = self.client.get(",
      "182:             f\"/api/v1/dags/{NOT_READABLE_DAG_ID}\",",
      "183:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "184:         )",
      "185:         assert response.status_code == 403",
      "186:         assert read_dag.status_code == 403",
      "188:     def test_should_respond_403_some_dags_not_readable_in_the_file(self, url_safe_serializer):",
      "189:         dagbag = DagBag(dag_folder=EXAMPLE_DAG_FILE)",
      "190:         dagbag.sync_to_db()",
      "191:         dag: DAG = dagbag.dags[TEST_MULTIPLE_DAGS_ID]",
      "193:         response = self.client.get(",
      "194:             f\"/api/v1/dagSources/{url_safe_serializer.dumps(dag.fileloc)}\",",
      "195:             headers={\"Accept\": \"text/plain\"},",
      "196:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "197:         )",
      "199:         read_dag = self.client.get(",
      "200:             f\"/api/v1/dags/{TEST_MULTIPLE_DAGS_ID}\",",
      "201:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "202:         )",
      "203:         assert response.status_code == 403",
      "204:         assert read_dag.status_code == 200",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "010bb695b8d3157f937e489ce6a20935b801b9d9",
      "candidate_info": {
        "commit_hash": "010bb695b8d3157f937e489ce6a20935b801b9d9",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/010bb695b8d3157f937e489ce6a20935b801b9d9",
        "files": [
          ".pre-commit-config.yaml",
          "STATIC_CODE_CHECKS.rst",
          "airflow/contrib/operators/__init__.py",
          "airflow/contrib/secrets/__init__.py",
          "airflow/contrib/sensors/__init__.py",
          "airflow/contrib/utils/__init__.py",
          "pyproject.toml",
          "scripts/ci/pre_commit/pre_commit_ruff_format.py",
          "setup.py"
        ],
        "message": "Upgrade to latest ruff and remove ISC001 warning from output (#36649)\n\nThis PR upgrades to latest ruff, and removes the ISC001 warning that\nwarns us against potential conflict between ruff and ruff-formatter\nwhen two strings in one line get concatenated.\n\nThis warning makes sense if you run both ruff and formatting at the\nsame time, but in our case we are doing it in two separate\nsteps - one step is to run ruff linting and the second step is to\nrun formatting and running formatting already runs after linting\nis complete.\n\nThis warnign is pretty misleading as it distracts from real formatting\nissues you might have.\n\nThere is - unfortunately - no standard way to remove the warning\nso we have to do it a little \"around\" - rather than running\nthe pre-commit directly from ruff website, we run our local pre-commit\nwith few lines of Python code that runs ruff through shell and\ngreps out the ISC001 warning. We also force color to make\nsure the output is still coloured.\n\n(cherry picked from commit 11c46fd2ec165da32202b464be7c2df5cca4d6c0)",
        "before_after_code_files": [
          "airflow/contrib/operators/__init__.py||airflow/contrioperators/__init__.py",
          "airflow/contrib/secrets/__init__.py||airflow/contrisecrets/__init__.py",
          "airflow/contrib/sensors/__init__.py||airflow/contrisensors/__init__.py",
          "airflow/contrib/utils/__init__.py||airflow/contriutils/__init__.py",
          "scripts/ci/pre_commit/pre_commit_ruff_format.py||scripts/ci/pre_commit/pre_commit_ruff_format.py",
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/contrib/operators/__init__.py||airflow/contrioperators/__init__.py": [
          "File: airflow/contrib/operators/__init__.py -> airflow/contrioperators/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "180:         \"DataprocDeleteClusterOperator\": (",
          "181:             \"airflow.providers.google.cloud.operators.dataproc.DataprocDeleteClusterOperator\"",
          "182:         ),",
          "186:         \"DataprocInstantiateWorkflowTemplateOperator\": (",
          "187:             \"airflow.providers.google.cloud.operators.dataproc.DataprocInstantiateWorkflowTemplateOperator\"",
          "188:         ),",
          "",
          "[Removed Lines]",
          "183:         \"DataprocInstantiateInlineWorkflowTemplateOperator\":",
          "184:             \"airflow.providers.google.cloud.operators.dataproc.\"",
          "185:             \"DataprocInstantiateInlineWorkflowTemplateOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "240:         \"DataProcSparkSqlOperator\": (",
          "241:             \"airflow.providers.google.cloud.operators.dataproc.DataprocSubmitSparkSqlJobOperator\"",
          "242:         ),",
          "246:         \"DataprocWorkflowTemplateInstantiateOperator\": (",
          "247:             \"airflow.providers.google.cloud.operators.dataproc.DataprocInstantiateWorkflowTemplateOperator\"",
          "248:         ),",
          "",
          "[Removed Lines]",
          "243:         \"DataprocWorkflowTemplateInstantiateInlineOperator\":",
          "244:             \"airflow.providers.google.cloud.operators.dataproc.\"",
          "245:             \"DataprocInstantiateInlineWorkflowTemplateOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "351:         \"ComputeEngineCopyInstanceTemplateOperator\": (",
          "352:             \"airflow.providers.google.cloud.operators.compute.ComputeEngineCopyInstanceTemplateOperator\"",
          "353:         ),",
          "357:         \"ComputeEngineSetMachineTypeOperator\": (",
          "358:             \"airflow.providers.google.cloud.operators.compute.ComputeEngineSetMachineTypeOperator\"",
          "359:         ),",
          "",
          "[Removed Lines]",
          "354:         \"ComputeEngineInstanceGroupUpdateManagerTemplateOperator\":",
          "355:             \"airflow.providers.google.cloud.operators.compute.\"",
          "356:             \"ComputeEngineInstanceGroupUpdateManagerTemplateOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "364:             \"airflow.providers.google.cloud.operators.compute.ComputeEngineStopInstanceOperator\"",
          "365:         ),",
          "366:         \"GceBaseOperator\": \"airflow.providers.google.cloud.operators.compute.ComputeEngineBaseOperator\",",
          "370:         \"GceInstanceStartOperator\": (",
          "371:             \"airflow.providers.google.cloud.operators.compute.ComputeEngineStartInstanceOperator\"",
          "372:         ),",
          "",
          "[Removed Lines]",
          "367:         \"GceInstanceGroupManagerUpdateTemplateOperator\":",
          "368:             \"airflow.providers.google.cloud.operators.compute.\"",
          "369:             \"ComputeEngineInstanceGroupUpdateManagerTemplateOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "513:         ),",
          "514:     },",
          "515:     \"gcp_natural_language_operator\": {",
          "540:     },",
          "541:     \"gcp_spanner_operator\": {",
          "542:         \"SpannerDeleteDatabaseInstanceOperator\": (",
          "",
          "[Removed Lines]",
          "516:         \"CloudNaturalLanguageAnalyzeEntitiesOperator\":",
          "517:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "518:             \"CloudNaturalLanguageAnalyzeEntitiesOperator\",",
          "519:         \"CloudNaturalLanguageAnalyzeEntitySentimentOperator\":",
          "520:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "521:             \"CloudNaturalLanguageAnalyzeEntitySentimentOperator\",",
          "522:         \"CloudNaturalLanguageAnalyzeSentimentOperator\":",
          "523:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "524:             \"CloudNaturalLanguageAnalyzeSentimentOperator\",",
          "525:         \"CloudNaturalLanguageClassifyTextOperator\":",
          "526:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "527:             \"CloudNaturalLanguageClassifyTextOperator\",",
          "528:         \"CloudLanguageAnalyzeEntitiesOperator\":",
          "529:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "530:             \"CloudNaturalLanguageAnalyzeEntitiesOperator\",",
          "531:         \"CloudLanguageAnalyzeEntitySentimentOperator\":",
          "532:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "533:             \"CloudNaturalLanguageAnalyzeEntitySentimentOperator\",",
          "534:         \"CloudLanguageAnalyzeSentimentOperator\":",
          "535:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "536:             \"CloudNaturalLanguageAnalyzeSentimentOperator\",",
          "537:         \"CloudLanguageClassifyTextOperator\":",
          "538:             \"airflow.providers.google.cloud.operators.natural_language.\"",
          "539:             \"CloudNaturalLanguageClassifyTextOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "692:         ),",
          "693:     },",
          "694:     \"gcp_transfer_operator\": {",
          "755:     },",
          "756:     \"gcp_translate_operator\": {",
          "757:         \"CloudTranslateTextOperator\": (",
          "",
          "[Removed Lines]",
          "695:         \"CloudDataTransferServiceCancelOperationOperator\":",
          "696:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "697:             \"CloudDataTransferServiceCancelOperationOperator\",",
          "698:         \"CloudDataTransferServiceCreateJobOperator\":",
          "699:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "700:             \"CloudDataTransferServiceCreateJobOperator\",",
          "701:         \"CloudDataTransferServiceDeleteJobOperator\":",
          "702:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "703:             \"CloudDataTransferServiceDeleteJobOperator\",",
          "704:         \"CloudDataTransferServiceGCSToGCSOperator\":",
          "705:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "706:             \"CloudDataTransferServiceGCSToGCSOperator\",",
          "707:         \"CloudDataTransferServiceGetOperationOperator\":",
          "708:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "709:             \"CloudDataTransferServiceGetOperationOperator\",",
          "710:         \"CloudDataTransferServiceListOperationsOperator\":",
          "711:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "712:             \"CloudDataTransferServiceListOperationsOperator\",",
          "713:         \"CloudDataTransferServicePauseOperationOperator\":",
          "714:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "715:             \"CloudDataTransferServicePauseOperationOperator\",",
          "716:         \"CloudDataTransferServiceResumeOperationOperator\":",
          "717:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "718:             \"CloudDataTransferServiceResumeOperationOperator\",",
          "719:         \"CloudDataTransferServiceS3ToGCSOperator\":",
          "720:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "721:             \"CloudDataTransferServiceS3ToGCSOperator\",",
          "722:         \"CloudDataTransferServiceUpdateJobOperator\":",
          "723:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "724:             \"CloudDataTransferServiceUpdateJobOperator\",",
          "725:         \"GcpTransferServiceJobCreateOperator\":",
          "726:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "727:             \"CloudDataTransferServiceCreateJobOperator\",",
          "728:         \"GcpTransferServiceJobDeleteOperator\":",
          "729:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "730:             \"CloudDataTransferServiceDeleteJobOperator\",",
          "731:         \"GcpTransferServiceJobUpdateOperator\":",
          "732:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "733:             \"CloudDataTransferServiceUpdateJobOperator\",",
          "734:         \"GcpTransferServiceOperationCancelOperator\":",
          "735:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "736:             \"CloudDataTransferServiceCancelOperationOperator\",",
          "737:         \"GcpTransferServiceOperationGetOperator\":",
          "738:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "739:             \"CloudDataTransferServiceGetOperationOperator\",",
          "740:         \"GcpTransferServiceOperationPauseOperator\":",
          "741:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "742:             \"CloudDataTransferServicePauseOperationOperator\",",
          "743:         \"GcpTransferServiceOperationResumeOperator\":",
          "744:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "745:             \"CloudDataTransferServiceResumeOperationOperator\",",
          "746:         \"GcpTransferServiceOperationsListOperator\":",
          "747:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "748:             \"CloudDataTransferServiceListOperationsOperator\",",
          "749:         \"GoogleCloudStorageToGoogleCloudStorageTransferOperator\":",
          "750:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "751:             \"CloudDataTransferServiceGCSToGCSOperator\",",
          "752:         \"S3ToGoogleCloudStorageTransferOperator\":",
          "753:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "754:             \"CloudDataTransferServiceS3ToGCSOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "767:         ),",
          "768:     },",
          "769:     \"gcp_video_intelligence_operator\": {",
          "779:     },",
          "780:     \"gcp_vision_operator\": {",
          "781:         \"CloudVisionAddProductToProductSetOperator\": (",
          "",
          "[Removed Lines]",
          "770:         \"CloudVideoIntelligenceDetectVideoExplicitContentOperator\":",
          "771:             \"airflow.providers.google.cloud.operators.video_intelligence.\"",
          "772:             \"CloudVideoIntelligenceDetectVideoExplicitContentOperator\",",
          "773:         \"CloudVideoIntelligenceDetectVideoLabelsOperator\":",
          "774:             \"airflow.providers.google.cloud.operators.video_intelligence.\"",
          "775:             \"CloudVideoIntelligenceDetectVideoLabelsOperator\",",
          "776:         \"CloudVideoIntelligenceDetectVideoShotsOperator\":",
          "777:             \"airflow.providers.google.cloud.operators.video_intelligence.\"",
          "778:             \"CloudVideoIntelligenceDetectVideoShotsOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "945:         \"JiraOperator\": \"airflow.providers.atlassian.jira.operators.jira.JiraOperator\",",
          "946:     },",
          "947:     \"kubernetes_pod_operator\": {",
          "951:     },",
          "952:     \"mlengine_operator\": {",
          "953:         \"MLEngineManageModelOperator\": (",
          "",
          "[Removed Lines]",
          "948:         \"KubernetesPodOperator\": (",
          "949:             \"airflow.providers.cncf.kubernetes.operators.pod.KubernetesPodOperator\"",
          "950:         ),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "997:         \"OpsgenieAlertOperator\": \"airflow.providers.opsgenie.operators.opsgenie.OpsgenieCreateAlertOperator\",",
          "998:     },",
          "999:     \"oracle_to_azure_data_lake_transfer\": {",
          "1003:     },",
          "1004:     \"oracle_to_oracle_transfer\": {",
          "1005:         \"OracleToOracleOperator\": (",
          "",
          "[Removed Lines]",
          "1000:         \"OracleToAzureDataLakeOperator\":",
          "1001:             \"airflow.providers.microsoft.azure.transfers.\"",
          "1002:             \"oracle_to_azure_data_lake.OracleToAzureDataLakeOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "1056:         \"S3ToGCSOperator\": \"airflow.providers.google.cloud.transfers.s3_to_gcs.S3ToGCSOperator\",",
          "1057:     },",
          "1058:     \"s3_to_gcs_transfer_operator\": {",
          "1062:     },",
          "1063:     \"s3_to_sftp_operator\": {",
          "1064:         \"S3ToSFTPOperator\": \"airflow.providers.amazon.aws.transfers.s3_to_sftp.S3ToSFTPOperator\",",
          "",
          "[Removed Lines]",
          "1059:         \"CloudDataTransferServiceS3ToGCSOperator\":",
          "1060:             \"airflow.providers.google.cloud.operators.cloud_storage_transfer_service.\"",
          "1061:             \"CloudDataTransferServiceS3ToGCSOperator\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/contrib/secrets/__init__.py||airflow/contrisecrets/__init__.py": [
          "File: airflow/contrib/secrets/__init__.py -> airflow/contrisecrets/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: warnings.warn(",
          "27:     \"This module is deprecated. Please use airflow.providers.*.secrets.\",",
          "28:     RemovedInAirflow3Warning,",
          "30: )",
          "32: __deprecated_classes = {",
          "",
          "[Removed Lines]",
          "29:     stacklevel=2",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/contrib/sensors/__init__.py||airflow/contrisensors/__init__.py": [
          "File: airflow/contrib/sensors/__init__.py -> airflow/contrisensors/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "85:         \"FTPSSensor\": \"airflow.providers.ftp.sensors.ftp.FTPSSensor\",",
          "86:     },",
          "87:     \"gcp_transfer_sensor\": {",
          "94:     },",
          "95:     \"gcs_sensor\": {",
          "96:         \"GCSObjectExistenceSensor\": \"airflow.providers.google.cloud.sensors.gcs.GCSObjectExistenceSensor\",",
          "",
          "[Removed Lines]",
          "88:         \"CloudDataTransferServiceJobStatusSensor\":",
          "89:             \"airflow.providers.google.cloud.sensors.cloud_storage_transfer_service.\"",
          "90:             \"CloudDataTransferServiceJobStatusSensor\",",
          "91:         \"GCPTransferServiceWaitForJobStatusSensor\":",
          "92:             \"airflow.providers.google.cloud.sensors.cloud_storage_transfer_service.\"",
          "93:             \"CloudDataTransferServiceJobStatusSensor\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/contrib/utils/__init__.py||airflow/contriutils/__init__.py": [
          "File: airflow/contrib/utils/__init__.py -> airflow/contriutils/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: from airflow.utils.deprecation_tools import add_deprecated_classes",
          "26: warnings.warn(",
          "30: )",
          "32: __deprecated_classes = {",
          "",
          "[Removed Lines]",
          "27:     \"This module is deprecated. Please use `airflow.utils`.\",",
          "28:     RemovedInAirflow3Warning,",
          "29:     stacklevel=2",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_ruff_format.py||scripts/ci/pre_commit/pre_commit_ruff_format.py": [
          "File: scripts/ci/pre_commit/pre_commit_ruff_format.py -> scripts/ci/pre_commit/pre_commit_ruff_format.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: import os",
          "21: import subprocess",
          "23: ruff_format_cmd = \"ruff format --force-exclude 2>&1 | grep -v '`ISC001`. To avoid unexpected behavior'\"",
          "24: envcopy = os.environ.copy()",
          "25: envcopy[\"CLICOLOR_FORCE\"] = \"1\"",
          "26: subprocess.run(ruff_format_cmd, shell=True, check=True, env=envcopy)",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "466: _devel_only_static_checks = [",
          "467:     \"pre-commit\",",
          "468:     \"black\",",
          "470:     \"yamllint\",",
          "471: ]",
          "",
          "[Removed Lines]",
          "469:     \"ruff>=0.0.219\",",
          "",
          "[Added Lines]",
          "469:     \"ruff==0.1.11\",",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8317580ccf671d344bc45cea151f641800826a23",
      "candidate_info": {
        "commit_hash": "8317580ccf671d344bc45cea151f641800826a23",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8317580ccf671d344bc45cea151f641800826a23",
        "files": [
          "airflow/operators/python.py"
        ],
        "message": "Fix PythonVirtualenvOperator tests (#36367)\n\n(cherry picked from commit af4b51cdfc8f98bec9922facd165ea8a6440c12b)",
        "before_after_code_files": [
          "airflow/operators/python.py||airflow/operators/python.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/operators/python.py||airflow/operators/python.py": [
          "File: airflow/operators/python.py -> airflow/operators/python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "472:                 else:",
          "473:                     raise",
          "475:             return self._read_result(output_path)",
          "477:     def determine_kwargs(self, context: Mapping[str, Any]) -> Mapping[str, Any]:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "475:             if 0 in self.skip_on_exit_code:",
          "476:                 raise AirflowSkipException(\"Process exited with code 0. Skipping.\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4f9aa6132e16cf51e4d5a3c2dd6920323c1b0d72",
      "candidate_info": {
        "commit_hash": "4f9aa6132e16cf51e4d5a3c2dd6920323c1b0d72",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4f9aa6132e16cf51e4d5a3c2dd6920323c1b0d72",
        "files": [
          "airflow/operators/trigger_dagrun.py"
        ],
        "message": "explicit string cast required to force integer-type run_ids to be passed as strings instead of integers (#36756)\n\n(cherry picked from commit e2335a00cea898d83e17b3eb69959656daae883e)",
        "before_after_code_files": [
          "airflow/operators/trigger_dagrun.py||airflow/operators/trigger_dagrun.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/operators/trigger_dagrun.py||airflow/operators/trigger_dagrun.py": [
          "File: airflow/operators/trigger_dagrun.py -> airflow/operators/trigger_dagrun.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "157:             raise AirflowException(\"conf parameter should be JSON Serializable\")",
          "159:         if self.trigger_run_id:",
          "161:         else:",
          "162:             run_id = DagRun.generate_run_id(DagRunType.MANUAL, parsed_execution_date)",
          "",
          "[Removed Lines]",
          "160:             run_id = self.trigger_run_id",
          "",
          "[Added Lines]",
          "160:             run_id = str(self.trigger_run_id)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "38573856e51b8988b8298af3394461a805d79a6a",
      "candidate_info": {
        "commit_hash": "38573856e51b8988b8298af3394461a805d79a6a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/38573856e51b8988b8298af3394461a805d79a6a",
        "files": [
          "airflow/settings.py",
          "tests/core/test_settings.py"
        ],
        "message": "Better error message when sqlite URL uses relative path (#36774)\n\nWhen sqlite URL uses relative path, the error printed is quite\ncryptic:\n\n```\nsqlalchemy.exc.OperationalError: (sqlite3.OperationalError) unable to open database file\n```\n\nThis might easily happen for example when you are in a hurry and put\nrelative value in your AIRFLOW_HOME.\n\nThis PR checks if sql is relative and throws more appropriate and\nexplicit message what is wrong.\n\n(cherry picked from commit 082055e23a38169a613f639296e144234f15d28c)",
        "before_after_code_files": [
          "airflow/settings.py||airflow/settings.py",
          "tests/core/test_settings.py||tests/core/test_settings.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/settings.py||airflow/settings.py": [
          "File: airflow/settings.py -> airflow/settings.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "192:     global PLUGINS_FOLDER",
          "193:     global DONOT_MODIFY_HANDLERS",
          "194:     SQL_ALCHEMY_CONN = conf.get(\"database\", \"SQL_ALCHEMY_CONN\")",
          "195:     DAGS_FOLDER = os.path.expanduser(conf.get(\"core\", \"DAGS_FOLDER\"))",
          "197:     PLUGINS_FOLDER = conf.get(\"core\", \"plugins_folder\", fallback=os.path.join(AIRFLOW_HOME, \"plugins\"))",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "195:     if SQL_ALCHEMY_CONN.startswith(\"sqlite\") and not SQL_ALCHEMY_CONN.startswith(\"sqlite:////\"):",
          "196:         from airflow.exceptions import AirflowConfigException",
          "198:         raise AirflowConfigException(",
          "199:             f\"Cannot use relative path: `{SQL_ALCHEMY_CONN}` to connect to sqlite. \"",
          "200:             \"Please use absolute path such as `sqlite:////tmp/airflow.db`.\"",
          "201:         )",
          "",
          "---------------"
        ],
        "tests/core/test_settings.py||tests/core/test_settings.py": [
          "File: tests/core/test_settings.py -> tests/core/test_settings.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import sys",
          "22: import tempfile",
          "23: from unittest import mock",
          "26: import pytest",
          "29: from tests.test_utils.config import conf_vars",
          "31: SETTINGS_FILE_POLICY = \"\"\"",
          "",
          "[Removed Lines]",
          "24: from unittest.mock import MagicMock, call",
          "28: from airflow.exceptions import AirflowClusterPolicyViolation",
          "",
          "[Added Lines]",
          "24: from unittest.mock import MagicMock, call, patch",
          "28: from airflow.exceptions import AirflowClusterPolicyViolation, AirflowConfigException",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "214:         session_lifetime_config = settings.get_session_lifetime_config()",
          "215:         default_timeout_minutes = 30 * 24 * 60",
          "216:         assert session_lifetime_config == default_timeout_minutes",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "219: def test_sqlite_relative_path():",
          "220:     from airflow import settings",
          "222:     with patch(\"airflow.settings.conf.get\") as conf_get_mock:",
          "223:         conf_get_mock.return_value = \"sqlite:///./relative_path.db\"",
          "224:         with pytest.raises(AirflowConfigException) as exc:",
          "225:             settings.configure_vars()",
          "226:         assert \"Cannot use relative path:\" in str(exc.value)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4d27170e3c5b237abcba1aa6e54d122bcb37efac",
      "candidate_info": {
        "commit_hash": "4d27170e3c5b237abcba1aa6e54d122bcb37efac",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/4d27170e3c5b237abcba1aa6e54d122bcb37efac",
        "files": [
          "airflow/decorators/__init__.pyi"
        ],
        "message": "Make `kubernetes` decorator type annotation consistent with operator (#36405)\n\n(cherry picked from commit 8af63683640358eaad2a9ed8d3a4ea26bfbee29a)",
        "before_after_code_files": [
          "airflow/decorators/__init__.pyi||airflow/decorators/__init__.pyi"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/decorators/__init__.pyi||airflow/decorators/__init__.pyi": [
          "File: airflow/decorators/__init__.pyi -> airflow/decorators/__init__.pyi",
          "--- Hunk 1 ---",
          "[Context before]",
          "461:     def kubernetes(",
          "462:         self,",
          "469:         ports: list[k8s.V1ContainerPort] | None = None,",
          "470:         volume_mounts: list[k8s.V1VolumeMount] | None = None,",
          "471:         volumes: list[k8s.V1Volume] | None = None,",
          "473:         env_from: list[k8s.V1EnvFromSource] | None = None,",
          "474:         secrets: list[Secret] | None = None,",
          "475:         in_cluster: bool | None = None,",
          "476:         cluster_context: str | None = None,",
          "477:         labels: dict | None = None,",
          "480:         get_logs: bool = True,",
          "481:         image_pull_policy: str | None = None,",
          "482:         annotations: dict | None = None,",
          "483:         container_resources: k8s.V1ResourceRequirements | None = None,",
          "484:         affinity: k8s.V1Affinity | None = None,",
          "486:         node_selector: dict | None = None,",
          "487:         image_pull_secrets: list[k8s.V1LocalObjectReference] | None = None,",
          "488:         service_account_name: str | None = None,",
          "490:         hostnetwork: bool = False,",
          "491:         tolerations: list[k8s.V1Toleration] | None = None,",
          "493:         dnspolicy: str | None = None,",
          "494:         schedulername: str | None = None,",
          "495:         init_containers: list[k8s.V1Container] | None = None,",
          "496:         log_events_on_failure: bool = False,",
          "497:         do_xcom_push: bool = False,",
          "498:         pod_template_file: str | None = None,",
          "499:         priority_class_name: str | None = None,",
          "500:         pod_runtime_info_envs: list[k8s.V1EnvVar] | None = None,",
          "501:         termination_grace_period: int | None = None,",
          "502:         configmaps: list[str] | None = None,",
          "504:     ) -> TaskDecorator:",
          "505:         \"\"\"Create a decorator to convert a callable to a Kubernetes Pod task.",
          "507:         :param kubernetes_conn_id: The Kubernetes cluster's",
          "508:             :ref:`connection ID <howto/connection:kubernetes>`.",
          "509:         :param namespace: Namespace to run within Kubernetes. Defaults to *default*.",
          "",
          "[Removed Lines]",
          "464:         image: str,",
          "465:         kubernetes_conn_id: str = ...,",
          "466:         namespace: str = \"default\",",
          "467:         name: str = ...,",
          "468:         random_name_suffix: bool = True,",
          "472:         env_vars: list[k8s.V1EnvVar] | None = None,",
          "478:         reattach_on_restart: bool = True,",
          "479:         startup_timeout_seconds: int = 120,",
          "485:         config_file: str = ...,",
          "489:         is_delete_operator_pod: bool = True,",
          "492:         security_context: dict | None = None,",
          "",
          "[Added Lines]",
          "464:         multiple_outputs: bool | None = None,",
          "465:         use_dill: bool = False,  # Added by _KubernetesDecoratedOperator.",
          "466:         # 'cmds' filled by _KubernetesDecoratedOperator.",
          "467:         kubernetes_conn_id: str | None = ...,",
          "468:         namespace: str | None = None,",
          "469:         image: str | None = None,",
          "470:         name: str | None = None,",
          "471:         random_name_suffix: bool = ...,",
          "472:         arguments: list[str] | None = None,",
          "476:         env_vars: list[k8s.V1EnvVar] | dict[str, str] | None = None,",
          "482:         reattach_on_restart: bool = ...,",
          "483:         startup_timeout_seconds: int = ...,",
          "484:         startup_check_interval_seconds: int = ...,",
          "486:         container_logs: Iterable[str] | str | Literal[True] = ...,",
          "491:         config_file: str | None = None,",
          "496:         host_aliases: list[k8s.V1HostAlias] | None = None,",
          "498:         security_context: k8s.V1PodSecurityContext | dict | None = None,",
          "499:         container_security_context: k8s.V1SecurityContext | dict | None = None,",
          "501:         dns_config: k8s.V1PodDNSConfig | None = None,",
          "502:         hostname: str | None = None,",
          "503:         subdomain: str | None = None,",
          "505:         full_pod_spec: k8s.V1Pod | None = None,",
          "510:         pod_template_dict: dict | None = None,",
          "515:         skip_on_exit_code: int | Container[int] | None = None,",
          "516:         base_container_name: str | None = None,",
          "517:         deferrable: bool = ...,",
          "518:         poll_interval: float = ...,",
          "519:         log_pod_spec_on_failure: bool = ...,",
          "520:         on_finish_action: str = ...,",
          "521:         termination_message_policy: str = ...,",
          "522:         active_deadline_seconds: int | None = None,",
          "523:         progress_callback: Callable[[str], None] | None = None,",
          "528:         :param multiple_outputs: If set, function return value will be unrolled to multiple XCom values.",
          "529:             Dict will unroll to XCom values with keys as XCom keys. Defaults to False.",
          "530:         :param use_dill: Whether to use dill or pickle for serialization",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "514:             (DNS-1123 subdomain, containing only ``[a-z0-9.-]``). Defaults to",
          "515:             ``k8s_airflow_pod_{RANDOM_UUID}``.",
          "516:         :param random_name_suffix: If *True*, will generate a random suffix.",
          "517:         :param ports: Ports for the launched pod.",
          "518:         :param volume_mounts: *volumeMounts* for the launched pod.",
          "519:         :param volumes: Volumes for the launched pod. Includes *ConfigMaps* and",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "541:         :param arguments: arguments of the entrypoint. (templated)",
          "542:             The docker image's CMD is used if this is not provided.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "533:             a new pod for each try.",
          "534:         :param labels: Labels to apply to the pod. (templated)",
          "535:         :param startup_timeout_seconds: Timeout in seconds to startup the pod.",
          "536:         :param get_logs: Get the stdout of the container as logs of the tasks.",
          "537:         :param image_pull_policy: Specify a policy to cache or always pull an",
          "538:             image.",
          "539:         :param annotations: Non-identifying metadata you can attach to the pod.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "562:         :param startup_check_interval_seconds: interval in seconds to check if the pod has already started",
          "564:         :param container_logs: list of containers whose logs will be published to stdout",
          "565:             Takes a sequence of containers, a single container name or True.",
          "566:             If True, all the containers logs are published. Works in conjunction with ``get_logs`` param.",
          "567:             The default value is the base container.",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "548:             pod. If more than one secret is required, provide a comma separated",
          "549:             list, e.g. ``secret_a,secret_b``.",
          "550:         :param service_account_name: Name of the service account.",
          "554:         :param hostnetwork: If *True*, enable host networking on the pod.",
          "555:         :param tolerations: A list of Kubernetes tolerations.",
          "556:         :param security_context: Security options the pod should run with",
          "557:             (PodSecurityContext).",
          "558:         :param dnspolicy: DNS policy for the pod.",
          "559:         :param schedulername: Specify a scheduler name for the pod",
          "560:         :param init_containers: Init containers for the launched pod.",
          "561:         :param log_events_on_failure: Log the pod's events if a failure occurs.",
          "562:         :param do_xcom_push: If *True*, the content of",
          "563:             ``/airflow/xcom/return.json`` in the container will also be pushed",
          "564:             to an XCom when the container completes.",
          "565:         :param pod_template_file: Path to pod template file (templated)",
          "566:         :param priority_class_name: Priority class name for the launched pod.",
          "567:         :param pod_runtime_info_envs: A list of environment variables",
          "568:             to be set in the container.",
          "",
          "[Removed Lines]",
          "551:         :param is_delete_operator_pod: What to do when the pod reaches its final",
          "552:             state, or the execution is interrupted. If *True* (default), delete",
          "553:             the pod; otherwise leave the pod.",
          "",
          "[Added Lines]",
          "583:         :param host_aliases: A list of host aliases to apply to the containers in the pod.",
          "587:         :param container_security_context: security options the container should run with.",
          "589:         :param dns_config: dns configuration (ip addresses, searches, options) for the pod.",
          "590:         :param hostname: hostname for the pod.",
          "591:         :param subdomain: subdomain for the pod.",
          "593:         :param full_pod_spec: The complete podSpec",
          "600:         :param pod_template_dict: pod template dictionary (templated)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "572:             ConfigMaps to populate the environment variables with. The contents",
          "573:             of the target ConfigMap's Data field will represent the key-value",
          "574:             pairs as environment variables. Extends env_from.",
          "575:         \"\"\"",
          "576:     @overload",
          "577:     def sensor(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "610:         :param skip_on_exit_code: If task exits with this exit code, leave the task",
          "611:             in ``skipped`` state (default: None). If set to ``None``, any non-zero",
          "612:             exit code will be treated as a failure.",
          "613:         :param base_container_name: The name of the base container in the pod. This container's logs",
          "614:             will appear as part of this task's logs if get_logs is True. Defaults to None. If None,",
          "615:             will consult the class variable BASE_CONTAINER_NAME (which defaults to \"base\") for the base",
          "616:             container name to use.",
          "617:         :param deferrable: Run operator in the deferrable mode.",
          "618:         :param poll_interval: Polling period in seconds to check for the status. Used only in deferrable mode.",
          "619:         :param log_pod_spec_on_failure: Log the pod's specification if a failure occurs",
          "620:         :param on_finish_action: What to do when the pod reaches its final state, or the execution is interrupted.",
          "621:             If \"delete_pod\", the pod will be deleted regardless its state; if \"delete_succeeded_pod\",",
          "622:             only succeeded pod will be deleted. You can set to \"keep_pod\" to keep the pod.",
          "623:         :param termination_message_policy: The termination message policy of the base container.",
          "624:             Default value is \"File\"",
          "625:         :param active_deadline_seconds: The active_deadline_seconds which matches to active_deadline_seconds",
          "626:             in V1PodSpec.",
          "627:         :param progress_callback: Callback function for receiving k8s container logs.",
          "",
          "---------------"
        ]
      }
    }
  ]
}