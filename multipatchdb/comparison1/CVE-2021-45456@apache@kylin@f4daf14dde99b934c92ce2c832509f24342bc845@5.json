{
  "cve_id": "CVE-2021-45456",
  "cve_desc": "Apache kylin checks the legitimacy of the project before executing some commands with the project name passed in by the user. There is a mismatch between what is being checked and what is being used as the shell command argument in DiagnosisService. This may cause an illegal project name to pass the check and perform the following steps, resulting in a command injection vulnerability. This issue affects Apache Kylin 4.0.0.",
  "repo": "apache/kylin",
  "patch_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
  "patch_info": {
    "commit_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
    "repo": "apache/kylin",
    "commit_url": "https://github.com/apache/kylin/commit/f4daf14dde99b934c92ce2c832509f24342bc845",
    "files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "server/src/main/webapp/WEB-INF/web.xml"
    ],
    "message": "test fix",
    "before_after_code_files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java"
    ]
  },
  "patch_diff": {
    "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "3403:     public String getKerberosPrincipal() {",
      "3404:         return getOptional(\"kylin.kerberos.principal\");",
      "3405:     }",
      "3406: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "3407:     public String getEncryptCipherIvSpec() {",
      "3408:         return getOptional(\"kylin.security.encrypt.cipher.ivSpec\", \"AAAAAAAAAAAAAAAA\");",
      "3409:     }",
      "",
      "---------------"
    ],
    "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java -> core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "25: import java.security.NoSuchAlgorithmException;",
      "27: import org.apache.commons.codec.binary.Base64;",
      "29: import javax.crypto.Cipher;",
      "30: import javax.crypto.NoSuchPaddingException;",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "28: import org.apache.kylin.common.KylinConfig;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "42:             InvalidKeyException, NoSuchPaddingException, NoSuchAlgorithmException, UnsupportedEncodingException {",
      "43:         Cipher cipher = Cipher.getInstance(\"AES/CFB/PKCS5Padding\");",
      "44:         final SecretKeySpec secretKey = new SecretKeySpec(key, \"AES\");",
      "46:         cipher.init(cipherMode, secretKey, ivSpec);",
      "47:         return cipher;",
      "48:     }",
      "",
      "[Removed Lines]",
      "45:         IvParameterSpec ivSpec = new IvParameterSpec(\"AAAAAAAAAAAAAAAA\".getBytes(\"UTF-8\"));",
      "",
      "[Added Lines]",
      "46:         IvParameterSpec ivSpec = new IvParameterSpec(KylinConfig.getInstanceFromEnv().getEncryptCipherIvSpec().getBytes(\"UTF-8\"));",
      "",
      "---------------"
    ],
    "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java": [
      "File: core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java -> core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: package org.apache.kylin.common.util;",
      "21: import org.junit.Assert;",
      "22: import org.junit.Test;",
      "26:     @Test",
      "27:     public void testAESEncrypt(){",
      "",
      "[Removed Lines]",
      "24: public class EncryptUtilTest {",
      "",
      "[Added Lines]",
      "21: import org.junit.After;",
      "23: import org.junit.Before;",
      "26: public class EncryptUtilTest extends LocalFileMetadataTestCase {",
      "27:     @Before",
      "28:     public void setUp() throws Exception {",
      "29:         this.createTestMetadata();",
      "30:     }",
      "32:     @After",
      "33:     public void after() throws Exception {",
      "34:         this.cleanupTestMetadata();",
      "35:     }",
      "",
      "---------------"
    ],
    "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java": [
      "File: server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java -> server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "87:     public String dumpProjectDiagnosisInfo(String project, File exportPath) throws IOException {",
      "88:         Message msg = MsgPicker.getMsg();",
      "89:         ProjectInstance projectInstance =",
      "90:                 ProjectManager.getInstance(KylinConfig.getInstanceFromEnv())",
      "92:         if (null == projectInstance) {",
      "93:             throw new BadRequestException(",
      "95:         }",
      "96:         aclEvaluate.checkProjectOperationPermission(projectInstance);",
      "98:         runDiagnosisCLI(args);",
      "99:         return getDiagnosisPackageName(exportPath);",
      "100:     }",
      "",
      "[Removed Lines]",
      "91:                         .getProject(ValidateUtil.convertStringToBeAlphanumericUnderscore(project));",
      "94:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), project));",
      "97:         String[] args = { project, exportPath.getAbsolutePath() };",
      "",
      "[Added Lines]",
      "89:         String projectName = ValidateUtil.convertStringToBeAlphanumericUnderscore(project);",
      "92:                         .getProject(projectName);",
      "95:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), projectName));",
      "98:         String[] args = { projectName, exportPath.getAbsolutePath() };",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "69814e01a28210c95f2d296f6a5ed4402a8ae1e4",
      "candidate_info": {
        "commit_hash": "69814e01a28210c95f2d296f6a5ed4402a8ae1e4",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/69814e01a28210c95f2d296f6a5ed4402a8ae1e4",
        "files": [
          "build/bin/download-spark.sh",
          "docker/dockerfile/cluster/client/Dockerfile",
          "pom.xml"
        ],
        "message": "KYLIN-4894 Upgrade Apache Spark version to 2.4.7\n\n(cherry picked from commit ed87e2d940bca2d518d3ac34fd01c4e5129e8ee7)",
        "before_after_code_files": [
          "build/bin/download-spark.sh||build/bin/download-spark.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "build/bin/download-spark.sh||build/bin/download-spark.sh": [
          "File: build/bin/download-spark.sh -> build/bin/download-spark.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "36:     alias md5cmd=\"md5 -q\"",
          "37: fi",
          "42: if [ ! -f \"spark-${spark_version}-bin-hadoop2.7.tgz\" ]",
          "43: then",
          "",
          "[Removed Lines]",
          "39: spark_version=\"2.4.6\"",
          "40: spark_pkg_md5=\"82364f8765d03dfb14cb9c606d678058\"",
          "",
          "[Added Lines]",
          "39: spark_version=\"2.4.7\"",
          "40: spark_pkg_md5=\"76afb611aaac5721c9fa91fdc9defa99\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0f94e8165427eeb0cbae516f9516d4742f7a1205",
      "candidate_info": {
        "commit_hash": "0f94e8165427eeb0cbae516f9516d4742f7a1205",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/0f94e8165427eeb0cbae516f9516d4742f7a1205",
        "files": [
          "build/CI/kylin-system-testing/data/auto_config/auto_config_no_distinct.json",
          "build/CI/kylin-system-testing/data/auto_config/auto_config_override_conf.json",
          "build/CI/kylin-system-testing/data/generic_desc_data/generic_desc_data_3x.json",
          "build/CI/kylin-system-testing/data/generic_desc_data/generic_desc_data_4x.json",
          "build/CI/kylin-system-testing/data/happy_path/happy_path.sql",
          "build/CI/kylin-system-testing/env/default/python.properties",
          "build/CI/kylin-system-testing/features/specs/auto_config/auto_config.spec",
          "build/CI/kylin-system-testing/features/specs/happy_path/happy_path.spec",
          "build/CI/kylin-system-testing/features/specs/project_model/model.spec",
          "build/CI/kylin-system-testing/features/specs/project_model/project.spec",
          "build/CI/kylin-system-testing/features/step_impl/auto_config/auto_config.py",
          "build/CI/kylin-system-testing/features/step_impl/before_suite.py",
          "build/CI/kylin-system-testing/features/step_impl/happy_path/happy_path.py",
          "build/CI/kylin-system-testing/features/step_impl/project_model/model.py",
          "build/CI/kylin-system-testing/features/step_impl/project_model/project.py",
          "build/CI/kylin-system-testing/kylin_instances/kylin_host.yml",
          "build/CI/kylin-system-testing/kylin_instances/kylin_instance.yml",
          "build/CI/kylin-system-testing/kylin_utils/equals.py",
          "build/CI/kylin-system-testing/kylin_utils/shell.py",
          "build/CI/kylin-system-testing/kylin_utils/util.py"
        ],
        "message": "KYLIN-4898 Automated test",
        "before_after_code_files": [
          "build/CI/kylin-system-testing/data/happy_path/happy_path.sql||build/CI/kylin-system-testing/data/happy_path/happy_path.sql",
          "build/CI/kylin-system-testing/env/default/python.properties||build/CI/kylin-system-testing/env/default/python.properties",
          "build/CI/kylin-system-testing/features/specs/auto_config/auto_config.spec||build/CI/kylin-system-testing/features/specs/auto_config/auto_config.spec",
          "build/CI/kylin-system-testing/features/specs/happy_path/happy_path.spec||build/CI/kylin-system-testing/features/specs/happy_path/happy_path.spec",
          "build/CI/kylin-system-testing/features/specs/project_model/model.spec||build/CI/kylin-system-testing/features/specs/project_model/model.spec",
          "build/CI/kylin-system-testing/features/specs/project_model/project.spec||build/CI/kylin-system-testing/features/specs/project_model/project.spec",
          "build/CI/kylin-system-testing/features/step_impl/auto_config/auto_config.py||build/CI/kylin-system-testing/features/step_impl/auto_config/auto_config.py",
          "build/CI/kylin-system-testing/features/step_impl/before_suite.py||build/CI/kylin-system-testing/features/step_impl/before_suite.py",
          "build/CI/kylin-system-testing/features/step_impl/happy_path/happy_path.py||build/CI/kylin-system-testing/features/step_impl/happy_path/happy_path.py",
          "build/CI/kylin-system-testing/features/step_impl/project_model/model.py||build/CI/kylin-system-testing/features/step_impl/project_model/model.py",
          "build/CI/kylin-system-testing/features/step_impl/project_model/project.py||build/CI/kylin-system-testing/features/step_impl/project_model/project.py",
          "build/CI/kylin-system-testing/kylin_utils/equals.py||build/CI/kylin-system-testing/kylin_utils/equals.py",
          "build/CI/kylin-system-testing/kylin_utils/shell.py||build/CI/kylin-system-testing/kylin_utils/shell.py",
          "build/CI/kylin-system-testing/kylin_utils/util.py||build/CI/kylin-system-testing/kylin_utils/util.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "build/CI/kylin-system-testing/data/happy_path/happy_path.sql||build/CI/kylin-system-testing/data/happy_path/happy_path.sql": [
          "File: build/CI/kylin-system-testing/data/happy_path/happy_path.sql -> build/CI/kylin-system-testing/data/happy_path/happy_path.sql",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: --",
          "2: -- Licensed to the Apache Software Foundation (ASF) under one",
          "3: -- or more contributor license agreements.  See the NOTICE file",
          "4: -- distributed with this work for additional information",
          "5: -- regarding copyright ownership.  The ASF licenses this file",
          "6: -- to you under the Apache License, Version 2.0 (the",
          "7: -- \"License\"); you may not use this file except in compliance",
          "8: -- with the License.  You may obtain a copy of the License at",
          "9: --",
          "10: --     http://www.apache.org/licenses/LICENSE-2.0",
          "11: --",
          "12: -- Unless required by applicable law or agreed to in writing, software",
          "13: -- distributed under the License is distributed on an \"AS IS\" BASIS,",
          "14: -- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
          "15: -- See the License for the specific language governing permissions and",
          "16: -- limitations under the License.",
          "17: --",
          "19: SELECT",
          "20: KYLIN_SALES.PART_DT,",
          "21: KYLIN_CAL_DT.YEAR_BEG_DT,",
          "22: SUM(BUYER_ACCOUNT.ACCOUNT_BUYER_LEVEL) as BUYER_LEVEL_SUM,",
          "23: COUNT(DISTINCT KYLIN_SALES.SELLER_ID) as SELLER_CNT_HLL,",
          "24: SUM(KYLIN_SALES.PRICE) as GMV,",
          "25: PERCENTILE(KYLIN_SALES.PRICE, 0.5) as PRICE_PERCENTILE,",
          "26: COUNT(DISTINCT KYLIN_SALES.ITEM_ID) as ITEM_COUNT",
          "27: FROM KYLIN_SALES",
          "28: INNER JOIN KYLIN_CAL_DT as KYLIN_CAL_DT",
          "29: ON KYLIN_SALES.PART_DT = KYLIN_CAL_DT.CAL_DT",
          "30: INNER JOIN KYLIN_CATEGORY_GROUPINGS",
          "31: ON KYLIN_SALES.LEAF_CATEG_ID = KYLIN_CATEGORY_GROUPINGS.LEAF_CATEG_ID AND KYLIN_SALES.LSTG_SITE_ID = KYLIN_CATEGORY_GROUPINGS.SITE_ID",
          "32: INNER JOIN KYLIN_ACCOUNT as BUYER_ACCOUNT",
          "33: ON KYLIN_SALES.BUYER_ID = BUYER_ACCOUNT.ACCOUNT_ID",
          "34: INNER JOIN KYLIN_ACCOUNT as SELLER_ACCOUNT",
          "35: ON KYLIN_SALES.SELLER_ID = SELLER_ACCOUNT.ACCOUNT_ID",
          "36: INNER JOIN KYLIN_COUNTRY as BUYER_COUNTRY",
          "37: ON BUYER_ACCOUNT.ACCOUNT_COUNTRY = BUYER_COUNTRY.COUNTRY",
          "38: INNER JOIN KYLIN_COUNTRY as SELLER_COUNTRY",
          "39: ON SELLER_ACCOUNT.ACCOUNT_COUNTRY = SELLER_COUNTRY.COUNTRY",
          "40: WHERE PART_DT >= '2012-12-25' and PART_DT < '2013-01-05' and SELLER_COUNTRY.COUNTRY in ('CN')",
          "41: GROUP BY KYLIN_SALES.PART_DT, KYLIN_CAL_DT.YEAR_BEG_DT",
          "42: ORDER BY KYLIN_SALES.PART_DT, KYLIN_CAL_DT.YEAR_BEG_DT;",
          "",
          "---------------"
        ],
        "build/CI/kylin-system-testing/env/default/python.properties||build/CI/kylin-system-testing/env/default/python.properties": [
          "File: build/CI/kylin-system-testing/env/default/python.properties -> build/CI/kylin-system-testing/env/default/python.properties",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: # limitations under the License.",
          "16: #",
          "20: # Comma seperated list of dirs. path should be relative to project root.",
          "21: STEP_IMPL_DIR = features/step_impl",
          "",
          "[Removed Lines]",
          "18: GAUGE_PYTHON_COMMAND = python3",
          "",
          "[Added Lines]",
          "18: GAUGE_PYTHON_COMMAND = python",
          "",
          "---------------"
        ],
        "build/CI/kylin-system-testing/features/specs/auto_config/auto_config.spec||build/CI/kylin-system-testing/features/specs/auto_config/auto_config.spec": [
          "File: build/CI/kylin-system-testing/features/specs/auto_config/auto_config.spec -> build/CI/kylin-system-testing/features/specs/auto_config/auto_config.spec",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Automically setting spark configurations",
          "2: Tags: auto_config, 4.x",
          "4: ## use default configuration",
          "6: ## auto configuration with user-defined parameters",
          "8: ## auto configuration with user-defined parameters",
          "10: ## auto_config and override on cube level",
          "",
          "---------------"
        ],
        "build/CI/kylin-system-testing/features/specs/happy_path/happy_path.spec||build/CI/kylin-system-testing/features/specs/happy_path/happy_path.spec": [
          "File: build/CI/kylin-system-testing/features/specs/happy_path/happy_path.spec -> build/CI/kylin-system-testing/features/specs/happy_path/happy_path.spec",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Automated test for Happy Path",
          "2: Tags: happy_path, 4.x",
          "4: ## automated happy path",
          "",
          "---------------"
        ],
        "build/CI/kylin-system-testing/features/specs/project_model/model.spec||build/CI/kylin-system-testing/features/specs/project_model/model.spec": [
          "File: build/CI/kylin-system-testing/features/specs/project_model/model.spec -> build/CI/kylin-system-testing/features/specs/project_model/model.spec",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # model management",
          "2: Tags: model, 4.x",
          "4: ## model clone",
          "6: ## model clone duplicated",
          "",
          "---------------"
        ],
        "build/CI/kylin-system-testing/features/specs/project_model/project.spec||build/CI/kylin-system-testing/features/specs/project_model/project.spec": [
          "File: build/CI/kylin-system-testing/features/specs/project_model/project.spec -> build/CI/kylin-system-testing/features/specs/project_model/project.spec",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # project management",
          "2: Tags: project, 4.x",
          "4: ## create duplicate project",
          "6: ## look up the project list",
          "8: ## update project description",
          "10: ## delete project and check",
          "",
          "---------------"
        ],
        "build/CI/kylin-system-testing/features/step_impl/auto_config/auto_config.py||build/CI/kylin-system-testing/features/step_impl/auto_config/auto_config.py": [
          "File: build/CI/kylin-system-testing/features/step_impl/auto_config/auto_config.py -> build/CI/kylin-system-testing/features/step_impl/auto_config/auto_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: from getgauge.python import step",
          "2: import os",
          "3: import json",
          "4: import pytest",
          "5: from kylin_utils import util",
          "6: from kylin_utils import shell",
          "7: import re",
          "8: import time",
          "9: from getgauge.python import Messages",
          "11: global client",
          "12: client = util.setup_instance('kylin_instance.yml')",
          "15: @step(\"use default configuration on cube <cube_name>\")",
          "16: def default_config(cube_name):",
          "17:     sh_config(set_auto=\"True\", executor_instance=\"5\", instance_strategy=\"100,2,500,3,1000,4\")",
          "19:     time.sleep(60)",
          "21:     resp = client.build_segment(start_time=\"1325376000000\",",
          "22:                                 end_time=\"1388534400000\",",
          "23:                                 cube_name=cube_name)",
          "24:     job_id = resp['uuid']",
          "25:     step_id = job_id + \"-01\"",
          "26:     client.await_job(job_id)",
          "27:     resp = client.get_step_output(job_id=job_id, step_id=step_id)",
          "28:     output = resp.get('cmd_output')",
          "30:     check_log(output=output, memorySize=\"4GB\", coresSize=\"5\", memoryOverheadSize=\"1GB\", instancesSize=\"5\", partitionsSize=\"2\")",
          "32:     client.disable_cube(cube_name)",
          "34:     time.sleep(30)",
          "35:     client.purge_cube(cube_name)",
          "37:     time.sleep(30)",
          "40: @step(\"auto configuration with user-defined parameters on cube <cube_name>\")",
          "41: def user_def_config(cube_name):",
          "42:     sh_config(set_auto=\"True\", executor_instance=\"1\", instance_strategy=\"100,1,500,2,1000,3\")",
          "44:     time.sleep(60)",
          "46:     resp = client.build_segment(start_time=\"1325376000000\",",
          "47:                                 end_time=\"1388534400000\",",
          "48:                                 cube_name=cube_name)",
          "50:     job_id = resp['uuid']",
          "51:     step_id = job_id + \"-01\"",
          "52:     client.await_job(job_id)",
          "53:     resp = client.get_step_output(job_id=job_id, step_id=step_id)",
          "54:     output = resp.get('cmd_output')",
          "56:     check_log(output=output, memorySize=\"4GB\", coresSize=\"5\", memoryOverheadSize=\"1GB\", instancesSize=\"1\", partitionsSize=\"2\")",
          "58:     client.disable_cube(cube_name)",
          "60:     time.sleep(30)",
          "61:     client.purge_cube(cube_name)",
          "63:     time.sleep(30)",
          "66: @step(\"auto configuration with user-defined parameters on <cube_name> and <cube_no_distinct>\")",
          "67: def user_defined_no_dist(cube_name, cube_no_distinct):",
          "68:     sh_config(set_auto=\"True\", executor_instance=\"2\", instance_strategy=\"100,4,500,6,1000,10\")",
          "70:     time.sleep(60)",
          "72:     resp = client.build_segment(start_time=\"1325376000000\",",
          "73:                                 end_time=\"1388534400000\",",
          "74:                                 cube_name=cube_name)",
          "76:     job_id = resp['uuid']",
          "77:     step_id = job_id + \"-01\"",
          "78:     client.await_job(job_id)",
          "79:     resp = client.get_step_output(job_id=job_id, step_id=step_id)",
          "80:     output = resp.get('cmd_output')",
          "82:     check_log(output=output, memorySize=\"4GB\", coresSize=\"5\", memoryOverheadSize=\"1GB\", instancesSize=\"2\", partitionsSize=\"2\")",
          "84:     client.disable_cube(cube_name)",
          "86:     time.sleep(30)",
          "87:     client.purge_cube(cube_name)",
          "89:     time.sleep(30)",
          "91:     with open(os.path.join('meta_data/auto_config', 'auto_config_no_distinct.json'), 'r') as f:",
          "92:         cube_desc_data = json.load(f)['cube_desc_data']",
          "94:     client.create_cube('learn_kylin', cube_no_distinct, cube_desc_data=cube_desc_data)",
          "96:     resp = client.build_segment(start_time=\"1325376000000\",",
          "97:                                 end_time=\"1388534400000\",",
          "98:                                 cube_name=cube_no_distinct)",
          "100:     job_id = resp['uuid']",
          "101:     step_id = job_id + \"-01\"",
          "102:     client.await_job(job_id)",
          "103:     resp = client.get_step_output(job_id=job_id, step_id=step_id)",
          "104:     output = resp.get('cmd_output')",
          "106:     check_log(output=output, memorySize=\"1GB\", coresSize=\"1\", memoryOverheadSize=\"512MB\", instancesSize=\"2\", partitionsSize=\"2\")",
          "108:     client.disable_cube(cube_no_distinct)",
          "110:     time.sleep(30)",
          "111:     client.delete_cube(cube_no_distinct)",
          "113:     time.sleep(30)",
          "116: @step(\"auto_config and override on cube level on cube <cube_override_conf>\")",
          "117: def override_on_cube(cube_override_conf):",
          "118:     sh_config(set_auto=\"True\", executor_instance=\"5\", instance_strategy=\"100,2,500,3,1000,4\")",
          "120:     time.sleep(60)",
          "122:     with open(os.path.join('meta_data/auto_config', 'auto_config_override_conf.json'), 'r') as f:",
          "123:         cube_desc_data = json.load(f)['cube_desc_data']",
          "125:     client.create_cube('learn_kylin', cube_override_conf, cube_desc_data=cube_desc_data)",
          "127:     resp = client.build_segment(start_time=\"1325376000000\",",
          "128:                                 end_time=\"1388534400000\",",
          "129:                                 cube_name=cube_override_conf)",
          "131:     job_id = resp['uuid']",
          "132:     step_id = job_id + \"-01\"",
          "133:     client.await_job(job_id)",
          "134:     resp = client.get_step_output(job_id=job_id, step_id=step_id)",
          "135:     output = resp.get('cmd_output')",
          "137:     memory = re.findall(\"Override user-defined spark conf, set spark.executor.memory.*\", output)",
          "138:     cores = re.findall(\"Override user-defined spark conf, set spark.executor.cores.*\", output)",
          "139:     memoryOverhead = re.findall(\"Override user-defined spark conf, set spark.executor.memoryOverhead.*\", output)",
          "140:     instances = re.findall(\"Override user-defined spark conf, set spark.executor.instances.*\", output)",
          "141:     partitions = re.findall(\"Override user-defined spark conf, set spark.sql.shuffle.partitions.*\", output)",
          "142:     assert memory[0] == \"Override user-defined spark conf, set spark.executor.memory=2G.\", \\",
          "143:         Messages.write_message(\"expected Override user-defined spark conf, set spark.executor.memory=2G; actually \" + memory[0])",
          "144:     assert cores[0] == \"Override user-defined spark conf, set spark.executor.cores=2.\", \\",
          "145:         Messages.write_message(\"expected Override user-defined spark conf, set spark.executor.cores=2; actually \" + cores[0])",
          "146:     assert memoryOverhead[0] == \"Override user-defined spark conf, set spark.executor.memoryOverhead=256M.\", \\",
          "147:         Messages.write_message(\"expected Override user-defined spark conf, set spark.executor.memoryOverhead=256M; actually \" + memoryOverhead[0])",
          "148:     assert instances[0] == \"Override user-defined spark conf, set spark.executor.instances=3.\",\\",
          "149:         Messages.write_message(\"expected Override user-defined spark conf, set spark.executor.instances=3; actually\" + instances[0])",
          "150:     assert partitions[0] == \"Override user-defined spark conf, set spark.sql.shuffle.partitions=3.\", \\",
          "151:         Messages.write_message(\"expected Override user-defined spark conf, set spark.sql.shuffle.partitions=3; actually \" + partitions[0])",
          "153:     client.disable_cube(cube_override_conf)",
          "155:     time.sleep(30)",
          "156:     client.delete_cube(cube_override_conf)",
          "158:     time.sleep(30)",
          "161: def sh_config(set_auto, executor_instance, instance_strategy):",
          "162:     sh = util.ssh_shell()",
          "164:     sh_command = \"cd $KYLIN_HOME/conf \" \\",
          "165:                  \"&& sed -i -r '/kylin.spark-conf.auto.prior/d' kylin.properties\" \\",
          "166:                  \"&& sed -i -r '/kylin.engine.base-executor-instance/d' kylin.properties\" \\",
          "167:                  \"&& sed -i -r '/kylin.engine.executor-instance-strategy/d' kylin.properties\" \\",
          "168:                  \"&& sed -i '$akylin.spark-conf.auto.prior={set_auto}' kylin.properties\" \\",
          "169:                  \"&& sed -i '$akylin.engine.base-executor-instance={executor_instance}' kylin.properties\" \\",
          "170:                  \"&& sed -i '$akylin.engine.executor-instance-strategy={instance_strategy}' kylin.properties\".format(",
          "171:         set_auto=set_auto, executor_instance=executor_instance, instance_strategy=instance_strategy)",
          "173:     resp = sh.command(sh_command)",
          "175:     resp = sh.command(\"cd $KYLIN_HOME/bin\"",
          "176:                       \"&& sh kylin.sh stop\"",
          "177:                       \"&& sh kylin.sh start\")",
          "180: def check_log(output, memorySize, coresSize, memoryOverheadSize, instancesSize, partitionsSize):",
          "181:     memory = re.findall(\"Auto set spark conf: spark.executor.memory = .*\", output)",
          "182:     cores = re.findall(\"Auto set spark conf: spark.executor.cores = .*\", output)",
          "183:     memoryOverhead = re.findall(\"Auto set spark conf: spark.executor.memoryOverhead = .*\", output)",
          "184:     instances = re.findall(\"Auto set spark conf: spark.executor.instances = .*\", output)",
          "185:     partitions = re.findall(\"Auto set spark conf: spark.sql.shuffle.partitions = .*\", output)",
          "186:     assert memory[0] == \"Auto set spark conf: spark.executor.memory = {memory}.\".format(memory = memorySize), \\",
          "187:         Messages.write_message(\"expected \"+\"Auto set spark conf: spark.executor.memory = {memory}.\".format(memory = memorySize)+ \", actually \" +memory[0])",
          "188:     assert cores[0] == \"Auto set spark conf: spark.executor.cores = {cores}.\".format(cores = coresSize), \\",
          "189:         Messages.write_message(\"expected \"+\"Auto set spark conf: spark.executor.cores = {cores}.\".format(cores = coresSize)+\", actually \" +cores[0])",
          "190:     assert memoryOverhead[0] == \"Auto set spark conf: spark.executor.memoryOverhead = {memoryOverhead}.\".format(memoryOverhead = memoryOverheadSize), \\",
          "191:         Messages.write_message(\"expected \"+\"Auto set spark conf: spark.executor.memoryOverhead = {memoryOverhead}.\".format(memoryOverhead = memoryOverheadSize)+\", actually \" +memoryOverhead[0])",
          "192:     assert instances[0] == \"Auto set spark conf: spark.executor.instances = {instances}.\".format(instances = instancesSize), \\",
          "193:         Messages.write_message(\"expected \"+\"Auto set spark conf: spark.executor.instances = {instances}.\".format(instances = instancesSize)+\", actually \" +instances[0])",
          "194:     assert partitions[0] == \"Auto set spark conf: spark.sql.shuffle.partitions = {partitions}.\".format(partitions = partitionsSize), \\",
          "195:         Messages.write_message(\"expected \"+\"Auto set spark conf: spark.sql.shuffle.partitions = {partitions}.\".format(partitions = partitionsSize)+\", actually \" +partitions[0])",
          "",
          "---------------"
        ],
        "build/CI/kylin-system-testing/features/step_impl/before_suite.py||build/CI/kylin-system-testing/features/step_impl/before_suite.py": [
          "File: build/CI/kylin-system-testing/features/step_impl/before_suite.py -> build/CI/kylin-system-testing/features/step_impl/before_suite.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:     model_desc_data = data.get('model_desc_data')",
          "45:     model_name = model_desc_data.get('name')",
          "47:     if not util.if_model_exists(kylin_client=client, model_name=model_name, project=project_name):",
          "48:         resp = client.create_model(project_name=project_name,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "46:     snowflake_left_incre_model = data.get('snowflake_left_incre_model')",
          "47:     left_model_name = snowflake_left_incre_model.get('name')",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "50:                                    model_desc_data=model_desc_data)",
          "51:         assert json.loads(resp['modelDescData'])['name'] == model_name",
          "53:     cube_desc_data = data.get('cube_desc_data')",
          "54:     cube_name = cube_desc_data.get('name')",
          "55:     if not util.if_cube_exists(kylin_client=client, cube_name=cube_name, project=project_name):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "55:     if not util.if_model_exists(kylin_client=client, model_name=left_model_name, project=project_name):",
          "56:         resp = client.create_model(project_name=project_name,",
          "57:                                    model_name=left_model_name,",
          "58:                                    model_desc_data=snowflake_left_incre_model)",
          "59:         assert json.loads(resp['modelDescData'])['name'] == left_model_name",
          "",
          "---------------"
        ],
        "build/CI/kylin-system-testing/features/step_impl/happy_path/happy_path.py||build/CI/kylin-system-testing/features/step_impl/happy_path/happy_path.py": [
          "File: build/CI/kylin-system-testing/features/step_impl/happy_path/happy_path.py -> build/CI/kylin-system-testing/features/step_impl/happy_path/happy_path.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: from getgauge.python import step",
          "2: import os",
          "3: import json",
          "4: import pytest",
          "5: from kylin_utils import util",
          "6: from kylin_utils import equals",
          "7: from kylin_utils import shell",
          "8: import re",
          "9: import time",
          "12: @step(\"automated happy path, check query using <sql_file>\")",
          "13: def happy_path(sql_file):",
          "14:     global client",
          "15:     client = util.setup_instance('kylin_instance.yml')",
          "17:     resp1 = client.build_segment(start_time=\"1325376000000\",",
          "18:                                 end_time=\"1356998400000\",",
          "19:                                 cube_name=\"kylin_sales_cube\")",
          "21:     resp2 = client.build_segment(start_time=\"1356998400000\",",
          "22:                                 end_time=\"1388620800000\",",
          "23:                                 cube_name=\"kylin_sales_cube\")",
          "25:     job_id1 = resp1['uuid']",
          "26:     job_id2 = resp2['uuid']",
          "27:     client.await_job(job_id1)",
          "28:     client.await_job(job_id2)",
          "30:     resp = client.merge_segment(cube_name=\"kylin_sales_cube\",",
          "31:                                 start_time=\"1325376000000\",",
          "32:                                 end_time=\"1388620800000\")",
          "34:     job_id = resp['uuid']",
          "35:     client.await_job(job_id)",
          "37:     resp = client.refresh_segment(cube_name=\"kylin_sales_cube\",",
          "38:                                   start_time=\"1325376000000\",",
          "39:                                   end_time=\"1388620800000\")",
          "40:     job_id = resp['uuid']",
          "41:     client.await_job(job_id)",
          "43:     with open(sql_file, 'r', encoding='utf8') as sql:",
          "44:         sql = sql.read()",
          "46:     equals.compare_sql_result(sql=sql, project='learn_kylin', kylin_client=client, expected_result=None)",
          "48:     resp = client.disable_cube(cube_name=\"kylin_sales_cube\")",
          "49:     assert resp['status'] == 'DISABLED'",
          "51:     time.sleep(10)",
          "53:     client.purge_cube(cube_name=\"kylin_sales_cube\")",
          "54:     time.sleep(30)",
          "56:     resp = client.get_cube_instance(cube_name=\"kylin_sales_cube\")",
          "58:     assert len(resp['segments']) == 0",
          "",
          "---------------"
        ],
        "build/CI/kylin-system-testing/features/step_impl/project_model/model.py||build/CI/kylin-system-testing/features/step_impl/project_model/model.py": [
          "File: build/CI/kylin-system-testing/features/step_impl/project_model/model.py -> build/CI/kylin-system-testing/features/step_impl/project_model/model.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: from getgauge.python import step",
          "2: import os",
          "3: import json",
          "4: import pytest",
          "5: from kylin_utils import util",
          "7: global client",
          "8: client = util.setup_instance('kylin_instance.yml')",
          "11: @step(\"in project <project_name>, clone model <model_name> and name it <clone_name>\")",
          "12: def check_model_clone(project_name, model_name, clone_name):",
          "13:     clone = client.clone_model(project_name, model_name, clone_name)",
          "14:     assert util.if_model_exists(kylin_client=client, model_name=clone_name, project=project_name) == 1",
          "15:     model_desc = client.list_model_desc(project_name, model_name)",
          "16:     clone_model_desc = client.list_model_desc(project_name, clone_name)",
          "17:     check_list = ['fact_table', 'lookups', 'dimensions', 'metrics', 'filter_condition', 'partition_desc']",
          "18:     for i in range(len(check_list)):",
          "19:         assert model_desc[0][check_list[i]] == clone_model_desc[0][check_list[i]]",
          "22: @step(\"again, in project <project_name>, clone model <model_name> and name it <clone_name>\")",
          "23: def check_clone_duplicated(project_name, model_name, clone_name):",
          "24:     with pytest.raises(Exception, match=r'Model name .* is duplicated, could not be created.'):",
          "25:         clone = client.clone_model(project_name, model_name, clone_name)",
          "",
          "---------------"
        ],
        "build/CI/kylin-system-testing/features/step_impl/project_model/project.py||build/CI/kylin-system-testing/features/step_impl/project_model/project.py": [
          "File: build/CI/kylin-system-testing/features/step_impl/project_model/project.py -> build/CI/kylin-system-testing/features/step_impl/project_model/project.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: from getgauge.python import step",
          "2: import os",
          "3: import json",
          "4: import pytest",
          "5: from kylin_utils import util",
          "7: global client",
          "8: client = util.setup_instance('kylin_instance.yml')",
          "10: @step(\"create a project <project_name> with description <project_description> and check that duplicate name is not allowed\")",
          "11: def create_duplicate_project(project_name, project_description):",
          "12:     client.create_project(project_name, description=project_description)",
          "13:     with pytest.raises(Exception, match=r'The project named .* already exists'):",
          "14:         client.create_project(project_name, description=project_description)",
          "17: @step(\"check that <project_name> is on the list\")",
          "18: def check_project(project_name):",
          "20:     assert util.if_project_exists(kylin_client=client, project=project_name) == 1",
          "23: @step(\"update the project <project_name> and edit the description to be <project_description>\")",
          "24: def update_project_description(project_name, project_description):",
          "25:     update = client.update_project(project_name, description=project_description)",
          "26:     resp = client.list_projects()",
          "27:     update = False",
          "28:     for i in range(len(resp)):",
          "29:         update = update | ((resp[i]['name'] == project_name) & (resp[i]['description'] == project_description))",
          "30:     assert update is True",
          "33: @step(\"delete project <project_name> and check that it's not on the list\")",
          "34: def check_delete_project(project_name):",
          "35:     delete = client.delete_project(project_name)",
          "36:     resp = client.list_projects()",
          "37:     exist = False",
          "38:     for i in range(len(resp)):",
          "39:         exist = exist | (resp[i]['name'] == project_name)",
          "40:     assert exist is False",
          "",
          "---------------"
        ],
        "build/CI/kylin-system-testing/kylin_utils/equals.py||build/CI/kylin-system-testing/kylin_utils/equals.py": [
          "File: build/CI/kylin-system-testing/kylin_utils/equals.py -> build/CI/kylin-system-testing/kylin_utils/equals.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "241:     kylin_resp = kylin_client.execute_query(cube_name=cube,",
          "242:                                             project_name=project,",
          "243:                                             sql=sql)",
          "246:     pushdown_resp = kylin_client.execute_query(project_name=pushdown_project, sql=sql)",
          "247:     assert pushdown_resp.get('isException') is False",
          "",
          "[Removed Lines]",
          "244:     assert kylin_resp.get('isException') is False, 'Thown Exception when execute ' + sql",
          "",
          "[Added Lines]",
          "244:     assert kylin_resp.get('isException') is False, 'Thrown Exception when execute ' + sql",
          "",
          "---------------"
        ],
        "build/CI/kylin-system-testing/kylin_utils/shell.py||build/CI/kylin-system-testing/kylin_utils/shell.py": [
          "File: build/CI/kylin-system-testing/kylin_utils/shell.py -> build/CI/kylin-system-testing/kylin_utils/shell.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "136: def shell():",
          "137:     return Bash()",
          "",
          "[Removed Lines]",
          "140: if __name__ == '__main__':",
          "141:     sh = sshshell('10.1.3.94', username='root', password='hadoop')",
          "142:     print(sh.command('pwd'))",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "build/CI/kylin-system-testing/kylin_utils/util.py||build/CI/kylin-system-testing/kylin_utils/util.py": [
          "File: build/CI/kylin-system-testing/kylin_utils/util.py -> build/CI/kylin-system-testing/kylin_utils/util.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from selenium import webdriver",
          "19: from yaml import load, loader",
          "20: import os",
          "22: from kylin_utils import kylin",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: from kylin_utils import shell",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "79:     if len(resp) == 1:",
          "80:         exists = 1",
          "81:     return exists",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "83: def ssh_shell(config_file='kylin_host.yml'):",
          "84:     instances_file = os.path.join('kylin_instances/', config_file)",
          "85:     stream = open(instances_file, 'r')",
          "86:     for item in load(stream, Loader=loader.SafeLoader):",
          "87:         host = item['host']",
          "88:         username = item['username']",
          "89:         password = item['password']",
          "90:     return shell.SSHShell(host=host, username=username, password=password)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cdc21bca775fd50ecb07bae05fa1ea56eac18882",
      "candidate_info": {
        "commit_hash": "cdc21bca775fd50ecb07bae05fa1ea56eac18882",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/cdc21bca775fd50ecb07bae05fa1ea56eac18882",
        "files": [
          "source-hive/src/main/java/org/apache/kylin/source/hive/SparkHiveClient.java"
        ],
        "message": "KYLIN-5069 Fix can't get location (#1776)",
        "before_after_code_files": [
          "source-hive/src/main/java/org/apache/kylin/source/hive/SparkHiveClient.java||source-hive/src/main/java/org/apache/kylin/source/hive/SparkHiveClient.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "source-hive/src/main/java/org/apache/kylin/source/hive/SparkHiveClient.java||source-hive/src/main/java/org/apache/kylin/source/hive/SparkHiveClient.java": [
          "File: source-hive/src/main/java/org/apache/kylin/source/hive/SparkHiveClient.java -> source-hive/src/main/java/org/apache/kylin/source/hive/SparkHiveClient.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "90:         Map<String, String> properties = catalogTable.ignoredProperties();",
          "91:         builder.setAllColumns(allColumns);",
          "92:         builder.setPartitionColumns(partitionColumns);",
          "94:         long totalSize = properties.contains(TABLE_TOTAL_SIZE) ? Long.parseLong(properties.apply(TABLE_TOTAL_SIZE)) : 0L;",
          "95:         builder.setFileSize(totalSize);",
          "96:         long totalFileNum = properties.contains(TABLE_FILE_NUM) ? Long.parseLong(properties.apply(TABLE_FILE_NUM)) : 0L;",
          "",
          "[Removed Lines]",
          "93:         builder.setSdLocation(catalogTable.location().getPath());",
          "",
          "[Added Lines]",
          "93:         if (catalogTable.tableType().equals(CatalogTableType.MANAGED())) {",
          "94:             builder.setSdLocation(catalogTable.location().getPath());",
          "95:         } else {",
          "96:             builder.setSdLocation(\"unknown\");",
          "97:         }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "af3a1b55ae1bbfb157ce9ec63a73651865c4c94a",
      "candidate_info": {
        "commit_hash": "af3a1b55ae1bbfb157ce9ec63a73651865c4c94a",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/af3a1b55ae1bbfb157ce9ec63a73651865c4c94a",
        "files": [
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "core-metrics/src/main/java/org/apache/kylin/metrics/QuerySparkMetrics.java",
          "core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkExecutionEnum.java",
          "core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkJobEnum.java",
          "core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkStageEnum.java",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/SparderMetricsListener.scala",
          "server-base/src/main/java/org/apache/kylin/rest/metrics/QueryMetricsFacade.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/CubeService.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/DashboardService.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java",
          "server/src/test/java/org/apache/kylin/rest/metrics/QueryMetricsTest.java",
          "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeDescCreator.java",
          "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeInstanceCreator.java",
          "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/HiveTableCreator.java",
          "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/KylinTableCreator.java",
          "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/ModelCreator.java",
          "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/SCCreator.java",
          "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/streamingv2/KafkaTopicCreator.java"
        ],
        "message": "KYLIN-4857 Refactor system cube for kylin4",
        "before_after_code_files": [
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "core-metrics/src/main/java/org/apache/kylin/metrics/QuerySparkMetrics.java||core-metrics/src/main/java/org/apache/kylin/metrics/QuerySparkMetrics.java",
          "core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkExecutionEnum.java||core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkExecutionEnum.java",
          "core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkJobEnum.java||core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkJobEnum.java",
          "core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkStageEnum.java||core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkStageEnum.java",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/SparderMetricsListener.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/SparderMetricsListener.scala",
          "server-base/src/main/java/org/apache/kylin/rest/metrics/QueryMetricsFacade.java||server-base/src/main/java/org/apache/kylin/rest/metrics/QueryMetricsFacade.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/CubeService.java||server-base/src/main/java/org/apache/kylin/rest/service/CubeService.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/DashboardService.java||server-base/src/main/java/org/apache/kylin/rest/service/DashboardService.java",
          "server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java||server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java",
          "server/src/test/java/org/apache/kylin/rest/metrics/QueryMetricsTest.java||server/src/test/java/org/apache/kylin/rest/metrics/QueryMetricsTest.java",
          "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeDescCreator.java||tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeDescCreator.java",
          "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeInstanceCreator.java||tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeInstanceCreator.java",
          "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/HiveTableCreator.java||tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/HiveTableCreator.java",
          "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/KylinTableCreator.java||tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/KylinTableCreator.java",
          "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/ModelCreator.java||tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/ModelCreator.java",
          "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/SCCreator.java||tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/SCCreator.java",
          "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/streamingv2/KafkaTopicCreator.java||tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/streamingv2/KafkaTopicCreator.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ],
          "candidate": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ]
        }
      },
      "candidate_diff": {
        "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "2349:                 + getKylinMetricsSubjectSuffix();",
          "2350:     }",
          "2354:     }",
          "2358:                 + getKylinMetricsSubjectSuffix();",
          "2359:     }",
          "2363:                 + getKylinMetricsSubjectSuffix();",
          "2364:     }",
          "2366:     public Map<String, String> getKylinMetricsConf() {",
          "2367:         return getPropertiesByPrefix(\"kylin.metrics.\");",
          "2368:     }",
          "",
          "[Removed Lines]",
          "2352:     public String getKylinMetricsSubjectQuery() {",
          "2353:         return getOptional(\"kylin.metrics.subject-query\", \"METRICS_QUERY\") + \"_\" + getKylinMetricsSubjectSuffix();",
          "2356:     public String getKylinMetricsSubjectQueryCube() {",
          "2357:         return getOptional(\"kylin.metrics.subject-query-cube\", \"METRICS_QUERY_CUBE\") + \"_\"",
          "2361:     public String getKylinMetricsSubjectQueryRpcCall() {",
          "2362:         return getOptional(\"kylin.metrics.subject-query-rpc\", \"METRICS_QUERY_RPC\") + \"_\"",
          "",
          "[Added Lines]",
          "2352:     public String getKylinMetricsSubjectQueryExecution() {",
          "2353:         return getOptional(\"kylin.metrics.subject-query\", \"METRICS_QUERY_EXECUTION\") + \"_\" + getKylinMetricsSubjectSuffix();",
          "2356:     public String getKylinMetricsSubjectQuerySparkJob() {",
          "2357:         return getOptional(\"kylin.metrics.subject-query-cube\", \"METRICS_QUERY_SPARK_JOB\") + \"_\"",
          "2361:     public String getKylinMetricsSubjectQuerySparkStage() {",
          "2362:         return getOptional(\"kylin.metrics.subject-query-rpc\", \"METRICS_QUERY_SPARK_STAGE\") + \"_\"",
          "2366:     public int getKylinMetricsCacheExpireSeconds() {",
          "2367:         return Integer.parseInt(this.getOptional(\"kylin.metrics.query-cache.expire-seconds\", \"600\"));",
          "2368:     }",
          "2370:     public int getKylinMetricsCacheMaxEntries() {",
          "2371:         return Integer.parseInt(this.getOptional(\"kylin.metrics.query-cache.max-entries\", \"10000\"));",
          "2372:     }",
          "",
          "---------------"
        ],
        "core-metrics/src/main/java/org/apache/kylin/metrics/QuerySparkMetrics.java||core-metrics/src/main/java/org/apache/kylin/metrics/QuerySparkMetrics.java": [
          "File: core-metrics/src/main/java/org/apache/kylin/metrics/QuerySparkMetrics.java -> core-metrics/src/main/java/org/apache/kylin/metrics/QuerySparkMetrics.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.metrics;",
          "21: import org.apache.kylin.common.KylinConfig;",
          "22: import org.apache.kylin.metrics.lib.impl.RecordEvent;",
          "23: import org.apache.kylin.metrics.lib.impl.TimedRecordEvent;",
          "24: import org.apache.kylin.metrics.property.QuerySparkExecutionEnum;",
          "25: import org.apache.kylin.metrics.property.QuerySparkJobEnum;",
          "26: import org.apache.kylin.metrics.property.QuerySparkStageEnum;",
          "27: import org.apache.kylin.shaded.com.google.common.cache.Cache;",
          "28: import org.apache.kylin.shaded.com.google.common.cache.CacheBuilder;",
          "29: import org.apache.kylin.shaded.com.google.common.cache.RemovalListener;",
          "30: import org.apache.kylin.shaded.com.google.common.cache.RemovalNotification;",
          "31: import org.apache.kylin.shaded.com.google.common.collect.Maps;",
          "32: import org.slf4j.Logger;",
          "33: import org.slf4j.LoggerFactory;",
          "35: import java.io.Serializable;",
          "36: import java.util.Map;",
          "37: import java.util.concurrent.ConcurrentMap;",
          "38: import java.util.concurrent.Executors;",
          "39: import java.util.concurrent.TimeUnit;",
          "41: public class QuerySparkMetrics {",
          "42:     private static final Logger logger = LoggerFactory.getLogger(QuerySparkMetrics.class);",
          "43:     private static final QuerySparkMetrics instance = new QuerySparkMetrics();",
          "44:     private static final int sparkMetricsNum = 10;",
          "45:     private org.apache.kylin.shaded.com.google.common.cache.Cache<String, QueryExecutionMetrics> queryExecutionMetricsMap;",
          "47:     private QuerySparkMetrics() {",
          "48:         queryExecutionMetricsMap = CacheBuilder.newBuilder()",
          "49:                 .maximumSize(KylinConfig.getInstanceFromEnv().getKylinMetricsCacheMaxEntries())",
          "50:                 .expireAfterWrite(KylinConfig.getInstanceFromEnv().getKylinMetricsCacheExpireSeconds(),",
          "51:                         TimeUnit.SECONDS)",
          "52:                 .removalListener(new RemovalListener<String, QueryExecutionMetrics>() {",
          "53:                     @Override",
          "54:                     public void onRemoval(RemovalNotification<String, QueryExecutionMetrics> notification) {",
          "55:                         try {",
          "56:                             updateMetricsToReservoir(notification.getKey(), notification.getValue());",
          "57:                             logger.info(\"Query metrics {} is removed due to {}, update to metrics reservoir successful\",",
          "58:                                     notification.getKey(), notification.getCause());",
          "59:                         } catch(Exception e) {",
          "60:                             logger.warn(\"Query metrics {} is removed due to {}, update to metrics reservoir failed\",",
          "61:                                     notification.getKey(), notification.getCause());",
          "62:                         }",
          "63:                     }",
          "64:                 }).build();",
          "66:         Executors.newSingleThreadScheduledExecutor().scheduleWithFixedDelay(new Runnable() {",
          "67:                                                                                 @Override",
          "68:                                                                                 public void run() {",
          "69:                                                                                     queryExecutionMetricsMap.cleanUp();",
          "70:                                                                                 }",
          "71:                                                                             },",
          "72:                 KylinConfig.getInstanceFromEnv().getKylinMetricsCacheExpireSeconds(),",
          "73:                 KylinConfig.getInstanceFromEnv().getKylinMetricsCacheExpireSeconds(), TimeUnit.SECONDS);",
          "75:     }",
          "77:     public static QuerySparkMetrics getInstance() {",
          "78:         return instance;",
          "79:     }",
          "81:     public void onJobStart(String queryId, String sparderName, long executionId, long executionStartTime, int jobId,",
          "82:             long jobStartTime) {",
          "83:         QueryExecutionMetrics queryExecutionMetrics = queryExecutionMetricsMap.getIfPresent(queryId);",
          "84:         if (queryExecutionMetrics == null) {",
          "85:             queryExecutionMetrics = new QueryExecutionMetrics();",
          "86:             ConcurrentMap<Integer, SparkJobMetrics> sparkJobMetricsMap = Maps.newConcurrentMap();",
          "87:             queryExecutionMetrics.setQueryId(queryId);",
          "88:             queryExecutionMetrics.setSparderName(sparderName);",
          "89:             queryExecutionMetrics.setExecutionId(executionId);",
          "90:             queryExecutionMetrics.setStartTime(executionStartTime);",
          "91:             queryExecutionMetrics.setSparkJobMetricsMap(sparkJobMetricsMap);",
          "92:             queryExecutionMetricsMap.put(queryId, queryExecutionMetrics);",
          "93:         }",
          "94:         SparkJobMetrics sparkJobMetrics = new SparkJobMetrics();",
          "95:         sparkJobMetrics.setExecutionId(executionId);",
          "96:         sparkJobMetrics.setJobId(jobId);",
          "97:         sparkJobMetrics.setStartTime(jobStartTime);",
          "99:         ConcurrentMap<Integer, SparkStageMetrics> sparkStageMetricsMap = Maps.newConcurrentMap();",
          "100:         sparkJobMetrics.setSparkStageMetricsMap(sparkStageMetricsMap);",
          "102:         queryExecutionMetrics.getSparkJobMetricsMap().put(jobId, sparkJobMetrics);",
          "103:     }",
          "105:     public void onSparkStageStart(String queryId, int jobId, int stageId, String stageType, long submitTime) {",
          "106:         QueryExecutionMetrics queryExecutionMetrics = queryExecutionMetricsMap.getIfPresent(queryId);",
          "107:         if (queryExecutionMetrics != null && queryExecutionMetrics.getSparkJobMetricsMap().get(jobId) != null) {",
          "108:             SparkStageMetrics sparkStageMetrics = new SparkStageMetrics();",
          "109:             sparkStageMetrics.setStageId(stageId);",
          "110:             sparkStageMetrics.setStageType(stageType);",
          "111:             sparkStageMetrics.setSubmitTime(submitTime);",
          "112:             queryExecutionMetrics.getSparkJobMetricsMap().get(jobId).getSparkStageMetricsMap().put(stageId,",
          "113:                     sparkStageMetrics);",
          "114:         }",
          "115:     }",
          "117:     public void updateSparkStageMetrics(String queryId, int jobId, int stageId, boolean isSuccess,",
          "118:             SparkStageMetrics sparkStageMetricsEnd) {",
          "119:         QueryExecutionMetrics queryExecutionMetrics = queryExecutionMetricsMap.getIfPresent(queryId);",
          "120:         if (queryExecutionMetrics != null) {",
          "121:             SparkJobMetrics sparkJobMetrics = queryExecutionMetrics.getSparkJobMetricsMap().get(jobId);",
          "122:             if (sparkJobMetrics != null) {",
          "123:                 SparkStageMetrics sparkStageMetrics = sparkJobMetrics.getSparkStageMetricsMap().get(stageId);",
          "124:                 if (sparkStageMetrics != null) {",
          "125:                     sparkStageMetrics.setSuccess(isSuccess);",
          "126:                     sparkStageMetrics.setMetrics(sparkStageMetricsEnd.getResultSize(),",
          "127:                             sparkStageMetricsEnd.getExecutorDeserializeTime(),",
          "128:                             sparkStageMetricsEnd.getExecutorDeserializeCpuTime(),",
          "129:                             sparkStageMetricsEnd.getExecutorRunTime(), sparkStageMetricsEnd.getExecutorCpuTime(),",
          "130:                             sparkStageMetricsEnd.getJvmGCTime(), sparkStageMetricsEnd.getResultSerializationTime(),",
          "131:                             sparkStageMetricsEnd.getMemoryBytesSpilled(), sparkStageMetricsEnd.getDiskBytesSpilled(),",
          "132:                             sparkStageMetricsEnd.getPeakExecutionMemory());",
          "133:                 }",
          "134:             }",
          "135:         }",
          "136:     }",
          "138:     public void updateSparkJobMetrics(String queryId, int jobId, long jobEndTime, boolean isSuccess) {",
          "139:         QueryExecutionMetrics queryExecutionMetrics = queryExecutionMetricsMap.getIfPresent(queryId);",
          "140:         if (queryExecutionMetrics != null && queryExecutionMetrics.getSparkJobMetricsMap().get(jobId) != null) {",
          "141:             SparkJobMetrics sparkJobMetrics = queryExecutionMetrics.getSparkJobMetricsMap().get(jobId);",
          "142:             sparkJobMetrics.setEndTime(jobEndTime);",
          "143:             sparkJobMetrics.setSuccess(isSuccess);",
          "144:         }",
          "145:     }",
          "147:     public void updateExecutionMetrics(String queryId, long executionEndTime) {",
          "148:         QueryExecutionMetrics queryExecutionMetrics = queryExecutionMetricsMap.getIfPresent(queryId);",
          "149:         if (queryExecutionMetrics != null) {",
          "150:             queryExecutionMetrics.setEndTime(executionEndTime);",
          "151:         }",
          "152:     }",
          "154:     public Cache<String, QueryExecutionMetrics> getQueryExecutionMetricsMap() {",
          "155:         return queryExecutionMetricsMap;",
          "156:     }",
          "158:     public QueryExecutionMetrics getQueryExecutionMetrics(String queryId) {",
          "159:         return queryExecutionMetricsMap.getIfPresent(queryId);",
          "160:     }",
          "162:     public void setQueryRealization(String queryId, String realizationName, int realizationType, String cuboidIds) {",
          "163:         QueryExecutionMetrics queryExecutionMetrics = queryExecutionMetricsMap.getIfPresent(queryId);",
          "164:         if (queryExecutionMetrics != null) {",
          "165:             queryExecutionMetrics.setRealization(realizationName);",
          "166:             queryExecutionMetrics.setRealizationType(realizationType);",
          "167:             queryExecutionMetrics.setCuboidIds(cuboidIds);",
          "168:         }",
          "169:     }",
          "174:     public void updateMetricsToReservoir(String queryId, QueryExecutionMetrics queryExecutionMetrics) {",
          "175:         if (!KylinConfig.getInstanceFromEnv().isKylinMetricsReporterForQueryEnabled()) {",
          "176:             return;",
          "177:         }",
          "178:         if (queryExecutionMetrics != null) {",
          "179:             RecordEvent queryExecutionMetricsEvent = new TimedRecordEvent(",
          "180:                     KylinConfig.getInstanceFromEnv().getKylinMetricsSubjectQueryExecution());",
          "182:             setQueryWrapper(queryExecutionMetricsEvent, queryExecutionMetrics.getUser(),",
          "183:                     queryExecutionMetrics.getSqlIdCode(), queryExecutionMetrics.getQueryType(),",
          "184:                     queryId, queryExecutionMetrics.getProject(),",
          "185:                     queryExecutionMetrics.getException());",
          "187:             setSparkExecutionWrapper(queryExecutionMetricsEvent, queryExecutionMetrics.getSparderName(),",
          "188:                     queryExecutionMetrics.getExecutionId(), queryExecutionMetrics.getRealization(),",
          "189:                     queryExecutionMetrics.getRealizationType(), queryExecutionMetrics.getCuboidIds(),",
          "190:                     queryExecutionMetrics.getStartTime(), queryExecutionMetrics.getEndTime());",
          "192:             setQueryMetrics(queryExecutionMetricsEvent, queryExecutionMetrics.getSqlDuration(),",
          "193:                     queryExecutionMetrics.getTotalScanCount(), queryExecutionMetrics.getTotalScanBytes(),",
          "194:                     queryExecutionMetrics.getResultCount());",
          "196:             long[] queryExecutionMetricsList = new long[sparkMetricsNum];",
          "197:             for (Map.Entry<Integer, QuerySparkMetrics.SparkJobMetrics> sparkJobMetricsEntry : queryExecutionMetrics",
          "198:                     .getSparkJobMetricsMap().entrySet()) {",
          "199:                 RecordEvent sparkJobMetricsEvent = new TimedRecordEvent(",
          "200:                         KylinConfig.getInstanceFromEnv().getKylinMetricsSubjectQuerySparkJob());",
          "202:                 setSparkJobWrapper(sparkJobMetricsEvent, queryExecutionMetrics.getProject(),",
          "203:                         queryId, queryExecutionMetrics.getExecutionId(),",
          "204:                         sparkJobMetricsEntry.getValue().getJobId(), sparkJobMetricsEntry.getValue().getStartTime(),",
          "205:                         sparkJobMetricsEntry.getValue().getEndTime(), sparkJobMetricsEntry.getValue().isSuccess());",
          "207:                 long[] sparkJobMetricsList = new long[sparkMetricsNum];",
          "208:                 for (Map.Entry<Integer, QuerySparkMetrics.SparkStageMetrics> sparkStageMetricsEntry : sparkJobMetricsEntry",
          "209:                         .getValue().getSparkStageMetricsMap().entrySet()) {",
          "210:                     RecordEvent sparkStageMetricsEvent = new TimedRecordEvent(",
          "211:                             KylinConfig.getInstanceFromEnv().getKylinMetricsSubjectQuerySparkStage());",
          "212:                     QuerySparkMetrics.SparkStageMetrics sparkStageMetrics = sparkStageMetricsEntry.getValue();",
          "213:                     setStageWrapper(sparkStageMetricsEvent, queryExecutionMetrics.getProject(), null,",
          "214:                             queryId, queryExecutionMetrics.getExecutionId(),",
          "215:                             sparkJobMetricsEntry.getValue().getJobId(), sparkStageMetrics.getStageId(),",
          "216:                             sparkStageMetrics.getSubmitTime(), sparkStageMetrics.isSuccess());",
          "217:                     setStageMetrics(sparkStageMetricsEvent, sparkStageMetrics.getResultSize(),",
          "218:                             sparkStageMetrics.getExecutorDeserializeTime(),",
          "219:                             sparkStageMetrics.getExecutorDeserializeCpuTime(), sparkStageMetrics.getExecutorRunTime(),",
          "220:                             sparkStageMetrics.getExecutorCpuTime(), sparkStageMetrics.getJvmGCTime(),",
          "221:                             sparkStageMetrics.getResultSerializationTime(), sparkStageMetrics.getMemoryBytesSpilled(),",
          "222:                             sparkStageMetrics.getDiskBytesSpilled(), sparkStageMetrics.getPeakExecutionMemory());",
          "224:                     MetricsManager.getInstance().update(sparkStageMetricsEvent);",
          "226:                     sparkJobMetricsList[0] += sparkStageMetrics.getResultSize();",
          "227:                     sparkJobMetricsList[1] += sparkStageMetrics.getExecutorDeserializeTime();",
          "228:                     sparkJobMetricsList[2] += sparkStageMetrics.getExecutorDeserializeCpuTime();",
          "229:                     sparkJobMetricsList[3] += sparkStageMetrics.getExecutorRunTime();",
          "230:                     sparkJobMetricsList[4] += sparkStageMetrics.getExecutorCpuTime();",
          "231:                     sparkJobMetricsList[5] += sparkStageMetrics.getJvmGCTime();",
          "232:                     sparkJobMetricsList[6] += sparkStageMetrics.getResultSerializationTime();",
          "233:                     sparkJobMetricsList[7] += sparkStageMetrics.getMemoryBytesSpilled();",
          "234:                     sparkJobMetricsList[8] += sparkStageMetrics.getDiskBytesSpilled();",
          "235:                     sparkJobMetricsList[9] += sparkStageMetrics.getPeakExecutionMemory();",
          "236:                 }",
          "237:                 setSparkJobMetrics(sparkJobMetricsEvent, sparkJobMetricsList[0], sparkJobMetricsList[1],",
          "238:                         sparkJobMetricsList[2], sparkJobMetricsList[3], sparkJobMetricsList[4], sparkJobMetricsList[5],",
          "239:                         sparkJobMetricsList[6], sparkJobMetricsList[7], sparkJobMetricsList[8], sparkJobMetricsList[9]);",
          "241:                 MetricsManager.getInstance().update(sparkJobMetricsEvent);",
          "243:                 for (int i = 0; i < sparkMetricsNum; i++) {",
          "244:                     queryExecutionMetricsList[i] += sparkJobMetricsList[i];",
          "245:                 }",
          "246:             }",
          "247:             setSparkExecutionMetrics(queryExecutionMetricsEvent, queryExecutionMetrics.getExecutionDuration(),",
          "248:                     queryExecutionMetricsList[0], queryExecutionMetricsList[1], queryExecutionMetricsList[2],",
          "249:                     queryExecutionMetricsList[3], queryExecutionMetricsList[4], queryExecutionMetricsList[5],",
          "250:                     queryExecutionMetricsList[6], queryExecutionMetricsList[7], queryExecutionMetricsList[8],",
          "251:                     queryExecutionMetricsList[9]);",
          "253:             MetricsManager.getInstance().update(queryExecutionMetricsEvent);",
          "254:         }",
          "255:     }",
          "257:     private static void setQueryWrapper(RecordEvent metricsEvent, String user, long sqlIdCode, String queryType,",
          "258:             String queryId, String project, String exception) {",
          "259:         metricsEvent.put(QuerySparkExecutionEnum.USER.toString(), user);",
          "260:         metricsEvent.put(QuerySparkExecutionEnum.ID_CODE.toString(), sqlIdCode);",
          "261:         metricsEvent.put(QuerySparkExecutionEnum.TYPE.toString(), queryType);",
          "262:         metricsEvent.put(QuerySparkExecutionEnum.QUERY_ID.toString(), queryId);",
          "263:         metricsEvent.put(QuerySparkExecutionEnum.PROJECT.toString(), project);",
          "264:         metricsEvent.put(QuerySparkExecutionEnum.EXCEPTION.toString(), exception);",
          "265:     }",
          "267:     private static void setSparkExecutionWrapper(RecordEvent metricsEvent, String sparderName, long executionId,",
          "268:             String realizationName, int realizationType, String cuboidIds, long startTime, long endTime) {",
          "269:         metricsEvent.put(QuerySparkExecutionEnum.SPARDER_NAME.toString(), sparderName);",
          "270:         metricsEvent.put(QuerySparkExecutionEnum.EXECUTION_ID.toString(), executionId);",
          "271:         metricsEvent.put(QuerySparkExecutionEnum.REALIZATION.toString(), realizationName);",
          "272:         metricsEvent.put(QuerySparkExecutionEnum.REALIZATION_TYPE.toString(), realizationType);",
          "273:         metricsEvent.put(QuerySparkExecutionEnum.CUBOID_IDS.toString(), cuboidIds);",
          "274:         metricsEvent.put(QuerySparkExecutionEnum.START_TIME.toString(), startTime);",
          "275:         metricsEvent.put(QuerySparkExecutionEnum.END_TIME.toString(), endTime);",
          "276:     }",
          "278:     private static void setQueryMetrics(RecordEvent metricsEvent, long sqlDuration, long totalScanCount,",
          "279:             long totalScanBytes, long resultCount) {",
          "280:         metricsEvent.put(QuerySparkExecutionEnum.TIME_COST.toString(), sqlDuration);",
          "281:         metricsEvent.put(QuerySparkExecutionEnum.TOTAL_SCAN_COUNT.toString(), totalScanCount);",
          "282:         metricsEvent.put(QuerySparkExecutionEnum.TOTAL_SCAN_BYTES.toString(), totalScanBytes);",
          "283:         metricsEvent.put(QuerySparkExecutionEnum.RESULT_COUNT.toString(), resultCount);",
          "284:     }",
          "286:     private static void setSparkExecutionMetrics(RecordEvent metricsEvent, long executionDuration, long resultSize,",
          "287:             long executorDeserializeTime, long executorDeserializeCpuTime, long executorRunTime, long executorCpuTime,",
          "288:             long jvmGCTime, long resultSerializationTime, long memoryBytesSpilled, long diskBytesSpilled,",
          "289:             long peakExecutionMemory) {",
          "290:         metricsEvent.put(QuerySparkExecutionEnum.EXECUTION_DURATION.toString(), executionDuration);",
          "292:         metricsEvent.put(QuerySparkExecutionEnum.RESULT_SIZE.toString(), resultSize);",
          "293:         metricsEvent.put(QuerySparkExecutionEnum.EXECUTOR_DESERIALIZE_TIME.toString(), executorDeserializeTime);",
          "294:         metricsEvent.put(QuerySparkExecutionEnum.EXECUTOR_DESERIALIZE_CPU_TIME.toString(), executorDeserializeCpuTime);",
          "295:         metricsEvent.put(QuerySparkExecutionEnum.EXECUTOR_RUN_TIME.toString(), executorRunTime);",
          "296:         metricsEvent.put(QuerySparkExecutionEnum.EXECUTOR_CPU_TIME.toString(), executorCpuTime);",
          "297:         metricsEvent.put(QuerySparkExecutionEnum.JVM_GC_TIME.toString(), jvmGCTime);",
          "298:         metricsEvent.put(QuerySparkExecutionEnum.RESULT_SERIALIZATION_TIME.toString(), resultSerializationTime);",
          "299:         metricsEvent.put(QuerySparkExecutionEnum.MEMORY_BYTE_SPILLED.toString(), memoryBytesSpilled);",
          "300:         metricsEvent.put(QuerySparkExecutionEnum.DISK_BYTES_SPILLED.toString(), diskBytesSpilled);",
          "301:         metricsEvent.put(QuerySparkExecutionEnum.PEAK_EXECUTION_MEMORY.toString(), peakExecutionMemory);",
          "302:     }",
          "304:     private static void setSparkJobMetrics(RecordEvent metricsEvent, long resultSize, long executorDeserializeTime,",
          "305:             long executorDeserializeCpuTime, long executorRunTime, long executorCpuTime, long jvmGCTime,",
          "306:             long resultSerializationTime, long memoryBytesSpilled, long diskBytesSpilled, long peakExecutionMemory) {",
          "307:         metricsEvent.put(QuerySparkJobEnum.RESULT_SIZE.toString(), resultSize);",
          "308:         metricsEvent.put(QuerySparkJobEnum.EXECUTOR_DESERIALIZE_TIME.toString(), executorDeserializeTime);",
          "309:         metricsEvent.put(QuerySparkJobEnum.EXECUTOR_DESERIALIZE_CPU_TIME.toString(), executorDeserializeCpuTime);",
          "310:         metricsEvent.put(QuerySparkJobEnum.EXECUTOR_RUN_TIME.toString(), executorRunTime);",
          "311:         metricsEvent.put(QuerySparkJobEnum.EXECUTOR_CPU_TIME.toString(), executorCpuTime);",
          "312:         metricsEvent.put(QuerySparkJobEnum.JVM_GC_TIME.toString(), jvmGCTime);",
          "313:         metricsEvent.put(QuerySparkJobEnum.RESULT_SERIALIZATION_TIME.toString(), resultSerializationTime);",
          "314:         metricsEvent.put(QuerySparkJobEnum.MEMORY_BYTE_SPILLED.toString(), memoryBytesSpilled);",
          "315:         metricsEvent.put(QuerySparkJobEnum.DISK_BYTES_SPILLED.toString(), diskBytesSpilled);",
          "316:         metricsEvent.put(QuerySparkJobEnum.PEAK_EXECUTION_MEMORY.toString(), peakExecutionMemory);",
          "317:     }",
          "319:     private static void setStageMetrics(RecordEvent metricsEvent, long resultSize, long executorDeserializeTime,",
          "320:             long executorDeserializeCpuTime, long executorRunTime, long executorCpuTime, long jvmGCTime,",
          "321:             long resultSerializationTime, long memoryBytesSpilled, long diskBytesSpilled, long peakExecutionMemory) {",
          "322:         metricsEvent.put(QuerySparkStageEnum.RESULT_SIZE.toString(), resultSize);",
          "323:         metricsEvent.put(QuerySparkStageEnum.EXECUTOR_DESERIALIZE_TIME.toString(), executorDeserializeTime);",
          "324:         metricsEvent.put(QuerySparkStageEnum.EXECUTOR_DESERIALIZE_CPU_TIME.toString(), executorDeserializeCpuTime);",
          "325:         metricsEvent.put(QuerySparkStageEnum.EXECUTOR_RUN_TIME.toString(), executorRunTime);",
          "326:         metricsEvent.put(QuerySparkStageEnum.EXECUTOR_CPU_TIME.toString(), executorCpuTime);",
          "327:         metricsEvent.put(QuerySparkStageEnum.JVM_GC_TIME.toString(), jvmGCTime);",
          "328:         metricsEvent.put(QuerySparkStageEnum.RESULT_SERIALIZATION_TIME.toString(), resultSerializationTime);",
          "329:         metricsEvent.put(QuerySparkStageEnum.MEMORY_BYTE_SPILLED.toString(), memoryBytesSpilled);",
          "330:         metricsEvent.put(QuerySparkStageEnum.DISK_BYTES_SPILLED.toString(), diskBytesSpilled);",
          "331:         metricsEvent.put(QuerySparkStageEnum.PEAK_EXECUTION_MEMORY.toString(), peakExecutionMemory);",
          "332:     }",
          "334:     private static void setStageWrapper(RecordEvent metricsEvent, String projectName, String realizationName,",
          "335:             String queryId, long executionId, int jobId, int stageId, long submitTime, boolean isSuccess) {",
          "336:         metricsEvent.put(QuerySparkStageEnum.PROJECT.toString(), projectName);",
          "337:         metricsEvent.put(QuerySparkStageEnum.REALIZATION.toString(), realizationName);",
          "338:         metricsEvent.put(QuerySparkStageEnum.QUERY_ID.toString(), queryId);",
          "339:         metricsEvent.put(QuerySparkStageEnum.EXECUTION_ID.toString(), executionId);",
          "340:         metricsEvent.put(QuerySparkStageEnum.JOB_ID.toString(), jobId);",
          "341:         metricsEvent.put(QuerySparkStageEnum.STAGE_ID.toString(), stageId);",
          "342:         metricsEvent.put(QuerySparkStageEnum.SUBMIT_TIME.toString(), submitTime);",
          "343:         metricsEvent.put(QuerySparkStageEnum.IF_SUCCESS.toString(), isSuccess);",
          "344:     }",
          "346:     private static void setSparkJobWrapper(RecordEvent metricsEvent, String projectName, String queryId,",
          "347:             long executionId, int jobId, long startTime, long endTime, boolean isSuccess) {",
          "348:         metricsEvent.put(QuerySparkJobEnum.PROJECT.toString(), projectName);",
          "349:         metricsEvent.put(QuerySparkJobEnum.QUERY_ID.toString(), queryId);",
          "350:         metricsEvent.put(QuerySparkJobEnum.EXECUTION_ID.toString(), executionId);",
          "351:         metricsEvent.put(QuerySparkJobEnum.JOB_ID.toString(), jobId);",
          "352:         metricsEvent.put(QuerySparkJobEnum.START_TIME.toString(), startTime);",
          "353:         metricsEvent.put(QuerySparkJobEnum.END_TIME.toString(), endTime);",
          "354:         metricsEvent.put(QuerySparkJobEnum.IF_SUCCESS.toString(), isSuccess);",
          "355:     }",
          "357:     public static class QueryExecutionMetrics implements Serializable {",
          "358:         private long sqlIdCode;",
          "359:         private String user;",
          "360:         private String queryType;",
          "361:         private String project;",
          "362:         private String exception;",
          "363:         private long executionId;",
          "364:         private String sparderName;",
          "365:         private long executionDuration;",
          "366:         private String queryId;",
          "367:         private String realization;",
          "368:         private int realizationType;",
          "369:         private String cuboidIds;",
          "370:         private long startTime;",
          "371:         private long endTime;",
          "372:         private ConcurrentMap<Integer, SparkJobMetrics> sparkJobMetricsMap;",
          "374:         private long sqlDuration;",
          "375:         private long totalScanCount;",
          "376:         private long totalScanBytes;",
          "377:         private int resultCount;",
          "379:         public String getUser() {",
          "380:             return user;",
          "381:         }",
          "383:         public void setUser(String user) {",
          "384:             this.user = user;",
          "385:         }",
          "387:         public int getResultCount() {",
          "388:             return resultCount;",
          "389:         }",
          "391:         public long getSqlDuration() {",
          "392:             return sqlDuration;",
          "393:         }",
          "395:         public long getTotalScanBytes() {",
          "396:             return totalScanBytes;",
          "397:         }",
          "399:         public long getTotalScanCount() {",
          "400:             return totalScanCount;",
          "401:         }",
          "403:         public void setResultCount(int resultCount) {",
          "404:             this.resultCount = resultCount;",
          "405:         }",
          "407:         public void setSqlDuration(long sqlDuration) {",
          "408:             this.sqlDuration = sqlDuration;",
          "409:         }",
          "411:         public void setTotalScanBytes(long totalScanBytes) {",
          "412:             this.totalScanBytes = totalScanBytes;",
          "413:         }",
          "415:         public void setTotalScanCount(long totalScanCount) {",
          "416:             this.totalScanCount = totalScanCount;",
          "417:         }",
          "419:         public String getException() {",
          "420:             return exception;",
          "421:         }",
          "423:         public void setException(String exception) {",
          "424:             this.exception = exception;",
          "425:         }",
          "427:         public void setProject(String project) {",
          "428:             this.project = project;",
          "429:         }",
          "431:         public String getProject() {",
          "432:             return project;",
          "433:         }",
          "435:         public String getQueryType() {",
          "436:             return queryType;",
          "437:         }",
          "439:         public long getSqlIdCode() {",
          "440:             return sqlIdCode;",
          "441:         }",
          "443:         public void setQueryType(String queryType) {",
          "444:             this.queryType = queryType;",
          "445:         }",
          "447:         public void setSqlIdCode(long sqlIdCode) {",
          "448:             this.sqlIdCode = sqlIdCode;",
          "449:         }",
          "451:         public long getEndTime() {",
          "452:             return endTime;",
          "453:         }",
          "455:         public long getStartTime() {",
          "456:             return startTime;",
          "457:         }",
          "459:         public void setEndTime(long endTime) {",
          "460:             this.endTime = endTime;",
          "461:         }",
          "463:         public void setStartTime(long startTime) {",
          "464:             this.startTime = startTime;",
          "465:         }",
          "467:         public void setQueryId(String queryId) {",
          "468:             this.queryId = queryId;",
          "469:         }",
          "471:         public String getQueryId() {",
          "472:             return queryId;",
          "473:         }",
          "475:         public long getExecutionDuration() {",
          "476:             return executionDuration;",
          "477:         }",
          "479:         public void setExecutionDuration(long executionDuration) {",
          "480:             this.executionDuration = executionDuration;",
          "481:         }",
          "483:         public ConcurrentMap<Integer, SparkJobMetrics> getSparkJobMetricsMap() {",
          "484:             return sparkJobMetricsMap;",
          "485:         }",
          "487:         public long getExecutionId() {",
          "488:             return executionId;",
          "489:         }",
          "491:         public String getSparderName() {",
          "492:             return sparderName;",
          "493:         }",
          "495:         public void setExecutionId(long executionId) {",
          "496:             this.executionId = executionId;",
          "497:         }",
          "499:         public void setSparderName(String sparderName) {",
          "500:             this.sparderName = sparderName;",
          "501:         }",
          "503:         public String getCuboidIds() {",
          "504:             return cuboidIds;",
          "505:         }",
          "507:         public void setCuboidIds(String cuboidIds) {",
          "508:             this.cuboidIds = cuboidIds;",
          "509:         }",
          "511:         public String getRealization() {",
          "512:             return realization;",
          "513:         }",
          "515:         public int getRealizationType() {",
          "516:             return realizationType;",
          "517:         }",
          "519:         public void setRealization(String realization) {",
          "520:             this.realization = realization;",
          "521:         }",
          "523:         public void setRealizationType(int realizationType) {",
          "524:             this.realizationType = realizationType;",
          "525:         }",
          "527:         public void setSparkJobMetricsMap(ConcurrentMap<Integer, SparkJobMetrics> sparkJobMetricsMap) {",
          "528:             this.sparkJobMetricsMap = sparkJobMetricsMap;",
          "529:         }",
          "530:     }",
          "532:     public static class SparkJobMetrics implements Serializable {",
          "533:         private long executionId;",
          "534:         private int jobId;",
          "535:         private long startTime;",
          "536:         private long endTime;",
          "537:         private boolean isSuccess;",
          "538:         private ConcurrentMap<Integer, SparkStageMetrics> sparkStageMetricsMap;",
          "540:         public void setStartTime(long startTime) {",
          "541:             this.startTime = startTime;",
          "542:         }",
          "544:         public void setEndTime(long endTime) {",
          "545:             this.endTime = endTime;",
          "546:         }",
          "548:         public long getStartTime() {",
          "549:             return startTime;",
          "550:         }",
          "552:         public long getEndTime() {",
          "553:             return endTime;",
          "554:         }",
          "556:         public void setExecutionId(long executionId) {",
          "557:             this.executionId = executionId;",
          "558:         }",
          "560:         public long getExecutionId() {",
          "561:             return executionId;",
          "562:         }",
          "564:         public void setSparkStageMetricsMap(ConcurrentMap<Integer, SparkStageMetrics> sparkStageMetricsMap) {",
          "565:             this.sparkStageMetricsMap = sparkStageMetricsMap;",
          "566:         }",
          "568:         public void setJobId(int jobId) {",
          "569:             this.jobId = jobId;",
          "570:         }",
          "572:         public void setSuccess(boolean success) {",
          "573:             isSuccess = success;",
          "574:         }",
          "576:         public boolean isSuccess() {",
          "577:             return isSuccess;",
          "578:         }",
          "580:         public ConcurrentMap<Integer, SparkStageMetrics> getSparkStageMetricsMap() {",
          "581:             return sparkStageMetricsMap;",
          "582:         }",
          "584:         public int getJobId() {",
          "585:             return jobId;",
          "586:         }",
          "587:     }",
          "589:     public static class SparkStageMetrics implements Serializable {",
          "590:         private int stageId;",
          "591:         private String stageType;",
          "592:         private long submitTime;",
          "593:         private long endTime;",
          "594:         private boolean isSuccess;",
          "595:         private long resultSize;",
          "596:         private long executorDeserializeTime;",
          "597:         private long executorDeserializeCpuTime;",
          "598:         private long executorRunTime;",
          "599:         private long executorCpuTime;",
          "600:         private long jvmGCTime;",
          "601:         private long resultSerializationTime;",
          "602:         private long memoryBytesSpilled;",
          "603:         private long diskBytesSpilled;",
          "604:         private long peakExecutionMemory;",
          "606:         public void setMetrics(long resultSize, long executorDeserializeTime, long executorDeserializeCpuTime,",
          "607:                 long executorRunTime, long executorCpuTime, long jvmGCTime, long resultSerializationTime,",
          "608:                 long memoryBytesSpilled, long diskBytesSpilled, long peakExecutionMemory) {",
          "609:             this.resultSize = resultSize;",
          "610:             this.executorDeserializeTime = executorDeserializeTime;",
          "611:             this.executorDeserializeCpuTime = executorDeserializeCpuTime;",
          "612:             this.executorRunTime = executorRunTime;",
          "613:             this.executorCpuTime = executorCpuTime;",
          "614:             this.jvmGCTime = jvmGCTime;",
          "615:             this.resultSerializationTime = resultSerializationTime;",
          "616:             this.memoryBytesSpilled = memoryBytesSpilled;",
          "617:             this.diskBytesSpilled = diskBytesSpilled;",
          "618:             this.peakExecutionMemory = peakExecutionMemory;",
          "619:         }",
          "621:         public long getEndTime() {",
          "622:             return endTime;",
          "623:         }",
          "625:         public long getSubmitTime() {",
          "626:             return submitTime;",
          "627:         }",
          "629:         public void setEndTime(long endTime) {",
          "630:             this.endTime = endTime;",
          "631:         }",
          "633:         public void setSubmitTime(long submitTime) {",
          "634:             this.submitTime = submitTime;",
          "635:         }",
          "637:         public boolean isSuccess() {",
          "638:             return isSuccess;",
          "639:         }",
          "641:         public void setSuccess(boolean success) {",
          "642:             isSuccess = success;",
          "643:         }",
          "645:         public void setStageType(String stageType) {",
          "646:             this.stageType = stageType;",
          "647:         }",
          "649:         public void setStageId(int stageId) {",
          "650:             this.stageId = stageId;",
          "651:         }",
          "653:         public void setResultSize(long resultSize) {",
          "654:             this.resultSize = resultSize;",
          "655:         }",
          "657:         public void setResultSerializationTime(long resultSerializationTime) {",
          "658:             this.resultSerializationTime = resultSerializationTime;",
          "659:         }",
          "661:         public void setPeakExecutionMemory(long peakExecutionMemory) {",
          "662:             this.peakExecutionMemory = peakExecutionMemory;",
          "663:         }",
          "665:         public void setMemoryBytesSpilled(long memoryBytesSpilled) {",
          "666:             this.memoryBytesSpilled = memoryBytesSpilled;",
          "667:         }",
          "669:         public void setJvmGCTime(long jvmGCTime) {",
          "670:             this.jvmGCTime = jvmGCTime;",
          "671:         }",
          "673:         public void setExecutorRunTime(long executorRunTime) {",
          "674:             this.executorRunTime = executorRunTime;",
          "675:         }",
          "677:         public void setExecutorDeserializeTime(long executorDeserializeTime) {",
          "678:             this.executorDeserializeTime = executorDeserializeTime;",
          "679:         }",
          "681:         public void setExecutorDeserializeCpuTime(long executorDeserializeCpuTime) {",
          "682:             this.executorDeserializeCpuTime = executorDeserializeCpuTime;",
          "683:         }",
          "685:         public void setExecutorCpuTime(long executorCpuTime) {",
          "686:             this.executorCpuTime = executorCpuTime;",
          "687:         }",
          "689:         public void setDiskBytesSpilled(long diskBytesSpilled) {",
          "690:             this.diskBytesSpilled = diskBytesSpilled;",
          "691:         }",
          "693:         public String getStageType() {",
          "694:             return stageType;",
          "695:         }",
          "697:         public long getResultSize() {",
          "698:             return resultSize;",
          "699:         }",
          "701:         public long getResultSerializationTime() {",
          "702:             return resultSerializationTime;",
          "703:         }",
          "705:         public long getPeakExecutionMemory() {",
          "706:             return peakExecutionMemory;",
          "707:         }",
          "709:         public long getMemoryBytesSpilled() {",
          "710:             return memoryBytesSpilled;",
          "711:         }",
          "713:         public long getJvmGCTime() {",
          "714:             return jvmGCTime;",
          "715:         }",
          "717:         public long getExecutorRunTime() {",
          "718:             return executorRunTime;",
          "719:         }",
          "721:         public long getExecutorDeserializeTime() {",
          "722:             return executorDeserializeTime;",
          "723:         }",
          "725:         public long getExecutorDeserializeCpuTime() {",
          "726:             return executorDeserializeCpuTime;",
          "727:         }",
          "729:         public long getExecutorCpuTime() {",
          "730:             return executorCpuTime;",
          "731:         }",
          "733:         public long getDiskBytesSpilled() {",
          "734:             return diskBytesSpilled;",
          "735:         }",
          "737:         public int getStageId() {",
          "738:             return stageId;",
          "739:         }",
          "740:     }",
          "741: }",
          "",
          "---------------"
        ],
        "core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkExecutionEnum.java||core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkExecutionEnum.java": [
          "File: core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkExecutionEnum.java -> core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkExecutionEnum.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.metrics.property;",
          "21: import org.apache.kylin.shaded.com.google.common.base.Strings;",
          "23: public enum QuerySparkExecutionEnum {",
          "24:     ID_CODE(\"QUERY_HASH_CODE\"),",
          "25:     SQL(\"QUERY_SQL\"),",
          "26:     PROJECT(\"PROJECT\"),",
          "27:     TYPE(\"QUERY_TYPE\"),",
          "28:     REALIZATION(\"REALIZATION\"),",
          "29:     REALIZATION_TYPE(\"REALIZATION_TYPE\"),",
          "30:     CUBOID_IDS(\"CUBOID_IDS\"),",
          "31:     QUERY_ID(\"QUERY_ID\"),",
          "32:     EXECUTION_ID(\"EXECUTION_ID\"),",
          "33:     USER(\"KUSER\"),",
          "34:     SPARDER_NAME(\"SPARDER_NAME\"),",
          "35:     EXCEPTION(\"EXCEPTION\"),",
          "36:     START_TIME(\"START_TIME\"),",
          "37:     END_TIME(\"END_TIME\"),",
          "40:     TIME_COST(\"QUERY_TIME_COST\"),",
          "41:     TOTAL_SCAN_COUNT(\"TOTAL_SCAN_COUNT\"),",
          "42:     TOTAL_SCAN_BYTES(\"TOTAL_SCAN_BYTES\"),",
          "43:     RESULT_COUNT(\"RESULT_COUNT\"),",
          "45:     EXECUTION_DURATION(\"EXECUTION_DURATION\"),",
          "46:     RESULT_SIZE(\"RESULT_SIZE\"),",
          "47:     EXECUTOR_DESERIALIZE_TIME(\"EXECUTOR_DESERIALIZE_TIME\"),",
          "48:     EXECUTOR_DESERIALIZE_CPU_TIME(\"EXECUTOR_DESERIALIZE_CPU_TIME\"),",
          "49:     EXECUTOR_RUN_TIME(\"EXECUTOR_RUN_TIME\"),",
          "50:     EXECUTOR_CPU_TIME(\"EXECUTOR_CPU_TIME\"),",
          "51:     JVM_GC_TIME(\"JVM_GC_TIME\"),",
          "52:     RESULT_SERIALIZATION_TIME(\"RESULT_SERIALIZATION_TIME\"),",
          "53:     MEMORY_BYTE_SPILLED(\"MEMORY_BYTE_SPILLED\"),",
          "54:     DISK_BYTES_SPILLED(\"DISK_BYTES_SPILLED\"),",
          "55:     PEAK_EXECUTION_MEMORY(\"PEAK_EXECUTION_MEMORY\");",
          "57:     private final String propertyName;",
          "59:     QuerySparkExecutionEnum(String name) {",
          "60:         this.propertyName = name;",
          "61:     }",
          "63:     public static QuerySparkExecutionEnum getByName(String name) {",
          "64:         if (Strings.isNullOrEmpty(name)) {",
          "65:             throw new IllegalArgumentException(\"Name should not be empty\");",
          "66:         }",
          "67:         for (QuerySparkExecutionEnum property : QuerySparkExecutionEnum.values()) {",
          "68:             if (property.propertyName.equalsIgnoreCase(name)) {",
          "69:                 return property;",
          "70:             }",
          "71:         }",
          "73:         return null;",
          "74:     }",
          "76:     @Override",
          "77:     public String toString() {",
          "78:         return propertyName;",
          "79:     }",
          "80: }",
          "",
          "---------------"
        ],
        "core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkJobEnum.java||core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkJobEnum.java": [
          "File: core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkJobEnum.java -> core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkJobEnum.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.metrics.property;",
          "21: import org.apache.kylin.shaded.com.google.common.base.Strings;",
          "23: public enum QuerySparkJobEnum {",
          "24:     PROJECT(\"PROJECT\"),",
          "25:     QUERY_ID(\"QUERY_ID\"),",
          "26:     EXECUTION_ID(\"EXECUTION_ID\"),",
          "27:     JOB_ID(\"JOB_ID\"),",
          "28:     START_TIME(\"START_TIME\"),",
          "29:     END_TIME(\"END_TIME\"),",
          "30:     IF_SUCCESS(\"IF_SUCCESS\"),",
          "32:     RESULT_SIZE(\"RESULT_SIZE\"),",
          "33:     EXECUTOR_DESERIALIZE_TIME(\"EXECUTOR_DESERIALIZE_TIME\"),",
          "34:     EXECUTOR_DESERIALIZE_CPU_TIME(\"EXECUTOR_DESERIALIZE_CPU_TIME\"),",
          "35:     EXECUTOR_RUN_TIME(\"EXECUTOR_RUN_TIME\"),",
          "36:     EXECUTOR_CPU_TIME(\"EXECUTOR_CPU_TIME\"),",
          "37:     JVM_GC_TIME(\"JVM_GC_TIME\"),",
          "38:     RESULT_SERIALIZATION_TIME(\"RESULT_SERIALIZATION_TIME\"),",
          "39:     MEMORY_BYTE_SPILLED(\"MEMORY_BYTE_SPILLED\"),",
          "40:     DISK_BYTES_SPILLED(\"DISK_BYTES_SPILLED\"),",
          "41:     PEAK_EXECUTION_MEMORY(\"PEAK_EXECUTION_MEMORY\");",
          "43:     private final String propertyName;",
          "45:     QuerySparkJobEnum(String name) {",
          "46:         this.propertyName = name;",
          "47:     }",
          "49:     public static QuerySparkJobEnum getByName(String name) {",
          "50:         if (Strings.isNullOrEmpty(name)) {",
          "51:             throw new IllegalArgumentException(\"Name should not be empty\");",
          "52:         }",
          "53:         for (QuerySparkJobEnum property : QuerySparkJobEnum.values()) {",
          "54:             if (property.propertyName.equalsIgnoreCase(name)) {",
          "55:                 return property;",
          "56:             }",
          "57:         }",
          "59:         return null;",
          "60:     }",
          "62:     @Override",
          "63:     public String toString() {",
          "64:         return propertyName;",
          "65:     }",
          "66: }",
          "",
          "---------------"
        ],
        "core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkStageEnum.java||core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkStageEnum.java": [
          "File: core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkStageEnum.java -> core-metrics/src/main/java/org/apache/kylin/metrics/property/QuerySparkStageEnum.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.kylin.metrics.property;",
          "21: import org.apache.kylin.shaded.com.google.common.base.Strings;",
          "26: public enum QuerySparkStageEnum {",
          "27:     PROJECT(\"PROJECT\"),",
          "28:     QUERY_ID(\"QUERY_ID\"),",
          "29:     EXECUTION_ID(\"EXECUTION_ID\"),",
          "30:     JOB_ID(\"JOB_ID\"),",
          "31:     STAGE_ID(\"STAGE_ID\"),",
          "32:     SUBMIT_TIME(\"SUBMIT_TIME\"),",
          "33:     REALIZATION(\"REALIZATION\"),",
          "34:     CUBOID_ID(\"CUBOID_NAME\"),",
          "35:     IF_SUCCESS(\"IF_SUCCESS\"),",
          "37:     RESULT_SIZE(\"RESULT_SIZE\"),",
          "38:     EXECUTOR_DESERIALIZE_TIME(\"EXECUTOR_DESERIALIZE_TIME\"),",
          "39:     EXECUTOR_DESERIALIZE_CPU_TIME(\"EXECUTOR_DESERIALIZE_CPU_TIME\"),",
          "40:     EXECUTOR_RUN_TIME(\"EXECUTOR_RUN_TIME\"),",
          "41:     EXECUTOR_CPU_TIME(\"EXECUTOR_CPU_TIME\"),",
          "42:     JVM_GC_TIME(\"JVM_GC_TIME\"),",
          "43:     RESULT_SERIALIZATION_TIME(\"RESULT_SERIALIZATION_TIME\"),",
          "44:     MEMORY_BYTE_SPILLED(\"MEMORY_BYTE_SPILLED\"),",
          "45:     DISK_BYTES_SPILLED(\"DISK_BYTES_SPILLED\"),",
          "46:     PEAK_EXECUTION_MEMORY(\"PEAK_EXECUTION_MEMORY\");",
          "48:     private final String propertyName;",
          "50:     QuerySparkStageEnum(String name) {",
          "51:         this.propertyName = name;",
          "52:     }",
          "54:     public static QuerySparkStageEnum getByName(String name) {",
          "55:         if (Strings.isNullOrEmpty(name)) {",
          "56:             throw new IllegalArgumentException(\"Name should not be empty\");",
          "57:         }",
          "58:         for (QuerySparkStageEnum property : QuerySparkStageEnum.values()) {",
          "59:             if (property.propertyName.equalsIgnoreCase(name)) {",
          "60:                 return property;",
          "61:             }",
          "62:         }",
          "64:         return null;",
          "65:     }",
          "67:     @Override",
          "68:     public String toString() {",
          "69:         return propertyName;",
          "70:     }",
          "71: }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala": [
          "File: kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala -> kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "37: import org.apache.kylin.spark.classloader.ClassLoaderUtils",
          "38: import org.apache.spark.{SparkConf, SparkContext, SparkEnv}",
          "39: import org.apache.spark.sql.execution.datasource.KylinSourceStrategy",
          "40: import org.apache.spark.utils.YarnInfoFetcherUtils",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "40: import org.apache.spark.sql.metrics.SparderMetricsListener",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "150:                     .enableHiveSupport()",
          "151:                     .getOrCreateKylinSession()",
          "152:               }",
          "153:               spark = sparkSession",
          "154:               val appid = sparkSession.sparkContext.applicationId",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "154:               if (kylinConf.isKylinMetricsReporterForQueryEnabled) {",
          "155:                 val appStatusListener = new SparderMetricsListener()",
          "156:                 sparkSession.sparkContext.addSparkListener(appStatusListener)",
          "157:                 logInfo(\"Query metrics reporter is enabled, sparder metrics listener is added.\")",
          "158:               }",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/SparderMetricsListener.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/SparderMetricsListener.scala": [
          "File: kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/SparderMetricsListener.scala -> kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/metrics/SparderMetricsListener.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: package org.apache.spark.sql.metrics",
          "21: import org.apache.kylin.metrics.QuerySparkMetrics",
          "22: import org.apache.spark.internal.Logging",
          "23: import org.apache.spark.scheduler._",
          "24: import org.apache.spark.sql.execution.SQLExecution",
          "25: import org.apache.spark.sql.execution.ui.{SparkListenerSQLExecutionEnd, SparkListenerSQLExecutionStart}",
          "27: class SparderMetricsListener() extends SparkListener with Logging {",
          "29:   var stageJobMap: Map[Int, Int] = Map()",
          "30:   var jobExecutionMap: Map[Int, QueryInformation] = Map()",
          "31:   var executionInformationMap: Map[Long, ExecutionInformation] = Map()",
          "33:   val queryExecutionMetrics = QuerySparkMetrics.getInstance()",
          "35:   override def onJobStart(event: SparkListenerJobStart): Unit = {",
          "36:     val executionIdString = event.properties.getProperty(SQLExecution.EXECUTION_ID_KEY)",
          "37:     val sparderName = event.properties.getProperty(\"spark.app.name\")",
          "38:     val kylinQueryId = event.properties.getProperty(\"kylin.query.id\")",
          "40:     if (executionIdString == null || kylinQueryId == null) {",
          "41:       logInfo(s\"The job ${event.jobId} is not a query job.\")",
          "42:       return",
          "43:     }",
          "45:     val executionId = executionIdString.toLong",
          "47:     if (executionInformationMap.apply(executionId).sparderName == null) {",
          "48:       val executionInformation = new ExecutionInformation(kylinQueryId,",
          "49:         executionInformationMap.apply(executionId).executionStartTime, sparderName)",
          "50:       executionInformationMap += (executionId -> executionInformation)",
          "51:     }",
          "53:     jobExecutionMap += (event.jobId -> new QueryInformation(kylinQueryId, executionId))",
          "55:     val stages = event.stageInfos.iterator",
          "56:     while (stages.hasNext) {",
          "57:       val stage: StageInfo = stages.next()",
          "58:       stageJobMap += (stage.stageId -> event.jobId)",
          "59:     }",
          "61:     queryExecutionMetrics.onJobStart(kylinQueryId, sparderName, executionId,",
          "62:       executionInformationMap.apply(executionId).executionStartTime, event.jobId, event.time)",
          "63:   }",
          "65:   override def onJobEnd(event: SparkListenerJobEnd): Unit = {",
          "66:     if (jobExecutionMap.contains(event.jobId)) {",
          "67:       val isSuccess = event.jobResult match {",
          "68:         case JobSucceeded => true",
          "69:         case _ => false",
          "70:       }",
          "71:       queryExecutionMetrics.updateSparkJobMetrics(jobExecutionMap.apply(event.jobId).queryId, event.jobId, event.time,",
          "72:         isSuccess)",
          "73:       logInfo(s\"The job ${event.jobId} has completed and the relevant metrics are updated to the cache\")",
          "74:       jobExecutionMap -= event.jobId",
          "75:     }",
          "76:   }",
          "78:   override def onStageSubmitted(event: SparkListenerStageSubmitted): Unit = {",
          "79:     val queryId = event.properties.getProperty(\"kylin.query.id\")",
          "80:     val stageId = event.stageInfo.stageId",
          "82:     if (stageJobMap.contains(stageId)) {",
          "83:       val submitTime = event.stageInfo.submissionTime match {",
          "84:         case Some(x) => x",
          "85:         case None => -1",
          "86:       }",
          "87:       queryExecutionMetrics.onSparkStageStart(queryId, stageJobMap.apply(stageId), stageId, event.stageInfo.name, submitTime)",
          "88:     }",
          "89:   }",
          "91:   override def onStageCompleted(event: SparkListenerStageCompleted): Unit = {",
          "92:     val stageInfo = event.stageInfo",
          "93:     if (stageJobMap.contains(stageInfo.stageId) && jobExecutionMap.contains(stageJobMap.apply(stageInfo.stageId))) {",
          "94:       val isSuccess = stageInfo.getStatusString match {",
          "95:         case \"succeeded\" => true",
          "96:         case _ => false",
          "97:       }",
          "98:       val stageMetrics = stageInfo.taskMetrics",
          "99:       val sparkStageMetrics = new QuerySparkMetrics.SparkStageMetrics",
          "100:       sparkStageMetrics.setMetrics(stageMetrics.resultSize, stageMetrics.executorDeserializeCpuTime,",
          "101:         stageMetrics.executorDeserializeCpuTime, stageMetrics.executorRunTime, stageMetrics.executorCpuTime,",
          "102:         stageMetrics.jvmGCTime, stageMetrics.resultSerializationTime,",
          "103:         stageMetrics.memoryBytesSpilled, stageMetrics.diskBytesSpilled, stageMetrics.peakExecutionMemory)",
          "104:       queryExecutionMetrics.updateSparkStageMetrics(jobExecutionMap.apply(stageJobMap.apply(stageInfo.stageId)).queryId,",
          "105:         stageJobMap.apply(stageInfo.stageId), stageInfo.stageId, isSuccess, sparkStageMetrics)",
          "106:       stageJobMap -= stageInfo.stageId",
          "108:       logInfo(s\"The stage ${event.stageInfo.stageId} has completed and the relevant metrics are updated to the cache\")",
          "109:     }",
          "110:   }",
          "112:   override def onOtherEvent(event: SparkListenerEvent): Unit = {",
          "113:     event match {",
          "114:       case e: SparkListenerSQLExecutionStart => onQueryExecutionStart(e)",
          "115:       case e: SparkListenerSQLExecutionEnd => onQueryExecutionEnd(e)",
          "116:       case _ => // Ignore",
          "117:     }",
          "118:   }",
          "120:   private def onQueryExecutionStart(event: SparkListenerSQLExecutionStart): Unit = {",
          "121:     executionInformationMap += (event.executionId -> new ExecutionInformation(null, event.time, null))",
          "122:   }",
          "124:   private def onQueryExecutionEnd(event: SparkListenerSQLExecutionEnd): Unit = {",
          "125:     val executionInformation = executionInformationMap.apply(event.executionId)",
          "126:     queryExecutionMetrics.updateExecutionMetrics(executionInformation.queryId, event.time)",
          "127:     executionInformationMap -= event.executionId",
          "128:     logInfo(s\"QueryExecution ${event.executionId} is completed at ${event.time} \" +",
          "129:       s\"and the relevant metrics are updated to the cache\")",
          "130:   }",
          "131: }",
          "135: class ExecutionInformation(",
          "136:                             var queryId: String,",
          "137:                             var executionStartTime: Long,",
          "138:                             var sparderName: String",
          "139:                           )",
          "141: class QueryInformation(",
          "142:                         val queryId: String,",
          "143:                         val executionId: Long",
          "144:                       )",
          "",
          "---------------"
        ],
        "server-base/src/main/java/org/apache/kylin/rest/metrics/QueryMetricsFacade.java||server-base/src/main/java/org/apache/kylin/rest/metrics/QueryMetricsFacade.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/metrics/QueryMetricsFacade.java -> server-base/src/main/java/org/apache/kylin/rest/metrics/QueryMetricsFacade.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import java.nio.charset.Charset;",
          "22: import java.util.Locale;",
          "24: import java.util.concurrent.ConcurrentHashMap;",
          "26: import javax.annotation.concurrent.ThreadSafe;",
          "",
          "[Removed Lines]",
          "23: import java.util.Map;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "28: import org.apache.hadoop.metrics2.MetricsException;",
          "29: import org.apache.hadoop.metrics2.lib.DefaultMetricsSystem;",
          "30: import org.apache.kylin.common.KylinConfig;",
          "40: import org.apache.kylin.rest.request.SQLRequest;",
          "41: import org.apache.kylin.rest.response.SQLResponse;",
          "42: import org.slf4j.Logger;",
          "43: import org.slf4j.LoggerFactory;",
          "46: import org.apache.kylin.shaded.com.google.common.hash.HashFunction;",
          "47: import org.apache.kylin.shaded.com.google.common.hash.Hashing;",
          "",
          "[Removed Lines]",
          "31: import org.apache.kylin.common.QueryContext;",
          "32: import org.apache.kylin.common.QueryContextFacade;",
          "33: import org.apache.kylin.metrics.MetricsManager;",
          "34: import org.apache.kylin.metrics.lib.impl.RecordEvent;",
          "35: import org.apache.kylin.metrics.lib.impl.TimedRecordEvent;",
          "36: import org.apache.kylin.metrics.property.QueryCubePropertyEnum;",
          "37: import org.apache.kylin.metrics.property.QueryPropertyEnum;",
          "38: import org.apache.kylin.metrics.property.QueryRPCPropertyEnum;",
          "39: import org.apache.kylin.query.enumerator.OLAPQuery;",
          "44: import org.springframework.security.core.context.SecurityContextHolder;",
          "",
          "[Added Lines]",
          "30: import org.apache.kylin.metrics.QuerySparkMetrics;",
          "38: import org.springframework.security.core.context.SecurityContextHolder;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "70:         return hashFunc.hashString(sql, Charset.forName(\"UTF-8\")).asLong();",
          "71:     }",
          "74:         updateMetricsToLocal(sqlRequest, sqlResponse);",
          "76:     }",
          "78:     private static void updateMetricsToLocal(SQLRequest sqlRequest, SQLResponse sqlResponse) {",
          "",
          "[Removed Lines]",
          "73:     public static void updateMetrics(SQLRequest sqlRequest, SQLResponse sqlResponse) {",
          "75:         updateMetricsToReservoir(sqlRequest, sqlResponse);",
          "",
          "[Added Lines]",
          "64:     public static void updateMetrics(String queryId, SQLRequest sqlRequest, SQLResponse sqlResponse) {",
          "66:         updateMetricsToCache(queryId, sqlRequest, sqlResponse);",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "89:         update(getQueryMetrics(cubeMetricName), sqlResponse);",
          "90:     }",
          "99:         String user = SecurityContextHolder.getContext().getAuthentication().getName();",
          "100:         if (user == null) {",
          "101:             user = \"unknown\";",
          "102:         }",
          "157:         }",
          "158:     }",
          "",
          "[Removed Lines]",
          "95:     private static void updateMetricsToReservoir(SQLRequest sqlRequest, SQLResponse sqlResponse) {",
          "96:         if (!KylinConfig.getInstanceFromEnv().isKylinMetricsReporterForQueryEnabled()) {",
          "97:             return;",
          "98:         }",
          "103:         for (QueryContext.RPCStatistics entry : QueryContextFacade.current().getRpcStatisticsList()) {",
          "104:             RecordEvent rpcMetricsEvent = new TimedRecordEvent(",
          "105:                     KylinConfig.getInstanceFromEnv().getKylinMetricsSubjectQueryRpcCall());",
          "106:             setRPCWrapper(rpcMetricsEvent, //",
          "107:                     norm(sqlRequest.getProject()), entry.getRealizationName(), entry.getRpcServer(),",
          "108:                     entry.getException());",
          "109:             setRPCStats(rpcMetricsEvent, //",
          "110:                     entry.getCallTimeMs(), entry.getSkippedRows(), entry.getScannedRows(), entry.getReturnedRows(),",
          "111:                     entry.getAggregatedRows());",
          "113:             MetricsManager.getInstance().update(rpcMetricsEvent);",
          "114:         }",
          "115:         for (QueryContext.CubeSegmentStatisticsResult contextEntry : sqlResponse.getCubeSegmentStatisticsList()) {",
          "116:             RecordEvent queryMetricsEvent = new TimedRecordEvent(",
          "117:                     KylinConfig.getInstanceFromEnv().getKylinMetricsSubjectQuery());",
          "118:             setQueryWrapper(queryMetricsEvent, //",
          "119:                     user, sqlRequest.getSql(), sqlResponse.isStorageCacheUsed() ? \"CACHE\" : contextEntry.getQueryType(),",
          "120:                     norm(sqlRequest.getProject()), contextEntry.getRealization(), contextEntry.getRealizationType(),",
          "121:                     sqlResponse.getThrowable());",
          "123:             long totalStorageReturnCount = 0L;",
          "124:             if (contextEntry.getQueryType().equalsIgnoreCase(OLAPQuery.EnumeratorTypeEnum.OLAP.name())) {",
          "125:                 for (Map<String, QueryContext.CubeSegmentStatistics> cubeEntry : contextEntry.getCubeSegmentStatisticsMap()",
          "126:                         .values()) {",
          "127:                     for (QueryContext.CubeSegmentStatistics segmentEntry : cubeEntry.values()) {",
          "128:                         RecordEvent cubeSegmentMetricsEvent = new TimedRecordEvent(",
          "129:                                 KylinConfig.getInstanceFromEnv().getKylinMetricsSubjectQueryCube());",
          "131:                         setCubeWrapper(cubeSegmentMetricsEvent, //",
          "132:                                 norm(sqlRequest.getProject()), segmentEntry.getCubeName(), segmentEntry.getSegmentName(),",
          "133:                                 segmentEntry.getSourceCuboidId(), segmentEntry.getTargetCuboidId(),",
          "134:                                 segmentEntry.getFilterMask());",
          "136:                         setCubeStats(cubeSegmentMetricsEvent, //",
          "137:                                 segmentEntry.getCallCount(), segmentEntry.getCallTimeSum(), segmentEntry.getCallTimeMax(),",
          "138:                                 segmentEntry.getStorageSkippedRows(), segmentEntry.getStorageScannedRows(),",
          "139:                                 segmentEntry.getStorageReturnedRows(), segmentEntry.getStorageAggregatedRows(),",
          "140:                                 segmentEntry.isIfSuccess(), 1.0 / cubeEntry.size());",
          "142:                         totalStorageReturnCount += segmentEntry.getStorageReturnedRows();",
          "144:                         MetricsManager.getInstance().update(cubeSegmentMetricsEvent);",
          "145:                     }",
          "146:                 }",
          "147:             } else {",
          "148:                 if (!sqlResponse.getIsException()) {",
          "149:                     totalStorageReturnCount = sqlResponse.getResults().size();",
          "150:                 }",
          "151:             }",
          "152:             setQueryStats(queryMetricsEvent, //",
          "153:                     sqlResponse.getDuration(), sqlResponse.getResults() == null ? 0 : sqlResponse.getResults().size(),",
          "154:                     totalStorageReturnCount);",
          "156:             MetricsManager.getInstance().update(queryMetricsEvent);",
          "",
          "[Added Lines]",
          "83:     private static void updateMetricsToCache(String queryId, SQLRequest sqlRequest, SQLResponse sqlResponse) {",
          "89:         QuerySparkMetrics.QueryExecutionMetrics queryExecutionMetrics = QuerySparkMetrics.getInstance()",
          "90:                 .getQueryExecutionMetricsMap().getIfPresent(queryId);",
          "91:         if (queryExecutionMetrics != null) {",
          "92:             queryExecutionMetrics.setUser(user);",
          "93:             queryExecutionMetrics.setSqlIdCode(getSqlHashCode(sqlRequest.getSql()));",
          "94:             queryExecutionMetrics.setProject(norm(sqlRequest.getProject()));",
          "95:             queryExecutionMetrics.setQueryType(sqlResponse.isStorageCacheUsed() ? \"CACHE\" : \"PARQUET\");",
          "97:             queryExecutionMetrics.setSqlDuration(sqlResponse.getDuration());",
          "98:             queryExecutionMetrics.setTotalScanCount(sqlResponse.getTotalScanCount());",
          "99:             queryExecutionMetrics.setTotalScanBytes(sqlResponse.getTotalScanBytes());",
          "100:             queryExecutionMetrics.setResultCount(sqlResponse.getResults() == null ? 0 : sqlResponse.getResults().size());",
          "102:             queryExecutionMetrics.setException(sqlResponse.getThrowable() == null ? \"NULL\" :",
          "103:                     sqlResponse.getThrowable().getClass().getName());",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "161:         return project.toUpperCase(Locale.ROOT);",
          "162:     }",
          "235:     private static void update(QueryMetrics queryMetrics, SQLResponse sqlResponse) {",
          "236:         try {",
          "237:             incrQueryCount(queryMetrics, sqlResponse);",
          "",
          "[Removed Lines]",
          "164:     private static void setRPCWrapper(RecordEvent metricsEvent, String projectName, String realizationName,",
          "165:             String rpcServer, Throwable throwable) {",
          "166:         metricsEvent.put(QueryRPCPropertyEnum.PROJECT.toString(), projectName);",
          "167:         metricsEvent.put(QueryRPCPropertyEnum.REALIZATION.toString(), realizationName);",
          "168:         metricsEvent.put(QueryRPCPropertyEnum.RPC_SERVER.toString(), rpcServer);",
          "169:         metricsEvent.put(QueryRPCPropertyEnum.EXCEPTION.toString(),",
          "170:                 throwable == null ? \"NULL\" : throwable.getClass().getName());",
          "171:     }",
          "173:     private static void setRPCStats(RecordEvent metricsEvent, long callTimeMs, long skipCount, long scanCount,",
          "174:             long returnCount, long aggrCount) {",
          "175:         metricsEvent.put(QueryRPCPropertyEnum.CALL_TIME.toString(), callTimeMs);",
          "176:         metricsEvent.put(QueryRPCPropertyEnum.SKIP_COUNT.toString(), skipCount); //Number of skips on region servers based on region meta or fuzzy filter",
          "177:         metricsEvent.put(QueryRPCPropertyEnum.SCAN_COUNT.toString(), scanCount); //Count scanned by region server",
          "178:         metricsEvent.put(QueryRPCPropertyEnum.RETURN_COUNT.toString(), returnCount);//Count returned by region server",
          "179:         metricsEvent.put(QueryRPCPropertyEnum.AGGR_FILTER_COUNT.toString(), scanCount - returnCount); //Count filtered & aggregated by coprocessor",
          "180:         metricsEvent.put(QueryRPCPropertyEnum.AGGR_COUNT.toString(), aggrCount); //Count aggregated by coprocessor",
          "181:     }",
          "183:     private static void setCubeWrapper(RecordEvent metricsEvent, String projectName, String cubeName,",
          "184:             String segmentName, long sourceCuboidId, long targetCuboidId, long filterMask) {",
          "185:         metricsEvent.put(QueryCubePropertyEnum.PROJECT.toString(), projectName);",
          "186:         metricsEvent.put(QueryCubePropertyEnum.CUBE.toString(), cubeName);",
          "187:         metricsEvent.put(QueryCubePropertyEnum.SEGMENT.toString(), segmentName);",
          "188:         metricsEvent.put(QueryCubePropertyEnum.CUBOID_SOURCE.toString(), sourceCuboidId);",
          "189:         metricsEvent.put(QueryCubePropertyEnum.CUBOID_TARGET.toString(), targetCuboidId);",
          "190:         metricsEvent.put(QueryCubePropertyEnum.IF_MATCH.toString(), sourceCuboidId == targetCuboidId);",
          "191:         metricsEvent.put(QueryCubePropertyEnum.FILTER_MASK.toString(), filterMask);",
          "192:     }",
          "194:     private static void setCubeStats(RecordEvent metricsEvent, long callCount, long callTimeSum, long callTimeMax,",
          "195:             long skipCount, long scanCount, long returnCount, long aggrCount, boolean ifSuccess, double weightPerHit) {",
          "196:         metricsEvent.put(QueryCubePropertyEnum.CALL_COUNT.toString(), callCount);",
          "197:         metricsEvent.put(QueryCubePropertyEnum.TIME_SUM.toString(), callTimeSum);",
          "198:         metricsEvent.put(QueryCubePropertyEnum.TIME_MAX.toString(), callTimeMax);",
          "199:         metricsEvent.put(QueryCubePropertyEnum.SKIP_COUNT.toString(), skipCount);",
          "200:         metricsEvent.put(QueryCubePropertyEnum.SCAN_COUNT.toString(), scanCount);",
          "201:         metricsEvent.put(QueryCubePropertyEnum.RETURN_COUNT.toString(), returnCount);",
          "202:         metricsEvent.put(QueryCubePropertyEnum.AGGR_FILTER_COUNT.toString(), scanCount - returnCount);",
          "203:         metricsEvent.put(QueryCubePropertyEnum.AGGR_COUNT.toString(), aggrCount);",
          "204:         metricsEvent.put(QueryCubePropertyEnum.IF_SUCCESS.toString(), ifSuccess);",
          "205:         metricsEvent.put(QueryCubePropertyEnum.WEIGHT_PER_HIT.toString(), weightPerHit);",
          "206:     }",
          "208:     private static void setQueryWrapper(RecordEvent metricsEvent, String user, String sql, String queryType,",
          "209:             String projectName, String realizationName, int realizationType, Throwable throwable) {",
          "210:         metricsEvent.put(QueryPropertyEnum.USER.toString(), user);",
          "211:         metricsEvent.put(QueryPropertyEnum.ID_CODE.toString(), getSqlHashCode(sql));",
          "212:         metricsEvent.put(QueryPropertyEnum.SQL.toString(), sql);",
          "213:         metricsEvent.put(QueryPropertyEnum.TYPE.toString(), queryType);",
          "214:         metricsEvent.put(QueryPropertyEnum.PROJECT.toString(), projectName);",
          "215:         metricsEvent.put(QueryPropertyEnum.REALIZATION.toString(), realizationName);",
          "216:         metricsEvent.put(QueryPropertyEnum.REALIZATION_TYPE.toString(), realizationType);",
          "217:         metricsEvent.put(QueryPropertyEnum.EXCEPTION.toString(),",
          "218:                 throwable == null ? \"NULL\" : throwable.getClass().getName());",
          "219:     }",
          "221:     private static void setQueryStats(RecordEvent metricsEvent, long callTimeMs, long returnCountByCalcite,",
          "222:             long returnCountByStorage) {",
          "223:         metricsEvent.put(QueryPropertyEnum.TIME_COST.toString(), callTimeMs);",
          "224:         metricsEvent.put(QueryPropertyEnum.CALCITE_RETURN_COUNT.toString(), returnCountByCalcite);",
          "225:         metricsEvent.put(QueryPropertyEnum.STORAGE_RETURN_COUNT.toString(), returnCountByStorage);",
          "226:         long countAggrAndFilter = returnCountByStorage - returnCountByCalcite;",
          "227:         if (countAggrAndFilter < 0) {",
          "228:             countAggrAndFilter = 0;",
          "229:             logger.warn(returnCountByStorage + \" rows returned by storage less than \" + returnCountByCalcite",
          "230:                     + \" rows returned by calcite\");",
          "231:         }",
          "232:         metricsEvent.put(QueryPropertyEnum.AGGR_FILTER_COUNT.toString(), countAggrAndFilter);",
          "233:     }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "server-base/src/main/java/org/apache/kylin/rest/service/CubeService.java||server-base/src/main/java/org/apache/kylin/rest/service/CubeService.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/service/CubeService.java -> server-base/src/main/java/org/apache/kylin/rest/service/CubeService.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "1042:         String cuboidColumn = isCuboidSource ? QueryCubePropertyEnum.CUBOID_SOURCE.toString()",
          "1043:                 : QueryCubePropertyEnum.CUBOID_TARGET.toString();",
          "1044:         String hitMeasure = QueryCubePropertyEnum.WEIGHT_PER_HIT.toString();",
          "1046:         String sql = \"select \" + cuboidColumn + \", sum(\" + hitMeasure + \")\" //",
          "1047:                 + \" from \" + table//",
          "1048:                 + \" where \" + QueryCubePropertyEnum.CUBE.toString() + \" = ?\" //",
          "",
          "[Removed Lines]",
          "1045:         String table = getMetricsManager().getSystemTableFromSubject(getConfig().getKylinMetricsSubjectQueryCube());",
          "",
          "[Added Lines]",
          "1045:         String table = getMetricsManager().getSystemTableFromSubject(getConfig().getKylinMetricsSubjectQuerySparkJob());",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1057:         String cuboidTgt = QueryCubePropertyEnum.CUBOID_TARGET.toString();",
          "1058:         String aggCount = QueryCubePropertyEnum.AGGR_COUNT.toString();",
          "1059:         String returnCount = QueryCubePropertyEnum.RETURN_COUNT.toString();",
          "1061:         String sql = \"select \" + cuboidSource + \", \" + cuboidTgt + \", avg(\" + aggCount + \"), avg(\" + returnCount + \")\"//",
          "1062:                 + \" from \" + table //",
          "1063:                 + \" where \" + QueryCubePropertyEnum.CUBE.toString() + \" = ?\" //",
          "",
          "[Removed Lines]",
          "1060:         String table = getMetricsManager().getSystemTableFromSubject(getConfig().getKylinMetricsSubjectQueryCube());",
          "",
          "[Added Lines]",
          "1060:         String table = getMetricsManager().getSystemTableFromSubject(getConfig().getKylinMetricsSubjectQuerySparkJob());",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1070:     public Map<Long, Long> getCuboidQueryMatchCount(String cubeName) {",
          "1071:         String cuboidSource = QueryCubePropertyEnum.CUBOID_SOURCE.toString();",
          "1072:         String hitMeasure = QueryCubePropertyEnum.WEIGHT_PER_HIT.toString();",
          "1074:         String sql = \"select \" + cuboidSource + \", sum(\" + hitMeasure + \")\" //",
          "1075:                 + \" from \" + table //",
          "1076:                 + \" where \" + QueryCubePropertyEnum.CUBE.toString() + \" = ?\" //",
          "",
          "[Removed Lines]",
          "1073:         String table = getMetricsManager().getSystemTableFromSubject(getConfig().getKylinMetricsSubjectQueryCube());",
          "",
          "[Added Lines]",
          "1073:         String table = getMetricsManager().getSystemTableFromSubject(getConfig().getKylinMetricsSubjectQuerySparkJob());",
          "",
          "---------------"
        ],
        "server-base/src/main/java/org/apache/kylin/rest/service/DashboardService.java||server-base/src/main/java/org/apache/kylin/rest/service/DashboardService.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/service/DashboardService.java -> server-base/src/main/java/org/apache/kylin/rest/service/DashboardService.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "31: import org.apache.kylin.metrics.MetricsManager;",
          "32: import org.apache.kylin.metrics.lib.impl.TimePropertyEnum;",
          "33: import org.apache.kylin.metrics.property.JobPropertyEnum;",
          "35: import org.apache.kylin.rest.constant.Constant;",
          "36: import org.apache.kylin.rest.exception.BadRequestException;",
          "37: import org.apache.kylin.rest.request.PrepareSqlRequest;",
          "",
          "[Removed Lines]",
          "34: import org.apache.kylin.metrics.property.QueryPropertyEnum;",
          "",
          "[Added Lines]",
          "34: import org.apache.kylin.metrics.property.QuerySparkExecutionEnum;",
          "35: import org.apache.kylin.metrics.property.QuerySparkExecutionEnum;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "119:         Map<String, String> filterMap = getBaseFilterMap(CategoryEnum.QUERY, projectName, startTime, endTime);",
          "120:         filterMap.putAll(getCubeFilterMap(CategoryEnum.QUERY, cubeName));",
          "121:         return createPrepareSqlRequest(null, metrics,",
          "123:     };",
          "125:     public PrepareSqlRequest getJobMetricsSQLRequest(String startTime, String endTime, String projectName,",
          "",
          "[Removed Lines]",
          "122:                 getMetricsManager().getSystemTableFromSubject(getConfig().getKylinMetricsSubjectQuery()), filterMap);",
          "",
          "[Added Lines]",
          "123:                 getMetricsManager().getSystemTableFromSubject(getConfig().getKylinMetricsSubjectQueryExecution()), filterMap);",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "143:             if (categoryEnum == CategoryEnum.QUERY) {",
          "144:                 dimensionSQL = new String[] { QueryDimensionEnum.valueOf(dimension).toSQL() };",
          "145:                 metricSQL = new String[] { QueryMetricEnum.valueOf(metric).toSQL() };",
          "147:             } else if (categoryEnum == CategoryEnum.JOB) {",
          "148:                 dimensionSQL = new String[] { JobDimensionEnum.valueOf(dimension).toSQL() };",
          "149:                 metricSQL = new String[] { JobMetricEnum.valueOf(metric).toSQL() };",
          "",
          "[Removed Lines]",
          "146:                 table = getMetricsManager().getSystemTableFromSubject(getConfig().getKylinMetricsSubjectQuery());",
          "",
          "[Added Lines]",
          "147:                 table = getMetricsManager().getSystemTableFromSubject(getConfig().getKylinMetricsSubjectQueryExecution());",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "217:         HashMap<String, String> filterMap = new HashMap<>();",
          "219:         if (category == CategoryEnum.QUERY) {",
          "222:             if (!Strings.isNullOrEmpty(cubeName)) {",
          "224:             }",
          "225:         } else if (category == CategoryEnum.JOB && !Strings.isNullOrEmpty(cubeName)) {",
          "226:             HybridInstance hybridInstance = getHybridManager().getHybridInstance(cubeName);",
          "",
          "[Removed Lines]",
          "220:             filterMap.put(QueryPropertyEnum.EXCEPTION.toString() + \" = ?\", \"NULL\");",
          "223:                 filterMap.put(QueryPropertyEnum.REALIZATION + \" = ?\", cubeName);",
          "",
          "[Added Lines]",
          "221:             filterMap.put(QuerySparkExecutionEnum.EXCEPTION.toString() + \" = ?\", \"NULL\");",
          "224:                 filterMap.put(QuerySparkExecutionEnum.REALIZATION + \" = ?\", cubeName);",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "299:     }",
          "301:     private enum QueryDimensionEnum {",
          "304:         DAY(TimePropertyEnum.DAY_DATE.toString()), //",
          "305:         WEEK(TimePropertyEnum.WEEK_BEGIN_DATE.toString()), //",
          "306:         MONTH(TimePropertyEnum.MONTH.toString());",
          "",
          "[Removed Lines]",
          "302:         PROJECT(QueryPropertyEnum.PROJECT.toString()), //",
          "303:         CUBE(QueryPropertyEnum.REALIZATION.toString()), //",
          "",
          "[Added Lines]",
          "303:         PROJECT(QuerySparkExecutionEnum.PROJECT.toString()), //",
          "304:         CUBE(QuerySparkExecutionEnum.REALIZATION.toString()), //",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "337:     private enum QueryMetricEnum {",
          "338:         QUERY_COUNT(\"count(*)\"), //",
          "343:         private final String sql;",
          "",
          "[Removed Lines]",
          "339:         AVG_QUERY_LATENCY(\"avg(\" + QueryPropertyEnum.TIME_COST.toString() + \")\"), //",
          "340:         MAX_QUERY_LATENCY(\"max(\" + QueryPropertyEnum.TIME_COST.toString() + \")\"), //",
          "341:         MIN_QUERY_LATENCY(\"min(\" + QueryPropertyEnum.TIME_COST.toString() + \")\");",
          "",
          "[Added Lines]",
          "340:         AVG_QUERY_LATENCY(\"avg(\" + QuerySparkExecutionEnum.TIME_COST.toString() + \")\"), //",
          "341:         MAX_QUERY_LATENCY(\"max(\" + QuerySparkExecutionEnum.TIME_COST.toString() + \")\"), //",
          "342:         MIN_QUERY_LATENCY(\"min(\" + QuerySparkExecutionEnum.TIME_COST.toString() + \")\");",
          "",
          "---------------"
        ],
        "server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java||server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java": [
          "File: server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java -> server-base/src/main/java/org/apache/kylin/rest/service/QueryService.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "68: import org.apache.kylin.common.KylinConfig;",
          "69: import org.apache.kylin.common.QueryContext;",
          "70: import org.apache.kylin.common.QueryContextFacade;",
          "71: import org.apache.kylin.common.debug.BackdoorToggles;",
          "72: import org.apache.kylin.common.exceptions.ResourceLimitExceededException;",
          "73: import org.apache.kylin.common.persistence.ResourceStore;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "71: import org.apache.kylin.metrics.QuerySparkMetrics;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "120: import org.apache.kylin.rest.util.TableauInterceptor;",
          "121: import org.apache.kylin.storage.hybrid.HybridInstance;",
          "122: import org.apache.kylin.storage.hybrid.HybridManager;",
          "123: import org.slf4j.Logger;",
          "124: import org.slf4j.LoggerFactory;",
          "125: import org.springframework.beans.factory.annotation.Autowired;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "124: import org.apache.spark.sql.SparderContext;",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "468:             }",
          "470:             sqlResponse.setDuration(queryContext.getAccumulatedMillis());",
          "471:             logQuery(queryContext.getQueryId(), sqlRequest, sqlResponse);",
          "472:             try {",
          "474:             } catch (Throwable th) {",
          "475:                 logger.warn(\"Write metric error.\", th);",
          "476:             }",
          "",
          "[Removed Lines]",
          "473:                 recordMetric(sqlRequest, sqlResponse);",
          "",
          "[Added Lines]",
          "473:             if (QuerySparkMetrics.getInstance().getQueryExecutionMetrics(queryContext.getQueryId()) != null) {",
          "474:                 String sqlTraceUrl = SparderContext.appMasterTrackURL() + \"/SQL/execution/?id=\" +",
          "475:                         QuerySparkMetrics.getInstance().getQueryExecutionMetrics(queryContext.getQueryId()).getExecutionId();",
          "476:                 sqlResponse.setTraceUrl(sqlTraceUrl);",
          "477:             }",
          "480:                 recordMetric(queryContext.getQueryId(), sqlRequest, sqlResponse);",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "585:                 checkCondition(!BackdoorToggles.getDisableCache(), \"query cache disabled in BackdoorToggles\");",
          "586:     }",
          "590:         QueryMetrics2Facade.updateMetrics(sqlRequest, sqlResponse);",
          "591:     }",
          "",
          "[Removed Lines]",
          "588:     protected void recordMetric(SQLRequest sqlRequest, SQLResponse sqlResponse) throws UnknownHostException {",
          "589:         QueryMetricsFacade.updateMetrics(sqlRequest, sqlResponse);",
          "",
          "[Added Lines]",
          "595:     protected void recordMetric(String queryId, SQLRequest sqlRequest, SQLResponse sqlResponse) throws UnknownHostException {",
          "596:         QueryMetricsFacade.updateMetrics(queryId, sqlRequest, sqlResponse);",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1201:                     realizations.add(realizationName);",
          "1202:                 }",
          "1204:             }",
          "",
          "[Removed Lines]",
          "1203:                 queryContext.setContextRealization(ctx.id, realizationName, realizationType);",
          "",
          "[Added Lines]",
          "1210:                 QuerySparkMetrics.getInstance().setQueryRealization(queryContext.getQueryId(), realizationName,",
          "1211:                         realizationType, cuboidIdsSb.toString());",
          "",
          "---------------"
        ],
        "server/src/test/java/org/apache/kylin/rest/metrics/QueryMetricsTest.java||server/src/test/java/org/apache/kylin/rest/metrics/QueryMetricsTest.java": [
          "File: server/src/test/java/org/apache/kylin/rest/metrics/QueryMetricsTest.java -> server/src/test/java/org/apache/kylin/rest/metrics/QueryMetricsTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "71:         sqlResponse.setResults(results);",
          "72:         sqlResponse.setStorageCacheUsed(true);",
          "76:         Thread.sleep(2000);",
          "",
          "[Removed Lines]",
          "74:         QueryMetricsFacade.updateMetrics(sqlRequest, sqlResponse);",
          "",
          "[Added Lines]",
          "74:         QueryMetricsFacade.updateMetrics(\"\", sqlRequest, sqlResponse);",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "100:         sqlResponse2.setCube(\"test_cube\");",
          "101:         sqlResponse2.setIsException(true);",
          "105:         Thread.sleep(2000);",
          "",
          "[Removed Lines]",
          "103:         QueryMetricsFacade.updateMetrics(sqlRequest, sqlResponse2);",
          "",
          "[Added Lines]",
          "103:         QueryMetricsFacade.updateMetrics(\"\", sqlRequest, sqlResponse2);",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "147:         sqlResponse.setCubeSegmentStatisticsList(context.getCubeSegmentStatisticsResultList());",
          "151:         Thread.sleep(2000);",
          "",
          "[Removed Lines]",
          "149:         QueryMetricsFacade.updateMetrics(sqlRequest, sqlResponse);",
          "",
          "[Added Lines]",
          "149:         QueryMetricsFacade.updateMetrics(\"\", sqlRequest, sqlResponse);",
          "",
          "---------------"
        ],
        "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeDescCreator.java||tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeDescCreator.java": [
          "File: tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeDescCreator.java -> tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeDescCreator.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "43: import org.apache.kylin.metrics.lib.impl.RecordEvent;",
          "44: import org.apache.kylin.metrics.lib.impl.TimePropertyEnum;",
          "45: import org.apache.kylin.metrics.property.JobPropertyEnum;",
          "50: import org.apache.kylin.shaded.com.google.common.collect.Lists;",
          "51: import org.apache.kylin.shaded.com.google.common.collect.Maps;",
          "52: import org.apache.kylin.shaded.com.google.common.collect.Sets;",
          "",
          "[Removed Lines]",
          "46: import org.apache.kylin.metrics.property.QueryCubePropertyEnum;",
          "47: import org.apache.kylin.metrics.property.QueryPropertyEnum;",
          "48: import org.apache.kylin.metrics.property.QueryRPCPropertyEnum;",
          "",
          "[Added Lines]",
          "46: import org.apache.kylin.metrics.property.QuerySparkExecutionEnum;",
          "47: import org.apache.kylin.metrics.property.QuerySparkJobEnum;",
          "48: import org.apache.kylin.metrics.property.QuerySparkStageEnum;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "55: public class CubeDescCreator {",
          "62:         dimensions.remove(TimePropertyEnum.DAY_TIME.toString());",
          "63:         dimensions.remove(RecordEvent.RecordReserveKeyEnum.TIME.toString());",
          "",
          "[Removed Lines]",
          "57:     public static CubeDesc generateKylinCubeDescForMetricsQuery(KylinConfig config, MetricsSinkDesc sinkDesc) {",
          "58:         String tableName = sinkDesc.getTableNameForMetrics(config.getKylinMetricsSubjectQuery());",
          "61:         List<String> dimensions = ModelCreator.getDimensionsForMetricsQuery();",
          "",
          "[Added Lines]",
          "56:     public static CubeDesc generateKylinCubeDescForMetricsQueryExecution(KylinConfig config, MetricsSinkDesc sinkDesc) {",
          "57:         String tableName = sinkDesc.getTableNameForMetrics(config.getKylinMetricsSubjectQueryExecution());",
          "60:         List<String> dimensions = ModelCreator.getDimensionsForMetricsQueryExecution();",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "68:         }",
          "73:         List<MeasureDesc> measureDescList = Lists.newArrayListWithExpectedSize(measures.size() * 2 + 1 + 1);",
          "76:         Map<String, String> measureTypeMap = Maps.newHashMapWithExpectedSize(measureTypeList.size());",
          "77:         for (Pair<String, String> entry : measureTypeList) {",
          "78:             measureTypeMap.put(entry.getFirst(), entry.getSecond());",
          "79:         }",
          "80:         measureDescList.add(getMeasureCount());",
          "83:         for (String measure : measures) {",
          "84:             measureDescList.add(getMeasureSum(measure, measureTypeMap.get(measure)));",
          "85:             measureDescList.add(getMeasureMax(measure, measureTypeMap.get(measure)));",
          "86:         }",
          "91:         RowKeyColDesc[] rowKeyColDescs = new RowKeyColDesc[dimensionDescList.size()];",
          "92:         int idx = getTimeRowKeyColDesc(tableName, rowKeyColDescs);",
          "94:         idx++;",
          "96:         idx++;",
          "98:         idx++;",
          "100:         idx++;",
          "102:         idx++;",
          "104:         idx++;",
          "105:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, RecordEvent.RecordReserveKeyEnum.HOST.toString(), idx + 1);",
          "106:         idx++;",
          "",
          "[Removed Lines]",
          "71:         List<String> measures = ModelCreator.getMeasuresForMetricsQuery();",
          "72:         measures.remove(QueryPropertyEnum.ID_CODE.toString());",
          "75:         List<Pair<String, String>> measureTypeList = HiveTableCreator.getHiveColumnsForMetricsQuery();",
          "81:         measureDescList.add(getMeasureMin(QueryPropertyEnum.TIME_COST.toString(),",
          "82:                 measureTypeMap.get(QueryPropertyEnum.TIME_COST.toString())));",
          "87:         measureDescList.add(getMeasureHLL(QueryPropertyEnum.ID_CODE.toString()));",
          "88:         measureDescList.add(getMeasurePercentile(QueryPropertyEnum.TIME_COST.toString()));",
          "93:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QueryPropertyEnum.USER.toString(), idx + 1);",
          "95:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QueryPropertyEnum.PROJECT.toString(), idx + 1);",
          "97:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QueryPropertyEnum.REALIZATION.toString(), idx + 1);",
          "99:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QueryPropertyEnum.REALIZATION_TYPE.toString(), idx + 1);",
          "101:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QueryPropertyEnum.EXCEPTION.toString(), idx + 1);",
          "103:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QueryPropertyEnum.TYPE.toString(), idx + 1);",
          "",
          "[Added Lines]",
          "70:         List<String> measures = ModelCreator.getMeasuresForMetricsQueryExecution();",
          "71:         measures.remove(QuerySparkExecutionEnum.ID_CODE.toString());",
          "74:         List<Pair<String, String>> measureTypeList = HiveTableCreator.getHiveColumnsForMetricsQueryExecution();",
          "80:         measureDescList.add(getMeasureMin(QuerySparkExecutionEnum.TIME_COST.toString(),",
          "81:                 measureTypeMap.get(QuerySparkExecutionEnum.TIME_COST.toString())));",
          "86:         measureDescList.add(getMeasureHLL(QuerySparkExecutionEnum.ID_CODE.toString()));",
          "87:         measureDescList.add(getMeasurePercentile(QuerySparkExecutionEnum.TIME_COST.toString()));",
          "92:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkExecutionEnum.USER.toString(), idx + 1);",
          "93:         idx++;",
          "94:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkExecutionEnum.PROJECT.toString(), idx + 1);",
          "95:         idx++;",
          "96:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkExecutionEnum.REALIZATION.toString(), idx + 1);",
          "97:         idx++;",
          "98:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkExecutionEnum.REALIZATION_TYPE.toString(), idx + 1);",
          "100:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkExecutionEnum.CUBOID_IDS.toString(), idx + 1);",
          "102:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkExecutionEnum.EXCEPTION.toString(), idx + 1);",
          "104:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkExecutionEnum.TYPE.toString(), idx + 1);",
          "106:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkExecutionEnum.SPARDER_NAME.toString(), idx + 1);",
          "108:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkExecutionEnum.QUERY_ID.toString(), idx + 1);",
          "109:         idx++;",
          "110:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkExecutionEnum.START_TIME.toString(), idx + 1);",
          "111:         idx++;",
          "112:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkExecutionEnum.END_TIME.toString(), idx + 1);",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "109:         rowKeyDesc.setRowkeyColumns(rowKeyColDescs);",
          "113:         hierarchy_dims[0] = getTimeHierarchy();",
          "117:         for (int i = 0; i < hierarchy_dims.length; i++) {",
          "118:             hierarchy_dims[i] = refineColumnWithTable(tableName, hierarchy_dims[i]);",
          "119:         }",
          "",
          "[Removed Lines]",
          "112:         String[][] hierarchy_dims = new String[2][];",
          "114:         hierarchy_dims[1] = new String[2];",
          "115:         hierarchy_dims[1][0] = QueryPropertyEnum.REALIZATION_TYPE.toString();",
          "116:         hierarchy_dims[1][1] = QueryPropertyEnum.REALIZATION.toString();",
          "",
          "[Added Lines]",
          "121:         String[][] hierarchy_dims = new String[4][];",
          "123:         hierarchy_dims[1] = new String[3];",
          "124:         hierarchy_dims[1][0] = QuerySparkExecutionEnum.REALIZATION_TYPE.toString();",
          "125:         hierarchy_dims[1][1] = QuerySparkExecutionEnum.REALIZATION.toString();",
          "126:         hierarchy_dims[1][2] = QuerySparkExecutionEnum.CUBOID_IDS.toString();",
          "127:         hierarchy_dims[2] = new String[2];",
          "128:         hierarchy_dims[2][0] = QuerySparkExecutionEnum.START_TIME.toString();",
          "129:         hierarchy_dims[2][1] = QuerySparkExecutionEnum.END_TIME.toString();",
          "130:         hierarchy_dims[3] = new String[2];",
          "131:         hierarchy_dims[3][0] = QuerySparkExecutionEnum.SPARDER_NAME.toString();",
          "132:         hierarchy_dims[3][1] = RecordEvent.RecordReserveKeyEnum.HOST.toString();",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "135:                 rowKeyDesc, aggGroup, hBaseMapping, sinkDesc.getCubeDescOverrideProperties());",
          "136:     }",
          "143:         dimensions.remove(TimePropertyEnum.DAY_TIME.toString());",
          "144:         dimensions.remove(RecordEvent.RecordReserveKeyEnum.TIME.toString());",
          "145:         dimensions.remove(RecordEvent.RecordReserveKeyEnum.HOST.toString());",
          "148:         List<DimensionDesc> dimensionDescList = Lists.newArrayListWithExpectedSize(dimensions.size());",
          "149:         for (String dimensionName : dimensions) {",
          "",
          "[Removed Lines]",
          "138:     public static CubeDesc generateKylinCubeDescForMetricsQueryCube(KylinConfig config, MetricsSinkDesc sinkDesc) {",
          "139:         String tableName = sinkDesc.getTableNameForMetrics(config.getKylinMetricsSubjectQueryCube());",
          "142:         List<String> dimensions = ModelCreator.getDimensionsForMetricsQueryCube();",
          "146:         dimensions.remove(QueryCubePropertyEnum.PROJECT.toString());",
          "",
          "[Added Lines]",
          "154:     public static CubeDesc generateKylinCubeDescForMetricsQuerySparkJob(KylinConfig config, MetricsSinkDesc sinkDesc) {",
          "155:         String tableName = sinkDesc.getTableNameForMetrics(config.getKylinMetricsSubjectQuerySparkJob());",
          "158:         List<String> dimensions = ModelCreator.getDimensionsForMetricsQuerySparkJob();",
          "162:         dimensions.remove(QuerySparkJobEnum.PROJECT.toString());",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "151:         }",
          "155:         List<MeasureDesc> measureDescList = Lists.newArrayListWithExpectedSize(measures.size() * 2);",
          "158:         Map<String, String> measureTypeMap = Maps.newHashMapWithExpectedSize(measureTypeList.size());",
          "159:         for (Pair<String, String> entry : measureTypeList) {",
          "160:             measureTypeMap.put(entry.getFirst(), entry.getSecond());",
          "",
          "[Removed Lines]",
          "154:         List<String> measures = ModelCreator.getMeasuresForMetricsQueryCube();",
          "157:         List<Pair<String, String>> measureTypeList = HiveTableCreator.getHiveColumnsForMetricsQueryCube();",
          "",
          "[Added Lines]",
          "170:         List<String> measures = ModelCreator.getMeasuresForMetricsQuerySparkJob();",
          "173:         List<Pair<String, String>> measureTypeList = HiveTableCreator.getHiveColumnsForMetricsQuerySparkJob();",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "162:         measureDescList.add(getMeasureCount());",
          "163:         for (String measure : measures) {",
          "164:             measureDescList.add(getMeasureSum(measure, measureTypeMap.get(measure)));",
          "168:         }",
          "171:         RowKeyColDesc[] rowKeyColDescs = new RowKeyColDesc[dimensionDescList.size()];",
          "172:         int idx = getTimeRowKeyColDesc(tableName, rowKeyColDescs);",
          "174:         idx++;",
          "176:         idx++;",
          "178:         idx++;",
          "180:         idx++;",
          "182:         idx++;",
          "186:         idx++;",
          "188:         RowKeyDesc rowKeyDesc = new RowKeyDesc();",
          "189:         rowKeyDesc.setRowkeyColumns(rowKeyColDescs);",
          "196:         hierarchy_dims[0] = getTimeHierarchy();",
          "197:         for (int i = 0; i < hierarchy_dims.length; i++) {",
          "198:             hierarchy_dims[i] = refineColumnWithTable(tableName, hierarchy_dims[i]);",
          "199:         }",
          "208:         SelectRule selectRule = new SelectRule();",
          "210:         selectRule.hierarchyDims = hierarchy_dims;",
          "213:         AggregationGroup aggGroup = new AggregationGroup();",
          "214:         aggGroup.setIncludes(refineColumnWithTable(tableName, dimensions));",
          "",
          "[Removed Lines]",
          "165:             if (!measure.equals(QueryCubePropertyEnum.WEIGHT_PER_HIT.toString())) {",
          "166:                 measureDescList.add(getMeasureMax(measure, measureTypeMap.get(measure)));",
          "167:             }",
          "173:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QueryCubePropertyEnum.CUBE.toString(), idx + 1);",
          "175:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QueryCubePropertyEnum.SEGMENT.toString(), idx + 1);",
          "177:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QueryCubePropertyEnum.CUBOID_SOURCE.toString(), idx + 1);",
          "179:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QueryCubePropertyEnum.CUBOID_TARGET.toString(), idx + 1);",
          "181:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QueryCubePropertyEnum.FILTER_MASK.toString(), idx + 1);",
          "183:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QueryCubePropertyEnum.IF_MATCH.toString(), idx + 1);",
          "184:         idx++;",
          "185:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QueryCubePropertyEnum.IF_SUCCESS.toString(), idx + 1);",
          "192:         String[] mandatory_dims = new String[] { QueryCubePropertyEnum.CUBE.toString() };",
          "193:         mandatory_dims = refineColumnWithTable(tableName, mandatory_dims);",
          "195:         String[][] hierarchy_dims = new String[1][];",
          "201:         String[][] joint_dims = new String[1][];",
          "202:         joint_dims[0] = new String[] { QueryCubePropertyEnum.CUBOID_SOURCE.toString(),",
          "203:                 QueryCubePropertyEnum.CUBOID_TARGET.toString() };",
          "204:         for (int i = 0; i < joint_dims.length; i++) {",
          "205:             joint_dims[i] = refineColumnWithTable(tableName, joint_dims[i]);",
          "206:         }",
          "209:         selectRule.mandatoryDims = mandatory_dims;",
          "211:         selectRule.jointDims = joint_dims;",
          "",
          "[Added Lines]",
          "181:             measureDescList.add(getMeasureMax(measure, measureTypeMap.get(measure)));",
          "187:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkJobEnum.JOB_ID.toString(), idx + 1);",
          "189:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkJobEnum.EXECUTION_ID.toString(), idx + 1);",
          "191:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkJobEnum.QUERY_ID.toString(), idx + 1);",
          "193:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkJobEnum.START_TIME.toString(), idx + 1);",
          "195:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkJobEnum.END_TIME.toString(), idx + 1);",
          "197:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkJobEnum.IF_SUCCESS.toString(), idx + 1);",
          "203:         String[][] hierarchy_dims = new String[2][];",
          "205:         hierarchy_dims[1] = new String[3];",
          "206:         hierarchy_dims[1][0] = QuerySparkJobEnum.QUERY_ID.toString();",
          "207:         hierarchy_dims[1][1] = QuerySparkJobEnum.EXECUTION_ID.toString();",
          "208:         hierarchy_dims[1][2] = QuerySparkJobEnum.JOB_ID.toString();",
          "215:         selectRule.mandatoryDims = new String[0];",
          "217:         selectRule.jointDims = new String[0][0];",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "222:                 rowKeyDesc, aggGroup, hBaseMapping, sinkDesc.getCubeDescOverrideProperties());",
          "223:     }",
          "230:         dimensions.remove(TimePropertyEnum.DAY_TIME.toString());",
          "231:         dimensions.remove(RecordEvent.RecordReserveKeyEnum.TIME.toString());",
          "",
          "[Removed Lines]",
          "225:     public static CubeDesc generateKylinCubeDescForMetricsQueryRPC(KylinConfig config, MetricsSinkDesc sinkDesc) {",
          "226:         String tableName = sinkDesc.getTableNameForMetrics(config.getKylinMetricsSubjectQueryRpcCall());",
          "229:         List<String> dimensions = ModelCreator.getDimensionsForMetricsQueryRPC();",
          "",
          "[Added Lines]",
          "231:     public static CubeDesc generateKylinCubeDescForMetricsQuerySparkStage(KylinConfig config, MetricsSinkDesc sinkDesc) {",
          "232:         String tableName = sinkDesc.getTableNameForMetrics(config.getKylinMetricsSubjectQuerySparkStage());",
          "235:         List<String> dimensions = ModelCreator.getDimensionsForMetricsQuerySparkStage();",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "236:         }",
          "240:         List<MeasureDesc> measureDescList = Lists.newArrayListWithExpectedSize(measures.size() * 2 + 1 + 1);",
          "243:         Map<String, String> measureTypeMap = Maps.newHashMapWithExpectedSize(measureTypeList.size());",
          "244:         for (Pair<String, String> entry : measureTypeList) {",
          "245:             measureTypeMap.put(entry.getFirst(), entry.getSecond());",
          "",
          "[Removed Lines]",
          "239:         List<String> measures = ModelCreator.getMeasuresForMetricsQueryRPC();",
          "242:         List<Pair<String, String>> measureTypeList = HiveTableCreator.getHiveColumnsForMetricsQueryRPC();",
          "",
          "[Added Lines]",
          "245:         List<String> measures = ModelCreator.getMeasuresForMetricsQuerySparkStage();",
          "248:         List<Pair<String, String>> measureTypeList = HiveTableCreator.getHiveColumnsForMetricsQuerySparkStage();",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "249:             measureDescList.add(getMeasureSum(measure, measureTypeMap.get(measure)));",
          "250:             measureDescList.add(getMeasureMax(measure, measureTypeMap.get(measure)));",
          "251:         }",
          "255:         RowKeyColDesc[] rowKeyColDescs = new RowKeyColDesc[dimensionDescList.size()];",
          "256:         int idx = getTimeRowKeyColDesc(tableName, rowKeyColDescs);",
          "258:         idx++;",
          "260:         idx++;",
          "262:         idx++;",
          "264:         idx++;",
          "266:         idx++;",
          "268:         RowKeyDesc rowKeyDesc = new RowKeyDesc();",
          "269:         rowKeyDesc.setRowkeyColumns(rowKeyColDescs);",
          "273:         hierarchy_dims[0] = getTimeHierarchy();",
          "274:         for (int i = 0; i < hierarchy_dims.length; i++) {",
          "275:             hierarchy_dims[i] = refineColumnWithTable(tableName, hierarchy_dims[i]);",
          "276:         }",
          "",
          "[Removed Lines]",
          "252:         measureDescList.add(getMeasurePercentile(QueryRPCPropertyEnum.CALL_TIME.toString()));",
          "257:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QueryRPCPropertyEnum.PROJECT.toString(), idx + 1);",
          "259:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QueryRPCPropertyEnum.REALIZATION.toString(), idx + 1);",
          "261:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QueryRPCPropertyEnum.RPC_SERVER.toString(), idx + 1);",
          "263:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, RecordEvent.RecordReserveKeyEnum.HOST.toString(), idx + 1);",
          "265:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QueryRPCPropertyEnum.EXCEPTION.toString(), idx + 1);",
          "272:         String[][] hierarchy_dims = new String[1][];",
          "",
          "[Added Lines]",
          "262:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkStageEnum.PROJECT.toString(), idx + 1);",
          "264:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkStageEnum.REALIZATION.toString(), idx + 1);",
          "266:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkStageEnum.CUBOID_ID.toString(), idx + 1);",
          "268:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkStageEnum.QUERY_ID.toString(), idx + 1);",
          "269:         idx++;",
          "270:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkStageEnum.EXECUTION_ID.toString(), idx + 1);",
          "272:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkStageEnum.JOB_ID.toString(), idx + 1);",
          "273:         idx++;",
          "274:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkStageEnum.STAGE_ID.toString(), idx + 1);",
          "275:         idx++;",
          "276:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkStageEnum.IF_SUCCESS.toString(), idx + 1);",
          "277:         idx++;",
          "278:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, QuerySparkStageEnum.SUBMIT_TIME.toString(), idx + 1);",
          "279:         idx++;",
          "280:         rowKeyColDescs[idx] = getRowKeyColDesc(tableName, RecordEvent.RecordReserveKeyEnum.HOST.toString(), idx + 1);",
          "287:         String[][] hierarchy_dims = new String[2][];",
          "289:         hierarchy_dims[1] = new String[4];",
          "290:         hierarchy_dims[1][0] = QuerySparkStageEnum.QUERY_ID.toString();",
          "291:         hierarchy_dims[1][1] = QuerySparkStageEnum.EXECUTION_ID.toString();",
          "292:         hierarchy_dims[1][2] = QuerySparkStageEnum.JOB_ID.toString();",
          "293:         hierarchy_dims[1][3] = QuerySparkStageEnum.STAGE_ID.toString();",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "447:         desc.setDimensions(dimensionDescList);",
          "448:         desc.setMeasures(measureDescList);",
          "449:         desc.setRowkey(rowKeyDesc);",
          "451:         desc.setNotifyList(Lists.<String> newArrayList());",
          "452:         desc.setStatusNeedNotify(Lists.newArrayList(JobStatusEnum.ERROR.toString()));",
          "453:         desc.setAutoMergeTimeRanges(new long[] { 86400000L, 604800000L, 2419200000L });",
          "",
          "[Removed Lines]",
          "450:         desc.setHbaseMapping(hBaseMapping);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeInstanceCreator.java||tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeInstanceCreator.java": [
          "File: tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeInstanceCreator.java -> tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/CubeInstanceCreator.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "34:     public static void main(String[] args) throws Exception {",
          "35:         KylinConfig config = KylinConfig.getInstanceFromEnv();",
          "38:         ByteArrayOutputStream buf = new ByteArrayOutputStream();",
          "39:         DataOutputStream dout = new DataOutputStream(buf);",
          "40:         CubeManager.CUBE_SERIALIZER.serialize(cubeInstance, dout);",
          "",
          "[Removed Lines]",
          "37:         CubeInstance cubeInstance = generateKylinCubeInstanceForMetricsQuery(\"ADMIN\", config, new MetricsSinkDesc());",
          "",
          "[Added Lines]",
          "37:         CubeInstance cubeInstance = generateKylinCubeInstanceForMetricsQueryExecution(\"ADMIN\", config, new MetricsSinkDesc());",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "43:         System.out.println(buf.toString(\"UTF-8\"));",
          "44:     }",
          "47:             MetricsSinkDesc sinkDesc) {",
          "49:     }",
          "52:             MetricsSinkDesc sinkDesc) {",
          "53:         return generateKylinCubeInstance(owner,",
          "55:     }",
          "58:             MetricsSinkDesc sinkDesc) {",
          "59:         return generateKylinCubeInstance(owner,",
          "61:     }",
          "63:     public static CubeInstance generateKylinCubeInstanceForMetricsJob(String owner, KylinConfig config,",
          "",
          "[Removed Lines]",
          "46:     public static CubeInstance generateKylinCubeInstanceForMetricsQuery(String owner, KylinConfig config,",
          "48:         return generateKylinCubeInstance(owner, sinkDesc.getTableNameForMetrics(config.getKylinMetricsSubjectQuery()));",
          "51:     public static CubeInstance generateKylinCubeInstanceForMetricsQueryCube(String owner, KylinConfig config,",
          "54:                 sinkDesc.getTableNameForMetrics(config.getKylinMetricsSubjectQueryCube()));",
          "57:     public static CubeInstance generateKylinCubeInstanceForMetricsQueryRPC(String owner, KylinConfig config,",
          "60:                 sinkDesc.getTableNameForMetrics(config.getKylinMetricsSubjectQueryRpcCall()));",
          "",
          "[Added Lines]",
          "46:     public static CubeInstance generateKylinCubeInstanceForMetricsQueryExecution(String owner, KylinConfig config,",
          "48:         return generateKylinCubeInstance(owner, sinkDesc.getTableNameForMetrics(config.getKylinMetricsSubjectQueryExecution()));",
          "51:     public static CubeInstance generateKylinCubeInstanceForMetricsQuerySparkJob(String owner, KylinConfig config,",
          "54:                 sinkDesc.getTableNameForMetrics(config.getKylinMetricsSubjectQuerySparkJob()));",
          "57:     public static CubeInstance generateKylinCubeInstanceForMetricsQuerySparkStage(String owner, KylinConfig config,",
          "60:                 sinkDesc.getTableNameForMetrics(config.getKylinMetricsSubjectQuerySparkStage()));",
          "",
          "---------------"
        ],
        "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/HiveTableCreator.java||tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/HiveTableCreator.java": [
          "File: tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/HiveTableCreator.java -> tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/HiveTableCreator.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "29: import org.apache.kylin.metrics.lib.impl.hive.HiveProducerRecord;",
          "30: import org.apache.kylin.metrics.lib.impl.hive.HiveReservoirReporter;",
          "31: import org.apache.kylin.metrics.property.JobPropertyEnum;",
          "36: import org.apache.kylin.shaded.com.google.common.base.Strings;",
          "37: import org.apache.kylin.shaded.com.google.common.collect.Lists;",
          "",
          "[Removed Lines]",
          "32: import org.apache.kylin.metrics.property.QueryCubePropertyEnum;",
          "33: import org.apache.kylin.metrics.property.QueryPropertyEnum;",
          "34: import org.apache.kylin.metrics.property.QueryRPCPropertyEnum;",
          "",
          "[Added Lines]",
          "32: import org.apache.kylin.metrics.property.QuerySparkExecutionEnum;",
          "33: import org.apache.kylin.metrics.property.QuerySparkJobEnum;",
          "34: import org.apache.kylin.metrics.property.QuerySparkStageEnum;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "97:     }",
          "99:     public static String generateHiveTableSQLForMetricsQuery(KylinConfig config) {",
          "102:     }",
          "104:     public static String generateHiveTableSQLForMetricsQueryCUBE(KylinConfig config) {",
          "107:     }",
          "109:     public static String generateHiveTableSQLForMetricsQueryRPC(KylinConfig config) {",
          "112:     }",
          "114:     public static String generateHiveTableSQLForMetricsJob(KylinConfig config) {",
          "",
          "[Removed Lines]",
          "100:         String tableName = HiveReservoirReporter.getTableFromSubject(config.getKylinMetricsSubjectQuery());",
          "101:         return generateHiveTableSQL(tableName, getHiveColumnsForMetricsQuery(), getPartitionKVsForHiveTable());",
          "105:         String tableName = HiveReservoirReporter.getTableFromSubject(config.getKylinMetricsSubjectQueryCube());",
          "106:         return generateHiveTableSQL(tableName, getHiveColumnsForMetricsQueryCube(), getPartitionKVsForHiveTable());",
          "110:         String tableName = HiveReservoirReporter.getTableFromSubject(config.getKylinMetricsSubjectQueryRpcCall());",
          "111:         return generateHiveTableSQL(tableName, getHiveColumnsForMetricsQueryRPC(), getPartitionKVsForHiveTable());",
          "",
          "[Added Lines]",
          "99:         String tableName = HiveReservoirReporter.getTableFromSubject(config.getKylinMetricsSubjectQueryExecution());",
          "100:         return generateHiveTableSQL(tableName, getHiveColumnsForMetricsQueryExecution(), getPartitionKVsForHiveTable());",
          "104:         String tableName = HiveReservoirReporter.getTableFromSubject(config.getKylinMetricsSubjectQuerySparkJob());",
          "105:         return generateHiveTableSQL(tableName, getHiveColumnsForMetricsQuerySparkJob(), getPartitionKVsForHiveTable());",
          "109:         String tableName = HiveReservoirReporter.getTableFromSubject(config.getKylinMetricsSubjectQuerySparkStage());",
          "110:         return generateHiveTableSQL(tableName, getHiveColumnsForMetricsQuerySparkStage(), getPartitionKVsForHiveTable());",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "121:         return generateHiveTableSQL(tableName, getHiveColumnsForMetricsJobException(), getPartitionKVsForHiveTable());",
          "122:     }",
          "125:         List<Pair<String, String>> columns = Lists.newLinkedList();",
          "127:         columns.add(new Pair<>(RecordEvent.RecordReserveKeyEnum.HOST.toString(), HiveTypeEnum.HSTRING.toString()));",
          "140:         columns.addAll(getTimeColumnsForMetrics());",
          "141:         return columns;",
          "142:     }",
          "145:         List<Pair<String, String>> columns = Lists.newLinkedList();",
          "146:         columns.add(new Pair<>(RecordEvent.RecordReserveKeyEnum.HOST.toString(), HiveTypeEnum.HSTRING.toString()));",
          "167:         columns.addAll(getTimeColumnsForMetrics());",
          "168:         return columns;",
          "169:     }",
          "172:         List<Pair<String, String>> columns = Lists.newLinkedList();",
          "173:         columns.add(new Pair<>(RecordEvent.RecordReserveKeyEnum.HOST.toString(), HiveTypeEnum.HSTRING.toString()));",
          "186:         columns.addAll(getTimeColumnsForMetrics());",
          "187:         return columns;",
          "",
          "[Removed Lines]",
          "124:     public static List<Pair<String, String>> getHiveColumnsForMetricsQuery() {",
          "126:         columns.add(new Pair<>(QueryPropertyEnum.ID_CODE.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "128:         columns.add(new Pair<>(QueryPropertyEnum.USER.toString(), HiveTypeEnum.HSTRING.toString()));",
          "129:         columns.add(new Pair<>(QueryPropertyEnum.PROJECT.toString(), HiveTypeEnum.HSTRING.toString()));",
          "130:         columns.add(new Pair<>(QueryPropertyEnum.REALIZATION.toString(), HiveTypeEnum.HSTRING.toString()));",
          "131:         columns.add(new Pair<>(QueryPropertyEnum.REALIZATION_TYPE.toString(), HiveTypeEnum.HINT.toString()));",
          "132:         columns.add(new Pair<>(QueryPropertyEnum.TYPE.toString(), HiveTypeEnum.HSTRING.toString()));",
          "134:         columns.add(new Pair<>(QueryPropertyEnum.EXCEPTION.toString(), HiveTypeEnum.HSTRING.toString()));",
          "135:         columns.add(new Pair<>(QueryPropertyEnum.TIME_COST.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "136:         columns.add(new Pair<>(QueryPropertyEnum.CALCITE_RETURN_COUNT.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "137:         columns.add(new Pair<>(QueryPropertyEnum.STORAGE_RETURN_COUNT.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "138:         columns.add(new Pair<>(QueryPropertyEnum.AGGR_FILTER_COUNT.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "144:     public static List<Pair<String, String>> getHiveColumnsForMetricsQueryCube() {",
          "147:         columns.add(new Pair<>(QueryCubePropertyEnum.PROJECT.toString(), HiveTypeEnum.HSTRING.toString()));",
          "148:         columns.add(new Pair<>(QueryCubePropertyEnum.CUBE.toString(), HiveTypeEnum.HSTRING.toString()));",
          "149:         columns.add(new Pair<>(QueryCubePropertyEnum.SEGMENT.toString(), HiveTypeEnum.HSTRING.toString()));",
          "150:         columns.add(new Pair<>(QueryCubePropertyEnum.CUBOID_SOURCE.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "151:         columns.add(new Pair<>(QueryCubePropertyEnum.CUBOID_TARGET.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "152:         columns.add(new Pair<>(QueryCubePropertyEnum.IF_MATCH.toString(), HiveTypeEnum.HBOOLEAN.toString()));",
          "153:         columns.add(new Pair<>(QueryCubePropertyEnum.FILTER_MASK.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "154:         columns.add(new Pair<>(QueryCubePropertyEnum.IF_SUCCESS.toString(), HiveTypeEnum.HBOOLEAN.toString()));",
          "156:         columns.add(new Pair<>(QueryCubePropertyEnum.WEIGHT_PER_HIT.toString(), HiveTypeEnum.HDOUBLE.toString()));",
          "158:         columns.add(new Pair<>(QueryCubePropertyEnum.CALL_COUNT.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "159:         columns.add(new Pair<>(QueryCubePropertyEnum.TIME_SUM.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "160:         columns.add(new Pair<>(QueryCubePropertyEnum.TIME_MAX.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "161:         columns.add(new Pair<>(QueryCubePropertyEnum.SKIP_COUNT.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "162:         columns.add(new Pair<>(QueryCubePropertyEnum.SCAN_COUNT.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "163:         columns.add(new Pair<>(QueryCubePropertyEnum.RETURN_COUNT.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "164:         columns.add(new Pair<>(QueryCubePropertyEnum.AGGR_FILTER_COUNT.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "165:         columns.add(new Pair<>(QueryCubePropertyEnum.AGGR_COUNT.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "171:     public static List<Pair<String, String>> getHiveColumnsForMetricsQueryRPC() {",
          "174:         columns.add(new Pair<>(QueryRPCPropertyEnum.PROJECT.toString(), HiveTypeEnum.HSTRING.toString()));",
          "175:         columns.add(new Pair<>(QueryRPCPropertyEnum.REALIZATION.toString(), HiveTypeEnum.HSTRING.toString()));",
          "176:         columns.add(new Pair<>(QueryRPCPropertyEnum.RPC_SERVER.toString(), HiveTypeEnum.HSTRING.toString()));",
          "177:         columns.add(new Pair<>(QueryRPCPropertyEnum.EXCEPTION.toString(), HiveTypeEnum.HSTRING.toString()));",
          "179:         columns.add(new Pair<>(QueryRPCPropertyEnum.CALL_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "180:         columns.add(new Pair<>(QueryRPCPropertyEnum.RETURN_COUNT.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "181:         columns.add(new Pair<>(QueryRPCPropertyEnum.SCAN_COUNT.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "182:         columns.add(new Pair<>(QueryRPCPropertyEnum.SKIP_COUNT.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "183:         columns.add(new Pair<>(QueryRPCPropertyEnum.AGGR_FILTER_COUNT.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "184:         columns.add(new Pair<>(QueryRPCPropertyEnum.AGGR_COUNT.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "",
          "[Added Lines]",
          "123:     public static List<Pair<String, String>> getHiveColumnsForMetricsQueryExecution() {",
          "125:         columns.add(new Pair<>(QuerySparkExecutionEnum.ID_CODE.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "126:         columns.add(new Pair<>(QuerySparkExecutionEnum.QUERY_ID.toString(), HiveTypeEnum.HSTRING.toString()));",
          "127:         columns.add(new Pair<>(QuerySparkExecutionEnum.EXECUTION_ID.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "129:         columns.add(new Pair<>(QuerySparkExecutionEnum.USER.toString(), HiveTypeEnum.HSTRING.toString()));",
          "130:         columns.add(new Pair<>(QuerySparkExecutionEnum.SPARDER_NAME.toString(), HiveTypeEnum.HSTRING.toString()));",
          "131:         columns.add(new Pair<>(QuerySparkExecutionEnum.PROJECT.toString(), HiveTypeEnum.HSTRING.toString()));",
          "132:         columns.add(new Pair<>(QuerySparkExecutionEnum.REALIZATION.toString(), HiveTypeEnum.HSTRING.toString()));",
          "133:         columns.add(new Pair<>(QuerySparkExecutionEnum.REALIZATION_TYPE.toString(), HiveTypeEnum.HINT.toString()));",
          "134:         columns.add(new Pair<>(QuerySparkExecutionEnum.CUBOID_IDS.toString(), HiveTypeEnum.HSTRING.toString()));",
          "136:         columns.add(new Pair<>(QuerySparkExecutionEnum.TYPE.toString(), HiveTypeEnum.HSTRING.toString()));",
          "137:         columns.add(new Pair<>(QuerySparkExecutionEnum.START_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "138:         columns.add(new Pair<>(QuerySparkExecutionEnum.END_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "140:         columns.add(new Pair<>(QuerySparkExecutionEnum.EXCEPTION.toString(), HiveTypeEnum.HSTRING.toString()));",
          "141:         columns.add(new Pair<>(QuerySparkExecutionEnum.TIME_COST.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "142:         columns.add(new Pair<>(QuerySparkExecutionEnum.TOTAL_SCAN_COUNT.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "143:         columns.add(new Pair<>(QuerySparkExecutionEnum.TOTAL_SCAN_BYTES.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "144:         columns.add(new Pair<>(QuerySparkExecutionEnum.RESULT_COUNT.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "146:         columns.add(new Pair<>(QuerySparkExecutionEnum.EXECUTION_DURATION.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "147:         columns.add(new Pair<>(QuerySparkExecutionEnum.RESULT_SIZE.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "148:         columns.add(new Pair<>(QuerySparkExecutionEnum.EXECUTOR_DESERIALIZE_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "149:         columns.add(new Pair<>(QuerySparkExecutionEnum.EXECUTOR_DESERIALIZE_CPU_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "150:         columns.add(new Pair<>(QuerySparkExecutionEnum.EXECUTOR_RUN_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "151:         columns.add(new Pair<>(QuerySparkExecutionEnum.EXECUTOR_CPU_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "152:         columns.add(new Pair<>(QuerySparkExecutionEnum.JVM_GC_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "153:         columns.add(new Pair<>(QuerySparkExecutionEnum.RESULT_SERIALIZATION_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "154:         columns.add(new Pair<>(QuerySparkExecutionEnum.MEMORY_BYTE_SPILLED.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "155:         columns.add(new Pair<>(QuerySparkExecutionEnum.DISK_BYTES_SPILLED.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "156:         columns.add(new Pair<>(QuerySparkExecutionEnum.PEAK_EXECUTION_MEMORY.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "162:     public static List<Pair<String, String>> getHiveColumnsForMetricsQuerySparkJob() {",
          "164:         columns.add(new Pair<>(QuerySparkJobEnum.QUERY_ID.toString(), HiveTypeEnum.HSTRING.toString()));",
          "165:         columns.add(new Pair<>(QuerySparkJobEnum.PROJECT.toString(), HiveTypeEnum.HSTRING.toString()));",
          "166:         columns.add(new Pair<>(QuerySparkJobEnum.EXECUTION_ID.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "167:         columns.add(new Pair<>(QuerySparkJobEnum.JOB_ID.toString(), HiveTypeEnum.HINT.toString()));",
          "169:         columns.add(new Pair<>(QuerySparkJobEnum.START_TIME.toString(), HiveTypeEnum.HSTRING.toString()));",
          "170:         columns.add(new Pair<>(QuerySparkJobEnum.END_TIME.toString(), HiveTypeEnum.HSTRING.toString()));",
          "171:         columns.add(new Pair<>(QuerySparkJobEnum.IF_SUCCESS.toString(), HiveTypeEnum.HBOOLEAN.toString()));",
          "173:         columns.add(new Pair<>(QuerySparkJobEnum.RESULT_SIZE.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "174:         columns.add(new Pair<>(QuerySparkJobEnum.EXECUTOR_DESERIALIZE_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "175:         columns.add(new Pair<>(QuerySparkJobEnum.EXECUTOR_DESERIALIZE_CPU_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "176:         columns.add(new Pair<>(QuerySparkJobEnum.EXECUTOR_RUN_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "177:         columns.add(new Pair<>(QuerySparkJobEnum.EXECUTOR_CPU_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "178:         columns.add(new Pair<>(QuerySparkJobEnum.JVM_GC_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "179:         columns.add(new Pair<>(QuerySparkJobEnum.RESULT_SERIALIZATION_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "180:         columns.add(new Pair<>(QuerySparkJobEnum.MEMORY_BYTE_SPILLED.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "181:         columns.add(new Pair<>(QuerySparkJobEnum.DISK_BYTES_SPILLED.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "182:         columns.add(new Pair<>(QuerySparkJobEnum.PEAK_EXECUTION_MEMORY.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "188:     public static List<Pair<String, String>> getHiveColumnsForMetricsQuerySparkStage() {",
          "191:         columns.add(new Pair<>(QuerySparkStageEnum.QUERY_ID.toString(), HiveTypeEnum.HSTRING.toString()));",
          "192:         columns.add(new Pair<>(QuerySparkStageEnum.EXECUTION_ID.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "193:         columns.add(new Pair<>(QuerySparkStageEnum.JOB_ID.toString(), HiveTypeEnum.HINT.toString()));",
          "194:         columns.add(new Pair<>(QuerySparkStageEnum.STAGE_ID.toString(), HiveTypeEnum.HINT.toString()));",
          "195:         columns.add(new Pair<>(QuerySparkStageEnum.SUBMIT_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "196:         columns.add(new Pair<>(QuerySparkStageEnum.PROJECT.toString(), HiveTypeEnum.HSTRING.toString()));",
          "197:         columns.add(new Pair<>(QuerySparkStageEnum.REALIZATION.toString(), HiveTypeEnum.HSTRING.toString()));",
          "198:         columns.add(new Pair<>(QuerySparkStageEnum.CUBOID_ID.toString(), HiveTypeEnum.HSTRING.toString()));",
          "199:         columns.add(new Pair<>(QuerySparkStageEnum.IF_SUCCESS.toString(), HiveTypeEnum.HBOOLEAN.toString()));",
          "201:         columns.add(new Pair<>(QuerySparkStageEnum.RESULT_SIZE.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "202:         columns.add(new Pair<>(QuerySparkStageEnum.EXECUTOR_DESERIALIZE_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "203:         columns.add(new Pair<>(QuerySparkStageEnum.EXECUTOR_DESERIALIZE_CPU_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "204:         columns.add(new Pair<>(QuerySparkStageEnum.EXECUTOR_RUN_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "205:         columns.add(new Pair<>(QuerySparkStageEnum.EXECUTOR_CPU_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "206:         columns.add(new Pair<>(QuerySparkStageEnum.JVM_GC_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "207:         columns.add(new Pair<>(QuerySparkStageEnum.RESULT_SERIALIZATION_TIME.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "208:         columns.add(new Pair<>(QuerySparkStageEnum.MEMORY_BYTE_SPILLED.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "209:         columns.add(new Pair<>(QuerySparkStageEnum.DISK_BYTES_SPILLED.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "210:         columns.add(new Pair<>(QuerySparkStageEnum.PEAK_EXECUTION_MEMORY.toString(), HiveTypeEnum.HBIGINT.toString()));",
          "",
          "---------------"
        ],
        "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/KylinTableCreator.java||tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/KylinTableCreator.java": [
          "File: tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/KylinTableCreator.java -> tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/KylinTableCreator.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "39:     public static void main(String[] args) throws Exception {",
          "40:         KylinConfig config = KylinConfig.getInstanceFromEnv();",
          "43:         ByteArrayOutputStream buf = new ByteArrayOutputStream();",
          "44:         DataOutputStream dout = new DataOutputStream(buf);",
          "45:         TableMetadataManager.TABLE_SERIALIZER.serialize(kylinTable, dout);",
          "",
          "[Removed Lines]",
          "42:         TableDesc kylinTable = generateKylinTableForMetricsQuery(config, new MetricsSinkDesc());",
          "",
          "[Added Lines]",
          "42:         TableDesc kylinTable = generateKylinTableForMetricsQueryExecution(config, new MetricsSinkDesc());",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "48:         System.out.println(buf.toString(\"UTF-8\"));",
          "49:     }",
          "52:         List<Pair<String, String>> columns = Lists.newLinkedList();",
          "54:         columns.addAll(HiveTableCreator.getPartitionKVsForHiveTable());",
          "56:     }",
          "59:         List<Pair<String, String>> columns = Lists.newLinkedList();",
          "61:         columns.addAll(HiveTableCreator.getPartitionKVsForHiveTable());",
          "63:     }",
          "66:         List<Pair<String, String>> columns = Lists.newLinkedList();",
          "68:         columns.addAll(HiveTableCreator.getPartitionKVsForHiveTable());",
          "70:     }",
          "72:     public static TableDesc generateKylinTableForMetricsJob(KylinConfig kylinConfig, MetricsSinkDesc sinkDesc) {",
          "",
          "[Removed Lines]",
          "51:     public static TableDesc generateKylinTableForMetricsQuery(KylinConfig kylinConfig, MetricsSinkDesc sinkDesc) {",
          "53:         columns.addAll(HiveTableCreator.getHiveColumnsForMetricsQuery());",
          "55:         return generateKylinTable(kylinConfig, sinkDesc, kylinConfig.getKylinMetricsSubjectQuery(), columns);",
          "58:     public static TableDesc generateKylinTableForMetricsQueryCube(KylinConfig kylinConfig, MetricsSinkDesc sinkDesc) {",
          "60:         columns.addAll(HiveTableCreator.getHiveColumnsForMetricsQueryCube());",
          "62:         return generateKylinTable(kylinConfig, sinkDesc, kylinConfig.getKylinMetricsSubjectQueryCube(), columns);",
          "65:     public static TableDesc generateKylinTableForMetricsQueryRPC(KylinConfig kylinConfig, MetricsSinkDesc sinkDesc) {",
          "67:         columns.addAll(HiveTableCreator.getHiveColumnsForMetricsQueryRPC());",
          "69:         return generateKylinTable(kylinConfig, sinkDesc, kylinConfig.getKylinMetricsSubjectQueryRpcCall(), columns);",
          "",
          "[Added Lines]",
          "51:     public static TableDesc generateKylinTableForMetricsQueryExecution(KylinConfig kylinConfig, MetricsSinkDesc sinkDesc) {",
          "53:         columns.addAll(HiveTableCreator.getHiveColumnsForMetricsQueryExecution());",
          "55:         return generateKylinTable(kylinConfig, sinkDesc, kylinConfig.getKylinMetricsSubjectQueryExecution(), columns);",
          "58:     public static TableDesc generateKylinTableForMetricsQuerySparkJob(KylinConfig kylinConfig, MetricsSinkDesc sinkDesc) {",
          "60:         columns.addAll(HiveTableCreator.getHiveColumnsForMetricsQuerySparkJob());",
          "62:         return generateKylinTable(kylinConfig, sinkDesc, kylinConfig.getKylinMetricsSubjectQuerySparkJob(), columns);",
          "65:     public static TableDesc generateKylinTableForMetricsQuerySparkStage(KylinConfig kylinConfig, MetricsSinkDesc sinkDesc) {",
          "67:         columns.addAll(HiveTableCreator.getHiveColumnsForMetricsQuerySparkStage());",
          "69:         return generateKylinTable(kylinConfig, sinkDesc, kylinConfig.getKylinMetricsSubjectQuerySparkStage(), columns);",
          "",
          "---------------"
        ],
        "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/ModelCreator.java||tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/ModelCreator.java": [
          "File: tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/ModelCreator.java -> tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/ModelCreator.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "32: import org.apache.kylin.metrics.lib.impl.RecordEvent;",
          "33: import org.apache.kylin.metrics.lib.impl.TimePropertyEnum;",
          "34: import org.apache.kylin.metrics.property.JobPropertyEnum;",
          "38: import org.apache.kylin.tool.metrics.systemcube.def.MetricsSinkDesc;",
          "40: import org.apache.kylin.shaded.com.google.common.collect.Lists;",
          "",
          "[Removed Lines]",
          "35: import org.apache.kylin.metrics.property.QueryCubePropertyEnum;",
          "36: import org.apache.kylin.metrics.property.QueryPropertyEnum;",
          "37: import org.apache.kylin.metrics.property.QueryRPCPropertyEnum;",
          "",
          "[Added Lines]",
          "35: import org.apache.kylin.metrics.property.QuerySparkExecutionEnum;",
          "36: import org.apache.kylin.metrics.property.QuerySparkJobEnum;",
          "37: import org.apache.kylin.metrics.property.QuerySparkStageEnum;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "66:     public static DataModelDesc generateKylinModelForMetricsQuery(String owner, KylinConfig kylinConfig,",
          "67:             MetricsSinkDesc sinkDesc) {",
          "71:     }",
          "73:     public static DataModelDesc generateKylinModelForMetricsQueryCube(String owner, KylinConfig kylinConfig,",
          "74:             MetricsSinkDesc sinkDesc) {",
          "78:     }",
          "80:     public static DataModelDesc generateKylinModelForMetricsQueryRPC(String owner, KylinConfig kylinConfig,",
          "81:             MetricsSinkDesc sinkDesc) {",
          "85:     }",
          "87:     public static DataModelDesc generateKylinModelForMetricsJob(String owner, KylinConfig kylinConfig,",
          "",
          "[Removed Lines]",
          "68:         String tableName = sinkDesc.getTableNameForMetrics(kylinConfig.getKylinMetricsSubjectQuery());",
          "69:         return generateKylinModel(owner, tableName, getDimensionsForMetricsQuery(), getMeasuresForMetricsQuery(),",
          "70:                 getPartitionDesc(tableName));",
          "75:         String tableName = sinkDesc.getTableNameForMetrics(kylinConfig.getKylinMetricsSubjectQueryCube());",
          "76:         return generateKylinModel(owner, tableName, getDimensionsForMetricsQueryCube(),",
          "77:                 getMeasuresForMetricsQueryCube(), getPartitionDesc(tableName));",
          "82:         String tableName = sinkDesc.getTableNameForMetrics(kylinConfig.getKylinMetricsSubjectQueryRpcCall());",
          "83:         return generateKylinModel(owner, tableName, getDimensionsForMetricsQueryRPC(), getMeasuresForMetricsQueryRPC(),",
          "84:                 getPartitionDesc(tableName));",
          "",
          "[Added Lines]",
          "68:         String tableName = sinkDesc.getTableNameForMetrics(kylinConfig.getKylinMetricsSubjectQueryExecution());",
          "69:         return generateKylinModel(owner, tableName, getDimensionsForMetricsQueryExecution(),",
          "70:                 getMeasuresForMetricsQueryExecution(), getPartitionDesc(tableName));",
          "75:         String tableName = sinkDesc.getTableNameForMetrics(kylinConfig.getKylinMetricsSubjectQuerySparkJob());",
          "76:         return generateKylinModel(owner, tableName, getDimensionsForMetricsQuerySparkJob(),",
          "77:                 getMeasuresForMetricsQuerySparkJob(), getPartitionDesc(tableName));",
          "82:         String tableName = sinkDesc.getTableNameForMetrics(kylinConfig.getKylinMetricsSubjectQuerySparkStage());",
          "83:         return generateKylinModel(owner, tableName, getDimensionsForMetricsQuerySparkStage(),",
          "84:                 getMeasuresForMetricsQuerySparkStage(), getPartitionDesc(tableName));",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "98:                 getMeasuresForMetricsJobException(), getPartitionDesc(tableName));",
          "99:     }",
          "102:         List<String> result = Lists.newLinkedList();",
          "103:         result.add(RecordEvent.RecordReserveKeyEnum.HOST.toString());",
          "111:         result.addAll(getTimeDimensionsForMetrics());",
          "112:         return result;",
          "113:     }",
          "116:         List<String> result = Lists.newLinkedList();",
          "123:         return result;",
          "124:     }",
          "127:         List<String> result = Lists.newLinkedList();",
          "128:         result.add(RecordEvent.RecordReserveKeyEnum.HOST.toString());",
          "138:         result.addAll(getTimeDimensionsForMetrics());",
          "139:         return result;",
          "140:     }",
          "143:         List<String> result = Lists.newLinkedList();",
          "154:         return result;",
          "155:     }",
          "158:         List<String> result = Lists.newLinkedList();",
          "159:         result.add(RecordEvent.RecordReserveKeyEnum.HOST.toString());",
          "165:         result.addAll(getTimeDimensionsForMetrics());",
          "166:         return result;",
          "167:     }",
          "170:         List<String> result = Lists.newLinkedList();",
          "178:         return result;",
          "179:     }",
          "",
          "[Removed Lines]",
          "101:     public static List<String> getDimensionsForMetricsQuery() {",
          "104:         result.add(QueryPropertyEnum.USER.toString());",
          "105:         result.add(QueryPropertyEnum.PROJECT.toString());",
          "106:         result.add(QueryPropertyEnum.REALIZATION.toString());",
          "107:         result.add(QueryPropertyEnum.REALIZATION_TYPE.toString());",
          "108:         result.add(QueryPropertyEnum.TYPE.toString());",
          "109:         result.add(QueryPropertyEnum.EXCEPTION.toString());",
          "115:     public static List<String> getMeasuresForMetricsQuery() {",
          "117:         result.add(QueryPropertyEnum.ID_CODE.toString());",
          "118:         result.add(QueryPropertyEnum.TIME_COST.toString());",
          "119:         result.add(QueryPropertyEnum.CALCITE_RETURN_COUNT.toString());",
          "120:         result.add(QueryPropertyEnum.STORAGE_RETURN_COUNT.toString());",
          "121:         result.add(QueryPropertyEnum.AGGR_FILTER_COUNT.toString());",
          "126:     public static List<String> getDimensionsForMetricsQueryCube() {",
          "129:         result.add(QueryCubePropertyEnum.PROJECT.toString());",
          "130:         result.add(QueryCubePropertyEnum.CUBE.toString());",
          "131:         result.add(QueryCubePropertyEnum.SEGMENT.toString());",
          "132:         result.add(QueryCubePropertyEnum.CUBOID_SOURCE.toString());",
          "133:         result.add(QueryCubePropertyEnum.CUBOID_TARGET.toString());",
          "134:         result.add(QueryCubePropertyEnum.FILTER_MASK.toString());",
          "135:         result.add(QueryCubePropertyEnum.IF_MATCH.toString());",
          "136:         result.add(QueryCubePropertyEnum.IF_SUCCESS.toString());",
          "142:     public static List<String> getMeasuresForMetricsQueryCube() {",
          "144:         result.add(QueryCubePropertyEnum.WEIGHT_PER_HIT.toString());",
          "145:         result.add(QueryCubePropertyEnum.CALL_COUNT.toString());",
          "146:         result.add(QueryCubePropertyEnum.TIME_SUM.toString());",
          "147:         result.add(QueryCubePropertyEnum.TIME_MAX.toString());",
          "148:         result.add(QueryCubePropertyEnum.SKIP_COUNT.toString());",
          "149:         result.add(QueryCubePropertyEnum.SCAN_COUNT.toString());",
          "150:         result.add(QueryCubePropertyEnum.RETURN_COUNT.toString());",
          "151:         result.add(QueryCubePropertyEnum.AGGR_FILTER_COUNT.toString());",
          "152:         result.add(QueryCubePropertyEnum.AGGR_COUNT.toString());",
          "157:     public static List<String> getDimensionsForMetricsQueryRPC() {",
          "160:         result.add(QueryRPCPropertyEnum.PROJECT.toString());",
          "161:         result.add(QueryRPCPropertyEnum.REALIZATION.toString());",
          "162:         result.add(QueryRPCPropertyEnum.RPC_SERVER.toString());",
          "163:         result.add(QueryRPCPropertyEnum.EXCEPTION.toString());",
          "169:     public static List<String> getMeasuresForMetricsQueryRPC() {",
          "171:         result.add(QueryRPCPropertyEnum.CALL_TIME.toString());",
          "172:         result.add(QueryRPCPropertyEnum.RETURN_COUNT.toString());",
          "173:         result.add(QueryRPCPropertyEnum.SCAN_COUNT.toString());",
          "174:         result.add(QueryRPCPropertyEnum.SKIP_COUNT.toString());",
          "175:         result.add(QueryRPCPropertyEnum.AGGR_FILTER_COUNT.toString());",
          "176:         result.add(QueryRPCPropertyEnum.AGGR_COUNT.toString());",
          "",
          "[Added Lines]",
          "101:     public static List<String> getDimensionsForMetricsQueryExecution() {",
          "104:         result.add(QuerySparkExecutionEnum.USER.toString());",
          "105:         result.add(QuerySparkExecutionEnum.PROJECT.toString());",
          "106:         result.add(QuerySparkExecutionEnum.REALIZATION.toString());",
          "107:         result.add(QuerySparkExecutionEnum.REALIZATION_TYPE.toString());",
          "108:         result.add(QuerySparkExecutionEnum.CUBOID_IDS.toString());",
          "109:         result.add(QuerySparkExecutionEnum.TYPE.toString());",
          "110:         result.add(QuerySparkExecutionEnum.EXCEPTION.toString());",
          "111:         result.add(QuerySparkExecutionEnum.SPARDER_NAME.toString());",
          "112:         result.add(QuerySparkExecutionEnum.QUERY_ID.toString());",
          "113:         result.add(QuerySparkExecutionEnum.START_TIME.toString());",
          "114:         result.add(QuerySparkExecutionEnum.END_TIME.toString());",
          "120:     public static List<String> getMeasuresForMetricsQueryExecution() {",
          "122:         result.add(QuerySparkExecutionEnum.ID_CODE.toString());",
          "123:         result.add(QuerySparkExecutionEnum.TIME_COST.toString());",
          "124:         result.add(QuerySparkExecutionEnum.TOTAL_SCAN_COUNT.toString());",
          "125:         result.add(QuerySparkExecutionEnum.TOTAL_SCAN_BYTES.toString());",
          "126:         result.add(QuerySparkExecutionEnum.RESULT_COUNT.toString());",
          "127:         result.add(QuerySparkExecutionEnum.EXECUTION_DURATION.toString());",
          "128:         result.add(QuerySparkExecutionEnum.RESULT_SIZE.toString());",
          "129:         result.add(QuerySparkExecutionEnum.EXECUTOR_DESERIALIZE_TIME.toString());",
          "130:         result.add(QuerySparkExecutionEnum.EXECUTOR_DESERIALIZE_CPU_TIME.toString());",
          "131:         result.add(QuerySparkExecutionEnum.EXECUTOR_RUN_TIME.toString());",
          "132:         result.add(QuerySparkExecutionEnum.EXECUTOR_CPU_TIME.toString());",
          "133:         result.add(QuerySparkExecutionEnum.JVM_GC_TIME.toString());",
          "134:         result.add(QuerySparkExecutionEnum.RESULT_SERIALIZATION_TIME.toString());",
          "135:         result.add(QuerySparkExecutionEnum.MEMORY_BYTE_SPILLED.toString());",
          "136:         result.add(QuerySparkExecutionEnum.DISK_BYTES_SPILLED.toString());",
          "137:         result.add(QuerySparkExecutionEnum.PEAK_EXECUTION_MEMORY.toString());",
          "141:     public static List<String> getDimensionsForMetricsQuerySparkJob() {",
          "144:         result.add(QuerySparkJobEnum.QUERY_ID.toString());",
          "145:         result.add(QuerySparkJobEnum.EXECUTION_ID.toString());",
          "146:         result.add(QuerySparkJobEnum.JOB_ID.toString());",
          "147:         result.add(QuerySparkJobEnum.PROJECT.toString());",
          "148:         result.add(QuerySparkJobEnum.START_TIME.toString());",
          "149:         result.add(QuerySparkJobEnum.END_TIME.toString());",
          "150:         result.add(QuerySparkJobEnum.IF_SUCCESS.toString());",
          "156:     public static List<String> getMeasuresForMetricsQuerySparkJob() {",
          "158:         result.add(QuerySparkJobEnum.RESULT_SIZE.toString());",
          "159:         result.add(QuerySparkJobEnum.EXECUTOR_DESERIALIZE_TIME.toString());",
          "160:         result.add(QuerySparkJobEnum.EXECUTOR_DESERIALIZE_CPU_TIME.toString());",
          "161:         result.add(QuerySparkJobEnum.EXECUTOR_RUN_TIME.toString());",
          "162:         result.add(QuerySparkJobEnum.EXECUTOR_CPU_TIME.toString());",
          "163:         result.add(QuerySparkJobEnum.JVM_GC_TIME.toString());",
          "164:         result.add(QuerySparkJobEnum.RESULT_SERIALIZATION_TIME.toString());",
          "165:         result.add(QuerySparkJobEnum.MEMORY_BYTE_SPILLED.toString());",
          "166:         result.add(QuerySparkJobEnum.DISK_BYTES_SPILLED.toString());",
          "167:         result.add(QuerySparkJobEnum.PEAK_EXECUTION_MEMORY.toString());",
          "172:     public static List<String> getDimensionsForMetricsQuerySparkStage() {",
          "175:         result.add(QuerySparkStageEnum.QUERY_ID.toString());",
          "176:         result.add(QuerySparkStageEnum.EXECUTION_ID.toString());",
          "177:         result.add(QuerySparkStageEnum.JOB_ID.toString());",
          "178:         result.add(QuerySparkStageEnum.STAGE_ID.toString());",
          "179:         result.add(QuerySparkStageEnum.SUBMIT_TIME.toString());",
          "180:         result.add(QuerySparkStageEnum.PROJECT.toString());",
          "181:         result.add(QuerySparkStageEnum.REALIZATION.toString());",
          "182:         result.add(QuerySparkStageEnum.CUBOID_ID.toString());",
          "183:         result.add(QuerySparkStageEnum.IF_SUCCESS.toString());",
          "189:     public static List<String> getMeasuresForMetricsQuerySparkStage() {",
          "191:         result.add(QuerySparkStageEnum.RESULT_SIZE.toString());",
          "192:         result.add(QuerySparkStageEnum.EXECUTOR_DESERIALIZE_TIME.toString());",
          "193:         result.add(QuerySparkStageEnum.EXECUTOR_DESERIALIZE_CPU_TIME.toString());",
          "194:         result.add(QuerySparkStageEnum.EXECUTOR_RUN_TIME.toString());",
          "195:         result.add(QuerySparkStageEnum.EXECUTOR_CPU_TIME.toString());",
          "196:         result.add(QuerySparkStageEnum.JVM_GC_TIME.toString());",
          "197:         result.add(QuerySparkStageEnum.RESULT_SERIALIZATION_TIME.toString());",
          "198:         result.add(QuerySparkStageEnum.MEMORY_BYTE_SPILLED.toString());",
          "199:         result.add(QuerySparkStageEnum.DISK_BYTES_SPILLED.toString());",
          "200:         result.add(QuerySparkStageEnum.PEAK_EXECUTION_MEMORY.toString());",
          "",
          "---------------"
        ],
        "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/SCCreator.java||tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/SCCreator.java": [
          "File: tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/SCCreator.java -> tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/SCCreator.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "187:     private List<TableDesc> generateKylinTableForSystemCube(MetricsSinkDesc sinkDesc) {",
          "188:         List<TableDesc> result = Lists.newLinkedList();",
          "192:         result.add(KylinTableCreator.generateKylinTableForMetricsJob(config, sinkDesc));",
          "193:         result.add(KylinTableCreator.generateKylinTableForMetricsJobException(config, sinkDesc));",
          "",
          "[Removed Lines]",
          "189:         result.add(KylinTableCreator.generateKylinTableForMetricsQuery(config, sinkDesc));",
          "190:         result.add(KylinTableCreator.generateKylinTableForMetricsQueryCube(config, sinkDesc));",
          "191:         result.add(KylinTableCreator.generateKylinTableForMetricsQueryRPC(config, sinkDesc));",
          "",
          "[Added Lines]",
          "189:         result.add(KylinTableCreator.generateKylinTableForMetricsQueryExecution(config, sinkDesc));",
          "190:         result.add(KylinTableCreator.generateKylinTableForMetricsQuerySparkJob(config, sinkDesc));",
          "191:         result.add(KylinTableCreator.generateKylinTableForMetricsQuerySparkStage(config, sinkDesc));",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "209:     private List<CubeDesc> generateKylinCubeDescForSystemCube(MetricsSinkDesc sinkDesc) {",
          "210:         List<CubeDesc> result = Lists.newLinkedList();",
          "214:         result.add(CubeDescCreator.generateKylinCubeDescForMetricsJob(config, sinkDesc));",
          "215:         result.add(CubeDescCreator.generateKylinCubeDescForMetricsJobException(config, sinkDesc));",
          "",
          "[Removed Lines]",
          "211:         result.add(CubeDescCreator.generateKylinCubeDescForMetricsQuery(config, sinkDesc));",
          "212:         result.add(CubeDescCreator.generateKylinCubeDescForMetricsQueryCube(config, sinkDesc));",
          "213:         result.add(CubeDescCreator.generateKylinCubeDescForMetricsQueryRPC(config, sinkDesc));",
          "",
          "[Added Lines]",
          "211:         result.add(CubeDescCreator.generateKylinCubeDescForMetricsQueryExecution(config, sinkDesc));",
          "212:         result.add(CubeDescCreator.generateKylinCubeDescForMetricsQuerySparkJob(config, sinkDesc));",
          "213:         result.add(CubeDescCreator.generateKylinCubeDescForMetricsQuerySparkStage(config, sinkDesc));",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "220:     private List<CubeInstance> generateKylinCubeInstanceForSystemCube(String owner, MetricsSinkDesc sinkDesc) {",
          "221:         List<CubeInstance> result = Lists.newLinkedList();",
          "225:         result.add(CubeInstanceCreator.generateKylinCubeInstanceForMetricsJob(owner, config, sinkDesc));",
          "226:         result.add(CubeInstanceCreator.generateKylinCubeInstanceForMetricsJobException(owner, config, sinkDesc));",
          "",
          "[Removed Lines]",
          "222:         result.add(CubeInstanceCreator.generateKylinCubeInstanceForMetricsQuery(owner, config, sinkDesc));",
          "223:         result.add(CubeInstanceCreator.generateKylinCubeInstanceForMetricsQueryCube(owner, config, sinkDesc));",
          "224:         result.add(CubeInstanceCreator.generateKylinCubeInstanceForMetricsQueryRPC(owner, config, sinkDesc));",
          "",
          "[Added Lines]",
          "222:         result.add(CubeInstanceCreator.generateKylinCubeInstanceForMetricsQueryExecution(owner, config, sinkDesc));",
          "223:         result.add(CubeInstanceCreator.generateKylinCubeInstanceForMetricsQuerySparkJob(owner, config, sinkDesc));",
          "224:         result.add(CubeInstanceCreator.generateKylinCubeInstanceForMetricsQuerySparkStage(owner, config, sinkDesc));",
          "",
          "---------------"
        ],
        "tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/streamingv2/KafkaTopicCreator.java||tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/streamingv2/KafkaTopicCreator.java": [
          "File: tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/streamingv2/KafkaTopicCreator.java -> tool/src/main/java/org/apache/kylin/tool/metrics/systemcube/streamingv2/KafkaTopicCreator.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "25:     public static String generateCreateCommand(KylinConfig config) {",
          "26:         StringBuilder sb = new StringBuilder();",
          "27:         String[] topics = new String[]{",
          "31:                 config.getKylinMetricsSubjectJob(),",
          "32:                 config.getKylinMetricsSubjectJobException()};",
          "33:         for (String topic : topics) {",
          "",
          "[Removed Lines]",
          "28:                 config.getKylinMetricsSubjectQuery(),",
          "29:                 config.getKylinMetricsSubjectQueryCube(),",
          "30:                 config.getKylinMetricsSubjectQueryRpcCall(),",
          "",
          "[Added Lines]",
          "28:                 config.getKylinMetricsSubjectQueryExecution(),",
          "29:                 config.getKylinMetricsSubjectQuerySparkJob(),",
          "30:                 config.getKylinMetricsSubjectQuerySparkStage(),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "d7ca7a5918f28ec7bf8de187014451c5709456e4",
      "candidate_info": {
        "commit_hash": "d7ca7a5918f28ec7bf8de187014451c5709456e4",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/d7ca7a5918f28ec7bf8de187014451c5709456e4",
        "files": [
          "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala",
          "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/ResetShufflePartition.scala"
        ],
        "message": "KYLIN-4918 Support Cube Level configuration in FilePruner",
        "before_after_code_files": [
          "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala||kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala",
          "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/ResetShufflePartition.scala||kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/ResetShufflePartition.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala||kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala": [
          "File: kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala -> kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/FilePruner.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import java.sql.{Date, Timestamp}",
          "23: import org.apache.hadoop.fs.{FileStatus, Path}",
          "25: import org.apache.kylin.common.util.DateFormat",
          "26: import org.apache.kylin.cube.cuboid.Cuboid",
          "27: import org.apache.kylin.cube.CubeInstance",
          "",
          "[Removed Lines]",
          "24: import org.apache.kylin.common.KylinConfig",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "78:                  val options: Map[String, String])",
          "79:   extends FileIndex with ResetShufflePartition with Logging {",
          "81:   private lazy val segmentDirs: Seq[SegmentDirectory] = {",
          "82:     cubeInstance.getSegments.asScala",
          "83:       .filter(_.getStatus.equals(SegmentStatusEnum.READY)).map(seg => {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "80:   val MAX_SHARDING_SIZE_PER_TASK: Long =",
          "81:     cubeInstance.getConfig.getMaxShardingSizeMBPerTask * 1024 * 1024",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "170:   }",
          "172:   private def genShardSpec(selected: Seq[SegmentDirectory]): Option[ShardSpec] = {",
          "174:       None",
          "175:     } else {",
          "176:       val segments = selected.par.map { segDir =>",
          "",
          "[Removed Lines]",
          "173:     if (!KylinConfig.getInstanceFromEnv.isShardingJoinOptEnabled || selected.isEmpty) {",
          "",
          "[Added Lines]",
          "175:     if (!cubeInstance.getConfig.isShardingJoinOptEnabled || selected.isEmpty) {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "190:           (FilePruner.getPartitionId(f.getPath), f.getLen)",
          "191:         ).groupBy(_._1).mapValues(_.map(_._2).sum)",
          "194:           logInfo(s\"There are some partition ids which the file size exceeds the \" +",
          "196:           None",
          "197:         } else {",
          "198:           val sortColumns = if (segments.length == 1) {",
          "",
          "[Removed Lines]",
          "193:         if (partitionSizePerId.exists(_._2 > FilePruner.MAX_SHARDING_SIZE_PER_TASK)) {",
          "195:             s\"threshold size ${FilePruner.MAX_SHARDING_SIZE_PER_TASK}, skip shard join.\")",
          "",
          "[Added Lines]",
          "195:         if (partitionSizePerId.exists(_._2 > MAX_SHARDING_SIZE_PER_TASK)) {",
          "197:             s\"threshold size ${MAX_SHARDING_SIZE_PER_TASK}, skip shard join.\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "260:     val totalFileSize = selected.flatMap(_.files).map(_.getLen).sum",
          "261:     logInfo(s\"After files pruning, total file size is ${totalFileSize}\")",
          "263:     logInfo(s\"Files pruning in ${(System.nanoTime() - startTime).toDouble / 1000000} ms\")",
          "264:     if (selected.isEmpty) {",
          "265:       val value = Seq.empty[PartitionDirectory]",
          "",
          "[Removed Lines]",
          "262:     setShufflePartitions(totalFileSize, session)",
          "",
          "[Added Lines]",
          "264:     setShufflePartitions(totalFileSize, session, cubeInstance.getConfig)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "448: object FilePruner {",
          "453:   def getPartitionId(p: Path): Int = {",
          "",
          "[Removed Lines]",
          "450:   val MAX_SHARDING_SIZE_PER_TASK: Long = KylinConfig.getInstanceFromEnv",
          "451:     .getMaxShardingSizeMBPerTask * 1024 * 1024",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/ResetShufflePartition.scala||kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/ResetShufflePartition.scala": [
          "File: kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/ResetShufflePartition.scala -> kylin-spark-project/kylin-spark-common/src/main/scala/org/apache/spark/sql/execution/datasource/ResetShufflePartition.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import org.apache.spark.utils.SparderUtils",
          "25: trait ResetShufflePartition extends Logging {",
          "30:     QueryContextFacade.current().addAndGetSourceScanBytes(bytes)",
          "31:     val defaultParallelism = SparderUtils.getTotalCore(sparkSession.sparkContext.getConf)",
          "35:     } else {",
          "37:         defaultParallelism).toInt",
          "38:     }",
          "",
          "[Removed Lines]",
          "26:   val PARTITION_SPLIT_BYTES: Long =",
          "27:     KylinConfig.getInstanceFromEnv.getQueryPartitionSplitSizeMB * 1024 * 1024 // 64MB",
          "29:   def setShufflePartitions(bytes: Long, sparkSession: SparkSession): Unit = {",
          "32:     val kylinConfig = KylinConfig.getInstanceFromEnv",
          "33:     val partitionsNum = if (kylinConfig.getSparkSqlShufflePartitions != -1) {",
          "34:       kylinConfig.getSparkSqlShufflePartitions",
          "36:       Math.min(QueryContextFacade.current().getSourceScanBytes / PARTITION_SPLIT_BYTES + 1,",
          "",
          "[Added Lines]",
          "27:   def setShufflePartitions(bytes: Long, sparkSession: SparkSession, conf: KylinConfig): Unit = {",
          "30:     val partitionsNum = if (conf.getSparkSqlShufflePartitions != -1) {",
          "31:       conf.getSparkSqlShufflePartitions",
          "33:       Math.min(QueryContextFacade.current().getSourceScanBytes /",
          "34:         (conf.getQueryPartitionSplitSizeMB * 1024 * 1024) + 1,",
          "",
          "---------------"
        ]
      }
    }
  ]
}