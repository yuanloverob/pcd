{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "606a99f4f2d91ea30c81285d6c95ee566e80577f",
      "candidate_info": {
        "commit_hash": "606a99f4f2d91ea30c81285d6c95ee566e80577f",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/606a99f4f2d91ea30c81285d6c95ee566e80577f",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala"
        ],
        "message": "[SPARK-39046][SQL] Return an empty context string if TreeNode.origin is wrongly set\n\n### What changes were proposed in this pull request?\n\nFor the query context `TreeNode.origin.context`, this PR proposal to return an empty context string if\n* the query text/ the start index/ the stop index is missing\n* the start index is less than 0\n* the stop index is larger than the length of query text\n* the start index is larger than the stop index\n\n### Why are the changes needed?\n\nThere are downstream projects that depend on Spark. There is no guarantee for the correctness of TreeNode.origin. Developers may create a plan/expression with a Origin containing wrong startIndex/stopIndex/sqlText.\nThus, to avoid errors in calling `String.substring` or showing misleading debug information, I suggest returning an empty context string if TreeNode.origin is wrongly set. The query context is just for better error messages and we should handle it cautiously.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo, the context framework is not released yet.\n\n### How was this patch tested?\n\nUT\n\nCloses #36379 from gengliangwang/safeContext.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>\n(cherry picked from commit 7fe2759e9f81ec267e92e1c6f8a48f42042db791)\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "79:       \"\"",
          "83:     } else {",
          "134:         currentIndex += 1",
          "135:       }",
          "137:     }",
          "140: }",
          "",
          "[Removed Lines]",
          "75:   lazy val context: String = sqlText.map { text =>",
          "76:     val positionContext = if (line.isDefined && startPosition.isDefined) {",
          "77:       s\"(line ${line.get}, position ${startPosition.get})\"",
          "78:     } else {",
          "80:     }",
          "81:     val objectContext = if (objectType.isDefined && objectName.isDefined) {",
          "82:       s\" of ${objectType.get} ${objectName.get}\"",
          "84:       \"\"",
          "85:     }",
          "86:     val builder = new StringBuilder",
          "87:     builder ++= s\"\\n== SQL$objectContext$positionContext ==\\n\"",
          "89:     val start = startIndex.getOrElse(0)",
          "90:     val stop = stopIndex.getOrElse(sqlText.get.length - 1)",
          "95:     val maxExtraContextLength = 32",
          "96:     val truncatedText = \"...\"",
          "97:     var lineStartIndex = start",
          "100:     while(lineStartIndex >= 0 &&",
          "101:       start - lineStartIndex <= maxExtraContextLength &&",
          "102:       text.charAt(lineStartIndex) != '\\n') {",
          "103:       lineStartIndex -= 1",
          "104:     }",
          "105:     val startTruncated = start - lineStartIndex > maxExtraContextLength",
          "106:     var currentIndex = lineStartIndex",
          "107:     if (startTruncated) {",
          "108:       currentIndex -= truncatedText.length",
          "109:     }",
          "111:     var lineStopIndex = stop",
          "114:     while(lineStopIndex < text.length &&",
          "115:       lineStopIndex - stop <= maxExtraContextLength &&",
          "116:       text.charAt(lineStopIndex) != '\\n') {",
          "117:       lineStopIndex += 1",
          "118:     }",
          "119:     val stopTruncated = lineStopIndex - stop > maxExtraContextLength",
          "121:     val subText = (if (startTruncated) truncatedText else \"\") +",
          "122:       text.substring(lineStartIndex + 1, lineStopIndex) +",
          "123:       (if (stopTruncated) truncatedText else \"\")",
          "124:     val lines = subText.split(\"\\n\")",
          "125:     lines.foreach { lineText =>",
          "126:       builder ++= lineText + \"\\n\"",
          "127:       currentIndex += 1",
          "128:       (0 until lineText.length).foreach { _ =>",
          "129:         if (currentIndex < start) {",
          "130:           builder ++= \" \"",
          "131:         } else if (currentIndex >= start && currentIndex <= stop) {",
          "132:           builder ++= \"^\"",
          "133:         }",
          "136:       builder ++= \"\\n\"",
          "138:     builder.result()",
          "139:   }.getOrElse(\"\")",
          "",
          "[Added Lines]",
          "75:   lazy val context: String = {",
          "77:     if (sqlText.isEmpty || startIndex.isEmpty || stopIndex.isEmpty ||",
          "78:       startIndex.get < 0 || stopIndex.get >= sqlText.get.length || startIndex.get > stopIndex.get) {",
          "81:       val positionContext = if (line.isDefined && startPosition.isDefined) {",
          "82:         s\"(line ${line.get}, position ${startPosition.get})\"",
          "83:       } else {",
          "84:         \"\"",
          "85:       }",
          "86:       val objectContext = if (objectType.isDefined && objectName.isDefined) {",
          "87:         s\" of ${objectType.get} ${objectName.get}\"",
          "88:       } else {",
          "89:         \"\"",
          "90:       }",
          "91:       val builder = new StringBuilder",
          "92:       builder ++= s\"\\n== SQL$objectContext$positionContext ==\\n\"",
          "94:       val text = sqlText.get",
          "95:       val start = math.max(startIndex.get, 0)",
          "96:       val stop = math.min(stopIndex.getOrElse(text.length - 1), text.length - 1)",
          "102:       val maxExtraContextLength = 32",
          "103:       val truncatedText = \"...\"",
          "104:       var lineStartIndex = start",
          "107:       while (lineStartIndex >= 0 &&",
          "108:         start - lineStartIndex <= maxExtraContextLength &&",
          "109:         text.charAt(lineStartIndex) != '\\n') {",
          "110:         lineStartIndex -= 1",
          "111:       }",
          "112:       val startTruncated = start - lineStartIndex > maxExtraContextLength",
          "113:       var currentIndex = lineStartIndex",
          "114:       if (startTruncated) {",
          "115:         currentIndex -= truncatedText.length",
          "116:       }",
          "118:       var lineStopIndex = stop",
          "121:       while (lineStopIndex < text.length &&",
          "122:         lineStopIndex - stop <= maxExtraContextLength &&",
          "123:         text.charAt(lineStopIndex) != '\\n') {",
          "124:         lineStopIndex += 1",
          "125:       }",
          "126:       val stopTruncated = lineStopIndex - stop > maxExtraContextLength",
          "128:       val truncatedSubText = (if (startTruncated) truncatedText else \"\") +",
          "129:         text.substring(lineStartIndex + 1, lineStopIndex) +",
          "130:         (if (stopTruncated) truncatedText else \"\")",
          "131:       val lines = truncatedSubText.split(\"\\n\")",
          "132:       lines.foreach { lineText =>",
          "133:         builder ++= lineText + \"\\n\"",
          "135:         (0 until lineText.length).foreach { _ =>",
          "136:           if (currentIndex < start) {",
          "137:             builder ++= \" \"",
          "138:           } else if (currentIndex >= start && currentIndex <= stop) {",
          "139:             builder ++= \"^\"",
          "140:           }",
          "141:           currentIndex += 1",
          "142:         }",
          "143:         builder ++= \"\\n\"",
          "145:       builder.result()",
          "147:   }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "93:         line = Some(1),",
          "94:         startPosition = Some(7),",
          "95:         startIndex = Some(7),",
          "96:         sqlText = Some(s\"select $query\"))",
          "97:       withOrigin(o) {",
          "98:         val expr = Add(maxValue, maxValue, failOnError = true)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "96:         stopIndex = Some(7 + query.length -1),",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "177:         line = Some(1),",
          "178:         startPosition = Some(7),",
          "179:         startIndex = Some(7),",
          "180:         sqlText = Some(s\"select $query\"))",
          "181:       withOrigin(o) {",
          "182:         val expr = Subtract(minValue, maxValue, failOnError = true)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "181:         stopIndex = Some(7 + query.length -1),",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "215:         line = Some(1),",
          "216:         startPosition = Some(7),",
          "217:         startIndex = Some(7),",
          "218:         sqlText = Some(s\"select $query\"))",
          "219:       withOrigin(o) {",
          "220:         val expr = Multiply(maxValue, maxValue, failOnError = true)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "220:         stopIndex = Some(7 + query.length -1),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "259:       line = Some(1),",
          "260:       startPosition = Some(7),",
          "261:       startIndex = Some(7),",
          "262:       sqlText = Some(s\"select $query\"))",
          "263:     withOrigin(o) {",
          "264:       val expr = Divide(Literal(1234.5, DoubleType), Literal(0.0, DoubleType), failOnError = true)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "265:       stopIndex = Some(7 + query.length -1),",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "312:         line = Some(1),",
          "313:         startPosition = Some(7),",
          "314:         startIndex = Some(7),",
          "315:         sqlText = Some(s\"select $query\"))",
          "316:       withOrigin(o) {",
          "317:         val expr =",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "319:         stopIndex = Some(7 + query.length -1),",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "362:           line = Some(1),",
          "363:           startPosition = Some(7),",
          "364:           startIndex = Some(7),",
          "365:           sqlText = Some(s\"select $query\"))",
          "366:         withOrigin(o) {",
          "367:           checkExceptionInExpression[ArithmeticException](expr, EmptyRow, query)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "370:           stopIndex = Some(7 + query.length -1),",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "889:     assert(origin.context == expected)",
          "890:   }",
          "891: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "892:   test(\"SPARK-39046: Return an empty context string if TreeNode.origin is wrongly set\") {",
          "893:     val text = Some(\"select a + b\")",
          "895:     val origin1 = Origin(",
          "896:       startIndex = Some(7),",
          "897:       stopIndex = None,",
          "898:       sqlText = text)",
          "900:     val origin2 = Origin(",
          "901:       startIndex = None,",
          "902:       stopIndex = Some(11),",
          "903:       sqlText = text)",
          "905:     val origin3 = Origin(",
          "906:       startIndex = Some(7),",
          "907:       stopIndex = Some(11),",
          "908:       sqlText = None)",
          "910:     val origin4 = Origin(",
          "911:       startIndex = Some(-1),",
          "912:       stopIndex = Some(11),",
          "913:       sqlText = text)",
          "915:     val origin5 = Origin(",
          "916:       startIndex = Some(-1),",
          "917:       stopIndex = Some(text.get.length),",
          "918:       sqlText = text)",
          "920:     val origin6 = Origin(",
          "921:       startIndex = Some(2),",
          "922:       stopIndex = Some(1),",
          "923:       sqlText = text)",
          "924:     Seq(origin1, origin2, origin3, origin4, origin5, origin6).foreach { origin =>",
          "925:       assert(origin.context.isEmpty)",
          "926:     }",
          "927:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ea1a426a889626f1ee1933e3befaa975a2f0a072",
      "candidate_info": {
        "commit_hash": "ea1a426a889626f1ee1933e3befaa975a2f0a072",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/ea1a426a889626f1ee1933e3befaa975a2f0a072",
        "files": [
          "assembly/pom.xml",
          "common/kvstore/pom.xml",
          "common/network-common/pom.xml",
          "common/network-shuffle/pom.xml",
          "common/network-yarn/pom.xml",
          "common/sketch/pom.xml",
          "common/tags/pom.xml",
          "common/unsafe/pom.xml",
          "core/pom.xml",
          "docs/_config.yml",
          "examples/pom.xml",
          "external/avro/pom.xml",
          "external/docker-integration-tests/pom.xml",
          "external/kafka-0-10-assembly/pom.xml",
          "external/kafka-0-10-sql/pom.xml",
          "external/kafka-0-10-token-provider/pom.xml",
          "external/kafka-0-10/pom.xml",
          "external/kinesis-asl-assembly/pom.xml",
          "external/kinesis-asl/pom.xml",
          "external/spark-ganglia-lgpl/pom.xml",
          "graphx/pom.xml",
          "hadoop-cloud/pom.xml",
          "launcher/pom.xml",
          "mllib-local/pom.xml",
          "mllib/pom.xml",
          "pom.xml",
          "python/pyspark/version.py",
          "repl/pom.xml",
          "resource-managers/kubernetes/core/pom.xml",
          "resource-managers/kubernetes/integration-tests/pom.xml",
          "resource-managers/mesos/pom.xml",
          "resource-managers/yarn/pom.xml",
          "sql/catalyst/pom.xml",
          "sql/core/pom.xml",
          "sql/hive-thriftserver/pom.xml",
          "sql/hive/pom.xml",
          "streaming/pom.xml",
          "tools/pom.xml"
        ],
        "message": "Preparing Spark release v3.3.1-rc1",
        "before_after_code_files": [
          "python/pyspark/version.py||python/pyspark/version.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/version.py||python/pyspark/version.py": [
          "File: python/pyspark/version.py -> python/pyspark/version.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # See the License for the specific language governing permissions and",
          "17: # limitations under the License.",
          "",
          "[Removed Lines]",
          "19: __version__: str = \"3.3.1.dev0\"",
          "",
          "[Added Lines]",
          "19: __version__: str = \"3.3.1\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2977791ca7974aaacdf02f9c7b4f7bd83a8c2628",
      "candidate_info": {
        "commit_hash": "2977791ca7974aaacdf02f9c7b4f7bd83a8c2628",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/2977791ca7974aaacdf02f9c7b4f7bd83a8c2628",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasingSuite.scala"
        ],
        "message": "[SPARK-38529][SQL] Prevent GeneratorNestedColumnAliasing to be applied to non-Explode generators\n\n### What changes were proposed in this pull request?\n1. Explicitly return in GeneratorNestedColumnAliasing when the generator is not Explode.\n2. Add extensive comment to GeneratorNestedColumnAliasing.\n3. An off-hand code refactor to make the code clearer.\n\n### Why are the changes needed?\nGeneratorNestedColumnAliasing does not handle other generators correctly. We only try to rewrite the generator for Explode but try to rewrite all ExtractValue expressions. This can cause inconsistency for non-Explode generators.\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nUnit tests.\n\nCloses #35850 from minyyy/gnca_non_explode.\n\nAuthored-by: minyyy <min.yang@databricks.com>\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>\n(cherry picked from commit 026102489b8edce827a05a1dba3b0ef8687f134f)\nSigned-off-by: Wenchen Fan <wenchen@databricks.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasingSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasingSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "366:         case _ =>",
          "367:       }",
          "371:       def collectNestedGetStructFields(e: Expression): Seq[Expression] = {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "401:       if (!g.generator.isInstanceOf[ExplodeBase]) {",
          "402:         return Some(pushedThrough)",
          "403:       }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasingSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasingSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasingSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasingSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "849:     comparePlans(optimized, expected)",
          "850:   }",
          "851: }",
          "853: object NestedColumnAliasingSuite {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "852:   test(\"SPARK-38529: GeneratorNestedColumnAliasing does not pushdown for non-Explode\") {",
          "853:     val employer = StructType.fromDDL(\"id int, company struct<name:string, address:string>\")",
          "854:     val input = LocalRelation(",
          "855:       'col1.int,",
          "856:       'col2.array(ArrayType(StructType.fromDDL(\"field1 struct<col1: int, col2: int>, field2 int\")))",
          "857:     )",
          "858:     val plan = input.generate(Inline('col2)).select('field1.getField(\"col1\")).analyze",
          "859:     val optimized = GeneratorNestedColumnAliasing.unapply(plan)",
          "861:     comparePlans(plan, RemoveNoopOperators.apply(optimized.get))",
          "862:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "9c5f38d808573ced34bb52bdf4c5102ff2d1a7e2",
      "candidate_info": {
        "commit_hash": "9c5f38d808573ced34bb52bdf4c5102ff2d1a7e2",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/9c5f38d808573ced34bb52bdf4c5102ff2d1a7e2",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SchemaPruning.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala"
        ],
        "message": "[SPARK-38977][SQL] Fix schema pruning with correlated subqueries\n\n### What changes were proposed in this pull request?\n\nThis PR fixes schema pruning for queries with multiple correlated subqueries. Previously, Spark would throw an exception trying to determine root fields in `SchemaPruning$identifyRootFields`. That was happening because expressions in predicates that referenced attributes in subqueries were not ignored. That's why attributes from multiple subqueries could conflict with each other (e.g. incompatible types) even though they should be ignored.\n\nFor instance, the following query would throw a runtime exception.\n\n```\nSELECT name FROM contacts c\nWHERE\n EXISTS (SELECT 1 FROM ids i WHERE i.value = c.id)\n AND\n EXISTS (SELECT 1 FROM first_names n WHERE c.name.first = n.value)\n```\n```\n[info]   org.apache.spark.SparkException: Failed to merge fields 'value' and 'value'. Failed to merge incompatible data types int and string\n[info]   at org.apache.spark.sql.errors.QueryExecutionErrors$.failedMergingFieldsError(QueryExecutionErrors.scala:936)\n```\n\n### Why are the changes needed?\n\nThese changes are needed to avoid exceptions for some queries with multiple correlated subqueries.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nThis PR comes with tests.\n\nCloses #36303 from aokolnychyi/spark-38977.\n\nAuthored-by: Anton Okolnychyi <aokolnychyi@apple.com>\nSigned-off-by: Liang-Chi Hsieh <viirya@gmail.com>\n(cherry picked from commit 0c9947dabcb71de414c97c0e60a1067e468f2642)\nSigned-off-by: Liang-Chi Hsieh <viirya@gmail.com>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SchemaPruning.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SchemaPruning.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SchemaPruning.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SchemaPruning.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SchemaPruning.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SchemaPruning.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "152:         RootField(field, derivedFromAtt = false, prunedIfAnyChildAccessed = true) :: Nil",
          "153:       case IsNotNull(_: Attribute) | IsNull(_: Attribute) =>",
          "154:         expr.children.flatMap(getRootFields).map(_.copy(prunedIfAnyChildAccessed = true))",
          "155:       case _ =>",
          "156:         expr.children.flatMap(getRootFields)",
          "157:     }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "155:       case s: SubqueryExpression =>",
          "158:         s.references.toSeq.flatMap(getRootFields)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "935:       .count()",
          "936:     assert(count == 0)",
          "937:   }",
          "938: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "939:   testSchemaPruning(\"SPARK-38977: schema pruning with correlated EXISTS subquery\") {",
          "941:     import testImplicits._",
          "943:     withTempView(\"ids\", \"first_names\") {",
          "944:       val df1 = Seq(1, 2, 3).toDF(\"value\")",
          "945:       df1.createOrReplaceTempView(\"ids\")",
          "947:       val df2 = Seq(\"John\", \"Bob\").toDF(\"value\")",
          "948:       df2.createOrReplaceTempView(\"first_names\")",
          "950:       val query = sql(",
          "951:         \"\"\"SELECT name FROM contacts c",
          "952:           |WHERE",
          "953:           |  EXISTS (SELECT 1 FROM ids i WHERE i.value = c.id)",
          "954:           |  AND",
          "955:           |  EXISTS (SELECT 1 FROM first_names n WHERE c.name.first = n.value)",
          "956:           |\"\"\".stripMargin)",
          "958:       checkScan(query, \"struct<id:int,name:struct<first:string,middle:string,last:string>>\")",
          "960:       checkAnswer(query, Row(Row(\"John\", \"Y.\", \"Doe\")) :: Nil)",
          "961:     }",
          "962:   }",
          "964:   testSchemaPruning(\"SPARK-38977: schema pruning with correlated NOT EXISTS subquery\") {",
          "966:     import testImplicits._",
          "968:     withTempView(\"ids\", \"first_names\") {",
          "969:       val df1 = Seq(1, 2, 3).toDF(\"value\")",
          "970:       df1.createOrReplaceTempView(\"ids\")",
          "972:       val df2 = Seq(\"John\", \"Bob\").toDF(\"value\")",
          "973:       df2.createOrReplaceTempView(\"first_names\")",
          "975:       val query = sql(",
          "976:         \"\"\"SELECT name FROM contacts c",
          "977:           |WHERE",
          "978:           |  NOT EXISTS (SELECT 1 FROM ids i WHERE i.value = c.id)",
          "979:           |  AND",
          "980:           |  NOT EXISTS (SELECT 1 FROM first_names n WHERE c.name.first = n.value)",
          "981:           |\"\"\".stripMargin)",
          "983:       checkScan(query, \"struct<id:int,name:struct<first:string,middle:string,last:string>>\")",
          "985:       checkAnswer(query, Row(Row(\"Jane\", \"X.\", \"Doe\")) :: Nil)",
          "986:     }",
          "987:   }",
          "989:   testSchemaPruning(\"SPARK-38977: schema pruning with correlated IN subquery\") {",
          "991:     import testImplicits._",
          "993:     withTempView(\"ids\", \"first_names\") {",
          "994:       val df1 = Seq(1, 2, 3).toDF(\"value\")",
          "995:       df1.createOrReplaceTempView(\"ids\")",
          "997:       val df2 = Seq(\"John\", \"Bob\").toDF(\"value\")",
          "998:       df2.createOrReplaceTempView(\"first_names\")",
          "1000:       val query = sql(",
          "1001:         \"\"\"SELECT name FROM contacts c",
          "1002:           |WHERE",
          "1003:           |  id IN (SELECT * FROM ids i WHERE c.pets > i.value)",
          "1004:           |  AND",
          "1005:           |  name.first IN (SELECT * FROM first_names n WHERE c.name.last < n.value)",
          "1006:           |\"\"\".stripMargin)",
          "1008:       checkScan(query,",
          "1009:         \"struct<id:int,name:struct<first:string,middle:string,last:string>,pets:int>\")",
          "1011:       checkAnswer(query, Row(Row(\"John\", \"Y.\", \"Doe\")) :: Nil)",
          "1012:     }",
          "1013:   }",
          "1015:   testSchemaPruning(\"SPARK-38977: schema pruning with correlated NOT IN subquery\") {",
          "1017:     import testImplicits._",
          "1019:     withTempView(\"ids\", \"first_names\") {",
          "1020:       val df1 = Seq(1, 2, 3).toDF(\"value\")",
          "1021:       df1.createOrReplaceTempView(\"ids\")",
          "1023:       val df2 = Seq(\"John\", \"Janet\", \"Jim\", \"Bob\").toDF(\"value\")",
          "1024:       df2.createOrReplaceTempView(\"first_names\")",
          "1026:       val query = sql(",
          "1027:         \"\"\"SELECT name FROM contacts c",
          "1028:           |WHERE",
          "1029:           |  id NOT IN (SELECT * FROM ids i WHERE c.pets > i.value)",
          "1030:           |  AND",
          "1031:           |  name.first NOT IN (SELECT * FROM first_names n WHERE c.name.last > n.value)",
          "1032:           |\"\"\".stripMargin)",
          "1034:       checkScan(query,",
          "1035:         \"struct<id:int,name:struct<first:string,middle:string,last:string>,pets:int>\")",
          "1037:       checkAnswer(query, Row(Row(\"Jane\", \"X.\", \"Doe\")) :: Nil)",
          "1038:     }",
          "1039:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bd79706a25ffe3e937964c56037313c1a7de752d",
      "candidate_info": {
        "commit_hash": "bd79706a25ffe3e937964c56037313c1a7de752d",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/bd79706a25ffe3e937964c56037313c1a7de752d",
        "files": [
          "dev/deps/spark-deps-hadoop-2-hive-2.3",
          "dev/deps/spark-deps-hadoop-3-hive-2.3",
          "pom.xml"
        ],
        "message": "[SPARK-40134][BUILD] Update ORC to 1.7.6\n\nThis PR aims to update ORC to 1.7.6.\n\nThis will bring the latest changes and bug fixes.\n\nhttps://github.com/apache/orc/releases/tag/v1.7.6\n\n- ORC-1204: ORC MapReduce writer to flush when long arrays\n- ORC-1205: `nextVector` should invoke `ensureSize` when reusing vectors\n- ORC-1215: Remove a wrong `NotNull` annotation on `value` of `setAttribute`\n- ORC-1222: Upgrade `tools.hadoop.version` to 2.10.2\n- ORC-1227: Use `Constructor.newInstance` instead of `Class.newInstance`\n- ORC-1228: Fix `setAttribute` to handle null value\n\nNo.\n\nPass the CIs.\n\nCloses #37563 from williamhyun/ORC-176.\n\nAuthored-by: William Hyun <william@apache.org>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>\n(cherry picked from commit a1a049f01986c15e50a2f76d1fa8538ca3b6307e)\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "dev/deps/spark-deps-hadoop-2-hive-2.3||dev/deps/spark-deps-hadoop-2-hive-2.3",
          "dev/deps/spark-deps-hadoop-3-hive-2.3||dev/deps/spark-deps-hadoop-3-hive-2.3"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/deps/spark-deps-hadoop-2-hive-2.3||dev/deps/spark-deps-hadoop-2-hive-2.3": [
          "File: dev/deps/spark-deps-hadoop-2-hive-2.3 -> dev/deps/spark-deps-hadoop-2-hive-2.3",
          "--- Hunk 1 ---",
          "[Context before]",
          "219: okhttp/3.12.12//okhttp-3.12.12.jar",
          "220: okio/1.14.0//okio-1.14.0.jar",
          "221: opencsv/2.3//opencsv-2.3.jar",
          "225: oro/2.0.8//oro-2.0.8.jar",
          "226: osgi-resource-locator/1.0.3//osgi-resource-locator-1.0.3.jar",
          "227: paranamer/2.8//paranamer-2.8.jar",
          "",
          "[Removed Lines]",
          "222: orc-core/1.7.5//orc-core-1.7.5.jar",
          "223: orc-mapreduce/1.7.5//orc-mapreduce-1.7.5.jar",
          "224: orc-shims/1.7.5//orc-shims-1.7.5.jar",
          "",
          "[Added Lines]",
          "222: orc-core/1.7.6//orc-core-1.7.6.jar",
          "223: orc-mapreduce/1.7.6//orc-mapreduce-1.7.6.jar",
          "224: orc-shims/1.7.6//orc-shims-1.7.6.jar",
          "",
          "---------------"
        ],
        "dev/deps/spark-deps-hadoop-3-hive-2.3||dev/deps/spark-deps-hadoop-3-hive-2.3": [
          "File: dev/deps/spark-deps-hadoop-3-hive-2.3 -> dev/deps/spark-deps-hadoop-3-hive-2.3",
          "--- Hunk 1 ---",
          "[Context before]",
          "208: opentracing-api/0.33.0//opentracing-api-0.33.0.jar",
          "209: opentracing-noop/0.33.0//opentracing-noop-0.33.0.jar",
          "210: opentracing-util/0.33.0//opentracing-util-0.33.0.jar",
          "214: oro/2.0.8//oro-2.0.8.jar",
          "215: osgi-resource-locator/1.0.3//osgi-resource-locator-1.0.3.jar",
          "216: paranamer/2.8//paranamer-2.8.jar",
          "",
          "[Removed Lines]",
          "211: orc-core/1.7.5//orc-core-1.7.5.jar",
          "212: orc-mapreduce/1.7.5//orc-mapreduce-1.7.5.jar",
          "213: orc-shims/1.7.5//orc-shims-1.7.5.jar",
          "",
          "[Added Lines]",
          "211: orc-core/1.7.6//orc-core-1.7.6.jar",
          "212: orc-mapreduce/1.7.6//orc-mapreduce-1.7.6.jar",
          "213: orc-shims/1.7.6//orc-shims-1.7.6.jar",
          "",
          "---------------"
        ]
      }
    }
  ]
}