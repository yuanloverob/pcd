{
  "cve_id": "CVE-2022-40604",
  "cve_desc": "In Apache Airflow 2.3.0 through 2.3.4, part of a url was unnecessarily formatted, allowing for possible information extraction.",
  "repo": "apache/airflow",
  "patch_hash": "6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
  "patch_info": {
    "commit_hash": "6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/6f24836e5ee56c452947aa87f84a21dd4f8eb87c",
    "files": [
      "airflow/utils/log/file_task_handler.py"
    ],
    "message": "Properly build URL to retrieve logs independently from system (#26337)\n\nThe previous way of building the path depended on the OS path\nbut it was really used to build the URL so we should use\nurllib instead of os.path.join.\n\n(cherry picked from commit 18386026c28939fa6d91d198c5489c295a05dcd2)",
    "before_after_code_files": [
      "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
    ]
  },
  "patch_diff": {
    "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
      "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "21: import warnings",
      "22: from pathlib import Path",
      "23: from typing import TYPE_CHECKING, Optional",
      "25: from airflow.configuration import AirflowConfigException, conf",
      "26: from airflow.exceptions import RemovedInAirflow3Warning",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "24: from urllib.parse import urljoin",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "194:         else:",
      "195:             import httpx",
      "199:             )",
      "200:             log += f\"*** Log file does not exist: {location}\\n\"",
      "201:             log += f\"*** Fetching from: {url}\\n\"",
      "",
      "[Removed Lines]",
      "197:             url = os.path.join(\"http://{ti.hostname}:{worker_log_server_port}/log\", log_relative_path).format(",
      "198:                 ti=ti, worker_log_server_port=conf.get('logging', 'WORKER_LOG_SERVER_PORT')",
      "",
      "[Added Lines]",
      "198:             url = urljoin(",
      "199:                 f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log\", log_relative_path",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "18386026c28939fa6d91d198c5489c295a05dcd2",
      "candidate_info": {
        "commit_hash": "18386026c28939fa6d91d198c5489c295a05dcd2",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/18386026c28939fa6d91d198c5489c295a05dcd2",
        "files": [
          "airflow/utils/log/file_task_handler.py"
        ],
        "message": "Properly build URL to retrieve logs independently from system (#26337)\n\nThe previous way of building the path depended on the OS path\nbut it was really used to build the URL so we should use\nurllib instead of os.path.join.",
        "before_after_code_files": [
          "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "diff_branch_cherry_pick": 1,
        "diff_branch_same_aad": 1,
        "olp_code_files": {
          "patch": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ],
          "candidate": [
            "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py"
          ]
        }
      },
      "candidate_diff": {
        "airflow/utils/log/file_task_handler.py||airflow/utils/log/file_task_handler.py": [
          "File: airflow/utils/log/file_task_handler.py -> airflow/utils/log/file_task_handler.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import warnings",
          "22: from pathlib import Path",
          "23: from typing import TYPE_CHECKING, Optional",
          "25: from airflow.configuration import AirflowConfigException, conf",
          "26: from airflow.exceptions import RemovedInAirflow3Warning",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24: from urllib.parse import urljoin",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "194:         else:",
          "195:             import httpx",
          "199:             )",
          "200:             log += f\"*** Log file does not exist: {location}\\n\"",
          "201:             log += f\"*** Fetching from: {url}\\n\"",
          "",
          "[Removed Lines]",
          "197:             url = os.path.join(\"http://{ti.hostname}:{worker_log_server_port}/log\", log_relative_path).format(",
          "198:                 ti=ti, worker_log_server_port=conf.get('logging', 'WORKER_LOG_SERVER_PORT')",
          "",
          "[Added Lines]",
          "198:             url = urljoin(",
          "199:                 f\"http://{ti.hostname}:{conf.get('logging', 'WORKER_LOG_SERVER_PORT')}/log\", log_relative_path",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f07436b73f747bf008c8112ab65610128d97e100",
      "candidate_info": {
        "commit_hash": "f07436b73f747bf008c8112ab65610128d97e100",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f07436b73f747bf008c8112ab65610128d97e100",
        "files": [
          "airflow/api_connexion/endpoints/xcom_endpoint.py",
          "airflow/api_connexion/openapi/v1.yaml",
          "airflow/www/static/js/types/api-generated.ts",
          "tests/api_connexion/endpoints/test_xcom_endpoint.py"
        ],
        "message": "Flag to deserialize value on custom XCom backend (#26343)\n\n(cherry picked from commit ffee6bceb32eba159a7a25a4613d573884a6a58d)",
        "before_after_code_files": [
          "airflow/api_connexion/endpoints/xcom_endpoint.py||airflow/api_connexion/endpoints/xcom_endpoint.py",
          "airflow/www/static/js/types/api-generated.ts||airflow/www/static/js/types/api-generated.ts",
          "tests/api_connexion/endpoints/test_xcom_endpoint.py||tests/api_connexion/endpoints/test_xcom_endpoint.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api_connexion/endpoints/xcom_endpoint.py||airflow/api_connexion/endpoints/xcom_endpoint.py": [
          "File: airflow/api_connexion/endpoints/xcom_endpoint.py -> airflow/api_connexion/endpoints/xcom_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "19: from flask import g",
          "20: from sqlalchemy import and_",
          "21: from sqlalchemy.orm import Session",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "19: import copy",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "68:     query = query.order_by(DR.execution_date, XCom.task_id, XCom.dag_id, XCom.key)",
          "69:     total_entries = query.count()",
          "70:     query = query.offset(offset).limit(limit)",
          "74: @security.requires_access(",
          "",
          "[Removed Lines]",
          "71:     return xcom_collection_schema.dump(XComCollection(xcom_entries=query.all(), total_entries=total_entries))",
          "",
          "[Added Lines]",
          "73:     return xcom_collection_schema.dump(XComCollection(xcom_entries=query, total_entries=total_entries))",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "86:     task_id: str,",
          "87:     dag_run_id: str,",
          "88:     xcom_key: str,",
          "89:     session: Session = NEW_SESSION,",
          "90: ) -> APIResponse:",
          "91:     \"\"\"Get an XCom entry\"\"\"",
          "93:     query = query.join(DR, and_(XCom.dag_id == DR.dag_id, XCom.run_id == DR.run_id))",
          "94:     query = query.filter(DR.run_id == dag_run_id)",
          "98:         raise NotFound(\"XCom entry not found\")",
          "",
          "[Removed Lines]",
          "92:     query = session.query(XCom).filter(XCom.dag_id == dag_id, XCom.task_id == task_id, XCom.key == xcom_key)",
          "96:     query_object = query.one_or_none()",
          "97:     if not query_object:",
          "99:     return xcom_schema.dump(query_object)",
          "",
          "[Added Lines]",
          "91:     deserialize: bool = False,",
          "95:     if deserialize:",
          "96:         query = session.query(XCom, XCom.value)",
          "97:     else:",
          "98:         query = session.query(XCom)",
          "100:     query = query.filter(XCom.dag_id == dag_id, XCom.task_id == task_id, XCom.key == xcom_key)",
          "104:     item = query.one_or_none()",
          "105:     if item is None:",
          "108:     if deserialize:",
          "109:         xcom, value = item",
          "110:         stub = copy.copy(xcom)",
          "111:         stub.value = value",
          "112:         stub.value = XCom.deserialize_value(stub)",
          "113:         item = stub",
          "115:     return xcom_schema.dump(item)",
          "",
          "---------------"
        ],
        "airflow/www/static/js/types/api-generated.ts||airflow/www/static/js/types/api-generated.ts": [
          "File: airflow/www/static/js/types/api-generated.ts -> airflow/www/static/js/types/api-generated.ts",
          "--- Hunk 1 ---",
          "[Context before]",
          "3414:         xcom_key: components[\"parameters\"][\"XComKey\"];",
          "3415:       };",
          "3416:     };",
          "3417:     responses: {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3416:       query: {",
          "3428:         deserialize?: boolean;",
          "3429:       };",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "4221: export type DeleteVariableVariables = CamelCasedPropertiesDeep<operations['delete_variable']['parameters']['path']>;",
          "4222: export type PatchVariableVariables = CamelCasedPropertiesDeep<operations['patch_variable']['parameters']['path'] & operations['patch_variable']['parameters']['query'] & operations['patch_variable']['requestBody']['content']['application/json']>;",
          "4223: export type GetXcomEntriesVariables = CamelCasedPropertiesDeep<operations['get_xcom_entries']['parameters']['path'] & operations['get_xcom_entries']['parameters']['query']>;",
          "4225: export type GetExtraLinksVariables = CamelCasedPropertiesDeep<operations['get_extra_links']['parameters']['path']>;",
          "4226: export type GetLogVariables = CamelCasedPropertiesDeep<operations['get_log']['parameters']['path'] & operations['get_log']['parameters']['query']>;",
          "4227: export type GetDagDetailsVariables = CamelCasedPropertiesDeep<operations['get_dag_details']['parameters']['path']>;",
          "",
          "[Removed Lines]",
          "4224: export type GetXcomEntryVariables = CamelCasedPropertiesDeep<operations['get_xcom_entry']['parameters']['path']>;",
          "",
          "[Added Lines]",
          "4238: export type GetXcomEntryVariables = CamelCasedPropertiesDeep<operations['get_xcom_entry']['parameters']['path'] & operations['get_xcom_entry']['parameters']['query']>;",
          "",
          "---------------"
        ],
        "tests/api_connexion/endpoints/test_xcom_endpoint.py||tests/api_connexion/endpoints/test_xcom_endpoint.py": [
          "File: tests/api_connexion/endpoints/test_xcom_endpoint.py -> tests/api_connexion/endpoints/test_xcom_endpoint.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: from __future__ import annotations",
          "19: from datetime import timedelta",
          "21: import pytest",
          "25: from airflow.operators.empty import EmptyOperator",
          "26: from airflow.security import permissions",
          "27: from airflow.utils.dates import parse_execution_date",
          "28: from airflow.utils.session import create_session",
          "29: from airflow.utils.types import DagRunType",
          "30: from tests.test_utils.api_connexion_utils import assert_401, create_user, delete_user",
          "31: from tests.test_utils.db import clear_db_dags, clear_db_runs, clear_db_xcom",
          "34: @pytest.fixture(scope=\"module\")",
          "35: def configured_app(minimal_app_for_api):",
          "36:     app = minimal_app_for_api",
          "",
          "[Removed Lines]",
          "22: from parameterized import parameterized",
          "24: from airflow.models import DagModel, DagRun, TaskInstance, XCom",
          "",
          "[Added Lines]",
          "20: from unittest import mock",
          "24: from airflow.models.dag import DagModel",
          "25: from airflow.models.dagrun import DagRun",
          "26: from airflow.models.taskinstance import TaskInstance",
          "27: from airflow.models.xcom import BaseXCom, XCom, resolve_xcom_backend",
          "32: from airflow.utils.timezone import utcnow",
          "35: from tests.test_utils.config import conf_vars",
          "39: class CustomXCom(BaseXCom):",
          "40:     @classmethod",
          "41:     def deserialize_value(cls, xcom: XCom):",
          "42:         return f\"real deserialized {super().deserialize_value(xcom)}\"",
          "44:     def orm_deserialize_value(self):",
          "45:         return f\"orm deserialized {super().orm_deserialize_value()}\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "147:         )",
          "148:         assert response.status_code == 403",
          "151:         with create_session() as session:",
          "152:             dagrun = DagRun(",
          "153:                 dag_id=dag_id,",
          "",
          "[Removed Lines]",
          "150:     def _create_xcom_entry(self, dag_id, run_id, execution_date, task_id, xcom_key):",
          "",
          "[Added Lines]",
          "164:     def _create_xcom_entry(self, dag_id, run_id, execution_date, task_id, xcom_key, *, backend=XCom):",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "160:             ti = TaskInstance(EmptyOperator(task_id=task_id), run_id=run_id)",
          "161:             ti.dag_id = dag_id",
          "162:             session.add(ti)",
          "164:             key=xcom_key,",
          "165:             value=\"TEST_VALUE\",",
          "166:             run_id=run_id,",
          "",
          "[Removed Lines]",
          "163:         XCom.set(",
          "",
          "[Added Lines]",
          "177:         backend.set(",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "168:             dag_id=dag_id,",
          "169:         )",
          "172: class TestGetXComEntries(TestXComEndpoint):",
          "173:     def test_should_respond_200(self):",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "185:     @pytest.mark.parametrize(",
          "186:         \"query, expected_value\",",
          "187:         [",
          "188:             pytest.param(\"?deserialize=true\", \"real deserialized TEST_VALUE\", id=\"true\"),",
          "189:             pytest.param(\"?deserialize=false\", \"orm deserialized TEST_VALUE\", id=\"false\"),",
          "190:             pytest.param(\"\", \"orm deserialized TEST_VALUE\", id=\"default\"),",
          "191:         ],",
          "192:     )",
          "193:     @conf_vars({(\"core\", \"xcom_backend\"): \"tests.api_connexion.endpoints.test_xcom_endpoint.CustomXCom\"})",
          "194:     def test_custom_xcom_deserialize(self, query, expected_value):",
          "195:         XCom = resolve_xcom_backend()",
          "196:         self._create_xcom_entry(\"dag\", \"run\", utcnow(), \"task\", \"key\", backend=XCom)",
          "198:         url = f\"/api/v1/dags/dag/dagRuns/run/taskInstances/task/xcomEntries/key{query}\"",
          "199:         with mock.patch(\"airflow.api_connexion.endpoints.xcom_endpoint.XCom\", XCom):",
          "200:             response = self.client.get(url, environ_overrides={'REMOTE_USER': \"test\"})",
          "202:         assert response.status_code == 200",
          "203:         assert response.json[\"value\"] == expected_value",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "388:         self.execution_date_parsed = parse_execution_date(self.execution_date)",
          "389:         self.run_id = DagRun.generate_run_id(DagRunType.MANUAL, self.execution_date_parsed)",
          "392:         [",
          "393:             (",
          "394:                 \"limit=1\",",
          "",
          "[Removed Lines]",
          "391:     @parameterized.expand(",
          "",
          "[Added Lines]",
          "425:     @pytest.mark.parametrize(",
          "426:         \"query_params, expected_xcom_ids\",",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "435:                 \"limit=2&offset=2\",",
          "436:                 [\"TEST_XCOM_KEY2\", \"TEST_XCOM_KEY3\"],",
          "437:             ),",
          "439:     )",
          "440:     def test_handle_limit_offset(self, query_params, expected_xcom_ids):",
          "444:         )",
          "445:         with create_session() as session:",
          "446:             dagrun = DagRun(",
          "",
          "[Removed Lines]",
          "438:         ]",
          "441:         url = \"/api/v1/dags/{dag_id}/dagRuns/{dag_run_id}/taskInstances/{task_id}/xcomEntries?{query_params}\"",
          "442:         url = url.format(",
          "443:             dag_id=self.dag_id, dag_run_id=self.run_id, task_id=self.task_id, query_params=query_params",
          "",
          "[Added Lines]",
          "473:         ],",
          "476:         url = (",
          "477:             f\"/api/v1/dags/{self.dag_id}/dagRuns/{self.run_id}/taskInstances/{self.task_id}/xcomEntries\"",
          "478:             f\"?{query_params}\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c642e1a8373173355013700aa614c87202b2e47f",
      "candidate_info": {
        "commit_hash": "c642e1a8373173355013700aa614c87202b2e47f",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c642e1a8373173355013700aa614c87202b2e47f",
        "files": [
          "airflow/models/dag.py",
          "tests/models/test_dag.py"
        ],
        "message": "Fix `dags_needing_dagruns` dataset info timestamp (#26288)\n\n(cherry picked from commit 3c9c0f940b67c25285259541478ebb413b94a73a)",
        "before_after_code_files": [
          "airflow/models/dag.py||airflow/models/dag.py",
          "tests/models/test_dag.py||tests/models/test_dag.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "3238:         you should ensure that any scheduling decisions are made in a single transaction -- as soon as the",
          "3239:         transaction is committed it will be unlocked.",
          "3240:         \"\"\"",
          "3243:         # these dag ids are triggered by datasets, and they are ready to go.",
          "3244:         dataset_triggered_dag_info_list = {",
          "3246:             for x in session.query(",
          "3247:                 DagScheduleDatasetReference.dag_id,",
          "3250:             )",
          "3251:             .join(DagScheduleDatasetReference.queue_records, isouter=True)",
          "3252:             .group_by(DagScheduleDatasetReference.dag_id)",
          "",
          "[Removed Lines]",
          "3241:         from airflow.models.dataset import DagScheduleDatasetReference, DatasetDagRunQueue as DDRQ",
          "3245:             x.dag_id: (x.first_event_time, x.last_event_time)",
          "3248:                 func.max(DDRQ.created_at).label('last_event_time'),",
          "3249:                 func.max(DDRQ.created_at).label('first_event_time'),",
          "",
          "[Added Lines]",
          "3243:             x.dag_id: (x.first_queued_time, x.last_queued_time)",
          "3246:                 func.max(DDRQ.created_at).label('last_queued_time'),",
          "3247:                 func.min(DDRQ.created_at).label('first_queued_time'),",
          "",
          "---------------"
        ],
        "tests/models/test_dag.py||tests/models/test_dag.py": [
          "File: tests/models/test_dag.py -> tests/models/test_dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "45: from airflow.models import DAG, DagModel, DagRun, DagTag, TaskFail, TaskInstance as TI",
          "46: from airflow.models.baseoperator import BaseOperator",
          "47: from airflow.models.dag import DagOwnerAttributes, dag as dag_decorator, get_dataset_triggered_next_run_info",
          "49: from airflow.models.param import DagParam, Param, ParamsDict",
          "50: from airflow.operators.bash import BashOperator",
          "51: from airflow.operators.empty import EmptyOperator",
          "",
          "[Removed Lines]",
          "48: from airflow.models.dataset import DatasetDagRunQueue, DatasetModel, TaskOutletDatasetReference",
          "",
          "[Added Lines]",
          "48: from airflow.models.dataset import DatasetDagRunQueue, DatasetEvent, DatasetModel, TaskOutletDatasetReference",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2221:         assert dag.relative_fileloc == expected_relative",
          "2224: class TestQueries:",
          "2225:     def setup_method(self) -> None:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "2223:     @pytest.mark.need_serialized_dag",
          "2224:     def test_dags_needing_dagruns_dataset_triggered_dag_info_queued_times(self, session, dag_maker):",
          "2225:         dataset1 = Dataset(uri=\"ds1\")",
          "2226:         dataset2 = Dataset(uri=\"ds2\")",
          "2228:         for dag_id, dataset in [(\"datasets-1\", dataset1), (\"datasets-2\", dataset2)]:",
          "2229:             with dag_maker(dag_id=dag_id, start_date=timezone.utcnow(), session=session):",
          "2230:                 EmptyOperator(task_id=\"task\", outlets=[dataset])",
          "2231:             dr = dag_maker.create_dagrun()",
          "2233:             ds_id = session.query(DatasetModel.id).filter_by(uri=dataset.uri).scalar()",
          "2235:             session.add(",
          "2236:                 DatasetEvent(",
          "2237:                     dataset_id=ds_id,",
          "2238:                     source_task_id=\"task\",",
          "2239:                     source_dag_id=dr.dag_id,",
          "2240:                     source_run_id=dr.run_id,",
          "2241:                     source_map_index=-1,",
          "2242:                 )",
          "2243:             )",
          "2245:         ds1_id = session.query(DatasetModel.id).filter_by(uri=dataset1.uri).scalar()",
          "2246:         ds2_id = session.query(DatasetModel.id).filter_by(uri=dataset2.uri).scalar()",
          "2248:         with dag_maker(dag_id=\"datasets-consumer-multiple\", schedule=[dataset1, dataset2]) as dag:",
          "2249:             pass",
          "2251:         session.flush()",
          "2252:         session.add_all(",
          "2253:             [",
          "2254:                 DatasetDagRunQueue(dataset_id=ds1_id, target_dag_id=dag.dag_id, created_at=DEFAULT_DATE),",
          "2255:                 DatasetDagRunQueue(",
          "2256:                     dataset_id=ds2_id, target_dag_id=dag.dag_id, created_at=DEFAULT_DATE + timedelta(hours=1)",
          "2257:                 ),",
          "2258:             ]",
          "2259:         )",
          "2260:         session.flush()",
          "2262:         query, dataset_triggered_dag_info = DagModel.dags_needing_dagruns(session)",
          "2263:         assert 1 == len(dataset_triggered_dag_info)",
          "2264:         assert dag.dag_id in dataset_triggered_dag_info",
          "2265:         first_queued_time, last_queued_time = dataset_triggered_dag_info[dag.dag_id]",
          "2266:         assert first_queued_time == DEFAULT_DATE",
          "2267:         assert last_queued_time == DEFAULT_DATE + timedelta(hours=1)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ea22657e3f243491c137a12f74fe2c5b8be0e29a",
      "candidate_info": {
        "commit_hash": "ea22657e3f243491c137a12f74fe2c5b8be0e29a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ea22657e3f243491c137a12f74fe2c5b8be0e29a",
        "files": [
          "airflow/datasets/manager.py",
          "tests/datasets/test_manager.py"
        ],
        "message": "Don't blow up when a task produces a dataset that is not consumed. (#26257)\n\nIf you have a dataset outlet on a task, and no DAG was recorded as\nconsuming that dataset it failed with a null value constraint violation\nin the db\n\n(cherry picked from commit fe82e9609522fa302d3eaf7193689718571d4af4)",
        "before_after_code_files": [
          "airflow/datasets/manager.py||airflow/datasets/manager.py",
          "tests/datasets/test_manager.py||tests/datasets/test_manager.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/datasets/manager.py||airflow/datasets/manager.py": [
          "File: airflow/datasets/manager.py -> airflow/datasets/manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "61:                 extra=extra,",
          "62:             )",
          "63:         )",
          "66:     def _queue_dagruns(self, dataset: DatasetModel, session: Session) -> None:",
          "67:         # Possible race condition: if multiple dags or multiple (usually",
          "",
          "[Removed Lines]",
          "64:         self._queue_dagruns(dataset_model, session)",
          "",
          "[Added Lines]",
          "64:         if dataset_model.consuming_dags:",
          "65:             self._queue_dagruns(dataset_model, session)",
          "66:         session.flush()",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "91:             except exc.IntegrityError:",
          "92:                 self.log.debug(\"Skipping record %s\", item, exc_info=True)",
          "96:     def _postgres_queue_dagruns(self, dataset: DatasetModel, session: Session) -> None:",
          "97:         from sqlalchemy.dialects.postgresql import insert",
          "",
          "[Removed Lines]",
          "94:         session.flush()",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "101:             stmt,",
          "102:             [{'target_dag_id': target_dag.dag_id} for target_dag in dataset.consuming_dags],",
          "103:         )",
          "107: def resolve_dataset_manager() -> \"DatasetManager\":",
          "",
          "[Removed Lines]",
          "104:         session.flush()",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/datasets/test_manager.py||tests/datasets/test_manager.py": [
          "File: tests/datasets/test_manager.py -> tests/datasets/test_manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "80:         # Ensure we've created a dataset",
          "81:         assert session.query(DatasetEvent).filter_by(dataset_id=dsm.id).count() == 1",
          "82:         assert session.query(DatasetDagRunQueue).count() == 2",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "84:     def test_register_dataset_change_no_downstreams(self, session, mock_task_instance):",
          "85:         dsem = DatasetManager()",
          "87:         ds = Dataset(uri=\"never_consumed\")",
          "88:         dsm = DatasetModel(uri=\"never_consumed\")",
          "89:         session.add(dsm)",
          "90:         session.flush()",
          "92:         dsem.register_dataset_change(task_instance=mock_task_instance, dataset=ds, session=session)",
          "94:         # Ensure we've created a dataset",
          "95:         assert session.query(DatasetEvent).filter_by(dataset_id=dsm.id).count() == 1",
          "96:         assert session.query(DatasetDagRunQueue).count() == 0",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "05a6fcfd3e6fc7ec4274e06ef3a8cea1ee4bdecd",
      "candidate_info": {
        "commit_hash": "05a6fcfd3e6fc7ec4274e06ef3a8cea1ee4bdecd",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/05a6fcfd3e6fc7ec4274e06ef3a8cea1ee4bdecd",
        "files": [
          "airflow/datasets/manager.py",
          "airflow/jobs/scheduler_job.py",
          "tests/models/test_taskinstance.py"
        ],
        "message": "Flush dataset events before queuing dagruns (#26276)\n\nWhen we go to schedule dagruns from the dataset dagrun queue, we assume\nthe events will happen before the queue records, which isn't the case\nunless we explicitly flush them first. This ensures that dagruns are\nproperly related to their upstream dataset events.\n\n(cherry picked from commit 954349a952d929dc82087e4bb20d19736f84d381)",
        "before_after_code_files": [
          "airflow/datasets/manager.py||airflow/datasets/manager.py",
          "airflow/jobs/scheduler_job.py||airflow/jobs/scheduler_job.py",
          "tests/models/test_taskinstance.py||tests/models/test_taskinstance.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/26371"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/datasets/manager.py||airflow/datasets/manager.py": [
          "File: airflow/datasets/manager.py -> airflow/datasets/manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "61:                 extra=extra,",
          "62:             )",
          "63:         )",
          "64:         if dataset_model.consuming_dags:",
          "65:             self._queue_dagruns(dataset_model, session)",
          "66:         session.flush()",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "64:         session.flush()",
          "",
          "---------------"
        ],
        "airflow/jobs/scheduler_job.py||airflow/jobs/scheduler_job.py": [
          "File: airflow/jobs/scheduler_job.py -> airflow/jobs/scheduler_job.py"
        ],
        "tests/models/test_taskinstance.py||tests/models/test_taskinstance.py": [
          "File: tests/models/test_taskinstance.py -> tests/models/test_taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1732:             DatasetEvent.source_task_instance == ti",
          "1733:         ).one() == ('s3://dag1/output_1.txt',)",
          "1735:     def test_outlet_datasets_failed(self, create_task_instance):",
          "1736:         \"\"\"",
          "1737:         Verify that when we have an outlet dataset on a task, and the task",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1735:         # check that the dataset event has an earlier timestamp than the DDRQ's",
          "1736:         ddrq_timestamps = (",
          "1737:             session.query(DatasetDagRunQueue.created_at).filter_by(dataset_id=event.dataset.id).all()",
          "1738:         )",
          "1739:         assert all([event.timestamp < ddrq_timestamp for (ddrq_timestamp,) in ddrq_timestamps])",
          "",
          "---------------"
        ]
      }
    }
  ]
}