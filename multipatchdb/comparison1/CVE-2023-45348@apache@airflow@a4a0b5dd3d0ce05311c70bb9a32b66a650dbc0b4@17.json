{
  "cve_id": "CVE-2023-45348",
  "cve_desc": "Apache Airflow, versions 2.7.0 and 2.7.1, is affected by a vulnerability that allows an authenticated user to retrieve sensitive configuration information when the \"expose_config\" option is set to \"non-sensitive-only\". The `expose_config` option is False by default.\nIt is recommended to upgrade to a version that is not affected.",
  "repo": "apache/airflow",
  "patch_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
  "patch_info": {
    "commit_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
    "files": [
      "airflow/api_connexion/endpoints/config_endpoint.py",
      "airflow/configuration.py",
      "tests/api_connexion/endpoints/test_config_endpoint.py"
    ],
    "message": "Check if the lower of provided values are sensitives in config endpoint (#34712)\n\n* Check if the lower of provided values are sensitives in config endpoint\n\n* update unit test\n\n* ensure that all values in sensitive dict are in lower characters\n\n(cherry picked from commit f044589b685855a8fce8f5376bea2564c5a001f7)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py",
      "airflow/configuration.py||airflow/configuration.py",
      "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py": [
      "File: airflow/api_connexion/endpoints/config_endpoint.py -> airflow/api_connexion/endpoints/config_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "123:                 \"Config not found.\", detail=f\"The option [{section}/{option}] is not found in config.\"",
      "124:             )",
      "127:             value = \"< hidden >\"",
      "128:         else:",
      "129:             value = conf.get(section, option)",
      "",
      "[Removed Lines]",
      "126:         if (section, option) in conf.sensitive_config_values:",
      "",
      "[Added Lines]",
      "126:         if (section.lower(), option.lower()) in conf.sensitive_config_values:",
      "",
      "---------------"
    ],
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "311:             for s, s_c in self.configuration_description.items()",
      "312:             for k, item in s_c.get(\"options\").items()  # type: ignore[union-attr]",
      "313:         }",
      "315:         depr_option = {self.deprecated_options[x][:-1] for x in sensitive if x in self.deprecated_options}",
      "316:         depr_section = {",
      "317:             (self.deprecated_sections[s][0], k) for s, k in sensitive if s in self.deprecated_sections",
      "",
      "[Removed Lines]",
      "314:         sensitive = {(section, key) for (section, key), v in flattened.items() if v.get(\"sensitive\") is True}",
      "",
      "[Added Lines]",
      "314:         sensitive = {",
      "315:             (section.lower(), key.lower())",
      "316:             for (section, key), v in flattened.items()",
      "317:             if v.get(\"sensitive\") is True",
      "318:         }",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_config_endpoint.py -> tests/api_connexion/endpoints/test_config_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "247:         return_value=MOCK_CONF_WITH_SENSITIVE_VALUE,",
      "248:     )",
      "249:     @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "251:         response = self.client.get(",
      "253:             headers={\"Accept\": \"text/plain\"},",
      "254:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "255:         )",
      "256:         assert response.status_code == 200",
      "257:         expected = textwrap.dedent(",
      "261:         \"\"\"",
      "262:         )",
      "263:         assert expected == response.data.decode()",
      "",
      "[Removed Lines]",
      "250:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict):",
      "252:             \"/api/v1/config/section/core/option/sql_alchemy_conn\",",
      "258:             \"\"\"\\",
      "259:         [core]",
      "260:         sql_alchemy_conn = < hidden >",
      "",
      "[Added Lines]",
      "250:     @pytest.mark.parametrize(",
      "251:         \"section, option\",",
      "252:         [",
      "253:             (\"core\", \"sql_alchemy_conn\"),",
      "254:             (\"core\", \"SQL_ALCHEMY_CONN\"),",
      "255:             (\"corE\", \"sql_alchemy_conn\"),",
      "256:             (\"CORE\", \"sql_alchemy_conn\"),",
      "257:         ],",
      "258:     )",
      "259:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict, section, option):",
      "261:             f\"/api/v1/config/section/{section}/option/{option}\",",
      "267:             f\"\"\"\\",
      "268:         [{section}]",
      "269:         {option} = < hidden >",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "c9f84c6644a04240769b16217fbd14c44735e9a8",
      "candidate_info": {
        "commit_hash": "c9f84c6644a04240769b16217fbd14c44735e9a8",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c9f84c6644a04240769b16217fbd14c44735e9a8",
        "files": [
          "airflow/providers/amazon/aws/hooks/batch_client.py",
          "airflow/utils/strings.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/provider_packages/prepare_provider_packages.py",
          "tests/models/test_cleartasks.py",
          "tests/providers/amazon/aws/hooks/test_batch_client.py",
          "tests/providers/oracle/operators/test_oracle.py",
          "tests/providers/ssh/operators/test_ssh.py",
          "tests/system/providers/google/cloud/dataplex/resources/spark_example_pi.py"
        ],
        "message": "Refactor: Consolidate import and usage of random (#34108)\n\n(cherry picked from commit 4fa66d17003f10d03a13eda659bca8670bdf5052)",
        "before_after_code_files": [
          "airflow/providers/amazon/aws/hooks/batch_client.py||airflow/providers/amazon/aws/hooks/batch_client.py",
          "airflow/utils/strings.py||airflow/utils/strings.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/provider_packages/prepare_provider_packages.py||dev/provider_packages/prepare_provider_packages.py",
          "tests/models/test_cleartasks.py||tests/models/test_cleartasks.py",
          "tests/providers/amazon/aws/hooks/test_batch_client.py||tests/providers/amazon/aws/hooks/test_batch_client.py",
          "tests/providers/oracle/operators/test_oracle.py||tests/providers/oracle/operators/test_oracle.py",
          "tests/providers/ssh/operators/test_ssh.py||tests/providers/ssh/operators/test_ssh.py",
          "tests/system/providers/google/cloud/dataplex/resources/spark_example_pi.py||tests/system/providers/google/cloud/dataplex/resources/spark_example_pi.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/amazon/aws/hooks/batch_client.py||airflow/providers/amazon/aws/hooks/batch_client.py": [
          "File: airflow/providers/amazon/aws/hooks/batch_client.py -> airflow/providers/amazon/aws/hooks/batch_client.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: from __future__ import annotations",
          "29: import itertools",
          "31: from time import sleep",
          "34: import botocore.client",
          "35: import botocore.exceptions",
          "",
          "[Removed Lines]",
          "30: from random import uniform",
          "32: from typing import Callable",
          "",
          "[Added Lines]",
          "30: import random",
          "32: from typing import TYPE_CHECKING, Callable",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "38: from airflow.exceptions import AirflowException",
          "39: from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook",
          "41: from airflow.typing_compat import Protocol, runtime_checkable",
          "44: @runtime_checkable",
          "45: class BatchProtocol(Protocol):",
          "",
          "[Removed Lines]",
          "40: from airflow.providers.amazon.aws.utils.task_log_fetcher import AwsTaskLogFetcher",
          "",
          "[Added Lines]",
          "42: if TYPE_CHECKING:",
          "43:     from airflow.providers.amazon.aws.utils.task_log_fetcher import AwsTaskLogFetcher",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "527:         minima = abs(minima)",
          "528:         lower = max(minima, delay - width)",
          "529:         upper = delay + width",
          "532:     @staticmethod",
          "533:     def delay(delay: int | float | None = None) -> None:",
          "",
          "[Removed Lines]",
          "530:         return uniform(lower, upper)",
          "",
          "[Added Lines]",
          "532:         return random.uniform(lower, upper)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "544:             when many concurrent tasks request job-descriptions.",
          "545:         \"\"\"",
          "546:         if delay is None:",
          "548:         else:",
          "549:             delay = BatchClientHook.add_jitter(delay)",
          "550:         sleep(delay)",
          "",
          "[Removed Lines]",
          "547:             delay = uniform(BatchClientHook.DEFAULT_DELAY_MIN, BatchClientHook.DEFAULT_DELAY_MAX)",
          "",
          "[Added Lines]",
          "549:             delay = random.uniform(BatchClientHook.DEFAULT_DELAY_MIN, BatchClientHook.DEFAULT_DELAY_MAX)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "593:         max_interval = 600.0  # results in 3 to 10 minute delay",
          "594:         delay = 1 + pow(tries * 0.6, 2)",
          "595:         delay = min(max_interval, delay)",
          "",
          "[Removed Lines]",
          "596:         return uniform(delay / 3, delay)",
          "",
          "[Added Lines]",
          "598:         return random.uniform(delay / 3, delay)",
          "",
          "---------------"
        ],
        "airflow/utils/strings.py||airflow/utils/strings.py": [
          "File: airflow/utils/strings.py -> airflow/utils/strings.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: \"\"\"Common utility functions with strings.\"\"\"",
          "18: from __future__ import annotations",
          "20: import string",
          "24: def get_random_string(length=8, choices=string.ascii_letters + string.digits):",
          "25:     \"\"\"Generate random string.\"\"\"",
          "29: TRUE_LIKE_VALUES = {\"on\", \"t\", \"true\", \"y\", \"yes\", \"1\"}",
          "",
          "[Removed Lines]",
          "21: from random import choice",
          "26:     return \"\".join(choice(choices) for _ in range(length))",
          "",
          "[Added Lines]",
          "20: import random",
          "26:     return \"\".join(random.choices(choices, k=length))",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: \"\"\"Various utils to prepare docker and docker compose commands.\"\"\"",
          "18: from __future__ import annotations",
          "20: import os",
          "21: import re",
          "22: import sys",
          "25: from subprocess import DEVNULL, CalledProcessError, CompletedProcess",
          "27: from airflow_breeze.params.build_ci_params import BuildCiParams",
          "",
          "[Removed Lines]",
          "23: from copy import deepcopy",
          "24: from random import randint",
          "",
          "[Added Lines]",
          "20: import copy",
          "22: import random",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "373:     value = \"true\" if raw_value is True else value",
          "374:     value = \"false\" if raw_value is False else value",
          "375:     if arg_name == \"upgrade_to_newer_dependencies\" and value == \"true\":",
          "377:     return value",
          "",
          "[Removed Lines]",
          "376:         value = f\"{randint(0, 2**32):x}\"",
          "",
          "[Added Lines]",
          "376:         value = f\"{random.randrange(2**32):x}\"",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "510: def build_cache(image_params: CommonBuildParams, output: Output | None) -> RunCommandResult:",
          "511:     build_command_result: CompletedProcess | CalledProcessError = CompletedProcess(args=[], returncode=0)",
          "512:     for platform in image_params.platforms:",
          "514:         # override the platform in the copied params to only be single platform per run",
          "515:         # as a workaround to https://github.com/docker/buildx/issues/1044",
          "516:         platform_image_params.platform = platform",
          "",
          "[Removed Lines]",
          "513:         platform_image_params = deepcopy(image_params)",
          "",
          "[Added Lines]",
          "513:         platform_image_params = copy.deepcopy(image_params)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "722:         return",
          "723:     docker_syntax = get_docker_syntax_version()",
          "724:     get_console().print(f\"[info]Warming up the {docker_context} builder for syntax: {docker_syntax}\")",
          "726:     warm_up_image_param.image_tag = \"warmup\"",
          "727:     warm_up_image_param.push = False",
          "728:     build_command = prepare_base_build_command(image_params=warm_up_image_param)",
          "",
          "[Removed Lines]",
          "725:     warm_up_image_param = deepcopy(image_params)",
          "",
          "[Added Lines]",
          "725:     warm_up_image_param = copy.deepcopy(image_params)",
          "",
          "---------------"
        ],
        "dev/provider_packages/prepare_provider_packages.py||dev/provider_packages/prepare_provider_packages.py": [
          "File: dev/provider_packages/prepare_provider_packages.py -> dev/provider_packages/prepare_provider_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: import json",
          "27: import logging",
          "28: import os",
          "29: import re",
          "30: import shutil",
          "31: import subprocess",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "29: import random",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "38: from enum import Enum",
          "39: from functools import lru_cache",
          "40: from pathlib import Path",
          "42: from shutil import copyfile",
          "43: from typing import Any, Generator, Iterable, NamedTuple",
          "",
          "[Removed Lines]",
          "41: from random import choice",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1215:     given_answer = \"\"",
          "1216:     if answer and answer.lower() in [\"yes\", \"y\"]:",
          "1217:         # Simulate all possible non-terminal answers",
          "1219:             [",
          "1220:                 TypeOfChange.DOCUMENTATION,",
          "1221:                 TypeOfChange.BUGFIX,",
          "",
          "[Removed Lines]",
          "1218:         return choice(",
          "",
          "[Added Lines]",
          "1218:         return random.choice(",
          "",
          "---------------"
        ],
        "tests/models/test_cleartasks.py||tests/models/test_cleartasks.py": [
          "File: tests/models/test_cleartasks.py -> tests/models/test_cleartasks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import datetime",
          "22: import pytest",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import random",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "580:             assert tis[i].max_tries == 1",
          "582:         # test only_failed",
          "588:         session.commit()",
          "590:         DAG.clear_dags(dags, only_failed=True)",
          "598:             else:",
          "603:     def test_operator_clear(self, dag_maker):",
          "604:         with dag_maker(",
          "",
          "[Removed Lines]",
          "583:         from random import randint",
          "585:         failed_dag_idx = randint(0, len(tis) - 1)",
          "586:         tis[failed_dag_idx].state = State.FAILED",
          "587:         session.merge(tis[failed_dag_idx])",
          "592:         for i in range(num_of_dags):",
          "593:             tis[i].refresh_from_db()",
          "594:             if i != failed_dag_idx:",
          "595:                 assert tis[i].state == State.SUCCESS",
          "596:                 assert tis[i].try_number == 3",
          "597:                 assert tis[i].max_tries == 1",
          "599:                 assert tis[i].state == State.NONE",
          "600:                 assert tis[i].try_number == 3",
          "601:                 assert tis[i].max_tries == 2",
          "",
          "[Added Lines]",
          "584:         failed_dag = random.choice(tis)",
          "585:         failed_dag.state = State.FAILED",
          "586:         session.merge(failed_dag)",
          "591:         for ti in tis:",
          "592:             ti.refresh_from_db()",
          "593:             if ti is failed_dag:",
          "594:                 assert ti.state == State.NONE",
          "595:                 assert ti.try_number == 3",
          "596:                 assert ti.max_tries == 2",
          "598:                 assert ti.state == State.SUCCESS",
          "599:                 assert ti.try_number == 3",
          "600:                 assert ti.max_tries == 1",
          "",
          "---------------"
        ],
        "tests/providers/amazon/aws/hooks/test_batch_client.py||tests/providers/amazon/aws/hooks/test_batch_client.py": [
          "File: tests/providers/amazon/aws/hooks/test_batch_client.py -> tests/providers/amazon/aws/hooks/test_batch_client.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "426:         assert result >= minima",
          "427:         assert result <= width",
          "430:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.sleep\")",
          "431:     def test_delay_defaults(self, mock_sleep, mock_uniform):",
          "432:         assert BatchClientHook.DEFAULT_DELAY_MIN == 1",
          "",
          "[Removed Lines]",
          "429:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.uniform\")",
          "",
          "[Added Lines]",
          "429:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.random.uniform\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "438:         )",
          "439:         mock_sleep.assert_called_once_with(0)",
          "442:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.sleep\")",
          "443:     def test_delay_with_zero(self, mock_sleep, mock_uniform):",
          "444:         self.batch_client.delay(0)",
          "445:         mock_uniform.assert_called_once_with(0, 1)  # in add_jitter",
          "446:         mock_sleep.assert_called_once_with(mock_uniform.return_value)",
          "449:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.sleep\")",
          "450:     def test_delay_with_int(self, mock_sleep, mock_uniform):",
          "451:         self.batch_client.delay(5)",
          "452:         mock_uniform.assert_called_once_with(4, 6)  # in add_jitter",
          "453:         mock_sleep.assert_called_once_with(mock_uniform.return_value)",
          "456:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.sleep\")",
          "457:     def test_delay_with_float(self, mock_sleep, mock_uniform):",
          "458:         self.batch_client.delay(5.0)",
          "",
          "[Removed Lines]",
          "441:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.uniform\")",
          "448:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.uniform\")",
          "455:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.uniform\")",
          "",
          "[Added Lines]",
          "441:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.random.uniform\")",
          "448:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.random.uniform\")",
          "455:     @mock.patch(\"airflow.providers.amazon.aws.hooks.batch_client.random.uniform\")",
          "",
          "---------------"
        ],
        "tests/providers/oracle/operators/test_oracle.py||tests/providers/oracle/operators/test_oracle.py": [
          "File: tests/providers/oracle/operators/test_oracle.py -> tests/providers/oracle/operators/test_oracle.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "19: import re",
          "21: from unittest import mock",
          "23: import oracledb",
          "",
          "[Removed Lines]",
          "20: from random import randrange",
          "",
          "[Added Lines]",
          "19: import random",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "90:         oracle_conn_id = \"oracle_default\"",
          "91:         parameters = {\"parameter\": \"value\"}",
          "92:         task_id = \"test_push\"",
          "94:         error = f\"ORA-{ora_exit_code}: This is a five-digit ORA error code\"",
          "95:         mock_callproc.side_effect = oracledb.DatabaseError(error)",
          "",
          "[Removed Lines]",
          "93:         ora_exit_code = f\"{randrange(10**5):05}\"",
          "",
          "[Added Lines]",
          "93:         ora_exit_code = f\"{random.randrange(10**5):05}\"",
          "",
          "---------------"
        ],
        "tests/providers/ssh/operators/test_ssh.py||tests/providers/ssh/operators/test_ssh.py": [
          "File: tests/providers/ssh/operators/test_ssh.py -> tests/providers/ssh/operators/test_ssh.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "21: from unittest import mock",
          "23: import pytest",
          "",
          "[Removed Lines]",
          "20: from random import randrange",
          "",
          "[Added Lines]",
          "20: import random",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "217:     def test_push_ssh_exit_to_xcom(self, request, dag_maker):",
          "218:         # Test pulls the value previously pushed to xcom and checks if it's the same",
          "219:         command = \"not_a_real_command\"",
          "221:         self.exec_ssh_client_command.return_value = (ssh_exit_code, b\"\", b\"ssh output\")",
          "223:         with dag_maker(dag_id=f\"dag_{request.node.name}\"):",
          "",
          "[Removed Lines]",
          "220:         ssh_exit_code = randrange(1, 100)",
          "",
          "[Added Lines]",
          "220:         ssh_exit_code = random.randrange(1, 100)",
          "",
          "---------------"
        ],
        "tests/system/providers/google/cloud/dataplex/resources/spark_example_pi.py||tests/system/providers/google/cloud/dataplex/resources/spark_example_pi.py": [
          "File: tests/system/providers/google/cloud/dataplex/resources/spark_example_pi.py -> tests/system/providers/google/cloud/dataplex/resources/spark_example_pi.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import sys",
          "21: from operator import add",
          "24: from pyspark.sql import SparkSession",
          "",
          "[Removed Lines]",
          "22: from random import random",
          "",
          "[Added Lines]",
          "20: import random",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "33:     n = 100000 * partitions",
          "35:     def f(_: int) -> float:",
          "38:         return 1 if x**2 + y**2 <= 1 else 0",
          "40:     count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)",
          "",
          "[Removed Lines]",
          "36:         x = random() * 2 - 1",
          "37:         y = random() * 2 - 1",
          "",
          "[Added Lines]",
          "36:         x = random.random() * 2 - 1",
          "37:         y = random.random() * 2 - 1",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8f424fdd88c070f5124fd3dc9ccdd6c10f40f386",
      "candidate_info": {
        "commit_hash": "8f424fdd88c070f5124fd3dc9ccdd6c10f40f386",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8f424fdd88c070f5124fd3dc9ccdd6c10f40f386",
        "files": [
          "tests/cli/commands/test_task_command.py",
          "tests/dag_processing/test_job_runner.py",
          "tests/providers/amazon/aws/hooks/test_eks.py",
          "tests/providers/google/cloud/operators/test_datafusion.py",
          "tests/providers/google/cloud/operators/test_dataproc.py",
          "tests/serialization/test_dag_serialization.py",
          "tests/utils/test_helpers.py",
          "tests/utils/test_python_virtualenv.py"
        ],
        "message": "Replace sequence concatination by unpacking in Airflow tests (#33935)\n\n(cherry picked from commit 732eba98b2f485646646574052c1c3c57a94e07f)",
        "before_after_code_files": [
          "tests/cli/commands/test_task_command.py||tests/cli/commands/test_task_command.py",
          "tests/dag_processing/test_job_runner.py||tests/dag_processing/test_job_runner.py",
          "tests/providers/amazon/aws/hooks/test_eks.py||tests/providers/amazon/aws/hooks/test_eks.py",
          "tests/providers/google/cloud/operators/test_datafusion.py||tests/providers/google/cloud/operators/test_datafusion.py",
          "tests/providers/google/cloud/operators/test_dataproc.py||tests/providers/google/cloud/operators/test_dataproc.py",
          "tests/serialization/test_dag_serialization.py||tests/serialization/test_dag_serialization.py",
          "tests/utils/test_helpers.py||tests/utils/test_helpers.py",
          "tests/utils/test_python_virtualenv.py||tests/utils/test_python_virtualenv.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/cli/commands/test_task_command.py||tests/cli/commands/test_task_command.py": [
          "File: tests/cli/commands/test_task_command.py -> tests/cli/commands/test_task_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "818:             session.commit()",
          "820:             assert session.query(TaskInstance).filter_by(pool=pool_name).first() is None",
          "822:             assert session.query(TaskInstance).filter_by(pool=pool_name).first() is not None",
          "824:             session.delete(pool)",
          "",
          "[Removed Lines]",
          "821:             task_command.task_run(self.parser.parse_args(self.task_args + [\"--pool\", pool_name]))",
          "",
          "[Added Lines]",
          "821:             task_command.task_run(self.parser.parse_args([*self.task_args, \"--pool\", pool_name]))",
          "",
          "---------------"
        ],
        "tests/dag_processing/test_job_runner.py||tests/dag_processing/test_job_runner.py": [
          "File: tests/dag_processing/test_job_runner.py -> tests/dag_processing/test_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "499:             [\"file_3.py\", \"file_2.py\", \"file_1.py\"]",
          "500:         )",
          "503:         manager.processor.add_new_file_path_to_queue()",
          "504:         assert manager.processor._file_path_queue == collections.deque(",
          "505:             [\"file_4.py\", \"file_3.py\", \"file_2.py\", \"file_1.py\"]",
          "",
          "[Removed Lines]",
          "502:         manager.processor.set_file_paths(dag_files + [\"file_4.py\"])",
          "",
          "[Added Lines]",
          "502:         manager.processor.set_file_paths([*dag_files, \"file_4.py\"])",
          "",
          "---------------"
        ],
        "tests/providers/amazon/aws/hooks/test_eks.py||tests/providers/amazon/aws/hooks/test_eks.py": [
          "File: tests/providers/amazon/aws/hooks/test_eks.py -> tests/providers/amazon/aws/hooks/test_eks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1167:         test_inputs = dict(",
          "1168:             deepcopy(",
          "1173:                     (ClusterAttributes.CLUSTER_NAME, cluster_name),",
          "1174:                     (FargateProfileAttributes.FARGATE_PROFILE_NAME, fargate_profile_name),",
          "1175:                 ]",
          "1178:             )",
          "1179:         )",
          "",
          "[Removed Lines]",
          "1169:                 # Required Constants",
          "1170:                 [POD_EXECUTION_ROLE_ARN]",
          "1171:                 # Required Variables",
          "1172:                 + [",
          "1176:                 # Test Case Values",
          "1177:                 + [(FargateProfileAttributes.SELECTORS, selectors)]",
          "",
          "[Added Lines]",
          "1169:                 [",
          "1170:                     # Required Constants",
          "1171:                     POD_EXECUTION_ROLE_ARN,",
          "1172:                     # Required Variables",
          "1175:                     # Test Case Values",
          "1176:                     (FargateProfileAttributes.SELECTORS, selectors),",
          "",
          "---------------"
        ],
        "tests/providers/google/cloud/operators/test_datafusion.py||tests/providers/google/cloud/operators/test_datafusion.py": [
          "File: tests/providers/google/cloud/operators/test_datafusion.py -> tests/providers/google/cloud/operators/test_datafusion.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "238:         )",
          "240:         mock_hook.return_value.wait_for_pipeline_state.assert_called_once_with(",
          "242:             pipeline_id=PIPELINE_ID,",
          "243:             pipeline_name=PIPELINE_NAME,",
          "244:             namespace=NAMESPACE,",
          "",
          "[Removed Lines]",
          "241:             success_states=SUCCESS_STATES + [PipelineStates.RUNNING],",
          "",
          "[Added Lines]",
          "241:             success_states=[*SUCCESS_STATES, PipelineStates.RUNNING],",
          "",
          "---------------"
        ],
        "tests/providers/google/cloud/operators/test_dataproc.py||tests/providers/google/cloud/operators/test_dataproc.py": [
          "File: tests/providers/google/cloud/operators/test_dataproc.py -> tests/providers/google/cloud/operators/test_dataproc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "461:             \"labels\": LABELS,",
          "462:             \"virtual_cluster_config\": None,",
          "463:         }",
          "465:             call.hook().create_cluster(**create_cluster_args),",
          "466:         ]",
          "",
          "[Removed Lines]",
          "464:         expected_calls = self.extra_links_expected_calls_base + [",
          "",
          "[Added Lines]",
          "464:         expected_calls = [",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "510:             \"labels\": LABELS,",
          "511:             \"virtual_cluster_config\": VIRTUAL_CLUSTER_CONFIG,",
          "512:         }",
          "514:             call.hook().create_cluster(**create_cluster_args),",
          "515:         ]",
          "",
          "[Removed Lines]",
          "513:         expected_calls = self.extra_links_expected_calls_base + [",
          "",
          "[Added Lines]",
          "514:         expected_calls = [",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "784:             \"graceful_decommission_timeout\": {\"seconds\": 600},",
          "785:             \"update_mask\": UPDATE_MASK,",
          "786:         }",
          "789:         ]",
          "791:         op = DataprocScaleClusterOperator(",
          "",
          "[Removed Lines]",
          "787:         expected_calls = self.extra_links_expected_calls_base + [",
          "788:             call.hook().update_cluster(**update_cluster_args)",
          "",
          "[Added Lines]",
          "789:         expected_calls = [",
          "791:             call.hook().update_cluster(**update_cluster_args),",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1224:             \"timeout\": TIMEOUT,",
          "1225:             \"metadata\": METADATA,",
          "1226:         }",
          "1229:         ]",
          "1231:         op = DataprocUpdateClusterOperator(",
          "",
          "[Removed Lines]",
          "1227:         expected_calls = self.extra_links_expected_calls_base + [",
          "1228:             call.hook().update_cluster(**update_cluster_args)",
          "",
          "[Added Lines]",
          "1230:         expected_calls = [",
          "1232:             call.hook().update_cluster(**update_cluster_args),",
          "",
          "---------------"
        ],
        "tests/serialization/test_dag_serialization.py||tests/serialization/test_dag_serialization.py": [
          "File: tests/serialization/test_dag_serialization.py -> tests/serialization/test_dag_serialization.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1481:             pass",
          "1483:         class DummyTask(BaseOperator):",
          "1486:         execution_date = datetime(2020, 1, 1)",
          "1487:         with DAG(dag_id=\"test_error_on_unregistered_ti_dep_serialization\", start_date=execution_date) as dag:",
          "",
          "[Removed Lines]",
          "1484:             deps = frozenset(list(BaseOperator.deps) + [DummyTriggerRule()])",
          "",
          "[Added Lines]",
          "1484:             deps = frozenset([*BaseOperator.deps, DummyTriggerRule()])",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1508:         from test_plugin import CustomTestTriggerRule",
          "1510:         class DummyTask(BaseOperator):",
          "1513:         execution_date = datetime(2020, 1, 1)",
          "1514:         with DAG(dag_id=\"test_serialize_custom_ti_deps\", start_date=execution_date) as dag:",
          "",
          "[Removed Lines]",
          "1511:             deps = frozenset(list(BaseOperator.deps) + [CustomTestTriggerRule()])",
          "",
          "[Added Lines]",
          "1511:             deps = frozenset([*BaseOperator.deps, CustomTestTriggerRule()])",
          "",
          "---------------"
        ],
        "tests/utils/test_helpers.py||tests/utils/test_helpers.py": [
          "File: tests/utils/test_helpers.py -> tests/utils/test_helpers.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "80:         assert list(helpers.chunks([1, 2, 3], 2)) == [[1, 2], [3]]",
          "82:     def test_reduce_in_chunks(self):",
          "87:         assert helpers.reduce_in_chunks(lambda x, y: x + y[0] * y[1], [1, 2, 3, 4], 0, 2) == 14",
          "",
          "[Removed Lines]",
          "83:         assert helpers.reduce_in_chunks(lambda x, y: x + [y], [1, 2, 3, 4, 5], []) == [[1, 2, 3, 4, 5]]",
          "85:         assert helpers.reduce_in_chunks(lambda x, y: x + [y], [1, 2, 3, 4, 5], [], 2) == [[1, 2], [3, 4], [5]]",
          "",
          "[Added Lines]",
          "83:         assert helpers.reduce_in_chunks(lambda x, y: [*x, y], [1, 2, 3, 4, 5], []) == [[1, 2, 3, 4, 5]]",
          "85:         assert helpers.reduce_in_chunks(lambda x, y: [*x, y], [1, 2, 3, 4, 5], [], 2) == [[1, 2], [3, 4], [5]]",
          "",
          "---------------"
        ],
        "tests/utils/test_python_virtualenv.py||tests/utils/test_python_virtualenv.py": [
          "File: tests/utils/test_python_virtualenv.py -> tests/utils/test_python_virtualenv.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "61:             [sys.executable, \"-m\", \"virtualenv\", \"/VENV\", \"--system-site-packages\", \"--python=pythonVER\"]",
          "62:         )",
          "63:         mock_execute_in_subprocess.assert_called_with(",
          "65:         )",
          "67:     @mock.patch(\"airflow.utils.python_virtualenv.execute_in_subprocess\")",
          "",
          "[Removed Lines]",
          "64:             [\"/VENV/bin/pip\", \"install\"] + pip_install_options + [\"apache-beam[gcp]\"]",
          "",
          "[Added Lines]",
          "64:             [\"/VENV/bin/pip\", \"install\", *pip_install_options, \"apache-beam[gcp]\"]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7cbdcf2b959067ad2e28f9ef360ba9e6cd937f06",
      "candidate_info": {
        "commit_hash": "7cbdcf2b959067ad2e28f9ef360ba9e6cd937f06",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7cbdcf2b959067ad2e28f9ef360ba9e6cd937f06",
        "files": [
          "airflow/decorators/base.py"
        ],
        "message": "Avoid top-level airflow import to avoid circular dependency (#34586)\n\n(cherry picked from commit d1b7bca6a36c146119fb5746019116c4d1e15275)",
        "before_after_code_files": [
          "airflow/decorators/base.py||airflow/decorators/base.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/decorators/base.py||airflow/decorators/base.py": [
          "File: airflow/decorators/base.py -> airflow/decorators/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "41: import typing_extensions",
          "42: from sqlalchemy.orm import Session",
          "45: from airflow.exceptions import AirflowException",
          "46: from airflow.models.abstractoperator import DEFAULT_RETRIES, DEFAULT_RETRY_DELAY",
          "47: from airflow.models.baseoperator import (",
          "",
          "[Removed Lines]",
          "44: from airflow import Dataset",
          "",
          "[Added Lines]",
          "44: from airflow.datasets import Dataset",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a87761af96f553606e8e6e1843e94e623af08261",
      "candidate_info": {
        "commit_hash": "a87761af96f553606e8e6e1843e94e623af08261",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/a87761af96f553606e8e6e1843e94e623af08261",
        "files": [
          "airflow/www/views.py"
        ],
        "message": "Fix the required permissions to clear a TI from the UI (#34123)\n\n(cherry picked from commit 2f5777c082189e6495f0fea44bb9050549c0056b)",
        "before_after_code_files": [
          "airflow/www/views.py||airflow/www/views.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "2312:     @auth.has_access(",
          "2313:         [",
          "2314:             (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),",
          "2316:         ]",
          "2317:     )",
          "2318:     @action_logging",
          "",
          "[Removed Lines]",
          "2315:             (permissions.ACTION_CAN_DELETE, permissions.RESOURCE_TASK_INSTANCE),",
          "",
          "[Added Lines]",
          "2315:             (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_TASK_INSTANCE),",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "2407:     @auth.has_access(",
          "2408:         [",
          "2409:             (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG),",
          "2411:         ]",
          "2412:     )",
          "2413:     @action_logging",
          "",
          "[Removed Lines]",
          "2410:             (permissions.ACTION_CAN_DELETE, permissions.RESOURCE_TASK_INSTANCE),",
          "",
          "[Added Lines]",
          "2410:             (permissions.ACTION_CAN_EDIT, permissions.RESOURCE_TASK_INSTANCE),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "0af9a5b21cf84adfe4107945c0f68bada12eb359",
      "candidate_info": {
        "commit_hash": "0af9a5b21cf84adfe4107945c0f68bada12eb359",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/0af9a5b21cf84adfe4107945c0f68bada12eb359",
        "files": [
          "airflow/api/__init__.py",
          "airflow/cli/commands/standalone_command.py",
          "airflow/executors/local_executor.py",
          "airflow/jobs/triggerer_job_runner.py",
          "airflow/triggers/external_task.py",
          "airflow/www/extensions/init_security.py"
        ],
        "message": "Move the try outside the loop when this is possible in Airflow core (#33975)\n\n* Move the try outside the loop when this is possible in Airflow core\n\n* Use supress instead of except pass\n\n(cherry picked from commit 8918b435be8c683bbd6bb2ffa871dbd31d476f48)",
        "before_after_code_files": [
          "airflow/api/__init__.py||airflow/api/__init__.py",
          "airflow/cli/commands/standalone_command.py||airflow/cli/commands/standalone_command.py",
          "airflow/executors/local_executor.py||airflow/executors/local_executor.py",
          "airflow/jobs/triggerer_job_runner.py||airflow/jobs/triggerer_job_runner.py",
          "airflow/triggers/external_task.py||airflow/triggers/external_task.py",
          "airflow/www/extensions/init_security.py||airflow/www/extensions/init_security.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api/__init__.py||airflow/api/__init__.py": [
          "File: airflow/api/__init__.py -> airflow/api/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "36:         pass",
          "38:     backends = []",
          "41:             auth = import_module(backend.strip())",
          "42:             log.info(\"Loaded API auth backend: %s\", backend)",
          "43:             backends.append(auth)",
          "47:     return backends",
          "",
          "[Removed Lines]",
          "39:     for backend in auth_backends.split(\",\"):",
          "40:         try:",
          "44:         except ImportError as err:",
          "45:             log.critical(\"Cannot import %s for API authentication due to: %s\", backend, err)",
          "46:             raise AirflowException(err)",
          "",
          "[Added Lines]",
          "39:     try:",
          "40:         for backend in auth_backends.split(\",\"):",
          "44:     except ImportError as err:",
          "45:         log.critical(\"Cannot import %s for API authentication due to: %s\", backend, err)",
          "46:         raise AirflowException(err)",
          "",
          "---------------"
        ],
        "airflow/cli/commands/standalone_command.py||airflow/cli/commands/standalone_command.py": [
          "File: airflow/cli/commands/standalone_command.py -> airflow/cli/commands/standalone_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "91:             command.start()",
          "92:         # Run output loop",
          "93:         shown_ready = False",
          "96:                 # Print all the current lines onto the screen",
          "97:                 self.update_output()",
          "98:                 # Print info banner when all components are ready and the",
          "",
          "[Removed Lines]",
          "94:         while True:",
          "95:             try:",
          "",
          "[Added Lines]",
          "94:         try:",
          "95:             while True:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "108:                     shown_ready = True",
          "109:                 # Ensure we idle-sleep rather than fast-looping",
          "110:                 time.sleep(0.1)",
          "113:         # Stop subcommand threads",
          "114:         self.print_output(\"standalone\", \"Shutting down components\")",
          "115:         for command in self.subcommands.values():",
          "",
          "[Removed Lines]",
          "111:             except KeyboardInterrupt:",
          "112:                 break",
          "",
          "[Added Lines]",
          "111:         except KeyboardInterrupt:",
          "112:             pass",
          "",
          "---------------"
        ],
        "airflow/executors/local_executor.py||airflow/executors/local_executor.py": [
          "File: airflow/executors/local_executor.py -> airflow/executors/local_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: \"\"\"",
          "25: from __future__ import annotations",
          "27: import logging",
          "28: import os",
          "29: import subprocess",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "27: import contextlib",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "330:         def sync(self):",
          "331:             \"\"\"Sync will get called periodically by the heartbeat method.\"\"\"",
          "334:                     results = self.executor.result_queue.get_nowait()",
          "335:                     try:",
          "336:                         self.executor.change_state(*results)",
          "337:                     finally:",
          "338:                         self.executor.result_queue.task_done()",
          "342:         def end(self):",
          "343:             \"\"\"",
          "",
          "[Removed Lines]",
          "332:             while True:",
          "333:                 try:",
          "339:                 except Empty:",
          "340:                     break",
          "",
          "[Added Lines]",
          "333:             with contextlib.suppress(Empty):",
          "334:                 while True:",
          "",
          "---------------"
        ],
        "airflow/jobs/triggerer_job_runner.py||airflow/jobs/triggerer_job_runner.py": [
          "File: airflow/jobs/triggerer_job_runner.py -> airflow/jobs/triggerer_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "460:         \"\"\"",
          "461:         watchdog = asyncio.create_task(self.block_watchdog())",
          "462:         last_status = time.time()",
          "465:                 # Run core logic",
          "466:                 await self.create_triggers()",
          "467:                 await self.cancel_triggers()",
          "",
          "[Removed Lines]",
          "463:         while not self.stop:",
          "464:             try:",
          "",
          "[Added Lines]",
          "463:         try:",
          "464:             while not self.stop:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "473:                     count = len(self.triggers)",
          "474:                     self.log.info(\"%i triggers currently running\", count)",
          "475:                     last_status = time.time()",
          "479:         # Wait for watchdog to complete",
          "480:         await watchdog",
          "",
          "[Removed Lines]",
          "476:             except Exception:",
          "477:                 self.stop = True",
          "478:                 raise",
          "",
          "[Added Lines]",
          "476:         except Exception:",
          "477:             self.stop = True",
          "478:             raise",
          "",
          "---------------"
        ],
        "airflow/triggers/external_task.py||airflow/triggers/external_task.py": [
          "File: airflow/triggers/external_task.py -> airflow/triggers/external_task.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "90:         If dag with specified name was not in the running state after _timeout_sec seconds",
          "91:         after starting execution process of the trigger, terminate with status 'timeout'.",
          "92:         \"\"\"",
          "95:                 delta = utcnow() - self.trigger_start_time",
          "96:                 if delta.total_seconds() < self._timeout_sec:",
          "97:                     # mypy confuses typing here",
          "",
          "[Removed Lines]",
          "93:         while True:",
          "94:             try:",
          "",
          "[Added Lines]",
          "93:         try:",
          "94:             while True:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "107:                     return",
          "108:                 self.log.info(\"Task is still running, sleeping for %s seconds...\", self.poll_interval)",
          "109:                 await asyncio.sleep(self.poll_interval)",
          "114:     @sync_to_async",
          "115:     @provide_session",
          "",
          "[Removed Lines]",
          "110:             except Exception:",
          "111:                 yield TriggerEvent({\"status\": \"failed\"})",
          "112:                 return",
          "",
          "[Added Lines]",
          "110:         except Exception:",
          "111:             yield TriggerEvent({\"status\": \"failed\"})",
          "",
          "---------------"
        ],
        "airflow/www/extensions/init_security.py||airflow/www/extensions/init_security.py": [
          "File: airflow/www/extensions/init_security.py -> airflow/www/extensions/init_security.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "56:         pass",
          "58:     app.api_auth = []",
          "61:             auth = import_module(backend.strip())",
          "62:             auth.init_app(app)",
          "63:             app.api_auth.append(auth)",
          "69: def init_check_user_active(app):",
          "",
          "[Removed Lines]",
          "59:     for backend in auth_backends.split(\",\"):",
          "60:         try:",
          "64:         except ImportError as err:",
          "65:             log.critical(\"Cannot import %s for API authentication due to: %s\", backend, err)",
          "66:             raise AirflowException(err)",
          "",
          "[Added Lines]",
          "59:     try:",
          "60:         for backend in auth_backends.split(\",\"):",
          "64:     except ImportError as err:",
          "65:         log.critical(\"Cannot import %s for API authentication due to: %s\", backend, err)",
          "66:         raise AirflowException(err)",
          "",
          "---------------"
        ]
      }
    }
  ]
}