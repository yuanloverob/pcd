{
  "cve_id": "CVE-2023-35005",
  "cve_desc": "In Apache Airflow, some potentially sensitive values were being shown to the user in certain situations.\n\nThis vulnerability is mitigated by the fact configuration is not shown in the UI by default (only if `[webserver] expose_config` is set to `non-sensitive-only`), and not all uncensored values are actually sentitive.\n\n\nThis issue affects Apache Airflow: from 2.5.0 before 2.6.2. Users are recommended to update to version 2.6.2 or later.\n\n\n",
  "repo": "apache/airflow",
  "patch_hash": "f6cda8fb63250fc4700658999739c1c3c5f6625c",
  "patch_info": {
    "commit_hash": "f6cda8fb63250fc4700658999739c1c3c5f6625c",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/f6cda8fb63250fc4700658999739c1c3c5f6625c",
    "files": [
      "airflow/configuration.py"
    ],
    "message": "Mark `[secrets] backend_kwargs` as a sensitive config (#31788)\n\n(cherry picked from commit 8062756fa9e01eeeee1f2c6df74f376c0a526bd5)",
    "before_after_code_files": [
      "airflow/configuration.py||airflow/configuration.py"
    ]
  },
  "patch_diff": {
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "156:     (\"atlas\", \"password\"),",
      "157:     (\"smtp\", \"smtp_password\"),",
      "158:     (\"webserver\", \"secret_key\"),",
      "159:     # The following options are deprecated",
      "160:     (\"core\", \"sql_alchemy_conn\"),",
      "161: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "159:     (\"secrets\", \"backend_kwargs\"),",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "8cd9c4a00b088eaa39a53f7b5b6875288ac8fd3b",
      "candidate_info": {
        "commit_hash": "8cd9c4a00b088eaa39a53f7b5b6875288ac8fd3b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8cd9c4a00b088eaa39a53f7b5b6875288ac8fd3b",
        "files": [
          "airflow/providers/apache/hive/hooks/hive.py",
          "airflow/providers/apache/hive/transfers/mssql_to_hive.py",
          "airflow/providers/apache/hive/transfers/mysql_to_hive.py",
          "airflow/providers/apache/hive/transfers/vertica_to_hive.py",
          "airflow/providers/google/cloud/transfers/sql_to_gcs.py",
          "airflow/providers/microsoft/azure/transfers/oracle_to_azure_data_lake.py",
          "airflow/providers/mysql/transfers/vertica_to_mysql.py",
          "docs/spelling_wordlist.txt",
          "setup.cfg",
          "tests/providers/apache/hive/transfers/test_mssql_to_hive.py",
          "tests/providers/apache/hive/transfers/test_mysql_to_hive.py",
          "tests/providers/google/cloud/transfers/test_sql_to_gcs.py",
          "tests/providers/microsoft/azure/transfers/test_oracle_to_azure_data_lake.py"
        ],
        "message": "Replace unicodecsv with standard csv library (#31693)\n\nunicodecsv appears to be missing a license which can cause trouble (see https://github.com/jdunck/python-unicodecsv/issues/80)\n\nAnd it appears that this library may no longer be required.\n\n(cherry picked from commit fbeb01cb17b7cb9c2e27ac7010f423a2bced78b4)",
        "before_after_code_files": [
          "airflow/providers/apache/hive/hooks/hive.py||airflow/providers/apache/hive/hooks/hive.py",
          "airflow/providers/apache/hive/transfers/mssql_to_hive.py||airflow/providers/apache/hive/transfers/mssql_to_hive.py",
          "airflow/providers/apache/hive/transfers/mysql_to_hive.py||airflow/providers/apache/hive/transfers/mysql_to_hive.py",
          "airflow/providers/apache/hive/transfers/vertica_to_hive.py||airflow/providers/apache/hive/transfers/vertica_to_hive.py",
          "airflow/providers/google/cloud/transfers/sql_to_gcs.py||airflow/providers/google/cloud/transfers/sql_to_gcs.py",
          "airflow/providers/microsoft/azure/transfers/oracle_to_azure_data_lake.py||airflow/providers/microsoft/azure/transfers/oracle_to_azure_data_lake.py",
          "airflow/providers/mysql/transfers/vertica_to_mysql.py||airflow/providers/mysql/transfers/vertica_to_mysql.py",
          "setup.cfg||setup.cfg",
          "tests/providers/apache/hive/transfers/test_mssql_to_hive.py||tests/providers/apache/hive/transfers/test_mssql_to_hive.py",
          "tests/providers/apache/hive/transfers/test_mysql_to_hive.py||tests/providers/apache/hive/transfers/test_mysql_to_hive.py",
          "tests/providers/google/cloud/transfers/test_sql_to_gcs.py||tests/providers/google/cloud/transfers/test_sql_to_gcs.py",
          "tests/providers/microsoft/azure/transfers/test_oracle_to_azure_data_lake.py||tests/providers/microsoft/azure/transfers/test_oracle_to_azure_data_lake.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/providers/apache/hive/hooks/hive.py||airflow/providers/apache/hive/hooks/hive.py": [
          "File: airflow/providers/apache/hive/hooks/hive.py -> airflow/providers/apache/hive/hooks/hive.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "36:     raise AirflowOptionalProviderFeatureException(e)",
          "40: from airflow.configuration import conf",
          "41: from airflow.exceptions import AirflowException",
          "",
          "[Removed Lines]",
          "38: import unicodecsv as csv",
          "",
          "[Added Lines]",
          "38: import csv",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "987:         message = None",
          "989:         i = 0",
          "992:             try:",
          "993:                 if output_header:",
          "994:                     self.log.debug(\"Cursor description is %s\", header)",
          "",
          "[Removed Lines]",
          "990:         with open(csv_filepath, \"wb\") as file:",
          "991:             writer = csv.writer(file, delimiter=delimiter, lineterminator=lineterminator, encoding=\"utf-8\")",
          "",
          "[Added Lines]",
          "990:         with open(csv_filepath, \"w\", encoding=\"utf-8\") as file:",
          "991:             writer = csv.writer(file, delimiter=delimiter, lineterminator=lineterminator)",
          "",
          "---------------"
        ],
        "airflow/providers/apache/hive/transfers/mssql_to_hive.py||airflow/providers/apache/hive/transfers/mssql_to_hive.py": [
          "File: airflow/providers/apache/hive/transfers/mssql_to_hive.py -> airflow/providers/apache/hive/transfers/mssql_to_hive.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: \"\"\"This module contains an operator to move data from MSSQL to Hive.\"\"\"",
          "19: from __future__ import annotations",
          "21: from collections import OrderedDict",
          "22: from tempfile import NamedTemporaryFile",
          "23: from typing import TYPE_CHECKING, Sequence",
          "25: import pymssql",
          "28: from airflow.models import BaseOperator",
          "29: from airflow.providers.apache.hive.hooks.hive import HiveCliHook",
          "",
          "[Removed Lines]",
          "26: import unicodecsv as csv",
          "",
          "[Added Lines]",
          "21: import csv",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "113:         with mssql.get_conn() as conn:",
          "114:             with conn.cursor() as cursor:",
          "115:                 cursor.execute(self.sql)",
          "118:                     field_dict = OrderedDict()",
          "119:                     for col_count, field in enumerate(cursor.description, start=1):",
          "120:                         col_position = f\"Column{col_count}\"",
          "",
          "[Removed Lines]",
          "116:                 with NamedTemporaryFile(\"w\") as tmp_file:",
          "117:                     csv_writer = csv.writer(tmp_file, delimiter=self.delimiter, encoding=\"utf-8\")",
          "",
          "[Added Lines]",
          "116:                 with NamedTemporaryFile(mode=\"w\", encoding=\"utf-8\") as tmp_file:",
          "117:                     csv_writer = csv.writer(tmp_file, delimiter=self.delimiter)",
          "",
          "---------------"
        ],
        "airflow/providers/apache/hive/transfers/mysql_to_hive.py||airflow/providers/apache/hive/transfers/mysql_to_hive.py": [
          "File: airflow/providers/apache/hive/transfers/mysql_to_hive.py -> airflow/providers/apache/hive/transfers/mysql_to_hive.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: \"\"\"This module contains an operator to move data from MySQL to Hive.\"\"\"",
          "19: from __future__ import annotations",
          "21: from collections import OrderedDict",
          "22: from contextlib import closing",
          "23: from tempfile import NamedTemporaryFile",
          "24: from typing import TYPE_CHECKING, Sequence",
          "26: import MySQLdb",
          "29: from airflow.models import BaseOperator",
          "30: from airflow.providers.apache.hive.hooks.hive import HiveCliHook",
          "",
          "[Removed Lines]",
          "27: import unicodecsv as csv",
          "",
          "[Added Lines]",
          "21: import csv",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "84:         recreate: bool = False,",
          "85:         partition: dict | None = None,",
          "86:         delimiter: str = chr(1),",
          "88:         quotechar: str = '\"',",
          "89:         escapechar: str | None = None,",
          "90:         mysql_conn_id: str = \"mysql_default\",",
          "",
          "[Removed Lines]",
          "87:         quoting: str | None = None,",
          "",
          "[Added Lines]",
          "87:         quoting: int | None = None,",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "133:         hive = HiveCliHook(hive_cli_conn_id=self.hive_cli_conn_id, auth=self.hive_auth)",
          "134:         mysql = MySqlHook(mysql_conn_id=self.mysql_conn_id)",
          "135:         self.log.info(\"Dumping MySQL query results to local file\")",
          "137:             with closing(mysql.get_conn()) as conn:",
          "138:                 with closing(conn.cursor()) as cursor:",
          "139:                     cursor.execute(self.sql)",
          "",
          "[Removed Lines]",
          "136:         with NamedTemporaryFile(\"wb\") as f:",
          "",
          "[Added Lines]",
          "136:         with NamedTemporaryFile(mode=\"w\", encoding=\"utf-8\") as f:",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "143:                         quoting=self.quoting,",
          "144:                         quotechar=self.quotechar if self.quoting != csv.QUOTE_NONE else None,",
          "145:                         escapechar=self.escapechar,",
          "147:                     )",
          "148:                     field_dict = OrderedDict()",
          "149:                     if cursor.description is not None:",
          "",
          "[Removed Lines]",
          "146:                         encoding=\"utf-8\",",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/providers/apache/hive/transfers/vertica_to_hive.py||airflow/providers/apache/hive/transfers/vertica_to_hive.py": [
          "File: airflow/providers/apache/hive/transfers/vertica_to_hive.py -> airflow/providers/apache/hive/transfers/vertica_to_hive.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: \"\"\"This module contains an operator to move data from Vertica to Hive.\"\"\"",
          "19: from __future__ import annotations",
          "21: from collections import OrderedDict",
          "22: from tempfile import NamedTemporaryFile",
          "23: from typing import TYPE_CHECKING, Any, Sequence",
          "27: from airflow.models import BaseOperator",
          "28: from airflow.providers.apache.hive.hooks.hive import HiveCliHook",
          "29: from airflow.providers.vertica.hooks.vertica import VerticaHook",
          "",
          "[Removed Lines]",
          "25: import unicodecsv as csv",
          "",
          "[Added Lines]",
          "21: import csv",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "117:         conn = vertica.get_conn()",
          "118:         cursor = conn.cursor()",
          "119:         cursor.execute(self.sql)",
          "122:             field_dict = OrderedDict()",
          "123:             for col_count, field in enumerate(cursor.description, start=1):",
          "124:                 col_position = f\"Column{col_count}\"",
          "",
          "[Removed Lines]",
          "120:         with NamedTemporaryFile(\"w\") as f:",
          "121:             csv_writer = csv.writer(f, delimiter=self.delimiter, encoding=\"utf-8\")",
          "",
          "[Added Lines]",
          "119:         with NamedTemporaryFile(mode=\"w\", encoding=\"utf-8\") as f:",
          "120:             csv_writer = csv.writer(f, delimiter=self.delimiter)",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/transfers/sql_to_gcs.py||airflow/providers/google/cloud/transfers/sql_to_gcs.py": [
          "File: airflow/providers/google/cloud/transfers/sql_to_gcs.py -> airflow/providers/google/cloud/transfers/sql_to_gcs.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: from __future__ import annotations",
          "21: import abc",
          "22: import json",
          "23: import os",
          "24: import warnings",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "22: import csv",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "28: import pyarrow as pa",
          "29: import pyarrow.parquet as pq",
          "32: from airflow.models import BaseOperator",
          "33: from airflow.providers.google.cloud.hooks.gcs import GCSHook",
          "",
          "[Removed Lines]",
          "30: import unicodecsv as csv",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "296:                 row = self.convert_types(schema, col_type_dict, row)",
          "297:                 row_dict = dict(zip(schema, row))",
          "303:                 # Append newline to make dumps BigQuery compatible.",
          "306:             # Stop if the file exceeds the file size limit.",
          "307:             fppos = tmp_file_handle.tell()",
          "",
          "[Removed Lines]",
          "299:                 tmp_file_handle.write(",
          "300:                     json.dumps(row_dict, sort_keys=True, ensure_ascii=False).encode(\"utf-8\")",
          "301:                 )",
          "304:                 tmp_file_handle.write(b\"\\n\")",
          "",
          "[Added Lines]",
          "299:                 json.dump(row_dict, tmp_file_handle, sort_keys=True, ensure_ascii=False)",
          "302:                 tmp_file_handle.write(\"\\n\")",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "332:             yield file_to_upload",
          "334:     def _get_file_to_upload(self, file_mime_type, file_no):",
          "337:         return (",
          "338:             {",
          "339:                 \"file_name\": self.filename.format(file_no),",
          "",
          "[Removed Lines]",
          "335:         \"\"\"Returns a dictionary that represents the file to upload\"\"\"",
          "336:         tmp_file_handle = NamedTemporaryFile(delete=True)",
          "",
          "[Added Lines]",
          "333:         \"\"\"Returns a dictionary that represents the file to upload.\"\"\"",
          "334:         tmp_file_handle = NamedTemporaryFile(mode=\"w\", encoding=\"utf-8\", delete=True)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "357:         \"\"\"Configure a csv writer with the file_handle and write schema",
          "358:         as headers for the new file.",
          "359:         \"\"\"",
          "361:         csv_writer.writerow(schema)",
          "362:         return csv_writer",
          "",
          "[Removed Lines]",
          "360:         csv_writer = csv.writer(file_handle, encoding=\"utf-8\", delimiter=self.field_delimiter)",
          "",
          "[Added Lines]",
          "358:         csv_writer = csv.writer(file_handle, delimiter=self.field_delimiter)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "446:         self.log.info(\"Using schema for %s\", self.schema_filename)",
          "447:         self.log.debug(\"Current schema: %s\", schema)",
          "451:         schema_file_to_upload = {",
          "452:             \"file_name\": self.schema_filename,",
          "453:             \"file_handle\": tmp_schema_file_handle,",
          "",
          "[Removed Lines]",
          "449:         tmp_schema_file_handle = NamedTemporaryFile(delete=True)",
          "450:         tmp_schema_file_handle.write(schema.encode(\"utf-8\"))",
          "",
          "[Added Lines]",
          "447:         tmp_schema_file_handle = NamedTemporaryFile(mode=\"w\", encoding=\"utf-8\", delete=True)",
          "448:         tmp_schema_file_handle.write(schema)",
          "",
          "---------------"
        ],
        "airflow/providers/microsoft/azure/transfers/oracle_to_azure_data_lake.py||airflow/providers/microsoft/azure/transfers/oracle_to_azure_data_lake.py": [
          "File: airflow/providers/microsoft/azure/transfers/oracle_to_azure_data_lake.py -> airflow/providers/microsoft/azure/transfers/oracle_to_azure_data_lake.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: import os",
          "21: from tempfile import TemporaryDirectory",
          "22: from typing import TYPE_CHECKING, Any, Sequence",
          "26: from airflow.models import BaseOperator",
          "27: from airflow.providers.microsoft.azure.hooks.data_lake import AzureDataLakeHook",
          "28: from airflow.providers.oracle.hooks.oracle import OracleHook",
          "",
          "[Removed Lines]",
          "24: import unicodecsv as csv",
          "",
          "[Added Lines]",
          "20: import csv",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "46:     :param delimiter: field delimiter in the file.",
          "47:     :param encoding: encoding type for the file.",
          "48:     :param quotechar: Character to use in quoting.",
          "50:     \"\"\"",
          "52:     template_fields: Sequence[str] = (\"filename\", \"sql\", \"sql_params\")",
          "",
          "[Removed Lines]",
          "49:     :param quoting: Quoting strategy. See unicodecsv quoting for more information.",
          "",
          "[Added Lines]",
          "48:     :param quoting: Quoting strategy. See csv library for more information.",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "65:         delimiter: str = \",\",",
          "66:         encoding: str = \"utf-8\",",
          "67:         quotechar: str = '\"',",
          "70:     ) -> None:",
          "71:         super().__init__(**kwargs)",
          "",
          "[Removed Lines]",
          "68:         quoting: str = csv.QUOTE_MINIMAL,",
          "",
          "[Added Lines]",
          "67:         quoting: int = csv.QUOTE_MINIMAL,",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "83:         self.quoting = quoting",
          "85:     def _write_temp_file(self, cursor: Any, path_to_save: str | bytes | int) -> None:",
          "87:             csv_writer = csv.writer(",
          "88:                 csvfile,",
          "89:                 delimiter=self.delimiter,",
          "91:                 quotechar=self.quotechar,",
          "92:                 quoting=self.quoting,",
          "93:             )",
          "",
          "[Removed Lines]",
          "86:         with open(path_to_save, \"wb\") as csvfile:",
          "90:                 encoding=self.encoding,",
          "",
          "[Added Lines]",
          "85:         with open(path_to_save, \"w\", encoding=self.encoding) as csvfile:",
          "",
          "---------------"
        ],
        "airflow/providers/mysql/transfers/vertica_to_mysql.py||airflow/providers/mysql/transfers/vertica_to_mysql.py": [
          "File: airflow/providers/mysql/transfers/vertica_to_mysql.py -> airflow/providers/mysql/transfers/vertica_to_mysql.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: from contextlib import closing",
          "21: from tempfile import NamedTemporaryFile",
          "22: from typing import TYPE_CHECKING, Sequence",
          "24: import MySQLdb",
          "27: from airflow.models import BaseOperator",
          "28: from airflow.providers.mysql.hooks.mysql import MySqlHook",
          "",
          "[Removed Lines]",
          "25: import unicodecsv as csv",
          "",
          "[Added Lines]",
          "20: import csv",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "125:             with closing(conn.cursor()) as cursor:",
          "126:                 cursor.execute(self.sql)",
          "127:                 selected_columns = [d.name for d in cursor.description]",
          "129:                     self.log.info(\"Selecting rows from Vertica to local file %s...\", tmpfile.name)",
          "130:                     self.log.info(self.sql)",
          "133:                     for row in cursor.iterate():",
          "134:                         csv_writer.writerow(row)",
          "135:                         count += 1",
          "",
          "[Removed Lines]",
          "128:                 with NamedTemporaryFile(\"w\") as tmpfile:",
          "132:                     csv_writer = csv.writer(tmpfile, delimiter=\"\\t\", encoding=\"utf-8\")",
          "",
          "[Added Lines]",
          "128:                 with NamedTemporaryFile(\"w\", encoding=\"utf-8\") as tmpfile:",
          "132:                     csv_writer = csv.writer(tmpfile, delimiter=\"\\t\")",
          "",
          "---------------"
        ],
        "setup.cfg||setup.cfg": [
          "File: setup.cfg -> setup.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "145:     tenacity>=6.2.0,!=8.2.0",
          "146:     termcolor>=1.1.0",
          "147:     typing-extensions>=4.0.0",
          "149:     werkzeug>=2.0",
          "151: [options.packages.find]",
          "",
          "[Removed Lines]",
          "148:     unicodecsv>=0.14.1",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/providers/apache/hive/transfers/test_mssql_to_hive.py||tests/providers/apache/hive/transfers/test_mssql_to_hive.py": [
          "File: tests/providers/apache/hive/transfers/test_mssql_to_hive.py -> tests/providers/apache/hive/transfers/test_mssql_to_hive.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "71:         mssql_to_hive_transfer.execute(context={})",
          "73:         mock_mssql_hook_cursor.return_value.execute.assert_called_once_with(mssql_to_hive_transfer.sql)",
          "77:         field_dict = OrderedDict()",
          "78:         for field in mock_mssql_hook_cursor.return_value.description:",
          "79:             field_dict[field[0]] = mssql_to_hive_transfer.type_map(field[1])",
          "",
          "[Removed Lines]",
          "74:         mock_csv.writer.assert_called_once_with(",
          "75:             mock_tmp_file, delimiter=mssql_to_hive_transfer.delimiter, encoding=\"utf-8\"",
          "76:         )",
          "",
          "[Added Lines]",
          "74:         mock_tmp_file.assert_called_with(mode=\"w\", encoding=\"utf-8\")",
          "75:         mock_csv.writer.assert_called_once_with(mock_tmp_file, delimiter=mssql_to_hive_transfer.delimiter)",
          "",
          "---------------"
        ],
        "tests/providers/apache/hive/transfers/test_mysql_to_hive.py||tests/providers/apache/hive/transfers/test_mysql_to_hive.py": [
          "File: tests/providers/apache/hive/transfers/test_mysql_to_hive.py -> tests/providers/apache/hive/transfers/test_mysql_to_hive.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: import textwrap",
          "21: from collections import OrderedDict",
          "22: from contextlib import closing",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import csv",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "277:                     )",
          "278:                     conn.commit()",
          "282:             op = MySqlToHiveOperator(",
          "283:                 task_id=\"test_m2h\",",
          "284:                 hive_cli_conn_id=\"hive_cli_default\",",
          "",
          "[Removed Lines]",
          "280:             import unicodecsv as csv",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "tests/providers/google/cloud/transfers/test_sql_to_gcs.py||tests/providers/google/cloud/transfers/test_sql_to_gcs.py": [
          "File: tests/providers/google/cloud/transfers/test_sql_to_gcs.py -> tests/providers/google/cloud/transfers/test_sql_to_gcs.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import pandas as pd",
          "24: import pytest",
          "27: from airflow.providers.google.cloud.hooks.gcs import GCSHook",
          "28: from airflow.providers.google.cloud.transfers.sql_to_gcs import BaseSQLToGCSOperator",
          "",
          "[Removed Lines]",
          "25: import unicodecsv as csv",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "88: class TestBaseSQLToGCSOperator:",
          "89:     @mock.patch(\"airflow.providers.google.cloud.transfers.sql_to_gcs.NamedTemporaryFile\")",
          "91:     @mock.patch.object(GCSHook, \"upload\")",
          "92:     @mock.patch.object(DummySQLToGCSOperator, \"query\")",
          "93:     @mock.patch.object(DummySQLToGCSOperator, \"convert_type\")",
          "95:         cursor_mock = Mock()",
          "96:         cursor_mock.description = CURSOR_DESCRIPTION",
          "97:         cursor_mock.__iter__ = Mock(return_value=iter(INPUT_DATA))",
          "98:         mock_query.return_value = cursor_mock",
          "99:         mock_convert_type.return_value = \"convert_type_return_value\"",
          "113:         mock_file.name = TMP_FILE_NAME",
          "120:         # Test CSV",
          "121:         operator = DummySQLToGCSOperator(",
          "122:             sql=SQL,",
          "",
          "[Removed Lines]",
          "90:     @mock.patch.object(csv.writer, \"writerow\")",
          "94:     def test_exec(self, mock_convert_type, mock_query, mock_upload, mock_writerow, mock_tempfile):",
          "101:         mock_file = Mock()",
          "103:         mock_tell = Mock()",
          "104:         mock_tell.return_value = 3",
          "105:         mock_file.tell = mock_tell",
          "107:         mock_flush = Mock()",
          "108:         mock_file.flush = mock_flush",
          "110:         mock_close = Mock()",
          "111:         mock_file.close = mock_close",
          "115:         mock_write = Mock()",
          "116:         mock_file.write = mock_write",
          "118:         mock_tempfile.return_value = mock_file",
          "",
          "[Added Lines]",
          "89:     @mock.patch(\"csv.writer\")",
          "93:     def test_exec(self, mock_convert_type, mock_query, mock_upload, mock_writer, mock_tempfile):",
          "100:         mock_file = mock_tempfile.return_value",
          "101:         mock_file.tell.return_value = 3",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "145:         }",
          "147:         mock_query.assert_called_once()",
          "160:         csv_calls = []",
          "161:         for i in range(0, 3):",
          "162:             csv_calls.append(",
          "",
          "[Removed Lines]",
          "148:         mock_writerow.assert_has_calls(",
          "149:             [",
          "150:                 mock.call(COLUMNS),",
          "151:                 mock.call(ROW),",
          "152:                 mock.call(COLUMNS),",
          "153:                 mock.call(ROW),",
          "154:                 mock.call(COLUMNS),",
          "155:                 mock.call(ROW),",
          "156:                 mock.call(COLUMNS),",
          "157:             ]",
          "158:         )",
          "159:         mock_flush.assert_has_calls([mock.call(), mock.call(), mock.call(), mock.call()])",
          "",
          "[Added Lines]",
          "132:         assert mock_writer.return_value.writerow.call_args_list == [",
          "133:             mock.call(COLUMNS),",
          "134:             mock.call(ROW),",
          "135:             mock.call(COLUMNS),",
          "136:             mock.call(ROW),",
          "137:             mock.call(COLUMNS),",
          "138:             mock.call(ROW),",
          "139:             mock.call(COLUMNS),",
          "140:         ]",
          "141:         mock_file.flush.assert_has_calls([mock.call(), mock.call(), mock.call(), mock.call()])",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "174:         )",
          "175:         upload_calls = [json_call, csv_calls[0], csv_calls[1], csv_calls[2]]",
          "176:         mock_upload.assert_has_calls(upload_calls)",
          "179:         mock_query.reset_mock()",
          "181:         mock_upload.reset_mock()",
          "183:         cursor_mock.reset_mock()",
          "185:         cursor_mock.__iter__ = Mock(return_value=iter(INPUT_DATA))",
          "",
          "[Removed Lines]",
          "177:         mock_close.assert_has_calls([mock.call(), mock.call(), mock.call(), mock.call()])",
          "180:         mock_flush.reset_mock()",
          "182:         mock_close.reset_mock()",
          "",
          "[Added Lines]",
          "159:         mock_file.close.assert_has_calls([mock.call(), mock.call(), mock.call(), mock.call()])",
          "162:         mock_file.flush.reset_mock()",
          "164:         mock_file.close.reset_mock()",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "200:         }",
          "202:         mock_query.assert_called_once()",
          "214:         mock_upload.assert_called_once_with(",
          "215:             BUCKET, FILENAME.format(0), TMP_FILE_NAME, mime_type=APP_JSON, gzip=False, metadata=None",
          "216:         )",
          "219:         mock_query.reset_mock()",
          "221:         mock_upload.reset_mock()",
          "223:         cursor_mock.reset_mock()",
          "225:         cursor_mock.__iter__ = Mock(return_value=iter(INPUT_DATA))",
          "",
          "[Removed Lines]",
          "203:         mock_write.assert_has_calls(",
          "204:             [",
          "205:                 mock.call(OUTPUT_DATA),",
          "206:                 mock.call(b\"\\n\"),",
          "207:                 mock.call(OUTPUT_DATA),",
          "208:                 mock.call(b\"\\n\"),",
          "209:                 mock.call(OUTPUT_DATA),",
          "210:                 mock.call(b\"\\n\"),",
          "211:             ]",
          "212:         )",
          "213:         mock_flush.assert_called_once()",
          "217:         mock_close.assert_called_once()",
          "220:         mock_flush.reset_mock()",
          "222:         mock_close.reset_mock()",
          "",
          "[Added Lines]",
          "185:         mock_file.write.call_args_list == [",
          "186:             mock.call(OUTPUT_DATA),",
          "187:             mock.call(b\"\\n\"),",
          "188:             mock.call(OUTPUT_DATA),",
          "189:             mock.call(b\"\\n\"),",
          "190:             mock.call(OUTPUT_DATA),",
          "191:             mock.call(b\"\\n\"),",
          "192:         ]",
          "196:         mock_file.close.assert_called_once()",
          "199:         mock_file.flush.reset_mock()",
          "201:         mock_file.close.reset_mock()",
          "202:         mock_file.write.reset_mock()",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "246:         }",
          "248:         mock_query.assert_called_once()",
          "261:         mock_upload.assert_called_once_with(",
          "262:             BUCKET,",
          "263:             FILENAME.format(0),",
          "",
          "[Removed Lines]",
          "249:         mock_write.assert_has_calls(",
          "250:             [",
          "251:                 mock.call(OUTPUT_DATA),",
          "252:                 mock.call(b\"\\n\"),",
          "253:                 mock.call(OUTPUT_DATA),",
          "254:                 mock.call(b\"\\n\"),",
          "255:                 mock.call(OUTPUT_DATA),",
          "256:                 mock.call(b\"\\n\"),",
          "257:             ]",
          "258:         )",
          "260:         mock_flush.assert_called_once()",
          "",
          "[Added Lines]",
          "229:         mock_file.write.call_args_list == [",
          "230:             mock.call(OUTPUT_DATA),",
          "231:             mock.call(b\"\\n\"),",
          "232:             mock.call(OUTPUT_DATA),",
          "233:             mock.call(b\"\\n\"),",
          "234:             mock.call(OUTPUT_DATA),",
          "235:             mock.call(b\"\\n\"),",
          "236:         ]",
          "238:         mock_file.flush.assert_called_once()",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "266:             gzip=False,",
          "267:             metadata={\"row_count\": 3},",
          "268:         )",
          "271:         mock_query.reset_mock()",
          "273:         mock_upload.reset_mock()",
          "275:         cursor_mock.reset_mock()",
          "277:         cursor_mock.__iter__ = Mock(return_value=iter(INPUT_DATA))",
          "",
          "[Removed Lines]",
          "269:         mock_close.assert_called_once()",
          "272:         mock_flush.reset_mock()",
          "274:         mock_close.reset_mock()",
          "",
          "[Added Lines]",
          "247:         mock_file.close.assert_called_once()",
          "250:         mock_file.flush.reset_mock()",
          "252:         mock_file.close.reset_mock()",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "296:         }",
          "298:         mock_query.assert_called_once()",
          "300:         mock_upload.assert_called_once_with(",
          "301:             BUCKET,",
          "302:             FILENAME.format(0),",
          "",
          "[Removed Lines]",
          "299:         mock_flush.assert_called_once()",
          "",
          "[Added Lines]",
          "277:         mock_file.flush.assert_called_once()",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "305:             gzip=False,",
          "306:             metadata=None,",
          "307:         )",
          "310:         mock_query.reset_mock()",
          "312:         mock_upload.reset_mock()",
          "314:         cursor_mock.reset_mock()",
          "316:         cursor_mock.__iter__ = Mock(return_value=iter(INPUT_DATA))",
          "",
          "[Removed Lines]",
          "308:         mock_close.assert_called_once()",
          "311:         mock_flush.reset_mock()",
          "313:         mock_close.reset_mock()",
          "",
          "[Added Lines]",
          "286:         mock_file.close.assert_called_once()",
          "289:         mock_file.flush.reset_mock()",
          "291:         mock_file.close.reset_mock()",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "351:         }",
          "353:         mock_query.assert_called_once()",
          "356:         mock_upload.assert_has_calls(",
          "357:             [",
          "358:                 mock.call(",
          "",
          "[Removed Lines]",
          "354:         assert mock_flush.call_count == 3",
          "355:         assert mock_close.call_count == 3",
          "",
          "[Added Lines]",
          "332:         assert mock_file.flush.call_count == 3",
          "333:         assert mock_file.close.call_count == 3",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "368:         )",
          "370:         mock_query.reset_mock()",
          "372:         mock_upload.reset_mock()",
          "374:         cursor_mock.reset_mock()",
          "376:         cursor_mock.__iter__ = Mock(return_value=iter(INPUT_DATA))",
          "",
          "[Removed Lines]",
          "371:         mock_flush.reset_mock()",
          "373:         mock_close.reset_mock()",
          "",
          "[Added Lines]",
          "349:         mock_file.flush.reset_mock()",
          "351:         mock_file.close.reset_mock()",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "396:             \"files\": [{\"file_name\": \"test_results_0.csv\", \"file_mime_type\": \"text/csv\", \"file_row_count\": 3}],",
          "397:         }",
          "400:             [",
          "401:                 mock.call(COLUMNS),",
          "402:                 mock.call([\"NULL\", \"NULL\", \"NULL\"]),",
          "",
          "[Removed Lines]",
          "399:         mock_writerow.assert_has_calls(",
          "",
          "[Added Lines]",
          "377:         mock_writer.return_value.writerow.assert_has_calls(",
          "",
          "---------------"
        ],
        "tests/providers/microsoft/azure/transfers/test_oracle_to_azure_data_lake.py||tests/providers/microsoft/azure/transfers/test_oracle_to_azure_data_lake.py": [
          "File: tests/providers/microsoft/azure/transfers/test_oracle_to_azure_data_lake.py -> tests/providers/microsoft/azure/transfers/test_oracle_to_azure_data_lake.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: import os",
          "21: from tempfile import TemporaryDirectory",
          "22: from unittest import mock",
          "23: from unittest.mock import MagicMock",
          "27: from airflow.providers.microsoft.azure.transfers.oracle_to_azure_data_lake import (",
          "28:     OracleToAzureDataLakeOperator,",
          "29: )",
          "",
          "[Removed Lines]",
          "25: import unicodecsv as csv",
          "",
          "[Added Lines]",
          "20: import csv",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "70:             assert os.path.exists(os.path.join(temp, filename)) == 1",
          "75:                 rownum = 0",
          "76:                 for row in temp_file:",
          "",
          "[Removed Lines]",
          "72:             with open(os.path.join(temp, filename), \"rb\") as csvfile:",
          "73:                 temp_file = csv.reader(csvfile, delimiter=delimiter, encoding=encoding)",
          "",
          "[Added Lines]",
          "71:             with open(os.path.join(temp, filename), encoding=encoding) as csvfile:",
          "72:                 temp_file = csv.reader(csvfile, delimiter=delimiter)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "47438559e349a4c6d0ebf55d7ff548cd1ddca96b",
      "candidate_info": {
        "commit_hash": "47438559e349a4c6d0ebf55d7ff548cd1ddca96b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/47438559e349a4c6d0ebf55d7ff548cd1ddca96b",
        "files": [
          "README.md",
          "RELEASE_NOTES.rst",
          "airflow/__init__.py",
          "airflow/api_connexion/openapi/v1.yaml",
          "airflow/executors/kubernetes_executor.py",
          "airflow/utils/db.py",
          "docs/apache-airflow/howto/docker-compose/index.rst",
          "docs/apache-airflow/installation/supported-versions.rst",
          "docs/docker-stack/README.md",
          "docs/docker-stack/docker-examples/extending/add-apt-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-build-essential-extend/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-providers/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-pypi-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/add-requirement-packages/Dockerfile",
          "docs/docker-stack/docker-examples/extending/custom-providers/Dockerfile",
          "docs/docker-stack/docker-examples/extending/embedding-dags/Dockerfile",
          "docs/docker-stack/docker-examples/extending/writable-directory/Dockerfile",
          "docs/docker-stack/entrypoint.rst",
          "images/breeze/output-commands-hash.txt",
          "images/breeze/output-commands.svg",
          "images/breeze/output_ci.svg",
          "images/breeze/output_k8s_configure-cluster.svg",
          "images/breeze/output_k8s_delete-cluster.svg",
          "images/breeze/output_k8s_deploy-airflow.svg",
          "images/breeze/output_k8s_k9s.svg",
          "images/breeze/output_k8s_logs.svg",
          "images/breeze/output_k8s_run-complete-tests.svg",
          "images/breeze/output_k8s_shell.svg",
          "images/breeze/output_k8s_status.svg",
          "images/breeze/output_k8s_tests.svg",
          "images/breeze/output_k8s_upload-k8s-image.svg",
          "images/breeze/output_release-management_generate-issue-content-providers.svg",
          "images/breeze/output_release-management_prepare-provider-documentation.svg",
          "images/breeze/output_release-management_prepare-provider-packages.svg",
          "images/breeze/output_shell.svg",
          "images/breeze/output_static-checks.svg",
          "images/breeze/output_testing_docker-compose-tests.svg",
          "images/breeze/output_testing_integration-tests.svg",
          "images/breeze/output_testing_tests.svg",
          "scripts/ci/pre_commit/pre_commit_supported_versions.py",
          "tests/decorators/test_python.py",
          "tests/operators/test_python.py",
          "tests/providers/odbc/hooks/test_odbc.py"
        ],
        "message": "Update RELEASE_NOTES.rst",
        "before_after_code_files": [
          "airflow/__init__.py||airflow/__init__.py",
          "airflow/executors/kubernetes_executor.py||airflow/executors/kubernetes_executor.py",
          "airflow/utils/db.py||airflow/utils/db.py",
          "scripts/ci/pre_commit/pre_commit_supported_versions.py||scripts/ci/pre_commit/pre_commit_supported_versions.py",
          "tests/decorators/test_python.py||tests/decorators/test_python.py",
          "tests/operators/test_python.py||tests/operators/test_python.py",
          "tests/providers/odbc/hooks/test_odbc.py||tests/providers/odbc/hooks/test_odbc.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/__init__.py||airflow/__init__.py": [
          "File: airflow/__init__.py -> airflow/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: \"\"\"",
          "27: from __future__ import annotations",
          "31: # flake8: noqa: F401",
          "",
          "[Removed Lines]",
          "29: __version__ = \"2.6.1\"",
          "",
          "[Added Lines]",
          "29: __version__ = \"2.6.2\"",
          "",
          "---------------"
        ],
        "airflow/executors/kubernetes_executor.py||airflow/executors/kubernetes_executor.py": [
          "File: airflow/executors/kubernetes_executor.py -> airflow/executors/kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "217:         event: Any,",
          "218:     ) -> None:",
          "219:         pod = event[\"object\"]",
          "221:         \"\"\"Process status response.\"\"\"",
          "222:         if status == \"Pending\":",
          "223:             # deletion_timestamp is set by kube server when a graceful deletion is requested.",
          "224:             # since kube server have received request to delete pod set TI state failed",
          "225:             if event[\"type\"] == \"DELETED\" and pod.metadata.deletion_timestamp:",
          "227:                 self.watcher_queue.put((pod_name, namespace, State.FAILED, annotations, resource_version))",
          "228:             else:",
          "229:                 self.log.debug(\"Event: %s Pending\", pod_name)",
          "",
          "[Removed Lines]",
          "220:         annotations_string = annotations_for_logging_task_metadata(annotations)",
          "226:                 self.log.info(\"Event: Failed to start pod %s, annotations: %s\", pod_name, annotations_string)",
          "",
          "[Added Lines]",
          "225:                 self.log.info(\"Event: Failed to start pod %s\", pod_name)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "252:             # deletion_timestamp is set by kube server when a graceful deletion is requested.",
          "253:             # since kube server have received request to delete pod set TI state failed",
          "254:             if event[\"type\"] == \"DELETED\" and pod.metadata.deletion_timestamp:",
          "260:                 self.watcher_queue.put((pod_name, namespace, State.FAILED, annotations, resource_version))",
          "261:             else:",
          "262:                 self.log.info(\"Event: %s is Running\", pod_name)",
          "",
          "[Removed Lines]",
          "255:                 self.log.info(",
          "256:                     \"Event: Pod %s deleted before it could complete, annotations: %s\",",
          "257:                     pod_name,",
          "258:                     annotations_string,",
          "259:                 )",
          "",
          "[Added Lines]",
          "254:                 self.log.info(\"Event: Pod %s deleted before it could complete\", pod_name)",
          "",
          "---------------"
        ],
        "airflow/utils/db.py||airflow/utils/db.py": [
          "File: airflow/utils/db.py -> airflow/utils/db.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "81:     \"2.5.3\": \"290244fb8b83\",",
          "82:     \"2.6.0\": \"98ae134e6fff\",",
          "83:     \"2.6.1\": \"98ae134e6fff\",",
          "84: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "84:     \"2.6.2\": \"98ae134e6fff\",",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_supported_versions.py||scripts/ci/pre_commit/pre_commit_supported_versions.py": [
          "File: scripts/ci/pre_commit/pre_commit_supported_versions.py -> scripts/ci/pre_commit/pre_commit_supported_versions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "27: HEADERS = (\"Version\", \"Current Patch/Minor\", \"State\", \"First Release\", \"Limited Support\", \"EOL/Terminated\")",
          "29: SUPPORTED_VERSIONS = (",
          "31:     (\"1.10\", \"1.10.15\", \"EOL\", \"Aug 27, 2018\", \"Dec 17, 2020\", \"June 17, 2021\"),",
          "32:     (\"1.9\", \"1.9.0\", \"EOL\", \"Jan 03, 2018\", \"Aug 27, 2018\", \"Aug 27, 2018\"),",
          "33:     (\"1.8\", \"1.8.2\", \"EOL\", \"Mar 19, 2017\", \"Jan 03, 2018\", \"Jan 03, 2018\"),",
          "",
          "[Removed Lines]",
          "30:     (\"2\", \"2.6.1\", \"Supported\", \"Dec 17, 2020\", \"TBD\", \"TBD\"),",
          "",
          "[Added Lines]",
          "30:     (\"2\", \"2.6.2\", \"Supported\", \"Dec 17, 2020\", \"TBD\", \"TBD\"),",
          "",
          "---------------"
        ],
        "tests/decorators/test_python.py||tests/decorators/test_python.py": [
          "File: tests/decorators/test_python.py -> tests/decorators/test_python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import pytest",
          "25: from airflow import PY38, PY311",
          "27: from airflow.decorators.base import DecoratedMappedOperator",
          "28: from airflow.exceptions import AirflowException",
          "29: from airflow.models import DAG",
          "",
          "[Removed Lines]",
          "26: from airflow.decorators import setup, task as task_decorator, teardown",
          "",
          "[Added Lines]",
          "26: from airflow.decorators import task as task_decorator",
          "",
          "---------------"
        ],
        "tests/operators/test_python.py||tests/operators/test_python.py": [
          "File: tests/operators/test_python.py -> tests/operators/test_python.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "32: from slugify import slugify",
          "34: from airflow.decorators import task_group",
          "36: from airflow.models import DAG, DagRun, TaskInstance as TI",
          "37: from airflow.models.baseoperator import BaseOperator",
          "38: from airflow.models.taskinstance import clear_task_instances, set_current_context",
          "",
          "[Removed Lines]",
          "35: from airflow.exceptions import AirflowException, DeserializingResultError, RemovedInAirflow3Warning",
          "",
          "[Added Lines]",
          "35: from airflow.exceptions import AirflowException, RemovedInAirflow3Warning",
          "",
          "---------------"
        ],
        "tests/providers/odbc/hooks/test_odbc.py||tests/providers/odbc/hooks/test_odbc.py": [
          "File: tests/providers/odbc/hooks/test_odbc.py -> tests/providers/odbc/hooks/test_odbc.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import json",
          "21: from unittest import mock",
          "22: from urllib.parse import quote_plus, urlsplit",
          "24: import pyodbc",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "21: import logging",
          "23: from unittest.mock import patch",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "5d9d97291ce3d9334eae214a3077bfeae96ff3d5",
      "candidate_info": {
        "commit_hash": "5d9d97291ce3d9334eae214a3077bfeae96ff3d5",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/5d9d97291ce3d9334eae214a3077bfeae96ff3d5",
        "files": [
          "airflow/jobs/backfill_job_runner.py",
          "airflow/jobs/base_job_runner.py",
          "airflow/jobs/dag_processor_job_runner.py",
          "airflow/jobs/local_task_job_runner.py",
          "airflow/jobs/scheduler_job_runner.py",
          "airflow/jobs/triggerer_job_runner.py"
        ],
        "message": "Make BaseJobRunner a generic on the job class (#31287)\n\n(cherry picked from commit 2b6462b5a44ea4502c8f4d1b3d7bc9814a65c937)",
        "before_after_code_files": [
          "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py",
          "airflow/jobs/base_job_runner.py||airflow/jobs/base_job_runner.py",
          "airflow/jobs/dag_processor_job_runner.py||airflow/jobs/dag_processor_job_runner.py",
          "airflow/jobs/local_task_job_runner.py||airflow/jobs/local_task_job_runner.py",
          "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py",
          "airflow/jobs/triggerer_job_runner.py||airflow/jobs/triggerer_job_runner.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py": [
          "File: airflow/jobs/backfill_job_runner.py -> airflow/jobs/backfill_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "57:     from airflow.models.abstractoperator import AbstractOperator",
          "61:     \"\"\"",
          "62:     A backfill job runner consists of a dag or subdag for a specific time range.",
          "",
          "[Removed Lines]",
          "60: class BackfillJobRunner(BaseJobRunner, LoggingMixin):",
          "",
          "[Added Lines]",
          "60: class BackfillJobRunner(BaseJobRunner[Job], LoggingMixin):",
          "",
          "---------------"
        ],
        "airflow/jobs/base_job_runner.py||airflow/jobs/base_job_runner.py": [
          "File: airflow/jobs/base_job_runner.py -> airflow/jobs/base_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "22: from airflow.utils.session import NEW_SESSION, provide_session",
          "",
          "[Removed Lines]",
          "20: from typing import TYPE_CHECKING",
          "",
          "[Added Lines]",
          "20: from typing import TYPE_CHECKING, Generic, TypeVar",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "25:     from sqlalchemy.orm import Session",
          "27:     from airflow.jobs.job import Job",
          "31:     \"\"\"Abstract class for job runners to derive from.\"\"\"",
          "33:     job_type = \"undefined\"",
          "36:         if job.job_type and job.job_type != self.job_type:",
          "37:             raise Exception(",
          "38:                 f\"The job is already assigned a different job_type: {job.job_type}.\"",
          "39:                 f\"This is a bug and should be reported.\"",
          "40:             )",
          "44:     def _execute(self) -> int | None:",
          "45:         \"\"\"",
          "",
          "[Removed Lines]",
          "30: class BaseJobRunner:",
          "35:     def __init__(self, job):",
          "41:         self.job = job",
          "42:         self.job.job_type = self.job_type",
          "",
          "[Added Lines]",
          "28:     from airflow.serialization.pydantic.job import JobPydantic",
          "30: J = TypeVar(\"J\", \"Job\", \"JobPydantic\", \"Job | JobPydantic\")",
          "33: class BaseJobRunner(Generic[J]):",
          "38:     def __init__(self, job: J) -> None:",
          "44:         job.job_type = self.job_type",
          "45:         self.job: J = job",
          "",
          "---------------"
        ],
        "airflow/jobs/dag_processor_job_runner.py||airflow/jobs/dag_processor_job_runner.py": [
          "File: airflow/jobs/dag_processor_job_runner.py -> airflow/jobs/dag_processor_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29:     pass",
          "33:     \"\"\"",
          "34:     DagProcessorJobRunner is a job runner that runs a DagFileProcessorManager processor.",
          "",
          "[Removed Lines]",
          "32: class DagProcessorJobRunner(BaseJobRunner, LoggingMixin):",
          "",
          "[Added Lines]",
          "32: class DagProcessorJobRunner(BaseJobRunner[Job], LoggingMixin):",
          "",
          "---------------"
        ],
        "airflow/jobs/local_task_job_runner.py||airflow/jobs/local_task_job_runner.py": [
          "File: airflow/jobs/local_task_job_runner.py -> airflow/jobs/local_task_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "72:     \"\"\"LocalTaskJob runs a single task instance.\"\"\"",
          "74:     job_type = \"LocalTaskJob\"",
          "",
          "[Removed Lines]",
          "71: class LocalTaskJobRunner(BaseJobRunner, LoggingMixin):",
          "",
          "[Added Lines]",
          "71: class LocalTaskJobRunner(BaseJobRunner[\"Job | JobPydantic\"], LoggingMixin):",
          "",
          "---------------"
        ],
        "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py": [
          "File: airflow/jobs/scheduler_job_runner.py -> airflow/jobs/scheduler_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "119:     return multiprocessing.current_process().name == \"MainProcess\"",
          "123:     \"\"\"",
          "124:     SchedulerJobRunner runs for a specific time interval and schedules jobs that are ready to run.",
          "",
          "[Removed Lines]",
          "122: class SchedulerJobRunner(BaseJobRunner, LoggingMixin):",
          "",
          "[Added Lines]",
          "122: class SchedulerJobRunner(BaseJobRunner[Job], LoggingMixin):",
          "",
          "---------------"
        ],
        "airflow/jobs/triggerer_job_runner.py||airflow/jobs/triggerer_job_runner.py": [
          "File: airflow/jobs/triggerer_job_runner.py -> airflow/jobs/triggerer_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "234:         return None",
          "238:     \"\"\"",
          "239:     TriggererJobRunner continuously runs active triggers in asyncio, watching",
          "240:     for them to fire off their events and then dispatching that information",
          "",
          "[Removed Lines]",
          "237: class TriggererJobRunner(BaseJobRunner, LoggingMixin):",
          "",
          "[Added Lines]",
          "237: class TriggererJobRunner(BaseJobRunner[\"Job | JobPydantic\"], LoggingMixin):",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "bb62a46a38385eabb3ee57f337fd9fc0fcf882fb",
      "candidate_info": {
        "commit_hash": "bb62a46a38385eabb3ee57f337fd9fc0fcf882fb",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/bb62a46a38385eabb3ee57f337fd9fc0fcf882fb",
        "files": [
          "airflow/cli/commands/user_command.py"
        ],
        "message": "fix airflow users delete CLI command (#31539)\n\n* fix airflow users delete CLI command\n\n* replace setting to empty list by clear method\n\n(cherry picked from commit 3ec66bb7cc686d060ff728bb6bf4d4e70e387ae3)",
        "before_after_code_files": [
          "airflow/cli/commands/user_command.py||airflow/cli/commands/user_command.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/commands/user_command.py||airflow/cli/commands/user_command.py": [
          "File: airflow/cli/commands/user_command.py -> airflow/cli/commands/user_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "112:     \"\"\"Deletes user from DB.\"\"\"",
          "113:     user = _find_user(args)",
          "115:     from airflow.utils.cli_app_builder import get_application_builder",
          "117:     with get_application_builder() as appbuilder:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "115:     # Clear the associated user roles first.",
          "116:     user.roles.clear()",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "945c20e37dd2ee75a94672299c20b88ab8c70ed6",
      "candidate_info": {
        "commit_hash": "945c20e37dd2ee75a94672299c20b88ab8c70ed6",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/945c20e37dd2ee75a94672299c20b88ab8c70ed6",
        "files": [
          "airflow/api/common/mark_tasks.py",
          "airflow/jobs/backfill_job_runner.py",
          "airflow/jobs/scheduler_job_runner.py",
          "airflow/models/dag.py",
          "airflow/models/taskinstance.py",
          "airflow/timetables/base.py",
          "airflow/timetables/simple.py",
          "tests/jobs/test_scheduler_job.py"
        ],
        "message": "Optimize scheduler by skipping \"non-scheduable\" DAGs (#30706)\n\nCo-authored-by: Jens Scheffler <jens.scheffler@de.bosch.com>\n(cherry picked from commit ec18db170745a8b1df0bb75569cd22e69892b3e2)",
        "before_after_code_files": [
          "airflow/api/common/mark_tasks.py||airflow/api/common/mark_tasks.py",
          "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py",
          "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py",
          "airflow/models/dag.py||airflow/models/dag.py",
          "airflow/models/taskinstance.py||airflow/models/taskinstance.py",
          "airflow/timetables/base.py||airflow/timetables/base.py",
          "airflow/timetables/simple.py||airflow/timetables/simple.py",
          "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/31796"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/api/common/mark_tasks.py||airflow/api/common/mark_tasks.py": [
          "File: airflow/api/common/mark_tasks.py -> airflow/api/common/mark_tasks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "306:     else:",
          "307:         start_date = execution_date",
          "308:     start_date = execution_date if not past else start_date",
          "310:         # If the DAG never schedules, need to look at existing DagRun if the user wants future or",
          "311:         # past runs.",
          "312:         dag_runs = dag.get_dagruns_between(start_date=start_date, end_date=end_date)",
          "",
          "[Removed Lines]",
          "309:     if not dag.timetable.can_run:",
          "",
          "[Added Lines]",
          "309:     if not dag.timetable.can_be_scheduled:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "338:     # determine run_id range of dag runs and tasks to consider",
          "339:     end_date = last_dagrun.logical_date if future else current_dagrun.logical_date",
          "340:     start_date = current_dagrun.logical_date if not past else first_dagrun.logical_date",
          "342:         # If the DAG never schedules, need to look at existing DagRun if the user wants future or",
          "343:         # past runs.",
          "344:         dag_runs = dag.get_dagruns_between(start_date=start_date, end_date=end_date, session=session)",
          "",
          "[Removed Lines]",
          "341:     if not dag.timetable.can_run:",
          "",
          "[Added Lines]",
          "341:     if not dag.timetable.can_be_scheduled:",
          "",
          "---------------"
        ],
        "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py": [
          "File: airflow/jobs/backfill_job_runner.py -> airflow/jobs/backfill_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "317:         run_date = dagrun_info.logical_date",
          "319:         # consider max_active_runs but ignore when running subdags",
          "322:         current_active_dag_count = dag.get_num_active_runs(external_trigger=False)",
          "",
          "[Removed Lines]",
          "320:         respect_dag_max_active_limit = bool(dag.timetable.can_run and not dag.is_subdag)",
          "",
          "[Added Lines]",
          "320:         respect_dag_max_active_limit = bool(dag.timetable.can_be_scheduled and not dag.is_subdag)",
          "",
          "---------------"
        ],
        "airflow/jobs/scheduler_job_runner.py||airflow/jobs/scheduler_job_runner.py": [
          "File: airflow/jobs/scheduler_job_runner.py -> airflow/jobs/scheduler_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1178:                     creating_job_id=self.job.id,",
          "1179:                 )",
          "1180:                 active_runs_of_dags[dag.dag_id] += 1",
          "1182:                 dag_model.calculate_dagrun_date_fields(dag, data_interval)",
          "1183:         # TODO[HA]: Should we do a session.flush() so we don't have to keep lots of state/object in",
          "1184:         # memory for larger dags? or expunge_all()",
          "",
          "[Removed Lines]",
          "1181:             if self._should_update_dag_next_dagruns(dag, dag_model, active_runs_of_dags[dag.dag_id]):",
          "",
          "[Added Lines]",
          "1181:             if self._should_update_dag_next_dagruns(",
          "1182:                 dag, dag_model, active_runs_of_dags[dag.dag_id], session=session",
          "1183:             ):",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1283:                     DatasetDagRunQueue.target_dag_id == dag_run.dag_id",
          "1284:                 ).delete()",
          "1287:         \"\"\"Check if the dag's next_dagruns_create_after should be updated.\"\"\"",
          "1289:             self.log.info(",
          "1290:                 \"DAG %s is at (or above) max_active_runs (%d of %d), not creating any more runs\",",
          "1291:                 dag_model.dag_id,",
          "",
          "[Removed Lines]",
          "1286:     def _should_update_dag_next_dagruns(self, dag, dag_model: DagModel, total_active_runs: int) -> bool:",
          "1288:         if total_active_runs >= dag.max_active_runs:",
          "",
          "[Added Lines]",
          "1288:     def _should_update_dag_next_dagruns(",
          "1289:         self, dag: DAG, dag_model: DagModel, total_active_runs: int | None = None, *, session: Session",
          "1290:     ) -> bool:",
          "1292:         # If the DAG never schedules skip save runtime",
          "1293:         if not dag.timetable.can_be_scheduled:",
          "1294:             return False",
          "1296:         # get active dag runs from DB if not available",
          "1297:         if not total_active_runs:",
          "1298:             total_active_runs = dag.get_num_active_runs(only_running=False, session=session)",
          "1300:         if total_active_runs and total_active_runs >= dag.max_active_runs:",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "1392:                 session.merge(task_instance)",
          "1393:             session.flush()",
          "1394:             self.log.info(\"Run %s of %s has timed-out\", dag_run.run_id, dag_run.dag_id)",
          "1396:             # Work out if we should allow creating a new DagRun now?",
          "1398:                 dag_model.calculate_dagrun_date_fields(dag, dag.get_run_data_interval(dag_run))",
          "1400:             callback_to_execute = DagCallbackRequest(",
          "",
          "[Removed Lines]",
          "1395:             active_runs = dag.get_num_active_runs(only_running=False, session=session)",
          "1397:             if self._should_update_dag_next_dagruns(dag, dag_model, active_runs):",
          "",
          "[Added Lines]",
          "1408:             if self._should_update_dag_next_dagruns(dag, dag_model, session=session):",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "1421:             return callback",
          "1422:         # TODO[HA]: Rename update_state -> schedule_dag_run, ?? something else?",
          "1423:         schedulable_tis, callback_to_run = dag_run.update_state(session=session, execute_callbacks=False)",
          "1424:         if dag_run.state in State.finished:",
          "1426:             # Work out if we should allow creating a new DagRun now?",
          "1428:                 dag_model.calculate_dagrun_date_fields(dag, dag.get_run_data_interval(dag_run))",
          "1429:         # This will do one query per dag run. We \"could\" build up a complex",
          "1430:         # query to update all the TIs across all the execution dates and dag",
          "",
          "[Removed Lines]",
          "1425:             active_runs = dag.get_num_active_runs(only_running=False, session=session)",
          "1427:             if self._should_update_dag_next_dagruns(dag, dag_model, active_runs):",
          "",
          "[Added Lines]",
          "1435:         # Check if DAG not scheduled then skip interval calculation to same scheduler runtime",
          "1438:             if self._should_update_dag_next_dagruns(dag, dag_model, session=session):",
          "",
          "---------------"
        ],
        "airflow/models/dag.py||airflow/models/dag.py": [
          "File: airflow/models/dag.py -> airflow/models/dag.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "871:         DO NOT use this method is there is a known data interval.",
          "872:         \"\"\"",
          "873:         timetable_type = type(self.timetable)",
          "875:             return DataInterval.exact(timezone.coerce_datetime(logical_date))",
          "876:         start = timezone.coerce_datetime(logical_date)",
          "877:         if issubclass(timetable_type, CronDataIntervalTimetable):",
          "",
          "[Removed Lines]",
          "874:         if issubclass(timetable_type, (NullTimetable, OnceTimetable)):",
          "",
          "[Added Lines]",
          "874:         if issubclass(timetable_type, (NullTimetable, OnceTimetable, DatasetTriggeredTimetable)):",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1252:     @property",
          "1253:     def allow_future_exec_dates(self) -> bool:",
          "1256:     @provide_session",
          "1257:     def get_concurrency_reached(self, session=NEW_SESSION) -> bool:",
          "",
          "[Removed Lines]",
          "1254:         return settings.ALLOW_FUTURE_EXEC_DATES and not self.timetable.can_run",
          "",
          "[Added Lines]",
          "1254:         return settings.ALLOW_FUTURE_EXEC_DATES and not self.timetable.can_be_scheduled",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "3085:         Validates & raise exception if there are any Params in the DAG which neither have a default value nor",
          "3086:         have the null in schema['type'] list, but the DAG have a schedule_interval which is not None.",
          "3087:         \"\"\"",
          "3089:             return",
          "3091:         for k, v in self.params.items():",
          "",
          "[Removed Lines]",
          "3088:         if not self.timetable.can_run:",
          "",
          "[Added Lines]",
          "3088:         if not self.timetable.can_be_scheduled:",
          "",
          "---------------"
        ],
        "airflow/models/taskinstance.py||airflow/models/taskinstance.py": [
          "File: airflow/models/taskinstance.py -> airflow/models/taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "962:         # or the DAG is never scheduled. For legacy reasons, when",
          "963:         # `catchup=True`, we use `get_previous_scheduled_dagrun` unless",
          "964:         # `ignore_schedule` is `True`.",
          "966:         if dag.catchup is True and not ignore_schedule:",
          "967:             last_dagrun = dr.get_previous_scheduled_dagrun(session=session)",
          "968:         else:",
          "",
          "[Removed Lines]",
          "965:         ignore_schedule = state is not None or not dag.timetable.can_run",
          "",
          "[Added Lines]",
          "965:         ignore_schedule = state is not None or not dag.timetable.can_be_scheduled",
          "",
          "---------------"
        ],
        "airflow/timetables/base.py||airflow/timetables/base.py": [
          "File: airflow/timetables/base.py -> airflow/timetables/base.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: from __future__ import annotations",
          "19: from typing import TYPE_CHECKING, Any, NamedTuple, Sequence",
          "21: from pendulum import DateTime",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: from warnings import warn",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "122:     like ``schedule=None`` and ``\"@once\"`` set it to *False*.",
          "123:     \"\"\"",
          "130:     \"\"\"",
          "132:     run_ordering: Sequence[str] = (\"data_interval_end\", \"execution_date\")",
          "",
          "[Removed Lines]",
          "125:     can_run: bool = True",
          "126:     \"\"\"Whether this timetable can actually schedule runs.",
          "128:     This defaults to and should generally be *True*, but ``NullTimetable`` sets",
          "129:     this to *False*.",
          "",
          "[Added Lines]",
          "126:     _can_be_scheduled: bool = True",
          "128:     @property",
          "129:     def can_be_scheduled(self):",
          "130:         if hasattr(self, \"can_run\"):",
          "131:             warn(",
          "132:                 'can_run class variable is deprecated. Use \"can_be_scheduled\" instead.',",
          "133:                 DeprecationWarning,",
          "134:                 stacklevel=2,",
          "135:             )",
          "136:             return self.can_run",
          "137:         return self._can_be_scheduled",
          "139:     \"\"\"Whether this timetable can actually schedule runs in an automated manner.",
          "141:     This defaults to and should generally be *True* (including non periodic",
          "142:     execution types like *@once* and data triggered tables), but",
          "143:     ``NullTimetable`` sets this to *False*.",
          "",
          "---------------"
        ],
        "airflow/timetables/simple.py||airflow/timetables/simple.py": [
          "File: airflow/timetables/simple.py -> airflow/timetables/simple.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "34:     \"\"\"Some code reuse for \"trivial\" timetables that has nothing complex.\"\"\"",
          "36:     periodic = False",
          "38:     run_ordering = (\"execution_date\",)",
          "40:     @classmethod",
          "",
          "[Removed Lines]",
          "37:     can_run = False",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "63:     This corresponds to ``schedule=None``.",
          "64:     \"\"\"",
          "66:     description: str = \"Never, external triggers only\"",
          "68:     @property",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "65:     can_be_scheduled = False",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "144:         return DagRunInfo.interval(start, end)",
          "148:     \"\"\"Timetable that never schedules anything.",
          "150:     This should not be directly used anywhere, but only set if a DAG is triggered by datasets.",
          "",
          "[Removed Lines]",
          "147: class DatasetTriggeredTimetable(NullTimetable):",
          "",
          "[Added Lines]",
          "147: class DatasetTriggeredTimetable(_TrivialTimetable):",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "188:             events, key=operator.attrgetter(\"source_dag_run.data_interval_end\")",
          "189:         ).source_dag_run.data_interval_end",
          "190:         return DataInterval(start, end)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "192:     def next_dagrun_info(",
          "193:         self,",
          "195:         last_automated_data_interval: DataInterval | None,",
          "196:         restriction: TimeRestriction,",
          "197:     ) -> DagRunInfo | None:",
          "198:         return None",
          "",
          "---------------"
        ],
        "tests/jobs/test_scheduler_job.py||tests/jobs/test_scheduler_job.py": [
          "File: tests/jobs/test_scheduler_job.py -> tests/jobs/test_scheduler_job.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "3397:             self.job_runner._send_sla_callbacks_to_processor(dag)",
          "3398:             scheduler_job.executor.callback_sink.send.assert_not_called()",
          "3400:     def test_create_dag_runs(self, dag_maker):",
          "3401:         \"\"\"",
          "3402:         Test various invariants of _create_dag_runs.",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "3400:     @pytest.mark.parametrize(",
          "3401:         \"schedule, number_running, excepted\",",
          "3402:         [",
          "3403:             (None, None, False),",
          "3404:             (\"*/1 * * * *\", None, False),",
          "3405:             (\"*/1 * * * *\", 1, True),",
          "3406:         ],",
          "3407:         ids=[\"no_dag_schedule\", \"dag_schedule_too_many_runs\", \"dag_schedule_less_runs\"],",
          "3408:     )",
          "3409:     def test_should_update_dag_next_dagruns(self, schedule, number_running, excepted, session, dag_maker):",
          "3410:         \"\"\"Test if really required to update next dagrun or possible to save run time\"\"\"",
          "3412:         with dag_maker(",
          "3413:             dag_id=\"test_should_update_dag_next_dagruns\", schedule=schedule, max_active_runs=2",
          "3414:         ) as dag:",
          "3415:             EmptyOperator(task_id=\"dummy\")",
          "3417:         dag_model = dag_maker.dag_model",
          "3419:         for index in range(2):",
          "3420:             dag_maker.create_dagrun(",
          "3421:                 run_id=f\"run_{index}\",",
          "3422:                 execution_date=(DEFAULT_DATE + timedelta(days=index)),",
          "3423:                 start_date=timezone.utcnow(),",
          "3424:                 state=State.RUNNING,",
          "3425:                 session=session,",
          "3426:             )",
          "3428:         session.flush()",
          "3429:         scheduler_job = Job(executor=self.null_exec)",
          "3430:         self.job_runner = SchedulerJobRunner(job=scheduler_job)",
          "3432:         assert excepted is self.job_runner._should_update_dag_next_dagruns(",
          "3433:             dag, dag_model, number_running, session=session",
          "3434:         )",
          "",
          "---------------"
        ]
      }
    }
  ]
}