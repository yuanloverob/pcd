{
  "cve_id": "CVE-2023-45348",
  "cve_desc": "Apache Airflow, versions 2.7.0 and 2.7.1, is affected by a vulnerability that allows an authenticated user to retrieve sensitive configuration information when the \"expose_config\" option is set to \"non-sensitive-only\". The `expose_config` option is False by default.\nIt is recommended to upgrade to a version that is not affected.",
  "repo": "apache/airflow",
  "patch_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
  "patch_info": {
    "commit_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
    "files": [
      "airflow/api_connexion/endpoints/config_endpoint.py",
      "airflow/configuration.py",
      "tests/api_connexion/endpoints/test_config_endpoint.py"
    ],
    "message": "Check if the lower of provided values are sensitives in config endpoint (#34712)\n\n* Check if the lower of provided values are sensitives in config endpoint\n\n* update unit test\n\n* ensure that all values in sensitive dict are in lower characters\n\n(cherry picked from commit f044589b685855a8fce8f5376bea2564c5a001f7)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py",
      "airflow/configuration.py||airflow/configuration.py",
      "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py": [
      "File: airflow/api_connexion/endpoints/config_endpoint.py -> airflow/api_connexion/endpoints/config_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "123:                 \"Config not found.\", detail=f\"The option [{section}/{option}] is not found in config.\"",
      "124:             )",
      "127:             value = \"< hidden >\"",
      "128:         else:",
      "129:             value = conf.get(section, option)",
      "",
      "[Removed Lines]",
      "126:         if (section, option) in conf.sensitive_config_values:",
      "",
      "[Added Lines]",
      "126:         if (section.lower(), option.lower()) in conf.sensitive_config_values:",
      "",
      "---------------"
    ],
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "311:             for s, s_c in self.configuration_description.items()",
      "312:             for k, item in s_c.get(\"options\").items()  # type: ignore[union-attr]",
      "313:         }",
      "315:         depr_option = {self.deprecated_options[x][:-1] for x in sensitive if x in self.deprecated_options}",
      "316:         depr_section = {",
      "317:             (self.deprecated_sections[s][0], k) for s, k in sensitive if s in self.deprecated_sections",
      "",
      "[Removed Lines]",
      "314:         sensitive = {(section, key) for (section, key), v in flattened.items() if v.get(\"sensitive\") is True}",
      "",
      "[Added Lines]",
      "314:         sensitive = {",
      "315:             (section.lower(), key.lower())",
      "316:             for (section, key), v in flattened.items()",
      "317:             if v.get(\"sensitive\") is True",
      "318:         }",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_config_endpoint.py -> tests/api_connexion/endpoints/test_config_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "247:         return_value=MOCK_CONF_WITH_SENSITIVE_VALUE,",
      "248:     )",
      "249:     @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "251:         response = self.client.get(",
      "253:             headers={\"Accept\": \"text/plain\"},",
      "254:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "255:         )",
      "256:         assert response.status_code == 200",
      "257:         expected = textwrap.dedent(",
      "261:         \"\"\"",
      "262:         )",
      "263:         assert expected == response.data.decode()",
      "",
      "[Removed Lines]",
      "250:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict):",
      "252:             \"/api/v1/config/section/core/option/sql_alchemy_conn\",",
      "258:             \"\"\"\\",
      "259:         [core]",
      "260:         sql_alchemy_conn = < hidden >",
      "",
      "[Added Lines]",
      "250:     @pytest.mark.parametrize(",
      "251:         \"section, option\",",
      "252:         [",
      "253:             (\"core\", \"sql_alchemy_conn\"),",
      "254:             (\"core\", \"SQL_ALCHEMY_CONN\"),",
      "255:             (\"corE\", \"sql_alchemy_conn\"),",
      "256:             (\"CORE\", \"sql_alchemy_conn\"),",
      "257:         ],",
      "258:     )",
      "259:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict, section, option):",
      "261:             f\"/api/v1/config/section/{section}/option/{option}\",",
      "267:             f\"\"\"\\",
      "268:         [{section}]",
      "269:         {option} = < hidden >",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "b6af4adf0f89b06b167e29cf6690abb581e8a34c",
      "candidate_info": {
        "commit_hash": "b6af4adf0f89b06b167e29cf6690abb581e8a34c",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/b6af4adf0f89b06b167e29cf6690abb581e8a34c",
        "files": [
          "airflow/www/session.py"
        ],
        "message": "Fix SesssionExemptMixin spelling (#34696)\n\nCo-authored-by: David Kalamarides <david.kalamarides@capitalone.com>\n(cherry picked from commit 63945c71241e7b1b278068e1786e610facd569e0)",
        "before_after_code_files": [
          "airflow/www/session.py||airflow/www/session.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/session.py||airflow/www/session.py": [
          "File: airflow/www/session.py -> airflow/www/session.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: from flask_session.sessions import SqlAlchemySessionInterface",
          "25:     \"\"\"Exempt certain blueprints/paths from autogenerated sessions.\"\"\"",
          "27:     def save_session(self, *args, **kwargs):",
          "",
          "[Removed Lines]",
          "24: class SesssionExemptMixin:",
          "",
          "[Added Lines]",
          "24: class SessionExemptMixin:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "33:         return super().save_session(*args, **kwargs)",
          "37:     \"\"\"Session interface that exempts some routes and stores session data in the database.\"\"\"",
          "41:     \"\"\"Session interface that exempts some routes and stores session data in a signed cookie.\"\"\"",
          "",
          "[Removed Lines]",
          "36: class AirflowDatabaseSessionInterface(SesssionExemptMixin, SqlAlchemySessionInterface):",
          "40: class AirflowSecureCookieSessionInterface(SesssionExemptMixin, SecureCookieSessionInterface):",
          "",
          "[Added Lines]",
          "36: class AirflowDatabaseSessionInterface(SessionExemptMixin, SqlAlchemySessionInterface):",
          "40: class AirflowSecureCookieSessionInterface(SessionExemptMixin, SecureCookieSessionInterface):",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "472d25c39ce0152dc5285adb64e07a16bea87319",
      "candidate_info": {
        "commit_hash": "472d25c39ce0152dc5285adb64e07a16bea87319",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/472d25c39ce0152dc5285adb64e07a16bea87319",
        "files": [
          "airflow/cli/commands/webserver_command.py",
          "airflow/providers/databricks/hooks/databricks_sql.py",
          "airflow/providers/google/cloud/operators/datafusion.py",
          "scripts/ci/pre_commit/pre_commit_unittest_testcase.py",
          "tests/core/test_configuration.py",
          "tests/decorators/test_setup_teardown.py"
        ],
        "message": "Do not create lists we don't need (#33519)\n\n(cherry picked from commit 4154cc04ce9702b09e6f13d423686fdf4cb7b877)",
        "before_after_code_files": [
          "airflow/cli/commands/webserver_command.py||airflow/cli/commands/webserver_command.py",
          "airflow/providers/databricks/hooks/databricks_sql.py||airflow/providers/databricks/hooks/databricks_sql.py",
          "airflow/providers/google/cloud/operators/datafusion.py||airflow/providers/google/cloud/operators/datafusion.py",
          "scripts/ci/pre_commit/pre_commit_unittest_testcase.py||scripts/ci/pre_commit/pre_commit_unittest_testcase.py",
          "tests/core/test_configuration.py||tests/core/test_configuration.py",
          "tests/decorators/test_setup_teardown.py||tests/decorators/test_setup_teardown.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/cli/commands/webserver_command.py||airflow/cli/commands/webserver_command.py": [
          "File: airflow/cli/commands/webserver_command.py -> airflow/cli/commands/webserver_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "144:                 pass",
          "145:             return False",
          "150:     def _get_num_workers_running(self) -> int:",
          "151:         \"\"\"Return number of running Gunicorn workers processes.\"\"\"",
          "",
          "[Removed Lines]",
          "147:         ready_workers = [proc for proc in workers if ready_prefix_on_cmdline(proc)]",
          "148:         return len(ready_workers)",
          "",
          "[Added Lines]",
          "147:         nb_ready_workers = sum(1 for proc in workers if ready_prefix_on_cmdline(proc))",
          "148:         return nb_ready_workers",
          "",
          "---------------"
        ],
        "airflow/providers/databricks/hooks/databricks_sql.py||airflow/providers/databricks/hooks/databricks_sql.py": [
          "File: airflow/providers/databricks/hooks/databricks_sql.py -> airflow/providers/databricks/hooks/databricks_sql.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: from contextlib import closing",
          "20: from copy import copy",
          "23: from databricks import sql  # type: ignore[attr-defined]",
          "26: from airflow.exceptions import AirflowException",
          "27: from airflow.providers.common.sql.hooks.sql import DbApiHook, return_single_query_results",
          "28: from airflow.providers.databricks.hooks.databricks_base import BaseDatabricksHook",
          "30: LIST_SQL_ENDPOINTS_ENDPOINT = (\"GET\", \"api/2.0/sql/endpoints\")",
          "",
          "[Removed Lines]",
          "21: from typing import Any, Callable, Iterable, Mapping, TypeVar, overload",
          "24: from databricks.sql.client import Connection  # type: ignore[attr-defined]",
          "",
          "[Added Lines]",
          "21: from typing import TYPE_CHECKING, Any, Callable, Iterable, Mapping, TypeVar, overload",
          "29: if TYPE_CHECKING:",
          "30:     from databricks.sql.client import Connection",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "91:         result = self._do_api_call(LIST_SQL_ENDPOINTS_ENDPOINT)",
          "92:         if \"endpoints\" not in result:",
          "93:             raise AirflowException(\"Can't list Databricks SQL endpoints\")",
          "99:     def get_conn(self) -> Connection:",
          "100:         \"\"\"Returns a Databricks SQL connection object.\"\"\"",
          "",
          "[Removed Lines]",
          "94:         lst = [endpoint for endpoint in result[\"endpoints\"] if endpoint[\"name\"] == endpoint_name]",
          "95:         if not lst:",
          "96:             raise AirflowException(f\"Can't f Databricks SQL endpoint with name '{endpoint_name}'\")",
          "97:         return lst[0]",
          "",
          "[Added Lines]",
          "96:         try:",
          "97:             endpoint = next(endpoint for endpoint in result[\"endpoints\"] if endpoint[\"name\"] == endpoint_name)",
          "98:         except StopIteration:",
          "99:             raise AirflowException(f\"Can't find Databricks SQL endpoint with name '{endpoint_name}'\")",
          "100:         else:",
          "101:             return endpoint",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/operators/datafusion.py||airflow/providers/google/cloud/operators/datafusion.py": [
          "File: airflow/providers/google/cloud/operators/datafusion.py -> airflow/providers/google/cloud/operators/datafusion.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:     @staticmethod",
          "45:     def get_project_id(instance):",
          "46:         instance = instance[\"name\"]",
          "48:         return project_id",
          "",
          "[Removed Lines]",
          "47:         project_id = [x for x in instance.split(\"/\") if x.startswith(\"airflow\")][0]",
          "",
          "[Added Lines]",
          "47:         project_id = next(x for x in instance.split(\"/\") if x.startswith(\"airflow\"))",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_unittest_testcase.py||scripts/ci/pre_commit/pre_commit_unittest_testcase.py": [
          "File: scripts/ci/pre_commit/pre_commit_unittest_testcase.py -> scripts/ci/pre_commit/pre_commit_unittest_testcase.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "29:     classes = [c for c in node.body if isinstance(c, ast.ClassDef)]",
          "30:     for c in classes:",
          "31:         # Some classes are returned as an ast.Attribute, some as an ast.Name object. Not quite sure why",
          "36:             found += 1",
          "37:             print(f\"The class {c.name} inherits from TestCase, please use pytest instead\")",
          "39:     return found",
          "42: def main(*args: str) -> int:",
          "46: if __name__ == \"__main__\":",
          "",
          "[Removed Lines]",
          "32:         parent_classes = [base.attr for base in c.bases if isinstance(base, ast.Attribute)]",
          "33:         parent_classes.extend([base.id for base in c.bases if isinstance(base, ast.Name)])",
          "35:         if \"TestCase\" in parent_classes:",
          "43:     return sum([check_test_file(file) for file in args[1:]])",
          "",
          "[Added Lines]",
          "32:         if any(",
          "33:             (isinstance(base, ast.Attribute) and base.attr == \"TestCase\")",
          "34:             or (isinstance(base, ast.Name) and base.id == \"TestCase\")",
          "35:             for base in c.bases",
          "36:         ):",
          "43:     return sum(check_test_file(file) for file in args[1:])",
          "",
          "---------------"
        ],
        "tests/core/test_configuration.py||tests/core/test_configuration.py": [
          "File: tests/core/test_configuration.py -> tests/core/test_configuration.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1570:         all_sections_including_defaults = airflow_cfg.get_sections_including_defaults()",
          "1571:         assert \"core\" in all_sections_including_defaults",
          "1572:         assert \"test-section\" in all_sections_including_defaults",
          "1575:     def test_get_options_including_defaults(self):",
          "1576:         airflow_cfg = AirflowConfigParser()",
          "",
          "[Removed Lines]",
          "1573:         assert len([section for section in all_sections_including_defaults if section == \"core\"]) == 1",
          "",
          "[Added Lines]",
          "1573:         assert sum(1 for section in all_sections_including_defaults if section == \"core\") == 1",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1594:         assert \"dags_folder\" in all_core_options_including_defaults",
          "1595:         assert \"test-value\" == airflow_cfg.get(\"core\", \"new-test-key\")",
          "1596:         assert \"test-runner\" == airflow_cfg.get(\"core\", \"task_runner\")",
          "1600: def test_sensitive_values():",
          "",
          "[Removed Lines]",
          "1597:         assert len([option for option in all_core_options_including_defaults if option == \"task_runner\"]) == 1",
          "",
          "[Added Lines]",
          "1597:         assert sum(1 for option in all_core_options_including_defaults if option == \"task_runner\") == 1",
          "",
          "---------------"
        ],
        "tests/decorators/test_setup_teardown.py||tests/decorators/test_setup_teardown.py": [
          "File: tests/decorators/test_setup_teardown.py -> tests/decorators/test_setup_teardown.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "255:             mytask2()",
          "257:         assert len(dag.task_group.children) == 6",
          "259:         assert dag.task_group.children[\"setuptask\"].is_setup",
          "260:         assert dag.task_group.children[\"teardowntask\"].is_teardown",
          "261:         assert dag.task_group.children[\"setuptask2\"].is_setup",
          "",
          "[Removed Lines]",
          "258:         assert [x for x in dag.tasks if not x.downstream_list]  # no deps have been set",
          "",
          "[Added Lines]",
          "258:         assert sum(1 for x in dag.tasks if not x.downstream_list) == 6",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "248b88b5fa75bc53fd167a8856e670f77a07dbc1",
      "candidate_info": {
        "commit_hash": "248b88b5fa75bc53fd167a8856e670f77a07dbc1",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/248b88b5fa75bc53fd167a8856e670f77a07dbc1",
        "files": [
          "dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/params/build_ci_params.py",
          "dev/breeze/src/airflow_breeze/params/build_prod_params.py",
          "dev/breeze/src/airflow_breeze/params/doc_build_params.py",
          "dev/breeze/src/airflow_breeze/params/shell_params.py",
          "dev/breeze/src/airflow_breeze/utils/coertions.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/src/airflow_breeze/utils/run_utils.py",
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "dev/breeze/tests/test_exclude_from_matrix.py",
          "dev/provider_packages/prepare_provider_packages.py"
        ],
        "message": "Simplify conditions on len() in dev (#33562)\n\n(cherry picked from commit 50abdcea19a909d7f048bb0ee4ac59cc0bcbb37c)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py||dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/params/build_ci_params.py||dev/breeze/src/airflow_breeze/params/build_ci_params.py",
          "dev/breeze/src/airflow_breeze/params/build_prod_params.py||dev/breeze/src/airflow_breeze/params/build_prod_params.py",
          "dev/breeze/src/airflow_breeze/params/doc_build_params.py||dev/breeze/src/airflow_breeze/params/doc_build_params.py",
          "dev/breeze/src/airflow_breeze/params/shell_params.py||dev/breeze/src/airflow_breeze/params/shell_params.py",
          "dev/breeze/src/airflow_breeze/utils/coertions.py||dev/breeze/src/airflow_breeze/utils/coertions.py",
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/src/airflow_breeze/utils/run_utils.py||dev/breeze/src/airflow_breeze/utils/run_utils.py",
          "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "dev/breeze/tests/test_exclude_from_matrix.py||dev/breeze/tests/test_exclude_from_matrix.py",
          "dev/provider_packages/prepare_provider_packages.py||dev/provider_packages/prepare_provider_packages.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py||dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py -> dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "386: def _delete_all_clusters():",
          "387:     clusters = list(K8S_CLUSTERS_PATH.iterdir())",
          "391:         get_console().print(\"\\n[info]Deleting clusters\")",
          "392:         for cluster_name in clusters:",
          "393:             resolved_path = cluster_name.resolve()",
          "",
          "[Removed Lines]",
          "388:     if len(clusters) == 0:",
          "389:         get_console().print(\"\\n[warning]No clusters.\\n\")",
          "390:     else:",
          "",
          "[Added Lines]",
          "388:     if clusters:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "407:                     shutil.rmtree(cluster_name.resolve(), ignore_errors=True)",
          "408:                 else:",
          "409:                     resolved_path.unlink()",
          "412: @kubernetes_group.command(",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "408:     else:",
          "409:         get_console().print(\"\\n[warning]No clusters.\\n\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "508:     make_sure_kubernetes_tools_are_installed()",
          "509:     if all:",
          "510:         clusters = list(K8S_CLUSTERS_PATH.iterdir())",
          "515:             failed = False",
          "516:             get_console().print(\"[info]\\nCluster status:\\n\")",
          "517:             for cluster_name in clusters:",
          "",
          "[Removed Lines]",
          "511:         if len(clusters) == 0:",
          "512:             get_console().print(\"\\n[warning]No clusters.\\n\")",
          "513:             sys.exit(1)",
          "514:         else:",
          "",
          "[Added Lines]",
          "511:         if clusters:",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "529:             if failed:",
          "530:                 get_console().print(\"\\n[error]Some clusters are not healthy!\\n\")",
          "531:                 sys.exit(1)",
          "532:     else:",
          "533:         if not _status(",
          "534:             python=python,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "529:         else:",
          "530:             get_console().print(\"\\n[warning]No clusters.\\n\")",
          "531:             sys.exit(1)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "1217: def logs(python: str, kubernetes_version: str, all: bool):",
          "1218:     if all:",
          "1219:         clusters = list(K8S_CLUSTERS_PATH.iterdir())",
          "1224:             get_console().print(\"[info]\\nDumping cluster logs:\\n\")",
          "1225:             for cluster_name in clusters:",
          "1226:                 name = cluster_name.name",
          "",
          "[Removed Lines]",
          "1220:         if len(clusters) == 0:",
          "1221:             get_console().print(\"\\n[warning]No clusters.\\n\")",
          "1222:             sys.exit(1)",
          "1223:         else:",
          "",
          "[Added Lines]",
          "1220:         if clusters:",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "1229:                     get_console().print(f\"[warning]\\nCould not get cluster from {name}. Skipping.\\n\")",
          "1230:                     continue",
          "1231:                 _logs(python=found_python, kubernetes_version=found_kubernetes_version)",
          "1232:     else:",
          "1233:         _logs(python=python, kubernetes_version=kubernetes_version)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1229:         else:",
          "1230:             get_console().print(\"\\n[warning]No clusters.\\n\")",
          "1231:             sys.exit(1)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "840:             \"Provide the path of cloned airflow-site repo\\n\"",
          "841:         )",
          "842:         sys.exit(1)",
          "844:         get_console().print(",
          "845:             \"\\n[error]You need to specify at least one package to generate back references for\\n\"",
          "846:         )",
          "",
          "[Removed Lines]",
          "843:     if len(packages_plus_all_providers) == 0:",
          "",
          "[Added Lines]",
          "843:     if not packages_plus_all_providers:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1419:     updated_constraint: tuple[str],",
          "1420: ) -> None:",
          "1421:     airflow_versions_array = airflow_versions.split(\",\")",
          "1423:         get_console().print(\"[error]No airflow versions specified - you provided empty string[/]\")",
          "1424:         sys.exit(1)",
          "",
          "[Removed Lines]",
          "1422:     if len(airflow_versions_array) == 0:",
          "",
          "[Added Lines]",
          "1422:     if not airflow_versions_array:",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/params/build_ci_params.py||dev/breeze/src/airflow_breeze/params/build_ci_params.py": [
          "File: dev/breeze/src/airflow_breeze/params/build_ci_params.py -> dev/breeze/src/airflow_breeze/params/build_ci_params.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "55:         extra_ci_flags.extend(",
          "56:             [\"--build-arg\", f\"AIRFLOW_CONSTRAINTS_REFERENCE={self.airflow_constraints_reference}\"]",
          "57:         )",
          "59:             extra_ci_flags.extend(",
          "60:                 [\"--build-arg\", f\"AIRFLOW_CONSTRAINTS_LOCATION={self.airflow_constraints_location}\"]",
          "61:             )",
          "",
          "[Removed Lines]",
          "58:         if self.airflow_constraints_location is not None and len(self.airflow_constraints_location) > 0:",
          "",
          "[Added Lines]",
          "58:         if self.airflow_constraints_location:",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/params/build_prod_params.py||dev/breeze/src/airflow_breeze/params/build_prod_params.py": [
          "File: dev/breeze/src/airflow_breeze/params/build_prod_params.py -> dev/breeze/src/airflow_breeze/params/build_prod_params.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "113:     @property",
          "114:     def extra_docker_build_flags(self) -> list[str]:",
          "115:         extra_build_flags = []",
          "117:             AIRFLOW_INSTALLATION_METHOD = (",
          "118:                 \"https://github.com/apache/airflow/archive/\"",
          "119:                 + self.install_airflow_reference",
          "",
          "[Removed Lines]",
          "116:         if len(self.install_airflow_reference) > 0:",
          "",
          "[Added Lines]",
          "116:         if self.install_airflow_reference:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "126:                 ]",
          "127:             )",
          "128:             extra_build_flags.extend(self.args_for_remote_install)",
          "130:             if not re.match(r\"^[0-9\\.]+((a|b|rc|alpha|beta|pre)[0-9]+)?$\", self.install_airflow_version):",
          "131:                 get_console().print(",
          "132:                     f\"\\n[error]ERROR: Bad value for install-airflow-version:{self.install_airflow_version}\"",
          "",
          "[Removed Lines]",
          "129:         elif len(self.install_airflow_version) > 0:",
          "",
          "[Added Lines]",
          "129:         elif self.install_airflow_version:",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/params/doc_build_params.py||dev/breeze/src/airflow_breeze/params/doc_build_params.py": [
          "File: dev/breeze/src/airflow_breeze/params/doc_build_params.py -> dev/breeze/src/airflow_breeze/params/doc_build_params.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "42:             doc_args.append(\"--one-pass-only\")",
          "43:         if AIRFLOW_BRANCH != \"main\":",
          "44:             doc_args.append(\"--disable-provider-checks\")",
          "46:             for single_filter in self.package_filter:",
          "47:                 doc_args.extend([\"--package-filter\", single_filter])",
          "48:         return doc_args",
          "",
          "[Removed Lines]",
          "45:         if self.package_filter and len(self.package_filter) > 0:",
          "",
          "[Added Lines]",
          "45:         if self.package_filter:",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/params/shell_params.py||dev/breeze/src/airflow_breeze/params/shell_params.py": [
          "File: dev/breeze/src/airflow_breeze/params/shell_params.py -> dev/breeze/src/airflow_breeze/params/shell_params.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "261:             integrations = ALL_INTEGRATIONS",
          "262:         else:",
          "263:             integrations = self.integration",
          "267:         if \"trino\" in integrations and \"kerberos\" not in integrations:",
          "268:             get_console().print(",
          "269:                 \"[warning]Adding `kerberos` integration as it is implicitly needed by trino\",",
          "",
          "[Removed Lines]",
          "264:         if len(integrations) > 0:",
          "265:             for integration in integrations:",
          "266:                 compose_file_list.append(DOCKER_COMPOSE_DIR / f\"integration-{integration}.yml\")",
          "",
          "[Added Lines]",
          "264:         for integration in integrations:",
          "265:             compose_file_list.append(DOCKER_COMPOSE_DIR / f\"integration-{integration}.yml\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "274:     @property",
          "275:     def command_passed(self):",
          "279:         return cmd",
          "281:     @property",
          "",
          "[Removed Lines]",
          "276:         cmd = None",
          "277:         if len(self.extra_args) > 0:",
          "278:             cmd = str(self.extra_args[0])",
          "",
          "[Added Lines]",
          "275:         cmd = str(self.extra_args[0]) if self.extra_args else None",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/coertions.py||dev/breeze/src/airflow_breeze/utils/coertions.py": [
          "File: dev/breeze/src/airflow_breeze/utils/coertions.py -> dev/breeze/src/airflow_breeze/utils/coertions.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "32: def one_or_none_set(iterable: Iterable[bool]) -> bool:",
          "",
          "[Removed Lines]",
          "33:     return len([i for i in iterable if i]) in (0, 1)",
          "",
          "[Added Lines]",
          "33:     return sum(1 for i in iterable if i) in (0, 1)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "399:         )",
          "400:     for optional_arg in image_params.optional_image_args:",
          "401:         param_value = get_env_variable_value(optional_arg, params=image_params)",
          "403:             args_command.append(\"--build-arg\")",
          "404:             args_command.append(optional_arg.upper() + \"=\" + param_value)",
          "405:     args_command.extend(image_params.docker_cache_directive)",
          "",
          "[Removed Lines]",
          "402:         if len(param_value) > 0:",
          "",
          "[Added Lines]",
          "402:         if param_value:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "809:         get_console().print(\"[warning]Could not detect docker builder. Using default.[/]\")",
          "810:         return \"default\"",
          "811:     context_list = output.stdout.splitlines()",
          "813:         get_console().print(\"[warning]Could not detect docker builder. Using default.[/]\")",
          "814:         return \"default\"",
          "816:         get_console().print(f\"[info]Using {context_list[0]} as context.[/]\")",
          "817:         return context_list[0]",
          "819:         for preferred_context in PREFERRED_CONTEXTS:",
          "820:             if preferred_context in context_list:",
          "821:                 get_console().print(f\"[info]Using {preferred_context} as context.[/]\")",
          "",
          "[Removed Lines]",
          "812:     if len(context_list) == 0:",
          "815:     if len(context_list) == 1:",
          "818:     if len(context_list) > 1:",
          "",
          "[Added Lines]",
          "812:     if not context_list:",
          "815:     elif len(context_list) == 1:",
          "818:     else:",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/run_utils.py||dev/breeze/src/airflow_breeze/utils/run_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/run_utils.py -> dev/breeze/src/airflow_breeze/utils/run_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "97:             return False",
          "98:         if _arg.startswith(\"-\"):",
          "99:             return True",
          "101:             return True",
          "102:         if _arg.startswith(\"/\"):",
          "103:             # Skip any absolute paths",
          "",
          "[Removed Lines]",
          "100:         if len(_arg) == 0:",
          "",
          "[Added Lines]",
          "100:         if not _arg:",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/selective_checks.py||dev/breeze/src/airflow_breeze/utils/selective_checks.py": [
          "File: dev/breeze/src/airflow_breeze/utils/selective_checks.py -> dev/breeze/src/airflow_breeze/utils/selective_checks.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "275:             get_console().print(",
          "276:                 \"[info]This PR had `allow suspended provider changes` label set so it will continue\"",
          "277:             )",
          "279:         return None",
          "280:     for provider in list(all_providers):",
          "281:         all_providers.update(",
          "",
          "[Removed Lines]",
          "278:     if len(all_providers) == 0:",
          "",
          "[Added Lines]",
          "278:     if not all_providers:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "356:         if self._github_event in [GithubEvents.PUSH, GithubEvents.SCHEDULE, GithubEvents.WORKFLOW_DISPATCH]:",
          "357:             get_console().print(f\"[warning]Full tests needed because event is {self._github_event}[/]\")",
          "358:             return True",
          "360:             get_console().print(\"[warning]Running everything because env files changed[/]\")",
          "361:             return True",
          "362:         if FULL_TESTS_NEEDED_LABEL in self._pr_labels:",
          "",
          "[Removed Lines]",
          "359:         if len(self._matching_files(FileGroupForCi.ENVIRONMENT_FILES, CI_FILE_GROUP_MATCHES)) > 0:",
          "",
          "[Added Lines]",
          "359:         if self._matching_files(FileGroupForCi.ENVIRONMENT_FILES, CI_FILE_GROUP_MATCHES):",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "496:             get_console().print(f\"[warning]{source_area} enabled because we are running everything[/]\")",
          "497:             return True",
          "498:         matched_files = self._matching_files(source_area, CI_FILE_GROUP_MATCHES)",
          "500:             get_console().print(",
          "501:                 f\"[warning]{source_area} enabled because it matched {len(matched_files)} changed files[/]\"",
          "502:             )",
          "",
          "[Removed Lines]",
          "499:         if len(matched_files) > 0:",
          "",
          "[Added Lines]",
          "499:         if matched_files:",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_exclude_from_matrix.py||dev/breeze/tests/test_exclude_from_matrix.py": [
          "File: dev/breeze/tests/test_exclude_from_matrix.py -> dev/breeze/tests/test_exclude_from_matrix.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "43:     exclusion_list = excluded_combos(list_1, list_2)",
          "44:     assert representative_list == expected_representative_list",
          "45:     assert len(representative_list) == len(list_1) * len(list_2) - len(exclusion_list)",
          "",
          "[Removed Lines]",
          "46:     assert len(set(representative_list) & set(exclusion_list)) == 0",
          "",
          "[Added Lines]",
          "46:     assert not set(representative_list).intersection(exclusion_list)",
          "",
          "---------------"
        ],
        "dev/provider_packages/prepare_provider_packages.py||dev/provider_packages/prepare_provider_packages.py": [
          "File: dev/provider_packages/prepare_provider_packages.py -> dev/provider_packages/prepare_provider_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1058:     changes_table += changes_table_for_version",
          "1059:     if verbose:",
          "1060:         print_changes_table(changes_table)",
          "1064: def get_provider_details(provider_package_id: str) -> ProviderPackageDetails:",
          "",
          "[Removed Lines]",
          "1061:     return True, list_of_list_of_changes if len(list_of_list_of_changes) > 0 else None, changes_table",
          "",
          "[Added Lines]",
          "1061:     return True, list_of_list_of_changes or None, changes_table",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1337:         version_suffix=version_suffix,",
          "1338:     )",
          "1339:     jinja_context[\"DETAILED_CHANGES_RST\"] = changes",
          "1341:     update_changelog_rst(",
          "1342:         jinja_context,",
          "1343:         provider_package_id,",
          "",
          "[Removed Lines]",
          "1340:     jinja_context[\"DETAILED_CHANGES_PRESENT\"] = len(changes) > 0",
          "",
          "[Added Lines]",
          "1340:     jinja_context[\"DETAILED_CHANGES_PRESENT\"] = bool(changes)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7252e2b09858757f439a161c63cd38c75eab4ca7",
      "candidate_info": {
        "commit_hash": "7252e2b09858757f439a161c63cd38c75eab4ca7",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7252e2b09858757f439a161c63cd38c75eab4ca7",
        "files": [
          "airflow/jobs/backfill_job_runner.py",
          "airflow/utils/dates.py"
        ],
        "message": "remove unnecessary map and rewrite it using list in Airflow core (#33764)\n\n(cherry picked from commit 4e545c8190d9e2a085d98c5097b7284099c7cd75)",
        "before_after_code_files": [
          "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py",
          "airflow/utils/dates.py||airflow/utils/dates.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/jobs/backfill_job_runner.py||airflow/jobs/backfill_job_runner.py": [
          "File: airflow/jobs/backfill_job_runner.py -> airflow/jobs/backfill_job_runner.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "743:             if all(key.map_index == -1 for key in ti_keys):",
          "744:                 headers = [\"DAG ID\", \"Task ID\", \"Run ID\", \"Try number\"]",
          "746:             else:",
          "747:                 headers = [\"DAG ID\", \"Task ID\", \"Run ID\", \"Map Index\", \"Try number\"]",
          "",
          "[Removed Lines]",
          "745:                 sorted_ti_keys = map(lambda k: k[0:4], sorted_ti_keys)",
          "",
          "[Added Lines]",
          "745:                 sorted_ti_keys = (k[0:4] for k in sorted_ti_keys)",
          "",
          "---------------"
        ],
        "airflow/utils/dates.py||airflow/utils/dates.py": [
          "File: airflow/utils/dates.py -> airflow/utils/dates.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "243: def scale_time_units(time_seconds_arr: Collection[float], unit: TimeUnit) -> Collection[float]:",
          "244:     \"\"\"Convert an array of time durations in seconds to the specified time unit.\"\"\"",
          "245:     if unit == \"minutes\":",
          "247:     elif unit == \"hours\":",
          "249:     elif unit == \"days\":",
          "251:     return time_seconds_arr",
          "",
          "[Removed Lines]",
          "246:         return list(map(lambda x: x / 60, time_seconds_arr))",
          "248:         return list(map(lambda x: x / (60 * 60), time_seconds_arr))",
          "250:         return list(map(lambda x: x / (24 * 60 * 60), time_seconds_arr))",
          "",
          "[Added Lines]",
          "246:         return [x / 60 for x in time_seconds_arr]",
          "248:         return [x / (60 * 60) for x in time_seconds_arr]",
          "250:         return [x / (24 * 60 * 60) for x in time_seconds_arr]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f502e2a88cf7123e2fdea5a4556b1fba4d1144cf",
      "candidate_info": {
        "commit_hash": "f502e2a88cf7123e2fdea5a4556b1fba4d1144cf",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f502e2a88cf7123e2fdea5a4556b1fba4d1144cf",
        "files": [
          "airflow/www/utils.py",
          "dev/stats/calculate_statistics_provider_testing_issues.py",
          "scripts/in_container/update_quarantined_test_status.py",
          "tests/models/test_dagcode.py",
          "tests/www/test_utils.py"
        ],
        "message": "Refactor integer division (#34180)\n\n(cherry picked from commit 9571d37cad376ab931dadf02304387c5286c1938)",
        "before_after_code_files": [
          "airflow/www/utils.py||airflow/www/utils.py",
          "dev/stats/calculate_statistics_provider_testing_issues.py||dev/stats/calculate_statistics_provider_testing_issues.py",
          "scripts/in_container/update_quarantined_test_status.py||scripts/in_container/update_quarantined_test_status.py",
          "tests/models/test_dagcode.py||tests/models/test_dagcode.py",
          "tests/www/test_utils.py||tests/www/test_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/utils.py||airflow/www/utils.py": [
          "File: airflow/www/utils.py -> airflow/www/utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "340:     output.append(previous_node.format(href_link=page_link, disabled=is_disabled))",
          "343:     last_page = num_of_pages - 1",
          "345:     if current_page <= mid or num_of_pages < window:",
          "",
          "[Removed Lines]",
          "342:     mid = int(window / 2)",
          "",
          "[Added Lines]",
          "342:     mid = window // 2",
          "",
          "---------------"
        ],
        "dev/stats/calculate_statistics_provider_testing_issues.py||dev/stats/calculate_statistics_provider_testing_issues.py": [
          "File: dev/stats/calculate_statistics_provider_testing_issues.py -> dev/stats/calculate_statistics_provider_testing_issues.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "78:     users_commented: set[str]",
          "80:     def percent_tested(self) -> int:",
          "83:     def num_involved_users_who_commented(self) -> int:",
          "84:         return len(self.users_involved.intersection(self.users_commented))",
          "",
          "[Removed Lines]",
          "81:         return int(100.0 * self.tested_issues / self.num_issues)",
          "",
          "[Added Lines]",
          "81:         return 100 * self.tested_issues // self.num_issues",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "87:         return len(self.users_commented - self.users_involved)",
          "89:     def percent_commented_among_involved(self) -> int:",
          "92:     def __str__(self):",
          "93:         return (",
          "",
          "[Removed Lines]",
          "90:         return int(100.0 * self.num_involved_users_who_commented() / len(self.users_involved))",
          "",
          "[Added Lines]",
          "90:         return 100 * self.num_involved_users_who_commented() // len(self.users_involved)",
          "",
          "---------------"
        ],
        "scripts/in_container/update_quarantined_test_status.py||scripts/in_container/update_quarantined_test_status.py": [
          "File: scripts/in_container/update_quarantined_test_status.py -> scripts/in_container/update_quarantined_test_status.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "152:         return \"Stable\"",
          "153:     if all(history.states[0 : num_runs - 1]):",
          "154:         return \"Just one more\"",
          "156:         return \"Almost there\"",
          "157:     return \"Flaky\"",
          "",
          "[Removed Lines]",
          "155:     if all(history.states[0 : int(num_runs / 2)]):",
          "",
          "[Added Lines]",
          "155:     if all(history.states[0 : num_runs // 2]):",
          "",
          "---------------"
        ],
        "tests/models/test_dagcode.py||tests/models/test_dagcode.py": [
          "File: tests/models/test_dagcode.py -> tests/models/test_dagcode.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "81:         \"\"\"Dg code can be bulk written into database.\"\"\"",
          "82:         example_dags = make_example_dags(example_dags_module)",
          "83:         files = [dag.fileloc for dag in example_dags.values()]",
          "85:         with create_session() as session:",
          "86:             DagCode.bulk_sync_to_db(half_files, session=session)",
          "87:             session.commit()",
          "",
          "[Removed Lines]",
          "84:         half_files = files[: int(len(files) / 2)]",
          "",
          "[Added Lines]",
          "84:         half_files = files[: len(files) // 2]",
          "",
          "---------------"
        ],
        "tests/www/test_utils.py||tests/www/test_utils.py": [
          "File: tests/www/test_utils.py -> tests/www/test_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "69:         assert min(window, total_pages) + extra_links == len(ulist_items)",
          "71:         page_items = ulist_items[2:-2]",
          "73:         all_nodes = []",
          "74:         pages = []",
          "",
          "[Removed Lines]",
          "72:         mid = int(len(page_items) / 2)",
          "",
          "[Added Lines]",
          "72:         mid = len(page_items) // 2",
          "",
          "---------------"
        ]
      }
    }
  ]
}