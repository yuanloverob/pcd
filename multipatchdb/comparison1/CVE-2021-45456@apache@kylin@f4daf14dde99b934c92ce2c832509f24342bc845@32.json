{
  "cve_id": "CVE-2021-45456",
  "cve_desc": "Apache kylin checks the legitimacy of the project before executing some commands with the project name passed in by the user. There is a mismatch between what is being checked and what is being used as the shell command argument in DiagnosisService. This may cause an illegal project name to pass the check and perform the following steps, resulting in a command injection vulnerability. This issue affects Apache Kylin 4.0.0.",
  "repo": "apache/kylin",
  "patch_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
  "patch_info": {
    "commit_hash": "f4daf14dde99b934c92ce2c832509f24342bc845",
    "repo": "apache/kylin",
    "commit_url": "https://github.com/apache/kylin/commit/f4daf14dde99b934c92ce2c832509f24342bc845",
    "files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "server/src/main/webapp/WEB-INF/web.xml"
    ],
    "message": "test fix",
    "before_after_code_files": [
      "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java"
    ]
  },
  "patch_diff": {
    "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "3403:     public String getKerberosPrincipal() {",
      "3404:         return getOptional(\"kylin.kerberos.principal\");",
      "3405:     }",
      "3406: }",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "3407:     public String getEncryptCipherIvSpec() {",
      "3408:         return getOptional(\"kylin.security.encrypt.cipher.ivSpec\", \"AAAAAAAAAAAAAAAA\");",
      "3409:     }",
      "",
      "---------------"
    ],
    "core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java||core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java": [
      "File: core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java -> core-common/src/main/java/org/apache/kylin/common/util/EncryptUtil.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "25: import java.security.NoSuchAlgorithmException;",
      "27: import org.apache.commons.codec.binary.Base64;",
      "29: import javax.crypto.Cipher;",
      "30: import javax.crypto.NoSuchPaddingException;",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "28: import org.apache.kylin.common.KylinConfig;",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "42:             InvalidKeyException, NoSuchPaddingException, NoSuchAlgorithmException, UnsupportedEncodingException {",
      "43:         Cipher cipher = Cipher.getInstance(\"AES/CFB/PKCS5Padding\");",
      "44:         final SecretKeySpec secretKey = new SecretKeySpec(key, \"AES\");",
      "46:         cipher.init(cipherMode, secretKey, ivSpec);",
      "47:         return cipher;",
      "48:     }",
      "",
      "[Removed Lines]",
      "45:         IvParameterSpec ivSpec = new IvParameterSpec(\"AAAAAAAAAAAAAAAA\".getBytes(\"UTF-8\"));",
      "",
      "[Added Lines]",
      "46:         IvParameterSpec ivSpec = new IvParameterSpec(KylinConfig.getInstanceFromEnv().getEncryptCipherIvSpec().getBytes(\"UTF-8\"));",
      "",
      "---------------"
    ],
    "core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java||core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java": [
      "File: core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java -> core-common/src/test/java/org/apache/kylin/common/util/EncryptUtilTest.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "19: package org.apache.kylin.common.util;",
      "21: import org.junit.Assert;",
      "22: import org.junit.Test;",
      "26:     @Test",
      "27:     public void testAESEncrypt(){",
      "",
      "[Removed Lines]",
      "24: public class EncryptUtilTest {",
      "",
      "[Added Lines]",
      "21: import org.junit.After;",
      "23: import org.junit.Before;",
      "26: public class EncryptUtilTest extends LocalFileMetadataTestCase {",
      "27:     @Before",
      "28:     public void setUp() throws Exception {",
      "29:         this.createTestMetadata();",
      "30:     }",
      "32:     @After",
      "33:     public void after() throws Exception {",
      "34:         this.cleanupTestMetadata();",
      "35:     }",
      "",
      "---------------"
    ],
    "server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java||server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java": [
      "File: server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java -> server-base/src/main/java/org/apache/kylin/rest/service/DiagnosisService.java",
      "--- Hunk 1 ---",
      "[Context before]",
      "87:     public String dumpProjectDiagnosisInfo(String project, File exportPath) throws IOException {",
      "88:         Message msg = MsgPicker.getMsg();",
      "89:         ProjectInstance projectInstance =",
      "90:                 ProjectManager.getInstance(KylinConfig.getInstanceFromEnv())",
      "92:         if (null == projectInstance) {",
      "93:             throw new BadRequestException(",
      "95:         }",
      "96:         aclEvaluate.checkProjectOperationPermission(projectInstance);",
      "98:         runDiagnosisCLI(args);",
      "99:         return getDiagnosisPackageName(exportPath);",
      "100:     }",
      "",
      "[Removed Lines]",
      "91:                         .getProject(ValidateUtil.convertStringToBeAlphanumericUnderscore(project));",
      "94:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), project));",
      "97:         String[] args = { project, exportPath.getAbsolutePath() };",
      "",
      "[Added Lines]",
      "89:         String projectName = ValidateUtil.convertStringToBeAlphanumericUnderscore(project);",
      "92:                         .getProject(projectName);",
      "95:                     String.format(Locale.ROOT, msg.getDIAG_PROJECT_NOT_FOUND(), projectName));",
      "98:         String[] args = { projectName, exportPath.getAbsolutePath() };",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "5187affe180fc88a8422cc193097d1ef9614ce89",
      "candidate_info": {
        "commit_hash": "5187affe180fc88a8422cc193097d1ef9614ce89",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/5187affe180fc88a8422cc193097d1ef9614ce89",
        "files": [
          "build/bin/check-hadoop-env.sh",
          "build/bin/find-hive-dependency.sh",
          "build/bin/kylin.sh",
          "build/bin/prepare_hadoop_dependency.sh",
          "build/bin/replace-jars-under-spark.sh",
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "core-common/src/test/java/org/apache/kylin/common/util/HiveCmdBuilderTest.java",
          "kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/ClassLoaderUtils.java",
          "kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/SparkClassLoader.java",
          "kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/TomcatClassLoader.java",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala",
          "source-hive/pom.xml",
          "source-hive/src/main/java/org/apache/kylin/source/hive/BeelineHiveClient.java",
          "source-hive/src/main/java/org/apache/kylin/source/hive/CLIHiveClient.java",
          "source-hive/src/main/java/org/apache/kylin/source/hive/HiveClientFactory.java",
          "source-hive/src/main/java/org/apache/kylin/source/hive/HiveTableMeta.java",
          "source-hive/src/main/java/org/apache/kylin/source/hive/HiveTableMetaBuilder.java",
          "source-hive/src/main/java/org/apache/kylin/source/hive/SparkHiveClient.java",
          "source-hive/src/test/java/org/apache/kylin/source/hive/BeelineHIveClientTest.java"
        ],
        "message": "KYLIN-5069 Refactor hive and hadoop dependency of kylin4",
        "before_after_code_files": [
          "build/bin/check-hadoop-env.sh||build/bin/check-hadoop-env.sh",
          "build/bin/find-hive-dependency.sh||build/bin/find-hive-dependency.sh",
          "build/bin/kylin.sh||build/bin/kylin.sh",
          "build/bin/prepare_hadoop_dependency.sh||build/bin/prepare_hadoop_dependency.sh",
          "build/bin/replace-jars-under-spark.sh||build/bin/replace-jars-under-spark.sh",
          "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "core-common/src/test/java/org/apache/kylin/common/util/HiveCmdBuilderTest.java||core-common/src/test/java/org/apache/kylin/common/util/HiveCmdBuilderTest.java",
          "kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/ClassLoaderUtils.java||kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/ClassLoaderUtils.java",
          "kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/SparkClassLoader.java||kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/SparkClassLoader.java",
          "kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/TomcatClassLoader.java||kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/TomcatClassLoader.java",
          "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala",
          "source-hive/src/main/java/org/apache/kylin/source/hive/BeelineHiveClient.java||source-hive/src/main/java/org/apache/kylin/source/hive/BeelineHiveClient.java",
          "source-hive/src/main/java/org/apache/kylin/source/hive/CLIHiveClient.java||source-hive/src/main/java/org/apache/kylin/source/hive/CLIHiveClient.java",
          "source-hive/src/main/java/org/apache/kylin/source/hive/HiveClientFactory.java||source-hive/src/main/java/org/apache/kylin/source/hive/HiveClientFactory.java",
          "source-hive/src/main/java/org/apache/kylin/source/hive/HiveTableMeta.java||source-hive/src/main/java/org/apache/kylin/source/hive/HiveTableMeta.java",
          "source-hive/src/main/java/org/apache/kylin/source/hive/HiveTableMetaBuilder.java||source-hive/src/main/java/org/apache/kylin/source/hive/HiveTableMetaBuilder.java",
          "source-hive/src/main/java/org/apache/kylin/source/hive/SparkHiveClient.java||source-hive/src/main/java/org/apache/kylin/source/hive/SparkHiveClient.java",
          "source-hive/src/test/java/org/apache/kylin/source/hive/BeelineHIveClientTest.java||source-hive/src/test/java/org/apache/kylin/source/hive/BeelineHIveClientTest.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ],
          "candidate": [
            "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java"
          ]
        }
      },
      "candidate_diff": {
        "build/bin/check-hadoop-env.sh||build/bin/check-hadoop-env.sh": [
          "File: build/bin/check-hadoop-env.sh -> build/bin/check-hadoop-env.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/bin/bash",
          "3: #",
          "4: # Licensed to the Apache Software Foundation (ASF) under one or more",
          "5: # contributor license agreements.  See the NOTICE file distributed with",
          "6: # this work for additional information regarding copyright ownership.",
          "7: # The ASF licenses this file to You under the Apache License, Version 2.0",
          "8: # (the \"License\"); you may not use this file except in compliance with",
          "9: # the License.  You may obtain a copy of the License at",
          "10: #",
          "11: #    http://www.apache.org/licenses/LICENSE-2.0",
          "12: #",
          "13: # Unless required by applicable law or agreed to in writing, software",
          "14: # distributed under the License is distributed on an \"AS IS\" BASIS,",
          "15: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
          "16: # See the License for the specific language governing permissions and",
          "17: # limitations under the License.",
          "18: #",
          "20: cdh_mapreduce_path=\"/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce\"",
          "21: hadoop_lib_path=\"/usr/lib/hadoop\"",
          "22: emr_spark_lib_path=\"/usr/lib/spark\"",
          "23: hdi3_flag_path=\"/usr/hdp/current/hdinsight-zookeeper\"",
          "25: cdh_version=`hadoop version | head -1 | awk -F '-' '{print $2}'`",
          "27: function is_cdh_6_x() {",
          "28:     if [ -d ${cdh_mapreduce_path}/../hadoop/ ]; then",
          "29:        hadoop_common_file=`find ${cdh_mapreduce_path}/../hadoop/ -maxdepth 1 -name \"hadoop-common-*.jar\" -not -name \"*test*\" | tail -1`",
          "30:        cdh_version=${hadoop_common_file##*-}",
          "32:        if [[ \"${cdh_version}\" == cdh6.* ]]; then",
          "33:           echo 1",
          "34:           return 1",
          "35:        fi",
          "36:     fi",
          "37:     echo 0",
          "38:     return 0",
          "39: }",
          "41: hdp_hadoop_path=$HDP_HADOOP_HOME",
          "42: if [[ -z ${hdp_hadoop_path} ]]",
          "43: then",
          "44:     if [[ -d \"/usr/hdp/current/hadoop-client\" ]]; then",
          "45:         hdp_hadoop_path=\"/usr/hdp/current/hadoop-client\"",
          "46:     fi",
          "47: fi",
          "49: if [ -d $hadoop_lib_path ]; then",
          "50:    # hadoop-common-3.2.1-amzn-0.jar",
          "51:    hadoop_common_file=$(find $hadoop_lib_path -maxdepth 1 -name \"hadoop-common-*.jar\" -not -name \"*test*\" | tail -1)",
          "52:    emr_version_1=${hadoop_common_file##*common-}",
          "53:    arrVersion=(${emr_version_1//-/ })",
          "54: fi",
          "56: function is_aws_emr() {",
          "57:   if [[ \"${arrVersion[1]}\" == *amzn* ]]; then",
          "58:      echo 1",
          "59:      return 1",
          "60:   fi",
          "61:   echo 0",
          "62:   return 0",
          "63: }",
          "65: function is_aws_emr_6() {",
          "66:   if [[ \"${arrVersion[0]}\" == 3.* && \"${arrVersion[1]}\" == *amzn* ]]; then",
          "67:       echo 1",
          "68:       return 1",
          "69:   fi",
          "70:   echo 0",
          "71:   return 0",
          "72: }",
          "74: function is_hdi_3_x() {",
          "75:   # get hdp_version",
          "76:   if [ -z \"${hdp_version}\" ]; then",
          "77:       hdp_version=`/bin/bash -x hadoop 2>&1 | sed -n \"s/\\(.*\\)export HDP_VERSION=\\(.*\\)/\\2/\"p`",
          "78:       verbose \"hdp_version is ${hdp_version}\"",
          "79:   fi",
          "81:   if [[ -d \"/usr/hdp/current/hdinsight-zookeeper\" && $hdp_version == \"2\"* ]];then",
          "82:      echo 1",
          "83:      return 1",
          "84:   fi",
          "86:   echo 0",
          "87:   return 0",
          "88: }",
          "",
          "---------------"
        ],
        "build/bin/find-hive-dependency.sh||build/bin/find-hive-dependency.sh": [
          "File: build/bin/find-hive-dependency.sh -> build/bin/find-hive-dependency.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "build/bin/kylin.sh||build/bin/kylin.sh": [
          "File: build/bin/kylin.sh -> build/bin/kylin.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "33: source ${dir}/set-java-home.sh",
          "35: function retrieveDependency() {",
          "37:     if [[ -z $reload_dependency && `ls -1 ${dir}/cached-* 2>/dev/null | wc -l` -eq 6 ]]",
          "38:     then",
          "39:         echo \"Using cached dependency...\"",
          "41:         #retrive $hbase_dependency",
          "42:         metadataUrl=`${dir}/get-properties.sh kylin.metadata.url`",
          "43:         if [[ \"${metadataUrl##*@}\" == \"hbase\" ]]",
          "",
          "[Removed Lines]",
          "36:     #retrive $hive_dependency and $hbase_dependency",
          "40:         source ${dir}/cached-hive-dependency.sh",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "45:             source ${dir}/cached-hbase-dependency.sh",
          "46:         fi",
          "47:         source ${dir}/cached-hadoop-conf-dir.sh",
          "49:         source ${dir}/cached-spark-dependency.sh",
          "51:     else",
          "53:         #retrive $hbase_dependency",
          "54:         metadataUrl=`${dir}/get-properties.sh kylin.metadata.url`",
          "55:         if [[ \"${metadataUrl##*@}\" == \"hbase\" ]]",
          "",
          "[Removed Lines]",
          "48:         # source ${dir}/cached-kafka-dependency.sh",
          "50:         # source ${dir}/cached-flink-dependency.sh",
          "52:         source ${dir}/find-hive-dependency.sh",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "57:             source ${dir}/find-hbase-dependency.sh",
          "58:         fi",
          "59:         source ${dir}/find-hadoop-conf-dir.sh",
          "61:         source ${dir}/find-spark-dependency.sh",
          "63:     fi",
          "68:     # get hdp_version",
          "69:     if [ -z \"${hdp_version}\" ]; then",
          "70:         hdp_version=`/bin/bash -x hadoop 2>&1 | sed -n \"s/\\(.*\\)export HDP_VERSION=\\(.*\\)/\\2/\"p`",
          "71:         verbose \"hdp_version is ${hdp_version}\"",
          "72:     fi",
          "127:     tomcat_root=${dir}/../tomcat",
          "128:     export tomcat_root",
          "",
          "[Removed Lines]",
          "60:         # source ${dir}/find-kafka-dependency.sh",
          "62:         # source ${dir}/find-flink-dependency.sh",
          "65:     # Replace jars for different hadoop dist",
          "66:     bash ${dir}/replace-jars-under-spark.sh",
          "74:     # Replace jars for HDI",
          "75:     KYLIN_SPARK_JARS_HOME=\"${KYLIN_HOME}/spark/jars\"",
          "76:     if [[ -d \"/usr/hdp/current/hdinsight-zookeeper\" && $hdp_version == \"2\"* ]]",
          "77:     then",
          "78:        echo \"The current Hadoop environment is HDI3, will replace some jars package for ${KYLIN_HOME}/spark/jars\"",
          "79:        if [[ -f ${KYLIN_HOME}/tomcat/webapps/kylin.war ]]",
          "80:        then",
          "81:           if [[ ! -d ${KYLIN_HOME}/tomcat/webapps/kylin ]]",
          "82:           then",
          "83:              mkdir ${KYLIN_HOME}/tomcat/webapps/kylin",
          "84:           fi",
          "85:           mv ${KYLIN_HOME}/tomcat/webapps/kylin.war ${KYLIN_HOME}/tomcat/webapps/kylin",
          "86:           cd ${KYLIN_HOME}/tomcat/webapps/kylin",
          "87:           jar -xf ${KYLIN_HOME}/tomcat/webapps/kylin/kylin.war",
          "88:           if [[ -f ${KYLIN_HOME}/tomcat/webapps/kylin/WEB-INF/lib/guava-14.0.jar ]]",
          "89:           then",
          "90:              echo \"Remove ${KYLIN_HOME}/tomcat/webapps/kylin/WEB-INF/lib/guava-14.0.jar to avoid version conflicts\"",
          "91:              rm -rf ${KYLIN_HOME}/tomcat/webapps/kylin/WEB-INF/lib/guava-14.0.jar",
          "92:              rm -rf ${KYLIN_HOME}/tomcat/webapps/kylin/kylin.war",
          "93:              cd ${KYLIN_HOME}/",
          "94:           fi",
          "95:        fi",
          "97:        if [[ -d \"${KYLIN_SPARK_JARS_HOME}\" ]]",
          "98:        then",
          "99:           if [[ -f ${KYLIN_HOME}/hdi3_spark_jars_flag ]]",
          "100:           then",
          "101:           echo \"Required jars have been added to ${KYLIN_HOME}/spark/jars, skip this step.\"",
          "102:           else",
          "103:              rm -rf ${KYLIN_HOME}/spark/jars/hadoop-*",
          "104:              cp /usr/hdp/current/spark2-client/jars/hadoop-* $KYLIN_SPARK_JARS_HOME",
          "105:              cp /usr/hdp/current/spark2-client/jars/azure-* $KYLIN_SPARK_JARS_HOME",
          "106:              cp /usr/hdp/current/hadoop-client/lib/microsoft-log4j-etwappender-1.0.jar $KYLIN_SPARK_JARS_HOME",
          "107:              cp /usr/hdp/current/hadoop-client/lib/hadoop-lzo-0.6.0.${hdp_version}.jar $KYLIN_SPARK_JARS_HOME",
          "109:              rm -rf $KYLIN_HOME/spark/jars/guava-14.0.1.jar",
          "110:              cp /usr/hdp/current/spark2-client/jars/guava-24.1.1-jre.jar $KYLIN_SPARK_JARS_HOME",
          "112:              echo \"Upload spark jars to HDFS\"",
          "113:              hdfs dfs -test -d /spark2_jars",
          "114:              if [ $? -eq 1 ]",
          "115:              then",
          "116:                 hdfs dfs -mkdir /spark2_jars",
          "117:              fi",
          "118:              hdfs dfs -put $KYLIN_SPARK_JARS_HOME/* /spark2_jars",
          "120:              touch ${KYLIN_HOME}/hdi3_spark_jars_flag",
          "121:           fi",
          "122:        else",
          "123:           echo \"${KYLIN_HOME}/spark/jars dose not exist. You can run ${KYLIN_HOME}/download-spark.sh to download spark.\"",
          "124:        fi",
          "125:     fi",
          "",
          "[Added Lines]",
          "64:     source ${KYLIN_HOME}/bin/prepare_hadoop_dependency.sh",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "141:         spring_profile=\"${spring_profile},${additional_security_profiles}\"",
          "142:     fi",
          "153:     # compose KYLIN_TOMCAT_CLASSPATH",
          "154:     tomcat_classpath=${tomcat_root}/bin/bootstrap.jar:${tomcat_root}/bin/tomcat-juli.jar:${tomcat_root}/lib/*",
          "157:     # compose KYLIN_TOOL_CLASSPATH",
          "160:     # compose kylin_common_opts",
          "164:     -Dkylin.server.host-address=${KYLIN_REST_ADDRESS} \\",
          "165:     -Dspring.profiles.active=${spring_profile} \\",
          "166:     -Dhdp.version=${hdp_version}\"",
          "",
          "[Removed Lines]",
          "144:     # compose hadoop_dependencies",
          "145:     hadoop_dependencies=${hadoop_dependencies}:`hadoop classpath`",
          "146:     if [ -n \"${hive_dependency}\" ]; then",
          "147:         hadoop_dependencies=${hive_dependency}:${hadoop_dependencies}",
          "148:     fi",
          "149:     if [ -n \"${kafka_dependency}\" ]; then",
          "150:         hadoop_dependencies=${hadoop_dependencies}:${kafka_dependency}",
          "151:     fi",
          "155:     export KYLIN_TOMCAT_CLASSPATH=${tomcat_classpath}:${KYLIN_HOME}/conf:${KYLIN_HOME}/lib/*:${KYLIN_HOME}/ext/*:${hadoop_dependencies}:${flink_dependency}",
          "158:     export KYLIN_TOOL_CLASSPATH=${KYLIN_HOME}/conf:${KYLIN_HOME}/tool/*:${KYLIN_HOME}/ext/*:${hadoop_dependencies}",
          "161:     kylin_common_opts=\"-Dkylin.hive.dependency=${hive_dependency} \\",
          "162:     -Dkylin.kafka.dependency=${kafka_dependency} \\",
          "163:     -Dkylin.hadoop.conf.dir=${kylin_hadoop_conf_dir} \\",
          "",
          "[Added Lines]",
          "85:     export KYLIN_TOMCAT_CLASSPATH=${tomcat_classpath}:${KYLIN_HOME}/conf:${KYLIN_HOME}/hadoop_conf:${KYLIN_HOME}/lib/*:${KYLIN_HOME}/ext/*:${SPARK_HOME}/jars/*",
          "88:     export KYLIN_TOOL_CLASSPATH=${KYLIN_HOME}/conf:${KYLIN_HOME}/tool/*:${KYLIN_HOME}/ext/*:${SPARK_HOME}/jars/*",
          "91:     kylin_common_opts=\"-Dkylin.hadoop.conf.dir=${kylin_hadoop_conf_dir} \\",
          "",
          "---------------"
        ],
        "build/bin/prepare_hadoop_dependency.sh||build/bin/prepare_hadoop_dependency.sh": [
          "File: build/bin/prepare_hadoop_dependency.sh -> build/bin/prepare_hadoop_dependency.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/bin/bash",
          "3: #",
          "4: # Licensed to the Apache Software Foundation (ASF) under one or more",
          "5: # contributor license agreements.  See the NOTICE file distributed with",
          "6: # this work for additional information regarding copyright ownership.",
          "7: # The ASF licenses this file to You under the Apache License, Version 2.0",
          "8: # (the \"License\"); you may not use this file except in compliance with",
          "9: # the License.  You may obtain a copy of the License at",
          "10: #",
          "11: #    http://www.apache.org/licenses/LICENSE-2.0",
          "12: #",
          "13: # Unless required by applicable law or agreed to in writing, software",
          "14: # distributed under the License is distributed on an \"AS IS\" BASIS,",
          "15: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
          "16: # See the License for the specific language governing permissions and",
          "17: # limitations under the License.",
          "18: #",
          "20: source ${KYLIN_HOME}/bin/check-hadoop-env.sh",
          "23: BYPASS=${SPARK_HOME}/jars/replace-jars-bypass",
          "25: if [[ -f ${BYPASS} ]]",
          "26: then",
          "27:     return",
          "28: fi",
          "30: if [ ! -d \"$KYLIN_HOME/spark\" ]; then",
          "31:   echo \"Skip spark which not owned by kylin. SPARK_HOME is $SPARK_HOME and KYLIN_HOME is $KYLIN_HOME .\"",
          "32:   exit 0",
          "33: fi",
          "35: echo \"Start replace hadoop jars under ${KYLIN_HOME}/spark/jars.\"",
          "37: hadoop_lib=${KYLIN_HOME}/spark/jars",
          "39: common_jars=",
          "40: hdfs_jars=",
          "41: mr_jars=",
          "42: yarn_jars=",
          "43: other_jars=",
          "45: function cdh_replace_jars() {",
          "46:     common_jars=$(find $cdh_mapreduce_path/../hadoop -maxdepth 2 \\",
          "47:     -name \"hadoop-annotations-*.jar\" -not -name \"*test*\" \\",
          "48:     -o -name \"hadoop-auth-*.jar\" -not -name \"*test*\" \\",
          "49:     -o -name \"hadoop-common-*.jar\" -not -name \"*test*\")",
          "51:     hdfs_jars=$(find $cdh_mapreduce_path/../hadoop-hdfs -maxdepth 1 -name \"hadoop-hdfs-*\" -not -name \"*test*\" -not -name \"*nfs*\")",
          "53:     mr_jars=$(find $cdh_mapreduce_path -maxdepth 1 \\",
          "54:     -name \"hadoop-mapreduce-client-app-*.jar\" -not -name \"*test*\"  \\",
          "55:     -o -name \"hadoop-mapreduce-client-common-*.jar\" -not -name \"*test*\" \\",
          "56:     -o -name \"hadoop-mapreduce-client-jobclient-*.jar\" -not -name \"*test*\" \\",
          "57:     -o -name \"hadoop-mapreduce-client-shuffle-*.jar\" -not -name \"*test*\" \\",
          "58:     -o -name \"hadoop-mapreduce-client-core-*.jar\" -not -name \"*test*\")",
          "60:     yarn_jars=$(find $cdh_mapreduce_path/../hadoop-yarn -maxdepth 1 \\",
          "61:     -name \"hadoop-yarn-api-*.jar\" -not -name \"*test*\"  \\",
          "62:     -o -name \"hadoop-yarn-client-*.jar\" -not -name \"*test*\" \\",
          "63:     -o -name \"hadoop-yarn-common-*.jar\" -not -name \"*test*\" \\",
          "64:     -o -name \"hadoop-yarn-server-common-*.jar\" -not -name \"*test*\" \\",
          "65:     -o -name \"hadoop-yarn-server-web-proxy-*.jar\" -not -name \"*test*\")",
          "67:     other_jars=$(find $cdh_mapreduce_path/../../jars -maxdepth 1 -name \"htrace-core4*\" || find $cdh_mapreduce_path/../hadoop -maxdepth 2 -name \"htrace-core4*\")",
          "69:     if [[ $(is_cdh_6_x) == 1 ]]; then",
          "70:         cdh6_jars=$(find ${cdh_mapreduce_path}/../../jars -maxdepth 1 \\",
          "71:         -name \"woodstox-core-*.jar\" -o -name \"stax2-*.jar\" -o -name \"commons-configuration2-*.jar\" -o -name \"re2j-*.jar\" )",
          "72:     fi",
          "73: }",
          "75: function emr_replace_jars() {",
          "76:     common_jars=$(find ${emr_spark_lib_path}/jars/ -maxdepth 1 \\",
          "77:     -name \"hadoop-*.jar\" -not -name \"*test*\" \\",
          "78:     -o -name \"htrace-core4*\" \\",
          "79:     -o -name \"emr-spark-goodies*\")",
          "81:     other_jars=$(find ${hadoop_lib_path}/lib/ -maxdepth 1 \\",
          "82:     -name \"woodstox-core-*.jar\" \\",
          "83:     -o -name \"stax2-api-3*.jar\")",
          "85:     lzo_jars=$(find ${hadoop_lib_path}/../hadoop-lzo/lib/ -maxdepth 1 \\",
          "86:     -name \"hadoop-lzo-*.jar\" )",
          "88:     if [[ $(is_aws_emr_6) == 1 ]]; then",
          "89:         emr6_jars=$(find ${emr_spark_lib_path}/jars/ -maxdepth 1 \\",
          "90:         -name \"re2j-*.jar\" -not -name \"*test*\" \\",
          "91:         -o -name \"commons-configuration2-*\" )",
          "92:     fi",
          "93: }",
          "95: function hdi_replace_jars() {",
          "96:     common_jars=$(find ${hdi3_flag_path}/../spark2-client/ -maxdepth 2 \\",
          "97:     -name \"hadoop-*.jar\" -not -name \"*test*\" \\",
          "98:     -o -name \"azure-*.jar\" -not -name \"*test*\" \\",
          "99:     -o -name \"guava-*.jar\")",
          "101:     other_jars=$(find ${hdi3_flag_path}/../hadoop-client/ -maxdepth 2 \\",
          "102:     -name \"microsoft-log4j-etwappender-*.jar\")",
          "104:     lzo_jars=$(find ${hdi3_flag_path}/../hadoop-client/ -maxdepth 2 \\",
          "105:     -name \"hadoop-lzo-*.jar\" )",
          "106: }",
          "108: if [ -d \"$cdh_mapreduce_path\" ]",
          "109: then",
          "110:     cdh_replace_jars",
          "111: elif [[ $(is_aws_emr) == 1 ]]",
          "112: then",
          "113:     emr_replace_jars",
          "114: elif [[ $(is_hdi_3_x) == 1 ]]",
          "115: then",
          "116:     hdi_replace_jars",
          "117: else",
          "118:     touch \"${BYPASS}\"",
          "119: fi",
          "121: jar_list=\"${common_jars} ${hdfs_jars} ${mr_jars} ${yarn_jars} ${other_jars} ${cdh6_jars} ${emr6_jars} ${lzo_jars}\"",
          "123: echo \"Find platform specific jars:${jar_list}, will replace with these jars under ${SPARK_HOME}/jars.\"",
          "125: if [[ $(is_aws_emr_6) == 1 ]]; then",
          "126:   find ${SPARK_HOME}/jars -name \"hive-exec-*.jar\" -exec rm -f {} \\;",
          "127:   hive_jars=$(find ${emr_spark_lib_path}/jars/ -maxdepth 1 -name \"hive-exec-*.jar\")",
          "128:   cp ${hive_jars} ${SPARK_HOME}/jars",
          "129:   configuration_jars=$(find ${emr_spark_lib_path}/../ -name \"commons-configuration-1.10*.jar\")",
          "130:   cp ${configuration_jars} ${KYLIN_HOME}/lib",
          "131: fi",
          "133: if [ $(is_cdh_6_x) == 1 ]; then",
          "134:    if [ -d \"${KYLIN_HOME}/bin/hadoop3_jars/cdh6\" ]; then",
          "135:      find ${SPARK_HOME}/jars -name \"hive-exec-*.jar\" -exec rm -f {} \\;",
          "136:      echo \"Copy jars from ${KYLIN_HOME}/bin/hadoop3_jars/cdh6\"",
          "137:      cp ${KYLIN_HOME}/hadoop3_jars/cdh6/*.jar ${SPARK_HOME}/jars",
          "138:    fi",
          "139: fi",
          "141: if [ ! -f ${BYPASS} ]; then",
          "142:    find ${SPARK_HOME}/jars -name \"htrace-core-*\" -exec rm -rf {} \\;",
          "143:    find ${SPARK_HOME}/jars -name \"hadoop-*.jar\" -exec rm -f {} \\;",
          "144: fi",
          "146: for jar_file in ${jar_list}",
          "147: do",
          "148:     `cp ${jar_file} ${SPARK_HOME}/jars`",
          "149: done",
          "151: if [[ (${is_emr} == 1) || ($(is_cdh_6_x) == 1)]]; then",
          "152:    log4j_jars=$(find ${SPARK_HOME}/jars/ -maxdepth 2 -name \"slf4j-*.jar\")",
          "153:    cp ${log4j_jars} ${KYLIN_HOME}/ext",
          "154: fi",
          "156: if [ $(is_hdi_3_x) == 1 ]; then",
          "157:    if [[ -f ${KYLIN_HOME}/tomcat/webapps/kylin.war ]]; then",
          "158:           if [[ ! -d ${KYLIN_HOME}/tomcat/webapps/kylin ]]",
          "159:           then",
          "160:              mkdir ${KYLIN_HOME}/tomcat/webapps/kylin",
          "161:           fi",
          "162:           mv ${KYLIN_HOME}/tomcat/webapps/kylin.war ${KYLIN_HOME}/tomcat/webapps/kylin",
          "163:           cd ${KYLIN_HOME}/tomcat/webapps/kylin",
          "164:           jar -xf ${KYLIN_HOME}/tomcat/webapps/kylin/kylin.war",
          "165:           if [[ -f ${KYLIN_HOME}/tomcat/webapps/kylin/WEB-INF/lib/guava-14.0.jar ]]",
          "166:           then",
          "167:              echo \"Remove ${KYLIN_HOME}/tomcat/webapps/kylin/WEB-INF/lib/guava-14.0.jar to avoid version conflicts\"",
          "168:              rm -rf ${KYLIN_HOME}/tomcat/webapps/kylin/WEB-INF/lib/guava-14.0.jar",
          "169:              rm -rf ${KYLIN_HOME}/tomcat/webapps/kylin/kylin.war",
          "170:              cd ${KYLIN_HOME}/",
          "171:           fi",
          "172:    fi",
          "173:    find ${SPARK_HOME}/jars -name \"guava-14*.jar\" -exec rm -f {} \\;",
          "174:    echo \"Upload spark jars to HDFS\"",
          "175:    hdfs dfs -test -d /spark2_jars",
          "176:    if [ $? -eq 1 ]; then",
          "177:       hdfs dfs -mkdir /spark2_jars",
          "178:    fi",
          "179:    hdfs dfs -put ${SPARK_HOME}/jars/* /spark2_jars",
          "180: fi",
          "182: # Remove all spaces",
          "183: jar_list=${jar_list// /}",
          "185: if [[ (-z \"${jar_list}\") && (! -f ${BYPASS}) ]]",
          "186: then",
          "187:     echo \"Please confirm that the corresponding hadoop jars have been replaced. The automatic replacement program cannot be executed correctly.\"",
          "188: else",
          "189:     touch \"${BYPASS}\"",
          "190: fi",
          "192: echo \"Done hadoop jars replacement under ${SPARK_HOME}/jars.\"",
          "",
          "---------------"
        ],
        "build/bin/replace-jars-under-spark.sh||build/bin/replace-jars-under-spark.sh": [
          "File: build/bin/replace-jars-under-spark.sh -> build/bin/replace-jars-under-spark.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java||core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java -> core-common/src/main/java/org/apache/kylin/common/KylinConfigBase.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "1129:     }",
          "1131:     public String getHiveClientMode() {",
          "1133:     }",
          "1135:     public String getHiveBeelineShell() {",
          "",
          "[Removed Lines]",
          "1132:         return getOptional(\"kylin.source.hive.client\", \"cli\");",
          "",
          "[Added Lines]",
          "1132:         return getOptional(\"kylin.source.hive.client\", \"spark_catalog\");",
          "",
          "---------------"
        ],
        "core-common/src/test/java/org/apache/kylin/common/util/HiveCmdBuilderTest.java||core-common/src/test/java/org/apache/kylin/common/util/HiveCmdBuilderTest.java": [
          "File: core-common/src/test/java/org/apache/kylin/common/util/HiveCmdBuilderTest.java -> core-common/src/test/java/org/apache/kylin/common/util/HiveCmdBuilderTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/ClassLoaderUtils.java||kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/ClassLoaderUtils.java": [
          "File: kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/ClassLoaderUtils.java -> kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/ClassLoaderUtils.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "24: import org.slf4j.LoggerFactory;",
          "26: public final class ClassLoaderUtils {",
          "28:     static URLClassLoader originClassLoader = null;",
          "29:     private static Logger logger = LoggerFactory.getLogger(ClassLoaderUtils.class);",
          "",
          "[Removed Lines]",
          "27:     static URLClassLoader sparkClassLoader = null;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "40:         return null;",
          "41:     }",
          "62:     public static ClassLoader getOriginClassLoader() {",
          "63:         if (originClassLoader == null) {",
          "64:             logger.error(\"originClassLoader not init\");",
          "",
          "[Removed Lines]",
          "43:     public static ClassLoader getSparkClassLoader() {",
          "44:         if (sparkClassLoader == null) {",
          "45:             return Thread.currentThread().getContextClassLoader();",
          "46:         } else {",
          "47:             return sparkClassLoader;",
          "48:         }",
          "49:     }",
          "51:     public static void setSparkClassLoader(URLClassLoader classLoader) {",
          "52:         if (sparkClassLoader != null) {",
          "53:             logger.error(\"sparkClassLoader already initialized\");",
          "54:         }",
          "55:         logger.info(\"set sparkClassLoader :\" + classLoader);",
          "56:         if (System.getenv(\"DEBUG_SPARK_CLASSLOADER\") != null) {",
          "57:             return;",
          "58:         }",
          "59:         sparkClassLoader = classLoader;",
          "60:     }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/SparkClassLoader.java||kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/SparkClassLoader.java": [
          "File: kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/SparkClassLoader.java -> kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/SparkClassLoader.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/TomcatClassLoader.java||kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/TomcatClassLoader.java": [
          "File: kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/TomcatClassLoader.java -> kylin-spark-project/kylin-spark-classloader/src/main/java/org/apache/kylin/spark/classloader/TomcatClassLoader.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "58:     }",
          "60:     private static Logger logger = LoggerFactory.getLogger(TomcatClassLoader.class);",
          "",
          "[Removed Lines]",
          "61:     private SparkClassLoader sparkClassLoader;",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "69:     public TomcatClassLoader(ClassLoader parent) throws IOException {",
          "70:         super(parent);",
          "73:         ClassLoaderUtils.setOriginClassLoader(this);",
          "74:         init();",
          "75:     }",
          "",
          "[Removed Lines]",
          "71:         sparkClassLoader = new SparkClassLoader(this);",
          "72:         ClassLoaderUtils.setSparkClassLoader(sparkClassLoader);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "104:         if (name.startsWith(\"org.apache.kylin.spark.classloader\")) {",
          "105:             return parent.loadClass(name);",
          "106:         }",
          "110:         if (isParentCLPrecedent(name)) {",
          "111:             logger.debug(\"Skipping exempt class \" + name + \" - delegating directly to parent\");",
          "112:             return parent.loadClass(name);",
          "",
          "[Removed Lines]",
          "107:         if (sparkClassLoader.classNeedPreempt(name)) {",
          "108:             return sparkClassLoader.loadClass(name);",
          "109:         }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "117:     @Override",
          "118:     public InputStream getResourceAsStream(String name) {",
          "122:         return super.getResourceAsStream(name);",
          "124:     }",
          "",
          "[Removed Lines]",
          "119:         if (sparkClassLoader.fileNeedPreempt(name)) {",
          "120:             return sparkClassLoader.getResourceAsStream(name);",
          "121:         }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala||kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala": [
          "File: kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala -> kylin-spark-project/kylin-spark-query/src/main/scala/org/apache/spark/sql/SparderContext.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "54:   @volatile",
          "55:   var master_app_url: String = _",
          "58:     if (spark == null || spark.sparkContext.isStopped) {",
          "59:       logInfo(\"Init spark.\")",
          "60:       initSpark()",
          "",
          "[Removed Lines]",
          "57:   def getOriginalSparkSession: SparkSession = withClassLoad {",
          "",
          "[Added Lines]",
          "57:   def getOriginalSparkSession: SparkSession = {",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "92:     spark != null && !spark.sparkContext.isStopped",
          "93:   }",
          "96:     this.synchronized {",
          "97:       if (spark != null && !spark.sparkContext.isStopped) {",
          "98:         Utils.tryWithSafeFinally {",
          "",
          "[Removed Lines]",
          "95:   def restartSpark(): Unit = withClassLoad {",
          "",
          "[Added Lines]",
          "95:   def restartSpark(): Unit = {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "107:     }",
          "108:   }",
          "111:     this.synchronized {",
          "112:       if (spark != null && !spark.sparkContext.isStopped) {",
          "113:         Utils.tryWithSafeFinally {",
          "",
          "[Removed Lines]",
          "110:   def stopSpark(): Unit = withClassLoad {",
          "",
          "[Added Lines]",
          "110:   def stopSpark(): Unit = {",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "119:     }",
          "120:   }",
          "123:     getOriginalSparkSession",
          "124:   }",
          "",
          "[Removed Lines]",
          "122:   def init(): Unit = withClassLoad {",
          "",
          "[Added Lines]",
          "122:   def init(): Unit = {",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "127:     getSparkSession.sparkContext.conf.get(key)",
          "128:   }",
          "131:     this.synchronized {",
          "132:       if (initializingThread == null && (spark == null || spark.sparkContext.isStopped)) {",
          "133:         initializingThread = new Thread(new Runnable {",
          "",
          "[Removed Lines]",
          "130:   def initSpark(): Unit = withClassLoad {",
          "",
          "[Added Lines]",
          "130:   def initSpark(): Unit =",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "220:       ShardFileStatusCache.getFileStatusCache(getOriginalSparkSession)",
          "221:     }",
          "224:   def registerListener(sc: SparkContext): Unit = {",
          "225:     val sparkListener = new SparkListener {",
          "",
          "[Removed Lines]",
          "222:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "250:     logicalPlan",
          "251:   }",
          "268:   val _isAsyncQuery = new ThreadLocal[JBoolean]",
          "269:   val _separator = new ThreadLocal[JString]",
          "270:   val _df = new ThreadLocal[Dataset[Row]]",
          "",
          "[Removed Lines]",
          "260:   def withClassLoad[T](body: => T): T = {",
          "262:     Thread.currentThread().setContextClassLoader(ClassLoaderUtils.getSparkClassLoader)",
          "263:     val t = body",
          "265:     t",
          "266:   }",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "source-hive/src/main/java/org/apache/kylin/source/hive/BeelineHiveClient.java||source-hive/src/main/java/org/apache/kylin/source/hive/BeelineHiveClient.java": [
          "File: source-hive/src/main/java/org/apache/kylin/source/hive/BeelineHiveClient.java -> source-hive/src/main/java/org/apache/kylin/source/hive/BeelineHiveClient.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "source-hive/src/main/java/org/apache/kylin/source/hive/CLIHiveClient.java||source-hive/src/main/java/org/apache/kylin/source/hive/CLIHiveClient.java": [
          "File: source-hive/src/main/java/org/apache/kylin/source/hive/CLIHiveClient.java -> source-hive/src/main/java/org/apache/kylin/source/hive/CLIHiveClient.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "source-hive/src/main/java/org/apache/kylin/source/hive/HiveClientFactory.java||source-hive/src/main/java/org/apache/kylin/source/hive/HiveClientFactory.java": [
          "File: source-hive/src/main/java/org/apache/kylin/source/hive/HiveClientFactory.java -> source-hive/src/main/java/org/apache/kylin/source/hive/HiveClientFactory.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: public class HiveClientFactory {",
          "25:     public static IHiveClient getHiveClient() {",
          "30:         } else {",
          "31:             throw new RuntimeException(\"cannot recognize hive client mode\");",
          "32:         }",
          "",
          "[Removed Lines]",
          "26:         if (\"cli\".equals(KylinConfig.getInstanceFromEnv().getHiveClientMode())) {",
          "27:             return new CLIHiveClient();",
          "28:         } else if (\"beeline\".equals(KylinConfig.getInstanceFromEnv().getHiveClientMode())) {",
          "29:             return new BeelineHiveClient(KylinConfig.getInstanceFromEnv().getHiveBeelineParams());",
          "",
          "[Added Lines]",
          "26:         if (\"spark_catalog\".equals(KylinConfig.getInstanceFromEnv().getHiveClientMode())) {",
          "27:             return new SparkHiveClient();",
          "",
          "---------------"
        ],
        "source-hive/src/main/java/org/apache/kylin/source/hive/HiveTableMeta.java||source-hive/src/main/java/org/apache/kylin/source/hive/HiveTableMeta.java": [
          "File: source-hive/src/main/java/org/apache/kylin/source/hive/HiveTableMeta.java -> source-hive/src/main/java/org/apache/kylin/source/hive/HiveTableMeta.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "45:     String owner;",
          "46:     String tableType;",
          "47:     int skipHeaderLineCount;",
          "49:     long fileSize;",
          "50:     long fileNum;",
          "51:     boolean isNative;",
          "52:     List<HiveTableColumnMeta> allColumns;",
          "53:     List<HiveTableColumnMeta> partitionColumns;",
          "56:         this.tableName = tableName;",
          "57:         this.sdLocation = sdLocation;",
          "58:         this.sdInputFormat = sdInputFormat;",
          "",
          "[Removed Lines]",
          "48:     int lastAccessTime;",
          "55:     public HiveTableMeta(String tableName, String sdLocation, String sdInputFormat, String sdOutputFormat, String owner, String tableType, int lastAccessTime, long fileSize, long fileNum, int skipHeaderLineCount, boolean isNative, List<HiveTableColumnMeta> allColumns, List<HiveTableColumnMeta> partitionColumns) {",
          "",
          "[Added Lines]",
          "48:     long lastAccessTime;",
          "55:     public HiveTableMeta(String tableName, String sdLocation, String sdInputFormat, String sdOutputFormat, String owner, String tableType, long lastAccessTime, long fileSize, long fileNum, int skipHeaderLineCount, boolean isNative, List<HiveTableColumnMeta> allColumns, List<HiveTableColumnMeta> partitionColumns) {",
          "",
          "---------------"
        ],
        "source-hive/src/main/java/org/apache/kylin/source/hive/HiveTableMetaBuilder.java||source-hive/src/main/java/org/apache/kylin/source/hive/HiveTableMetaBuilder.java": [
          "File: source-hive/src/main/java/org/apache/kylin/source/hive/HiveTableMetaBuilder.java -> source-hive/src/main/java/org/apache/kylin/source/hive/HiveTableMetaBuilder.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "29:     private String sdOutputFormat;",
          "30:     private String owner;",
          "31:     private String tableType;",
          "33:     private long fileSize;",
          "34:     private long fileNum;",
          "35:     private int skipHeaderLineCount;",
          "",
          "[Removed Lines]",
          "32:     private int lastAccessTime;",
          "",
          "[Added Lines]",
          "32:     private long lastAccessTime;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "67:         return this;",
          "68:     }",
          "71:         this.lastAccessTime = lastAccessTime;",
          "72:         return this;",
          "73:     }",
          "",
          "[Removed Lines]",
          "70:     public HiveTableMetaBuilder setLastAccessTime(int lastAccessTime) {",
          "",
          "[Added Lines]",
          "70:     public HiveTableMetaBuilder setLastAccessTime(long lastAccessTime) {",
          "",
          "---------------"
        ],
        "source-hive/src/main/java/org/apache/kylin/source/hive/SparkHiveClient.java||source-hive/src/main/java/org/apache/kylin/source/hive/SparkHiveClient.java": [
          "File: source-hive/src/main/java/org/apache/kylin/source/hive/SparkHiveClient.java -> source-hive/src/main/java/org/apache/kylin/source/hive/SparkHiveClient.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: package org.apache.kylin.source.hive;",
          "22: import org.apache.kylin.shaded.com.google.common.collect.Lists;",
          "23: import org.apache.spark.sql.SparderContext;",
          "24: import org.apache.spark.sql.SparkSession;",
          "25: import org.apache.spark.sql.catalyst.TableIdentifier;",
          "26: import org.apache.spark.sql.catalyst.catalog.CatalogTable;",
          "27: import org.apache.spark.sql.catalyst.catalog.CatalogTableType;",
          "28: import org.apache.spark.sql.catalyst.catalog.SessionCatalog;",
          "29: import org.apache.spark.sql.types.Metadata;",
          "30: import org.apache.spark.sql.types.StructField;",
          "31: import scala.Option;",
          "32: import scala.collection.Iterator;",
          "34: import java.io.IOException;",
          "35: import java.util.List;",
          "36: import java.util.stream.Collectors;",
          "38: public class SparkHiveClient implements IHiveClient {",
          "40:     private static final String CHAR_VARCHAR_TYPE_STRING = \"__CHAR_VARCHAR_TYPE_STRING\";",
          "41:     private static final String HIVE_COMMENT = \"comment\";",
          "42:     private static final String HIVE_TABLE_ROWS = \"numRows\";",
          "43:     private static final String TABLE_TOTAL_SIZE = \"totalSize\";",
          "44:     private static final String TABLE_FILE_NUM = \"numFiles\";",
          "46:     protected SparkSession ss;",
          "47:     protected SessionCatalog catalog;",
          "49:     public SparkHiveClient() {",
          "50:         ss = SparderContext.getOriginalSparkSession();",
          "51:         catalog = ss.sessionState().catalog();",
          "52:     }",
          "55:     @Override",
          "56:     public void executeHQL(String hql) throws IOException {",
          "57:         throw new UnsupportedOperationException();",
          "58:     }",
          "60:     @Override",
          "61:     public void executeHQL(String[] hqls) throws IOException {",
          "62:         throw new UnsupportedOperationException();",
          "63:     }",
          "65:     @Override",
          "66:     public HiveTableMeta getHiveTableMeta(String database, String tableName) throws Exception {",
          "67:         HiveTableMetaBuilder builder = new HiveTableMetaBuilder();",
          "68:         CatalogTable catalogTable = catalog",
          "69:                 .getTempViewOrPermanentTableMetadata(new TableIdentifier(tableName, Option.apply(database)));",
          "70:         scala.collection.immutable.List<StructField> structFieldList = catalogTable.schema().toList();",
          "71:         Iterator<StructField> structFieldIterator = structFieldList.iterator();",
          "73:         List<HiveTableMeta.HiveTableColumnMeta> allColumns = Lists.newArrayList();",
          "74:         List<HiveTableMeta.HiveTableColumnMeta> partitionColumns = Lists.newArrayList();",
          "75:         while (structFieldIterator.hasNext()) {",
          "76:             StructField structField = structFieldIterator.next();",
          "77:             String name = structField.name();",
          "78:             String hiveDataType = structField.dataType().simpleString();",
          "79:             Metadata metadata = structField.metadata();",
          "80:             String description = metadata.contains(HIVE_COMMENT) ? metadata.getString(HIVE_COMMENT) : \"\";",
          "81:             String datatype = metadata.contains(CHAR_VARCHAR_TYPE_STRING) ? metadata.getString(CHAR_VARCHAR_TYPE_STRING) : hiveDataType;",
          "83:             allColumns.add(new HiveTableMeta.HiveTableColumnMeta(name, datatype, description));",
          "84:             if (catalogTable.partitionColumnNames().contains(name)) {",
          "85:                 partitionColumns.add(new HiveTableMeta.HiveTableColumnMeta(name, datatype, description));",
          "86:             }",
          "87:         }",
          "89:         builder.setAllColumns(allColumns);",
          "90:         builder.setPartitionColumns(partitionColumns);",
          "91:         builder.setSdLocation(catalogTable.location().getPath());",
          "92:         builder.setFileSize(Long.parseLong(catalogTable.ignoredProperties().apply(TABLE_TOTAL_SIZE)));",
          "93:         builder.setFileNum(Long.parseLong(catalogTable.ignoredProperties().apply(TABLE_FILE_NUM)));",
          "94:         builder.setIsNative(catalogTable.tableType().equals(CatalogTableType.MANAGED()));",
          "95:         builder.setTableName(tableName);",
          "96:         builder.setSdInputFormat(catalogTable.storage().inputFormat().toString());",
          "97:         builder.setSdOutputFormat(catalogTable.storage().outputFormat().toString());",
          "98:         builder.setOwner(catalogTable.owner());",
          "99:         builder.setLastAccessTime(catalogTable.lastAccessTime());",
          "100:         builder.setTableType(catalogTable.tableType().name());",
          "102:         return builder.createHiveTableMeta();",
          "103:     }",
          "105:     @Override",
          "106:     public List<String> getHiveDbNames() throws Exception {",
          "107:         return scala.collection.JavaConversions.seqAsJavaList(catalog.listDatabases());",
          "108:     }",
          "110:     @Override",
          "111:     public List<String> getHiveTableNames(String database) throws Exception {",
          "112:         List<TableIdentifier> tableIdentifiers = scala.collection.JavaConversions.seqAsJavaList(catalog.listTables(database));",
          "113:         List<String> tableNames = tableIdentifiers.stream().map(table -> table.table()).collect(Collectors.toList());",
          "114:         return tableNames;",
          "115:     }",
          "117:     @Override",
          "118:     public long getHiveTableRows(String database, String tableName) throws Exception {",
          "119:         return Long.parseLong(catalog.getTempViewOrPermanentTableMetadata(new TableIdentifier(tableName, Option.apply(database)))",
          "120:                 .ignoredProperties().apply(HIVE_TABLE_ROWS));",
          "121:     }",
          "124:     This method was originally used for pushdown query.",
          "125:     The method of pushdown query in kylin4 is PushDownRunnerSparkImpl.executeQuery, so getHiveResult is not implemented here.",
          "127:     @Override",
          "128:     public List<Object[]> getHiveResult(String sql) throws Exception {",
          "129:         return null;",
          "130:     }",
          "131: }",
          "",
          "---------------"
        ],
        "source-hive/src/test/java/org/apache/kylin/source/hive/BeelineHIveClientTest.java||source-hive/src/test/java/org/apache/kylin/source/hive/BeelineHIveClientTest.java": [
          "File: source-hive/src/test/java/org/apache/kylin/source/hive/BeelineHIveClientTest.java -> source-hive/src/test/java/org/apache/kylin/source/hive/BeelineHIveClientTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "a8cecf3d6786ceabb8138d4b54fa2edfc3526ebe",
      "candidate_info": {
        "commit_hash": "a8cecf3d6786ceabb8138d4b54fa2edfc3526ebe",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/a8cecf3d6786ceabb8138d4b54fa2edfc3526ebe",
        "files": [
          "pom.xml",
          "query/pom.xml",
          "query/src/main/codegen/javacc/org/apache/kylin/query/util/.gitignore",
          "query/src/main/codegen/javacc/org/apache/kylin/query/util/CommentParser.jj",
          "query/src/main/java/org/apache/kylin/query/util/QueryUtil.java",
          "query/src/test/java/org/apache/kylin/query/util/QueryUtilTest.java"
        ],
        "message": "KYLIN-4879 Fix the function of remove sql comments",
        "before_after_code_files": [
          "query/src/main/codegen/javacc/org/apache/kylin/query/util/CommentParser.jj||query/src/main/codegen/javacc/org/apache/kylin/query/util/CommentParser.jj",
          "query/src/main/java/org/apache/kylin/query/util/QueryUtil.java||query/src/main/java/org/apache/kylin/query/util/QueryUtil.java",
          "query/src/test/java/org/apache/kylin/query/util/QueryUtilTest.java||query/src/test/java/org/apache/kylin/query/util/QueryUtilTest.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "query/src/main/codegen/javacc/org/apache/kylin/query/util/CommentParser.jj||query/src/main/codegen/javacc/org/apache/kylin/query/util/CommentParser.jj": [
          "File: query/src/main/codegen/javacc/org/apache/kylin/query/util/CommentParser.jj -> query/src/main/codegen/javacc/org/apache/kylin/query/util/CommentParser.jj",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: options {",
          "2:     IGNORE_CASE = true;",
          "3:     STATIC = false;",
          "4:     UNICODE_INPUT=true;",
          "5: }",
          "7: PARSER_BEGIN(CommentParser)",
          "8: package org.apache.kylin.query.util;",
          "9: import java.io.StringReader;",
          "10: import java.util.List;",
          "11: import java.util.ArrayList;",
          "12: import java.util.Scanner;",
          "13: import org.slf4j.Logger;",
          "14: import org.slf4j.LoggerFactory;",
          "16: public class CommentParser {",
          "18:     private final static Logger logger = LoggerFactory.getLogger(CommentParser.class);",
          "21:     public static void main(String args[]) throws ParseException {",
          "22:         System.out.println(\"Input SQL:\");",
          "23:         Scanner reader = new Scanner(System.in, \"UTF-8\");",
          "24:         String sql = reader.nextLine();",
          "25:         reader.close();",
          "26:         CommentParser parser = new CommentParser(new StringReader(sql));",
          "27:         String parseResult = parser.Input();",
          "28:         System.out.println(\"Translated SQL:\");",
          "29:         System.out.println(parseResult);",
          "30:     }",
          "32:     public CommentParser(String sql) {",
          "33:         this(new StringReader(sql));",
          "34:     }",
          "35: }",
          "37: PARSER_END(CommentParser)",
          "47: <DEFAULT>",
          "48: SKIP : {",
          "49:     \"/*\" : WITHINCOMMENT",
          "50:     | <\"--\" (~[\"\\n\",\"\\r\"])*>",
          "51: }",
          "53: < DEFAULT >",
          "54: TOKEN :",
          "55: {",
          "56:  < REMAIN_TOKEN : [\" \", \"\\t\",\"\\n\", \"\\r\", \",\", \"/\", \"-\" ] >",
          "57: | < QUOTE: \"'\" >",
          "58: | < QUOTED_STRING: <QUOTE> ( (~[\"'\"]) | (\"''\"))* <QUOTE> >",
          "59: | < DOUBLE_QUOTE: \"\\\"\" >",
          "60: | < DOUBLE_QUOTE_STRING: <DOUBLE_QUOTE> ( (~[\"\\\"\"]) | (\"\\\"\\\"\"))* <DOUBLE_QUOTE> >",
          "61: | < ANY : (~[\" \", \",\", \"\\t\", \"\\n\", \"\\r\", \"/\", \"-\" ])+ >",
          "62: }",
          "64: <WITHINCOMMENT>",
          "65: SKIP :",
          "66: {",
          "67:   \"*/\" : DEFAULT",
          "68: }",
          "70: <WITHINCOMMENT>",
          "71: MORE :",
          "72: {",
          "73:   < ~[] >",
          "74: }",
          "77: String Input() :",
          "78: {",
          "79:     String innerString;",
          "80:     StringBuilder transformedStr = new StringBuilder();",
          "81: }",
          "82: {",
          "83:     (",
          "84:     LOOKAHEAD(2)",
          "85:     innerString = Expression()",
          "86:     {",
          "87:         transformedStr.append(innerString);",
          "88:     }",
          "89:     )+",
          "90:     <EOF>",
          "91:     {",
          "92:         return transformedStr.toString();",
          "93:     }",
          "94: }",
          "97: String Expression() :",
          "98: {",
          "99:     String innerString = \"\";",
          "100:     String nextString = \"\";",
          "101: }",
          "102: {",
          "103:     {",
          "104:         if (Thread.currentThread().isInterrupted()) {",
          "105:             throw new ParseException(\"CommentParser is interrupted\");",
          "106:         }",
          "107:     }",
          "108:     (",
          "109:      innerString = QuotedString()",
          "110:     | innerString = DoubleQuoteString()",
          "111:     | innerString = RemainToken()",
          "112:     | innerString = Any()",
          "113:     )",
          "114:     {",
          "115:         return innerString + nextString;",
          "116:     }",
          "117: }",
          "119: String RemainToken() :",
          "120: {}",
          "121: {",
          "122:     < REMAIN_TOKEN >",
          "123:     {",
          "124:         logger.trace(\"meet token <REMAIN_TOKEN>\");",
          "125:         return getToken(0).image;",
          "126:     }",
          "127: }",
          "129: String QuotedString() :",
          "130: {",
          "131:     String s;",
          "132: }",
          "133: {",
          "134:     <QUOTED_STRING>",
          "135:     {",
          "136:         logger.trace(\"meet token in <QUOTED_STRING>: \" + getToken(0).image);",
          "137:         return getToken(0).image;",
          "138:     }",
          "139: }",
          "141: String DoubleQuoteString() :",
          "142: {",
          "143:     String s;",
          "144: }",
          "145: {",
          "146:     <DOUBLE_QUOTE_STRING>",
          "147:     {",
          "148:         logger.trace(\"meet token in <QUOTED2_STRING>: \" + getToken(0).image);",
          "149:         return getToken(0).image;",
          "150:     }",
          "151: }",
          "153: String Any() :",
          "154: {}",
          "155: {",
          "156:     < ANY >",
          "157:     {",
          "158:         logger.trace(\"meet token in <ANY>: \" + getToken(0).image);",
          "159:         return getToken(0).image;",
          "160:     }",
          "161: }",
          "",
          "---------------"
        ],
        "query/src/main/java/org/apache/kylin/query/util/QueryUtil.java||query/src/main/java/org/apache/kylin/query/util/QueryUtil.java": [
          "File: query/src/main/java/org/apache/kylin/query/util/QueryUtil.java -> query/src/main/java/org/apache/kylin/query/util/QueryUtil.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "213:                 || (sql1.startsWith(KEYWORD_EXPLAIN) && sql1.contains(KEYWORD_SELECT));",
          "214:     }",
          "222:         }",
          "227:     }",
          "229:     public interface IQueryTransformer {",
          "",
          "[Removed Lines]",
          "216:     public static String removeCommentInSql(String sql1) {",
          "218:         final String[] commentPatterns = new String[] { \"--(?!.*\\\\*/).*?[\\r\\n]\", \"/\\\\*(.|\\r|\\n)*?\\\\*/\" };",
          "220:         for (int i = 0; i < commentPatterns.length; i++) {",
          "221:             sql1 = sql1.replaceAll(commentPatterns[i], \"\");",
          "224:         sql1 = sql1.trim();",
          "226:         return sql1;",
          "",
          "[Added Lines]",
          "216:     public static String removeCommentInSql(String sql) {",
          "218:         try {",
          "219:             return new CommentParser(sql).Input().trim();",
          "220:         } catch (ParseException e) {",
          "221:             logger.error(\"Failed to parse sql: {}\", sql, e);",
          "222:             return sql;",
          "",
          "---------------"
        ],
        "query/src/test/java/org/apache/kylin/query/util/QueryUtilTest.java||query/src/test/java/org/apache/kylin/query/util/QueryUtilTest.java": [
          "File: query/src/test/java/org/apache/kylin/query/util/QueryUtilTest.java -> query/src/test/java/org/apache/kylin/query/util/QueryUtilTest.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "200:     @Test",
          "201:     public void testRemoveCommentInSql() {",
          "206:         {",
          "207:             String sqlWithComment = \"-- comment \\n\" + originSql;",
          "",
          "[Removed Lines]",
          "203:         String originSql = \"select count(*) from test_kylin_fact where price > 10.0\";",
          "204:         String originSql2 = \"select count(*) from test_kylin_fact where TEST_COLUMN != 'not--a comment'\";",
          "",
          "[Added Lines]",
          "203:         final String originSql = \"select count(*) from test_kylin_fact where price > 10.0\";",
          "204:         final String originSql2 = \"select count(*) from test_kylin_fact where TEST_COLUMN != 'not--a comment'\";",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "294:             Assert.assertEquals(originSql2, QueryUtil.removeCommentInSql(sqlWithComment2));",
          "295:         }",
          "309:     }",
          "311:     @Test",
          "",
          "[Removed Lines]",
          "297:         String content = \"        --  One-line comment and /**range\\n\" +",
          "298:                 \"/*\\n\" +",
          "299:                 \"Multi-line comment\\r\\n\" +",
          "300:                 \"--  Multi-line comment*/\\n\" +",
          "301:                 \"select price as \" +",
          "302:                 \"/*\\n\" +",
          "303:                 \"Multi-line comment\\r\\n\" +",
          "304:                 \"--  Multi-line comment*/\\n\" +",
          "305:                 \"revenue from /*One-line comment-- One-line comment*/ v_lineitem;\";",
          "306:         String expectedContent = \"select price as revenue from  v_lineitem;\";",
          "307:         String trimmedContent = QueryUtil.removeCommentInSql(content).replaceAll(\"\\n\", \"\").trim();",
          "308:         Assert.assertEquals(trimmedContent, expectedContent);",
          "",
          "[Added Lines]",
          "297:         {",
          "298:             String content = \"        --  One-line comment and /**range\\n\" +",
          "299:                     \"/*\\n\" +",
          "300:                     \"Multi-line comment\\r\\n\" +",
          "301:                     \"--  Multi-line comment*/\\n\" +",
          "302:                     \"select price as \" +",
          "303:                     \"/*\\n\" +",
          "304:                     \"Multi-line comment\\r\\n\" +",
          "305:                     \"--  Multi-line comment*/\\n\" +",
          "306:                     \"revenue from /*One-line comment-- One-line comment*/ v_lineitem;\";",
          "307:             String expectedContent = \"select price as revenue from  v_lineitem;\";",
          "308:             String trimmedContent = QueryUtil.removeCommentInSql(content).replaceAll(\"\\n\", \"\").trim();",
          "309:             Assert.assertEquals(trimmedContent, expectedContent);",
          "310:         }",
          "312:         {",
          "313:             String sqlWithComment = \"select count(*) from test_kylin_fact WHERE column_name = '--this is not comment'\\n \" +",
          "314:                     \"LIMIT 100 offset 0\";",
          "315:             Assert.assertEquals(sqlWithComment, QueryUtil.removeCommentInSql(sqlWithComment));",
          "316:         }",
          "318:         {",
          "319:             String sqlWithComment = \"select count(*) from test_kylin_fact WHERE column_name = '--this is not comment'\";",
          "320:             Assert.assertEquals(sqlWithComment, QueryUtil.removeCommentInSql(sqlWithComment));",
          "321:         }",
          "323:         {",
          "324:             String sqlWithComment = \"select count(*) from test_kylin_fact WHERE column_name = '/*--this is not comment*/'\";",
          "325:             Assert.assertEquals(sqlWithComment, QueryUtil.removeCommentInSql(sqlWithComment));",
          "326:         }",
          "328:         {",
          "329:             String sqlWithComment = \"-- comment \\n\" + originSql + \"/* comment */;\" + \"-- comment \\n\" + originSql + \"/* comment */\";",
          "330:             Assert.assertEquals(\"select count(*) from test_kylin_fact where price > 10.0;\\n\" +",
          "331:                     \"select count(*) from test_kylin_fact where price > 10.0\", QueryUtil.removeCommentInSql(sqlWithComment));",
          "332:         }",
          "334:         {",
          "335:             String sqlWithComment = \"select count(*) from test_kylin_fact /* this is test*/  where price > 10.0 -- comment \\n\" +",
          "336:                     \";\" + \"insert into test_kylin_fact(id) values(?); -- comment \\n\";",
          "337:             Assert.assertEquals(\"select count(*) from test_kylin_fact   where price > 10.0 \\n\" +",
          "338:                     \";insert into test_kylin_fact(id) values(?);\", QueryUtil.removeCommentInSql(sqlWithComment));",
          "339:         }",
          "341:         {",
          "342:             String sqlWithoutComment = \"select * from test_kylin_fact where price=\\\"/* this is not comment */\\\"\";",
          "343:             Assert.assertEquals(sqlWithoutComment, QueryUtil.removeCommentInSql(sqlWithoutComment));",
          "344:         }",
          "346:         {",
          "347:             String sqlWithoutComment = \"select count(*) from test_kylin_fact -- 'comment'\\n\";",
          "348:             Assert.assertEquals(\"select count(*) from test_kylin_fact\", QueryUtil.removeCommentInSql(sqlWithoutComment));",
          "349:         }",
          "350:         {",
          "351:             Assert.assertEquals(\"select count(*)\",",
          "352:                     QueryUtil.removeCommentInSql(\"select count(*) -- , --\\t --/ --\"));",
          "353:         }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "9f8eec31953d18696d4abb83cd2cfb9a22f5e2a7",
      "candidate_info": {
        "commit_hash": "9f8eec31953d18696d4abb83cd2cfb9a22f5e2a7",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/9f8eec31953d18696d4abb83cd2cfb9a22f5e2a7",
        "files": [
          "core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java"
        ],
        "message": "KYLIN-4597 Fix NPE when download diagnosis info for a job\n\nProblems:\nWhen download diagnosis info for a job, it throws NPE.\n\nSolutions:\nDon't replace '-' to '' for job id, which will change the value of job id and can't find the job.\n\n(cherry picked from commit 9e700abf174450be5e37bbb4919ac851ffdab806)",
        "before_after_code_files": [
          "core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java||core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java||core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java": [
          "File: core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java -> core-common/src/main/java/org/apache/kylin/common/util/CliCommandExecutor.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "176:         }",
          "177:     }",
          "180:     public static final String COMMAND_WHITE_LIST = \"[^\\\\w%,@/:=?.\\\"\\\\[\\\\]]\";",
          "181:     public static final String HIVE_BLOCK_LIST = \"[ <>()$;\\\\-#!+*\\\"'/=%@]+\";",
          "",
          "[Removed Lines]",
          "179:     public static final String COMMAND_BLOCK_LIST = \"[ &`>|{}()$;\\\\-#~!+*\\\\\\\\]+\";",
          "",
          "[Added Lines]",
          "179:     public static final String COMMAND_BLOCK_LIST = \"[ &`>|{}()$;\\\\#~!+*\\\\\\\\]+\";",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "07a749068939bdbaf633b8cb3fd0bd4803ff1906",
      "candidate_info": {
        "commit_hash": "07a749068939bdbaf633b8cb3fd0bd4803ff1906",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/07a749068939bdbaf633b8cb3fd0bd4803ff1906",
        "files": [
          "core-cube/src/main/java/org/apache/kylin/cube/CubeInstance.java",
          "core-cube/src/main/java/org/apache/kylin/cube/cuboid/CuboidModeEnum.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala"
        ],
        "message": "KYLIN-5071 Still estimate the rowcount of baseCuboid when baseCuboid is not built",
        "before_after_code_files": [
          "core-cube/src/main/java/org/apache/kylin/cube/CubeInstance.java||core-cube/src/main/java/org/apache/kylin/cube/CubeInstance.java",
          "core-cube/src/main/java/org/apache/kylin/cube/cuboid/CuboidModeEnum.java||core-cube/src/main/java/org/apache/kylin/cube/cuboid/CuboidModeEnum.java",
          "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala||kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "core-cube/src/main/java/org/apache/kylin/cube/CubeInstance.java||core-cube/src/main/java/org/apache/kylin/cube/CubeInstance.java": [
          "File: core-cube/src/main/java/org/apache/kylin/cube/CubeInstance.java -> core-cube/src/main/java/org/apache/kylin/cube/CubeInstance.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import static org.apache.kylin.shaded.com.google.common.base.Preconditions.checkNotNull;",
          "22: import static org.apache.kylin.cube.cuboid.CuboidModeEnum.CURRENT;",
          "23: import static org.apache.kylin.cube.cuboid.CuboidModeEnum.RECOMMEND;",
          "25: import java.io.IOException;",
          "26: import java.nio.charset.StandardCharsets;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "24: import static org.apache.kylin.cube.cuboid.CuboidModeEnum.CURRENT_WITH_BASE;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "458:     }",
          "460:     public Set<Long> getCuboidsByMode(CuboidModeEnum cuboidMode) {",
          "461:         if (cuboidMode == null || cuboidMode == CURRENT) {",
          "463:         }",
          "464:         Set<Long> cuboidsRecommend = getCuboidsRecommend();",
          "465:         if (cuboidsRecommend == null || cuboidMode == RECOMMEND) {",
          "",
          "[Removed Lines]",
          "462:             return getCuboidScheduler().getAllCuboidIds();",
          "",
          "[Added Lines]",
          "462:         Set<Long> currentCuboid = getCuboidScheduler().getAllCuboidIds();",
          "464:             return currentCuboid;",
          "465:         }",
          "466:         if (cuboidMode == CURRENT_WITH_BASE) {",
          "467:             currentCuboid.add(getCuboidScheduler().getBaseCuboidId());",
          "468:             return currentCuboid;",
          "",
          "---------------"
        ],
        "core-cube/src/main/java/org/apache/kylin/cube/cuboid/CuboidModeEnum.java||core-cube/src/main/java/org/apache/kylin/cube/cuboid/CuboidModeEnum.java": [
          "File: core-cube/src/main/java/org/apache/kylin/cube/cuboid/CuboidModeEnum.java -> core-cube/src/main/java/org/apache/kylin/cube/cuboid/CuboidModeEnum.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: import org.apache.kylin.shaded.com.google.common.base.Strings;",
          "25: public enum CuboidModeEnum {",
          "27:             \"RECOMMEND_MISSING\"), RECOMMEND_MISSING_WITH_BASE(\"RECOMMEND_MISSING_WITH_BASE\");",
          "29:     private final String modeName;",
          "",
          "[Removed Lines]",
          "26:     CURRENT(\"CURRENT\"), RECOMMEND(\"RECOMMEND\"), RECOMMEND_EXISTING(\"RECOMMEND_EXISTING\"), RECOMMEND_MISSING(",
          "",
          "[Added Lines]",
          "26:     CURRENT(\"CURRENT\"), CURRENT_WITH_BASE(\"CURRENT_WITH_BASE\"), RECOMMEND(\"RECOMMEND\"), RECOMMEND_EXISTING(\"RECOMMEND_EXISTING\"), RECOMMEND_MISSING(",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java||kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java": [
          "File: kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java -> kylin-spark-project/kylin-spark-engine/src/main/scala/org/apache/kylin/engine/spark/job/CubeBuildJob.java",
          "--- Hunk 1 ---",
          "[Context before]",
          "35: import java.util.stream.Collectors;",
          "37: import org.apache.hadoop.fs.FSDataInputStream;",
          "38: import org.apache.kylin.common.persistence.ResourceStore;",
          "39: import org.apache.kylin.engine.mr.common.BatchConstants;",
          "40: import org.apache.kylin.engine.mr.common.CubeStatsWriter;",
          "41: import org.apache.kylin.engine.mr.common.StatisticsDecisionUtil;",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "38: import org.apache.kylin.common.KylinConfig;",
          "40: import org.apache.kylin.cube.cuboid.CuboidModeEnum;",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "112:         String firstSegmentId = segmentIds.iterator().next();",
          "113:         String cubeId = getParam(MetadataConstants.P_CUBE_ID);",
          "115:         cubeManager = CubeManager.getInstance(config);",
          "116:         cubeInstance = cubeManager.getCubeByUuid(cubeId);",
          "117:         CubeSegment newSegment = cubeInstance.getSegmentById(firstSegmentId);",
          "",
          "[Removed Lines]",
          "114:         SegmentInfo seg = ManagerHub.getSegmentInfo(config, cubeId, firstSegmentId);",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "125:         if (needStatistics) {",
          "127:             long startMills = System.currentTimeMillis();",
          "130:             sourceChooser.setNeedStatistics();",
          "131:             sourceChooser.decideFlatTableSource(null);",
          "132:             Map<Long, HLLCounter> hllMap = new HashMap<>();",
          "",
          "[Removed Lines]",
          "128:             spanningTree = new ForestSpanningTree(JavaConversions.asJavaCollection(seg.toBuildLayouts()));",
          "129:             sourceChooser = new ParentSourceChooser(spanningTree, seg, newSegment, jobId, ss, config, false);",
          "",
          "[Added Lines]",
          "128:             SegmentInfo statisticsSeg = ManagerHub.getSegmentInfo(config, cubeId, firstSegmentId, CuboidModeEnum.CURRENT_WITH_BASE);",
          "130:             spanningTree = new ForestSpanningTree(JavaConversions.asJavaCollection(statisticsSeg.toBuildLayouts()));",
          "131:             sourceChooser = new ParentSourceChooser(spanningTree, statisticsSeg, newSegment, jobId, ss, config, false);",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "163:         try {",
          "165:             for (String segId : segmentIds) {",
          "167:                 spanningTree = new ForestSpanningTree(",
          "168:                         JavaConversions.asJavaCollection(seg.toBuildLayouts()));",
          "169:                 logger.info(\"There are {} cuboids to be built in segment {}.\",",
          "",
          "[Removed Lines]",
          "166:                 seg = ManagerHub.getSegmentInfo(config, cubeId, segId);",
          "",
          "[Added Lines]",
          "168:                 SegmentInfo seg = ManagerHub.getSegmentInfo(config, cubeId, segId);",
          "",
          "---------------"
        ],
        "kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala||kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala": [
          "File: kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala -> kylin-spark-project/kylin-spark-metadata/src/main/scala/org/apache/kylin/engine/spark/metadata/MetadataConverter.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "130:   }",
          "132:   def extractEntityAndMeasures(cubeInstance: CubeInstance, cuboidMode: CuboidModeEnum): (List[LayoutEntity], Map[Integer, FunctionDesc]) = {",
          "133:     val (columnIndexes, shardByColumnsId, idToColumnMap, measureId) = genIDToColumnMap(cubeInstance)",
          "134:     (cubeInstance.getCuboidsByMode(cuboidMode)",
          "135:       .asScala",
          "137:       .map { long =>",
          "138:         genLayoutEntity(columnIndexes, shardByColumnsId, idToColumnMap, measureId, long)",
          "139:       }.toList, measureId.asScala.toMap)",
          "",
          "[Removed Lines]",
          "136:       .filter(id => cubeInstance.getConfig.isBuildBaseCuboid || !id.equals(cubeInstance.getCuboidScheduler.getBaseCuboidId))",
          "",
          "[Added Lines]",
          "133:     val buildBaseCuboid = cubeInstance.getConfig.isBuildBaseCuboid || cuboidMode.equals(CuboidModeEnum.CURRENT_WITH_BASE)",
          "137:       .filter(id => buildBaseCuboid.equals(true) || !id.equals(cubeInstance.getCuboidScheduler.getBaseCuboidId))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8a8f574e7c56f4a0131f8b0f938e58dc98238a72",
      "candidate_info": {
        "commit_hash": "8a8f574e7c56f4a0131f8b0f938e58dc98238a72",
        "repo": "apache/kylin",
        "commit_url": "https://github.com/apache/kylin/commit/8a8f574e7c56f4a0131f8b0f938e58dc98238a72",
        "files": [
          "webapp/app/js/model/jobConfig.js",
          "webapp/app/js/model/jobListModel.js"
        ],
        "message": "KYLIN-4559 show cardinality and lookup snapshot job on job\n\n(cherry picked from commit 7b8d820585ab058948c7239f8d06938f380048f3)",
        "before_after_code_files": [
          "webapp/app/js/model/jobConfig.js||webapp/app/js/model/jobConfig.js",
          "webapp/app/js/model/jobListModel.js||webapp/app/js/model/jobListModel.js"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/kylin/pull/1893",
          "https://github.com/apache/kylin/pull/2018",
          "https://github.com/apache/kylin/pull/2125",
          "https://github.com/apache/kylin/pull/2033",
          "https://github.com/apache/kylin/pull/2112",
          "https://github.com/apache/kylin/pull/2115",
          "https://github.com/apache/kylin/pull/1865",
          "https://github.com/apache/kylin/pull/1913",
          "https://github.com/apache/kylin/pull/2135"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "webapp/app/js/model/jobConfig.js||webapp/app/js/model/jobConfig.js": [
          "File: webapp/app/js/model/jobConfig.js -> webapp/app/js/model/jobConfig.js",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:   searchMode: [",
          "45:     {name: 'CUBING', value: 'CUBING_ONLY'},",
          "46:     {name: 'CHECK POINT', value: 'CHECKPOINT_ONLY'},",
          "47:     {name: 'ALL', value: 'ALL'}",
          "48:   ],",
          "49:   queryitems: [",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "47:     {name: 'CARDINALITY', value: 'CARDINALITY_ONLY'},",
          "48:     {name: 'SNAPSHOT', value: 'SNAPSHOT_ONLY'},",
          "",
          "---------------"
        ],
        "webapp/app/js/model/jobListModel.js||webapp/app/js/model/jobListModel.js": [
          "File: webapp/app/js/model/jobListModel.js -> webapp/app/js/model/jobListModel.js",
          "--- Hunk 1 ---",
          "[Context before]",
          "26:     this.jobFilter = {",
          "27:         cubeName : null,",
          "28:         timeFilterId : kylinConfig.getJobTimeFilterId(),",
          "30:         statusIds: []",
          "31:     };",
          "",
          "[Removed Lines]",
          "29:         searchModeId: 2,",
          "",
          "[Added Lines]",
          "29:         searchModeId: 4,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "34:         this.jobFilter = {",
          "35:           cubeName : null,",
          "36:           timeFilterId : kylinConfig.getJobTimeFilterId(),",
          "38:           statusIds: []",
          "39:         };",
          "40:     };",
          "",
          "[Removed Lines]",
          "37:           searchModeId: 2,",
          "",
          "[Added Lines]",
          "37:           searchModeId: 4,",
          "",
          "---------------"
        ]
      }
    }
  ]
}