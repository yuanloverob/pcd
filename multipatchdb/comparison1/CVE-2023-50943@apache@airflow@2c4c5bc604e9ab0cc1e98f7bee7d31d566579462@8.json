{
  "cve_id": "CVE-2023-50943",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows a potential attacker to poison the XCom data by bypassing the protection of \"enable_xcom_pickling=False\" configuration setting resulting in poisoned data after XCom deserialization. This vulnerability is considered low since it requires a DAG author to exploit it. Users are recommended to upgrade to version 2.8.1 or later, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
  "patch_info": {
    "commit_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "files": [
      "airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py"
    ],
    "message": "Stop deserializing pickle when enable_xcom_pickling is False (#36255)\n\n* Stop deserializing pickle when enable_xcom_pickling is False\n\n* Fix unit tests\n\n(cherry picked from commit 63e97abec5d56bc62a293c93f5227f364561e51c)",
    "before_after_code_files": [
      "airflow/models/xcom.py||airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py||tests/models/test_xcom.py"
    ]
  },
  "patch_diff": {
    "airflow/models/xcom.py||airflow/models/xcom.py": [
      "File: airflow/models/xcom.py -> airflow/models/xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "685:             except pickle.UnpicklingError:",
      "686:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "687:         else:",
      "693:     @staticmethod",
      "694:     def deserialize_value(result: XCom) -> Any:",
      "",
      "[Removed Lines]",
      "688:             try:",
      "689:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "690:             except (json.JSONDecodeError, UnicodeDecodeError):",
      "691:                 return pickle.loads(result.value)",
      "",
      "[Added Lines]",
      "688:             # Since xcom_pickling is disabled, we should only try to deserialize with JSON",
      "689:             return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "",
      "---------------"
    ],
    "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py": [
      "File: tests/api_connexion/schemas/test_xcom_schema.py -> tests/api_connexion/schemas/test_xcom_schema.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from airflow.models import DagRun, XCom",
      "31: from airflow.utils.dates import parse_execution_date",
      "32: from airflow.utils.session import create_session",
      "34: pytestmark = pytest.mark.db_test",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "188:     default_time = \"2016-04-02T21:00:00+00:00\"",
      "189:     default_time_parsed = parse_execution_date(default_time)",
      "191:     def test_serialize(self, create_xcom, session):",
      "192:         create_xcom(",
      "193:             dag_id=\"test_dag\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "192:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "208:             \"map_index\": -1,",
      "209:         }",
      "211:     def test_deserialize(self):",
      "212:         xcom_dump = {",
      "213:             \"key\": \"test_key\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "213:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------"
    ],
    "tests/models/test_xcom.py||tests/models/test_xcom.py": [
      "File: tests/models/test_xcom.py -> tests/models/test_xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "140:             ret_value = XCom.get_value(key=\"xcom_test3\", ti_key=ti_key, session=session)",
      "141:         assert ret_value == {\"key\": \"value\"}",
      "144:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"}):",
      "145:             XCom.set(",
      "146:                 key=\"xcom_test3\",",
      "",
      "[Removed Lines]",
      "143:     def test_xcom_deserialize_with_pickle_to_json_switch(self, task_instance, session):",
      "",
      "[Added Lines]",
      "143:     def test_xcom_deserialize_pickle_when_xcom_pickling_is_disabled(self, task_instance, session):",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "151:                 session=session,",
      "152:             )",
      "153:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"False\"}):",
      "163:     @conf_vars({(\"core\", \"xcom_enable_pickling\"): \"False\"})",
      "164:     def test_xcom_disable_pickle_type_fail_on_non_json(self, task_instance, session):",
      "",
      "[Removed Lines]",
      "154:             ret_value = XCom.get_one(",
      "155:                 key=\"xcom_test3\",",
      "156:                 dag_id=task_instance.dag_id,",
      "157:                 task_id=task_instance.task_id,",
      "158:                 run_id=task_instance.run_id,",
      "159:                 session=session,",
      "160:             )",
      "161:         assert ret_value == {\"key\": \"value\"}",
      "",
      "[Added Lines]",
      "154:             with pytest.raises(UnicodeDecodeError):",
      "155:                 XCom.get_one(",
      "156:                     key=\"xcom_test3\",",
      "157:                     dag_id=task_instance.dag_id,",
      "158:                     task_id=task_instance.task_id,",
      "159:                     run_id=task_instance.run_id,",
      "160:                     session=session,",
      "161:                 )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "8c41a8f14d381e7b0f1068d2d2503408b97fc707",
      "candidate_info": {
        "commit_hash": "8c41a8f14d381e7b0f1068d2d2503408b97fc707",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8c41a8f14d381e7b0f1068d2d2503408b97fc707",
        "files": [
          "airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py",
          "airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py",
          "airflow/models/taskinstance.py",
          "airflow/utils/db.py",
          "docs/apache-airflow/img/airflow_erd.sha256",
          "docs/apache-airflow/img/airflow_erd.svg",
          "docs/apache-airflow/migrations-ref.rst",
          "scripts/in_container/run_mypy.sh"
        ],
        "message": "Remove usused index on task instance (#36737)\n\nIndex is only helpful for a user's custom query -- not for airflow in general (see comment https://github.com/apache/airflow/pull/30762#issuecomment-1886658295).  Noticed that this query had zero scans over a period of months.  I also observed that it also takes up as much space as the table itself.  Since it's not generally useful, it doesn't belong in airflow OSS.\n\nReverts #30762\n\n(cherry picked from commit e20b400317ae4eb41181c5b0cee466eff768b521)",
        "before_after_code_files": [
          "airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py||airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py",
          "airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py||airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py",
          "airflow/models/taskinstance.py||airflow/models/taskinstance.py",
          "airflow/utils/db.py||airflow/utils/db.py",
          "scripts/in_container/run_mypy.sh||scripts/in_container/run_mypy.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py||airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py": [
          "File: airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py -> airflow/migrations/versions/0126_2_7_0_add_index_to_task_instance_table.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "38: def upgrade():",
          "39:     \"\"\"Apply Add index to task_instance table\"\"\"",
          "48: def downgrade():",
          "49:     \"\"\"Unapply Add index to task_instance table\"\"\"",
          "",
          "[Removed Lines]",
          "40:     op.create_index(",
          "41:         \"ti_state_incl_start_date\",",
          "42:         \"task_instance\",",
          "43:         [\"dag_id\", \"task_id\", \"state\"],",
          "44:         postgresql_include=[\"start_date\"],",
          "45:     )",
          "50:     op.drop_index(\"ti_state_incl_start_date\", table_name=\"task_instance\")",
          "",
          "[Added Lines]",
          "40:     # We don't add this index anymore because it's not useful.",
          "41:     pass",
          "46:     # At 2.8.1 we removed this index as it is not used, and changed this migration not to add it",
          "47:     # So we use drop if exists (cus it might not be there)",
          "48:     import sqlalchemy",
          "49:     from contextlib import suppress",
          "51:     with suppress(sqlalchemy.exc.DatabaseError):  # mysql does not support drop if exists index",
          "52:         op.drop_index(\"ti_state_incl_start_date\", table_name=\"task_instance\", if_exists=True)",
          "",
          "---------------"
        ],
        "airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py||airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py": [
          "File: airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py -> airflow/migrations/versions/0133_2_8_1_refactor_dag_run_indexes.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "19: \"\"\"Drop unused TI index",
          "21: Revision ID: 88344c1d9134",
          "22: Revises: 10b52ebd31f7",
          "23: Create Date: 2024-01-11 11:54:48.232030",
          "25: \"\"\"",
          "27: import sqlalchemy as sa",
          "28: from alembic import op",
          "31: # revision identifiers, used by Alembic.",
          "32: revision = \"88344c1d9134\"",
          "33: down_revision = \"10b52ebd31f7\"",
          "34: branch_labels = None",
          "35: depends_on = None",
          "36: airflow_version = \"2.8.1\"",
          "39: def upgrade():",
          "40:     \"\"\"Apply refactor dag run indexes\"\"\"",
          "41:     # This index may have been created in 2.7 but we've since removed it from migrations",
          "42:     import sqlalchemy",
          "43:     from contextlib import suppress",
          "45:     with suppress(sqlalchemy.exc.DatabaseError):  # mysql does not support drop if exists index",
          "46:         op.drop_index(\"ti_state_incl_start_date\", table_name=\"task_instance\", if_exists=True)",
          "49: def downgrade():",
          "50:     \"\"\"Unapply refactor dag run indexes\"\"\"",
          "",
          "---------------"
        ],
        "airflow/models/taskinstance.py||airflow/models/taskinstance.py": [
          "File: airflow/models/taskinstance.py -> airflow/models/taskinstance.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1256:         # Existing \"ti_state_lkp\" is not enough for such query when this table has millions of rows, since",
          "1257:         # rows have to be fetched in order to retrieve the start_date column. With this index, INDEX ONLY SCAN",
          "1258:         # is performed and that query runs within milliseconds.",
          "1260:         Index(\"ti_pool\", pool, state, priority_weight),",
          "1261:         Index(\"ti_job_id\", job_id),",
          "1262:         Index(\"ti_trigger_id\", trigger_id),",
          "",
          "[Removed Lines]",
          "1259:         Index(\"ti_state_incl_start_date\", dag_id, task_id, state, postgresql_include=[\"start_date\"]),",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "airflow/utils/db.py||airflow/utils/db.py": [
          "File: airflow/utils/db.py -> airflow/utils/db.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "89:     \"2.6.2\": \"c804e5c76e3e\",",
          "90:     \"2.7.0\": \"405de8318b3a\",",
          "91:     \"2.8.0\": \"10b52ebd31f7\",",
          "92: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "92:     \"2.8.1\": \"88344c1d9134\",",
          "",
          "---------------"
        ],
        "scripts/in_container/run_mypy.sh||scripts/in_container/run_mypy.sh": [
          "File: scripts/in_container/run_mypy.sh -> scripts/in_container/run_mypy.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: ADDITIONAL_MYPY_OPTIONS=()",
          "25: if [[ ${SUSPENDED_PROVIDERS_FOLDERS=} != \"\" ]];",
          "26: then",
          "27:     for folder in ${SUSPENDED_PROVIDERS_FOLDERS=}",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "25: export MYPY_FORCE_COLOR=true",
          "26: export TERM=ansi",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f0b18881a6917b292617a66c1efcfdc7119fe447",
      "candidate_info": {
        "commit_hash": "f0b18881a6917b292617a66c1efcfdc7119fe447",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f0b18881a6917b292617a66c1efcfdc7119fe447",
        "files": [
          ".pre-commit-config.yaml",
          ".rat-excludes",
          "STATIC_CODE_CHECKS.rst",
          "airflow/reproducible_build.yaml",
          "dev/README_RELEASE_AIRFLOW.md",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "images/breeze/output_static-checks.svg",
          "images/breeze/output_static-checks.txt",
          "scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py",
          "scripts/in_container/run_prepare_airflow_packages.py"
        ],
        "message": "Add support for reproducible build date epoch for Airflow releases (#36726)\n\nHatch has built-in support for reproducible builds, however it\nuses a hard-coded 2020 date to generate the reproducible binaries,\nwhich produces whl, tar.gz files that contain file dates that are\npretty old. This might be confusing for anyone who is looking at\nthe file contents and timestamp inside.\n\nThis PR adds support (similar to provider approach) to store current\nreproducible date in the repository - so that it can be committed\nand tagged together with Airflow sources. It is updated fully\nautomaticallly by pre-commit whenever release notes change, which\nbasically means that whenever release notes are update just\nbefore release, the reproducible date is updated to current date.\n\nFor now we only check if the packages produced by hatchling\nbuild are reproducible.\n\n(cherry picked from commit a2d6c389f69034c526554b3291874dc4d66c4529)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/pre_commit_ids.py||dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py||scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py",
          "scripts/in_container/run_prepare_airflow_packages.py||scripts/in_container/run_prepare_airflow_packages.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "209: RICH_VERSION = \"13.7.0\"",
          "210: NODE_VERSION = \"21.2.0\"",
          "211: PRE_COMMIT_VERSION = \"3.5.0\"",
          "213: AIRFLOW_BUILD_DOCKERFILE = f\"\"\"",
          "214: FROM python:{DEFAULT_PYTHON_MAJOR_MINOR_VERSION}-slim-{ALLOWED_DEBIAN_VERSIONS[0]}",
          "215: RUN apt-get update && apt-get install -y --no-install-recommends git",
          "218: COPY . /opt/airflow",
          "219: \"\"\"",
          "",
          "[Removed Lines]",
          "216: RUN pip install pip=={AIRFLOW_PIP_VERSION} hatch==1.9.1 \\",
          "217:   gitpython=={GITPYTHON_VERSION} rich=={RICH_VERSION} pre-commit=={PRE_COMMIT_VERSION}",
          "",
          "[Added Lines]",
          "212: PYYAML_VERSION = \"6.0.1\"",
          "217: RUN pip install pip=={AIRFLOW_PIP_VERSION} hatch==1.9.1 pyyaml=={PYYAML_VERSION}\\",
          "218:  gitpython=={GITPYTHON_VERSION} rich=={RICH_VERSION} pre-commit=={PRE_COMMIT_VERSION}",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/pre_commit_ids.py||dev/breeze/src/airflow_breeze/pre_commit_ids.py": [
          "File: dev/breeze/src/airflow_breeze/pre_commit_ids.py -> dev/breeze/src/airflow_breeze/pre_commit_ids.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "129:     \"update-local-yml-file\",",
          "130:     \"update-migration-references\",",
          "131:     \"update-providers-dependencies\",",
          "132:     \"update-spelling-wordlist-to-be-sorted\",",
          "133:     \"update-supported-versions\",",
          "134:     \"update-vendored-in-k8s-json-schema\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "132:     \"update-reproducible-source-date-epoch\",",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py||scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py": [
          "File: scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py -> scripts/ci/pre_commit/pre_commit_update_source_date_epoch.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "18: from __future__ import annotations",
          "20: import sys",
          "21: from hashlib import md5",
          "22: from pathlib import Path",
          "23: from time import time",
          "25: import yaml",
          "27: sys.path.insert(0, str(Path(__file__).parent.resolve()))  # make sure common_precommit_utils is importable",
          "29: from common_precommit_utils import AIRFLOW_SOURCES_ROOT_PATH",
          "31: RELEASE_NOTES_FILE_PATH = AIRFLOW_SOURCES_ROOT_PATH / \"RELEASE_NOTES.rst\"",
          "32: REPRODUCIBLE_BUILD_FILE = AIRFLOW_SOURCES_ROOT_PATH / \"airflow\" / \"reproducible_build.yaml\"",
          "34: if __name__ == \"__main__\":",
          "35:     hash_md5 = md5()",
          "36:     hash_md5.update(RELEASE_NOTES_FILE_PATH.read_bytes())",
          "37:     release_notes_hash = hash_md5.hexdigest()",
          "38:     reproducible_build_text = REPRODUCIBLE_BUILD_FILE.read_text()",
          "39:     reproducible_build = yaml.safe_load(reproducible_build_text)",
          "40:     old_hash = reproducible_build[\"release-notes-hash\"]",
          "41:     if release_notes_hash != old_hash:",
          "42:         # Replace the hash in the file",
          "43:         reproducible_build[\"release-notes-hash\"] = release_notes_hash",
          "44:         reproducible_build[\"source-date-epoch\"] = int(time())",
          "45:     REPRODUCIBLE_BUILD_FILE.write_text(yaml.dump(reproducible_build))",
          "",
          "---------------"
        ],
        "scripts/in_container/run_prepare_airflow_packages.py||scripts/in_container/run_prepare_airflow_packages.py": [
          "File: scripts/in_container/run_prepare_airflow_packages.py -> scripts/in_container/run_prepare_airflow_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: from pathlib import Path",
          "27: from shutil import rmtree",
          "29: from rich.console import Console",
          "31: console = Console(color_system=\"standard\", width=200)",
          "33: AIRFLOW_SOURCES_ROOT = Path(__file__).parents[2].resolve()",
          "34: AIRFLOW_INIT_FILE = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"__init__.py\"",
          "35: WWW_DIRECTORY = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"www\"",
          "36: VERSION_SUFFIX = os.environ.get(\"VERSION_SUFFIX_FOR_PYPI\", \"\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "29: import yaml",
          "35: REPRODUCIBLE_BUILD_FILE = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"reproducible_build.yaml\"",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "81:     if package_format in [\"both\", \"sdist\"]:",
          "82:         build_command.extend([\"-t\", \"sdist\"])",
          "84:     console.print(f\"[bright_blue]Building packages: {package_format}\\n\")",
          "87:     if build_process.returncode != 0:",
          "88:         console.print(\"[red]Error building Airflow packages\")",
          "",
          "[Removed Lines]",
          "85:     build_process = subprocess.run(build_command, capture_output=False, cwd=AIRFLOW_SOURCES_ROOT)",
          "",
          "[Added Lines]",
          "86:     reproducible_date = yaml.safe_load(REPRODUCIBLE_BUILD_FILE.read_text())[\"source-date-epoch\"]",
          "88:     envcopy = os.environ.copy()",
          "89:     envcopy[\"SOURCE_DATE_EPOCH\"] = str(reproducible_date)",
          "91:     build_process = subprocess.run(",
          "92:         build_command,",
          "93:         capture_output=False,",
          "94:         cwd=AIRFLOW_SOURCES_ROOT,",
          "95:         env=envcopy,",
          "96:     )",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "dd6572fb0350d0f8f5473a2d30163a550b7dff6a",
      "candidate_info": {
        "commit_hash": "dd6572fb0350d0f8f5473a2d30163a550b7dff6a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/dd6572fb0350d0f8f5473a2d30163a550b7dff6a",
        "files": [
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/src/airflow_breeze/utils/path_utils.py"
        ],
        "message": "Check executable permission for entrypoints at breeze start (#36482)\n\n* Check executable permission for entrypoints at breeze start\n\nSometimes our contributors check out Airflow repository on filesystems\nthat are not POSIX compliant and do not have support for executable bits\n(for example when you check-out the repository in Windows and attempt to\nmap it to a Linux VM). Breeze and building CI images will not\nwork in this case, but the error that you see might be misleading.\n\nThis PR performs additional environment check and informs you that\nyou should not do it, if executable bits are missing from entrypoints.\n\n* Update dev/breeze/src/airflow_breeze/utils/docker_command_utils.py\n\nCo-authored-by: Jens Scheffler <95105677+jscheffl@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Jens Scheffler <95105677+jscheffl@users.noreply.github.com>\n(cherry picked from commit 5551e14f6c0d8706bf8beaaab6a8fa5c80719a89)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/src/airflow_breeze/utils/path_utils.py||dev/breeze/src/airflow_breeze/utils/path_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: from airflow_breeze.utils.host_info_utils import get_host_group_id, get_host_os, get_host_user_id",
          "31: from airflow_breeze.utils.path_utils import (",
          "32:     AIRFLOW_SOURCES_ROOT,",
          "33:     cleanup_python_generated_files,",
          "34:     create_mypy_volume_if_needed,",
          "35: )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "33:     SCRIPTS_DOCKER_DIR,",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "481:         env_variables[\"AIRFLOW__CELERY__BROKER_URL\"] = url_map[params.celery_broker]",
          "484: def perform_environment_checks(quiet: bool = False):",
          "485:     check_docker_is_running()",
          "486:     check_docker_version(quiet)",
          "487:     check_docker_compose_version(quiet)",
          "490: def get_docker_syntax_version() -> str:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "485: def check_executable_entrypoint_permissions(quiet: bool = False):",
          "486:     \"\"\"",
          "487:     Checks if the user has executable permissions on the entrypoints in checked-out airflow repository..",
          "488:     \"\"\"",
          "489:     for entrypoint in SCRIPTS_DOCKER_DIR.glob(\"entrypoint*.sh\"):",
          "490:         if get_verbose() and not quiet:",
          "491:             get_console().print(f\"[info]Checking executable permissions on {entrypoint.as_posix()}[/]\")",
          "492:         if not os.access(entrypoint.as_posix(), os.X_OK):",
          "493:             get_console().print(",
          "494:                 f\"[error]You do not have executable permissions on {entrypoint}[/]\\n\"",
          "495:                 f\"You likely checked out airflow repo on a filesystem that does not support executable \"",
          "496:                 f\"permissions (for example on a Windows filesystem that is mapped to Linux VM). Airflow \"",
          "497:                 f\"repository should only be checked out on a filesystem that is POSIX compliant.\"",
          "498:             )",
          "499:             sys.exit(1)",
          "500:     if not quiet:",
          "501:         get_console().print(\"[success]Executable permissions on entrypoints are OK[/]\")",
          "508:     check_executable_entrypoint_permissions(quiet)",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/path_utils.py||dev/breeze/src/airflow_breeze/utils/path_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/path_utils.py -> dev/breeze/src/airflow_breeze/utils/path_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "291: GENERATED_PROVIDER_PACKAGES_DIR = DIST_DIR / \"provider_packages\"",
          "292: DOCS_DIR = AIRFLOW_SOURCES_ROOT / \"docs\"",
          "293: SCRIPTS_CI_DIR = AIRFLOW_SOURCES_ROOT / \"scripts\" / \"ci\"",
          "294: SCRIPTS_CI_DOCKER_COMPOSE_DIR = SCRIPTS_CI_DIR / \"docker-compose\"",
          "295: SCRIPTS_CI_DOCKER_COMPOSE_LOCAL_YAML_FILE = SCRIPTS_CI_DOCKER_COMPOSE_DIR / \"local.yml\"",
          "296: GENERATED_DOCKER_COMPOSE_ENV_FILE = SCRIPTS_CI_DOCKER_COMPOSE_DIR / \"_generated_docker_compose.env\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "294: SCRIPTS_DOCKER_DIR = AIRFLOW_SOURCES_ROOT / \"scripts\" / \"docker\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "eefc0f88d0454871fb22a1f6deff584dd1f9b83a",
      "candidate_info": {
        "commit_hash": "eefc0f88d0454871fb22a1f6deff584dd1f9b83a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/eefc0f88d0454871fb22a1f6deff584dd1f9b83a",
        "files": [
          "Dockerfile.ci",
          "airflow/providers/apache/spark/provider.yaml",
          "generated/provider_dependencies.json"
        ],
        "message": "Bump min version for grpcio-status in spark provider (#36662)\n\nPreviously we limited grpcio minimum version to stop backtracking\nof `pip` from happening and we could not do it in the limits of\nspark provider, becaue some google dependencies used it and\nconflicted with it. This problem is now gone as we have newer\nversions of google dependencies and we can not only safely move\nit to spark provider but also bump it slightly higher to limit\nthe amount of backtracking we need to do.\n\nExtracted from #36537\n\n(cherry picked from commit ded01a5aba337882fb19e03c24d7736c7154fdd8)",
        "before_after_code_files": [
          "Dockerfile.ci||Dockerfile.ci"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "Dockerfile.ci||Dockerfile.ci": [
          "File: Dockerfile.ci -> Dockerfile.ci",
          "--- Hunk 1 ---",
          "[Context before]",
          "1116: # force them on the main Airflow package. Currently we need no extra limits as PIP 23.1+ has much better",
          "1117: # dependency resolution and we do not need to limit the versions of the dependencies",
          "1118: #",
          "1122: # Aiobotocore is limited for eager upgrade because it either causes a long backtracking or",
          "1123: # conflict when we do not limit it. It seems that `pip` has a hard time figuring the right",
          "1124: # combination of dependencies for aiobotocore, botocore, boto3 and s3fs together",
          "1125: #",
          "1127: ARG UPGRADE_TO_NEWER_DEPENDENCIES=\"false\"",
          "1128: ARG VERSION_SUFFIX_FOR_PYPI=\"\"",
          "",
          "[Removed Lines]",
          "1119: # Without grpcio-status limit, pip gets into very long backtracking",
          "1120: # We should attempt to remove it in the future",
          "1121: #",
          "1126: ARG EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=\"grpcio-status>=1.55.0 aiobotocore>=2.5.4\"",
          "",
          "[Added Lines]",
          "1123: ARG EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS=\"aiobotocore>=2.5.4\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cf83862ccb502f22b4294a813b792cb8b068ddbd",
      "candidate_info": {
        "commit_hash": "cf83862ccb502f22b4294a813b792cb8b068ddbd",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cf83862ccb502f22b4294a813b792cb8b068ddbd",
        "files": [
          ".rat-excludes",
          "3rd-party-licenses/LICENSE-reproducible.txt",
          "BREEZE.rst",
          "dev/README_RELEASE_AIRFLOW.md",
          "dev/breeze/src/airflow_breeze/commands/release_candidate_command.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/utils/reproducible.py",
          "images/breeze/output_release-management.svg",
          "images/breeze/output_release-management.txt",
          "images/breeze/output_release-management_prepare-airflow-package.svg",
          "images/breeze/output_release-management_prepare-airflow-package.txt",
          "images/breeze/output_release-management_prepare-airflow-tarball.svg",
          "images/breeze/output_release-management_prepare-airflow-tarball.txt",
          "images/breeze/output_setup_check-all-params-in-groups.svg",
          "images/breeze/output_setup_check-all-params-in-groups.txt",
          "images/breeze/output_setup_regenerate-command-images.svg",
          "images/breeze/output_setup_regenerate-command-images.txt",
          "scripts/in_container/run_prepare_airflow_packages.py"
        ],
        "message": "Update Airflow release process to include reproducible tarballs (#36744)\n\nSource tarball is the main artifact produced by the release\nprocess - one that is the \"official\" release and named like that\nby the Apache Software Foundation.\n\nThis PR makes the source tarball generation reproducible - following\nreproducibility of the `.whl` and `sdist` packages.\n\nThis change adds:\n\n* vendors-in reproducible.py script that repacks .tar.gz package\n  in reproducible way using source-date-epoch as timestamps\n* breeze release-management prepare-airflow-tarball command\n* adds verification of the tarballs to PMC verification process\n* adds --use-local-hatch for package building command to allow for\n  faster / non-docker build of packages for PMC verification\n* improves diagnostic output of the release and build commands\n\n(cherry picked from commit 72a571dc6d21d90f92d5ce683a5d40c6a527fcb0)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/commands/release_candidate_command.py||dev/breeze/src/airflow_breeze/commands/release_candidate_command.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "dev/breeze/src/airflow_breeze/utils/reproducible.py||dev/breeze/src/airflow_breeze/utils/reproducible.py",
          "scripts/in_container/run_prepare_airflow_packages.py||scripts/in_container/run_prepare_airflow_packages.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/commands/release_candidate_command.py||dev/breeze/src/airflow_breeze/commands/release_candidate_command.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_candidate_command.py -> dev/breeze/src/airflow_breeze/commands/release_candidate_command.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "17: from __future__ import annotations",
          "19: import os",
          "21: import click",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "20: import shutil",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "25: from airflow_breeze.utils.confirm import confirm_action",
          "26: from airflow_breeze.utils.console import console_print",
          "27: from airflow_breeze.utils.path_utils import AIRFLOW_SOURCES_ROOT",
          "28: from airflow_breeze.utils.run_utils import run_command",
          "30: CI = os.environ.get(\"CI\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "29: from airflow_breeze.utils.reproducible import archive_deterministically, get_source_date_epoch",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "59: def git_tag(version):",
          "60:     if confirm_action(f\"Tag {version}?\"):",
          "61:         run_command([\"git\", \"tag\", \"-s\", f\"{version}\", \"-m\", f\"Apache Airflow {version}\"], check=True)",
          "65: def git_clean():",
          "66:     if confirm_action(\"Clean git repo?\"):",
          "67:         run_command([\"breeze\", \"ci\", \"fix-ownership\"], dry_run_override=DRY_RUN, check=True)",
          "68:         run_command([\"git\", \"clean\", \"-fxd\"], dry_run_override=DRY_RUN, check=True)",
          "77:         run_command(",
          "78:             [",
          "79:                 \"git\",",
          "",
          "[Removed Lines]",
          "62:         console_print(\"Tagged\")",
          "69:         console_print(\"Git repo cleaned\")",
          "72: def tarball_release(version, version_without_rc):",
          "73:     if confirm_action(\"Create tarball?\"):",
          "74:         run_command([\"rm\", \"-rf\", \"dist\"], check=True)",
          "76:         run_command([\"mkdir\", \"dist\"], check=True)",
          "",
          "[Added Lines]",
          "64:         console_print(\"[success]Tagged\")",
          "71:         console_print(\"[success]Git repo cleaned\")",
          "74: DIST_DIR = AIRFLOW_SOURCES_ROOT / \"dist\"",
          "75: OUT_DIR = AIRFLOW_SOURCES_ROOT / \"out\"",
          "76: REPRODUCIBLE_DIR = OUT_DIR / \"reproducible\"",
          "79: def tarball_release(version: str, version_without_rc: str, source_date_epoch: int):",
          "80:     if confirm_action(\"Create tarball?\"):",
          "81:         console_print(f\"[info]Creating tarball for Airflow {version}\")",
          "82:         shutil.rmtree(OUT_DIR, ignore_errors=True)",
          "83:         DIST_DIR.mkdir(exist_ok=True)",
          "84:         OUT_DIR.mkdir(exist_ok=True)",
          "85:         REPRODUCIBLE_DIR.mkdir(exist_ok=True)",
          "86:         archive_name = f\"apache-airflow-{version_without_rc}-source.tar.gz\"",
          "87:         temporary_archive = OUT_DIR / archive_name",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "82:                 f\"{version}\",",
          "83:                 f\"--prefix=apache-airflow-{version_without_rc}/\",",
          "84:                 \"-o\",",
          "86:             ],",
          "87:             check=True,",
          "88:         )",
          "98:     run_command(",
          "99:         [",
          "100:             \"breeze\",",
          "",
          "[Removed Lines]",
          "85:                 f\"dist/apache-airflow-{version_without_rc}-source.tar.gz\",",
          "89:         console_print(\"Tarball created\")",
          "92: def create_artifacts_with_sdist():",
          "93:     run_command([\"hatch\", \"build\", \"-t\", \"sdist\", \"-t\", \"wheel\"], check=True)",
          "94:     console_print(\"Artifacts created\")",
          "97: def create_artifacts_with_breeze():",
          "",
          "[Added Lines]",
          "96:                 temporary_archive.as_posix(),",
          "100:         run_command(",
          "101:             [",
          "102:                 \"tar\",",
          "103:                 \"-xf\",",
          "104:                 temporary_archive.as_posix(),",
          "105:                 \"-C\",",
          "106:                 REPRODUCIBLE_DIR.as_posix(),",
          "107:                 \"--strip\",",
          "108:                 \"1\",",
          "109:             ]",
          "110:         )",
          "111:         final_archive = DIST_DIR / archive_name",
          "112:         archive_deterministically(",
          "113:             dir_to_archive=REPRODUCIBLE_DIR.as_posix(),",
          "114:             dest_archive=final_archive.as_posix(),",
          "115:             prepend_path=None,",
          "116:             timestamp=source_date_epoch,",
          "117:         )",
          "118:         console_print(f\"[success]Tarball created in {final_archive}\")",
          "121: def create_artifacts_with_hatch(source_date_epoch: int):",
          "122:     console_print(\"[info]Creating artifacts with hatch\")",
          "123:     shutil.rmtree(DIST_DIR, ignore_errors=True)",
          "124:     DIST_DIR.mkdir(exist_ok=True)",
          "125:     env_copy = os.environ.copy()",
          "126:     env_copy[\"SOURCE_DATE_EPOCH\"] = str(source_date_epoch)",
          "127:     run_command(",
          "128:         [\"hatch\", \"build\", \"-c\", \"-t\", \"custom\", \"-t\", \"sdist\", \"-t\", \"wheel\"], check=True, env=env_copy",
          "129:     )",
          "130:     console_print(\"[success]Successfully prepared Airflow packages:\")",
          "131:     for file in sorted(DIST_DIR.glob(\"apache_airflow*\")):",
          "132:         console_print(print(file.name))",
          "133:     console_print()",
          "136: def create_artifacts_with_docker():",
          "137:     console_print(\"[info]Creating artifacts with docker\")",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "105:         ],",
          "106:         check=True,",
          "107:     )",
          "111: def sign_the_release(repo_root):",
          "112:     if confirm_action(\"Do you want to sign the release?\"):",
          "113:         os.chdir(f\"{repo_root}/dist\")",
          "114:         run_command(\"./../dev/sign.sh *\", dry_run_override=DRY_RUN, check=True, shell=True)",
          "118: def tag_and_push_constraints(version, version_branch):",
          "",
          "[Removed Lines]",
          "108:     console_print(\"Artifacts created\")",
          "115:         console_print(\"Release signed\")",
          "",
          "[Added Lines]",
          "148:     console_print(\"[success]Artifacts created\")",
          "155:         console_print(\"[success]Release signed\")",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "135:         run_command(",
          "136:             [\"git\", \"push\", \"origin\", \"tag\", f\"constraints-{version}\"], dry_run_override=DRY_RUN, check=True",
          "137:         )",
          "141: def clone_asf_repo(version, repo_root):",
          "",
          "[Removed Lines]",
          "138:         console_print(\"Constraints tagged and pushed\")",
          "",
          "[Added Lines]",
          "178:         console_print(\"[success]Constraints tagged and pushed\")",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "146:             check=True,",
          "147:         )",
          "148:         run_command([\"svn\", \"update\", \"--set-depth=infinity\", \"asf-dist/dev/airflow\"], check=True)",
          "152: def move_artifacts_to_svn(version, repo_root):",
          "",
          "[Removed Lines]",
          "149:         console_print(\"Cloned ASF repo successfully\")",
          "",
          "[Added Lines]",
          "189:         console_print(\"[success]Cloned ASF repo successfully\")",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "154:         os.chdir(f\"{repo_root}/asf-dist/dev/airflow\")",
          "155:         run_command([\"svn\", \"mkdir\", f\"{version}\"], dry_run_override=DRY_RUN, check=True)",
          "156:         run_command(f\"mv {repo_root}/dist/* {version}/\", dry_run_override=DRY_RUN, check=True, shell=True)",
          "158:         run_command([\"ls\"], dry_run_override=DRY_RUN)",
          "",
          "[Removed Lines]",
          "157:         console_print(\"Moved artifacts to SVN:\")",
          "",
          "[Added Lines]",
          "197:         console_print(\"[success]Moved artifacts to SVN:\")",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "171:             dry_run_override=DRY_RUN,",
          "172:             check=True,",
          "173:         )",
          "177: def delete_asf_repo(repo_root):",
          "",
          "[Removed Lines]",
          "174:         console_print(\"Files pushed to svn\")",
          "",
          "[Added Lines]",
          "214:         console_print(\"[success]Files pushed to svn\")",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "183: def prepare_pypi_packages(version, version_suffix, repo_root):",
          "184:     if confirm_action(\"Prepare pypi packages?\"):",
          "186:         os.chdir(repo_root)",
          "187:         run_command([\"git\", \"checkout\", f\"{version}\"], dry_run_override=DRY_RUN, check=True)",
          "188:         run_command(",
          "",
          "[Removed Lines]",
          "185:         console_print(\"Preparing PyPI packages\")",
          "",
          "[Added Lines]",
          "225:         console_print(\"[info]Preparing PyPI packages\")",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "198:             check=True,",
          "199:         )",
          "200:         run_command([\"twine\", \"check\", \"dist/*\"], check=True)",
          "204: def push_packages_to_pypi(version):",
          "205:     if confirm_action(\"Do you want to push packages to production PyPI?\"):",
          "206:         run_command([\"twine\", \"upload\", \"-r\", \"pypi\", \"dist/*\"], dry_run_override=DRY_RUN, check=True)",
          "208:         console_print(",
          "209:             \"Again, confirm that the package is available here: https://pypi.python.org/pypi/apache-airflow\"",
          "210:         )",
          "",
          "[Removed Lines]",
          "201:         console_print(\"PyPI packages prepared\")",
          "207:         console_print(\"Packages pushed to production PyPI\")",
          "",
          "[Added Lines]",
          "241:         console_print(\"[success]PyPI packages prepared\")",
          "247:         console_print(\"[success]Packages pushed to production PyPI\")",
          "",
          "---------------",
          "--- Hunk 12 ---",
          "[Context before]",
          "240:         )",
          "241:         confirm_action(f\"Confirm that {version} is pushed to PyPI(not PyPI test). Is it pushed?\", abort=True)",
          "242:         run_command([\"git\", \"push\", \"origin\", \"tag\", f\"{version}\"], dry_run_override=DRY_RUN, check=True)",
          "246: def create_issue_for_testing(version, previous_version, github_token):",
          "",
          "[Removed Lines]",
          "243:         console_print(\"Release candidate tag pushed to GitHub\")",
          "",
          "[Added Lines]",
          "283:         console_print(\"[success]Release candidate tag pushed to GitHub\")",
          "",
          "---------------",
          "--- Hunk 13 ---",
          "[Context before]",
          "293:                 dry_run_override=DRY_RUN,",
          "294:                 check=True,",
          "295:             )",
          "297:     os.chdir(repo_root)",
          "300: @release_management.command(",
          "301:     name=\"start-rc-process\",",
          "302:     short_help=\"Start RC process\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "336:     console_print(\"[success]Old releases removed\")",
          "340: @release_management.command(",
          "341:     name=\"prepare-airflow-tarball\",",
          "342:     help=\"Prepare airflow's source tarball.\",",
          "343: )",
          "344: @click.option(",
          "345:     \"--version\", required=True, help=\"The release candidate version e.g. 2.4.3rc1\", envvar=\"VERSION\"",
          "346: )",
          "347: def prepare_airflow_tarball(version: str):",
          "348:     from packaging.version import Version",
          "350:     airflow_version = Version(version)",
          "351:     if not airflow_version.is_prerelease:",
          "352:         exit(\"--version value must be a pre-release\")",
          "353:     source_date_epoch = get_source_date_epoch()",
          "354:     version_without_rc = airflow_version.base_version",
          "355:     # Create the tarball",
          "356:     tarball_release(",
          "357:         version=version, version_without_rc=version_without_rc, source_date_epoch=source_date_epoch",
          "358:     )",
          "",
          "---------------",
          "--- Hunk 14 ---",
          "[Context before]",
          "311: def publish_release_candidate(version, previous_version, github_token):",
          "312:     from packaging.version import Version",
          "315:         exit(\"--version value must be a pre-release\")",
          "316:     if Version(previous_version).is_prerelease:",
          "317:         exit(\"--previous-version value must be a release not a pre-release\")",
          "",
          "[Removed Lines]",
          "314:     if not Version(version).is_prerelease:",
          "",
          "[Added Lines]",
          "375:     airflow_version = Version(version)",
          "376:     if not airflow_version.is_prerelease:",
          "",
          "---------------",
          "--- Hunk 15 ---",
          "[Context before]",
          "320:         if not github_token:",
          "321:             console_print(\"GITHUB_TOKEN is not set! Issue generation will fail.\")",
          "322:             confirm_action(\"Do you want to continue?\", abort=True)",
          "326:     os.chdir(AIRFLOW_SOURCES_ROOT)",
          "327:     airflow_repo_root = os.getcwd()",
          "",
          "[Removed Lines]",
          "323:     version_suffix = version[5:]",
          "324:     version_branch = version[:3].replace(\".\", \"-\")",
          "325:     version_without_rc = version[:5]",
          "",
          "[Added Lines]",
          "386:     version_suffix = airflow_version.pre[0] + str(airflow_version.pre[1])",
          "387:     version_branch = str(airflow_version.release[0]) + \"-\" + str(airflow_version.release[1])",
          "388:     version_without_rc = airflow_version.base_version",
          "",
          "---------------",
          "--- Hunk 16 ---",
          "[Context before]",
          "343:     confirm_action(\"Pushes will be made to origin. Do you want to continue?\", abort=True)",
          "344:     # Merge the sync PR",
          "345:     merge_pr(version_branch)",
          "348:     git_tag(version)",
          "349:     git_clean()",
          "353:     # Create the tarball",
          "355:     # Create the artifacts",
          "358:     elif confirm_action(\"Use hatch to create artifacts?\"):",
          "360:     # Sign the release",
          "361:     sign_the_release(airflow_repo_root)",
          "362:     # Tag and push constraints",
          "",
          "[Removed Lines]",
          "347:     # Tag & clean the repo",
          "350:     # Build the latest image",
          "351:     if confirm_action(\"Build latest breeze image?\"):",
          "352:         run_command([\"breeze\", \"ci-image\", \"build\", \"--python\", \"3.8\"], dry_run_override=DRY_RUN, check=True)",
          "354:     tarball_release(version, version_without_rc)",
          "356:     if confirm_action(\"Use breeze to create artifacts?\"):",
          "357:         create_artifacts_with_breeze()",
          "359:         create_artifacts_with_sdist()",
          "",
          "[Added Lines]",
          "409:     #",
          "410:     # # Tag & clean the repo",
          "413:     source_date_epoch = get_source_date_epoch()",
          "414:     shutil.rmtree(DIST_DIR, ignore_errors=True)",
          "416:     tarball_release(",
          "417:         version=version, version_without_rc=version_without_rc, source_date_epoch=source_date_epoch",
          "418:     )",
          "420:     if confirm_action(\"Use docker to create artifacts?\"):",
          "421:         create_artifacts_with_docker()",
          "423:         create_artifacts_with_hatch()",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "142:     get_related_providers,",
          "143: )",
          "144: from airflow_breeze.utils.python_versions import get_python_version_list",
          "145: from airflow_breeze.utils.run_utils import (",
          "146:     run_command,",
          "147: )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "145: from airflow_breeze.utils.reproducible import get_source_date_epoch",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "282:     name=\"prepare-airflow-package\",",
          "283:     help=\"Prepare sdist/whl package of Airflow.\",",
          "284: )",
          "285: @option_package_format",
          "286: @option_version_suffix_for_pypi",
          "287: @option_verbose",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "286: @click.option(",
          "287:     \"--use-local-hatch\",",
          "288:     is_flag=True,",
          "289:     help=\"Use local hatch instead of docker to build the package. You need to have hatch installed.\",",
          "290: )",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "289: def prepare_airflow_packages(",
          "290:     package_format: str,",
          "291:     version_suffix_for_pypi: str,",
          "292: ):",
          "293:     perform_environment_checks()",
          "294:     fix_ownership_using_docker()",
          "295:     cleanup_python_generated_files()",
          "296:     # This is security feature.",
          "297:     #",
          "298:     # Building the image needed to build airflow package including .git directory",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "298:     use_local_hatch: bool,",
          "303:     source_date_epoch = get_source_date_epoch()",
          "304:     if use_local_hatch:",
          "305:         hatch_build_command = [\"hatch\", \"build\", \"-c\", \"-t\", \"custom\"]",
          "306:         if package_format in [\"sdist\", \"both\"]:",
          "307:             hatch_build_command.extend([\"-t\", \"sdist\"])",
          "308:         if package_format in [\"wheel\", \"both\"]:",
          "309:             hatch_build_command.extend([\"-t\", \"wheel\"])",
          "310:         env_copy = os.environ.copy()",
          "311:         env_copy[\"SOURCE_DATE_EPOCH\"] = str(source_date_epoch)",
          "312:         run_command(",
          "313:             hatch_build_command,",
          "314:             check=True,",
          "315:             env=env_copy,",
          "316:         )",
          "317:         get_console().print(\"[success]Successfully prepared Airflow packages:\")",
          "318:         for file in sorted(DIST_DIR.glob(\"apache_airflow*\")):",
          "319:             get_console().print(file.name)",
          "320:         get_console().print()",
          "321:         return",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "350:     # Copy all files in the dist directory in container to the host dist directory (note '/.' in SRC)",
          "351:     run_command([\"docker\", \"cp\", f\"{container_id}:/opt/airflow/dist/.\", \"./dist\"], check=True)",
          "352:     run_command([\"docker\", \"rm\", \"--force\", container_id], check=True)",
          "356: def provider_action_summary(description: str, message_type: MessageType, packages: list[str]):",
          "",
          "[Removed Lines]",
          "353:     get_console().print(\"[success]Successfully prepared Airflow package!\\n\\n\")",
          "",
          "[Added Lines]",
          "379:     get_console().print(\"[success]Successfully prepared Airflow packages:\")",
          "380:     for file in sorted(DIST_DIR.glob(\"apache_airflow*\")):",
          "381:         get_console().print(file.name)",
          "382:     get_console().print()",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py||dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands_config.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "19: RELEASE_AIRFLOW_COMMANDS: dict[str, str | list[str]] = {",
          "20:     \"name\": \"Airflow release commands\",",
          "21:     \"commands\": [",
          "23:         \"create-minor-branch\",",
          "24:         \"start-rc-process\",",
          "25:         \"start-release\",",
          "26:         \"release-prod-images\",",
          "",
          "[Removed Lines]",
          "22:         \"prepare-airflow-package\",",
          "",
          "[Added Lines]",
          "23:         \"prepare-airflow-package\",",
          "24:         \"prepare-airflow-tarball\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "43: RELEASE_OTHER_COMMANDS: dict[str, str | list[str]] = {",
          "44:     \"name\": \"Other release commands\",",
          "45:     \"commands\": [",
          "46:         \"publish-docs\",",
          "47:         \"generate-constraints\",",
          "49:     ],",
          "50: }",
          "",
          "[Removed Lines]",
          "48:         \"add-back-references\",",
          "",
          "[Added Lines]",
          "47:         \"add-back-references\",",
          "50:         \"update-constraints\",",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "55:             \"name\": \"Package flags\",",
          "56:             \"options\": [",
          "57:                 \"--package-format\",",
          "58:                 \"--version-suffix-for-pypi\",",
          "59:             ],",
          "60:         }",
          "61:     ],",
          "62:     \"breeze release-management verify-provider-packages\": [",
          "63:         {",
          "64:             \"name\": \"Provider verification flags\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "60:                 \"--use-local-hatch\",",
          "65:     \"breeze release-management prepare-airflow-tarball\": [",
          "66:         {",
          "67:             \"name\": \"Package flags\",",
          "68:             \"options\": [",
          "69:                 \"--version\",",
          "70:             ],",
          "71:         }",
          "72:     ],",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/reproducible.py||dev/breeze/src/airflow_breeze/utils/reproducible.py": [
          "File: dev/breeze/src/airflow_breeze/utils/reproducible.py -> dev/breeze/src/airflow_breeze/utils/reproducible.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python3",
          "4: # Licensed to the Apache Software Foundation (ASF) under one",
          "5: # or more contributor license agreements.  See the NOTICE file",
          "6: # distributed with this work for additional information",
          "7: # regarding copyright ownership.  The ASF licenses this file",
          "8: # to you under the Apache License, Version 2.0 (the",
          "9: # \"License\"); you may not use this file except in compliance",
          "10: # with the License.  You may obtain a copy of the License at",
          "11: #",
          "12: #   http://www.apache.org/licenses/LICENSE-2.0",
          "13: #",
          "14: # Unless required by applicable law or agreed to in writing,",
          "15: # software distributed under the License is distributed on an",
          "16: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "17: # KIND, either express or implied.  See the License for the",
          "18: # specific language governing permissions and limitations",
          "19: # under the License.",
          "21: # Copyright 2013 The Servo Project Developers.",
          "22: # Copyright 2017 zerolib Developers.",
          "23: #",
          "24: # Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or",
          "25: # http://www.apache.org/licenses/LICENSE-2.0> or the MIT license",
          "26: # <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your",
          "27: # option. This file may not be copied, modified, or distributed",
          "28: # except according to those terms.",
          "30: # This command is a largely vendored-in script from",
          "31: # https://github.com/MuxZeroNet/reproducible/blob/master/reproducible.py",
          "32: from __future__ import annotations",
          "34: import contextlib",
          "35: import gzip",
          "36: import itertools",
          "37: import locale",
          "38: import os",
          "39: import tarfile",
          "40: from argparse import ArgumentParser",
          "42: from airflow_breeze.utils.path_utils import AIRFLOW_SOURCES_ROOT",
          "45: def get_source_date_epoch():",
          "46:     import yaml",
          "48:     reproducible_build_yaml = AIRFLOW_SOURCES_ROOT / \"airflow\" / \"reproducible_build.yaml\"",
          "49:     reproducible_build_dict = yaml.safe_load(reproducible_build_yaml.read_text())",
          "50:     source_date_epoch: int = reproducible_build_dict[\"source-date-epoch\"]",
          "51:     return source_date_epoch",
          "54: @contextlib.contextmanager",
          "55: def cd(new_path):",
          "56:     \"\"\"Context manager for changing the current working directory\"\"\"",
          "57:     previous_path = os.getcwd()",
          "58:     try:",
          "59:         os.chdir(new_path)",
          "60:         yield",
          "61:     finally:",
          "62:         os.chdir(previous_path)",
          "65: @contextlib.contextmanager",
          "66: def setlocale(name):",
          "67:     \"\"\"Context manager for changing the current locale\"\"\"",
          "68:     saved_locale = locale.setlocale(locale.LC_ALL)",
          "69:     try:",
          "70:         yield locale.setlocale(locale.LC_ALL, name)",
          "71:     finally:",
          "72:         locale.setlocale(locale.LC_ALL, saved_locale)",
          "75: def archive_deterministically(dir_to_archive, dest_archive, prepend_path=None, timestamp=0):",
          "76:     \"\"\"Create a .tar.gz archive in a deterministic (reproducible) manner.",
          "78:     See https://reproducible-builds.org/docs/archives/ for more details.\"\"\"",
          "80:     def reset(tarinfo):",
          "81:         \"\"\"Helper to reset owner/group and modification time for tar entries\"\"\"",
          "82:         tarinfo.uid = tarinfo.gid = 0",
          "83:         tarinfo.uname = tarinfo.gname = \"root\"",
          "84:         tarinfo.mtime = timestamp",
          "85:         return tarinfo",
          "87:     dest_archive = os.path.abspath(dest_archive)",
          "88:     with cd(dir_to_archive):",
          "89:         current_dir = \".\"",
          "90:         file_list = [current_dir]",
          "91:         for root, dirs, files in os.walk(current_dir):",
          "92:             for name in itertools.chain(dirs, files):",
          "93:                 file_list.append(os.path.join(root, name))",
          "95:         # Sort file entries with the fixed locale",
          "96:         with setlocale(\"C\"):",
          "97:             file_list.sort(key=locale.strxfrm)",
          "99:         # Use a temporary file and atomic rename to avoid partially-formed",
          "100:         # packaging (in case of exceptional situations like running out of disk space).",
          "101:         temp_file = f\"{dest_archive}.temp~\"",
          "102:         with os.fdopen(os.open(temp_file, os.O_WRONLY | os.O_CREAT, 0o644), \"wb\") as out_file:",
          "103:             with gzip.GzipFile(\"wb\", fileobj=out_file, mtime=0) as gzip_file:",
          "104:                 with tarfile.open(fileobj=gzip_file, mode=\"w:\") as tar_file:",
          "105:                     for entry in file_list:",
          "106:                         arcname = entry",
          "107:                         if prepend_path is not None:",
          "108:                             arcname = os.path.normpath(os.path.join(prepend_path, arcname))",
          "109:                         tar_file.add(entry, filter=reset, recursive=False, arcname=arcname)",
          "110:         os.rename(temp_file, dest_archive)",
          "113: def main():",
          "114:     parser = ArgumentParser()",
          "115:     parser.add_argument(\"-d\", \"--dir\", help=\"directory to archive\")",
          "116:     parser.add_argument(\"-o\", \"--out\", help=\"archive destination\")",
          "117:     parser.add_argument(\"-p\", \"--prepend\", help=\"prepend path\")",
          "118:     parser.add_argument(",
          "119:         \"-t\", \"--timestamp\", help=\"timestamp of files\", type=int, default=get_source_date_epoch()",
          "120:     )",
          "122:     args = parser.parse_args()",
          "124:     if not args.dir or not args.out:",
          "125:         error = (",
          "126:             \"You should provide a directory to archive, and the \"",
          "127:             f\"archive file name, not {repr((args.dir, args.out))}\"",
          "128:         )",
          "129:         raise ValueError(error)",
          "131:     archive_deterministically(args.dir, args.out, args.prepend, args.timestamp)",
          "134: if __name__ == \"__main__\":",
          "135:     main()",
          "",
          "---------------"
        ],
        "scripts/in_container/run_prepare_airflow_packages.py||scripts/in_container/run_prepare_airflow_packages.py": [
          "File: scripts/in_container/run_prepare_airflow_packages.py -> scripts/in_container/run_prepare_airflow_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "78: def build_airflow_packages(package_format: str):",
          "79:     build_command = [sys.executable, \"-m\", \"hatch\", \"build\", \"-t\", \"custom\"]",
          "83:     if package_format in [\"both\", \"sdist\"]:",
          "84:         build_command.extend([\"-t\", \"sdist\"])",
          "86:     reproducible_date = yaml.safe_load(REPRODUCIBLE_BUILD_FILE.read_text())[\"source-date-epoch\"]",
          "",
          "[Removed Lines]",
          "81:     if package_format in [\"both\", \"wheel\"]:",
          "82:         build_command.extend([\"-t\", \"wheel\"])",
          "",
          "[Added Lines]",
          "82:     if package_format in [\"both\", \"wheel\"]:",
          "83:         build_command.extend([\"-t\", \"wheel\"])",
          "",
          "---------------"
        ]
      }
    }
  ]
}