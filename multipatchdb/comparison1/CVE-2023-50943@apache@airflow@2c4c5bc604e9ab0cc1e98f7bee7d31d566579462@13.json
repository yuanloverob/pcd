{
  "cve_id": "CVE-2023-50943",
  "cve_desc": "Apache Airflow, versions before 2.8.1, have a vulnerability that allows a potential attacker to poison the XCom data by bypassing the protection of \"enable_xcom_pickling=False\" configuration setting resulting in poisoned data after XCom deserialization. This vulnerability is considered low since it requires a DAG author to exploit it. Users are recommended to upgrade to version 2.8.1 or later, which fixes this issue.",
  "repo": "apache/airflow",
  "patch_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
  "patch_info": {
    "commit_hash": "2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/2c4c5bc604e9ab0cc1e98f7bee7d31d566579462",
    "files": [
      "airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py"
    ],
    "message": "Stop deserializing pickle when enable_xcom_pickling is False (#36255)\n\n* Stop deserializing pickle when enable_xcom_pickling is False\n\n* Fix unit tests\n\n(cherry picked from commit 63e97abec5d56bc62a293c93f5227f364561e51c)",
    "before_after_code_files": [
      "airflow/models/xcom.py||airflow/models/xcom.py",
      "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py",
      "tests/models/test_xcom.py||tests/models/test_xcom.py"
    ]
  },
  "patch_diff": {
    "airflow/models/xcom.py||airflow/models/xcom.py": [
      "File: airflow/models/xcom.py -> airflow/models/xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "685:             except pickle.UnpicklingError:",
      "686:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "687:         else:",
      "693:     @staticmethod",
      "694:     def deserialize_value(result: XCom) -> Any:",
      "",
      "[Removed Lines]",
      "688:             try:",
      "689:                 return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "690:             except (json.JSONDecodeError, UnicodeDecodeError):",
      "691:                 return pickle.loads(result.value)",
      "",
      "[Added Lines]",
      "688:             # Since xcom_pickling is disabled, we should only try to deserialize with JSON",
      "689:             return json.loads(result.value.decode(\"UTF-8\"), cls=XComDecoder, object_hook=object_hook)",
      "",
      "---------------"
    ],
    "tests/api_connexion/schemas/test_xcom_schema.py||tests/api_connexion/schemas/test_xcom_schema.py": [
      "File: tests/api_connexion/schemas/test_xcom_schema.py -> tests/api_connexion/schemas/test_xcom_schema.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "30: from airflow.models import DagRun, XCom",
      "31: from airflow.utils.dates import parse_execution_date",
      "32: from airflow.utils.session import create_session",
      "34: pytestmark = pytest.mark.db_test",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "33: from tests.test_utils.config import conf_vars",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "188:     default_time = \"2016-04-02T21:00:00+00:00\"",
      "189:     default_time_parsed = parse_execution_date(default_time)",
      "191:     def test_serialize(self, create_xcom, session):",
      "192:         create_xcom(",
      "193:             dag_id=\"test_dag\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "192:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------",
      "--- Hunk 3 ---",
      "[Context before]",
      "208:             \"map_index\": -1,",
      "209:         }",
      "211:     def test_deserialize(self):",
      "212:         xcom_dump = {",
      "213:             \"key\": \"test_key\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "213:     @conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"})",
      "",
      "---------------"
    ],
    "tests/models/test_xcom.py||tests/models/test_xcom.py": [
      "File: tests/models/test_xcom.py -> tests/models/test_xcom.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "140:             ret_value = XCom.get_value(key=\"xcom_test3\", ti_key=ti_key, session=session)",
      "141:         assert ret_value == {\"key\": \"value\"}",
      "144:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"True\"}):",
      "145:             XCom.set(",
      "146:                 key=\"xcom_test3\",",
      "",
      "[Removed Lines]",
      "143:     def test_xcom_deserialize_with_pickle_to_json_switch(self, task_instance, session):",
      "",
      "[Added Lines]",
      "143:     def test_xcom_deserialize_pickle_when_xcom_pickling_is_disabled(self, task_instance, session):",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "151:                 session=session,",
      "152:             )",
      "153:         with conf_vars({(\"core\", \"enable_xcom_pickling\"): \"False\"}):",
      "163:     @conf_vars({(\"core\", \"xcom_enable_pickling\"): \"False\"})",
      "164:     def test_xcom_disable_pickle_type_fail_on_non_json(self, task_instance, session):",
      "",
      "[Removed Lines]",
      "154:             ret_value = XCom.get_one(",
      "155:                 key=\"xcom_test3\",",
      "156:                 dag_id=task_instance.dag_id,",
      "157:                 task_id=task_instance.task_id,",
      "158:                 run_id=task_instance.run_id,",
      "159:                 session=session,",
      "160:             )",
      "161:         assert ret_value == {\"key\": \"value\"}",
      "",
      "[Added Lines]",
      "154:             with pytest.raises(UnicodeDecodeError):",
      "155:                 XCom.get_one(",
      "156:                     key=\"xcom_test3\",",
      "157:                     dag_id=task_instance.dag_id,",
      "158:                     task_id=task_instance.task_id,",
      "159:                     run_id=task_instance.run_id,",
      "160:                     session=session,",
      "161:                 )",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "1e56cfade7aaf28eb8e55f1173a0fd85d225a47a",
      "candidate_info": {
        "commit_hash": "1e56cfade7aaf28eb8e55f1173a0fd85d225a47a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/1e56cfade7aaf28eb8e55f1173a0fd85d225a47a",
        "files": [
          "tests/conftest.py"
        ],
        "message": "Disable `PytestDeprecationWarning` when create warnings_recorder (#36759)\n\n(cherry picked from commit 70cefebbd5e20989b48386742089ebf747d991c1)",
        "before_after_code_files": [
          "tests/conftest.py||tests/conftest.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "tests/conftest.py||tests/conftest.py": [
          "File: tests/conftest.py -> tests/conftest.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "30: import pytest",
          "31: import time_machine",
          "34: # We should set these before loading _any_ of the rest of airflow so that the",
          "35: # unit test mode config is set as early as possible.",
          "",
          "[Removed Lines]",
          "32: from _pytest.recwarn import WarningsRecorder",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "1149: captured_warnings: dict[tuple[str, int, type[Warning], str], warnings.WarningMessage] = {}",
          "1150: captured_warnings_count: dict[tuple[str, int, type[Warning], str], int] = {}",
          "1152: default_formatwarning = warnings_recorder._module.formatwarning  # type: ignore[attr-defined]",
          "1153: default_showwarning = warnings_recorder._module.showwarning  # type: ignore[attr-defined]",
          "",
          "[Removed Lines]",
          "1151: warnings_recorder = WarningsRecorder()",
          "",
          "[Added Lines]",
          "1150: # By set ``_ispytest=True`` in WarningsRecorder we suppress annoying warnings:",
          "1151: # PytestDeprecationWarning: A private pytest class or function was used.",
          "1152: warnings_recorder = pytest.WarningsRecorder(_ispytest=True)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "c730881000f479f6b5f2844e3e2beebf75980bf3",
      "candidate_info": {
        "commit_hash": "c730881000f479f6b5f2844e3e2beebf75980bf3",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/c730881000f479f6b5f2844e3e2beebf75980bf3",
        "files": [
          "BREEZE.rst",
          "dev/breeze/README.md",
          "dev/breeze/pyproject.toml",
          "scripts/ci/install_breeze.sh"
        ],
        "message": "Upgrade to latest versions of `pip` and `pipx` in CI runners (#36646)\n\nThe CI runners did not have latest version of `pip` and `pipx`. This\nchange updates the installation scripts to fix `pip` to the same\nversion as in the CI image and down-binds pipx to 1.4.1 which is\nrecently released bugfix version with better logging and installation\ninstructions.\n\n(cherry picked from commit 75bc05ce1f53de112f7eee7be524a26f2a3f6845)",
        "before_after_code_files": [
          "scripts/ci/install_breeze.sh||scripts/ci/install_breeze.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/install_breeze.sh||scripts/ci/install_breeze.sh": [
          "File: scripts/ci/install_breeze.sh -> scripts/ci/install_breeze.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "20: cd \"$( dirname \"${BASH_SOURCE[0]}\" )/../../\"",
          "23: python -m pipx install --editable ./dev/breeze/ --force",
          "24: echo '/home/runner/.local/bin' >> \"${GITHUB_PATH}\"",
          "",
          "[Removed Lines]",
          "22: python -m pip install \"pipx>=1.2.1\"",
          "",
          "[Added Lines]",
          "22: python -m pip install --upgrade pip==23.3.2",
          "23: python -m pip install \"pipx>=1.4.1\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "ae0ceb382dc0dfb91adc5d22b480c321ffbe3fe0",
      "candidate_info": {
        "commit_hash": "ae0ceb382dc0dfb91adc5d22b480c321ffbe3fe0",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/ae0ceb382dc0dfb91adc5d22b480c321ffbe3fe0",
        "files": [
          ".pre-commit-config.yaml",
          ".rat-excludes",
          "CONTRIBUTING.rst",
          "docs/apache-airflow-providers-fab/img/diagram_fab_auth_manager_airflow_architecture.png",
          "docs/apache-airflow/img/diagram_auth_manager_airflow_architecture.md5sum",
          "docs/apache-airflow/img/diagram_auth_manager_airflow_architecture.png",
          "docs/apache-airflow/img/diagram_auth_manager_airflow_architecture.py",
          "docs/apache-airflow/img/diagram_basic_airflow_architecture.md5sum",
          "docs/apache-airflow/img/diagram_basic_airflow_architecture.py",
          "docs/apache-airflow/img/diagram_dag_processor_airflow_architecture.md5sum",
          "docs/apache-airflow/img/diagram_dag_processor_airflow_architecture.py",
          "docs/apache-airflow/img/diagram_fab_auth_manager_airflow_architecture.md5sum",
          "docs/apache-airflow/img/diagram_fab_auth_manager_airflow_architecture.png",
          "docs/apache-airflow/img/diagram_fab_auth_manager_airflow_architecture.py",
          "docs/diagrams/python_multiprocess_logo.png",
          "images/diagrams/python_multiprocess_logo.png",
          "scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py"
        ],
        "message": "Improve pre-commit to generate Airflow diagrams as a code (#36333)\n\nSince we are getting more diagrams generated in Airflow using the\n\"diagram as a code\" approach, this PR improves the pre-commit to be\nmore suitable to support generation of more of the images coming\nfrom different sources, placed in different directories and generated\nindependently, so that the whole process is more distributed and easy\nfor whoever creates diagrams to add their own diagram.\n\nThe changes implemented in this PR:\n\n* the code to generate the diagrams is now next to the diagram they\n  generate. It has the same name as the diagram, but it has the .py\n  extension. This way it is immediately visible where is the source\n  of each diagram (right next to each diagram)\n\n* each of the .py diagram Python files is runnable on its own. This\n  way you can easily regenerate the diagrams by running corresponding\n  Python file or even automate it by running \"save\" action and generate\n  the diagrams automatically by running the Python code every time\n  the file is saved. That makes a very nice workflow on iterating on\n  each diagram, independently from each othere\n\n* the pre-commit script is given a set of folders which should be\n  scanned and it finds and run the diagrams on pre-commmit. It also\n  creates and verifies the md5sum hash of the source Python file\n  separately for each diagram and only runs diagram generation when\n  the source file changed vs. last time the hash was saved and\n  committed. The hash sum is stored next to the image and sources\n  with .md5sum extension\n\nAlso updated documentation in the CONTRIBUTING.rst explaining how\nto generate the diagrams and what is the mechanism of that\ngeneration.\n\n(cherry picked from commit b35b08ec41814b6fe5d7388296db83a726e6d6d0)",
        "before_after_code_files": [
          "scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py||scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py||scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py": [
          "File: scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py -> scripts/ci/pre_commit/pre_commit_generate_airflow_diagrams.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "18: from __future__ import annotations",
          "20: import hashlib",
          "22: from pathlib import Path",
          "29: from rich.console import Console",
          "31: console = Console(width=400, color_system=\"standard\")",
          "33: LOCAL_DIR = Path(__file__).parent",
          "34: AIRFLOW_SOURCES_ROOT = Path(__file__).parents[3]",
          "124: def main():",
          "138: if __name__ == \"__main__\":",
          "",
          "[Removed Lines]",
          "21: import os",
          "24: from diagrams import Cluster, Diagram, Edge",
          "25: from diagrams.custom import Custom",
          "26: from diagrams.onprem.client import User",
          "27: from diagrams.onprem.database import PostgreSQL",
          "28: from diagrams.programming.flowchart import MultipleDocuments",
          "35: DOCS_IMAGES_DIR = AIRFLOW_SOURCES_ROOT / \"docs\" / \"apache-airflow\" / \"img\"",
          "36: PYTHON_MULTIPROCESS_LOGO = AIRFLOW_SOURCES_ROOT / \"images\" / \"diagrams\" / \"python_multiprocess_logo.png\"",
          "38: BASIC_ARCHITECTURE_IMAGE_NAME = \"diagram_basic_airflow_architecture\"",
          "39: DAG_PROCESSOR_AIRFLOW_ARCHITECTURE_IMAGE_NAME = \"diagram_dag_processor_airflow_architecture\"",
          "40: DIAGRAM_HASH_FILE_NAME = \"diagram_hash.txt\"",
          "43: def generate_basic_airflow_diagram(filename: str):",
          "44:     basic_architecture_image_file = (DOCS_IMAGES_DIR / BASIC_ARCHITECTURE_IMAGE_NAME).with_suffix(\".png\")",
          "45:     console.print(f\"[bright_blue]Generating architecture image {basic_architecture_image_file}\")",
          "46:     with Diagram(name=\"\", show=False, direction=\"LR\", curvestyle=\"ortho\", filename=filename):",
          "47:         with Cluster(\"Parsing & Scheduling\"):",
          "48:             schedulers = Custom(\"Scheduler(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "50:         metadata_db = PostgreSQL(\"Metadata DB\")",
          "52:         dag_author = User(\"DAG Author\")",
          "53:         dag_files = MultipleDocuments(\"DAG files\")",
          "55:         dag_author >> Edge(color=\"black\", style=\"dashed\", reverse=False) >> dag_files",
          "57:         with Cluster(\"Execution\"):",
          "58:             workers = Custom(\"Worker(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "59:             triggerer = Custom(\"Triggerer(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "61:         schedulers - Edge(color=\"blue\", style=\"dashed\", taillabel=\"Executor\") - workers",
          "63:         schedulers >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> metadata_db",
          "64:         workers >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> metadata_db",
          "65:         triggerer >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> metadata_db",
          "67:         operations_user = User(\"Operations User\")",
          "68:         with Cluster(\"UI\"):",
          "69:             webservers = Custom(\"Webserver(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "71:         webservers >> Edge(color=\"black\", style=\"dashed\", reverse=True) >> operations_user",
          "73:         metadata_db >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> webservers",
          "75:         dag_files >> Edge(color=\"brown\", style=\"solid\") >> workers",
          "76:         dag_files >> Edge(color=\"brown\", style=\"solid\") >> schedulers",
          "77:         dag_files >> Edge(color=\"brown\", style=\"solid\") >> triggerer",
          "78:     console.print(f\"[green]Generating architecture image {basic_architecture_image_file}\")",
          "81: def generate_dag_processor_airflow_diagram(filename: str):",
          "82:     dag_processor_architecture_image_file = (",
          "83:         DOCS_IMAGES_DIR / DAG_PROCESSOR_AIRFLOW_ARCHITECTURE_IMAGE_NAME",
          "84:     ).with_suffix(\".png\")",
          "85:     console.print(f\"[bright_blue]Generating architecture image {dag_processor_architecture_image_file}\")",
          "86:     with Diagram(name=\"\", show=False, direction=\"LR\", curvestyle=\"ortho\", filename=filename):",
          "87:         operations_user = User(\"Operations User\")",
          "88:         with Cluster(\"No DAG Python Code Execution\", graph_attr={\"bgcolor\": \"lightgrey\"}):",
          "89:             with Cluster(\"Scheduling\"):",
          "90:                 schedulers = Custom(\"Scheduler(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "92:             with Cluster(\"UI\"):",
          "93:                 webservers = Custom(\"Webserver(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "95:         webservers >> Edge(color=\"black\", style=\"dashed\", reverse=True) >> operations_user",
          "97:         metadata_db = PostgreSQL(\"Metadata DB\")",
          "99:         dag_author = User(\"DAG Author\")",
          "100:         with Cluster(\"DAG Python Code Execution\"):",
          "101:             with Cluster(\"Execution\"):",
          "102:                 workers = Custom(\"Worker(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "103:                 triggerer = Custom(\"Triggerer(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "104:             with Cluster(\"Parsing\"):",
          "105:                 dag_processors = Custom(\"DAG\\nProcessor(s)\", PYTHON_MULTIPROCESS_LOGO.as_posix())",
          "106:             dag_files = MultipleDocuments(\"DAG files\")",
          "108:         dag_author >> Edge(color=\"black\", style=\"dashed\", reverse=False) >> dag_files",
          "110:         workers - Edge(color=\"blue\", style=\"dashed\", headlabel=\"Executor\") - schedulers",
          "112:         metadata_db >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> webservers",
          "113:         metadata_db >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> schedulers",
          "114:         dag_processors >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> metadata_db",
          "115:         workers >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> metadata_db",
          "116:         triggerer >> Edge(color=\"red\", style=\"dotted\", reverse=True) >> metadata_db",
          "118:         dag_files >> Edge(color=\"brown\", style=\"solid\") >> workers",
          "119:         dag_files >> Edge(color=\"brown\", style=\"solid\") >> dag_processors",
          "120:         dag_files >> Edge(color=\"brown\", style=\"solid\") >> triggerer",
          "121:     console.print(f\"[green]Generating architecture image {dag_processor_architecture_image_file}\")",
          "125:     hash_md5 = hashlib.md5()",
          "126:     hash_md5.update(Path(__file__).resolve().read_bytes())",
          "127:     my_file_hash = hash_md5.hexdigest()",
          "128:     hash_file = LOCAL_DIR / DIAGRAM_HASH_FILE_NAME",
          "129:     if not hash_file.exists() or not hash_file.read_text().strip() == str(my_file_hash).strip():",
          "130:         os.chdir(DOCS_IMAGES_DIR)",
          "131:         generate_basic_airflow_diagram(BASIC_ARCHITECTURE_IMAGE_NAME)",
          "132:         generate_dag_processor_airflow_diagram(DAG_PROCESSOR_AIRFLOW_ARCHITECTURE_IMAGE_NAME)",
          "133:         hash_file.write_text(str(my_file_hash) + \"\\n\")",
          "134:     else:",
          "135:         console.print(\"[bright_blue]No changes to generation script. Not regenerating the images.\")",
          "",
          "[Added Lines]",
          "21: import subprocess",
          "22: import sys",
          "33: def _get_file_hash(file_to_check: Path) -> str:",
          "34:     hash_md5 = hashlib.md5()",
          "35:     hash_md5.update(Path(file_to_check).resolve().read_bytes())",
          "36:     return hash_md5.hexdigest()",
          "40:     # get all files as arguments",
          "41:     for arg in sys.argv[1:]:",
          "42:         source_file = Path(arg).resolve()",
          "43:         checksum = _get_file_hash(source_file)",
          "44:         hash_file = source_file.with_suffix(\".md5sum\")",
          "45:         if not hash_file.exists() or not hash_file.read_text().strip() == str(checksum).strip():",
          "46:             console.print(f\"[bright_blue]Changes in {source_file}. Regenerating the image.\")",
          "47:             subprocess.run(",
          "48:                 [sys.executable, source_file.resolve().as_posix()], check=True, cwd=source_file.parent",
          "49:             )",
          "50:             hash_file.write_text(str(checksum) + \"\\n\")",
          "51:         else:",
          "52:             console.print(f\"[bright_blue]No changes in {source_file}. Not regenerating the image.\")",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "23d06bed8eaf99e4c44c4ba46c05238de1ed8c91",
      "candidate_info": {
        "commit_hash": "23d06bed8eaf99e4c44c4ba46c05238de1ed8c91",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/23d06bed8eaf99e4c44c4ba46c05238de1ed8c91",
        "files": [
          ".github/ISSUE_TEMPLATE/airflow_providers_bug_report.yml",
          "PROVIDERS.rst",
          "airflow/provider.yaml.schema.json",
          "airflow/providers/MANAGING_PROVIDERS_LIFECYCLE.rst",
          "airflow/providers/airbyte/provider.yaml",
          "airflow/providers/alibaba/provider.yaml",
          "airflow/providers/amazon/provider.yaml",
          "airflow/providers/apache/beam/provider.yaml",
          "airflow/providers/apache/cassandra/provider.yaml",
          "airflow/providers/apache/drill/provider.yaml",
          "airflow/providers/apache/druid/provider.yaml",
          "airflow/providers/apache/flink/provider.yaml",
          "airflow/providers/apache/hdfs/provider.yaml",
          "airflow/providers/apache/hive/provider.yaml",
          "airflow/providers/apache/impala/provider.yaml",
          "airflow/providers/apache/kafka/provider.yaml",
          "airflow/providers/apache/kylin/provider.yaml",
          "airflow/providers/apache/livy/provider.yaml",
          "airflow/providers/apache/pig/provider.yaml",
          "airflow/providers/apache/pinot/provider.yaml",
          "airflow/providers/apache/spark/provider.yaml",
          "airflow/providers/apache/sqoop/provider.yaml",
          "airflow/providers/apprise/provider.yaml",
          "airflow/providers/arangodb/provider.yaml",
          "airflow/providers/asana/provider.yaml",
          "airflow/providers/atlassian/jira/provider.yaml",
          "airflow/providers/celery/provider.yaml",
          "airflow/providers/cloudant/provider.yaml",
          "airflow/providers/cncf/kubernetes/provider.yaml",
          "airflow/providers/cohere/provider.yaml",
          "airflow/providers/common/io/provider.yaml",
          "airflow/providers/common/sql/provider.yaml",
          "airflow/providers/daskexecutor/provider.yaml",
          "airflow/providers/databricks/provider.yaml",
          "airflow/providers/datadog/provider.yaml",
          "airflow/providers/dbt/cloud/provider.yaml",
          "airflow/providers/dingding/provider.yaml",
          "airflow/providers/discord/provider.yaml",
          "airflow/providers/docker/provider.yaml",
          "airflow/providers/elasticsearch/provider.yaml",
          "airflow/providers/exasol/provider.yaml",
          "airflow/providers/facebook/provider.yaml",
          "airflow/providers/ftp/provider.yaml",
          "airflow/providers/github/provider.yaml",
          "airflow/providers/google/provider.yaml",
          "airflow/providers/grpc/provider.yaml",
          "airflow/providers/hashicorp/provider.yaml",
          "airflow/providers/http/provider.yaml",
          "airflow/providers/imap/provider.yaml",
          "airflow/providers/influxdb/provider.yaml",
          "airflow/providers/jdbc/provider.yaml",
          "airflow/providers/jenkins/provider.yaml",
          "airflow/providers/microsoft/azure/provider.yaml",
          "airflow/providers/microsoft/mssql/provider.yaml",
          "airflow/providers/microsoft/psrp/provider.yaml",
          "airflow/providers/microsoft/winrm/provider.yaml",
          "airflow/providers/mongo/provider.yaml",
          "airflow/providers/mysql/provider.yaml",
          "airflow/providers/neo4j/provider.yaml",
          "airflow/providers/odbc/provider.yaml",
          "airflow/providers/openai/provider.yaml",
          "airflow/providers/openfaas/provider.yaml",
          "airflow/providers/openlineage/provider.yaml",
          "airflow/providers/opensearch/provider.yaml",
          "airflow/providers/opsgenie/provider.yaml",
          "airflow/providers/oracle/provider.yaml",
          "airflow/providers/pagerduty/provider.yaml",
          "airflow/providers/papermill/provider.yaml",
          "airflow/providers/pgvector/provider.yaml",
          "airflow/providers/pinecone/provider.yaml",
          "airflow/providers/plexus/provider.yaml",
          "airflow/providers/postgres/provider.yaml",
          "airflow/providers/presto/provider.yaml",
          "airflow/providers/redis/provider.yaml",
          "airflow/providers/salesforce/provider.yaml",
          "airflow/providers/samba/provider.yaml",
          "airflow/providers/segment/provider.yaml",
          "airflow/providers/sendgrid/provider.yaml",
          "airflow/providers/sftp/provider.yaml",
          "airflow/providers/singularity/provider.yaml",
          "airflow/providers/slack/provider.yaml",
          "airflow/providers/smtp/provider.yaml",
          "airflow/providers/snowflake/provider.yaml",
          "airflow/providers/sqlite/provider.yaml",
          "airflow/providers/ssh/provider.yaml",
          "airflow/providers/tableau/provider.yaml",
          "airflow/providers/tabular/provider.yaml",
          "airflow/providers/telegram/provider.yaml",
          "airflow/providers/trino/provider.yaml",
          "airflow/providers/vertica/provider.yaml",
          "airflow/providers/weaviate/provider.yaml",
          "airflow/providers/yandex/provider.yaml",
          "airflow/providers/zendesk/provider.yaml",
          "dev/breeze/src/airflow_breeze/breeze.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/utils/packages.py",
          "dev/breeze/tests/test_packages.py",
          "docs/exts/docs_build/package_filter.py",
          "docs/exts/provider_yaml_utils.py",
          "generated/provider_dependencies.json",
          "images/breeze/output_build-docs.svg",
          "images/breeze/output_build-docs.txt",
          "images/breeze/output_release-management_add-back-references.svg",
          "images/breeze/output_release-management_add-back-references.txt",
          "images/breeze/output_release-management_generate-constraints.svg",
          "images/breeze/output_release-management_publish-docs.svg",
          "images/breeze/output_release-management_publish-docs.txt",
          "images/breeze/output_sbom_generate-providers-requirements.svg",
          "images/breeze/output_sbom_generate-providers-requirements.txt",
          "scripts/ci/pre_commit/pre_commit_check_provider_docs.py",
          "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py",
          "scripts/in_container/run_provider_yaml_files_check.py",
          "setup.py",
          "tests/always/test_example_dags.py"
        ],
        "message": "Speed up autocompletion of Breeze by simplifying provider state (#36499)\n\nSome recent changes, adding removed and suspended state for breeze\ncaused significant slow-down of autocompletion retrieval - as it\nturned out, because we loaded and parsed all provider yaml files\nduring auto-completion - in order to determine list of providers\navailable for some commands.\n\nWe already planned to replace the several states (suspended,\nnot-ready, removed) with a single state field - by doing it and\naddding the field to pre-commit generated \"provider_dependencies.json\"\nwe could switch to parsing the single provider_dependencies.json\nfile and retrieve provider list from there following the state stored\nin that json file.\n\nThis also simplifies state management following the recently\nadded state diagram by following the same state lifecycle:\n\n\"not-ready\" -> \"ready\" -> \"suspended\" -> \"removed\"",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/breeze.py||dev/breeze/src/airflow_breeze/breeze.py",
          "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "dev/breeze/src/airflow_breeze/utils/packages.py||dev/breeze/src/airflow_breeze/utils/packages.py",
          "dev/breeze/tests/test_packages.py||dev/breeze/tests/test_packages.py",
          "scripts/ci/pre_commit/pre_commit_check_provider_docs.py||scripts/ci/pre_commit/pre_commit_check_provider_docs.py",
          "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py||scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py",
          "scripts/in_container/run_provider_yaml_files_check.py||scripts/in_container/run_provider_yaml_files_check.py",
          "setup.py||setup.py",
          "tests/always/test_example_dags.py||tests/always/test_example_dags.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/breeze.py||dev/breeze/src/airflow_breeze/breeze.py": [
          "File: dev/breeze/src/airflow_breeze/breeze.py -> dev/breeze/src/airflow_breeze/breeze.py"
        ],
        "dev/breeze/src/airflow_breeze/commands/release_management_commands.py||dev/breeze/src/airflow_breeze/commands/release_management_commands.py": [
          "File: dev/breeze/src/airflow_breeze/commands/release_management_commands.py -> dev/breeze/src/airflow_breeze/commands/release_management_commands.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "450:         except PrepareReleaseDocsUserQuitException:",
          "451:             break",
          "452:         else:",
          "454:                 removed_packages.append(provider_id)",
          "455:             else:",
          "456:                 success_packages.append(provider_id)",
          "",
          "[Removed Lines]",
          "453:             if provider_metadata.get(\"removed\"):",
          "",
          "[Added Lines]",
          "453:             if provider_metadata[\"state\"] == \"removed\":",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "481:     if not provider_metadata:",
          "482:         get_console().print(f\"[error]The package {provider_package_id} is not a provider package. Exiting[/]\")",
          "483:         sys.exit(1)",
          "485:         get_console().print(",
          "486:             f\"[warning]The package: {provider_package_id} is scheduled for removal, but \"",
          "487:             f\"since you asked for it, it will be built [/]\\n\"",
          "488:         )",
          "490:         get_console().print(f\"[warning]The package: {provider_package_id} is suspended \" f\"skipping it [/]\\n\")",
          "491:         raise PackageSuspendedException()",
          "492:     return provider_metadata",
          "",
          "[Removed Lines]",
          "484:     if provider_metadata.get(\"removed\", False):",
          "489:     elif provider_metadata.get(\"suspended\"):",
          "",
          "[Added Lines]",
          "484:     if provider_metadata[\"state\"] == \"removed\":",
          "489:     elif provider_metadata.get(\"state\") == \"suspended\":",
          "",
          "---------------"
        ],
        "dev/breeze/src/airflow_breeze/utils/packages.py||dev/breeze/src/airflow_breeze/utils/packages.py": [
          "File: dev/breeze/src/airflow_breeze/utils/packages.py -> dev/breeze/src/airflow_breeze/utils/packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "156:     refresh_provider_metadata_from_yaml_file(provider_yaml_path)",
          "159: def get_provider_packages_metadata() -> dict[str, dict[str, Any]]:",
          "160:     \"\"\"",
          "161:     Load all data from providers files",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "159: @lru_cache(maxsize=1)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "208: @lru_cache",
          "209: def get_suspended_provider_ids() -> list[str]:",
          "217: @lru_cache",
          "",
          "[Removed Lines]",
          "210:     return [",
          "211:         provider_id",
          "212:         for provider_id, provider_metadata in get_provider_packages_metadata().items()",
          "213:         if provider_metadata.get(\"suspended\", False)",
          "214:     ]",
          "",
          "[Added Lines]",
          "211:     return get_available_packages(include_suspended=True, include_regular=False)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "222: @lru_cache",
          "223: def get_removed_provider_ids() -> list[str]:",
          "231: @lru_cache",
          "232: def get_not_ready_provider_ids() -> list[str]:",
          "240: def get_provider_requirements(provider_id: str) -> list[str]:",
          "",
          "[Removed Lines]",
          "224:     return [",
          "225:         provider_id",
          "226:         for provider_id, provider_metadata in get_provider_packages_metadata().items()",
          "227:         if provider_metadata.get(\"removed\", False)",
          "228:     ]",
          "233:     return [",
          "234:         provider_id",
          "235:         for provider_id, provider_metadata in get_provider_packages_metadata().items()",
          "236:         if provider_metadata.get(\"not-ready\", False)",
          "237:     ]",
          "",
          "[Added Lines]",
          "221:     return get_available_packages(include_removed=True, include_regular=False)",
          "226:     return get_available_packages(include_not_ready=True, include_regular=False)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "249:     include_suspended: bool = False,",
          "250:     include_removed: bool = False,",
          "251:     include_not_ready: bool = False,",
          "252: ) -> list[str]:",
          "253:     \"\"\"",
          "254:     Return provider ids for all packages that are available currently (not suspended).",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "241:     include_regular: bool = True,",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "256:     :rtype: object",
          "257:     :param include_suspended: whether the suspended packages should be included",
          "258:     :param include_removed: whether the removed packages should be included",
          "260:     :param include_non_provider_doc_packages: whether the non-provider doc packages should be included",
          "261:            (packages like apache-airflow, helm-chart, docker-stack)",
          "262:     :param include_all_providers: whether \"all-providers\" should be included ni the list.",
          "264:     \"\"\"",
          "272:     if include_non_provider_doc_packages:",
          "273:         available_packages.extend(REGULAR_DOC_PACKAGES)",
          "274:     if include_all_providers:",
          "275:         available_packages.append(\"all-providers\")",
          "281:     return sorted(set(available_packages))",
          "",
          "[Removed Lines]",
          "259:     :param include_not_ready: whether the not-ready ppackages should be included",
          "265:     provider_ids: list[str] = list(json.loads(PROVIDER_DEPENDENCIES_JSON_FILE_PATH.read_text()).keys())",
          "266:     available_packages = []",
          "267:     not_ready_provider_ids = get_not_ready_provider_ids()",
          "268:     if not include_not_ready:",
          "269:         provider_ids = [",
          "270:             provider_id for provider_id in provider_ids if provider_id not in not_ready_provider_ids",
          "271:         ]",
          "276:     available_packages.extend(provider_ids)",
          "277:     if include_suspended:",
          "278:         available_packages.extend(get_suspended_provider_ids())",
          "279:     if include_removed:",
          "280:         available_packages.extend(get_removed_provider_ids())",
          "",
          "[Added Lines]",
          "249:     :param include_not_ready: whether the not-ready packages should be included",
          "250:     :param include_regular: whether the regular packages should be included",
          "256:     provider_dependencies = json.loads(PROVIDER_DEPENDENCIES_JSON_FILE_PATH.read_text())",
          "258:     valid_states = set()",
          "259:     if include_not_ready:",
          "260:         valid_states.add(\"not-ready\")",
          "261:     if include_regular:",
          "262:         valid_states.add(\"ready\")",
          "263:     if include_suspended:",
          "264:         valid_states.add(\"suspended\")",
          "265:     if include_removed:",
          "266:         valid_states.add(\"removed\")",
          "267:     available_packages: list[str] = [",
          "268:         provider_id",
          "269:         for provider_id, provider_dependencies in provider_dependencies.items()",
          "270:         if provider_dependencies[\"state\"] in valid_states",
          "271:     ]",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "499:         versions=provider_info[\"versions\"],",
          "500:         excluded_python_versions=provider_info.get(\"excluded-python-versions\") or [],",
          "501:         plugins=plugins,",
          "503:     )",
          "",
          "[Removed Lines]",
          "502:         removed=provider_info.get(\"removed\", False),",
          "",
          "[Added Lines]",
          "497:         removed=provider_info[\"state\"] == \"removed\",",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_packages.py||dev/breeze/tests/test_packages.py": [
          "File: dev/breeze/tests/test_packages.py -> dev/breeze/tests/test_packages.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "110: def test_get_removed_providers():",
          "111:     # Modify it every time we schedule provider for removal or remove it",
          "115: def test_get_suspended_provider_ids():",
          "",
          "[Removed Lines]",
          "112:     assert [\"apache.sqoop\", \"daskexecutor\", \"plexus\"] == get_removed_provider_ids()",
          "",
          "[Added Lines]",
          "112:     assert [] == get_removed_provider_ids()",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "340:     assert provider_info_dict[\"name\"] == \"Amazon\"",
          "341:     assert provider_info_dict[\"package-name\"] == \"apache-airflow-providers-amazon\"",
          "342:     assert \"Amazon\" in provider_info_dict[\"description\"]",
          "344:     assert provider_info_dict[\"filesystems\"] == [\"airflow.providers.amazon.aws.fs.s3\"]",
          "345:     assert len(provider_info_dict[\"versions\"]) > 45",
          "346:     assert len(provider_info_dict[\"dependencies\"]) > 10",
          "",
          "[Removed Lines]",
          "343:     assert provider_info_dict[\"suspended\"] is False",
          "",
          "[Added Lines]",
          "343:     assert provider_info_dict[\"state\"] == \"ready\"",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_check_provider_docs.py||scripts/ci/pre_commit/pre_commit_check_provider_docs.py": [
          "File: scripts/ci/pre_commit/pre_commit_check_provider_docs.py -> scripts/ci/pre_commit/pre_commit_check_provider_docs.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "94:     for provider_file in AIRFLOW_PROVIDERS_DIR.rglob(\"provider.yaml\"):",
          "95:         provider_name = str(provider_file.parent.relative_to(AIRFLOW_PROVIDERS_DIR)).replace(os.sep, \".\")",
          "96:         provider_info = yaml.safe_load(provider_file.read_text())",
          "98:             ALL_PROVIDERS[provider_name] = provider_info",
          "",
          "[Removed Lines]",
          "97:         if not provider_info[\"suspended\"]:",
          "",
          "[Added Lines]",
          "97:         if provider_info[\"state\"] != \"suspended\":",
          "",
          "---------------"
        ],
        "scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py||scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py": [
          "File: scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py -> scripts/ci/pre_commit/pre_commit_update_providers_dependencies.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "97:                     os.sep, \".\"",
          "98:                 )",
          "99:                 provider_info = yaml.safe_load(provider_file.read_text())",
          "103:                     suspended_paths.append(provider_file.parent.relative_to(AIRFLOW_PROVIDERS_DIR).as_posix())",
          "104:             path = Path(root, filename)",
          "105:             if path.is_file() and path.name.endswith(\".py\"):",
          "106:                 ALL_PROVIDER_FILES.append(Path(root, filename))",
          "",
          "[Removed Lines]",
          "100:                 if not provider_info[\"suspended\"]:",
          "101:                     ALL_PROVIDERS[provider_name] = provider_info",
          "102:                 else:",
          "",
          "[Added Lines]",
          "100:                 if provider_info[\"state\"] == \"suspended\":",
          "102:                 ALL_PROVIDERS[provider_name] = provider_info",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "174:             ALL_DEPENDENCIES[file_provider][\"cross-providers-deps\"].append(imported_provider)",
          "177: if __name__ == \"__main__\":",
          "178:     find_all_providers_and_provider_files()",
          "179:     num_files = len(ALL_PROVIDER_FILES)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "176: STATES: dict[str, str] = {}",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "182:     for file in ALL_PROVIDER_FILES:",
          "183:         check_if_different_provider_used(file)",
          "184:     for provider, provider_yaml_content in ALL_PROVIDERS.items():",
          "187:     if warnings:",
          "188:         console.print(\"[yellow]Warnings!\\n\")",
          "189:         for warning in warnings:",
          "",
          "[Removed Lines]",
          "185:         if not provider_yaml_content.get(\"suspended\"):",
          "186:             ALL_DEPENDENCIES[provider][\"deps\"].extend(provider_yaml_content[\"dependencies\"])",
          "",
          "[Added Lines]",
          "187:         ALL_DEPENDENCIES[provider][\"deps\"].extend(provider_yaml_content[\"dependencies\"])",
          "188:         STATES[provider] = provider_yaml_content[\"state\"]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "194:         for error in errors:",
          "195:             console.print(f\"[red] {error}\")",
          "196:         console.print(f\"[bright_blue]Total: {len(errors)} errors.\")",
          "198:     for key in sorted(ALL_DEPENDENCIES.keys()):",
          "199:         unique_sorted_dependencies[key][\"deps\"] = sorted(ALL_DEPENDENCIES[key][\"deps\"])",
          "200:         unique_sorted_dependencies[key][\"cross-providers-deps\"] = sorted(",
          "",
          "[Removed Lines]",
          "197:     unique_sorted_dependencies: dict[str, dict[str, list[str]]] = defaultdict(dict)",
          "",
          "[Added Lines]",
          "199:     unique_sorted_dependencies: dict[str, dict[str, list[str] | str]] = defaultdict(dict)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "202:         )",
          "203:         excluded_versions = ALL_PROVIDERS[key].get(\"excluded-python-versions\")",
          "204:         unique_sorted_dependencies[key][\"excluded-python-versions\"] = excluded_versions or []",
          "205:     if errors:",
          "206:         console.print()",
          "207:         console.print(\"[red]Errors found during verification. Exiting!\")",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "207:         unique_sorted_dependencies[key][\"state\"] = STATES[key]",
          "",
          "---------------"
        ],
        "scripts/in_container/run_provider_yaml_files_check.py||scripts/in_container/run_provider_yaml_files_check.py": [
          "File: scripts/in_container/run_provider_yaml_files_check.py -> scripts/in_container/run_provider_yaml_files_check.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "114:             jsonschema.validate(provider, schema=schema)",
          "115:         except jsonschema.ValidationError:",
          "116:             raise Exception(f\"Unable to parse: {rel_path}.\")",
          "118:             result[rel_path] = provider",
          "119:         else:",
          "120:             suspended_providers.add(provider[\"package-name\"])",
          "",
          "[Removed Lines]",
          "117:         if not provider.get(\"suspended\"):",
          "",
          "[Added Lines]",
          "117:         if provider[\"state\"] not in [\"suspended\", \"removed\"]:",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "557:                 op[\"how-to-guide\"] for op in provider[\"transfers\"] if \"how-to-guide\" in op",
          "558:             )",
          "559:     if suspended_providers:",
          "561:         console.print(suspended_providers)",
          "563:     expected_doc_files = itertools.chain(",
          "",
          "[Removed Lines]",
          "560:         console.print(\"[yellow]Suspended providers:[/]\")",
          "",
          "[Added Lines]",
          "560:         console.print(\"[yellow]Suspended/Removed providers:[/]\")",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "680:     return num_providers, num_errors",
          "703: if __name__ == \"__main__\":",
          "704:     ProvidersManager().initialize_providers_configuration()",
          "705:     architecture = Architecture.get_current()",
          "",
          "[Removed Lines]",
          "683: @run_check(\"Checking remove flag only set for suspended providers\")",
          "684: def check_removed_flag_only_set_for_suspended_providers(yaml_files: dict[str, dict]):",
          "685:     num_errors = 0",
          "686:     num_providers = 0",
          "687:     for package_info in yaml_files.values():",
          "688:         num_providers += 1",
          "689:         package_name = package_info[\"package-name\"]",
          "690:         suspended = package_info[\"suspended\"]",
          "691:         removed = package_info.get(\"removed\", False)",
          "692:         if removed and not suspended:",
          "693:             errors.append(",
          "694:                 f\"The provider {package_name} has removed set to True in their provider.yaml file \"",
          "695:                 f\"but suspended flag is set to false. You should only set removed flag in order to \"",
          "696:                 f\"prepare last release for a provider that has been previously suspended. \"",
          "697:                 f\"[yellow]How to fix it[/]: Please suspend the provider first before removing it.\"",
          "698:             )",
          "699:             num_errors += 1",
          "700:     return num_providers, num_errors",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "726:     check_notification_classes(all_parsed_yaml_files)",
          "727:     check_unique_provider_name(all_parsed_yaml_files)",
          "728:     check_providers_have_all_documentation_files(all_parsed_yaml_files)",
          "731:     if all_files_loaded:",
          "732:         # Only check those if all provider files are loaded",
          "",
          "[Removed Lines]",
          "729:     check_removed_flag_only_set_for_suspended_providers(all_parsed_yaml_files)",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "105:             dependencies = json.load(f)",
          "106:         provider_dict = {}",
          "107:         for key, value in dependencies.items():",
          "108:             if value.get(DEPS):",
          "109:                 apply_pypi_suffix_to_airflow_packages(value[DEPS])",
          "110:             if CURRENT_PYTHON_VERSION not in value[\"excluded-python-versions\"] or skip_python_version_check:",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "108:             if value[\"state\"] in [\"suspended\", \"removed\"]:",
          "109:                 continue",
          "",
          "---------------"
        ],
        "tests/always/test_example_dags.py||tests/always/test_example_dags.py": [
          "File: tests/always/test_example_dags.py -> tests/always/test_example_dags.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "40:     suspended_providers = []",
          "41:     for provider_path in AIRFLOW_PROVIDERS_ROOT.rglob(\"provider.yaml\"):",
          "42:         provider_yaml = yaml.safe_load(provider_path.read_text())",
          "44:             suspended_providers.append(",
          "45:                 provider_path.parent.relative_to(AIRFLOW_SOURCES_ROOT)",
          "46:                 .as_posix()",
          "",
          "[Removed Lines]",
          "43:         if provider_yaml.get(\"suspended\"):",
          "",
          "[Added Lines]",
          "43:         if provider_yaml[\"state\"] == \"suspended\":",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "fef77f5a2c0a49ac5536c6f9c4e11ca4d001f3ae",
      "candidate_info": {
        "commit_hash": "fef77f5a2c0a49ac5536c6f9c4e11ca4d001f3ae",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/fef77f5a2c0a49ac5536c6f9c4e11ca4d001f3ae",
        "files": [
          "airflow/www/decorators.py",
          "tests/www/views/test_views_paused.py"
        ],
        "message": "Bugfix/logging for pausing (#36182)\n\n---------\n\nCo-authored-by: Aleph Melo <alephmelo@icloud.com>\n(cherry picked from commit c884f3ce3250bb9dd58cf3dd8dde7c2555e664a5)",
        "before_after_code_files": [
          "airflow/www/decorators.py||airflow/www/decorators.py",
          "tests/www/views/test_views_paused.py||tests/www/views/test_views_paused.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/36788"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/decorators.py||airflow/www/decorators.py": [
          "File: airflow/www/decorators.py -> airflow/www/decorators.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "93:                     user = get_auth_manager().get_user_name()",
          "94:                     user_display = get_auth_manager().get_user_display_name()",
          "97:                 extra_fields = [",
          "98:                     (k, secrets_masker.redact(v, k))",
          "99:                     for k, v in itertools.chain(request.values.items(multi=True), request.view_args.items())",
          "",
          "[Removed Lines]",
          "96:                 fields_skip_logging = {\"csrf_token\", \"_csrf_token\"}",
          "",
          "[Added Lines]",
          "96:                 fields_skip_logging = {\"csrf_token\", \"_csrf_token\", \"is_paused\"}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "107:                 params = {**request.values, **request.view_args}",
          "109:                 log = Log(",
          "110:                     event=event or f.__name__,",
          "111:                     task_instance=None,",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "109:                 if params and \"is_paused\" in params:",
          "110:                     extra_fields.append((\"is_paused\", params[\"is_paused\"] == \"false\"))",
          "",
          "---------------"
        ],
        "tests/www/views/test_views_paused.py||tests/www/views/test_views_paused.py": [
          "File: tests/www/views/test_views_paused.py -> tests/www/views/test_views_paused.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "17: from __future__ import annotations",
          "19: import pytest",
          "21: from airflow.models.log import Log",
          "22: from tests.test_utils.db import clear_db_dags",
          "24: pytestmark = pytest.mark.db_test",
          "27: @pytest.fixture(autouse=True)",
          "28: def dags(create_dummy_dag):",
          "29:     paused_dag, _ = create_dummy_dag(dag_id=\"paused_dag\", is_paused_upon_creation=True)",
          "30:     dag, _ = create_dummy_dag(dag_id=\"unpaused_dag\")",
          "32:     yield dag, paused_dag",
          "34:     clear_db_dags()",
          "37: def test_logging_pause_dag(admin_client, dags, session):",
          "38:     dag, _ = dags",
          "39:     # is_paused=false mean pause the dag",
          "40:     admin_client.post(f\"/paused?is_paused=false&dag_id={dag.dag_id}\", follow_redirects=True)",
          "41:     dag_query = session.query(Log).filter(Log.dag_id == dag.dag_id)",
          "42:     assert \"('is_paused', True)\" in dag_query.first().extra",
          "45: def test_logging_unpuase_dag(admin_client, dags, session):",
          "46:     _, paused_dag = dags",
          "47:     # is_paused=true mean unpause the dag",
          "48:     admin_client.post(f\"/paused?is_paused=true&dag_id={paused_dag.dag_id}\", follow_redirects=True)",
          "49:     dag_query = session.query(Log).filter(Log.dag_id == paused_dag.dag_id)",
          "50:     assert \"('is_paused', False)\" in dag_query.first().extra",
          "",
          "---------------"
        ]
      }
    }
  ]
}