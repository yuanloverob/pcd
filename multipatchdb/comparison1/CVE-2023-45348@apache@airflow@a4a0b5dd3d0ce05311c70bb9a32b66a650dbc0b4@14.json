{
  "cve_id": "CVE-2023-45348",
  "cve_desc": "Apache Airflow, versions 2.7.0 and 2.7.1, is affected by a vulnerability that allows an authenticated user to retrieve sensitive configuration information when the \"expose_config\" option is set to \"non-sensitive-only\". The `expose_config` option is False by default.\nIt is recommended to upgrade to a version that is not affected.",
  "repo": "apache/airflow",
  "patch_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
  "patch_info": {
    "commit_hash": "a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/a4a0b5dd3d0ce05311c70bb9a32b66a650dbc0b4",
    "files": [
      "airflow/api_connexion/endpoints/config_endpoint.py",
      "airflow/configuration.py",
      "tests/api_connexion/endpoints/test_config_endpoint.py"
    ],
    "message": "Check if the lower of provided values are sensitives in config endpoint (#34712)\n\n* Check if the lower of provided values are sensitives in config endpoint\n\n* update unit test\n\n* ensure that all values in sensitive dict are in lower characters\n\n(cherry picked from commit f044589b685855a8fce8f5376bea2564c5a001f7)",
    "before_after_code_files": [
      "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py",
      "airflow/configuration.py||airflow/configuration.py",
      "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py"
    ]
  },
  "patch_diff": {
    "airflow/api_connexion/endpoints/config_endpoint.py||airflow/api_connexion/endpoints/config_endpoint.py": [
      "File: airflow/api_connexion/endpoints/config_endpoint.py -> airflow/api_connexion/endpoints/config_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "123:                 \"Config not found.\", detail=f\"The option [{section}/{option}] is not found in config.\"",
      "124:             )",
      "127:             value = \"< hidden >\"",
      "128:         else:",
      "129:             value = conf.get(section, option)",
      "",
      "[Removed Lines]",
      "126:         if (section, option) in conf.sensitive_config_values:",
      "",
      "[Added Lines]",
      "126:         if (section.lower(), option.lower()) in conf.sensitive_config_values:",
      "",
      "---------------"
    ],
    "airflow/configuration.py||airflow/configuration.py": [
      "File: airflow/configuration.py -> airflow/configuration.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "311:             for s, s_c in self.configuration_description.items()",
      "312:             for k, item in s_c.get(\"options\").items()  # type: ignore[union-attr]",
      "313:         }",
      "315:         depr_option = {self.deprecated_options[x][:-1] for x in sensitive if x in self.deprecated_options}",
      "316:         depr_section = {",
      "317:             (self.deprecated_sections[s][0], k) for s, k in sensitive if s in self.deprecated_sections",
      "",
      "[Removed Lines]",
      "314:         sensitive = {(section, key) for (section, key), v in flattened.items() if v.get(\"sensitive\") is True}",
      "",
      "[Added Lines]",
      "314:         sensitive = {",
      "315:             (section.lower(), key.lower())",
      "316:             for (section, key), v in flattened.items()",
      "317:             if v.get(\"sensitive\") is True",
      "318:         }",
      "",
      "---------------"
    ],
    "tests/api_connexion/endpoints/test_config_endpoint.py||tests/api_connexion/endpoints/test_config_endpoint.py": [
      "File: tests/api_connexion/endpoints/test_config_endpoint.py -> tests/api_connexion/endpoints/test_config_endpoint.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "247:         return_value=MOCK_CONF_WITH_SENSITIVE_VALUE,",
      "248:     )",
      "249:     @conf_vars({(\"webserver\", \"expose_config\"): \"non-sensitive-only\"})",
      "251:         response = self.client.get(",
      "253:             headers={\"Accept\": \"text/plain\"},",
      "254:             environ_overrides={\"REMOTE_USER\": \"test\"},",
      "255:         )",
      "256:         assert response.status_code == 200",
      "257:         expected = textwrap.dedent(",
      "261:         \"\"\"",
      "262:         )",
      "263:         assert expected == response.data.decode()",
      "",
      "[Removed Lines]",
      "250:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict):",
      "252:             \"/api/v1/config/section/core/option/sql_alchemy_conn\",",
      "258:             \"\"\"\\",
      "259:         [core]",
      "260:         sql_alchemy_conn = < hidden >",
      "",
      "[Added Lines]",
      "250:     @pytest.mark.parametrize(",
      "251:         \"section, option\",",
      "252:         [",
      "253:             (\"core\", \"sql_alchemy_conn\"),",
      "254:             (\"core\", \"SQL_ALCHEMY_CONN\"),",
      "255:             (\"corE\", \"sql_alchemy_conn\"),",
      "256:             (\"CORE\", \"sql_alchemy_conn\"),",
      "257:         ],",
      "258:     )",
      "259:     def test_should_respond_200_text_plain_with_non_sensitive_only(self, mock_as_dict, section, option):",
      "261:             f\"/api/v1/config/section/{section}/option/{option}\",",
      "267:             f\"\"\"\\",
      "268:         [{section}]",
      "269:         {option} = < hidden >",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "05d11a34303d0fb20b08a0241661f4d10168ed74",
      "candidate_info": {
        "commit_hash": "05d11a34303d0fb20b08a0241661f4d10168ed74",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/05d11a34303d0fb20b08a0241661f4d10168ed74",
        "files": [
          "airflow/www/views.py"
        ],
        "message": "Fix typo in views (#33830)\n\n(cherry picked from commit b68beb930fa8d84dda041e27d9fe36c2d0d72f8b)",
        "before_after_code_files": [
          "airflow/www/views.py||airflow/www/views.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "4186: class AirflowModelView(ModelView):",
          "4187:     \"\"\"",
          "4190:     Overridden `__getattribute__` to wraps REST methods with action_logger",
          "4191:     \"\"\"",
          "",
          "[Removed Lines]",
          "4188:     Airflow Mode View.",
          "",
          "[Added Lines]",
          "4188:     Airflow Model View.",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f8fc3d69244427ff1096c93d3fa155e201c09459",
      "candidate_info": {
        "commit_hash": "f8fc3d69244427ff1096c93d3fa155e201c09459",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f8fc3d69244427ff1096c93d3fa155e201c09459",
        "files": [
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/tests/test_docker_command_utils.py"
        ],
        "message": "Parse 'docker context ls --format=json' correctly (#34711)\n\n(cherry picked from commit 19284981f88e45dca4c4003837e3cead1723caf1)",
        "before_after_code_files": [
          "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "dev/breeze/tests/test_docker_command_utils.py||dev/breeze/tests/test_docker_command_utils.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "dev/breeze/src/airflow_breeze/utils/docker_command_utils.py||dev/breeze/src/airflow_breeze/utils/docker_command_utils.py": [
          "File: dev/breeze/src/airflow_breeze/utils/docker_command_utils.py -> dev/breeze/src/airflow_breeze/utils/docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "826:     if result.returncode != 0:",
          "827:         get_console().print(\"[warning]Could not detect docker builder. Using default.[/]\")",
          "828:         return \"default\"",
          "834:     if not known_contexts:",
          "835:         get_console().print(\"[warning]Could not detect docker builder. Using default.[/]\")",
          "836:         return \"default\"",
          "",
          "[Removed Lines]",
          "829:     context_json = json.loads(result.stdout)",
          "830:     if isinstance(context_json, dict):",
          "831:         # In case there is one context it is returned as dict not array of dicts \u00af\\_(\u30c4)_/\u00af",
          "832:         context_json = [context_json]",
          "833:     known_contexts = {info[\"Name\"]: info for info in context_json}",
          "",
          "[Added Lines]",
          "829:     context_dicts = (json.loads(line) for line in result.stdout.splitlines() if line.strip())",
          "830:     known_contexts = {info[\"Name\"]: info for info in context_dicts}",
          "",
          "---------------"
        ],
        "dev/breeze/tests/test_docker_command_utils.py||dev/breeze/tests/test_docker_command_utils.py": [
          "File: dev/breeze/tests/test_docker_command_utils.py -> dev/breeze/tests/test_docker_command_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "195:     )",
          "205: @pytest.mark.parametrize(",
          "206:     \"context_output, selected_context, console_output\",",
          "207:     [",
          "208:         (",
          "210:             \"default\",",
          "211:             \"[info]Using default as context\",",
          "212:         ),",
          "214:         (",
          "216:             \"a\",",
          "217:             \"[warning]Could not use any of the preferred docker contexts\",",
          "218:         ),",
          "219:         (",
          "221:             \"desktop-linux\",",
          "222:             \"[info]Using desktop-linux as context\",",
          "223:         ),",
          "224:         (",
          "226:             \"default\",",
          "227:             \"[info]Using default as context\",",
          "228:         ),",
          "229:         (",
          "231:             \"desktop-linux\",",
          "232:             \"[info]Using desktop-linux as context\",",
          "233:         ),",
          "",
          "[Removed Lines]",
          "198: def _fake_ctx(name: str) -> dict[str, str]:",
          "199:     return {",
          "200:         \"Name\": name,",
          "201:         \"DockerEndpoint\": f\"unix://{name}\",",
          "202:     }",
          "209:             json.dumps([_fake_ctx(\"default\")]),",
          "213:         (\"[]\", \"default\", \"[warning]Could not detect docker builder\"),",
          "215:             json.dumps([_fake_ctx(\"a\"), _fake_ctx(\"b\")]),",
          "220:             json.dumps([_fake_ctx(\"a\"), _fake_ctx(\"desktop-linux\")]),",
          "225:             json.dumps([_fake_ctx(\"a\"), _fake_ctx(\"default\")]),",
          "230:             json.dumps([_fake_ctx(\"a\"), _fake_ctx(\"default\"), _fake_ctx(\"desktop-linux\")]),",
          "",
          "[Added Lines]",
          "198: def _fake_ctx_output(*names: str) -> str:",
          "199:     return \"\\n\".join(json.dumps({\"Name\": name, \"DockerEndpoint\": f\"unix://{name}\"}) for name in names)",
          "206:             _fake_ctx_output(\"default\"),",
          "210:         (\"\\n\", \"default\", \"[warning]Could not detect docker builder\"),",
          "212:             _fake_ctx_output(\"a\", \"b\"),",
          "217:             _fake_ctx_output(\"a\", \"desktop-linux\"),",
          "222:             _fake_ctx_output(\"a\", \"default\"),",
          "227:             _fake_ctx_output(\"a\", \"default\", \"desktop-linux\"),",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "8a120bdb2345db63969908df5b4e46dbd948032b",
      "candidate_info": {
        "commit_hash": "8a120bdb2345db63969908df5b4e46dbd948032b",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/8a120bdb2345db63969908df5b4e46dbd948032b",
        "files": [
          "airflow/www/extensions/init_appbuilder.py",
          "airflow/www/fab_security/sqla/manager.py"
        ],
        "message": "replace loop by any when looking for a positive value in core (#33985)\n\n(cherry picked from commit b8165941d007081f0b7b24458d461a8760909ac8)",
        "before_after_code_files": [
          "airflow/www/extensions/init_appbuilder.py||airflow/www/extensions/init_appbuilder.py",
          "airflow/www/fab_security/sqla/manager.py||airflow/www/fab_security/sqla/manager.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/extensions/init_appbuilder.py||airflow/www/extensions/init_appbuilder.py": [
          "File: airflow/www/extensions/init_appbuilder.py -> airflow/www/extensions/init_appbuilder.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "645:         )",
          "647:     def _view_exists(self, view):",
          "653:     def _process_inner_views(self):",
          "654:         for view in self.baseviews:",
          "",
          "[Removed Lines]",
          "648:         for baseview in self.baseviews:",
          "649:             if baseview.__class__ == view.__class__:",
          "650:                 return True",
          "651:         return False",
          "",
          "[Added Lines]",
          "648:         return any(baseview.__class__ == view.__class__ for baseview in self.baseviews)",
          "",
          "---------------"
        ],
        "airflow/www/fab_security/sqla/manager.py||airflow/www/fab_security/sqla/manager.py": [
          "File: airflow/www/fab_security/sqla/manager.py -> airflow/www/fab_security/sqla/manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "537:             self.get_session.rollback()",
          "539:     def perms_include_action(self, perms, action_name):",
          "545:     def add_permission_to_role(self, role: Role, permission: Permission | None) -> None:",
          "546:         \"\"\"",
          "",
          "[Removed Lines]",
          "540:         for perm in perms:",
          "541:             if perm.action and perm.action.name == action_name:",
          "542:                 return True",
          "543:         return False",
          "",
          "[Added Lines]",
          "540:         return any(perm.action and perm.action.name == action_name for perm in perms)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "25005a6e58b60f34db71667f0b075074101e3de4",
      "candidate_info": {
        "commit_hash": "25005a6e58b60f34db71667f0b075074101e3de4",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/25005a6e58b60f34db71667f0b075074101e3de4",
        "files": [
          "airflow/dag_processing/manager.py",
          "airflow/plugins_manager.py",
          "airflow/providers/amazon/aws/hooks/datasync.py",
          "airflow/providers/apache/hive/hooks/hive.py",
          "airflow/providers/google/cloud/operators/bigquery_dts.py",
          "airflow/providers/google/cloud/sensors/dataproc_metastore.py",
          "airflow/providers/google/cloud/transfers/gcs_to_gcs.py",
          "airflow/providers/snowflake/utils/sql_api_generate_jwt.py",
          "tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py"
        ],
        "message": "Refactor: Simplify comparisons (#34181)\n\n(cherry picked from commit 94d07908a2188eb650bfab21d89a49b287aee35c)",
        "before_after_code_files": [
          "airflow/dag_processing/manager.py||airflow/dag_processing/manager.py",
          "airflow/plugins_manager.py||airflow/plugins_manager.py",
          "airflow/providers/amazon/aws/hooks/datasync.py||airflow/providers/amazon/aws/hooks/datasync.py",
          "airflow/providers/apache/hive/hooks/hive.py||airflow/providers/apache/hive/hooks/hive.py",
          "airflow/providers/google/cloud/operators/bigquery_dts.py||airflow/providers/google/cloud/operators/bigquery_dts.py",
          "airflow/providers/google/cloud/sensors/dataproc_metastore.py||airflow/providers/google/cloud/sensors/dataproc_metastore.py",
          "airflow/providers/google/cloud/transfers/gcs_to_gcs.py||airflow/providers/google/cloud/transfers/gcs_to_gcs.py",
          "airflow/providers/snowflake/utils/sql_api_generate_jwt.py||airflow/providers/snowflake/utils/sql_api_generate_jwt.py",
          "tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py||tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/dag_processing/manager.py||airflow/dag_processing/manager.py": [
          "File: airflow/dag_processing/manager.py -> airflow/dag_processing/manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "1078:         # needs to be done before this process is forked to create the DAG parsing processes.",
          "1079:         SecretCache.init()",
          "1082:             file_path = self._file_path_queue.popleft()",
          "1083:             # Stop creating duplicate processor i.e. processor with the same filepath",
          "1084:             if file_path in self._processors:",
          "",
          "[Removed Lines]",
          "1081:         while self._parallelism - len(self._processors) > 0 and self._file_path_queue:",
          "",
          "[Added Lines]",
          "1081:         while self._parallelism > len(self._processors) and self._file_path_queue:",
          "",
          "---------------"
        ],
        "airflow/plugins_manager.py||airflow/plugins_manager.py": [
          "File: airflow/plugins_manager.py -> airflow/plugins_manager.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "341:         for plugin in plugins:",
          "342:             registered_hooks.extend(plugin.hooks)",
          "349: def initialize_web_ui_plugins():",
          "",
          "[Removed Lines]",
          "344:     num_loaded = len(plugins)",
          "345:     if num_loaded > 0:",
          "346:         log.debug(\"Loading %d plugin(s) took %.2f seconds\", num_loaded, timer.duration)",
          "",
          "[Added Lines]",
          "344:     if plugins:",
          "345:         log.debug(\"Loading %d plugin(s) took %.2f seconds\", len(plugins), timer.duration)",
          "",
          "---------------"
        ],
        "airflow/providers/amazon/aws/hooks/datasync.py||airflow/providers/amazon/aws/hooks/datasync.py": [
          "File: airflow/providers/amazon/aws/hooks/datasync.py -> airflow/providers/amazon/aws/hooks/datasync.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "57:         self.locations: list = []",
          "58:         self.tasks: list = []",
          "59:         # wait_interval_seconds = 0 is used during unit tests",
          "61:             raise ValueError(f\"Invalid wait_interval_seconds {wait_interval_seconds}\")",
          "64:     def create_location(self, location_uri: str, **create_location_kwargs) -> str:",
          "65:         \"\"\"",
          "",
          "[Removed Lines]",
          "60:         if wait_interval_seconds < 0 or wait_interval_seconds > 15 * 60:",
          "62:         self.wait_interval_seconds = wait_interval_seconds",
          "",
          "[Added Lines]",
          "60:         if 0 <= wait_interval_seconds <= 15 * 60:",
          "61:             self.wait_interval_seconds = wait_interval_seconds",
          "62:         else:",
          "",
          "---------------"
        ],
        "airflow/providers/apache/hive/hooks/hive.py||airflow/providers/apache/hive/hooks/hive.py": [
          "File: airflow/providers/apache/hive/hooks/hive.py -> airflow/providers/apache/hive/hooks/hive.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "176:             )",
          "177:         try:",
          "178:             int_port = int(conn.port)",
          "180:                 raise Exception(f\"The port used in beeline command ({conn.port}) should be in range 0-65535)\")",
          "181:         except (ValueError, TypeError) as e:",
          "182:             raise Exception(f\"The port used in beeline command ({conn.port}) should be a valid integer: {e})\")",
          "",
          "[Removed Lines]",
          "179:             if int_port <= 0 or int_port > 65535:",
          "",
          "[Added Lines]",
          "179:             if not 0 < int_port <= 65535:",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/operators/bigquery_dts.py||airflow/providers/google/cloud/operators/bigquery_dts.py": [
          "File: airflow/providers/google/cloud/operators/bigquery_dts.py -> airflow/providers/google/cloud/operators/bigquery_dts.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "355:         )",
          "357:     def _wait_for_transfer_to_be_done(self, run_id: str, transfer_config_id: str, interval: int = 10):",
          "359:             raise ValueError(\"Interval must be > 0\")",
          "361:         while True:",
          "",
          "[Removed Lines]",
          "358:         if interval < 0:",
          "",
          "[Added Lines]",
          "358:         if interval <= 0:",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/sensors/dataproc_metastore.py||airflow/providers/google/cloud/sensors/dataproc_metastore.py": [
          "File: airflow/providers/google/cloud/sensors/dataproc_metastore.py -> airflow/providers/google/cloud/sensors/dataproc_metastore.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "114:         # Return True if we got all requested partitions.",
          "115:         # If no partitions were given in the request, then we expect to find at least one.",
          "",
          "[Removed Lines]",
          "116:         return found_partitions > 0 and found_partitions >= len(set(self.partitions))",
          "",
          "[Added Lines]",
          "116:         return found_partitions >= max(1, len(set(self.partitions)))",
          "",
          "---------------"
        ],
        "airflow/providers/google/cloud/transfers/gcs_to_gcs.py||airflow/providers/google/cloud/transfers/gcs_to_gcs.py": [
          "File: airflow/providers/google/cloud/transfers/gcs_to_gcs.py -> airflow/providers/google/cloud/transfers/gcs_to_gcs.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "307:             ]",
          "309:         objects = set(objects) - set(existing_objects)",
          "311:             self.log.info(\"%s files are going to be synced: %s.\", len(objects), objects)",
          "312:         else:",
          "313:             self.log.info(\"There are no new files to sync. Have a nice day!\")",
          "",
          "[Removed Lines]",
          "310:         if len(objects) > 0:",
          "",
          "[Added Lines]",
          "310:         if objects:",
          "",
          "---------------"
        ],
        "airflow/providers/snowflake/utils/sql_api_generate_jwt.py||airflow/providers/snowflake/utils/sql_api_generate_jwt.py": [
          "File: airflow/providers/snowflake/utils/sql_api_generate_jwt.py -> airflow/providers/snowflake/utils/sql_api_generate_jwt.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "102:         account = raw_account",
          "103:         if \".global\" not in account:",
          "104:             # Handle the general case.",
          "108:         else:",
          "109:             # Handle the replication case.",
          "113:         # Use uppercase for the account identifier.",
          "114:         return account.upper()",
          "",
          "[Removed Lines]",
          "105:             idx = account.find(\".\")",
          "106:             if idx > 0:",
          "107:                 account = account[0:idx]",
          "110:             idx = account.find(\"-\")",
          "111:             if idx > 0:",
          "112:                 account = account[0:idx]  # pragma: no cover",
          "",
          "[Added Lines]",
          "105:             account = account.partition(\".\")[0]",
          "108:             account = account.partition(\"-\")[0]",
          "",
          "---------------"
        ],
        "tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py||tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py": [
          "File: tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py -> tests/providers/cncf/kubernetes/executors/test_kubernetes_executor.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "92:     @staticmethod",
          "93:     def _is_valid_pod_id(name):",
          "94:         regex = r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$\"",
          "97:     @staticmethod",
          "98:     def _is_safe_label_value(value):",
          "",
          "[Removed Lines]",
          "95:         return len(name) <= 253 and all(ch.lower() == ch for ch in name) and re.match(regex, name)",
          "",
          "[Added Lines]",
          "95:         return len(name) <= 253 and name.islower() and re.match(regex, name)",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "2c1d4d691ff37bf1bd7c9da5f7d2c79fbe202565",
      "candidate_info": {
        "commit_hash": "2c1d4d691ff37bf1bd7c9da5f7d2c79fbe202565",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/2c1d4d691ff37bf1bd7c9da5f7d2c79fbe202565",
        "files": [
          "airflow/www/views.py"
        ],
        "message": "Add missing audit logs for Flask actions add, edit and delete (#34090)\n\n(cherry picked from commit 988632fd67abc10375ad9fe2cbd8c9edccc609a5)",
        "before_after_code_files": [
          "airflow/www/views.py||airflow/www/views.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/34775"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "airflow/www/views.py||airflow/www/views.py": [
          "File: airflow/www/views.py -> airflow/www/views.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "5228:     class_permission_name = permissions.RESOURCE_DAG_RUN",
          "5229:     method_permission_name = {",
          "5230:         \"list\": \"read\",",
          "5231:         \"action_clear\": \"edit\",",
          "5232:         \"action_muldelete\": \"delete\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "5230:         \"delete\": \"delete\",",
          "5231:         \"edit\": \"edit\",",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "5605:     class_permission_name = permissions.RESOURCE_TASK_INSTANCE",
          "5606:     method_permission_name = {",
          "5607:         \"list\": \"read\",",
          "5608:         \"action_clear\": \"edit\",",
          "5609:         \"action_muldelete\": \"delete\",",
          "5610:         \"action_set_running\": \"edit\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "5610:         \"delete\": \"delete\",",
          "",
          "---------------"
        ]
      }
    }
  ]
}