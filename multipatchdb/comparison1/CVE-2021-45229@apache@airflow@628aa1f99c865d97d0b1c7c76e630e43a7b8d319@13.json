{
  "cve_id": "CVE-2021-45229",
  "cve_desc": "It was discovered that the \"Trigger DAG with config\" screen was susceptible to XSS attacks via the `origin` query argument. This issue affects Apache Airflow versions 2.2.3 and below.",
  "repo": "apache/airflow",
  "patch_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
  "patch_info": {
    "commit_hash": "628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "repo": "apache/airflow",
    "commit_url": "https://github.com/apache/airflow/commit/628aa1f99c865d97d0b1c7c76e630e43a7b8d319",
    "files": [
      "airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py"
    ],
    "message": "Simplify trigger cancel button (#21591)\n\nCo-authored-by: Jed Cunningham <jedcunningham@apache.org>\n(cherry picked from commit 65297673a318660fba76797e50d0c06804dfcafc)",
    "before_after_code_files": [
      "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html",
      "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py"
    ]
  },
  "patch_diff": {
    "airflow/www/templates/airflow/trigger.html||airflow/www/templates/airflow/trigger.html": [
      "File: airflow/www/templates/airflow/trigger.html -> airflow/www/templates/airflow/trigger.html",
      "--- Hunk 1 ---",
      "[Context before]",
      "63:       </label>",
      "64:     </div>",
      "65:     <button type=\"submit\" class=\"btn btn-primary\">Trigger</button>",
      "67:   </form>",
      "68: {% endblock %}",
      "",
      "[Removed Lines]",
      "66:     <button type=\"button\" class=\"btn\" onclick=\"location.href = '{{ origin }}'; return false\">Cancel</button>",
      "",
      "[Added Lines]",
      "66:     <a class=\"btn\" href=\"{{ origin }}\">Cancel</a>",
      "",
      "---------------"
    ],
    "tests/www/views/test_views_trigger_dag.py||tests/www/views/test_views_trigger_dag.py": [
      "File: tests/www/views/test_views_trigger_dag.py -> tests/www/views/test_views_trigger_dag.py",
      "--- Hunk 1 ---",
      "[Context before]",
      "133:         (\"javascript:alert(1)\", \"/home\"),",
      "134:         (\"http://google.com\", \"/home\"),",
      "135:         (\"36539'%3balert(1)%2f%2f166\", \"/home\"),",
      "136:         (",
      "137:             \"%2Ftree%3Fdag_id%3Dexample_bash_operator';alert(33)//\",",
      "138:             \"/home\",",
      "",
      "[Removed Lines]",
      "[None]",
      "",
      "[Added Lines]",
      "136:         (",
      "137:             '\"><script>alert(99)</script><a href=\"',",
      "138:             \"&#34;&gt;&lt;script&gt;alert(99)&lt;/script&gt;&lt;a href=&#34;\",",
      "139:         ),",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "145:     test_dag_id = \"example_bash_operator\"",
      "147:     resp = admin_client.get(f'trigger?dag_id={test_dag_id}&origin={test_origin}')",
      "156: @pytest.mark.parametrize(",
      "",
      "[Removed Lines]",
      "148:     check_content_in_response(",
      "149:         '<button type=\"button\" class=\"btn\" onclick=\"location.href = \\'{}\\'; return false\">'.format(",
      "150:             expected_origin",
      "151:         ),",
      "152:         resp,",
      "153:     )",
      "",
      "[Added Lines]",
      "152:     check_content_in_response(f'<a class=\"btn\" href=\"{expected_origin}\">Cancel</a>', resp)",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "44f5dc569409417265d7d6db45f40fc821425994",
      "candidate_info": {
        "commit_hash": "44f5dc569409417265d7d6db45f40fc821425994",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/44f5dc569409417265d7d6db45f40fc821425994",
        "files": [
          "setup.py"
        ],
        "message": "Add hdfs requirement for hdfs provider (#19540)\n\n(cherry picked from commit 317953a4c7749bb74c1db750caec53f92c05f1ff)",
        "before_after_code_files": [
          "setup.py||setup.py"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.py||setup.py": [
          "File: setup.py -> setup.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "344: ]",
          "345: hdfs = [",
          "346:     'snakebite-py3',",
          "347: ]",
          "348: hive = [",
          "349:     'hmsclient>=0.1.0',",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "347:     'hdfs[avro,dataframe,kerberos]>=2.0.4',",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cfa1bae5396609980bb974fc4f06c1047595440a",
      "candidate_info": {
        "commit_hash": "cfa1bae5396609980bb974fc4f06c1047595440a",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cfa1bae5396609980bb974fc4f06c1047595440a",
        "files": [
          "scripts/in_container/check_environment.sh"
        ],
        "message": "Make scripts/in_container/check_environment.sh Google Shell Guide Compliant (#19350)\n\n(cherry picked from commit 7b293c548a92d2cd0eea4f9571c007057aa06482)",
        "before_after_code_files": [
          "scripts/in_container/check_environment.sh||scripts/in_container/check_environment.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/in_container/check_environment.sh||scripts/in_container/check_environment.sh": [
          "File: scripts/in_container/check_environment.sh -> scripts/in_container/check_environment.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "40: }",
          "42: function check_service {",
          "48:     while true",
          "49:     do",
          "50:         set +e",
          "53:         set -e",
          "55:             echo  \"${COLOR_GREEN}OK.  ${COLOR_RESET}\"",
          "56:             break",
          "57:         else",
          "58:             echo -n \".\"",
          "60:         fi",
          "62:             echo \"${COLOR_RED}ERROR: Maximum number of retries while checking service. Exiting ${COLOR_RESET}\"",
          "63:             break",
          "64:         else",
          "65:             sleep 1",
          "66:         fi",
          "67:     done",
          "69:         echo \"Service could not be started!\"",
          "70:         echo",
          "73:         echo",
          "75:     fi",
          "76: }",
          "78: function check_integration {",
          "88:         fi",
          "89:         return",
          "90:     fi",
          "92: }",
          "94: function check_db_backend {",
          "97:     if [[ ${BACKEND} == \"postgres\" ]]; then",
          "99:     elif [[ ${BACKEND} == \"mysql\" ]]; then",
          "101:     elif [[ ${BACKEND} == \"mssql\" ]]; then",
          "104:     elif [[ ${BACKEND} == \"sqlite\" ]]; then",
          "105:         return",
          "106:     else",
          "",
          "[Removed Lines]",
          "43:     LABEL=$1",
          "44:     CALL=$2",
          "45:     MAX_CHECK=${3:=1}",
          "47:     echo -n \"${LABEL}: \"",
          "51:         LAST_CHECK_RESULT=$(eval \"${CALL}\" 2>&1)",
          "52:         RES=$?",
          "54:         if [[ ${RES} == 0 ]]; then",
          "59:             MAX_CHECK=$((MAX_CHECK-1))",
          "61:         if [[ ${MAX_CHECK} == 0 ]]; then",
          "68:     if [[ ${RES} != 0 ]]; then",
          "71:         echo \"$ ${CALL}\"",
          "72:         echo \"${LAST_CHECK_RESULT}\"",
          "74:         EXIT_CODE=${RES}",
          "79:     INTEGRATION_LABEL=$1",
          "80:     INTEGRATION_NAME=$2",
          "81:     CALL=$3",
          "82:     MAX_CHECK=${4:=1}",
          "84:     ENV_VAR_NAME=INTEGRATION_${INTEGRATION_NAME^^}",
          "85:     if [[ ${!ENV_VAR_NAME:=} != \"true\" ]]; then",
          "86:         if [[ ! ${DISABLED_INTEGRATIONS} == *\" ${INTEGRATION_NAME}\"* ]]; then",
          "87:             DISABLED_INTEGRATIONS=\"${DISABLED_INTEGRATIONS} ${INTEGRATION_NAME}\"",
          "91:     check_service \"${INTEGRATION_LABEL}\" \"${CALL}\" \"${MAX_CHECK}\"",
          "95:     MAX_CHECK=${1:=1}",
          "98:         check_service \"PostgreSQL\" \"run_nc postgres 5432\" \"${MAX_CHECK}\"",
          "100:         check_service \"MySQL\" \"run_nc mysql 3306\" \"${MAX_CHECK}\"",
          "102:         check_service \"MSSQL\" \"run_nc mssql 1433\" \"${MAX_CHECK}\"",
          "103:         check_service \"MSSQL Login Check\" \"airflow db check\" \"${MAX_CHECK}\"",
          "",
          "[Added Lines]",
          "43:     local label=$1",
          "44:     local call=$2",
          "45:     local max_check=${3:=1}",
          "47:     echo -n \"${label}: \"",
          "51:         local last_check_result",
          "52:         last_check_result=$(eval \"${call}\" 2>&1)",
          "53:         local res=$?",
          "55:         if [[ ${res} == 0 ]]; then",
          "60:             max_check=$((max_check-1))",
          "62:         if [[ ${max_check} == 0 ]]; then",
          "69:     if [[ ${res} != 0 ]]; then",
          "72:         echo \"$ ${call}\"",
          "73:         echo \"${last_check_result}\"",
          "75:         EXIT_CODE=${res}",
          "80:     local integration_label=$1",
          "81:     local integration_name=$2",
          "82:     local call=$3",
          "83:     local max_check=${4:=1}",
          "85:     local env_var_name",
          "86:     env_var_name=INTEGRATION_${integration_name^^}",
          "87:     if [[ ${!env_var_name:=} != \"true\" ]]; then",
          "88:         if [[ ! ${DISABLED_INTEGRATIONS} == *\" ${integration_name}\"* ]]; then",
          "89:             DISABLED_INTEGRATIONS=\"${DISABLED_INTEGRATIONS} ${integration_name}\"",
          "93:     check_service \"${integration_label}\" \"${call}\" \"${max_check}\"",
          "97:     local max_check=${1:=1}",
          "100:         check_service \"PostgreSQL\" \"run_nc postgres 5432\" \"${max_check}\"",
          "102:         check_service \"MySQL\" \"run_nc mysql 3306\" \"${max_check}\"",
          "104:         check_service \"MSSQL\" \"run_nc mssql 1433\" \"${max_check}\"",
          "105:         check_service \"MSSQL Login Check\" \"airflow db check\" \"${max_check}\"",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "7250d89f239a42482faa758a67f6afa3229ad709",
      "candidate_info": {
        "commit_hash": "7250d89f239a42482faa758a67f6afa3229ad709",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/7250d89f239a42482faa758a67f6afa3229ad709",
        "files": [
          "setup.cfg"
        ],
        "message": "Lift off upper bound for MarkupSafe (#20113)\n\nPer discussion and guidance from #19753, opening this PR for review. Based on if all the tests pass, this could be reviewed further. Resolves #19761.\n\n(cherry picked from commit bcacc51a16697a656357c29c7a40240e422e4bf9)",
        "before_after_code_files": [
          "setup.cfg||setup.cfg"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "setup.cfg||setup.cfg": [
          "File: setup.cfg -> setup.cfg",
          "--- Hunk 1 ---",
          "[Context before]",
          "123:     lazy-object-proxy",
          "124:     lockfile>=0.12.2",
          "125:     markdown>=2.5.2, <4.0",
          "127:     marshmallow-oneofschema>=2.0.1",
          "128:     # Required by vendored-in connexion",
          "129:     openapi-spec-validator>=0.2.4",
          "",
          "[Removed Lines]",
          "126:     markupsafe>=1.1.1, <=2.0",
          "",
          "[Added Lines]",
          "126:     markupsafe>=1.1.1",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "f9c444cbaa540582b1d986ed1555626e90d422ff",
      "candidate_info": {
        "commit_hash": "f9c444cbaa540582b1d986ed1555626e90d422ff",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/f9c444cbaa540582b1d986ed1555626e90d422ff",
        "files": [
          ".github/CODEOWNERS",
          ".github/boring-cyborg.yml",
          ".pre-commit-config.yaml",
          "docker_tests/__init__.py",
          "docker_tests/ci_image.py",
          "docker_tests/docker_tests_utils.py",
          "docker_tests/prod_image.py",
          "scripts/ci/docker-compose/local.yml",
          "scripts/ci/images/ci_run_docker_tests.py",
          "scripts/ci/images/ci_wait_for_and_verify_all_ci_images.sh",
          "scripts/ci/images/ci_wait_for_and_verify_all_prod_images.sh",
          "scripts/ci/installed_providers.txt",
          "scripts/ci/libraries/_build_images.sh",
          "scripts/ci/libraries/_initialization.sh",
          "scripts/ci/libraries/_local_mounts.sh",
          "scripts/ci/libraries/_verify_image.sh",
          "scripts/ci/tools/verify_docker_image.sh"
        ],
        "message": "Tests for Docker images in Python (#19737)\n\n(cherry picked from commit 621d17bb77e3160c1a927803e5d190c0e2aade3c)",
        "before_after_code_files": [
          "docker_tests/__init__.py||docker_tests/__init__.py",
          "docker_tests/ci_image.py||docker_tests/ci_image.py",
          "docker_tests/docker_tests_utils.py||docker_tests/docker_tests_utils.py",
          "docker_tests/prod_image.py||docker_tests/prod_image.py",
          "scripts/ci/images/ci_run_docker_tests.py||scripts/ci/images/ci_run_docker_tests.py",
          "scripts/ci/images/ci_wait_for_and_verify_all_ci_images.sh||scripts/ci/images/ci_wait_for_and_verify_all_ci_images.sh",
          "scripts/ci/images/ci_wait_for_and_verify_all_prod_images.sh||scripts/ci/images/ci_wait_for_and_verify_all_prod_images.sh",
          "scripts/ci/libraries/_build_images.sh||scripts/ci/libraries/_build_images.sh",
          "scripts/ci/libraries/_initialization.sh||scripts/ci/libraries/_initialization.sh",
          "scripts/ci/libraries/_local_mounts.sh||scripts/ci/libraries/_local_mounts.sh",
          "scripts/ci/libraries/_verify_image.sh||scripts/ci/libraries/_verify_image.sh",
          "scripts/ci/tools/verify_docker_image.sh||scripts/ci/tools/verify_docker_image.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "docker_tests/__init__.py||docker_tests/__init__.py": [
          "File: docker_tests/__init__.py -> docker_tests/__init__.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "",
          "---------------"
        ],
        "docker_tests/ci_image.py||docker_tests/ci_image.py": [
          "File: docker_tests/ci_image.py -> docker_tests/ci_image.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "18: import subprocess",
          "20: from docker_tests.docker_tests_utils import (",
          "21:     display_dependency_conflict_message,",
          "22:     docker_image,",
          "23:     run_bash_in_docker,",
          "24:     run_command,",
          "25: )",
          "28: class TestFiles:",
          "29:     def test_dist_folder_should_exists(self):",
          "30:         run_bash_in_docker('[ -f /opt/airflow/airflow/www/static/dist/manifest.json ] || exit 1')",
          "33: class TestPythonPackages:",
          "34:     def test_pip_dependencies_conflict(self):",
          "35:         try:",
          "36:             run_command(",
          "37:                 [\"docker\", \"run\", \"--rm\", \"--entrypoint\", \"/bin/bash\", docker_image, \"-c\", 'pip check']",
          "38:             )",
          "39:         except subprocess.CalledProcessError as ex:",
          "40:             display_dependency_conflict_message()",
          "41:             raise ex",
          "",
          "---------------"
        ],
        "docker_tests/docker_tests_utils.py||docker_tests/docker_tests_utils.py": [
          "File: docker_tests/docker_tests_utils.py -> docker_tests/docker_tests_utils.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "18: import os",
          "19: import shlex",
          "20: import subprocess",
          "21: from pathlib import Path",
          "22: from typing import List",
          "24: docker_image = os.environ.get('DOCKER_IMAGE')",
          "25: SOURCE_ROOT = Path(__file__).resolve().parents[1]",
          "27: if not docker_image:",
          "28:     raise Exception(\"The DOCKER_IMAGE environment variable is required\")",
          "31: def run_command(cmd: List[str], print_output_on_error: bool = True, **kwargs):",
          "32:     print(f\"$ {' '.join(shlex.quote(c) for c in cmd)}\")",
          "33:     try:",
          "34:         return subprocess.check_output(cmd, **kwargs).decode()",
          "35:     except subprocess.CalledProcessError as ex:",
          "36:         if print_output_on_error:",
          "37:             print(\"========================= OUTPUT start ============================\")",
          "38:             print(ex.stderr)",
          "39:             print(ex.stdout)",
          "40:             print(\"========================= OUTPUT end ============================\")",
          "41:         raise",
          "44: def run_bash_in_docker(bash_script, **kwargs):",
          "45:     docker_command = [",
          "46:         \"docker\",",
          "47:         \"run\",",
          "48:         \"--rm\",",
          "49:         \"-e\",",
          "50:         \"COLUMNS=180\",",
          "51:         \"--entrypoint\",",
          "52:         \"/bin/bash\",",
          "53:         docker_image,",
          "54:         \"-c\",",
          "55:         bash_script,",
          "56:     ]",
          "57:     return run_command(docker_command, **kwargs)",
          "60: def run_python_in_docker(python_script, **kwargs):",
          "61:     docker_command = [",
          "62:         \"docker\",",
          "63:         \"run\",",
          "64:         \"--rm\",",
          "65:         \"-e\",",
          "66:         \"COLUMNS=180\",",
          "67:         \"-e\",",
          "68:         \"PYTHONDONTWRITEBYTECODE=true\",",
          "69:         docker_image,",
          "70:         \"python\",",
          "71:         \"-c\",",
          "72:         python_script,",
          "73:     ]",
          "74:     return run_command(docker_command, **kwargs)",
          "77: def display_dependency_conflict_message():",
          "78:     print(",
          "79:         \"\"\"",
          "82: The image did not pass 'pip check' verification. This means that there are some conflicting dependencies",
          "83: in the image.",
          "85: It can mean one of those:",
          "87: 1) The main is currently broken (other PRs will fail with the same error)",
          "88: 2) You changed some dependencies in setup.py or setup.cfg and they are conflicting.",
          "92: In case 1) - apologies for the trouble.Please let committers know and they will fix it. You might",
          "93: be asked to rebase to the latest main after the problem is fixed.",
          "95: In case 2) - Follow the steps below:",
          "98:   (repeat it for all python versions)",
          "100: CI image:",
          "102:      ./breeze build-image --upgrade-to-newer-dependencies --python 3.6",
          "104: Production image:",
          "106:      ./breeze build-image --production-image --upgrade-to-newer-dependencies --python 3.6",
          "109:   conflict. Add the limitation that caused the conflict to EAGER_UPGRADE_ADDITIONAL_REQUIREMENTS",
          "110:   variable in Dockerfile.ci. Note that the limitations might be different for Dockerfile.ci and Dockerfile",
          "111:   because not all packages are installed by default in the PROD Dockerfile. So you might find that you",
          "112:   only need to add the limitation to the Dockerfile.ci",
          "115: \"\"\"",
          "116:     )",
          "",
          "---------------"
        ],
        "docker_tests/prod_image.py||docker_tests/prod_image.py": [
          "File: docker_tests/prod_image.py -> docker_tests/prod_image.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: # Licensed to the Apache Software Foundation (ASF) under one",
          "2: # or more contributor license agreements.  See the NOTICE file",
          "3: # distributed with this work for additional information",
          "4: # regarding copyright ownership.  The ASF licenses this file",
          "5: # to you under the Apache License, Version 2.0 (the",
          "6: # \"License\"); you may not use this file except in compliance",
          "7: # with the License.  You may obtain a copy of the License at",
          "8: #",
          "9: #   http://www.apache.org/licenses/LICENSE-2.0",
          "10: #",
          "11: # Unless required by applicable law or agreed to in writing,",
          "12: # software distributed under the License is distributed on an",
          "13: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "14: # KIND, either express or implied.  See the License for the",
          "15: # specific language governing permissions and limitations",
          "16: # under the License.",
          "18: import json",
          "19: import subprocess",
          "20: import tempfile",
          "21: from pathlib import Path",
          "23: import pytest",
          "25: from docker_tests.docker_tests_utils import (",
          "26:     SOURCE_ROOT,",
          "27:     display_dependency_conflict_message,",
          "28:     docker_image,",
          "29:     run_bash_in_docker,",
          "30:     run_command,",
          "31:     run_python_in_docker,",
          "32: )",
          "34: INSTALLED_PROVIDER_PATH = SOURCE_ROOT / \"scripts\" / \"ci\" / \"installed_providers.txt\"",
          "37: class TestCommands:",
          "38:     def test_without_command(self):",
          "39:         \"\"\"Checking the image without a command. It should return non-zero exit code.\"\"\"",
          "40:         with pytest.raises(subprocess.CalledProcessError) as ctx:",
          "41:             run_command([\"docker\", \"run\", \"--rm\", \"-e\", \"COLUMNS=180\", docker_image])",
          "42:         assert 2 == ctx.value.returncode",
          "44:     def test_airflow_command(self):",
          "45:         \"\"\"Checking 'airflow' command  It should return non-zero exit code.\"\"\"",
          "46:         with pytest.raises(subprocess.CalledProcessError) as ctx:",
          "47:             run_command([\"docker\", \"run\", \"--rm\", \"-e\", \"COLUMNS=180\", docker_image, \"airflow\"])",
          "48:         assert 2 == ctx.value.returncode",
          "50:     def test_airflow_version(self):",
          "51:         \"\"\"Checking 'airflow version' command  It should return zero exit code.\"\"\"",
          "52:         output = run_command(",
          "53:             [\"docker\", \"run\", \"--rm\", \"-e\", \"COLUMNS=180\", docker_image, \"airflow\", \"version\"]",
          "54:         )",
          "55:         assert \"2.\" in output",
          "57:     def test_python_version(self):",
          "58:         \"\"\"Checking 'python --version' command  It should return zero exit code.\"\"\"",
          "59:         output = run_command(",
          "60:             [\"docker\", \"run\", \"--rm\", \"-e\", \"COLUMNS=180\", docker_image, \"python\", \"--version\"]",
          "61:         )",
          "62:         assert \"Python 3.\" in output",
          "64:     def test_bash_version(self):",
          "65:         \"\"\"Checking 'bash --version' command  It should return zero exit code.\"\"\"",
          "66:         output = run_command(",
          "67:             [\"docker\", \"run\", \"--rm\", \"-e\", \"COLUMNS=180\", docker_image, \"bash\", \"--version\"]",
          "68:         )",
          "69:         assert \"GNU bash,\" in output",
          "72: class TestPythonPackages:",
          "73:     def test_required_providers_are_installed(self):",
          "74:         lines = (d.strip() for d in INSTALLED_PROVIDER_PATH.read_text().splitlines())",
          "75:         lines = (d for d in lines)",
          "76:         packages_to_install = {f\"apache-airflow-providers-{d.replace('.', '-')}\" for d in lines}",
          "77:         assert len(packages_to_install) != 0",
          "79:         output = run_bash_in_docker(\"airflow providers list --output json\", stderr=subprocess.DEVNULL)",
          "80:         providers = json.loads(output)",
          "81:         packages_installed = {d['package_name'] for d in providers}",
          "82:         assert len(packages_installed) != 0",
          "84:         assert packages_to_install == packages_installed, (",
          "85:             f\"List of expected installed packages and image content mismatch. \"",
          "86:             f\"Check {INSTALLED_PROVIDER_PATH} file.\"",
          "87:         )",
          "89:     def test_pip_dependencies_conflict(self):",
          "90:         try:",
          "91:             run_bash_in_docker(\"pip check\")",
          "92:         except subprocess.CalledProcessError as ex:",
          "93:             display_dependency_conflict_message()",
          "94:             raise ex",
          "96:     PACKAGE_IMPORTS = {",
          "97:         \"amazon\": [\"boto3\", \"botocore\", \"watchtower\"],",
          "98:         \"async\": [\"gevent\", \"eventlet\", \"greenlet\"],",
          "99:         \"azure\": [",
          "100:             'azure.batch',",
          "101:             'azure.cosmos',",
          "102:             'azure.datalake.store',",
          "103:             'azure.identity',",
          "104:             'azure.keyvault',",
          "105:             'azure.kusto.data',",
          "106:             'azure.mgmt.containerinstance',",
          "107:             'azure.mgmt.datalake.store',",
          "108:             'azure.mgmt.resource',",
          "109:             'azure.storage',",
          "110:         ],",
          "111:         \"celery\": [\"celery\", \"flower\", \"vine\"],",
          "112:         \"cncf.kubernetes\": [\"kubernetes\", \"cryptography\"],",
          "113:         \"dask\": [\"cloudpickle\", \"distributed\"],",
          "114:         \"docker\": [\"docker\"],",
          "115:         \"elasticsearch\": [\"elasticsearch\", \"es.elastic\", \"elasticsearch_dsl\"],",
          "116:         \"google\": [",
          "117:             'OpenSSL',",
          "118:             'google.ads',",
          "119:             'googleapiclient',",
          "120:             'google.auth',",
          "121:             'google_auth_httplib2',",
          "122:             'google.cloud.automl',",
          "123:             'google.cloud.bigquery_datatransfer',",
          "124:             'google.cloud.bigtable',",
          "125:             'google.cloud.container',",
          "126:             'google.cloud.datacatalog',",
          "127:             'google.cloud.dataproc',",
          "128:             'google.cloud.dlp',",
          "129:             'google.cloud.kms',",
          "130:             'google.cloud.language',",
          "131:             'google.cloud.logging',",
          "132:             'google.cloud.memcache',",
          "133:             'google.cloud.monitoring',",
          "134:             'google.cloud.oslogin',",
          "135:             'google.cloud.pubsub',",
          "136:             'google.cloud.redis',",
          "137:             'google.cloud.secretmanager',",
          "138:             'google.cloud.spanner',",
          "139:             'google.cloud.speech',",
          "140:             'google.cloud.storage',",
          "141:             'google.cloud.tasks',",
          "142:             'google.cloud.texttospeech',",
          "143:             'google.cloud.translate',",
          "144:             'google.cloud.videointelligence',",
          "145:             'google.cloud.vision',",
          "146:         ],",
          "147:         \"grpc\": [\"grpc\", \"google.auth\", \"google_auth_httplib2\"],",
          "148:         \"hashicorp\": [\"hvac\"],",
          "149:         \"ldap\": [\"ldap\"],",
          "150:         \"mysql\": [\"mysql\"],",
          "151:         \"postgres\": [\"psycopg2\"],",
          "152:         \"pyodbc\": [\"pyodbc\"],",
          "153:         \"redis\": [\"redis\"],",
          "154:         \"sendgrid\": [\"sendgrid\"],",
          "155:         \"sftp/ssh\": [\"paramiko\", \"pysftp\", \"sshtunnel\"],",
          "156:         \"slack\": [\"slack_sdk\"],",
          "157:         \"statsd\": [\"statsd\"],",
          "158:         \"virtualenv\": [\"virtualenv\"],",
          "159:     }",
          "161:     @pytest.mark.parametrize(\"package_name,import_names\", PACKAGE_IMPORTS.items())",
          "162:     def test_check_dependencies_imports(self, package_name, import_names):",
          "163:         run_python_in_docker(f\"import {','.join(import_names)}\")",
          "166: class TestExecuteAsRoot:",
          "167:     def test_execute_airflow_as_root(self):",
          "168:         run_command(",
          "169:             [",
          "170:                 \"docker\",",
          "171:                 \"run\",",
          "172:                 \"--rm\",",
          "173:                 \"--user\",",
          "174:                 \"0\",",
          "175:                 \"-e\",",
          "176:                 \"PYTHONDONTWRITEBYTECODE=true\",",
          "177:                 docker_image,",
          "178:                 \"airflow\",",
          "179:                 \"info\",",
          "180:             ]",
          "181:         )",
          "183:     def test_run_custom_python_packages_as_root(self):",
          "184:         with tempfile.TemporaryDirectory() as tmp_dir:",
          "185:             (Path(tmp_dir) / \"__init__.py\").write_text('')",
          "186:             (Path(tmp_dir) / \"awesome.py\").write_text('print(\"Awesome\")')",
          "188:             run_command(",
          "189:                 [",
          "190:                     \"docker\",",
          "191:                     \"run\",",
          "192:                     \"--rm\",",
          "193:                     \"-e\",",
          "194:                     f\"PYTHONPATH={tmp_dir}\",",
          "195:                     \"-e\",",
          "196:                     \"PYTHONDONTWRITEBYTECODE=true\",",
          "197:                     \"-v\",",
          "198:                     f\"{tmp_dir}:{tmp_dir}\",",
          "199:                     \"--user\",",
          "200:                     \"0\",",
          "201:                     docker_image,",
          "202:                     \"python\",",
          "203:                     \"-c\",",
          "204:                     \"import awesome\",",
          "205:                 ]",
          "206:             )",
          "",
          "---------------"
        ],
        "scripts/ci/images/ci_run_docker_tests.py||scripts/ci/images/ci_run_docker_tests.py": [
          "File: scripts/ci/images/ci_run_docker_tests.py -> scripts/ci/images/ci_run_docker_tests.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1: #!/usr/bin/env python3",
          "2: # Licensed to the Apache Software Foundation (ASF) under one",
          "3: # or more contributor license agreements.  See the NOTICE file",
          "4: # distributed with this work for additional information",
          "5: # regarding copyright ownership.  The ASF licenses this file",
          "6: # to you under the Apache License, Version 2.0 (the",
          "7: # \"License\"); you may not use this file except in compliance",
          "8: # with the License.  You may obtain a copy of the License at",
          "9: #",
          "10: #   http://www.apache.org/licenses/LICENSE-2.0",
          "11: #",
          "12: # Unless required by applicable law or agreed to in writing,",
          "13: # software distributed under the License is distributed on an",
          "14: # \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "19: import argparse",
          "20: import shlex",
          "21: import subprocess",
          "22: import sys",
          "23: from pathlib import Path",
          "24: from typing import List",
          "26: AIRFLOW_SOURCE = Path(__file__).resolve().parent.parent.parent",
          "27: BUILD_CACHE_DIR = AIRFLOW_SOURCE / \".build\"",
          "29: CBLUE = '\\033[94m'",
          "30: CEND = '\\033[0m'",
          "33: def get_parser():",
          "34:     parser = argparse.ArgumentParser(",
          "35:         prog=\"ci_run_docker_tests\",",
          "36:         description=\"Running Docker tests using pytest\",",
          "37:         epilog=\"Unknown arguments are passed unchanged to Pytest.\",",
          "38:     )",
          "39:     parser.add_argument(",
          "40:         \"--interactive\",",
          "41:         \"-i\",",
          "42:         action='store_true',",
          "43:         help=\"Activates virtual environment ready to run tests and drops you in\",",
          "44:     )",
          "45:     parser.add_argument(\"--initialize\", action=\"store_true\", help=\"Initialize virtual environment and exit\")",
          "46:     parser.add_argument(\"pytestopts\", nargs=argparse.REMAINDER, help=\"Tests to run\")",
          "47:     return parser",
          "50: def run_verbose(cmd: List[str], **kwargs):",
          "51:     print(f\"{CBLUE}$ {' '.join(shlex.quote(c) for c in cmd)}{CEND}\")",
          "52:     subprocess.run(cmd, **kwargs)",
          "55: def create_virtualenv():",
          "56:     virtualenv_path = (",
          "57:         BUILD_CACHE_DIR / \".docker_venv\" / f\"host_python_{sys.version_info[0]}.{sys.version_info[1]}\"",
          "58:     )",
          "59:     virtualenv_path.parent.mkdir(parents=True, exist_ok=True)",
          "60:     if not virtualenv_path.exists():",
          "61:         print(\"Creating virtualenv environment\")",
          "62:         run_verbose([sys.executable, \"-m\", \"venv\", str(virtualenv_path)])",
          "64:     python_bin = virtualenv_path / \"bin\" / \"python\"",
          "65:     run_verbose([str(python_bin), \"-m\", \"pip\", \"install\", \"pytest\", \"pytest-xdist\"])",
          "66:     return python_bin",
          "69: def main():",
          "70:     parser = get_parser()",
          "71:     args = parser.parse_args()",
          "73:     python_bin = create_virtualenv()",
          "75:     if args.initialize:",
          "76:         return",
          "77:     if args.interactive:",
          "78:         activate_bin = python_bin.parent / \"activate\"",
          "79:         bash_trampoline = f\"source {shlex.quote(str(activate_bin))}\"",
          "80:         print(\"To enter virtual environment, run:\")",
          "81:         print(f\"    {bash_trampoline}\")",
          "82:         return",
          "84:     extra_pytest_args = (",
          "85:         args.pytestopts[1:] if args.pytestopts and args.pytestopts[0] == \"--\" else args.pytestopts",
          "86:     )",
          "87:     if not extra_pytest_args:",
          "88:         raise SystemExit(\"You must select the tests to run.\")",
          "90:     pytest_args = (",
          "91:         \"--pythonwarnings=ignore::DeprecationWarning\",",
          "92:         \"--pythonwarnings=ignore::PendingDeprecationWarning\",",
          "93:         \"-n\",",
          "94:         \"auto\",",
          "95:     )",
          "97:     run_verbose([str(python_bin), \"-m\", \"pytest\", *pytest_args, *extra_pytest_args])",
          "100: if __name__ == \"__main__\":",
          "101:     main()",
          "",
          "---------------"
        ],
        "scripts/ci/images/ci_wait_for_and_verify_all_ci_images.sh||scripts/ci/images/ci_wait_for_and_verify_all_ci_images.sh": [
          "File: scripts/ci/images/ci_wait_for_and_verify_all_ci_images.sh -> scripts/ci/images/ci_wait_for_and_verify_all_ci_images.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "23: # shellcheck source=scripts/ci/libraries/_all_libs.sh",
          "24: source \"${LIBRARIES_DIR}/_all_libs.sh\"",
          "26: initialization::set_output_color_variables",
          "28: export PARALLEL_TAIL_LENGTH=5",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "26: python3 \"$( dirname \"${BASH_SOURCE[0]}\" )/ci_run_docker_tests.py\" \"--initialize\"",
          "",
          "---------------"
        ],
        "scripts/ci/images/ci_wait_for_and_verify_all_prod_images.sh||scripts/ci/images/ci_wait_for_and_verify_all_prod_images.sh": [
          "File: scripts/ci/images/ci_wait_for_and_verify_all_prod_images.sh -> scripts/ci/images/ci_wait_for_and_verify_all_prod_images.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "26: initialization::set_output_color_variables",
          "28: export PARALLEL_TAIL_LENGTH=5",
          "30: parallel::make_sure_gnu_parallel_is_installed",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "28: python3 \"$( dirname \"${BASH_SOURCE[0]}\" )/ci_run_docker_tests.py\" \"--initialize\"",
          "",
          "---------------"
        ],
        "scripts/ci/libraries/_build_images.sh||scripts/ci/libraries/_build_images.sh": [
          "File: scripts/ci/libraries/_build_images.sh -> scripts/ci/libraries/_build_images.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "938:     build_images::cleanup_docker_context_files",
          "940:     # Build necessary provider packages",
          "942:     mv \"${AIRFLOW_SOURCES}/dist/\"* \"${AIRFLOW_SOURCES}/docker-context-files/\"",
          "944:     # Build apache airflow packages",
          "",
          "[Removed Lines]",
          "941:     runs::run_prepare_provider_packages \"${INSTALLED_PROVIDERS[@]}\"",
          "",
          "[Added Lines]",
          "941:     IFS=$'\\n' read -d '' -r -a installed_providers < \"${AIRFLOW_SOURCES}/scripts/ci/installed_providers.txt\"",
          "942:     runs::run_prepare_provider_packages \"${installed_providers[@]}\"",
          "",
          "---------------"
        ],
        "scripts/ci/libraries/_initialization.sh||scripts/ci/libraries/_initialization.sh": [
          "File: scripts/ci/libraries/_initialization.sh -> scripts/ci/libraries/_initialization.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "416:     INSTALL_PROVIDERS_FROM_SOURCES=${INSTALL_PROVIDERS_FROM_SOURCES:=\"true\"}",
          "417:     export INSTALL_PROVIDERS_FROM_SOURCES",
          "443:     export INSTALLED_EXTRAS=\"async,amazon,celery,cncf.kubernetes,docker,dask,elasticsearch,ftp,grpc,hashicorp,http,imap,ldap,google,microsoft.azure,mysql,postgres,redis,sendgrid,sftp,slack,ssh,statsd,virtualenv\"",
          "445:     AIRFLOW_PIP_VERSION=${AIRFLOW_PIP_VERSION:=\"21.2.4\"}",
          "",
          "[Removed Lines]",
          "419:     INSTALLED_PROVIDERS+=(",
          "420:         \"amazon\"",
          "421:         \"celery\"",
          "422:         \"cncf.kubernetes\"",
          "423:         \"docker\"",
          "424:         \"elasticsearch\"",
          "425:         \"ftp\"",
          "426:         \"grpc\"",
          "427:         \"hashicorp\"",
          "428:         \"http\"",
          "429:         \"imap\"",
          "430:         \"google\"",
          "431:         \"microsoft.azure\"",
          "432:         \"mysql\"",
          "433:         \"postgres\"",
          "434:         \"redis\"",
          "435:         \"sendgrid\"",
          "436:         \"sqlite\"",
          "437:         \"sftp\"",
          "438:         \"slack\"",
          "439:         \"sqlite\"",
          "440:         \"ssh\"",
          "441:     )",
          "442:     export INSTALLED_PROVIDERS",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ],
        "scripts/ci/libraries/_local_mounts.sh||scripts/ci/libraries/_local_mounts.sh": [
          "File: scripts/ci/libraries/_local_mounts.sh -> scripts/ci/libraries/_local_mounts.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "50:         \"$prefix\"setup.py:/opt/airflow/setup.py:cached",
          "51:         \"$prefix\"tests:/opt/airflow/tests:cached",
          "52:         \"$prefix\"kubernetes_tests:/opt/airflow/kubernetes_tests:cached",
          "53:         \"$prefix\"chart:/opt/airflow/chart:cached",
          "54:         \"$prefix\"metastore_browser:/opt/airflow/metastore_browser:cached",
          "55:     )",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "53:         \"$prefix\"docker_tests:/opt/airflow/docker_tests:cached",
          "",
          "---------------"
        ],
        "scripts/ci/libraries/_verify_image.sh||scripts/ci/libraries/_verify_image.sh": [
          "File: scripts/ci/libraries/_verify_image.sh -> scripts/ci/libraries/_verify_image.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "15: # KIND, either express or implied.  See the License for the",
          "16: # specific language governing permissions and limitations",
          "17: # under the License.",
          "361: function verify_image::verify_prod_image {",
          "363:     DOCKER_IMAGE=\"${1}\"",
          "377: }",
          "379: function verify_image::verify_ci_image {",
          "381:     DOCKER_IMAGE=\"${1}\"",
          "387: }",
          "",
          "[Removed Lines]",
          "18: function verify_image::run_command_in_image() {",
          "19:     docker_v run --rm \\",
          "20:             -e COLUMNS=180 \\",
          "21:             --entrypoint /bin/bash \"${DOCKER_IMAGE}\" \\",
          "22:             -c \"${@}\"",
          "23: }",
          "25: IMAGE_VALID=\"true\"",
          "27: function verify_image::check_command() {",
          "28:     DESCRIPTION=\"${1}\"",
          "29:     COMMAND=${2}",
          "30:     set +e",
          "31:     echo -n \"Feature: ${DESCRIPTION} \"",
          "32:     local output",
          "33:     output=$(verify_image::run_command_in_image \"${COMMAND}\" 2>&1)",
          "34:     local res=$?",
          "35:     if [[ ${res} == \"0\" ]]; then",
          "36:         echo \"${COLOR_GREEN}OK${COLOR_RESET}\"",
          "37:     else",
          "38:         echo \"${COLOR_RED}NOK${COLOR_RESET}\"",
          "39:         echo \"${COLOR_BLUE}========================= OUTPUT start ============================${COLOR_RESET}\"",
          "40:         echo \"${output}\"",
          "41:         echo \"${COLOR_BLUE}========================= OUTPUT end   ===========================${COLOR_RESET}\"",
          "42:         IMAGE_VALID=\"false\"",
          "43:     fi",
          "44:     set -e",
          "45: }",
          "47: function verify_image::verify_prod_image_commands() {",
          "48:     start_end::group_start \"Checking command supports\"",
          "49:     set +e",
          "51:     echo -n \"Feature: Checking the image without a command. It should return non-zero exit code.\"",
          "52:     local output",
          "53:     output=$(docker_v run --rm \\",
          "54:             -e COLUMNS=180 \\",
          "55:             \"${DOCKER_IMAGE}\" \\",
          "56:             2>&1)",
          "57:     local res=$?",
          "58:     if [[ ${res} == \"2\" ]]; then",
          "59:         echo \"${COLOR_GREEN}OK${COLOR_RESET}\"",
          "60:     else",
          "61:         echo \"${COLOR_RED}NOK${COLOR_RESET}\"",
          "62:         echo \"${COLOR_BLUE}========================= OUTPUT start ============================${COLOR_RESET}\"",
          "63:         echo \"${output}\"",
          "64:         echo \"${COLOR_BLUE}========================= OUTPUT end   ===========================${COLOR_RESET}\"",
          "65:         IMAGE_VALID=\"false\"",
          "66:     fi",
          "67:     echo -n \"Feature: Checking 'airflow' command  It should return non-zero exit code.\"",
          "68:     output=$(docker_v run --rm \\",
          "69:             -e COLUMNS=180 \\",
          "70:             \"${DOCKER_IMAGE}\" \\",
          "71:             \"airflow\" 2>&1)",
          "72:     local res=$?",
          "73:     if [[ ${res} == \"2\" ]]; then",
          "74:         echo \"${COLOR_GREEN}OK${COLOR_RESET}\"",
          "75:     else",
          "76:         echo \"${COLOR_RED}NOK${COLOR_RESET}\"",
          "77:         echo \"${COLOR_BLUE}========================= OUTPUT start ============================${COLOR_RESET}\"",
          "78:         echo \"${output}\"",
          "79:         echo \"${COLOR_BLUE}========================= OUTPUT end   ===========================${COLOR_RESET}\"",
          "80:         IMAGE_VALID=\"false\"",
          "81:     fi",
          "83:     echo -n \"Feature: Checking 'airflow version' command  It should return zero exit code.\"",
          "84:     output=$(docker_v run --rm \\",
          "85:             -e COLUMNS=180 \\",
          "86:             \"${DOCKER_IMAGE}\" \\",
          "87:             \"airflow\" \"version\" 2>&1)",
          "88:     local res=$?",
          "89:     if [[ ${res} == \"0\" ]]; then",
          "90:         echo \"${COLOR_GREEN}OK${COLOR_RESET}\"",
          "91:     else",
          "92:         echo \"${COLOR_RED}NOK${COLOR_RESET}\"",
          "93:         echo \"${COLOR_BLUE}========================= OUTPUT start ============================${COLOR_RESET}\"",
          "94:         echo \"${output}\"",
          "95:         echo \"${COLOR_BLUE}========================= OUTPUT end   ===========================${COLOR_RESET}\"",
          "96:         IMAGE_VALID=\"false\"",
          "97:     fi",
          "99:     echo -n \"Feature: Checking 'python --version' command  It should return zero exit code.\"",
          "100:     output=$(docker_v run --rm \\",
          "101:             -e COLUMNS=180 \\",
          "102:             \"${DOCKER_IMAGE}\" \\",
          "103:             python --version | grep \"Python 3.\" 2>&1)",
          "104:     local res=$?",
          "105:     if [[ ${res} == \"0\" ]]; then",
          "106:         echo \"${COLOR_GREEN}OK${COLOR_RESET}\"",
          "107:     else",
          "108:         echo \"${COLOR_RED}NOK${COLOR_RESET}\"",
          "109:         echo \"${COLOR_BLUE}========================= OUTPUT start ============================${COLOR_RESET}\"",
          "110:         echo \"${output}\"",
          "111:         echo \"${COLOR_BLUE}========================= OUTPUT end   ===========================${COLOR_RESET}\"",
          "112:         IMAGE_VALID=\"false\"",
          "113:     fi",
          "114:     echo -n \"Feature: Checking 'bash --version' command  It should return zero exit code.\"",
          "115:     output=$(docker_v run --rm \\",
          "116:             -e COLUMNS=180 \\",
          "117:             \"${DOCKER_IMAGE}\" \\",
          "118:             bash --version | grep \"GNU bash, \" 2>&1)",
          "119:     local res=$?",
          "120:     if [[ ${res} == \"0\" ]]; then",
          "121:         echo \"${COLOR_GREEN}OK${COLOR_RESET}\"",
          "122:     else",
          "123:         echo \"${COLOR_RED}NOK${COLOR_RESET}\"",
          "124:         echo \"${COLOR_BLUE}========================= OUTPUT start ============================${COLOR_RESET}\"",
          "125:         echo \"${output}\"",
          "126:         echo \"${COLOR_BLUE}========================= OUTPUT end   ===========================${COLOR_RESET}\"",
          "127:         IMAGE_VALID=\"false\"",
          "128:     fi",
          "130:     set -e",
          "131: }",
          "133: function verify_image::verify_prod_image_has_airflow_and_providers() {",
          "134:     start_end::group_start \"Verify prod image: ${DOCKER_IMAGE}\"",
          "135:     echo",
          "136:     echo \"Checking if Providers are installed\"",
          "137:     echo",
          "139:     all_providers_installed_in_image=$(verify_image::run_command_in_image \"airflow providers list --output table\")",
          "141:     echo",
          "142:     echo \"Installed providers:\"",
          "143:     echo",
          "144:     echo \"${all_providers_installed_in_image}\"",
          "145:     echo",
          "146:     local error=\"false\"",
          "147:     for provider in \"${INSTALLED_PROVIDERS[@]}\"; do",
          "148:         echo -n \"Verifying if provider ${provider} installed: \"",
          "149:         if [[ ${all_providers_installed_in_image} == *\"apache-airflow-providers-${provider//./-}\"* ]]; then",
          "150:             echo \"${COLOR_GREEN}OK${COLOR_RESET}\"",
          "151:         else",
          "152:             echo \"${COLOR_RED}NOK${COLOR_RESET}\"",
          "153:             error=\"true\"",
          "154:         fi",
          "155:     done",
          "156:     if [[ ${error} == \"true\" ]]; then",
          "157:         echo",
          "158:         echo \"${COLOR_RED}ERROR: Some expected providers are not installed!${COLOR_RESET}\"",
          "159:         echo",
          "160:         IMAGE_VALID=\"false\"",
          "161:     else",
          "162:         echo",
          "163:         echo \"${COLOR_GREEN}OK. All expected providers installed!${COLOR_RESET}\"",
          "164:         echo",
          "165:     fi",
          "166:     start_end::group_end",
          "167: }",
          "169: function verify_image::verify_ci_image_dependencies() {",
          "170:     start_end::group_start \"Checking if Airflow dependencies are non-conflicting in ${DOCKER_IMAGE} image.\"",
          "171:     set +e",
          "172:     docker_v run --rm --entrypoint /bin/bash \"${DOCKER_IMAGE}\" -c 'pip check'",
          "173:     local res=$?",
          "174:     if [[ ${res} != \"0\" ]]; then",
          "175:         echo  \"${COLOR_RED}ERROR: ^^^ Some dependencies are conflicting. See instructions below on how to deal with it.  ${COLOR_RESET}\"",
          "176:         echo",
          "177:         build_images::inform_about_pip_check \"\"",
          "178:         IMAGE_VALID=\"false\"",
          "179:     else",
          "180:         echo",
          "181:         echo  \"${COLOR_GREEN}OK. The ${DOCKER_IMAGE} image dependencies are consistent.  ${COLOR_RESET}\"",
          "182:         echo",
          "183:     fi",
          "184:     set -e",
          "185:     start_end::group_end",
          "186: }",
          "188: function verify_image::verify_ci_image_has_dist_folder() {",
          "189:     start_end::group_start \"Verify CI image dist folder (compiled www assets): ${DOCKER_IMAGE}\"",
          "191:     verify_image::check_command \"Dist folder\" '[ -f /opt/airflow/airflow/www/static/dist/manifest.json ] || exit 1'",
          "193:     start_end::group_end",
          "194: }",
          "197: function verify_image::verify_prod_image_dependencies() {",
          "198:     start_end::group_start \"Checking if Airflow dependencies are non-conflicting in ${DOCKER_IMAGE} image.\"",
          "200:     set +e",
          "201:     verify_image::run_command_in_image 'pip check'",
          "202:     local res=$?",
          "203:     if [[ ${res} != \"0\" ]]; then",
          "204:         echo \"${COLOR_RED}ERROR: ^^^ Some dependencies are conflicting. See instructions below on how to deal with it.  ${COLOR_RESET}\"",
          "205:         echo",
          "206:         build_images::inform_about_pip_check \"--production \"",
          "207:         IMAGE_VALID=\"false\"",
          "208:     else",
          "209:         echo",
          "210:         echo \"${COLOR_GREEN}OK. The ${DOCKER_IMAGE} image dependencies are consistent.  ${COLOR_RESET}\"",
          "211:         echo",
          "212:     fi",
          "213:     set -e",
          "214:     start_end::group_end",
          "215: }",
          "217: GOOGLE_IMPORTS=(",
          "218:     'OpenSSL'",
          "219:     'google.ads'",
          "220:     'googleapiclient'",
          "221:     'google.auth'",
          "222:     'google_auth_httplib2'",
          "223:     'google.cloud.automl'",
          "224:     'google.cloud.bigquery_datatransfer'",
          "225:     'google.cloud.bigtable'",
          "226:     'google.cloud.container'",
          "227:     'google.cloud.datacatalog'",
          "228:     'google.cloud.dataproc'",
          "229:     'google.cloud.dlp'",
          "230:     'google.cloud.kms'",
          "231:     'google.cloud.language'",
          "232:     'google.cloud.logging'",
          "233:     'google.cloud.memcache'",
          "234:     'google.cloud.monitoring'",
          "235:     'google.cloud.oslogin'",
          "236:     'google.cloud.pubsub'",
          "237:     'google.cloud.redis'",
          "238:     'google.cloud.secretmanager'",
          "239:     'google.cloud.spanner'",
          "240:     'google.cloud.speech'",
          "241:     'google.cloud.storage'",
          "242:     'google.cloud.tasks'",
          "243:     'google.cloud.texttospeech'",
          "244:     'google.cloud.translate'",
          "245:     'google.cloud.videointelligence'",
          "246:     'google.cloud.vision'",
          "247: )",
          "249: AZURE_IMPORTS=(",
          "250:     'azure.batch'",
          "251:     'azure.cosmos'",
          "252:     'azure.datalake.store'",
          "253:     'azure.identity'",
          "254:     'azure.keyvault'",
          "255:     'azure.kusto.data'",
          "256:     'azure.mgmt.containerinstance'",
          "257:     'azure.mgmt.datalake.store'",
          "258:     'azure.mgmt.resource'",
          "259:     'azure.storage'",
          "260: )",
          "262: function verify_image::verify_production_image_python_modules() {",
          "263:     start_end::group_start \"Verify prod image features: ${DOCKER_IMAGE}\"",
          "265:     verify_image::check_command \"Import: async\" \"python -c 'import gevent, eventlet, greenlet'\"",
          "266:     verify_image::check_command \"Import: amazon\" \"python -c 'import boto3, botocore, watchtower'\"",
          "267:     verify_image::check_command \"Import: celery\" \"python -c 'import celery, flower, vine'\"",
          "268:     verify_image::check_command \"Import: cncf.kubernetes\" \"python -c 'import kubernetes, cryptography'\"",
          "269:     verify_image::check_command \"Import: docker\" \"python -c 'import docker'\"",
          "270:     verify_image::check_command \"Import: dask\" \"python -c 'import cloudpickle, distributed'\"",
          "271:     verify_image::check_command \"Import: elasticsearch\" \"python -c 'import elasticsearch,es.elastic, elasticsearch_dsl'\"",
          "272:     verify_image::check_command \"Import: grpc\" \"python -c 'import grpc, google.auth, google_auth_httplib2'\"",
          "273:     verify_image::check_command \"Import: hashicorp\" \"python -c 'import hvac'\"",
          "274:     verify_image::check_command \"Import: ldap\" \"python -c 'import ldap'\"",
          "275:     for google_import in \"${GOOGLE_IMPORTS[@]}\"",
          "276:     do",
          "277:         verify_image::check_command \"Import google: ${google_import}\" \"python -c 'import ${google_import}'\"",
          "278:     done",
          "279:     for azure_import in \"${AZURE_IMPORTS[@]}\"",
          "280:     do",
          "281:         verify_image::check_command \"Import azure: ${azure_import}\" \"python -c 'import ${azure_import}'\"",
          "282:     done",
          "283:     verify_image::check_command \"Import: mysql\" \"python -c 'import mysql'\"",
          "284:     verify_image::check_command \"Import: postgres\" \"python -c 'import psycopg2'\"",
          "285:     verify_image::check_command \"Import: redis\" \"python -c 'import redis'\"",
          "286:     verify_image::check_command \"Import: sendgrid\" \"python -c 'import sendgrid'\"",
          "287:     verify_image::check_command \"Import: sftp/ssh\" \"python -c 'import paramiko, pysftp, sshtunnel'\"",
          "288:     verify_image::check_command \"Import: slack\" \"python -c 'import slack_sdk'\"",
          "289:     verify_image::check_command \"Import: statsd\" \"python -c 'import statsd'\"",
          "290:     verify_image::check_command \"Import: virtualenv\" \"python -c 'import virtualenv'\"",
          "291:     verify_image::check_command \"Import: pyodbc\" \"python -c 'import pyodbc'\"",
          "293:     start_end::group_end",
          "294: }",
          "296: function verify_image::verify_prod_image_as_root() {",
          "297:     start_end::group_start \"Checking if the image can be run as root.\"",
          "298:     set +e",
          "299:     echo \"Checking airflow as root\"",
          "300:     local output",
          "301:     local res",
          "302:     output=$(docker_v run --rm --user 0 \"${DOCKER_IMAGE}\" \"airflow\" \"info\" 2>&1)",
          "303:     res=$?",
          "304:     if [[ ${res} == \"0\" ]]; then",
          "305:         echo \"${COLOR_GREEN}OK${COLOR_RESET}\"",
          "306:     else",
          "307:         echo \"${COLOR_RED}NOK${COLOR_RESET}\"",
          "308:         echo \"${COLOR_BLUE}========================= OUTPUT start ============================${COLOR_RESET}\"",
          "309:         echo \"${output}\"",
          "310:         echo \"${COLOR_BLUE}========================= OUTPUT end   ===========================${COLOR_RESET}\"",
          "311:         IMAGE_VALID=\"false\"",
          "312:     fi",
          "314:     echo \"Checking root container with custom PYTHONPATH\"",
          "315:     local tmp_dir",
          "316:     tmp_dir=\"$(mktemp -d)\"",
          "317:     touch \"${tmp_dir}/__init__.py\"",
          "318:     echo 'print(\"Awesome\")' >> \"${tmp_dir}/awesome.py\"",
          "319:     output=$(docker_v run \\",
          "320:         --rm \\",
          "321:         -e \"PYTHONPATH=${tmp_dir}\" \\",
          "322:         -v \"${tmp_dir}:${tmp_dir}\" \\",
          "323:         --user 0 \"${DOCKER_IMAGE}\" \\",
          "324:             \"python\" \"-c\" \"import awesome\" \\",
          "325:         2>&1)",
          "326:     res=$?",
          "327:     if [[ ${res} == \"0\" ]]; then",
          "328:         echo \"${COLOR_GREEN}OK${COLOR_RESET}\"",
          "329:     else",
          "330:         echo \"${COLOR_RED}NOK${COLOR_RESET}\"",
          "331:         echo \"${COLOR_BLUE}========================= OUTPUT start ============================${COLOR_RESET}\"",
          "332:         echo \"${output}\"",
          "333:         echo \"${COLOR_BLUE}========================= OUTPUT end   ===========================${COLOR_RESET}\"",
          "334:         IMAGE_VALID=\"false\"",
          "335:     fi",
          "336:     rm -rf \"${tmp_dir}\"",
          "337:     set -e",
          "338: }",
          "340: function verify_image::verify_production_image_has_dist_folder() {",
          "341:     start_end::group_start \"Verify prod image has dist folder (compiled www assets): ${DOCKER_IMAGE}\"",
          "342:     # shellcheck disable=SC2016",
          "343:     verify_image::check_command \"Dist folder\" '[ -f $(python -m site --user-site)/airflow/www/static/dist/manifest.json ] || exit 1'",
          "345:     start_end::group_end",
          "346: }",
          "348: function verify_image::display_result {",
          "349:     if [[ ${IMAGE_VALID} == \"true\" ]]; then",
          "350:         echo",
          "351:         echo \"${COLOR_GREEN}OK. The ${DOCKER_IMAGE} features are all OK.  ${COLOR_RESET}\"",
          "352:         echo",
          "353:     else",
          "354:         echo",
          "355:         echo \"${COLOR_RED}ERROR: Some features were not ok!${COLOR_RESET}\"",
          "356:         echo",
          "357:         exit 1",
          "358:     fi",
          "359: }",
          "362:     IMAGE_VALID=\"true\"",
          "364:     verify_image::verify_prod_image_commands",
          "366:     verify_image::verify_prod_image_has_airflow_and_providers",
          "368:     verify_image::verify_production_image_python_modules",
          "370:     verify_image::verify_prod_image_dependencies",
          "372:     verify_image::verify_prod_image_as_root",
          "374:     verify_image::verify_production_image_has_dist_folder",
          "376:     verify_image::display_result",
          "380:     IMAGE_VALID=\"true\"",
          "382:     verify_image::verify_ci_image_dependencies",
          "384:     verify_image::verify_ci_image_has_dist_folder",
          "386:     verify_image::display_result",
          "",
          "[Added Lines]",
          "21:     export DOCKER_IMAGE",
          "22:     python3 \"${SCRIPTS_CI_DIR}/images/ci_run_docker_tests.py\" \"${AIRFLOW_SOURCES}/docker_tests/prod_image.py\"",
          "27:     export DOCKER_IMAGE",
          "28:     python3 \"${SCRIPTS_CI_DIR}/images/ci_run_docker_tests.py\" \"${AIRFLOW_SOURCES}/docker_tests/ci_image.py\"",
          "",
          "---------------"
        ],
        "scripts/ci/tools/verify_docker_image.sh||scripts/ci/tools/verify_docker_image.sh": [
          "File: scripts/ci/tools/verify_docker_image.sh -> scripts/ci/tools/verify_docker_image.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "[No context available]",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "[None]",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "cc9a03461d1bcdc742e170a572e1b6e91917e847",
      "candidate_info": {
        "commit_hash": "cc9a03461d1bcdc742e170a572e1b6e91917e847",
        "repo": "apache/airflow",
        "commit_url": "https://github.com/apache/airflow/commit/cc9a03461d1bcdc742e170a572e1b6e91917e847",
        "files": [
          "scripts/ci/libraries/_build_images.sh"
        ],
        "message": "Logs in to Github Registry when preparing cache (#21069)\n\nWhe we are preparing cache on CI, we should login to the\nGitHub registry (using GITHUB_TOKEN) in order for --cache-to\nto be able to push images.",
        "before_after_code_files": [
          "scripts/ci/libraries/_build_images.sh||scripts/ci/libraries/_build_images.sh"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/apache/airflow/pull/21659"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "scripts/ci/libraries/_build_images.sh||scripts/ci/libraries/_build_images.sh": [
          "File: scripts/ci/libraries/_build_images.sh -> scripts/ci/libraries/_build_images.sh",
          "--- Hunk 1 ---",
          "[Context before]",
          "470:         exit 1",
          "471:     fi",
          "472:     if [[ ${PREPARE_BUILDX_CACHE} == \"true\" ]]; then",
          "473:         docker_ci_cache_directive+=(",
          "474:             \"--cache-to=type=registry,ref=${AIRFLOW_CI_IMAGE}:cache\"",
          "475:             \"--load\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "473:         # we need to login to docker registry so that we can push cache there",
          "474:         build_images::login_to_docker_registry",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "624:         exit 1",
          "625:     fi",
          "626:     if [[ ${PREPARE_BUILDX_CACHE} == \"true\" ]]; then",
          "627:         # Cache for prod image contains also build stage for buildx when mode=max specified!",
          "628:         docker_cache_prod_directive+=(",
          "629:             \"--cache-to=type=registry,ref=${AIRFLOW_PROD_IMAGE}:cache,mode=max\"",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "629:         # we need to login to docker registry so that we can push cache there",
          "630:         build_images::login_to_docker_registry",
          "",
          "---------------"
        ]
      }
    }
  ]
}