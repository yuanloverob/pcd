{
  "cve_id": "CVE-2022-31777",
  "cve_desc": "A stored cross-site scripting (XSS) vulnerability in Apache Spark 3.2.1 and earlier, and 3.3.0, allows remote attackers to execute arbitrary JavaScript in the web browser of a user, by including a malicious payload into the logs which would be returned in logs rendered in the UI.",
  "repo": "apache/spark",
  "patch_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
  "patch_info": {
    "commit_hash": "ad90195de56688ce0898691eb9d04297ab0871ad",
    "repo": "apache/spark",
    "commit_url": "https://github.com/apache/spark/commit/ad90195de56688ce0898691eb9d04297ab0871ad",
    "files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ],
    "message": "[SPARK-39505][UI] Escape log content rendered in UI\n\n### What changes were proposed in this pull request?\n\nEscape log content rendered to the UI.\n\n### Why are the changes needed?\n\nLog content may contain reserved characters or other code in the log and be misinterpreted in the UI as HTML.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nExisting tests\n\nCloses #36902 from srowen/LogViewEscape.\n\nAuthored-by: Sean Owen <srowen@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
    "before_after_code_files": [
      "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js"
    ]
  },
  "patch_diff": {
    "core/src/main/resources/org/apache/spark/ui/static/log-view.js||core/src/main/resources/org/apache/spark/ui/static/log-view.js": [
      "File: core/src/main/resources/org/apache/spark/ui/static/log-view.js -> core/src/main/resources/org/apache/spark/ui/static/log-view.js",
      "--- Hunk 1 ---",
      "[Context before]",
      "85:       if (retStartByte == 0) {",
      "86:         disableMoreButton();",
      "87:       }",
      "90:       curLogLength = curLogLength + (startByte - retStartByte);",
      "91:       startByte = retStartByte;",
      "",
      "[Removed Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(cleanData);",
      "",
      "[Added Lines]",
      "88:       $(\"pre\", \".log-content\").prepend(document.createTextNode(cleanData));",
      "",
      "---------------",
      "--- Hunk 2 ---",
      "[Context before]",
      "115:             var retLogLength = dataInfo[2];",
      "117:             var cleanData = data.substring(newlineIndex + 1);",
      "120:             curLogLength = curLogLength + (retEndByte - retStartByte);",
      "121:             endByte = retEndByte;",
      "",
      "[Removed Lines]",
      "118:             $(\"pre\", \".log-content\").append(cleanData);",
      "",
      "[Added Lines]",
      "118:             $(\"pre\", \".log-content\").append(document.createTextNode(cleanData));",
      "",
      "---------------"
    ]
  },
  "candidates": [
    {
      "candidate_hash": "2faaf8a95609b44bfd0bcfd59629cd27a9abb024",
      "candidate_info": {
        "commit_hash": "2faaf8a95609b44bfd0bcfd59629cd27a9abb024",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/2faaf8a95609b44bfd0bcfd59629cd27a9abb024",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala",
          "sql/core/src/test/resources/sql-tests/results/ansi/cast.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/date.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/datetime-parsing-invalid.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/decimalArithmeticOperations.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/interval.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/map.sql.out",
          "sql/core/src/test/resources/sql-tests/results/ansi/string-functions.sql.out",
          "sql/core/src/test/resources/sql-tests/results/interval.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/boolean.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/float4.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/float8.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/int4.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/int8.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/select_having.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/text.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part2.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part3.sql.out",
          "sql/core/src/test/resources/sql-tests/results/postgreSQL/window_part4.sql.out",
          "sql/core/src/test/resources/sql-tests/results/udf/postgreSQL/udf-select_having.sql.out"
        ],
        "message": "[SPARK-39272][SQL][3.3] Increase the start position of query context by 1\n\n### What changes were proposed in this pull request?\n\nIncrease the start position of query context by 1\n\n### Why are the changes needed?\n\n  Currently, the line number starts from 1, while the start position starts from 0.\nThus it's better to increase the start position by 1 for consistency.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nUT\n\nCloses #36684 from gengliangwang/portSPARK-39234.\n\nAuthored-by: Gengliang Wang <gengliang@apache.org>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "79:       \"\"",
          "80:     } else {",
          "81:       val positionContext = if (line.isDefined && startPosition.isDefined) {",
          "83:       } else {",
          "84:         \"\"",
          "85:       }",
          "",
          "[Removed Lines]",
          "82:         s\"(line ${line.get}, position ${startPosition.get})\"",
          "",
          "[Added Lines]",
          "84:         s\"(line ${line.get}, position ${startPosition.get + 1})\"",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/trees/TreeNodeSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "876:       objectType = Some(\"VIEW\"),",
          "877:       objectName = Some(\"some_view\"))",
          "878:     val expected =",
          "880:         |...7890 + 1234567890 + 1234567890, cast('a'",
          "881:         |                                   ^^^^^^^^",
          "",
          "[Removed Lines]",
          "879:       \"\"\"== SQL of VIEW some_view(line 3, position 38) ==",
          "",
          "[Added Lines]",
          "879:       \"\"\"== SQL of VIEW some_view(line 3, position 39) ==",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "427148f86e28576ec5eb9799edff4d8106758082",
      "candidate_info": {
        "commit_hash": "427148f86e28576ec5eb9799edff4d8106758082",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/427148f86e28576ec5eb9799edff4d8106758082",
        "files": [
          "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDeserializer.scala",
          "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroCatalystDataConversionSuite.scala"
        ],
        "message": "[SPARK-39575][AVRO] add ByteBuffer#rewind after ByteBuffer#get in Avr\u2026\n\n\u2026oDeserializer\n\n### What changes were proposed in this pull request?\nAdd ByteBuffer#rewind after ByteBuffer#get in AvroDeserializer.\n\n### Why are the changes needed?\n- HeapBuffer.get(bytes) puts the data from POS to the end into bytes, and sets POS as the end. The next call will return empty bytes.\n- The second call of AvroDeserializer will return an InternalRow with empty binary column when avro record has binary column.\n\n### Does this PR introduce _any_ user-facing change?\nNo.\n\n### How was this patch tested?\nAdd ut in AvroCatalystDataConversionSuite.\n\nCloses #36973 from wzx140/avro-fix.\n\nAuthored-by: wangzixuan.wzxuan <wangzixuan.wzxuan@bytedance.com>\nSigned-off-by: Sean Owen <srowen@gmail.com>\n(cherry picked from commit 558b395880673ec45bf9514c98983e50e21d9398)\nSigned-off-by: Sean Owen <srowen@gmail.com>",
        "before_after_code_files": [
          "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDeserializer.scala||external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDeserializer.scala",
          "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroCatalystDataConversionSuite.scala||external/avro/src/test/scala/org/apache/spark/sql/avro/AvroCatalystDataConversionSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 0,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDeserializer.scala||external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDeserializer.scala": [
          "File: external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDeserializer.scala -> external/avro/src/main/scala/org/apache/spark/sql/avro/AvroDeserializer.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "195:           case b: ByteBuffer =>",
          "196:             val bytes = new Array[Byte](b.remaining)",
          "197:             b.get(bytes)",
          "198:             bytes",
          "199:           case b: Array[Byte] => b",
          "200:           case other =>",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "199:             b.rewind()",
          "",
          "---------------"
        ],
        "external/avro/src/test/scala/org/apache/spark/sql/avro/AvroCatalystDataConversionSuite.scala||external/avro/src/test/scala/org/apache/spark/sql/avro/AvroCatalystDataConversionSuite.scala": [
          "File: external/avro/src/test/scala/org/apache/spark/sql/avro/AvroCatalystDataConversionSuite.scala -> external/avro/src/test/scala/org/apache/spark/sql/avro/AvroCatalystDataConversionSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "360:       None,",
          "361:       new OrderedFilters(Seq(Not(EqualTo(\"Age\", 39))), sqlSchema))",
          "362:   }",
          "363: }",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "364:   test(\"AvroDeserializer with binary type\") {",
          "365:     val jsonFormatSchema =",
          "366:       \"\"\"",
          "367:         |{",
          "368:         |  \"type\": \"record\",",
          "369:         |  \"name\": \"record\",",
          "370:         |  \"fields\" : [",
          "371:         |    {\"name\": \"a\", \"type\": \"bytes\"}",
          "372:         |  ]",
          "373:         |}",
          "374:       \"\"\".stripMargin",
          "375:     val avroSchema = new Schema.Parser().parse(jsonFormatSchema)",
          "376:     val avroRecord = new GenericData.Record(avroSchema)",
          "377:     val bb = java.nio.ByteBuffer.wrap(Array[Byte](97, 48, 53))",
          "378:     avroRecord.put(\"a\", bb)",
          "380:     val expected = InternalRow(Array[Byte](97, 48, 53))",
          "381:     checkDeserialization(avroSchema, avroRecord, Some(expected))",
          "382:     checkDeserialization(avroSchema, avroRecord, Some(expected))",
          "383:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "3df34214a2f6e55e95e424df0ba5629ccdfe4980",
      "candidate_info": {
        "commit_hash": "3df34214a2f6e55e95e424df0ba5629ccdfe4980",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/3df34214a2f6e55e95e424df0ba5629ccdfe4980",
        "files": [
          "python/pyspark/sql/tests/test_udf.py",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ExtractPythonUDFFromJoinConditionSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala",
          "sql/core/src/test/resources/sql-tests/results/join-lateral.sql.out",
          "sql/core/src/test/resources/sql-tests/results/transform.sql.out",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/SparkScriptTransformationSuite.scala"
        ],
        "message": "[SPARK-39027][SQL][3.3] Output SQL statements in error messages in upper case and w/o double quotes\n\n### What changes were proposed in this pull request?\nIn the PR, I propose to output any SQL statement in error messages in upper case, and apply new implementation of `QueryErrorsBase.toSQLStmt()` to all exceptions in `Query.*Errors` w/ error classes. Also this PR modifies all affected tests, see the list in the section \"How was this patch tested?\".\n\nThis is a backport of https://github.com/apache/spark/pull/36359.\n\n### Why are the changes needed?\nTo improve user experience with Spark SQL by highlighting SQL statements in error massage and make them more visible to users. Also this PR makes SQL statements in error messages consistent to the docs where such elements are showed in upper case w/ any quotes.\n\n### Does this PR introduce _any_ user-facing change?\nYes. The changes might influence on error messages:\n\nBefore:\n```sql\nThe operation \"DESC PARTITION\" is not allowed\n```\n\nAfter:\n```sql\nThe operation DESC PARTITION is not allowed\n```\n\n### How was this patch tested?\nBy running affected test suites:\n```\n$ build/sbt \"sql/testOnly *QueryExecutionErrorsSuite\"\n$ build/sbt \"sql/testOnly *QueryParsingErrorsSuite\"\n$ build/sbt \"sql/testOnly *QueryCompilationErrorsSuite\"\n$ build/sbt \"test:testOnly *QueryCompilationErrorsDSv2Suite\"\n$ build/sbt \"test:testOnly *ExtractPythonUDFFromJoinConditionSuite\"\n$ build/sbt \"testOnly *PlanParserSuite\"\n$ build/sbt \"sql/testOnly *SQLQueryTestSuite -- -z transform.sql\"\n$ build/sbt \"sql/testOnly *SQLQueryTestSuite -- -z join-lateral.sql\"\n$ build/sbt \"sql/testOnly *SQLQueryTestSuite -- -z describe.sql\"\n$ build/sbt \"testOnly *DDLParserSuite\"\n```\n\nCloses #36363 from MaxGekk/error-class-toSQLStmt-no-quotes-3.3.\n\nAuthored-by: Max Gekk <max.gekk@gmail.com>\nSigned-off-by: Dongjoon Hyun <dongjoon@apache.org>",
        "before_after_code_files": [
          "python/pyspark/sql/tests/test_udf.py||python/pyspark/sql/tests/test_udf.py",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ExtractPythonUDFFromJoinConditionSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ExtractPythonUDFFromJoinConditionSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/SparkScriptTransformationSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/SparkScriptTransformationSuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "python/pyspark/sql/tests/test_udf.py||python/pyspark/sql/tests/test_udf.py": [
          "File: python/pyspark/sql/tests/test_udf.py -> python/pyspark/sql/tests/test_udf.py",
          "--- Hunk 1 ---",
          "[Context before]",
          "258:         def runWithJoinType(join_type, type_string):",
          "259:             with self.assertRaisesRegex(",
          "260:                 AnalysisException,",
          "262:                 % type_string,",
          "263:             ):",
          "264:                 left.join(right, [f(\"a\", \"b\"), left.a1 == right.b1], join_type).collect()",
          "",
          "[Removed Lines]",
          "261:                 \"\"\"Using PythonUDF in join condition of join type \"%s\" is not supported\"\"\"",
          "",
          "[Added Lines]",
          "261:                 \"\"\"Using PythonUDF in join condition of join type %s is not supported\"\"\"",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryErrorsBase.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "52:     \"\\\"\" + elem + \"\\\"\"",
          "53:   }",
          "56:   def toSQLStmt(text: String): String = {",
          "58:   }",
          "60:   def toSQLId(parts: Seq[String]): String = {",
          "",
          "[Removed Lines]",
          "57:     quoteByDefault(text.toUpperCase(Locale.ROOT))",
          "",
          "[Added Lines]",
          "56:     text.toUpperCase(Locale.ROOT)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ExtractPythonUDFFromJoinConditionSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ExtractPythonUDFFromJoinConditionSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ExtractPythonUDFFromJoinConditionSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ExtractPythonUDFFromJoinConditionSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "189:       }",
          "190:       assert(e.message ==",
          "191:         \"The feature is not supported: \" +",
          "194:       val query2 = testRelationLeft.join(",
          "195:         testRelationRight,",
          "",
          "[Removed Lines]",
          "192:         s\"\"\"Using PythonUDF in join condition of join type \"${joinType.sql}\" is not supported.\"\"\")",
          "",
          "[Added Lines]",
          "192:         s\"\"\"Using PythonUDF in join condition of join type ${joinType.sql} is not supported.\"\"\")",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/DDLParserSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "2069:     comparePlans(",
          "2070:       parsePlan(\"SHOW FUNCTIONS IN db LIKE 'funct*'\"),",
          "2071:       ShowFunctions(UnresolvedNamespace(Seq(\"db\")), true, true, Some(\"funct*\")))",
          "2073:     intercept(\"SHOW FUNCTIONS IN db f1\",",
          "2075:     intercept(\"SHOW FUNCTIONS IN db LIKE f1\",",
          "2079:     comparePlans(",
          "",
          "[Removed Lines]",
          "2072:     intercept(\"SHOW other FUNCTIONS\", \"\\\"SHOW\\\" other \\\"FUNCTIONS\\\" not supported\")",
          "2074:       \"Invalid pattern in \\\"SHOW FUNCTIONS\\\": `f1`\")",
          "2076:       \"Invalid pattern in \\\"SHOW FUNCTIONS\\\": `f1`\")",
          "",
          "[Added Lines]",
          "2072:     intercept(\"SHOW other FUNCTIONS\", \"SHOW other FUNCTIONS not supported\")",
          "2074:       \"Invalid pattern in SHOW FUNCTIONS: `f1`\")",
          "2076:       \"Invalid pattern in SHOW FUNCTIONS: `f1`\")",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/PlanParserSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1245:         |    \"escapeChar\" = \"\\\\\")",
          "1246:         |FROM testData",
          "1247:       \"\"\".stripMargin,",
          "1249:   }",
          "",
          "[Removed Lines]",
          "1248:       \"\\\"TRANSFORM\\\" with serde is only supported in hive mode\")",
          "",
          "[Added Lines]",
          "1248:       \"TRANSFORM with serde is only supported in hive mode\")",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsDSv2Suite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:         checkAnswer(spark.table(tbl), spark.emptyDataFrame)",
          "45:         assert(e.getMessage === \"The feature is not supported: \" +",
          "47:         assert(e.getErrorClass === \"UNSUPPORTED_FEATURE\")",
          "48:         assert(e.getSqlState === \"0A000\")",
          "49:       }",
          "",
          "[Removed Lines]",
          "46:           s\"\"\"\"IF NOT EXISTS\" for the table `testcat`.`ns1`.`ns2`.`tbl` by \"INSERT INTO\".\"\"\")",
          "",
          "[Added Lines]",
          "46:           s\"\"\"IF NOT EXISTS for the table `testcat`.`ns1`.`ns2`.`tbl` by INSERT INTO.\"\"\")",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryCompilationErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "149:     assert(e.getSqlState === \"0A000\")",
          "150:     assert(e.message ===",
          "151:       \"The feature is not supported: \" +",
          "153:   }",
          "155:   test(\"UNSUPPORTED_FEATURE: Using pandas UDF aggregate expression with pivot\") {",
          "",
          "[Removed Lines]",
          "152:       \"Using PythonUDF in join condition of join type \\\"LEFT OUTER\\\" is not supported.\")",
          "",
          "[Added Lines]",
          "152:       \"Using PythonUDF in join condition of join type LEFT OUTER is not supported.\")",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryExecutionErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "156:     }",
          "157:     assert(e1.getErrorClass === \"UNSUPPORTED_FEATURE\")",
          "158:     assert(e1.getSqlState === \"0A000\")",
          "161:     val e2 = intercept[SparkUnsupportedOperationException] {",
          "162:       trainingSales",
          "",
          "[Removed Lines]",
          "159:     assert(e1.getMessage === \"\"\"The feature is not supported: Repeated \"PIVOT\"s.\"\"\")",
          "",
          "[Added Lines]",
          "159:     assert(e1.getMessage === \"\"\"The feature is not supported: Repeated PIVOTs.\"\"\")",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "167:     }",
          "168:     assert(e2.getErrorClass === \"UNSUPPORTED_FEATURE\")",
          "169:     assert(e2.getSqlState === \"0A000\")",
          "171:   }",
          "173:   test(\"INCONSISTENT_BEHAVIOR_CROSS_VERSION: \" +",
          "",
          "[Removed Lines]",
          "170:     assert(e2.getMessage === \"\"\"The feature is not supported: \"PIVOT\" not after a \"GROUP BY\".\"\"\")",
          "",
          "[Added Lines]",
          "170:     assert(e2.getMessage === \"\"\"The feature is not supported: PIVOT not after a GROUP BY.\"\"\")",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/errors/QueryParsingErrorsSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "44:       sqlState = \"0A000\",",
          "45:       message =",
          "46:         \"\"\"",
          "48:           |",
          "49:           |== SQL ==",
          "50:           |SELECT * FROM t1 NATURAL JOIN LATERAL (SELECT c1 + c2 AS c2)",
          "",
          "[Removed Lines]",
          "47:           |The feature is not supported: \"LATERAL\" join with \"NATURAL\" join.(line 1, pos 14)",
          "",
          "[Added Lines]",
          "47:           |The feature is not supported: LATERAL join with NATURAL join.(line 1, pos 14)",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "59:       sqlState = \"0A000\",",
          "60:       message =",
          "61:         \"\"\"",
          "63:           |",
          "64:           |== SQL ==",
          "65:           |SELECT * FROM t1 JOIN LATERAL (SELECT c1 + c2 AS c2) USING (c2)",
          "",
          "[Removed Lines]",
          "62:           |The feature is not supported: \"LATERAL\" join with \"USING\" join.(line 1, pos 14)",
          "",
          "[Added Lines]",
          "62:           |The feature is not supported: LATERAL join with USING join.(line 1, pos 14)",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "75:         sqlState = \"0A000\",",
          "76:         message =",
          "77:           s\"\"\"",
          "79:             |",
          "80:             |== SQL ==",
          "81:             |SELECT * FROM t1 $joinType JOIN LATERAL (SELECT c1 + c2 AS c3) ON c2 = c3",
          "",
          "[Removed Lines]",
          "78:             |The feature is not supported: \"LATERAL\" join type \"$joinType\".(line 1, pos 14)",
          "",
          "[Added Lines]",
          "78:             |The feature is not supported: LATERAL join type $joinType.(line 1, pos 14)",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "99:         sqlState = \"42000\",",
          "100:         message =",
          "101:           s\"\"\"",
          "103:             |",
          "104:             |== SQL ==",
          "105:             |$sqlText",
          "",
          "[Removed Lines]",
          "102:             |Invalid SQL syntax: \"LATERAL\" can only be used with subquery.(line 1, pos $pos)",
          "",
          "[Added Lines]",
          "102:             |Invalid SQL syntax: LATERAL can only be used with subquery.(line 1, pos $pos)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "115:       sqlState = \"0A000\",",
          "116:       message =",
          "117:         \"\"\"",
          "119:           |",
          "120:           |== SQL ==",
          "121:           |SELECT * FROM a NATURAL CROSS JOIN b",
          "",
          "[Removed Lines]",
          "118:           |The feature is not supported: \"NATURAL CROSS JOIN\".(line 1, pos 14)",
          "",
          "[Added Lines]",
          "118:           |The feature is not supported: NATURAL CROSS JOIN.(line 1, pos 14)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "175:       sqlState = \"0A000\",",
          "176:       message =",
          "177:         \"\"\"",
          "179:           |",
          "180:           |== SQL ==",
          "181:           |SELECT TRANSFORM(DISTINCT a) USING 'a' FROM t",
          "",
          "[Removed Lines]",
          "178:           |The feature is not supported: \"TRANSFORM\" does not support \"DISTINCT\"/\"ALL\" in inputs(line 1, pos 17)",
          "",
          "[Added Lines]",
          "178:           |The feature is not supported: TRANSFORM does not support DISTINCT/ALL in inputs(line 1, pos 17)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "191:       sqlState = \"0A000\",",
          "192:       message =",
          "193:         \"\"\"",
          "195:           |",
          "196:           |== SQL ==",
          "197:           |SELECT TRANSFORM(a) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' USING 'a' FROM t",
          "",
          "[Removed Lines]",
          "194:           |The feature is not supported: \"TRANSFORM\" with serde is only supported in hive mode(line 1, pos 0)",
          "",
          "[Added Lines]",
          "194:           |The feature is not supported: TRANSFORM with serde is only supported in hive mode(line 1, pos 0)",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/SparkScriptTransformationSuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/SparkScriptTransformationSuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/SparkScriptTransformationSuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/SparkScriptTransformationSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "56:             |FROM v",
          "57:           \"\"\".stripMargin)",
          "58:       }.getMessage",
          "60:     }",
          "61:   }",
          "62: }",
          "",
          "[Removed Lines]",
          "59:       assert(e.contains(\"\\\"TRANSFORM\\\" with serde is only supported in hive mode\"))",
          "",
          "[Added Lines]",
          "59:       assert(e.contains(\"TRANSFORM with serde is only supported in hive mode\"))",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "4e5ada90cfb89caa25addd8991cec2af843e24a9",
      "candidate_info": {
        "commit_hash": "4e5ada90cfb89caa25addd8991cec2af843e24a9",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/4e5ada90cfb89caa25addd8991cec2af843e24a9",
        "files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala"
        ],
        "message": "[SPARK-39417][SQL] Handle Null partition values in PartitioningUtils\n\n### What changes were proposed in this pull request?\n\nWe should not try casting everything returned by `removeLeadingZerosFromNumberTypePartition` to string, as it returns null value for the cases when partition has null value and is already replaced by `DEFAULT_PARTITION_NAME`\n\n### Why are the changes needed?\n\nfor null partitions where `removeLeadingZerosFromNumberTypePartition` is called it would throw a NPE and hence the query would fail.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo.\n\n### How was this patch tested?\n\nAdded a UT, which would fail with an NPE otherwise.\n\nCloses #36810 from singhpk234/psinghvk/fix-npe.\n\nAuthored-by: Prashant Singh <psinghvk@amazon.com>\nSigned-off-by: huaxingao <huaxin_gao@apple.com>\n(cherry picked from commit dcfd9f01289f26c1a25e97432710a13772b3ad4c)\nSigned-off-by: huaxingao <huaxin_gao@apple.com>",
        "before_after_code_files": [
          "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala||sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala": [
          "File: sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala -> sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "359:   def removeLeadingZerosFromNumberTypePartition(value: String, dataType: DataType): String =",
          "360:     dataType match {",
          "361:       case ByteType | ShortType | IntegerType | LongType | FloatType | DoubleType =>",
          "363:       case _ => value",
          "364:     }",
          "",
          "[Removed Lines]",
          "362:         castPartValueToDesiredType(dataType, value, null).toString",
          "",
          "[Added Lines]",
          "362:         Option(castPartValueToDesiredType(dataType, value, null)).map(_.toString).orNull",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetPartitionDiscoverySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "1259:     assert(\"p_int=10/p_float=1.0\" === path)",
          "1260:   }",
          "1262:   test(\"read partitioned table - partition key included in Parquet file\") {",
          "1263:     withTempDir { base =>",
          "1264:       for {",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "1262:   test(\"SPARK-39417: Null partition value\") {",
          "1264:     val spec = Map(\"p_int\"-> ExternalCatalogUtils.DEFAULT_PARTITION_NAME)",
          "1265:     val schema = new StructType().add(\"p_int\", \"int\")",
          "1266:     val path = PartitioningUtils.getPathFragment(spec, schema)",
          "1267:     assert(s\"p_int=${ExternalCatalogUtils.DEFAULT_PARTITION_NAME}\" === path)",
          "1268:   }",
          "",
          "---------------"
        ]
      }
    },
    {
      "candidate_hash": "63f20c526bed8346fe3399aff6c0b2f7a78b441e",
      "candidate_info": {
        "commit_hash": "63f20c526bed8346fe3399aff6c0b2f7a78b441e",
        "repo": "apache/spark",
        "commit_url": "https://github.com/apache/spark/commit/63f20c526bed8346fe3399aff6c0b2f7a78b441e",
        "files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/arithmetic.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ],
        "message": "[SPARK-39166][SQL] Provide runtime error query context for binary arithmetic when WSCG is off\n\n### What changes were proposed in this pull request?\n\nCurrently, for most of the cases, the project https://issues.apache.org/jira/browse/SPARK-38615 is able to show where the runtime errors happen within the original query.\nHowever, after trying on production, I found that the following queries won't show where the divide by 0 error happens\n```\ncreate table aggTest(i int, j int, k int, d date) using parquet\ninsert into aggTest values(1, 2, 0, date'2022-01-01')\nselect sum(j)/sum(k),percentile(i, 0.9) from aggTest group by d\n```\nWith `percentile` function in the query, the plan can't execute with whole stage codegen. Thus the child plan of `Project` is serialized to executors for execution, from ProjectExec:\n```\n  protected override def doExecute(): RDD[InternalRow] = {\n    child.execute().mapPartitionsWithIndexInternal { (index, iter) =>\n      val project = UnsafeProjection.create(projectList, child.output)\n      project.initialize(index)\n      iter.map(project)\n    }\n  }\n```\n\nNote that the `TreeNode.origin` is not serialized to executors since `TreeNode` doesn't extend the trait `Serializable`, which results in an empty query context on errors. For more details, please read https://issues.apache.org/jira/browse/SPARK-39140\n\nA dummy fix is to make `TreeNode` extend the trait `Serializable`. However, it can be performance regression if the query text is long (every `TreeNode` carries it for serialization).\nA better fix is to introduce a new trait `SupportQueryContext` and materialize the truncated query context for special expressions. This PR targets on binary arithmetic expressions only. I will create follow-ups for the remaining expressions which support runtime error query context.\n\n### Why are the changes needed?\n\nImprove the error context framework and make sure it works when whole stage codegen is not available.\n\n### Does this PR introduce _any_ user-facing change?\n\nNo\n\n### How was this patch tested?\n\nUnit tests\n\nCloses #36525 from gengliangwang/serializeContext.\n\nLead-authored-by: Gengliang Wang <gengliang@apache.org>\nCo-authored-by: Gengliang Wang <ltnwgl@gmail.com>\nSigned-off-by: Gengliang Wang <gengliang@apache.org>\n(cherry picked from commit e336567c8a9704b500efecd276abaf5bd3988679)\nSigned-off-by: Gengliang Wang <gengliang@apache.org>",
        "before_after_code_files": [
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala",
          "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/arithmetic.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/arithmetic.scala",
          "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala",
          "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala"
        ]
      },
      "candidate_patch_features": {
        "candidate_earlier_than_patch": 1,
        "same_pr": 1,
        "olp_pr_links": [
          "https://github.com/jack-steven-root/spark/pull/1"
        ],
        "olp_code_files": {
          "patch": [],
          "candidate": []
        }
      },
      "candidate_diff": {
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "588:   }",
          "589: }",
          "592: object UnaryExpression {",
          "593:   def unapply(e: UnaryExpression): Option[Expression] = Some(e.child)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "595: trait SupportQueryContext extends Expression with Serializable {",
          "596:   protected var queryContext: String = initQueryContext()",
          "598:   def initQueryContext(): String",
          "604:   override def copyTagsFrom(other: Expression): Unit = {",
          "605:     other match {",
          "606:       case s: SupportQueryContext =>",
          "607:         queryContext = s.queryContext",
          "608:       case _ =>",
          "609:     }",
          "610:     super.copyTagsFrom(other)",
          "611:   }",
          "612: }",
          "",
          "---------------"
        ],
        "sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/arithmetic.scala||sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/arithmetic.scala": [
          "File: sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/arithmetic.scala -> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/arithmetic.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "21: import org.apache.spark.sql.catalyst.analysis.{FunctionRegistry, TypeCheckResult, TypeCoercion}",
          "22: import org.apache.spark.sql.catalyst.expressions.codegen._",
          "23: import org.apache.spark.sql.catalyst.expressions.codegen.Block._",
          "26: import org.apache.spark.sql.catalyst.util.{IntervalUtils, MathUtils, TypeUtils}",
          "27: import org.apache.spark.sql.errors.QueryExecutionErrors",
          "28: import org.apache.spark.sql.internal.SQLConf",
          "",
          "[Removed Lines]",
          "24: import org.apache.spark.sql.catalyst.trees.TreePattern.{BINARY_ARITHMETIC, TreePattern,",
          "25:   UNARY_POSITIVE}",
          "",
          "[Added Lines]",
          "24: import org.apache.spark.sql.catalyst.trees.TreePattern.{BINARY_ARITHMETIC, TreePattern, UNARY_POSITIVE}",
          "",
          "---------------",
          "--- Hunk 2 ---",
          "[Context before]",
          "209:   override protected def withNewChildInternal(newChild: Expression): Abs = copy(child = newChild)",
          "210: }",
          "214:   protected val failOnError: Boolean",
          "",
          "[Removed Lines]",
          "212: abstract class BinaryArithmetic extends BinaryOperator with NullIntolerant {",
          "",
          "[Added Lines]",
          "211: abstract class BinaryArithmetic extends BinaryOperator with NullIntolerant",
          "212:     with SupportQueryContext {",
          "",
          "---------------",
          "--- Hunk 3 ---",
          "[Context before]",
          "220:   override lazy val resolved: Boolean = childrenResolved && checkInputDataTypes().isSuccess",
          "223:   def decimalMethod: String =",
          "224:     throw QueryExecutionErrors.notOverrideExpectedMethodsError(\"BinaryArithmetics\",",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "222:   override def initQueryContext(): String = {",
          "223:     if (failOnError) {",
          "224:       origin.context",
          "225:     } else {",
          "226:       \"\"",
          "227:     }",
          "228:   }",
          "",
          "---------------",
          "--- Hunk 4 ---",
          "[Context before]",
          "270:       })",
          "271:     case IntegerType | LongType if failOnError && exactMathMethod.isDefined =>",
          "272:       nullSafeCodeGen(ctx, ev, (eval1, eval2) => {",
          "274:         val mathUtils = MathUtils.getClass.getCanonicalName.stripSuffix(\"$\")",
          "275:         s\"\"\"",
          "276:            |${ev.value} = $mathUtils.${exactMathMethod.get}($eval1, $eval2, $errorContext);",
          "",
          "[Removed Lines]",
          "273:         val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "",
          "[Added Lines]",
          "281:         val errorContext = ctx.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------",
          "--- Hunk 5 ---",
          "[Context before]",
          "331:     case _: YearMonthIntervalType =>",
          "332:       MathUtils.addExact(input1.asInstanceOf[Int], input2.asInstanceOf[Int])",
          "333:     case _: IntegerType if failOnError =>",
          "335:     case _: LongType if failOnError =>",
          "337:     case _ => numeric.plus(input1, input2)",
          "338:   }",
          "",
          "[Removed Lines]",
          "334:       MathUtils.addExact(input1.asInstanceOf[Int], input2.asInstanceOf[Int], origin.context)",
          "336:       MathUtils.addExact(input1.asInstanceOf[Long], input2.asInstanceOf[Long], origin.context)",
          "",
          "[Added Lines]",
          "342:       MathUtils.addExact(input1.asInstanceOf[Int], input2.asInstanceOf[Int], queryContext)",
          "344:       MathUtils.addExact(input1.asInstanceOf[Long], input2.asInstanceOf[Long], queryContext)",
          "",
          "---------------",
          "--- Hunk 6 ---",
          "[Context before]",
          "381:     case _: YearMonthIntervalType =>",
          "382:       MathUtils.subtractExact(input1.asInstanceOf[Int], input2.asInstanceOf[Int])",
          "383:     case _: IntegerType if failOnError =>",
          "385:     case _: LongType if failOnError =>",
          "387:     case _ => numeric.minus(input1, input2)",
          "388:   }",
          "",
          "[Removed Lines]",
          "384:       MathUtils.subtractExact(input1.asInstanceOf[Int], input2.asInstanceOf[Int], origin.context)",
          "386:       MathUtils.subtractExact(input1.asInstanceOf[Long], input2.asInstanceOf[Long], origin.context)",
          "",
          "[Added Lines]",
          "392:       MathUtils.subtractExact(input1.asInstanceOf[Int], input2.asInstanceOf[Int], queryContext)",
          "394:       MathUtils.subtractExact(input1.asInstanceOf[Long], input2.asInstanceOf[Long], queryContext)",
          "",
          "---------------",
          "--- Hunk 7 ---",
          "[Context before]",
          "419:   protected override def nullSafeEval(input1: Any, input2: Any): Any = dataType match {",
          "420:     case _: IntegerType if failOnError =>",
          "422:     case _: LongType if failOnError =>",
          "424:     case _ => numeric.times(input1, input2)",
          "425:   }",
          "",
          "[Removed Lines]",
          "421:       MathUtils.multiplyExact(input1.asInstanceOf[Int], input2.asInstanceOf[Int], origin.context)",
          "423:       MathUtils.multiplyExact(input1.asInstanceOf[Long], input2.asInstanceOf[Long], origin.context)",
          "",
          "[Added Lines]",
          "429:       MathUtils.multiplyExact(input1.asInstanceOf[Int], input2.asInstanceOf[Int], queryContext)",
          "431:       MathUtils.multiplyExact(input1.asInstanceOf[Long], input2.asInstanceOf[Long], queryContext)",
          "",
          "---------------",
          "--- Hunk 8 ---",
          "[Context before]",
          "457:       } else {",
          "458:         if (isZero(input2)) {",
          "461:         }",
          "462:         if (checkDivideOverflow && input1 == Long.MinValue && input2 == -1) {",
          "464:         }",
          "465:         evalOperation(input1, input2)",
          "466:       }",
          "",
          "[Removed Lines]",
          "460:           throw QueryExecutionErrors.divideByZeroError(origin.context)",
          "463:           throw QueryExecutionErrors.overflowInIntegralDivideError(origin.context)",
          "",
          "[Added Lines]",
          "468:           throw QueryExecutionErrors.divideByZeroError(queryContext)",
          "471:           throw QueryExecutionErrors.overflowInIntegralDivideError(queryContext)",
          "",
          "---------------",
          "--- Hunk 9 ---",
          "[Context before]",
          "487:     } else {",
          "488:       s\"($javaType)(${eval1.value} $symbol ${eval2.value})\"",
          "489:     }",
          "491:     val checkIntegralDivideOverflow = if (checkDivideOverflow) {",
          "492:       s\"\"\"",
          "493:         |if (${eval1.value} == ${Long.MinValue}L && ${eval2.value} == -1)",
          "",
          "[Removed Lines]",
          "490:     lazy val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "",
          "[Added Lines]",
          "498:     lazy val errorContext = ctx.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------",
          "--- Hunk 10 ---",
          "[Context before]",
          "743:       } else {",
          "744:         if (isZero(input2)) {",
          "747:         }",
          "748:         input1 match {",
          "749:           case i: Integer => pmod(i, input2.asInstanceOf[java.lang.Integer])",
          "",
          "[Removed Lines]",
          "746:           throw QueryExecutionErrors.divideByZeroError(origin.context)",
          "",
          "[Added Lines]",
          "754:           throw QueryExecutionErrors.divideByZeroError(queryContext)",
          "",
          "---------------",
          "--- Hunk 11 ---",
          "[Context before]",
          "768:     }",
          "769:     val remainder = ctx.freshName(\"remainder\")",
          "770:     val javaType = CodeGenerator.javaType(dataType)",
          "772:     val result = dataType match {",
          "773:       case DecimalType.Fixed(_, _) =>",
          "774:         val decimalAdd = \"$plus\"",
          "",
          "[Removed Lines]",
          "771:     lazy val errorContext = ctx.addReferenceObj(\"errCtx\", origin.context)",
          "",
          "[Added Lines]",
          "779:     lazy val errorContext = ctx.addReferenceObj(\"errCtx\", queryContext)",
          "",
          "---------------"
        ],
        "sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala||sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala": [
          "File: sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala -> sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/ArithmeticExpressionSuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "359:   }",
          "361:   test(\"Remainder/Pmod: exception should contain SQL text context\") {",
          "375:     }",
          "376:   }",
          "",
          "[Removed Lines]",
          "362:     Seq(",
          "363:       Remainder(Literal(1L, LongType), Literal(0L, LongType), failOnError = true),",
          "364:       Pmod(Literal(1L, LongType), Literal(0L, LongType), failOnError = true)).foreach { expr =>",
          "365:         val query = s\"1L ${expr.symbol} 0L\"",
          "366:         val o = Origin(",
          "367:           line = Some(1),",
          "368:           startPosition = Some(7),",
          "369:           startIndex = Some(7),",
          "370:           stopIndex = Some(7 + query.length -1),",
          "371:           sqlText = Some(s\"select $query\"))",
          "372:         withOrigin(o) {",
          "373:           checkExceptionInExpression[ArithmeticException](expr, EmptyRow, query)",
          "374:         }",
          "",
          "[Added Lines]",
          "362:     Seq((\"%\", Remainder), (\"pmod\", Pmod)).foreach { case (symbol, exprBuilder) =>",
          "363:       val query = s\"1L $symbol 0L\"",
          "364:       val o = Origin(",
          "365:         line = Some(1),",
          "366:         startPosition = Some(7),",
          "367:         startIndex = Some(7),",
          "368:         stopIndex = Some(7 + query.length -1),",
          "369:         sqlText = Some(s\"select $query\"))",
          "370:       withOrigin(o) {",
          "371:         val expression = exprBuilder(Literal(1L, LongType), Literal(0L, LongType), true)",
          "372:         checkExceptionInExpression[ArithmeticException](expression, EmptyRow, query)",
          "373:       }",
          "",
          "---------------"
        ],
        "sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala||sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala": [
          "File: sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala -> sql/core/src/test/scala/org/apache/spark/sql/SQLQuerySuite.scala",
          "--- Hunk 1 ---",
          "[Context before]",
          "4359:     }",
          "4360:   }",
          "4362:   test(\"SPARK-38589: try_avg should return null if overflow happens before merging\") {",
          "4363:     val yearMonthDf = Seq(Int.MaxValue, Int.MaxValue, 2)",
          "4364:       .map(Period.ofMonths)",
          "",
          "[Removed Lines]",
          "[None]",
          "",
          "[Added Lines]",
          "4362:   test(\"SPARK-39166: Query context should be serialized to executors when WSCG is off\") {",
          "4363:     withSQLConf(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> \"false\",",
          "4364:       SQLConf.ANSI_ENABLED.key -> \"true\") {",
          "4365:       withTable(\"t\") {",
          "4366:         sql(\"create table t(i int, j int) using parquet\")",
          "4367:         sql(\"insert into t values(2147483647, 10)\")",
          "4368:         Seq(",
          "4369:           \"select i + j from t\",",
          "4370:           \"select -i - j from t\",",
          "4371:           \"select i * j from t\",",
          "4372:           \"select i / (j - 10) from t\").foreach { query =>",
          "4373:           val msg = intercept[SparkException] {",
          "4374:             sql(query).collect()",
          "4375:           }.getMessage",
          "4376:           assert(msg.contains(query))",
          "4377:         }",
          "4378:       }",
          "4379:     }",
          "4380:   }",
          "",
          "---------------"
        ]
      }
    }
  ]
}